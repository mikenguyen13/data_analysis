% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\usepackage{soul}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{titling}
\pretitle{\begin{center} \includegraphics[width=2in,height=2in]{logo.png}\LARGE\\}
\posttitle{\end{center}}
\usepackage{array}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{siunitx}
\usepackage[normalem]{ulem}
\usepackage{colortbl}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{calc}
\usepackage{tabularx}
\usepackage{threeparttable}
\usepackage{wrapfig}
\usepackage{adjustbox}
\usepackage{hyperref}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={A Guide on Data Analysis},
  pdfauthor={Mike Nguyen},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{A Guide on Data Analysis}
\author{Mike Nguyen}
\date{2025-02-03}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}
\addcontentsline{toc}{chapter}{Preface}

This book is an effort to simplify and demystify the complex process of data analysis, making it accessible to a wide audience. While I do not claim to be a professional statistician, econometrician, or data scientist, I firmly believe in the value of learning through teaching and practical application. Writing this book has been as much a learning journey for me as I hope it will be for you.

The intended audience includes those with little to no experience in statistics, econometrics, or data science, as well as individuals with a budding interest in these fields who are eager to deepen their knowledge. While my primary domain of interest is marketing, the principles and methods discussed in this book are universally applicable to any discipline that employs scientific methods or data analysis.

I hope this book provides a valuable starting point for aspiring statisticians, econometricians, and data scientists, empowering you to navigate the fascinating world of causal inference and data analysis with confidence.

\includegraphics[width=3.125in,height=3.125in]{logo.png}

\hypertarget{how-to-cite-this-book}{%
\section*{How to cite this book}\label{how-to-cite-this-book}}
\addcontentsline{toc}{section}{How to cite this book}

\begin{quote}
\textbf{1. APA (7th edition):}

Nguyen, M. (2020). \emph{A Guide on Data Analysis}. Bookdown.

\href{https://bookdown.org/mike/data_analysis/}{\textbf{https://bookdown.org/mike/data\_analysis/}}

\textbf{2. MLA (8th edition):}

Nguyen, Mike. \emph{A Guide on Data Analysis}. Bookdown, 2020. \href{https://bookdown.org/mike/data_analysis/}{\textbf{https://bookdown.org/mike/data\_analysis/}}

\textbf{3. Chicago (17th edition):}

Nguyen, Mike. 2020. \emph{A Guide on Data Analysis}. Bookdown. \href{https://bookdown.org/mike/data_analysis/}{\textbf{https://bookdown.org/mike/data\_analysis/}}

\textbf{4. Harvard:}

Nguyen, M. (2020) \emph{A Guide on Data Analysis}. Bookdown. Available at: \href{https://bookdown.org/mike/data_analysis/}{\textbf{https://bookdown.org/mike/data\_analysis/}}
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\SpecialCharTok{@}\NormalTok{book\{nguyen2020guide,}
\NormalTok{  title}\OtherTok{=}\NormalTok{\{A Guide on Data Analysis\},}
\NormalTok{  author}\OtherTok{=}\NormalTok{\{Nguyen, Mike\},}
\NormalTok{  year}\OtherTok{=}\NormalTok{\{}\DecValTok{2020}\NormalTok{\},}
\NormalTok{  publisher}\OtherTok{=}\NormalTok{\{Bookdown\},}
\NormalTok{  url}\OtherTok{=}\NormalTok{\{https}\SpecialCharTok{:}\ErrorTok{//}\NormalTok{bookdown.org}\SpecialCharTok{/}\NormalTok{mike}\SpecialCharTok{/}\NormalTok{data\_analysis}\SpecialCharTok{/}\NormalTok{\}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{more-books}{%
\section*{More books}\label{more-books}}
\addcontentsline{toc}{section}{More books}

More books by the author can be found \href{https://mikenguyen.netlify.app/books/written_books/}{here}:

\begin{itemize}
\tightlist
\item
  \href{https://bookdown.org/mike/advanced_data_analysis/}{Advanced Data Analysis}: the second book in the data analysis series, which covers machine learning models (with a focus on prediction)
\item
  \href{https://bookdown.org/mike/marketing_research/}{Marketing Research}
\item
  \href{https://bookdown.org/mike/comm_theory/}{Communication Theory}
\end{itemize}

\hypertarget{introduction}{%
\chapter{Introduction}\label{introduction}}

Since the turn of the century, we have witnessed remarkable advancements and innovations, particularly in statistics, information technology, computer science, and the rapidly emerging field of data science. However, one challenge of these developments is the overuse of \textbf{buzzwords} like \emph{big data}, \emph{machine learning}, and \emph{deep learning}. While these terms are powerful in context, they can sometimes obscure the foundational principles underlying their application.

Every substantive field often has its own specialized metric subfield, such as:

\begin{itemize}
\tightlist
\item
  \textbf{Econometrics} in economics\\
\item
  \textbf{Psychometrics} in psychology\\
\item
  \textbf{Chemometrics} in chemistry\\
\item
  \textbf{Sabermetrics} in sports analytics\\
\item
  \textbf{Biostatistics} in public health and medicine
\end{itemize}

To the layperson, these disciplines are often grouped under broader terms like:

\begin{itemize}
\tightlist
\item
  \textbf{Data Science}\\
\item
  \textbf{Applied Statistics}\\
\item
  \textbf{Computational Social Science}
\end{itemize}

As exciting as it is to explore these new tools and techniques, I must admit that retaining these concepts can be challenging. For me, the most effective way to internalize and apply these ideas has been to document the data analysis process from start to finish.

With that in mind, let's dive in and explore the fascinating world of data analysis together.

\includegraphics[width=4.6875in,height=3.64583in]{images/meme.jpg}

\hypertarget{general-recommendations}{%
\section{General Recommendations}\label{general-recommendations}}

\begin{itemize}
\item
  The journey of mastering data analysis is fueled by practice and repetition. The more lines of code you write, the more functions you familiarize yourself with, and the more you experiment, the more enjoyable and rewarding this process becomes.
\item
  Readers can approach this book in several ways:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Focused Learning}: If you are interested in specific methods or tools, you can jump directly to the relevant section by navigating through the table of contents.
  \item
    \textbf{Sequential Learning}: To follow a traditional path of data analysis, start with the \protect\hyperlink{linear-regression}{Linear Regression} section.
  \item
    \textbf{Experimental Approach}: If you are interested in designing experiments and testing hypotheses, explore the \protect\hyperlink{sec-analysis-of-variance-anova}{Analysis of Variance (ANOVA)} section.
  \end{itemize}
\item
  For those primarily interested in applications and less concerned with theoretical foundations, focus on the summary and application sections of each chapter.
\item
  If a concept is unclear, consider researching the topic online. This book serves as a guide, and external resources like tutorials or articles can provide additional insights.
\item
  To customize the code examples provided in this book, use R's built-in help functions. For instance:

  \begin{itemize}
  \tightlist
  \item
    To learn more about a specific function, type \texttt{help(function\_name)} or \texttt{?function\_name} in the R console.
  \item
    For example, to find details about the \texttt{hist} function, type \texttt{?hist} or \texttt{help(hist)} in the console.
  \end{itemize}
\item
  Additionally, searching online is a powerful resource (e.g., Google, ChatGPT, etc.). Different practitioners often use various R packages to achieve similar results. For instance, if you need to create a histogram in R, a simple search like \emph{``histogram in R''} will provide multiple approaches and examples.
\end{itemize}

By adopting these strategies, you can tailor your learning experience and maximize the value of this book.

\textbf{Tools of statistics}

\begin{itemize}
\tightlist
\item
  Probability Theory
\item
  Mathematical Analysis
\item
  Computer Science
\item
  Numerical Analysis
\item
  Database Management
\end{itemize}

\textbf{Code Replication}

This book was built with R version 4.4.2 (2024-10-31 ucrt) and the following packages:

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
package & version & source \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
abind & 1.4-5 & CRAN (R 4.2.0) \\
agridat & 1.21 & CRAN (R 4.2.3) \\
ape & 5.7-1 & CRAN (R 4.2.3) \\
assertthat & 0.2.1 & CRAN (R 4.2.3) \\
backports & 1.4.1 & CRAN (R 4.2.0) \\
bookdown & 0.35 & CRAN (R 4.2.3) \\
boot & 1.3-28.1 & CRAN (R 4.2.3) \\
broom & 1.0.5 & CRAN (R 4.2.3) \\
bslib & 0.6.1 & CRAN (R 4.2.3) \\
cachem & 1.0.8 & CRAN (R 4.2.3) \\
callr & 3.7.3 & CRAN (R 4.2.3) \\
car & 3.1-2 & CRAN (R 4.2.3) \\
carData & 3.0-5 & CRAN (R 4.2.3) \\
cellranger & 1.1.0 & CRAN (R 4.2.3) \\
cli & 3.6.1 & CRAN (R 4.2.3) \\
coda & 0.19-4 & CRAN (R 4.2.3) \\
colorspace & 2.1-0 & CRAN (R 4.2.3) \\
corpcor & 1.6.10 & CRAN (R 4.2.0) \\
crayon & 1.5.2 & CRAN (R 4.2.3) \\
cubature & 2.1.0 & CRAN (R 4.2.3) \\
curl & 5.1.0 & CRAN (R 4.2.3) \\
data.table & 1.14.8 & CRAN (R 4.2.3) \\
DBI & 1.2.0 & CRAN (R 4.2.3) \\
dbplyr & 2.4.0 & CRAN (R 4.2.3) \\
desc & 1.4.3 & CRAN (R 4.2.3) \\
devtools & 2.4.5 & CRAN (R 4.2.3) \\
digest & 0.6.31 & CRAN (R 4.2.3) \\
dplyr & 1.1.2 & CRAN (R 4.2.3) \\
ellipsis & 0.3.2 & CRAN (R 4.2.3) \\
evaluate & 0.23 & CRAN (R 4.2.3) \\
extrafont & 0.19 & CRAN (R 4.2.2) \\
extrafontdb & 1.0 & CRAN (R 4.2.0) \\
fansi & 1.0.4 & CRAN (R 4.2.3) \\
faraway & 1.0.8 & CRAN (R 4.2.3) \\
fastmap & 1.1.1 & CRAN (R 4.2.3) \\
forcats & 1.0.0 & CRAN (R 4.2.3) \\
foreign & 0.8-84 & CRAN (R 4.2.3) \\
fs & 1.6.3 & CRAN (R 4.2.3) \\
generics & 0.1.3 & CRAN (R 4.2.3) \\
ggplot2 & 3.4.4 & CRAN (R 4.2.3) \\
glue & 1.6.2 & CRAN (R 4.2.3) \\
gtable & 0.3.4 & CRAN (R 4.2.3) \\
haven & 2.5.3 & CRAN (R 4.2.3) \\
Hmisc & 5.1-0 & CRAN (R 4.2.3) \\
hms & 1.1.3 & CRAN (R 4.2.3) \\
htmltools & 0.5.7 & CRAN (R 4.2.3) \\
htmlwidgets & 1.6.2 & CRAN (R 4.2.3) \\
httr & 1.4.7 & CRAN (R 4.2.3) \\
investr & 1.4.2 & CRAN (R 4.2.3) \\
jpeg & 0.1-10 & CRAN (R 4.2.2) \\
jquerylib & 0.1.4 & CRAN (R 4.2.3) \\
jsonlite & 1.8.8 & CRAN (R 4.2.3) \\
kableExtra & 1.3.4 & CRAN (R 4.2.3) \\
knitr & 1.45 & CRAN (R 4.2.3) \\
lattice & 0.21-8 & CRAN (R 4.2.3) \\
latticeExtra & 0.6-30 & CRAN (R 4.2.3) \\
lifecycle & 1.0.4 & CRAN (R 4.2.3) \\
lme4 & 1.1-35.1 & CRAN (R 4.2.3) \\
lmerTest & 3.1-3 & CRAN (R 4.2.3) \\
lsr & 0.5.2 & CRAN (R 4.2.3) \\
ltm & 1.2-0 & CRAN (R 4.2.3) \\
lubridate & 1.9.2 & CRAN (R 4.2.3) \\
magrittr & 2.0.3 & CRAN (R 4.2.3) \\
MASS & 7.3-60 & CRAN (R 4.2.3) \\
matlib & 0.9.6 & CRAN (R 4.2.3) \\
Matrix & 1.6-1 & CRAN (R 4.2.3) \\
MCMCglmm & 2.35 & CRAN (R 4.2.3) \\
memoise & 2.0.1 & CRAN (R 4.2.3) \\
mgcv & 1.9-0 & CRAN (R 4.2.3) \\
minqa & 1.2.6 & CRAN (R 4.2.3) \\
modelr & 0.1.11 & CRAN (R 4.2.3) \\
munsell & 0.5.0 & CRAN (R 4.2.3) \\
nlme & 3.1-163 & CRAN (R 4.2.3) \\
nloptr & 2.0.3 & CRAN (R 4.2.3) \\
nlstools & 2.0-0 & CRAN (R 4.2.3) \\
nnet & 7.3-19 & CRAN (R 4.2.3) \\
numDeriv & 2016.8-1.1 & CRAN (R 4.2.0) \\
openxlsx & 4.2.5.2 & CRAN (R 4.2.3) \\
pbkrtest & 0.5.2 & CRAN (R 4.2.3) \\
pillar & 1.9.0 & CRAN (R 4.2.3) \\
pkgbuild & 1.4.3 & CRAN (R 4.2.3) \\
pkgconfig & 2.0.3 & CRAN (R 4.2.3) \\
pkgload & 1.3.3 & CRAN (R 4.2.3) \\
png & 0.1-8 & CRAN (R 4.2.2) \\
ppsr & 0.0.2 & CRAN (R 4.2.3) \\
prettyunits & 1.2.0 & CRAN (R 4.2.3) \\
processx & 3.8.2 & CRAN (R 4.2.3) \\
ps & 1.7.5 & CRAN (R 4.2.3) \\
pscl & 1.5.5.1 & CRAN (R 4.2.3) \\
purrr & 1.0.2 & CRAN (R 4.2.3) \\
R6 & 2.5.1 & CRAN (R 4.2.3) \\
RColorBrewer & 1.1-3 & CRAN (R 4.2.0) \\
Rcpp & 1.0.11 & CRAN (R 4.2.3) \\
readr & 2.1.4 & CRAN (R 4.2.3) \\
readxl & 1.4.3 & CRAN (R 4.2.3) \\
remotes & 2.4.2.1 & CRAN (R 4.2.3) \\
reprex & 2.0.2 & CRAN (R 4.2.3) \\
rgl & 1.2.1 & CRAN (R 4.2.3) \\
rio & 1.0.1 & CRAN (R 4.2.3) \\
rlang & 1.1.1 & CRAN (R 4.2.3) \\
RLRsim & 3.1-8 & CRAN (R 4.2.3) \\
rmarkdown & 2.25 & CRAN (R 4.2.3) \\
rprojroot & 2.0.4 & CRAN (R 4.2.3) \\
rstudioapi & 0.15.0 & CRAN (R 4.2.3) \\
Rttf2pt1 & 1.3.12 & CRAN (R 4.2.2) \\
rvest & 1.0.3 & CRAN (R 4.2.3) \\
sass & 0.4.8 & CRAN (R 4.2.3) \\
scales & 1.3.0 & CRAN (R 4.2.3) \\
sessioninfo & 1.2.2 & CRAN (R 4.2.3) \\
stringi & 1.7.12 & CRAN (R 4.2.2) \\
stringr & 1.5.1 & CRAN (R 4.2.3) \\
svglite & 2.1.1 & CRAN (R 4.2.3) \\
systemfonts & 1.0.5 & CRAN (R 4.2.3) \\
tensorA & 0.36.2 & CRAN (R 4.2.0) \\
testthat & 3.1.10 & CRAN (R 4.2.3) \\
tibble & 3.2.1 & CRAN (R 4.2.3) \\
tidyr & 1.3.0 & CRAN (R 4.2.3) \\
tidyselect & 1.2.0 & CRAN (R 4.2.3) \\
tidyverse & 2.0.0 & CRAN (R 4.2.3) \\
tzdb & 0.4.0 & CRAN (R 4.2.3) \\
usethis & 2.2.2 & CRAN (R 4.2.3) \\
utf8 & 1.2.3 & CRAN (R 4.2.3) \\
vctrs & 0.6.3 & CRAN (R 4.2.3) \\
viridisLite & 0.4.2 & CRAN (R 4.2.3) \\
webshot & 0.5.5 & CRAN (R 4.2.3) \\
withr & 2.5.2 & CRAN (R 4.2.3) \\
xfun & 0.39 & CRAN (R 4.2.3) \\
xml2 & 1.3.6 & CRAN (R 4.2.3) \\
xtable & 1.8-4 & CRAN (R 4.2.3) \\
yaml & 2.3.7 & CRAN (R 4.2.3) \\
zip & 2.3.0 & CRAN (R 4.2.3) \\
\end{longtable}

\begin{verbatim}
#> - Session info ---------------------------------------------------------------
#>  setting  value
#>  version  R version 4.2.3 (2023-03-15 ucrt)
#>  os       Windows 10 x64 (build 22631)
#>  system   x86_64, mingw32
#>  ui       RTerm
#>  language (EN)
#>  collate  English_United States.utf8
#>  ctype    English_United States.utf8
#>  tz       America/Los_Angeles
#>  date     2024-02-08
#>  pandoc   3.1.1 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)
#> 
#> - Packages -------------------------------------------------------------------
#>  package     * version date (UTC) lib source
#>  bookdown      0.35    2023-08-09 [1] CRAN (R 4.2.3)
#>  cachem        1.0.8   2023-05-01 [1] CRAN (R 4.2.3)
#>  cli           3.6.1   2023-03-23 [1] CRAN (R 4.2.3)
#>  codetools     0.2-19  2023-02-01 [1] CRAN (R 4.2.3)
#>  colorspace    2.1-0   2023-01-23 [1] CRAN (R 4.2.3)
#>  desc          1.4.3   2023-12-10 [1] CRAN (R 4.2.3)
#>  devtools      2.4.5   2022-10-11 [1] CRAN (R 4.2.3)
#>  digest        0.6.31  2022-12-11 [1] CRAN (R 4.2.3)
#>  dplyr       * 1.1.2   2023-04-20 [1] CRAN (R 4.2.3)
#>  ellipsis      0.3.2   2021-04-29 [1] CRAN (R 4.2.3)
#>  evaluate      0.23    2023-11-01 [1] CRAN (R 4.2.3)
#>  fansi         1.0.4   2023-01-22 [1] CRAN (R 4.2.3)
#>  fastmap       1.1.1   2023-02-24 [1] CRAN (R 4.2.3)
#>  forcats     * 1.0.0   2023-01-29 [1] CRAN (R 4.2.3)
#>  fs            1.6.3   2023-07-20 [1] CRAN (R 4.2.3)
#>  generics      0.1.3   2022-07-05 [1] CRAN (R 4.2.3)
#>  ggplot2     * 3.4.4   2023-10-12 [1] CRAN (R 4.2.3)
#>  glue          1.6.2   2022-02-24 [1] CRAN (R 4.2.3)
#>  gtable        0.3.4   2023-08-21 [1] CRAN (R 4.2.3)
#>  hms           1.1.3   2023-03-21 [1] CRAN (R 4.2.3)
#>  htmltools     0.5.7   2023-11-03 [1] CRAN (R 4.2.3)
#>  htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.2.3)
#>  httpuv        1.6.11  2023-05-11 [1] CRAN (R 4.2.3)
#>  jpeg        * 0.1-10  2022-11-29 [1] CRAN (R 4.2.2)
#>  knitr         1.45    2023-10-30 [1] CRAN (R 4.2.3)
#>  later         1.3.1   2023-05-02 [1] CRAN (R 4.2.3)
#>  lifecycle     1.0.4   2023-11-07 [1] CRAN (R 4.2.3)
#>  lubridate   * 1.9.2   2023-02-10 [1] CRAN (R 4.2.3)
#>  magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.2.3)
#>  memoise       2.0.1   2021-11-26 [1] CRAN (R 4.2.3)
#>  mime          0.12    2021-09-28 [1] CRAN (R 4.2.0)
#>  miniUI        0.1.1.1 2018-05-18 [1] CRAN (R 4.2.3)
#>  munsell       0.5.0   2018-06-12 [1] CRAN (R 4.2.3)
#>  pillar        1.9.0   2023-03-22 [1] CRAN (R 4.2.3)
#>  pkgbuild      1.4.3   2023-12-10 [1] CRAN (R 4.2.3)
#>  pkgconfig     2.0.3   2019-09-22 [1] CRAN (R 4.2.3)
#>  pkgload       1.3.3   2023-09-22 [1] CRAN (R 4.2.3)
#>  profvis       0.3.8   2023-05-02 [1] CRAN (R 4.2.3)
#>  promises      1.2.1   2023-08-10 [1] CRAN (R 4.2.3)
#>  purrr       * 1.0.2   2023-08-10 [1] CRAN (R 4.2.3)
#>  R6            2.5.1   2021-08-19 [1] CRAN (R 4.2.3)
#>  Rcpp          1.0.11  2023-07-06 [1] CRAN (R 4.2.3)
#>  readr       * 2.1.4   2023-02-10 [1] CRAN (R 4.2.3)
#>  remotes       2.4.2.1 2023-07-18 [1] CRAN (R 4.2.3)
#>  rlang         1.1.1   2023-04-28 [1] CRAN (R 4.2.3)
#>  rmarkdown     2.25    2023-09-18 [1] CRAN (R 4.2.3)
#>  rstudioapi    0.15.0  2023-07-07 [1] CRAN (R 4.2.3)
#>  scales      * 1.3.0   2023-11-28 [1] CRAN (R 4.2.3)
#>  sessioninfo   1.2.2   2021-12-06 [1] CRAN (R 4.2.3)
#>  shiny         1.7.5   2023-08-12 [1] CRAN (R 4.2.3)
#>  stringi       1.7.12  2023-01-11 [1] CRAN (R 4.2.2)
#>  stringr     * 1.5.1   2023-11-14 [1] CRAN (R 4.2.3)
#>  tibble      * 3.2.1   2023-03-20 [1] CRAN (R 4.2.3)
#>  tidyr       * 1.3.0   2023-01-24 [1] CRAN (R 4.2.3)
#>  tidyselect    1.2.0   2022-10-10 [1] CRAN (R 4.2.3)
#>  tidyverse   * 2.0.0   2023-02-22 [1] CRAN (R 4.2.3)
#>  timechange    0.2.0   2023-01-11 [1] CRAN (R 4.2.3)
#>  tzdb          0.4.0   2023-05-12 [1] CRAN (R 4.2.3)
#>  urlchecker    1.0.1   2021-11-30 [1] CRAN (R 4.2.3)
#>  usethis       2.2.2   2023-07-06 [1] CRAN (R 4.2.3)
#>  utf8          1.2.3   2023-01-31 [1] CRAN (R 4.2.3)
#>  vctrs         0.6.3   2023-06-14 [1] CRAN (R 4.2.3)
#>  withr         2.5.2   2023-10-30 [1] CRAN (R 4.2.3)
#>  xfun          0.39    2023-04-20 [1] CRAN (R 4.2.3)
#>  xtable        1.8-4   2019-04-21 [1] CRAN (R 4.2.3)
#>  yaml          2.3.7   2023-01-23 [1] CRAN (R 4.2.3)
#> 
#>  [1] C:/Program Files/R/R-4.2.3/library
#> 
#> ------------------------------------------------------------------------------
\end{verbatim}

\hypertarget{prerequisites}{%
\chapter{Prerequisites}\label{prerequisites}}

This chapter serves as a concise review of fundamental concepts in \protect\hyperlink{matrix-theory}{Matrix Theory} and \protect\hyperlink{probability-theory}{Probability Theory}.

If you are confident in your understanding of these topics, you can proceed directly to the \protect\hyperlink{descriptive-statistics}{Descriptive Statistics} section to begin exploring applied data analysis.

\includegraphics[width=0.9\textwidth,height=\textheight]{images/Famous-internet-meme-displaying-the-relation-between-statistics-Machine-Learning-and.png}

\hypertarget{matrix-theory}{%
\section{Matrix Theory}\label{matrix-theory}}

Matrix \(A\) represents the original matrix. It's a 2x2 matrix with elements \(a_{ij}\), where \(i\) represents the row and \(j\) represents the column.

\[
A =
\begin{bmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22}
\end{bmatrix}
\] \(A'\) is the transpose of \(A\). The transpose of a matrix flips its rows and columns.

\[
A' =
\begin{bmatrix}
a_{11} & a_{21} \\
a_{12} & a_{22}
\end{bmatrix}
\]

Fundamental properties and rules of matrices, essential for understanding operations in linear algebra:

\[
\begin{aligned}
\mathbf{(ABC)'}   & = \mathbf{C'B'A'} \quad &\text{(Transpose reverses order in a product)} \\
\mathbf{A(B+C)}   & = \mathbf{AB + AC} \quad &\text{(Distributive property)} \\
\mathbf{AB}       & \neq \mathbf{BA} \quad &\text{(Multiplication is not commutative)} \\
\mathbf{(A')'}    & = \mathbf{A} \quad &\text{(Double transpose is the original matrix)} \\
\mathbf{(A+B)'}   & = \mathbf{A' + B'} \quad &\text{(Transpose of a sum is the sum of transposes)} \\
\mathbf{(AB)'}    & = \mathbf{B'A'} \quad &\text{(Transpose reverses order in a product)} \\
\mathbf{(AB)^{-1}} & = \mathbf{B^{-1}A^{-1}} \quad &\text{(Inverse reverses order in a product)} \\
\mathbf{A+B}      & = \mathbf{B + A} \quad &\text{(Addition is commutative)} \\
\mathbf{AA^{-1}}  & = \mathbf{I} \quad &\text{(Matrix times its inverse is identity)} 
\end{aligned}
\] These properties are critical in solving systems of equations, optimizing models, and performing data transformations.

If a matrix \(\mathbf{A}\) has an inverse, it is called \textbf{invertible}. If \(\mathbf{A}\) does not have an inverse, it is referred to as \textbf{singular}.

The product of two matrices \(\mathbf{A}\) and \(\mathbf{B}\) is computed as:

\[
\begin{aligned}
\mathbf{A} &= 
\begin{bmatrix}
a_{11} & a_{12} & a_{13} \\ 
a_{21} & a_{22} & a_{23} 
\end{bmatrix}
\begin{bmatrix}
b_{11} & b_{12} & b_{13} \\
b_{21} & b_{22} & b_{23} \\
b_{31} & b_{32} & b_{33}
\end{bmatrix} \\
&= 
\begin{bmatrix}
a_{11}b_{11}+a_{12}b_{21}+a_{13}b_{31} & \sum_{i=1}^{3}a_{1i}b_{i2} & \sum_{i=1}^{3}a_{1i}b_{i3} \\
\sum_{i=1}^{3}a_{2i}b_{i1} & \sum_{i=1}^{3}a_{2i}b_{i2} & \sum_{i=1}^{3}a_{2i}b_{i3}
\end{bmatrix} 
\end{aligned}
\]

\textbf{Quadratic Form}

Let \(\mathbf{a}\) be a \(3 \times 1\) vector. The quadratic form involving a matrix \(\mathbf{B}\) is given by:

\[
\mathbf{a'Ba} = \sum_{i=1}^{3}\sum_{j=1}^{3}a_i b_{ij} a_{j}
\]

\textbf{Length of a Vector}

The \textbf{length} (or 2-norm) of a vector \(\mathbf{a}\), denoted as \(||\mathbf{a}||\), is defined as the square root of the inner product of the vector with itself:

\[
||\mathbf{a}|| = \sqrt{\mathbf{a'a}}
\]

\hypertarget{rank-of-a-matrix}{%
\subsection{Rank of a Matrix}\label{rank-of-a-matrix}}

The \textbf{rank} of a matrix refers to:

\begin{itemize}
\tightlist
\item
  The dimension of the space spanned by its columns (or rows).
\item
  The number of linearly independent columns or rows.
\end{itemize}

For an \(n \times k\) matrix \(\mathbf{A}\) and a \(k \times k\) matrix \(\mathbf{B}\), the following properties hold:

\begin{itemize}
\tightlist
\item
  \(\text{rank}(\mathbf{A}) \leq \min(n, k)\)
\item
  \(\text{rank}(\mathbf{A}) = \text{rank}(\mathbf{A'}) = \text{rank}(\mathbf{A'A}) = \text{rank}(\mathbf{AA'})\)
\item
  \(\text{rank}(\mathbf{AB}) = \min(\text{rank}(\mathbf{A}), \text{rank}(\mathbf{B}))\)
\item
  \(\mathbf{B}\) is invertible (non-singular) if and only if \(\text{rank}(\mathbf{B}) = k\).
\end{itemize}

\hypertarget{inverse-of-a-matrix}{%
\subsection{Inverse of a Matrix}\label{inverse-of-a-matrix}}

In scalar algebra, if \(a = 0\), then \(1/a\) does not exist.

In matrix algebra, a matrix is invertible if it is \textbf{non-singular}, meaning it has a non-zero determinant and its inverse exists. A square matrix \(\mathbf{A}\) is invertible if there exists another square matrix \(\mathbf{B}\) such that:

\[
\mathbf{AB} = \mathbf{I} \quad \text{(Identity Matrix)}.
\]

In this case, \(\mathbf{A}^{-1} = \mathbf{B}\).

For a \(2 \times 2\) matrix:

\[
\mathbf{A} =
\begin{bmatrix}
a & b \\
c & d
\end{bmatrix}
\]

The inverse is:

\[
\mathbf{A}^{-1} =
\frac{1}{ad-bc}
\begin{bmatrix}
d & -b \\
-c & a
\end{bmatrix}
\]

This inverse exists only if \(ad - bc \neq 0\), where \(ad - bc\) is the determinant of \(\mathbf{A}\).

For a partitioned block matrix:

\[
\begin{bmatrix}
A & B \\
C & D
\end{bmatrix}^{-1}
=
\begin{bmatrix}
\mathbf{(A-BD^{-1}C)^{-1}} & \mathbf{-(A-BD^{-1}C)^{-1}BD^{-1}} \\
\mathbf{-D^{-1}C(A-BD^{-1}C)^{-1}} & \mathbf{D^{-1}+D^{-1}C(A-BD^{-1}C)^{-1}BD^{-1}}
\end{bmatrix}
\]

This formula assumes that \(\mathbf{D}\) and \(\mathbf{A - BD^{-1}C}\) are invertible.

Properties of the Inverse for Non-Singular Matrices

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(\mathbf{(A^{-1})^{-1}} = \mathbf{A}\)\\
\item
  For a non-zero scalar \(b\), \(\mathbf{(bA)^{-1} = b^{-1}A^{-1}}\)\\
\item
  For a matrix \(\mathbf{B}\), \(\mathbf{(BA)^{-1} = B^{-1}A^{-1}}\) (only if \(\mathbf{B}\) is non-singular).\\
\item
  \(\mathbf{(A^{-1})' = (A')^{-1}}\) (the transpose of the inverse equals the inverse of the transpose).\\
\item
  Never notate \(\mathbf{1/A}\); use \(\mathbf{A^{-1}}\) instead.
\end{enumerate}

\textbf{Notes}: - The determinant of a matrix determines whether it is invertible. For square matrices, a determinant of \(0\) means the matrix is \textbf{singular} and has no inverse.\\
- Always verify the conditions for invertibility, particularly when dealing with partitioned or block matrices.

\hypertarget{definiteness-of-a-matrix}{%
\subsection{Definiteness of a Matrix}\label{definiteness-of-a-matrix}}

A symmetric square \(k \times k\) matrix \(\mathbf{A}\) is classified based on the following conditions:

\begin{itemize}
\item
  \textbf{Positive Semi-Definite (PSD)}: \(\mathbf{A}\) is PSD if, for any non-zero \(k \times 1\) vector \(\mathbf{x}\): \[
  \mathbf{x'Ax \geq 0}.
  \]
\item
  \textbf{Negative Semi-Definite (NSD)}: \(\mathbf{A}\) is NSD if, for any non-zero \(k \times 1\) vector \(\mathbf{x}\): \[
  \mathbf{x'Ax \leq 0}.
  \]
\item
  \textbf{Indefinite}: \(\mathbf{A}\) is indefinite if it is neither PSD nor NSD.
\end{itemize}

The \textbf{identity matrix} is always positive definite (PD).

Example

Let \(\mathbf{x} = (x_1, x_2)'\), and consider a \(2 \times 2\) identity matrix \(\mathbf{I}\):

\[
\begin{aligned}
\mathbf{x'Ix} 
&= (x_1, x_2) 
\begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2
\end{bmatrix} \\
&=
(x_1, x_2)
\begin{bmatrix}
x_1 \\
x_2
\end{bmatrix} \\
&=
x_1^2 + x_2^2 \geq 0.
\end{aligned}
\]

Thus, \(\mathbf{I}\) is PD because \(\mathbf{x'Ix} > 0\) for all non-zero \(\mathbf{x}\).

\textbf{Properties of Definiteness}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Any variance-covariance matrix is PSD.
\item
  A matrix \(\mathbf{A}\) is PSD if and only if there exists a matrix \(\mathbf{B}\) such that: \[
  \mathbf{A = B'B}.
  \]
\item
  If \(\mathbf{A}\) is PSD, then \(\mathbf{B'AB}\) is also PSD for any conformable matrix \(\mathbf{B}\).
\item
  If \(\mathbf{A}\) and \(\mathbf{C}\) are non-singular, then \(\mathbf{A - C}\) is PSD if and only if \(\mathbf{C^{-1} - A^{-1}}\) is PSD.
\item
  If \(\mathbf{A}\) is PD (or ND), then \(\mathbf{A^{-1}}\) is also PD (or ND).
\end{enumerate}

\textbf{Notes}

\begin{itemize}
\tightlist
\item
  An \textbf{indefinite} matrix \(\mathbf{A}\) is neither PSD nor NSD. This concept does not have a direct counterpart in scalar algebra.
\item
  If a square matrix is PSD and invertible, then it is PD.
\end{itemize}

Examples of Definiteness

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Invertible / Indefinite}: \[
  \begin{bmatrix}
  -1 & 0 \\
  0 & 10
  \end{bmatrix}
  \]
\item
  \textbf{Non-Invertible / Indefinite}: \[
  \begin{bmatrix}
  0 & 1 \\
  0 & 0
  \end{bmatrix}
  \]
\item
  \textbf{Invertible / PSD}: \[
  \begin{bmatrix}
  1 & 0 \\
  0 & 1
  \end{bmatrix}
  \]
\item
  \textbf{Non-Invertible / PSD}: \[
  \begin{bmatrix}
  0 & 0 \\
  0 & 1
  \end{bmatrix}
  \]
\end{enumerate}

\hypertarget{matrix-calculus}{%
\subsection{Matrix Calculus}\label{matrix-calculus}}

Consider a scalar function \(y = f(x_1, x_2, \dots, x_k) = f(x)\), where \(x\) is a \(1 \times k\) row vector.

\hypertarget{gradient-first-order-derivative}{%
\subsubsection{Gradient (First-Order Derivative)}\label{gradient-first-order-derivative}}

The gradient, or the first-order derivative of \(f(x)\) with respect to the vector \(x\), is given by:

\[
\frac{\partial f(x)}{\partial x} =
\begin{bmatrix}
\frac{\partial f(x)}{\partial x_1} \\
\frac{\partial f(x)}{\partial x_2} \\
\vdots \\
\frac{\partial f(x)}{\partial x_k}
\end{bmatrix}
\]

\hypertarget{hessian-second-order-derivative}{%
\subsubsection{Hessian (Second-Order Derivative)}\label{hessian-second-order-derivative}}

The Hessian, or the second-order derivative of \(f(x)\) with respect to \(x\), is a symmetric matrix defined as:

\[
\frac{\partial^2 f(x)}{\partial x \partial x'} =
\begin{bmatrix}
\frac{\partial^2 f(x)}{\partial x_1^2} & \frac{\partial^2 f(x)}{\partial x_1 \partial x_2} & \cdots & \frac{\partial^2 f(x)}{\partial x_1 \partial x_k} \\
\frac{\partial^2 f(x)}{\partial x_2 \partial x_1} & \frac{\partial^2 f(x)}{\partial x_2^2} & \cdots & \frac{\partial^2 f(x)}{\partial x_2 \partial x_k} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2 f(x)}{\partial x_k \partial x_1} & \frac{\partial^2 f(x)}{\partial x_k \partial x_2} & \cdots & \frac{\partial^2 f(x)}{\partial x_k^2}
\end{bmatrix}
\]

\hypertarget{derivative-of-a-scalar-function-with-respect-to-a-matrix}{%
\subsubsection{Derivative of a Scalar Function with Respect to a Matrix}\label{derivative-of-a-scalar-function-with-respect-to-a-matrix}}

Let \(f(\mathbf{X})\) be a scalar function, where \(\mathbf{X}\) is an \(n \times p\) matrix. The derivative is:

\[
\frac{\partial f(\mathbf{X})}{\partial \mathbf{X}} = 
\begin{bmatrix}
\frac{\partial f(\mathbf{X})}{\partial x_{11}} & \cdots & \frac{\partial f(\mathbf{X})}{\partial x_{1p}} \\
\vdots & \ddots & \vdots \\
\frac{\partial f(\mathbf{X})}{\partial x_{n1}} & \cdots & \frac{\partial f(\mathbf{X})}{\partial x_{np}}
\end{bmatrix}
\]

\hypertarget{common-matrix-derivatives}{%
\subsubsection{Common Matrix Derivatives}\label{common-matrix-derivatives}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  If \(\mathbf{a}\) is a vector and \(\mathbf{A}\) is a matrix independent of \(\mathbf{y}\):

  \begin{itemize}
  \tightlist
  \item
    \(\frac{\partial \mathbf{a'y}}{\partial \mathbf{y}} = \mathbf{a}\)
  \item
    \(\frac{\partial \mathbf{y'y}}{\partial \mathbf{y}} = 2\mathbf{y}\)
  \item
    \(\frac{\partial \mathbf{y'Ay}}{\partial \mathbf{y}} = (\mathbf{A} + \mathbf{A'})\mathbf{y}\)
  \end{itemize}
\item
  If \(\mathbf{X}\) is symmetric:

  \begin{itemize}
  \tightlist
  \item
    \(\frac{\partial |\mathbf{X}|}{\partial x_{ij}} = \begin{cases} X_{ii}, & i = j \\ X_{ij}, & i \neq j \end{cases}\) where \(X_{ij}\) is the \((i,j)\)-th cofactor of \(\mathbf{X}\).
  \end{itemize}
\item
  If \(\mathbf{X}\) is symmetric and \(\mathbf{A}\) is a matrix independent of \(\mathbf{X}\):

  \begin{itemize}
  \tightlist
  \item
    \(\frac{\partial \text{tr}(\mathbf{XA})}{\partial \mathbf{X}} = \mathbf{A} + \mathbf{A'} - \text{diag}(\mathbf{A})\).
  \end{itemize}
\item
  If \(\mathbf{X}\) is symmetric, let \(\mathbf{J}_{ij}\) be a matrix with 1 at the \((i,j)\)-th position and 0 elsewhere:

  \begin{itemize}
  \tightlist
  \item
    \(\frac{\partial \mathbf{X}^{-1}}{\partial x_{ij}} = \begin{cases} -\mathbf{X}^{-1}\mathbf{J}_{ii}\mathbf{X}^{-1}, & i = j \\ -\mathbf{X}^{-1}(\mathbf{J}_{ij} + \mathbf{J}_{ji})\mathbf{X}^{-1}, & i \neq j \end{cases}.\)
  \end{itemize}
\end{enumerate}

\hypertarget{optimization-in-scalar-and-vector-spaces}{%
\subsection{Optimization in Scalar and Vector Spaces}\label{optimization-in-scalar-and-vector-spaces}}

Optimization is the process of finding the minimum or maximum of a function. The conditions for optimization differ depending on whether the function involves a scalar or a vector. Below is a comparison of scalar and vector optimization:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2893}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2487}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4569}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Condition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Scalar Optimization}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Vector Optimization}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{First-Order Condition} & \(\frac{\partial f(x_0)}{\partial x} = 0\) & \(\frac{\partial f(x_0)}{\partial x} = \begin{bmatrix} 0 \\ \vdots \\ 0 \end{bmatrix}\) \\
\textbf{Second-Order Condition}

For \textbf{convex} functions, this implies a \textbf{minimum}. & \(\frac{\partial^2 f(x_0)}{\partial x^2} > 0\) & \(\frac{\partial^2 f(x_0)}{\partial x \partial x'} > 0\) \\
For \textbf{concave} functions, this implies a \textbf{maximum}. & \(\frac{\partial^2 f(x_0)}{\partial x^2} < 0\) & \(\frac{\partial^2 f(x_0)}{\partial x \partial x'} < 0\) \\
\end{longtable}

\textbf{Key Concepts}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{First-Order Condition}:

  \begin{itemize}
  \tightlist
  \item
    The \textbf{first-order derivative} of the function must equal zero at a critical point. This holds for both scalar and vector functions:

    \begin{itemize}
    \tightlist
    \item
      In the scalar case, \(\frac{\partial f(x)}{\partial x} = 0\) identifies critical points.
    \item
      In the vector case, \(\frac{\partial f(x)}{\partial x}\) is a \textbf{gradient vector}, and the condition is satisfied when all elements of the gradient are zero.
    \end{itemize}
  \end{itemize}
\item
  \textbf{Second-Order Condition}:

  \begin{itemize}
  \tightlist
  \item
    The \textbf{second-order derivative} determines whether the critical point is a minimum, maximum, or saddle point:

    \begin{itemize}
    \tightlist
    \item
      For scalar functions, \(\frac{\partial^2 f(x)}{\partial x^2} > 0\) implies a \textbf{local minimum}, while \(\frac{\partial^2 f(x)}{\partial x^2} < 0\) implies a \textbf{local maximum}.
    \item
      For vector functions, the Hessian matrix \(\frac{\partial^2 f(x)}{\partial x \partial x'}\) must be:

      \begin{itemize}
      \tightlist
      \item
        \textbf{Positive Definite}: For a \textbf{minimum} (convex function).
      \item
        \textbf{Negative Definite}: For a \textbf{maximum} (concave function).
      \item
        \textbf{Indefinite}: For a \textbf{saddle point} (neither minimum nor maximum).
      \end{itemize}
    \end{itemize}
  \end{itemize}
\item
  \textbf{Convex and Concave Functions}:

  \begin{itemize}
  \tightlist
  \item
    A function \(f(x)\) is:

    \begin{itemize}
    \tightlist
    \item
      \textbf{Convex} if \(\frac{\partial^2 f(x)}{\partial x^2} > 0\) or the Hessian \(\frac{\partial^2 f(x)}{\partial x \partial x'}\) is positive definite.
    \item
      \textbf{Concave} if \(\frac{\partial^2 f(x)}{\partial x^2} < 0\) or the Hessian is negative definite.
    \end{itemize}
  \item
    Convexity ensures global optimization for minimization problems, while concavity ensures global optimization for maximization problems.
  \end{itemize}
\item
  \textbf{Hessian Matrix}:

  \begin{itemize}
  \tightlist
  \item
    In vector optimization, the Hessian \(\frac{\partial^2 f(x)}{\partial x \partial x'}\) plays a crucial role in determining the nature of critical points:

    \begin{itemize}
    \tightlist
    \item
      Positive definite Hessian: All eigenvalues are positive.
    \item
      Negative definite Hessian: All eigenvalues are negative.
    \item
      Indefinite Hessian: Eigenvalues have mixed signs.
    \end{itemize}
  \end{itemize}
\end{enumerate}

\hypertarget{cholesky-decomposition}{%
\subsection{Cholesky Decomposition}\label{cholesky-decomposition}}

In statistical analysis and numerical linear algebra, decomposing matrices into more tractable forms is crucial for efficient computation. One such important factorization is the \textbf{Cholesky Decomposition}. It applies to \textbf{Hermitian} (in the complex case) or \textbf{symmetric} (in the real case), \textbf{positive-definite} matrices.

Given an \(n \times n\) positive-definite matrix \(A\), the Cholesky Decomposition states:

\[
A = L L^{*},
\]

where:

\begin{itemize}
\tightlist
\item
  \(L\) is a lower-triangular matrix with strictly positive diagonal entries.
\item
  \(L^{*}\) denotes the \emph{conjugate transpose} of \(L\) (simply the transpose \(L^{T}\) for real matrices).
\end{itemize}

Cholesky Decomposition is both computationally efficient and numerically stable, making it a go-to technique for many applications---particularly in statistics where we deal extensively with covariance matrices, linear systems, and probability distributions.

Before diving into how we compute a Cholesky Decomposition, we need to clarify what it means for a matrix to be \emph{positive-definite}. For a real symmetric matrix \(A\):

\begin{itemize}
\tightlist
\item
  \(A\) is \emph{positive-definite} if for every nonzero vector \(x\), we have \[
  x^T A \, x > 0.
  \]
\item
  Alternatively, you can characterize positive-definiteness by noting that all eigenvalues of \(A\) are strictly positive.
\end{itemize}

Many important matrices in statistics---particularly \emph{covariance} or \emph{precision} matrices---are both symmetric and positive-definite.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{existence}{%
\subsubsection{Existence}\label{existence}}

A real \(n \times n\) matrix \(A\) that is symmetric and positive-definite always admits a Cholesky Decomposition \(A = L L^T\). This theorem guarantees that for any covariance matrix in statistics---assuming it is valid (i.e., positive-definite)---we can decompose it via Cholesky.

\hypertarget{uniqueness}{%
\subsubsection{Uniqueness}\label{uniqueness}}

If we additionally require that the diagonal entries of \(L\) are \emph{strictly positive}, then \(L\) is \emph{unique}. That is, no other lower-triangular matrix with strictly positive diagonal entries will produce the same factorization. This uniqueness is helpful for ensuring consistent numerical outputs in software implementations.

\hypertarget{constructing-the-cholesky-factor-l}{%
\subsubsection{\texorpdfstring{Constructing the Cholesky Factor \(L\)}{Constructing the Cholesky Factor L}}\label{constructing-the-cholesky-factor-l}}

Given a real, symmetric, positive-definite matrix \(A \in \mathbb{R}^{n \times n}\), we want to find the lower-triangular matrix \(L\) such that \(A = LL^T\). One way to do this is by using a simple step-by-step procedure (often part of standard linear algebra libraries):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Initialize} \(L\) to be an \(n \times n\) zero matrix.
\item
  \textbf{Iterate} through the rows \(i = 1, 2, \dots, n\):

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    For each row \(i\), compute \[
    L_{ii} = \sqrt{A_{ii} - \sum_{k=1}^{i-1} L_{ik}^2}.
    \]
  \item
    For each column \(j = i+1, i+2, \dots, n\): \[
    L_{ji} = \frac{1}{L_{ii}}
              \Bigl(A_{ji} - \sum_{k=1}^{i-1} L_{jk} L_{ik}\Bigr).
    \]
  \item
    All other entries of \(L\) remain zero or are computed in subsequent steps.
  \end{enumerate}
\item
  \textbf{Result}: \(L\) is lower-triangular, and \(L^T\) is its transpose.
\end{enumerate}

Cholesky Decomposition is roughly half the computational cost of a more general LU Decomposition. Specifically, it requires on the order of \(\frac{1}{3} n^3\) floating-point operations (flops), making it significantly more efficient in practice than other decompositions for positive-definite systems.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{illustrative-example}{%
\subsubsection{Illustrative Example}\label{illustrative-example}}

Consider a small \(3 \times 3\) positive-definite matrix:

\[
A = 
\begin{pmatrix}
4 & 2 & 4 \\
2 & 5 & 6 \\
4 & 6 & 20
\end{pmatrix}.
\]

We claim \(A\) is positive-definite (one could check by calculating principal minors or verifying \(x^T A x > 0\) for all \(x \neq 0\)). We find \(L\) step-by-step:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Compute} \(L_{11}\):\\
  \[
  L_{11} = \sqrt{A_{11}} = \sqrt{4} = 2.
  \]
\item
  \textbf{Compute} \(L_{21}\) and \(L_{31}\):

  \begin{itemize}
  \tightlist
  \item
    \(L_{21} = \frac{A_{21}}{L_{11}} = \frac{2}{2} = 1.\)\\
  \item
    \(L_{31} = \frac{A_{31}}{L_{11}} = \frac{4}{2} = 2.\)
  \end{itemize}
\item
  \textbf{Compute} \(L_{22}\):\\
  \[
  L_{22} = \sqrt{A_{22} - L_{21}^2} 
          = \sqrt{5 - 1^2}
          = \sqrt{4} = 2.
  \]
\item
  \textbf{Compute} \(L_{32}\):\\
  \[
  L_{32} = \frac{A_{32} - L_{31} L_{21}}{L_{22}}
          = \frac{6 - (2)(1)}{2}
          = \frac{4}{2} = 2.
  \]
\item
  \textbf{Compute} \(L_{33}\):\\
  \[
  L_{33} = \sqrt{A_{33} - (L_{31}^2 + L_{32}^2)}
          = \sqrt{20 - (2^2 + 2^2)}
          = \sqrt{20 - 8}
          = \sqrt{12}
          = 2\sqrt{3}.
  \]
\end{enumerate}

Thus,

\[
L = 
\begin{pmatrix}
2 & 0 & 0 \\
1 & 2 & 0 \\
2 & 2 & 2\sqrt{3}
\end{pmatrix}.
\]

One can verify \(L L^T = A\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{applications-in-statistics}{%
\subsubsection{Applications in Statistics}\label{applications-in-statistics}}

\hypertarget{solving-linear-systems}{%
\paragraph{Solving Linear Systems}\label{solving-linear-systems}}

A common statistical problem is solving \(A x = b\) for \(x\). For instance, in regression or in computing Bayesian posterior modes, we often need to solve linear equations with covariance or precision matrices. With \(A = LL^T\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Forward Substitution}: Solve \(L \, y = b\).
\item
  \textbf{Backward Substitution}: Solve \(L^T x = y\).
\end{enumerate}

This two-step process is more stable and efficient than directly inverting \(A\) (which is typically discouraged due to numerical issues).

\hypertarget{generating-correlated-random-vectors}{%
\paragraph{Generating Correlated Random Vectors}\label{generating-correlated-random-vectors}}

In simulation-based statistics (e.g., Monte Carlo methods), we often need to generate random draws from a \textbf{multivariate normal} distribution \(\mathcal{N}(\mu, \Sigma)\), where \(\Sigma\) is the covariance matrix. The steps are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Generate a vector \(z \sim \mathcal{N}(0, I)\) of independent standard normal variables.
\item
  Compute \(x = \mu + Lz\), where \(\Sigma = LL^T\).
\end{enumerate}

Then \(x\) has the desired covariance structure \(\Sigma\). This technique is widely used in Bayesian statistics (e.g., MCMC sampling) and financial modeling (e.g., portfolio simulations).

\hypertarget{gaussian-processes-and-kriging}{%
\paragraph{Gaussian Processes and Kriging}\label{gaussian-processes-and-kriging}}

In Gaussian Process modeling (common in spatial statistics, machine learning, and geostatistics), we frequently work with large covariance matrices that describe the correlations between observed data points:

\[
\Sigma = 
\begin{pmatrix}
k(x_1, x_1) & k(x_1, x_2) & \cdots & k(x_1, x_n) \\
k(x_2, x_1) & k(x_2, x_2) & \cdots & k(x_2, x_n) \\
\vdots      & \vdots      & \ddots & \vdots      \\
k(x_n, x_1) & k(x_n, x_2) & \cdots & k(x_n, x_n)
\end{pmatrix},
\]

where \(k(\cdot, \cdot)\) is a covariance (kernel) function. We may need to invert or factorize \(\Sigma\) repeatedly to evaluate the log-likelihood:

\[
\log \mathcal{L}(\theta) \sim 
- \tfrac{1}{2} \bigl( y - m(\theta) \bigr)^T \Sigma^{-1} \bigl( y - m(\theta) \bigr) 
- \tfrac{1}{2} \log \bigl| \Sigma \bigr|,
\]

where \(m(\theta)\) is the mean function and \(\theta\) are parameters. Using the Cholesky factor \(L\) of \(\Sigma\) helps:

\begin{itemize}
\tightlist
\item
  \(\Sigma^{-1}\) can be implied by solving systems with \(L\) instead of explicitly computing the inverse.
\item
  \(\log|\Sigma|\) can be computed as \(2 \sum_{i=1}^n \log L_{ii}\).
\end{itemize}

Hence, Cholesky Decomposition becomes the backbone of Gaussian Process computations.

\hypertarget{bayesian-inference-with-covariance-matrices}{%
\paragraph{Bayesian Inference with Covariance Matrices}\label{bayesian-inference-with-covariance-matrices}}

Many Bayesian models---especially hierarchical models---assume a multivariate normal prior on parameters. Cholesky Decomposition is used to:

\begin{itemize}
\tightlist
\item
  Sample from these priors or from posterior distributions.
\item
  Regularize large covariance matrices.
\item
  Speed up Markov Chain Monte Carlo (MCMC) computations by factorizing covariance structures.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{other-notes}{%
\subsubsection{Other Notes}\label{other-notes}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Numerical Stability Considerations
\end{enumerate}

Cholesky Decomposition is considered more stable than a general LU Decomposition when applied to positive-definite matrices. Since no row or column pivots are required, rounding errors can be smaller. Of course, in practice, software implementations can vary, and extremely ill-conditioned matrices can still pose numerical challenges.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Why We \emph{Don't} Usually Compute \(\mathbf{A}^{-1}\)
\end{enumerate}

It is common in statistics (especially in older texts) to see formulas involving \(\Sigma^{-1}\). However, computing an inverse explicitly is often discouraged because:

\begin{itemize}
\tightlist
\item
  It is numerically less stable.
\item
  It requires more computations.
\item
  Many tasks that \emph{appear} to need \(\Sigma^{-1}\) can be done more efficiently by solving systems via the Cholesky factor \(L\).
\end{itemize}

Hence, ``\textbf{solve, don't invert}'' is a common mantra. If you see an expression like \(\Sigma^{-1} b\), you can use the Cholesky factors \(L\) and \(L^T\) to solve \(\Sigma x = b\) by forward and backward substitution, bypassing the direct inverse calculation.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Further Variants and Extensions
\end{enumerate}

\begin{itemize}
\tightlist
\item
  \textbf{Incomplete Cholesky}: Sometimes used in iterative solvers where a full Cholesky factorization is too expensive, especially for large sparse systems.
\item
  \textbf{LDL\^{}T Decomposition}: A variant that avoids taking square roots; used for positive \emph{semi}-definite or indefinite systems, but with caution about pivoting strategies.
\end{itemize}

\hypertarget{probability-theory}{%
\section{Probability Theory}\label{probability-theory}}

\includegraphics[width=0.8\textwidth,height=\textheight]{images/meme-stat.jpg}

\hypertarget{axioms-and-theorems-of-probability}{%
\subsection{Axioms and Theorems of Probability}\label{axioms-and-theorems-of-probability}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Let \(S\) denote the sample space of an experiment. Then: \[
  P[S] = 1
  \] (The probability of the sample space is always 1.)
\item
  For any event \(A\): \[
  P[A] \geq 0
  \] (Probabilities are always non-negative.)
\item
  Let \(A_1, A_2, A_3, \dots\) be a finite or infinite collection of mutually exclusive events. Then: \[
  P[A_1 \cup A_2 \cup A_3 \dots] = P[A_1] + P[A_2] + P[A_3] + \dots
  \] (The probability of the union of mutually exclusive events is the sum of their probabilities.)
\item
  The probability of the empty set is: \[
  P[\emptyset] = 0
  \]
\item
  The complement rule: \[
  P[A'] = 1 - P[A]
  \]
\item
  The probability of the union of two events: \[
  P[A_1 \cup A_2] = P[A_1] + P[A_2] - P[A_1 \cap A_2]
  \]
\end{enumerate}

\hypertarget{conditional-probability}{%
\subsubsection{Conditional Probability}\label{conditional-probability}}

The conditional probability of \(A\) given \(B\) is defined as:

\[
P[A|B] = \frac{P[A \cap B]}{P[B]}, \quad \text{provided } P[B] \neq 0.
\]

\hypertarget{independent-events}{%
\subsubsection{Independent Events}\label{independent-events}}

Two events \(A\) and \(B\) are independent if and only if:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(P[A \cap B] = P[A]P[B]\)
\item
  \(P[A|B] = P[A]\)
\item
  \(P[B|A] = P[B]\)
\end{enumerate}

A collection of events \(A_1, A_2, \dots, A_n\) is independent if and only if every subcollection is independent.

\hypertarget{multiplication-rule}{%
\subsubsection{Multiplication Rule}\label{multiplication-rule}}

The probability of the intersection of two events can be calculated as: \[
P[A \cap B] = P[A|B]P[B] = P[B|A]P[A].
\]

\hypertarget{bayes-theorem}{%
\subsubsection{Bayes' Theorem}\label{bayes-theorem}}

Let \(A_1, A_2, \dots, A_n\) be a collection of mutually exclusive events whose union is \(S\), and let \(B\) be an event with \(P[B] \neq 0\). Then, for any event \(A_j\) (\(j = 1, 2, \dots, n\)): \[
P[A_j|B] = \frac{P[B|A_j]P[A_j]}{\sum_{i=1}^n P[B|A_i]P[A_i]}.
\]

\hypertarget{jensens-inequality}{%
\subsubsection{Jensen's Inequality}\label{jensens-inequality}}

\begin{itemize}
\item
  If \(g(x)\) is convex, then: \[
  E[g(X)] \geq g(E[X])
  \]
\item
  If \(g(x)\) is concave, then: \[
  E[g(X)] \leq g(E[X]).
  \]
\end{itemize}

Jensen's inequality provides a useful way to demonstrate why the standard error calculated using the sample standard deviation (\(s\)) as a proxy for the population standard deviation (\(\sigma\)) is a biased estimator.

\begin{itemize}
\item
  The population standard deviation \(\sigma\) is defined as: \[
  \sigma = \sqrt{\mathbb{E}[(X - \mu)^2]},
  \] where \(\mu = \mathbb{E}[X]\) is the population mean.
\item
  The sample standard deviation \(s\) is given by: \[
  s = \sqrt{\frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X})^2},
  \] where \(\bar{X}\) is the sample mean.
\item
  When \(s\) is used as an estimator for \(\sigma\), the expectation involves the square root function, which is concave.
\end{itemize}

\textbf{Applying Jensen's Inequality}

The standard error formula involves the square root: \[
\sqrt{\mathbb{E}[s^2]}.
\]

However, because the square root function is concave, Jensen's inequality implies: \[
\sqrt{\mathbb{E}[s^2]} \leq \mathbb{E}[\sqrt{s^2}] = \mathbb{E}[s].
\]

This inequality shows that the expected value of \(s\) (the sample standard deviation) systematically underestimates the population standard deviation \(\sigma\).

\textbf{Quantifying the Bias}

The bias arises because: \[
\mathbb{E}[s] \neq \sigma.
\]

To correct this bias, we note that the sample standard deviation is related to the population standard deviation by: \[
\mathbb{E}[s] = \sigma \cdot \sqrt{\frac{n-1}{n}},
\] where \(n\) is the sample size. This bias decreases as \(n\) increases, and the estimator becomes asymptotically unbiased.

By leveraging Jensen's inequality, we observe that the concavity of the square root function ensures that \(s\) is a biased estimator of \(\sigma\), systematically underestimating the population standard deviation.

\hypertarget{law-of-iterated-expectation}{%
\subsubsection{Law of Iterated Expectation}\label{law-of-iterated-expectation}}

The \textbf{Law of Iterated Expectation} states that for random variables \(X\) and \(Y\):

\[
E(X) = E(E(X|Y)).
\]

This means the expected value of \(X\) can be obtained by first calculating the conditional expectation \(E(X|Y)\) and then taking the expectation of this quantity over the distribution of \(Y\).

\hypertarget{correlation-and-independence}{%
\subsubsection{Correlation and Independence}\label{correlation-and-independence}}

The strength of the relationship between random variables can be ranked from strongest to weakest as:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Independence}:

  \begin{itemize}
  \tightlist
  \item
    \(f(x, y) = f_X(x)f_Y(y)\)
  \item
    \(f_{Y|X}(y|x) = f_Y(y)\) and \(f_{X|Y}(x|y) = f_X(x)\)
  \item
    \(E[g_1(X)g_2(Y)] = E[g_1(X)]E[g_2(Y)]\)
  \end{itemize}
\item
  \textbf{Mean Independence} (implied by independence):

  \begin{itemize}
  \tightlist
  \item
    \(Y\) is mean independent of \(X\) if: \[
    E[Y|X] = E[Y].
    \]
  \item
    \(E[Xg(Y)] = E[X]E[g(Y)]\)
  \end{itemize}
\item
  \textbf{Uncorrelatedness} (implied by independence and mean independence):

  \begin{itemize}
  \tightlist
  \item
    \(\text{Cov}(X, Y) = 0\)
  \item
    \(\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y)\)
  \item
    \(E[XY] = E[X]E[Y]\)
  \end{itemize}
\end{enumerate}

\hypertarget{central-limit-theorem}{%
\subsection{Central Limit Theorem}\label{central-limit-theorem}}

The \textbf{Central Limit Theorem} states that for a sufficiently large sample size (\(n \geq 25\)), the sampling distribution of the sample mean or proportion approaches a normal distribution, regardless of the population's original distribution.

Let \(X_1, X_2, \dots, X_n\) be a random sample of size \(n\) from a distribution \(X\) with mean \(\mu\) and variance \(\sigma^2\). Then, for large \(n\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The sample mean \(\bar{X}\) is approximately normal: \[
  \mu_{\bar{X}} = \mu, \quad \sigma^2_{\bar{X}} = \frac{\sigma^2}{n}.
  \]
\item
  The sample proportion \(\hat{p}\) is approximately normal: \[
  \mu_{\hat{p}} = p, \quad \sigma^2_{\hat{p}} = \frac{p(1-p)}{n}.
  \]
\item
  The difference in sample proportions \(\hat{p}_1 - \hat{p}_2\) is approximately normal: \[
  \mu_{\hat{p}_1 - \hat{p}_2} = p_1 - p_2, \quad \sigma^2_{\hat{p}_1 - \hat{p}_2} = \frac{p_1(1-p_1)}{n_1} + \frac{p_2(1-p_2)}{n_2}.
  \]
\item
  The difference in sample means \(\bar{X}_1 - \bar{X}_2\) is approximately normal: \[
  \mu_{\bar{X}_1 - \bar{X}_2} = \mu_1 - \mu_2, \quad \sigma^2_{\bar{X}_1 - \bar{X}_2} = \frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}.
  \]
\item
  The following random variables are approximately standard normal:

  \begin{itemize}
  \tightlist
  \item
    \(\frac{\bar{X} - \mu}{\sigma / \sqrt{n}}\)
  \item
    \(\frac{\hat{p} - p}{\sqrt{\frac{p(1-p)}{n}}}\)
  \item
    \(\frac{(\hat{p}_1 - \hat{p}_2) - (p_1 - p_2)}{\sqrt{\frac{p_1(1-p_1)}{n_1} + \frac{p_2(1-p_2)}{n_2}}}\)
  \item
    \(\frac{(\bar{X}_1 - \bar{X}_2) - (\mu_1 - \mu_2)}{\sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}}\)
  \end{itemize}
\end{enumerate}

\hypertarget{limiting-distribution-of-the-sample-mean}{%
\subsubsection{Limiting Distribution of the Sample Mean}\label{limiting-distribution-of-the-sample-mean}}

If \(\{X_i\}_{i=1}^{n}\) is an iid random sample from a distribution with finite mean \(\mu\) and finite variance \(\sigma^2\), the sample mean \(\bar{X}\) scaled by \(\sqrt{n}\) has the following limiting distribution:

\[
\sqrt{n}(\bar{X} - \mu) \xrightarrow{d} N(0, \sigma^2).
\]

Standardizing the sample mean gives: \[
\frac{\sqrt{n}(\bar{X} - \mu)}{\sigma} \xrightarrow{d} N(0, 1).
\]

\textbf{Notes}:

\begin{itemize}
\tightlist
\item
  The CLT holds for most random samples from any distribution (continuous, discrete, or unknown).
\item
  It extends to the multivariate case: A random sample of a random vector converges to a multivariate normal distribution.
\end{itemize}

\hypertarget{asymptotic-variance-and-limiting-variance}{%
\subsubsection{Asymptotic Variance and Limiting Variance}\label{asymptotic-variance-and-limiting-variance}}

\textbf{Asymptotic Variance} (Avar): \[
Avar(\sqrt{n}(\bar{X} - \mu)) = \sigma^2.
\]

\begin{itemize}
\item
  Refers to the variance of the limiting distribution of an estimator as the sample size (\(n\)) approaches infinity.
\item
  It characterizes the variability of the scaled estimator \(\sqrt{n}(\bar{x} - \mu)\) in its asymptotic distribution (e.g., normal distribution).
\end{itemize}

\textbf{Limiting Variance} (\(\lim_{n \to \infty} Var\))

\[
\lim_{n \to \infty} Var(\sqrt{n}(\bar{x}-\mu)) = \sigma^2
\]

\begin{itemize}
\tightlist
\item
  Represents the value that the actual variance of \(\sqrt{n}(\bar{x} - \mu)\) converges to as \(n \to \infty\).
\end{itemize}

For a well-behaved estimator,

\[
Avar(\sqrt{n}(\bar{X} - \mu)) = \lim_{n \to \infty} Var(\sqrt{n}(\bar{x}-\mu)) = \sigma^2.
\]

However, \textbf{asymptotic variance is not necessarily equal to the limiting value of the variance} because asymptotic variance is derived from the limiting distribution, while limiting variance is a convergence result of the sequence of variances.

\[
Avar(.) \neq lim_{n \to \infty} Var(.)
\]

\begin{itemize}
\item
  Both the asymptotic variance \(Avar\) and the limiting variance \(\lim_{n \to \infty} Var\) are numerically equal to \(\sigma^2\), but their conceptual definitions differ.
\item
  \(Avar(\cdot) \neq \lim_{n \to \infty} Var(\cdot)\). This emphasizes that while the numerical result may match, their derivation and meaning differ:

  \begin{itemize}
  \item
    \(Avar\) depends on the asymptotic (large-sample) distribution of the estimator.
  \item
    \(\lim_{n \to \infty} Var(\cdot)\) involves the sequence of variances as \(n\) grows.
  \end{itemize}
\end{itemize}

Cases where the two do not match:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Sample Quantiles}: Consider the sample quantile of order \(p\), for some \(0 < p < 1\). Under regularity conditions, the asymptotic distribution of the sample quantile is normal, with a variance that depends on \(p\) and the density of the distribution at the \(p\)-th quantile. However, the variance of the sample quantile itself does not necessarily converge to this limit as the sample size grows.
\item
  \textbf{Bootstrap Methods}: When using bootstrapping techniques to estimate the distribution of a statistic, the bootstrap distribution might converge to a different limiting distribution than the original statistic. In these cases, the variance of the bootstrap distribution (or the bootstrap variance) might differ from the limiting variance of the original statistic.
\item
  \textbf{Statistics with Randomly Varying Asymptotic Behavior}: In some cases, the asymptotic behavior of a statistic can vary randomly depending on the sample path. For such statistics, the asymptotic variance might not provide a consistent estimate of the limiting variance.
\item
  \textbf{M-estimators with Varying Asymptotic Behavior}: M-estimators can sometimes have different asymptotic behaviors depending on the tail behavior of the underlying distribution. For heavy-tailed distributions, the variance of the estimator might not stabilize even as the sample size grows large, making the asymptotic variance different from the variance of any limiting distribution.
\end{enumerate}

\hypertarget{random-variable}{%
\subsection{Random Variable}\label{random-variable}}

Random variables can be categorized as either \textbf{discrete} or \textbf{continuous}, with distinct properties and functions defining each type.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3883}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4652}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Discrete Variable}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Continuous Variable}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Definition} & A random variable is discrete if it can assume at most a finite or countably infinite number of values. & A random variable is continuous if it can assume any value in some interval or intervals of real numbers, with \(P(X=x) = 0\). \\
\textbf{Density Function} & A function \(f\) is called a density for \(X\) if: & A function \(f\) is called a density for \(X\) if: \\
& 1. \(f(x) \geq 0\) & 1. \(f(x) \geq 0\) for \(x\) real \\
& 2. \(\sum_{x} f(x) = 1\) & 2. \(\int_{-\infty}^{\infty} f(x) \, dx = 1\) \\
& 3. \(f(x) = P(X = x)\) for \(x\) real & 3. \(P[a \leq X \leq b] = \int_{a}^{b} f(x) \, dx\) for \(a, b\) real \\
\textbf{Cumulative Distribution Function} & \(F(x) = P(X \leq x)\) & \(F(x) = P(X \leq x) = \int_{-\infty}^{x} f(t) \, dt\) \\
\(E[H(X)]\) & \(\sum_{x} H(x) f(x)\) & \(\int_{-\infty}^{\infty} H(x) f(x) \, dx\) \\
\(\mu = E[X]\) & \(\sum_{x} x f(x)\) & \(\int_{-\infty}^{\infty} x f(x) \, dx\) \\
\textbf{Ordinary Moments} & \(\sum_{x} x^k f(x)\) & \(\int_{-\infty}^{\infty} x^k f(x) \, dx\) \\
\textbf{Moment Generating Function} & \(m_X(t) = E[e^{tX}] = \sum_{x} e^{tx} f(x)\) & \(m_X(t) = E[e^{tX}] = \int_{-\infty}^{\infty} e^{tx} f(x) \, dx\) \\
\end{longtable}

\textbf{Expected Value Properties}

\begin{itemize}
\tightlist
\item
  \(E[c] = c\) for any constant \(c\).
\item
  \(E[cX] = cE[X]\) for any constant \(c\).
\item
  \(E[X + Y] = E[X] + E[Y]\).
\item
  \(E[XY] = E[X]E[Y]\) (if \(X\) and \(Y\) are independent).
\end{itemize}

\textbf{Variance Properties}

\begin{itemize}
\tightlist
\item
  \(\text{Var}(c) = 0\) for any constant \(c\).
\item
  \(\text{Var}(cX) = c^2 \text{Var}(X)\) for any constant \(c\).
\item
  \(\text{Var}(X) \geq 0\).
\item
  \(\text{Var}(X) = E[X^2] - (E[X])^2\).
\item
  \(\text{Var}(X + c) = \text{Var}(X)\).
\item
  \(\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y)\) (if \(X\) and \(Y\) are independent).
\end{itemize}

The standard deviation \(\sigma\) is given by: \[
\sigma = \sqrt{\sigma^2} = \sqrt{\text{Var}(X)}.
\]

\hypertarget{multivariate-random-variables}{%
\subsubsection{Multivariate Random Variables}\label{multivariate-random-variables}}

Suppose \(y_1, \dots, y_p\) are random variables with means \(\mu_1, \dots, \mu_p\). Then:

\[
\mathbf{y} = \begin{bmatrix}
y_1 \\
\vdots \\
y_p
\end{bmatrix}, \quad E[\mathbf{y}] = \begin{bmatrix}
\mu_1 \\
\vdots \\
\mu_p
\end{bmatrix} = \boldsymbol{\mu}.
\]

The covariance between \(y_i\) and \(y_j\) is \(\sigma_{ij} = \text{Cov}(y_i, y_j)\). The variance-covariance (or dispersion) matrix is:

\[
\mathbf{\Sigma} = (\sigma_{ij})= \begin{bmatrix}
\sigma_{11} & \sigma_{12} & \dots & \sigma_{1p} \\
\sigma_{21} & \sigma_{22} & \dots & \sigma_{2p} \\
\vdots & \vdots & \ddots & \vdots \\
\sigma_{p1} & \sigma_{p2} & \dots & \sigma_{pp}
\end{bmatrix}.
\]

And \(\mathbf{\Sigma}\) is symmetric with \((p+1)p/2\) unique parameters.

Alternatively, let \(u_{p \times 1}\) and \(v_{v \times 1}\) be random vectors with means \(\mathbf{\mu_u}\) and \(\mathbf{\mu_v}\). then

\[ \mathbf{\Sigma_{uv}} = cov(\mathbf{u,v}) = E[\mathbf{(u-\mu_u)(v-\mu_v)'}] \]

\(\Sigma_{uv} \neq \Sigma_{vu}\) (but \(\Sigma_{uv} = \Sigma_{vu}'\))

\textbf{Properties of Covariance Matrices}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Symmetry}: \(\mathbf{\Sigma}' = \mathbf{\Sigma}\).
\item
  \textbf{Eigen-Decomposition} (spectral decomposition,symmetric decomposition): \(\mathbf{\Sigma = \Phi \Lambda \Phi}\), where \(\mathbf{\Phi}\) is a matrix of eigenvectors such that \(\mathbf{\Phi \Phi' = I}\) (orthonormal), and \(\mathbf{\Lambda}\) is a diagonal matrix with eigenvalues \((\lambda_1,...,\lambda_p)\) on the diagonal.
\item
  \textbf{Non-Negative Definiteness}: \(\mathbf{a \Sigma a} \ge 0\) for any \(\mathbf{a} \in R^p\). Equivalently, the eigenvalues of \(\mathbf{\Sigma}\), \(\lambda_1 \ge ... \ge \lambda_p \ge 0\)
\item
  \textbf{Generalized Variance}: \(|\mathbf{\Sigma}| = \lambda_1 \dots \lambda_p \geq 0\).
\item
  \textbf{Trace}: \(\text{tr}(\mathbf{\Sigma}) = \lambda_1 + \dots + \lambda_p = \sigma_{11} + \dots+ \sigma_{pp} = \sum \sigma_{ii}\) = sum of variances (total variance).
\end{enumerate}

\textbf{Note}: \(\mathbf{\Sigma}\) is required to be positive definite. This implies that all eigenvalues are positive, and \(\mathbf{\Sigma}\) has an inverse \(\mathbf{\Sigma}^{-1}\), such that \(\mathbf{\Sigma}^{-1}\mathbf{\Sigma}= \mathbf{I}_{p \times p} = \mathbf{\Sigma}\mathbf{\Sigma}^{-1}\)

\hypertarget{correlation-matrices}{%
\subsubsection{Correlation Matrices}\label{correlation-matrices}}

The correlation coefficient \(\rho_{ij}\) and correlation matrix \(\mathbf{R}\) are defined as:

\[
\rho_{ij} = \frac{\sigma_{ij}}{\sqrt{\sigma_{ii}\sigma_{jj}}}, \quad \mathbf{R} = \begin{bmatrix}
1 & \rho_{12} & \dots & \rho_{1p} \\
\rho_{21} & 1 & \dots & \rho_{2p} \\
\vdots & \vdots & \ddots & \vdots \\
\rho_{p1} & \rho_{p2} & \dots & 1
\end{bmatrix}.
\]

where \(\rho_{ii} = 1 \forall i\)

\hypertarget{linear-transformations}{%
\subsubsection{Linear Transformations}\label{linear-transformations}}

Let \(\mathbf{A}\) and \(\mathbf{B}\) be matrices of constants, and \(\mathbf{c}\) and \(\mathbf{d}\) be vectors of constants. Then:

\begin{itemize}
\tightlist
\item
  \(E[\mathbf{Ay + c}] = \mathbf{A \mu_y + c}\).
\item
  \(\text{Var}(\mathbf{Ay + c}) = \mathbf{A \Sigma_y A'}\).
\item
  \(\text{Cov}(\mathbf{Ay + c, By + d}) = \mathbf{A \Sigma_y B'}\).
\end{itemize}

\hypertarget{moment-generating-function}{%
\subsection{Moment Generating Function}\label{moment-generating-function}}

\hypertarget{properties-of-the-moment-generating-function}{%
\subsubsection{Properties of the Moment Generating Function}\label{properties-of-the-moment-generating-function}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(\frac{d^k(m_X(t))}{dt^k} \bigg|_{t=0} = E[X^k]\) (The \(k\)-th derivative at \(t=0\) gives the \(k\)-th moment of \(X\)).
\item
  \(\mu = E[X] = m_X'(0)\) (The first derivative at \(t=0\) gives the mean).
\item
  \(E[X^2] = m_X''(0)\) (The second derivative at \(t=0\) gives the second moment).
\end{enumerate}

\hypertarget{theorems-involving-mgfs}{%
\subsubsection{Theorems Involving MGFs}\label{theorems-involving-mgfs}}

Let \(X_1, X_2, \dots, X_n, Y\) be random variables with MGFs \(m_{X_1}(t), m_{X_2}(t), \dots, m_{X_n}(t), m_Y(t)\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  If \(m_{X_1}(t) = m_{X_2}(t)\) for all \(t\) in some open interval about 0, then \(X_1\) and \(X_2\) have the same distribution.
\item
  If \(Y = \alpha + \beta X_1\), then: \[
  m_Y(t) = e^{\alpha t}m_{X_1}(\beta t).
  \]
\item
  If \(X_1, X_2, \dots, X_n\) are independent and \(Y = \alpha_0 + \alpha_1 X_1 + \alpha_2 X_2 + \dots + \alpha_n X_n\), where \(\alpha_0, \alpha_1, \dots, \alpha_n\) are constants, then: \[
  m_Y(t) = e^{\alpha_0 t} m_{X_1}(\alpha_1 t) m_{X_2}(\alpha_2 t) \dots m_{X_n}(\alpha_n t).
  \]
\item
  Suppose \(X_1, X_2, \dots, X_n\) are independent normal random variables with means \(\mu_1, \mu_2, \dots, \mu_n\) and variances \(\sigma_1^2, \sigma_2^2, \dots, \sigma_n^2\). If \(Y = \alpha_0 + \alpha_1 X_1 + \alpha_2 X_2 + \dots + \alpha_n X_n\), then:

  \begin{itemize}
  \tightlist
  \item
    \(Y\) is normally distributed.
  \item
    Mean: \(\mu_Y = \alpha_0 + \alpha_1 \mu_1 + \alpha_2 \mu_2 + \dots + \alpha_n \mu_n\).
  \item
    Variance: \(\sigma_Y^2 = \alpha_1^2 \sigma_1^2 + \alpha_2^2 \sigma_2^2 + \dots + \alpha_n^2 \sigma_n^2\).
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{moments}{%
\subsection{Moments}\label{moments}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1630}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3478}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4783}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Moment
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Uncentered
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Centered
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1st & \(E[X] = \mu = \text{Mean}(X)\) & \\
2nd & \(E[X^2]\) & \(E[(X-\mu)^2] = \text{Var}(X) = \sigma^2\) \\
3rd & \(E[X^3]\) & \(E[(X-\mu)^3]\) \\
4th & \(E[X^4]\) & \(E[(X-\mu)^4]\) \\
\end{longtable}

\begin{itemize}
\item
  \textbf{Skewness}: \(\text{Skewness}(X) = \frac{E[(X-\mu)^3]}{\sigma^3}\)

  \begin{itemize}
  \tightlist
  \item
    \textbf{Definition}: Skewness measures the asymmetry of a probability distribution around its mean.
  \item
    \textbf{Interpretation}:

    \begin{itemize}
    \item
      \textbf{Positive skewness}: The right tail (higher values) is longer or heavier than the left tail.
    \item
      \textbf{Negative skewness}: The left tail (lower values) is longer or heavier than the right tail.
    \item
      \textbf{Zero skewness}: The data is symmetric.
    \end{itemize}
  \end{itemize}
\item
  \textbf{Kurtosis}: \(\text{Kurtosis}(X) = \frac{E[(X-\mu)^4]}{\sigma^4}\)

  \begin{itemize}
  \item
    \textbf{Definition}: Kurtosis measures the ``tailedness'' or the heaviness of the tails of a probability distribution.
  \item
    Excess kurtosis (often reported) is the kurtosis minus 3 (to compare against the normal distribution's kurtosis of 3).
  \item
    \textbf{Interpretation}:

    \begin{itemize}
    \item
      \textbf{High kurtosis (\textgreater3)}: Heavy tails, more extreme outliers.
    \item
      \textbf{Low kurtosis (\textless3)}: Light tails, fewer outliers.
    \item
      \textbf{Normal distribution kurtosis = 3}: Benchmark for comparison.
    \end{itemize}
  \end{itemize}
\end{itemize}

\hypertarget{skewness}{%
\subsection{Skewness}\label{skewness}}

Skewness measures the asymmetry of the distribution:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Positive skew}: The right side (high values) is stretched out.

  \begin{itemize}
  \item
    Positive skew occurs when the right tail (higher values) of the distribution is longer or heavier.
  \item
    \textbf{Examples}:

    \begin{itemize}
    \item
      \textbf{Income Distribution}: In many countries, most people earn a moderate income, but a small fraction of ultra-high earners stretches the distribution's right tail.
    \item
      \textbf{Housing Prices}: Most homes may be around an affordable price, but a few extravagant mansions create a very long (and expensive) upper tail.
    \end{itemize}
  \end{itemize}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load required libraries}
\FunctionTok{library}\NormalTok{(ggplot2)}

\CommentTok{\# Simulate data for positive skew}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{positive\_skew\_income }\OtherTok{\textless{}{-}}
    \FunctionTok{rbeta}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{2}\NormalTok{) }\SpecialCharTok{*} \DecValTok{100}  \CommentTok{\# Income distribution example}
\NormalTok{positive\_skew\_housing }\OtherTok{\textless{}{-}}
    \FunctionTok{rbeta}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{2}\NormalTok{) }\SpecialCharTok{*} \DecValTok{1000}  \CommentTok{\# Housing prices example}

\CommentTok{\# Combine data}
\NormalTok{data\_positive\_skew }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
    \AttributeTok{value =} \FunctionTok{c}\NormalTok{(positive\_skew\_income, positive\_skew\_housing),}
    \AttributeTok{example =} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\StringTok{"Income Distribution"}\NormalTok{, }\DecValTok{1000}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\StringTok{"Housing Prices"}\NormalTok{, }\DecValTok{1000}\NormalTok{))}
\NormalTok{)}

\CommentTok{\# Plot positive skew}
\FunctionTok{ggplot}\NormalTok{(data\_positive\_skew, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ value, }\AttributeTok{fill =}\NormalTok{ example)) }\SpecialCharTok{+}
    \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{bins =} \DecValTok{30}\NormalTok{,}
                   \AttributeTok{alpha =} \FloatTok{0.7}\NormalTok{,}
                   \AttributeTok{position =} \StringTok{"identity"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{facet\_wrap}\NormalTok{( }\SpecialCharTok{\textasciitilde{}}\NormalTok{ example, }\AttributeTok{scales =} \StringTok{"free"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Visualization of Positive Skew"}\NormalTok{,}
         \AttributeTok{x =} \StringTok{"Value"}\NormalTok{,}
         \AttributeTok{y =} \StringTok{"Frequency"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{02.1-prerequisites_files/figure-latex/unnamed-chunk-1-1} \end{center}

\begin{itemize}
\tightlist
\item
  In the \textbf{Income Distribution} example, most people earn moderate incomes, but a few high earners stretch the right tail.
\item
  In the \textbf{Housing Prices} example, most homes are reasonably priced, but a few mansions create a long, expensive right tail.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Negative Skew (Left Skew)
\end{enumerate}

\begin{itemize}
\item
  Negative skew occurs when the left tail (lower values) of the distribution is longer or heavier.
\item
  \textbf{Examples}:

  \begin{itemize}
  \item
    \textbf{Scores on an Easy Test}: If an exam is very easy, most students score quite high, and only a few students score low, creating a left tail.
  \item
    \textbf{Age of Retirement}: Most people might retire around a common age (say 65+), with fewer retiring very early (stretching the left tail).
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Simulate data for negative skew}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{negative\_skew\_test }\OtherTok{\textless{}{-}}
    \DecValTok{10} \SpecialCharTok{{-}} \FunctionTok{rbeta}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{2}\NormalTok{) }\SpecialCharTok{*} \DecValTok{10}  \CommentTok{\# Easy test scores example}
\NormalTok{negative\_skew\_retirement }\OtherTok{\textless{}{-}}
    \DecValTok{80} \SpecialCharTok{{-}} \FunctionTok{rbeta}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{2}\NormalTok{) }\SpecialCharTok{*} \DecValTok{20}  \CommentTok{\# Retirement age example}

\CommentTok{\# Combine data}
\NormalTok{data\_negative\_skew }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
    \AttributeTok{value =} \FunctionTok{c}\NormalTok{(negative\_skew\_test, negative\_skew\_retirement),}
    \AttributeTok{example =} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\StringTok{"Easy Test Scores"}\NormalTok{, }\DecValTok{1000}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\StringTok{"Retirement Age"}\NormalTok{, }\DecValTok{1000}\NormalTok{))}
\NormalTok{)}

\CommentTok{\# Plot negative skew}
\FunctionTok{ggplot}\NormalTok{(data\_negative\_skew, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ value, }\AttributeTok{fill =}\NormalTok{ example)) }\SpecialCharTok{+}
    \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{bins =} \DecValTok{30}\NormalTok{,}
                   \AttributeTok{alpha =} \FloatTok{0.7}\NormalTok{,}
                   \AttributeTok{position =} \StringTok{"identity"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{facet\_wrap}\NormalTok{( }\SpecialCharTok{\textasciitilde{}}\NormalTok{ example, }\AttributeTok{scales =} \StringTok{"free"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Visualization of Negative Skew"}\NormalTok{,}
         \AttributeTok{x =} \StringTok{"Value"}\NormalTok{,}
         \AttributeTok{y =} \StringTok{"Frequency"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{02.1-prerequisites_files/figure-latex/unnamed-chunk-2-1} \end{center}

\begin{itemize}
\item
  In the \textbf{Easy Test Scores} example, most students perform well, but a few low scores stretch the left tail.
\item
  In the \textbf{Retirement Age} example, most people retire around the same age, but a small number of individuals retire very early, stretching the left tail.
\end{itemize}

\hypertarget{kurtosis}{%
\subsection{Kurtosis}\label{kurtosis}}

Kurtosis measures the ``peakedness'' or heaviness of the tails:

\begin{itemize}
\item
  \textbf{High kurtosis}: Tall, sharp peak with heavy tails.

  \begin{itemize}
  \tightlist
  \item
    \textbf{Example}: Financial market returns during a crisis (extreme losses or gains).
  \end{itemize}
\item
  \textbf{Low kurtosis}: Flatter peak with thinner tails.

  \begin{itemize}
  \tightlist
  \item
    \textbf{Example}: Human height distribution (fewer extreme deviations from the mean).
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Simulate data for kurtosis}
\NormalTok{low\_kurtosis }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{)  }\CommentTok{\# Low kurtosis}
\NormalTok{high\_kurtosis }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{900}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{))  }\CommentTok{\# High kurtosis}

\CommentTok{\# Combine data}
\NormalTok{data\_kurtosis }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{value =} \FunctionTok{c}\NormalTok{(low\_kurtosis, high\_kurtosis),}
  \AttributeTok{kurtosis\_type =} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\StringTok{"Low Kurtosis (Height Distribution)"}\NormalTok{, }\DecValTok{1000}\NormalTok{), }
                    \FunctionTok{rep}\NormalTok{(}\StringTok{"High Kurtosis (Market Returns)"}\NormalTok{, }\DecValTok{1000}\NormalTok{))}
\NormalTok{)}

\CommentTok{\# Plot kurtosis}
\FunctionTok{ggplot}\NormalTok{(data\_kurtosis, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ value, }\AttributeTok{fill =}\NormalTok{ kurtosis\_type)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{bins =} \DecValTok{30}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.7}\NormalTok{, }\AttributeTok{position =} \StringTok{"identity"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{kurtosis\_type) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{title =} \StringTok{"Visualization of Kurtosis"}\NormalTok{,}
    \AttributeTok{x =} \StringTok{"Value"}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"Frequency"}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{02.1-prerequisites_files/figure-latex/unnamed-chunk-3-1} \end{center}

\begin{itemize}
\item
  The left panel shows \textbf{low kurtosis}, similar to the distribution of human height, which has a flatter peak and thinner tails.
\item
  The right panel shows \textbf{high kurtosis}, reflecting financial market returns, where there are more extreme outliers in gains or losses.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{conditional-moments}{%
\subsubsection{Conditional Moments}\label{conditional-moments}}

For a random variable \(Y\) given \(X=x\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Expected Value}: \[
  E[Y|X=x] =
  \begin{cases}
  \sum_y y f_Y(y|x) & \text{for discrete RV}, \\
  \int_y y f_Y(y|x) \, dy & \text{for continuous RV}.
  \end{cases}
  \]
\item
  \textbf{Variance}: \[
  \text{Var}(Y|X=x) =
  \begin{cases}
  \sum_y (y - E[Y|X=x])^2 f_Y(y|x) & \text{for discrete RV}, \\
  \int_y (y - E[Y|X=x])^2 f_Y(y|x) \, dy & \text{for continuous RV}.
  \end{cases}
  \]
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{multivariate-moments}{%
\subsubsection{Multivariate Moments}\label{multivariate-moments}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Expected Value}: \[
  E
  \begin{bmatrix}
  X \\
  Y
  \end{bmatrix}
  =
  \begin{bmatrix}
  E[X] \\
  E[Y]
  \end{bmatrix}
  =
  \begin{bmatrix}
  \mu_X \\
  \mu_Y
  \end{bmatrix}
  \]
\item
  \textbf{Variance-Covariance Matrix}: \[
  \begin{aligned}
  \text{Var}
  \begin{bmatrix}
  X \\
  Y
  \end{bmatrix}
  &=
  \begin{bmatrix}
  \text{Var}(X) & \text{Cov}(X, Y) \\
  \text{Cov}(X, Y) & \text{Var}(Y)
  \end{bmatrix}
  \\
  &=
  \begin{bmatrix}
  E[(X-\mu_X)^2] & E[(X-\mu_X)(Y-\mu_Y)] \\
  E[(X-\mu_X)(Y-\mu_Y)] & E[(Y-\mu_Y)^2]
  \end{bmatrix}
  \end{aligned}
  \]
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{properties-of-moments}{%
\subsubsection{Properties of Moments}\label{properties-of-moments}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(E[aX + bY + c] = aE[X] + bE[Y] + c\)
\item
  \(\text{Var}(aX + bY + c) = a^2 \text{Var}(X) + b^2 \text{Var}(Y) + 2ab \text{Cov}(X, Y)\)
\item
  \(\text{Cov}(aX + bY, cX + dY) = ac \text{Var}(X) + bd \text{Var}(Y) + (ad + bc) \text{Cov}(X, Y)\)
\item
  Correlation: \(\rho_{XY} = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y}\)
\end{enumerate}

\hypertarget{distributions}{%
\subsection{Distributions}\label{distributions}}

\hypertarget{conditional-distributions}{%
\subsubsection{Conditional Distributions}\label{conditional-distributions}}

\[
f_{X|Y}(x|y) = \frac{f(x, y)}{f_Y(y)}
\]

If \(X\) and \(Y\) are independent:

\[
f_{X|Y}(x|y) = f_X(x).
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{discrete-distributions}{%
\subsubsection{Discrete Distributions}\label{discrete-distributions}}

\hypertarget{bernoulli-distribution}{%
\paragraph{Bernoulli Distribution}\label{bernoulli-distribution}}

A random variable \(X\) follows a Bernoulli distribution, denoted as \(X \sim \text{Bernoulli}(p)\), if it represents a single trial with:

\begin{itemize}
\item
  Success probability \(p\)
\item
  Failure probability \(q = 1-p\).
\end{itemize}

\textbf{Density Function}\[
f(x) = p^x (1-p)^{1-x}, \quad x \in \{0, 1\}
\]

\textbf{CDF}: Use table or manual computation.

\textbf{PDF}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(}
\NormalTok{    mc2d}\SpecialCharTok{::}\FunctionTok{rbern}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\AttributeTok{prob =} \FloatTok{0.5}\NormalTok{),}
    \AttributeTok{main =} \StringTok{"Histogram of Bernoulli Distribution"}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"Value"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"Frequency"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{02.1-prerequisites_files/figure-latex/unnamed-chunk-4-1} \end{center}

\textbf{Mean}

\[
\mu = E[X] = p
\]

\textbf{Variance}

\[
\sigma^2 = \text{Var}(X) = p(1-p)
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{binomial-distribution}{%
\paragraph{Binomial Distribution}\label{binomial-distribution}}

\(X \sim B(n, p)\) is the number of successes in \(n\) independent Bernoulli trials, where:

\begin{itemize}
\item
  \(n\) is the number of trials
\item
  \(p\) is the success probability.
\item
  The trials are identical and independent, and probability of success (\(p\)) and probability of failure (\(q = 1 - p\)) remains the same for all trials.
\end{itemize}

\textbf{Density Function}

\[
f(x) = \binom{n}{x} p^x (1-p)^{n-x}, \quad x = 0, 1, \dots, n
\]

\textbf{PDF}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(}
    \FunctionTok{rbinom}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\AttributeTok{size =} \DecValTok{100}\NormalTok{, }\AttributeTok{prob =} \FloatTok{0.5}\NormalTok{),}
    \AttributeTok{main =} \StringTok{"Histogram of Binomial Distribution"}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"Value"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"Frequency"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{02.1-prerequisites_files/figure-latex/unnamed-chunk-5-1} \end{center}

\textbf{MGF}

\[
m_X(t) = (1 - p + p e^t)^n
\]

\textbf{Mean}

\[
\mu = np
\]

\textbf{Variance}

\[
\sigma^2 = np(1-p)
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{poisson-distribution}{%
\paragraph{Poisson Distribution}\label{poisson-distribution}}

\(X \sim \text{Poisson}(\lambda)\) models the number of occurrences of an event in a fixed interval, with average rate \(\lambda\).

\begin{itemize}
\item
  Arises with Poisson process, which involves observing discrete events in a continuous ``interval'' of time, length, or space.
\item
  The random variable \(X\) is the number of occurrences of the event within an interval of \(s\) units.
\item
  The parameter \(\lambda\) is the average number of occurrences of the event in question per measurement unit. For the distribution, we use the parameter \(k = \lambda s\).
\end{itemize}

\textbf{Density Function}

\[
f(x) = \frac{e^{-k} k^x}{x!}, \quad x = 0, 1, 2, \dots
\]

\textbf{CDF}

\textbf{PDF}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(}\FunctionTok{rpois}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\AttributeTok{lambda =} \DecValTok{5}\NormalTok{),}
     \AttributeTok{main =} \StringTok{"Histogram of Poisson Distribution"}\NormalTok{,}
     \AttributeTok{xlab =} \StringTok{"Value"}\NormalTok{,}
     \AttributeTok{ylab =} \StringTok{"Frequency"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{02.1-prerequisites_files/figure-latex/unnamed-chunk-6-1} \end{center}

\textbf{MGF}

\[
m_X(t) = e^{k (e^t - 1)}
\]

\textbf{Mean}

\[
\mu = E(X) = k
\]

\textbf{Variance}

\[
\sigma^2 = Var(X) = k
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{geometric-distribution}{%
\paragraph{Geometric Distribution}\label{geometric-distribution}}

\(X \sim \text{G}(p)\) models the number of trials needed to obtain the first success, with:

\begin{itemize}
\item
  \(p\): probability of success
\item
  \(q = 1-p\): probability of failure.
\item
  The experiment consists of a series of trails. The outcome of each trial can be classed as being either a ``success'' (s) or ``failure'' (f). (i.e., Bernoulli trial).
\item
  The trials are identical and independent in the sense that the outcome of one trial has no effect on the outcome of any other (i..e, lack of memory - momerylessness). The probability of success (\(p\)) and probability of failure (\(q = 1- p\)) remains the same from trial to trial.
\end{itemize}

\textbf{Density Function}

\[
f(x) = p(1-p)^{x-1}, \quad x = 1, 2, \dots
\]

\textbf{CDF}\[
F(x) = 1 - (1-p)^x
\]

\textbf{PDF}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(}\FunctionTok{rgeom}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\AttributeTok{prob =} \FloatTok{0.5}\NormalTok{),}
     \AttributeTok{main =} \StringTok{"Histogram of Geometric Distribution"}\NormalTok{,}
     \AttributeTok{xlab =} \StringTok{"Value"}\NormalTok{,}
     \AttributeTok{ylab =} \StringTok{"Frequency"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{02.1-prerequisites_files/figure-latex/unnamed-chunk-7-1} \end{center}

\textbf{MGF}

\[
m_X(t) = \frac{p e^t}{1 - (1-p)e^t}, \quad t < -\ln(1-p)
\]

\textbf{Mean}

\[
\mu = \frac{1}{p}
\]

\textbf{Variance}

\[
\sigma^2 = \frac{1-p}{p^2}
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{hypergeometric-distribution}{%
\paragraph{Hypergeometric Distribution}\label{hypergeometric-distribution}}

\(X \sim \text{H}(N, r, n)\) models the number of successes in a sample of size \(n\) drawn without replacement from a population of size \(N\), where:

\begin{itemize}
\item
  \(r\) objects have the trait of interest
\item
  \(N-r\) do not have the trait.
\end{itemize}

\textbf{Density Function}

\[
f(x) = \frac{\binom{r}{x} \binom{N-r}{n-x}}{\binom{N}{n}}, \quad \max(0, n-(N-r)) \leq x \leq \min(n, r)
\]

\textbf{PDF}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(}
    \FunctionTok{rhyper}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\AttributeTok{m =} \DecValTok{50}\NormalTok{, }\AttributeTok{n =} \DecValTok{20}\NormalTok{, }\AttributeTok{k =} \DecValTok{30}\NormalTok{),}
    \AttributeTok{main =} \StringTok{"Histogram of Hypergeometric Distribution"}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"Value"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"Frequency"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{02.1-prerequisites_files/figure-latex/unnamed-chunk-8-1} \end{center}

\textbf{Mean}\[
\mu = E[X] = \frac{n r}{N}
\]

\textbf{Variance}\[
\sigma^2 = \text{Var}(X) = n \frac{r}{N} \frac{N-r}{N} \frac{N-n}{N-1}
\]

\textbf{Note}: For large \(N\) (when \(\frac{n}{N} \leq 0.05\)), the hypergeometric distribution can be approximated by a binomial distribution with \(p = \frac{r}{N}\).

\hypertarget{continuous-distributions}{%
\subsubsection{Continuous Distributions}\label{continuous-distributions}}

\hypertarget{uniform-distribution}{%
\paragraph{Uniform Distribution}\label{uniform-distribution}}

Defined over an interval \((a, b)\), where the probabilities are ``equally likely'' for subintervals of equal length.

\textbf{Density Function}: \[
f(x) = \frac{1}{b-a}, \quad a < x < b
\]

\textbf{CDF}\[
F(x) =
\begin{cases}
0 & \text{if } x < a \\
\frac{x-a}{b-a} & a \le x \le b \\
1 & \text{if } x > b
\end{cases}
\]

\textbf{PDF}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(}
    \FunctionTok{runif}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\AttributeTok{min =} \DecValTok{0}\NormalTok{, }\AttributeTok{max =} \DecValTok{1}\NormalTok{),}
    \AttributeTok{main =} \StringTok{"Histogram of Uniform Distribution"}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"Value"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"Frequency"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{02.1-prerequisites_files/figure-latex/unnamed-chunk-9-1} \end{center}

\textbf{MGF}\[
m_X(t) =
\begin{cases}
\frac{e^{tb} - e^{ta}}{t(b-a)} & \text{if } t \neq 0 \\
1 & \text{if } t = 0
\end{cases}
\]

\textbf{Mean}\[
\mu = E[X] = \frac{a + b}{2}
\]

\textbf{Variance}

\[
\sigma^2 = \text{Var}(X) = \frac{(b-a)^2}{12}
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{gamma-distribution}{%
\paragraph{Gamma Distribution}\label{gamma-distribution}}

The gamma distribution is used to define the exponential and \(\chi^2\) distributions.

The \textbf{gamma function} is defined as: \[
\Gamma(\alpha) = \int_0^{\infty} z^{\alpha-1}e^{-z}dz, \quad \alpha > 0
\]

\textbf{Properties of the Gamma Function}:

\begin{itemize}
\item
  \(\Gamma(1) = 1\)
\item
  For \(\alpha > 1\), \(\Gamma(\alpha) = (\alpha-1)\Gamma(\alpha-1)\)
\item
  If \(n\) is an integer and \(n > 1\), then \(\Gamma(n) = (n-1)!\)
\end{itemize}

\textbf{Density Function}:

\[
f(x) = \frac{1}{\Gamma(\alpha)\beta^{\alpha}} x^{\alpha-1} e^{-x/\beta}, \quad x > 0
\]

\textbf{CDF} (for \(\alpha = n\), and \(x>0\) a positive integer):

\[
F(x, n, \beta) = 1 - \sum_{k=0}^{n-1} \frac{(\frac{x}{\beta})^k e^{-x/\beta}}{k!}
\]

\textbf{PDF}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(}
    \FunctionTok{rgamma}\NormalTok{(}\AttributeTok{n =} \DecValTok{1000}\NormalTok{, }\AttributeTok{shape =} \DecValTok{5}\NormalTok{, }\AttributeTok{rate =} \DecValTok{1}\NormalTok{),}
    \AttributeTok{main =} \StringTok{"Histogram of Gamma Distribution"}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"Value"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"Frequency"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{02.1-prerequisites_files/figure-latex/unnamed-chunk-10-1} \end{center}

\textbf{MGF}

\[
m_X(t) = (1 - \beta t)^{-\alpha}, \quad t < \frac{1}{\beta}
\]

\textbf{Mean}

\[
\mu = E[X] = \alpha \beta
\]

\textbf{Variance}

\[
\sigma^2 = \text{Var}(X) = \alpha \beta^2
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{normal-distribution}{%
\paragraph{Normal Distribution}\label{normal-distribution}}

The normal distribution, denoted as \(N(\mu, \sigma^2)\), is symmetric and bell-shaped with parameters \(\mu\) (mean) and \(\sigma^2\) (variance). It is also known as the Gaussian distribution.

\textbf{Density Function}:

\[
f(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{1}{2} \left(\frac{x-\mu}{\sigma}\right)^2}, \quad -\infty < x < \infty, \; \sigma > 0
\]

\textbf{CDF}: Use table or numerical methods.

\textbf{PDF}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(}
    \FunctionTok{rnorm}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{),}
    \AttributeTok{main =} \StringTok{"Histogram of Normal Distribution"}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"Value"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"Frequency"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{02.1-prerequisites_files/figure-latex/unnamed-chunk-11-1} \end{center}

\textbf{MGF}

\[
m_X(t) = e^{\mu t + \frac{\sigma^2 t^2}{2}}
\]

\textbf{Mean}

\[
\mu = E[X]
\]

\textbf{Variance}

\[
\sigma^2 = \text{Var}(X)
\]

\textbf{Standard Normal Random Variable}:

\begin{itemize}
\item
  The normal random variable \(Z\) with mean \(\mu = 0\) and standard deviation \(\sigma = 1\) is called a standard normal random variable.
\item
  Any normal random variable \(X\) with mean \(\mu\) and standard deviation \(\sigma\) can be converted to the standard normal random variable \(Z\): \[
  Z = \frac{X - \mu}{\sigma}
  \]
\end{itemize}

\textbf{Normal Approximation to the Binomial Distribution}:

Let \(X\) be binomial with parameters \(n\) and \(p\). For large \(n\):

\begin{itemize}
\item
  If \(p \le 0.5\) and \(np > 5\), or
\item
  If \(p > 0.5\) and \(n(1-p) > 5\),
\end{itemize}

\(X\) is approximately normally distributed with mean \(\mu = np\) and standard deviation \(\sigma = \sqrt{np(1-p)}\).

When using the normal approximation, add or subtract 0.5 as needed for the continuity correction.

\textbf{Discrete Approximate Normal (Corrected)}:

\begin{longtable}[]{@{}cc@{}}
\caption{\textbf{Normal Probability Rule}}\tabularnewline
\toprule\noalign{}
Discrete & Approximate Normal (corrected) \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
Discrete & Approximate Normal (corrected) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(P(X = c)\) & \(P(c -0.5 < Y < c + 0.5)\) \\
\(P(X < c)\) & \(P(Y < c - 0.5)\) \\
\(P(X \le c)\) & \(P(Y < c + 0.5)\) \\
\(P(X > c)\) & \(P(Y > c + 0.5)\) \\
\(P(X \ge c)\) & \(P(Y > c - 0.5)\) \\
\end{longtable}

If X is normally distributed with parameters \(\mu\) and \(\sigma\), then

\begin{itemize}
\tightlist
\item
  \(P(-\sigma < X - \mu < \sigma) \approx .68\)
\item
  \(P(-2\sigma < X - \mu < 2\sigma) \approx .95\)
\item
  \(P(-3\sigma < X - \mu < 3\sigma) \approx .997\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{logistic-distribution}{%
\paragraph{Logistic Distribution}\label{logistic-distribution}}

The logistic distribution is a continuous probability distribution commonly used in logistic regression and other types of statistical modeling. It resembles the normal distribution but has heavier tails, allowing for more extreme values. - The logistic distribution is symmetric around \(\mu\). - Its heavier tails make it useful for modeling outcomes with occasional extreme values.

\textbf{Density Function}

\[
f(x; \mu, s) = \frac{e^{-(x-\mu)/s}}{s \left(1 + e^{-(x-\mu)/s}\right)^2}, \quad -\infty < x < \infty
\]

where \(\mu\) is the location parameter (mean) and \(s > 0\) is the scale parameter.

\textbf{CDF}

\[
F(x; \mu, s) = \frac{1}{1 + e^{-(x-\mu)/s}}, \quad -\infty < x < \infty
\]

\textbf{PDF}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(}
    \FunctionTok{rlogis}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\AttributeTok{location =} \DecValTok{0}\NormalTok{, }\AttributeTok{scale =} \DecValTok{1}\NormalTok{),}
    \AttributeTok{main =} \StringTok{"Histogram of Logistic Distribution"}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"Value"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"Frequency"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{02.1-prerequisites_files/figure-latex/unnamed-chunk-12-1} \end{center}

\textbf{MGF}

The MGF of the logistic distribution does not exist because its expected value diverges for most \(t\).

\textbf{Mean}

\[
\mu = E[X] = \mu
\]

\textbf{Variance}

\[
\sigma^2 = \text{Var}(X) = \frac{\pi^2 s^2}{3}
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{laplace-distribution}{%
\paragraph{Laplace Distribution}\label{laplace-distribution}}

The Laplace distribution, also known as the double exponential distribution, is a continuous probability distribution often used in economics, finance, and engineering. It is characterized by a peak at its mean and heavier tails compared to the normal distribution.

\begin{itemize}
\tightlist
\item
  The Laplace distribution is symmetric around \(\mu\).
\item
  It has heavier tails than the normal distribution, making it suitable for modeling data with more extreme outliers.
\end{itemize}

\textbf{Density Function}

\[
f(x; \mu, b) = \frac{1}{2b} e^{-|x-\mu|/b}, \quad -\infty < x < \infty
\]

where \(\mu\) is the location parameter (mean) and \(b > 0\) is the scale parameter.

\textbf{CDF}

\[
F(x; \mu, b) =
\begin{cases}
    \frac{1}{2} e^{(x-\mu)/b} & \text{if } x < \mu \\
    1 - \frac{1}{2} e^{-(x-\mu)/b} & \text{if } x \ge \mu
\end{cases}
\]

\textbf{PDF}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(}
\NormalTok{    VGAM}\SpecialCharTok{::}\FunctionTok{rlaplace}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\AttributeTok{location =} \DecValTok{0}\NormalTok{, }\AttributeTok{scale =} \DecValTok{1}\NormalTok{),}
    \AttributeTok{main =} \StringTok{"Histogram of Laplace Distribution"}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"Value"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"Frequency"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{02.1-prerequisites_files/figure-latex/unnamed-chunk-13-1} \end{center}

\textbf{MGF}

\[
m_X(t) = \frac{e^{\mu t}}{1 - b^2 t^2}, \quad |t| < \frac{1}{b}
\]

\textbf{Mean}

\[
\mu = E[X] = \mu
\]

\textbf{Variance}

\[
\sigma^2 = \text{Var}(X) = 2b^2
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{log-normal-distribution}{%
\paragraph{Log-normal Distribution}\label{log-normal-distribution}}

The log-normal distribution is denoted as \(\text{Lognormal}(\mu, \sigma^2)\).

\textbf{PDF}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(}\FunctionTok{rlnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{1000}\NormalTok{, }\AttributeTok{meanlog =} \DecValTok{0}\NormalTok{, }\AttributeTok{sdlog =} \DecValTok{1}\NormalTok{), }\AttributeTok{main=}\StringTok{"Histogram of Log{-}normal Distribution"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Value"}\NormalTok{, }\AttributeTok{ylab=}\StringTok{"Frequency"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{02.1-prerequisites_files/figure-latex/unnamed-chunk-14-1} \end{center}

\hypertarget{lognormal-distribution}{%
\paragraph{Lognormal Distribution}\label{lognormal-distribution}}

The lognormal distribution is a continuous probability distribution of a random variable whose logarithm is normally distributed. It is often used to model variables that are positively skewed, such as income or biological measurements.

\begin{itemize}
\tightlist
\item
  The lognormal distribution is positively skewed.
\item
  It is useful for modeling data that cannot take negative values and is often used in finance and environmental studies.
\end{itemize}

\textbf{Density Function}

\[
f(x; \mu, \sigma) = \frac{1}{x \sigma \sqrt{2\pi}} e^{-(\ln(x) - \mu)^2 / (2\sigma^2)}, \quad x > 0
\]

where \(\mu\) is the mean of the underlying normal distribution and \(\sigma > 0\) is the standard deviation.

\textbf{CDF}

The cumulative distribution function of the lognormal distribution is given by:

\[
F(x; \mu, \sigma) = \frac{1}{2} \left[ 1 + \text{erf}\left( \frac{\ln(x) - \mu}{\sigma \sqrt{2}} \right) \right], \quad x > 0
\]

\textbf{PDF}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(}
    \FunctionTok{rlnorm}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\AttributeTok{meanlog =} \DecValTok{0}\NormalTok{, }\AttributeTok{sdlog =} \DecValTok{1}\NormalTok{),}
    \AttributeTok{main =} \StringTok{"Histogram of Lognormal Distribution"}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"Value"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"Frequency"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{02.1-prerequisites_files/figure-latex/unnamed-chunk-15-1} \end{center}

\textbf{MGF}

The moment generating function (MGF) of the lognormal distribution does not exist in a simple closed form.

\textbf{Mean}

\[
E[X] = e^{\mu + \sigma^2 / 2}
\]

\textbf{Variance}

\[
\sigma^2 = \text{Var}(X) = \left( e^{\sigma^2} - 1 \right) e^{2\mu + \sigma^2}
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{exponential-distribution}{%
\paragraph{Exponential Distribution}\label{exponential-distribution}}

The exponential distribution, denoted as \(\text{Exp}(\lambda)\), is a special case of the gamma distribution with \(\alpha = 1\).

\begin{itemize}
\item
  It is commonly used to model the time between independent events that occur at a constant rate. It is often applied in reliability analysis and queuing theory.
\item
  The exponential distribution is memoryless, meaning the probability of an event occurring in the future is independent of the past.
\item
  It is commonly used to model waiting times, such as the time until the next customer arrives or the time until a radioactive particle decays.
\end{itemize}

\textbf{Density Function}

\[
f(x) = \frac{1}{\beta} e^{-x/\beta}, \quad x, \beta > 0
\]

\textbf{CDF}\[
F(x) =
\begin{cases}
0 & \text{if } x \le 0 \\
1 - e^{-x/\beta} & \text{if } x > 0
\end{cases}
\]

\textbf{PDF}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(}\FunctionTok{rexp}\NormalTok{(}\AttributeTok{n =} \DecValTok{1000}\NormalTok{, }\AttributeTok{rate =} \DecValTok{1}\NormalTok{),}
     \AttributeTok{main =} \StringTok{"Histogram of Exponential Distribution"}\NormalTok{,}
     \AttributeTok{xlab =} \StringTok{"Value"}\NormalTok{,}
     \AttributeTok{ylab =} \StringTok{"Frequency"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{02.1-prerequisites_files/figure-latex/unnamed-chunk-16-1} \end{center}

\textbf{MGF}\[
m_X(t) = (1-\beta t)^{-1}, \quad t < 1/\beta
\]

\textbf{Mean}\[
\mu = E[X] = \beta
\]

\textbf{Variance}\[
\sigma^2 = \text{Var}(X) = \beta^2
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{chi-squared-distribution}{%
\paragraph{Chi-Squared Distribution}\label{chi-squared-distribution}}

The chi-squared distribution is a continuous probability distribution commonly used in statistical inference, particularly in hypothesis testing and construction of confidence intervals for variance. It is also used in goodness-of-fit tests.

\begin{itemize}
\tightlist
\item
  The chi-squared distribution is defined only for positive values.
\item
  It is often used to model the distribution of the sum of the squares of \(k\) independent standard normal random variables.
\end{itemize}

\textbf{Density Function}

\[
f(x; k) = \frac{1}{2^{k/2} \Gamma(k/2)} x^{k/2 - 1} e^{-x/2}, \quad x \ge 0
\]

where \(k\) is the degrees of freedom and \(\Gamma\) is the gamma function.

\textbf{CDF}

The cumulative distribution function of the chi-squared distribution is given by:

\[
F(x; k) = \frac{\gamma(k/2, x/2)}{\Gamma(k/2)}, \quad x \ge 0
\]

where \(\gamma\) is the lower incomplete gamma function.

\textbf{PDF}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(}
    \FunctionTok{rchisq}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\AttributeTok{df =} \DecValTok{5}\NormalTok{),}
    \AttributeTok{main =} \StringTok{"Histogram of Chi{-}Squared Distribution"}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"Value"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"Frequency"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{02.1-prerequisites_files/figure-latex/unnamed-chunk-17-1} \end{center}

\textbf{MGF}

\[
m_X(t) = (1 - 2t)^{-k/2}, \quad t < \frac{1}{2}
\]

\textbf{Mean}

\[
E[X] = k
\]

\textbf{Variance}

\[
\sigma^2 = \text{Var}(X) = 2k
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{students-t-distribution}{%
\paragraph{Student's T Distribution}\label{students-t-distribution}}

The \textbf{Student's t-distribution} is named after \textbf{William Sealy Gosset}, a statistician at Guinness Brewery in the early 20th century. Gosset developed the t-distribution to address small-sample problems in quality control. Since Guinness prohibited employees from publishing under their names, Gosset used the pseudonym \textbf{``Student''} when he published his work in 1908 \citep{student1908probable}. The name has stuck ever since, honoring his contribution to statistics.

The Student's t-distribution, denoted as \(T(v)\), is defined by: \[
T = \frac{Z}{\sqrt{\chi^2_v / v}},
\] where \(Z\) is a standard normal random variable and \(\chi^2_v\) follows a chi-squared distribution with \(v\) degrees of freedom.

The Student's T distribution is a continuous probability distribution used in statistical inference, particularly for estimating population parameters when the sample size is small and/or the population variance is unknown. It is similar to the normal distribution but has heavier tails, which makes it more robust for small sample sizes.

\begin{itemize}
\tightlist
\item
  The Student's T distribution is symmetric around 0.
\item
  It has heavier tails than the normal distribution, making it useful for dealing with outliers or small sample sizes.
\end{itemize}

\textbf{Density Function}

\[
f(x;v) = \frac{\Gamma((v + 1)/2)}{\sqrt{v \pi} \Gamma(v/2)} \left( 1 + \frac{x^2}{v} \right)^{-(v + 1)/2}
\]

where \(v\) is the degrees of freedom and \(\Gamma(x)\) is the Gamma function.

\textbf{CDF}

The cumulative distribution function of the Student's T distribution is more complex and typically evaluated using numerical methods.

\textbf{PDF}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(}
    \FunctionTok{rt}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\AttributeTok{df =} \DecValTok{5}\NormalTok{),}
    \AttributeTok{main =} \StringTok{"Histogram of Student\textquotesingle{}s T Distribution"}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"Value"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"Frequency"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{02.1-prerequisites_files/figure-latex/unnamed-chunk-18-1} \end{center}

\textbf{MGF}

The moment generating function (MGF) of the Student's T distribution does not exist in a simple closed form.

\textbf{Mean}

For \(v > 1\):

\[
E[X] = 0
\]

\textbf{Variance}

For \(v > 2\):

\[
\sigma^2 =  \text{Var}(X) = \frac{v}{v - 2}
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{non-central-t-distribution}{%
\paragraph{Non-central T Distribution}\label{non-central-t-distribution}}

The non-central t-distribution, denoted as \(T(v, \lambda)\), is a generalization of the Student's t-distribution. It is defined as: \[
T = \frac{Z + \lambda}{\sqrt{\chi^2_v / v}},
\] where \(Z\) is a standard normal random variable, \(\chi^2_v\) follows a chi-squared distribution with \(v\) degrees of freedom, and \(\lambda\) is the \textbf{non-centrality parameter}. This additional parameter introduces asymmetry to the distribution.

The non-central t-distribution arises in scenarios where the null hypothesis does not hold, such as under the alternative hypothesis in hypothesis testing. The non-centrality parameter \(\lambda\) represents the degree to which the mean deviates from zero.

\begin{itemize}
\tightlist
\item
  For \(\lambda = 0\), the non-central t-distribution reduces to the Student's t-distribution.
\item
  The distribution is skewed for \(\lambda \neq 0\), with the skewness increasing as \(\lambda\) grows.
\end{itemize}

\textbf{Density Function}

The density function of the non-central t-distribution is more complex and depends on \(v\) and \(\lambda\). It can be expressed in terms of an infinite sum:

\[
f(x; v, \lambda) = \sum_{k=0}^\infty \frac{e^{-\lambda^2/2}(\lambda^2/2)^k}{k!} \cdot \frac{\Gamma((v + k + 1)/2)}{\sqrt{v \pi} \Gamma((v + k)/2)} \left( 1 + \frac{x^2}{v} \right)^{-(v + k + 1)/2}.
\]

\textbf{PDF}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{100}  \CommentTok{\# Number of samples}
\NormalTok{df }\OtherTok{\textless{}{-}} \DecValTok{5}    \CommentTok{\# Degrees of freedom}
\NormalTok{lambda }\OtherTok{\textless{}{-}} \DecValTok{2}  \CommentTok{\# Non{-}centrality parameter}


\FunctionTok{hist}\NormalTok{(}
  \FunctionTok{rt}\NormalTok{(n, }\AttributeTok{df =}\NormalTok{ df, }\AttributeTok{ncp =}\NormalTok{ lambda),}
  \AttributeTok{main =} \StringTok{"Histogram of Non{-}central T Distribution"}\NormalTok{,}
  \AttributeTok{xlab =} \StringTok{"Value"}\NormalTok{,}
  \AttributeTok{ylab =} \StringTok{"Frequency"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{02.1-prerequisites_files/figure-latex/unnamed-chunk-19-1} \end{center}

\textbf{CDF}

The cumulative distribution function of the non-central t-distribution is typically computed using numerical methods due to its complexity.

\textbf{Mean}

For \(v > 1\):

\[
E[T] = \lambda \sqrt{\frac{v}{2}} \cdot \frac{\Gamma((v - 1)/2)}{\Gamma(v/2)}.
\]

\textbf{Variance}

For \(v > 2\):

\[
\text{Var}(T) = \frac{v}{v - 2} + \lambda^2.
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Comparison: Student's T vs.~Non-central T}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2301}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3363}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4248}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Feature
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Student's t-distribution
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Non-central t-distribution
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Definition & \(T = \frac{Z}{\sqrt{\chi^2_v / v}}\) & \(T = \frac{Z + \lambda}{\sqrt{\chi^2_v / v}}\) \\
Centered at & 0 & \(\lambda\) \\
Symmetry & Symmetric & Skewed for \(\lambda \neq 0\) \\
Parameters & Degrees of freedom (\(v\)) & \(v\) and \(\lambda\) \\
Shape as \(v \to \infty\)

(df \(\to \infty\)) & Normal(0, 1) & Normal(\(\lambda\), 1) \\
Applications & Hypothesis testing under null & Power analysis, alternative testing \\
\end{longtable}

While the Student's t-distribution is used for standard hypothesis testing and confidence intervals, the non-central t-distribution finds its applications in scenarios involving non-null hypotheses, such as power and sample size calculations.

\hypertarget{f-distribution}{%
\paragraph{F Distribution}\label{f-distribution}}

The F-distribution, denoted as \(F(d_1, d_2)\), is strictly positive and used to compare variances.

\textbf{Definition}: \[
F = \frac{\chi^2_{d_1} / d_1}{\chi^2_{d_2} / d_2},
\] where \(\chi^2_{d_1}\) and \(\chi^2_{d_2}\) are independent chi-squared random variables with degrees of freedom \(d_1\) and \(d_2\), respectively.

The distribution is asymmetric and never negative.

The F distribution arises frequently as the null distribution of a test statistic, especially in the context of comparing variances, such as in analysis of variance (ANOVA).

\textbf{Density Function}

\[
f(x; d_1, d_2) = \frac{\sqrt{\frac{(d_1 x)^{d_1} d_2^{d_2}}{(d_1 x + d_2)^{d_1 + d_2}}}}{x B\left( \frac{d_1}{2}, \frac{d_2}{2} \right)}, \quad x > 0
\]

where \(d_1\) and \(d_2\) are the degrees of freedom and \(B\) is the beta function.

\textbf{CDF}

The cumulative distribution function of the F distribution is typically evaluated using numerical methods.

\textbf{PDF}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(}
    \FunctionTok{rf}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\AttributeTok{df1 =} \DecValTok{5}\NormalTok{, }\AttributeTok{df2 =} \DecValTok{2}\NormalTok{),}
    \AttributeTok{main =} \StringTok{"Histogram of F Distribution"}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"Value"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"Frequency"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{02.1-prerequisites_files/figure-latex/unnamed-chunk-20-1} \end{center}

\textbf{MGF}

The moment generating function (MGF) of the F distribution does not exist in a simple closed form.

\textbf{Mean}

For \(d_2 > 2\):

\[
E[X] = \frac{d_2}{d_2 - 2}
\]

\textbf{Variance}

For \(d_2 > 4\):

\[
\sigma^2 = \text{Var}(X) = \frac{2 d_2^2 (d_1 + d_2 - 2)}{d_1 (d_2 - 2)^2 (d_2 - 4)}
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{cauchy-distribution}{%
\paragraph{Cauchy Distribution}\label{cauchy-distribution}}

The Cauchy distribution is a continuous probability distribution that is often used in physics and has heavier tails than the normal distribution. It is notable because it does not have a finite mean or variance.

\begin{itemize}
\tightlist
\item
  The Cauchy distribution does not have a finite mean or variance.
\item
  The \protect\hyperlink{central-limit-theorem}{Central Limit Theorem} and Weak Law of Large Numbers do not apply to the Cauchy distribution.
\end{itemize}

\textbf{Density Function}

\[
f(x; x_0, \gamma) = \frac{1}{\pi \gamma \left[ 1 + \left( \frac{x - x_0}{\gamma} 
\right)^2 \right]}
\]

where \(x_0\) is the location parameter and \(\gamma > 0\) is the scale parameter.

\textbf{CDF}

The cumulative distribution function of the Cauchy distribution is given by:

\[
F(x; x_0, \gamma) = \frac{1}{\pi} \arctan \left( \frac{x - x_0}{\gamma}  \right) + \frac{1}{2}
\]

\textbf{PDF}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(}
    \FunctionTok{rcauchy}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\AttributeTok{location =} \DecValTok{0}\NormalTok{, }\AttributeTok{scale =} \DecValTok{1}\NormalTok{),}
    \AttributeTok{main =} \StringTok{"Histogram of Cauchy Distribution"}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"Value"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"Frequency"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{02.1-prerequisites_files/figure-latex/unnamed-chunk-21-1} \end{center}

\textbf{MGF}

The MGF of the Cauchy distribution does not exist.

\textbf{Mean}

The mean of the Cauchy distribution is undefined.

\textbf{Variance}

The variance of the Cauchy distribution is undefined.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{multivariate-normal-distribution}{%
\paragraph{Multivariate Normal Distribution}\label{multivariate-normal-distribution}}

Let \(y\) be a \(p\)-dimensional multivariate normal (MVN) random variable with mean \(\mu\) and variance-covariance matrix \(\Sigma\). The density function of \(y\) is given by:

\[ f(\mathbf{y}) = \frac{1}{(2\pi)^{p/2}|\mathbf{\Sigma}|^{1/2}} \exp\left(-\frac{1}{2} (\mathbf{y}-\mu)' \Sigma^{-1} (\mathbf{y}-\mu)\right) \]

where \(|\mathbf{\Sigma}|\) represents the determinant of the variance-covariance matrix \(\Sigma\), and \(\mathbf{y} \sim N_p(\mathbf{\mu}, \mathbf{\Sigma})\).

\textbf{Properties}:

\begin{itemize}
\tightlist
\item
  Let \(\mathbf{A}_{r \times p}\) be a fixed matrix. Then \(\mathbf{A y} \sim N_r(\mathbf{A \mu}, \mathbf{A \Sigma A'})\). Note that \(r \le p\), and all rows of \(\mathbf{A}\) must be linearly independent to guarantee that \(\mathbf{A \Sigma A'}\) is non-singular.
\item
  Let \(\mathbf{G}\) be a matrix such that \(\mathbf{\Sigma^{-1} = G G'}\). Then \(\mathbf{G'y} \sim N_p(\mathbf{G'\mu}, \mathbf{I})\) and \(\mathbf{G'(y - \mu)} \sim N_p(\mathbf{0}, \mathbf{I})\).
\item
  Any fixed linear combination of \(y_1, \dots, y_p\), say \(\mathbf{c'y}\), follows \(\mathbf{c'y} \sim N_1(\mathbf{c'\mu}, \mathbf{c'\Sigma c})\).
\end{itemize}

\textbf{Large Sample Properties}

Suppose that \(y_1, \dots, y_n\) are a random sample from some population with mean \(\mu\) and variance-covariance matrix \(\Sigma\):

\[ \mathbf{Y} \sim MVN(\mathbf{\mu}, \mathbf{\Sigma}) \]

Then:

\begin{itemize}
\tightlist
\item
  \(\bar{\mathbf{y}} = \frac{1}{n} \sum_{i=1}^n \mathbf{y}_i\) is a consistent estimator for \(\mathbf{\mu}\).
\item
  \(\mathbf{S} = \frac{1}{n-1} \sum_{i=1}^n (\mathbf{y}_i - \bar{\mathbf{y}})(\mathbf{y}_i - \bar{\mathbf{y}})'\) is a consistent estimator for \(\mathbf{\Sigma}\).
\item
  \textbf{Multivariate Central Limit Theorem}: Similar to the univariate case, \(\sqrt{n}(\bar{\mathbf{y}} - \mu) \sim N_p(\mathbf{0}, \mathbf{\Sigma})\) when \(n\) is large relative to \(p\) (e.g., \(n \ge 25p\)), which is equivalent to \(\bar{\mathbf{y}} \sim N_p(\mathbf{\mu}, \mathbf{\Sigma/n})\).
\item
  \textbf{Wald's Theorem}: \(n(\bar{\mathbf{y}} - \mu)' \mathbf{S^{-1}} (\bar{\mathbf{y}} - \mu) \sim \chi^2_{(p)}\) when \(n\) is large relative to \(p\).
\end{itemize}

\textbf{Density Function}

\[
f(\mathbf{x}; \boldsymbol{\mu}, \boldsymbol{\Sigma}) = \frac{1}{(2\pi)^{k/2} | \boldsymbol{\Sigma}|^{1/2}} \exp\left( -\frac{1}{2} (\mathbf{x} - \boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu}) 
\right)
\]

where \(\boldsymbol{\mu}\) is the mean vector, \(\boldsymbol{\Sigma}\) is the covariance matrix, \(\mathbf{x} \in \mathbb{R}^k\) and \(k\) is the number of variables.

\textbf{CDF}

The cumulative distribution function of the multivariate normal distribution does not have a simple closed form and is typically evaluated using numerical methods.

\textbf{PDF}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{k }\OtherTok{\textless{}{-}} \DecValTok{2}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{mu }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{sigma }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{nrow =}\NormalTok{ k)}
\FunctionTok{library}\NormalTok{(MASS)}
\FunctionTok{hist}\NormalTok{(}
    \FunctionTok{mvrnorm}\NormalTok{(n, }\AttributeTok{mu =}\NormalTok{ mu, }\AttributeTok{Sigma =}\NormalTok{ sigma)[,}\DecValTok{1}\NormalTok{],}
    \AttributeTok{main =} \StringTok{"Histogram of MVN Distribution (1st Var)"}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"Value"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"Frequency"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{02.1-prerequisites_files/figure-latex/unnamed-chunk-22-1} \end{center}

\textbf{MGF}

\[
m_{\mathbf{X}}(\mathbf{t}) = \exp\left(\boldsymbol{\mu}^T \mathbf{t} + \frac{1}{2} \mathbf{t}^T \boldsymbol{\Sigma} \mathbf{t} 
\right)
\]

\textbf{Mean}

\[
E[\mathbf{X}] = \boldsymbol{\mu}
\]

\textbf{Variance}

\[
\text{Var}(\mathbf{X}) = \boldsymbol{\Sigma}
\]

\hypertarget{general-math}{%
\section{General Math}\label{general-math}}

\hypertarget{number-sets}{%
\subsection{Number Sets}\label{number-sets}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2083}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2083}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5833}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Notation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Denotes
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Examples
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(\emptyset\) & Empty set & No members \\
\(\mathbb{N}\) & Natural numbers & \(\{1, 2, \ldots\}\) \\
\(\mathbb{Z}\) & Integers & \(\{\ldots, -1, 0, 1, \ldots\}\) \\
\(\mathbb{Q}\) & Rational numbers & Including fractions \\
\(\mathbb{R}\) & Real numbers & Including all finite decimals, irrational numbers \\
\(\mathbb{C}\) & Complex numbers & Including numbers of the form \(a + bi\) where \(i^2 = -1\) \\
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{summation-notation-and-series}{%
\subsection{Summation Notation and Series}\label{summation-notation-and-series}}

\hypertarget{chebyshevs-inequality}{%
\subsubsection{Chebyshev's Inequality}\label{chebyshevs-inequality}}

Let \(X\) be a random variable with mean \(\mu\) and standard deviation \(\sigma\). For any positive number \(k\), Chebyshev's Inequality states:

\[
P(|X-\mu| \geq k\sigma) \leq \frac{1}{k^2}
\]

This provides a probabilistic bound on the deviation of \(X\) from its mean and does not require \(X\) to follow a normal distribution.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{geometric-sum}{%
\subsubsection{Geometric Sum}\label{geometric-sum}}

For a geometric series of the form \(\sum_{k=0}^{n-1} ar^k\), the sum is given by:

\[
\sum_{k=0}^{n-1} ar^k = a\frac{1-r^n}{1-r} \quad \text{where } r \neq 1
\]

\hypertarget{infinite-geometric-series}{%
\subsubsection{Infinite Geometric Series}\label{infinite-geometric-series}}

When \(|r| < 1\), the geometric series converges to:

\[
\sum_{k=0}^\infty ar^k = \frac{a}{1-r}
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{binomial-theorem}{%
\subsubsection{Binomial Theorem}\label{binomial-theorem}}

The binomial expansion for \((x + y)^n\) is:

\[
(x + y)^n = \sum_{k=0}^n \binom{n}{k} x^{n-k} y^k \quad \text{where } n \geq 0
\]

\hypertarget{binomial-series}{%
\subsubsection{Binomial Series}\label{binomial-series}}

For non-integer exponents \(\alpha\):

\[
\sum_{k=0}^\infty \binom{\alpha}{k} x^k = (1 + x)^\alpha \quad \text{where } |x| < 1
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{telescoping-sum}{%
\subsubsection{Telescoping Sum}\label{telescoping-sum}}

A telescoping sum simplifies as intermediate terms cancel, leaving:

\[
\sum_{a \leq k < b} \Delta F(k) = F(b) - F(a) \quad \text{where } a, b \in \mathbb{Z}, a \leq b
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{vandermonde-convolution}{%
\subsubsection{Vandermonde Convolution}\label{vandermonde-convolution}}

The Vandermonde convolution identity is:

\[
\sum_{k=0}^n \binom{r}{k} \binom{s}{n-k} = \binom{r+s}{n} \quad \text{where } n \in \mathbb{Z}
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{exponential-series}{%
\subsubsection{Exponential Series}\label{exponential-series}}

The exponential function \(e^x\) can be represented as:

\[
\sum_{k=0}^\infty \frac{x^k}{k!} = e^x \quad \text{where } x \in \mathbb{C}
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{taylor-series}{%
\subsubsection{Taylor Series}\label{taylor-series}}

The Taylor series expansion for a function \(f(x)\) about \(x=a\) is:

\[
\sum_{k=0}^\infty \frac{f^{(k)}(a)}{k!} (x-a)^k = f(x)
\]

For \(a = 0\), this becomes the \textbf{Maclaurin series}.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{maclaurin-series-for-ez}{%
\subsubsection{\texorpdfstring{Maclaurin Series for \(e^z\)}{Maclaurin Series for e\^{}z}}\label{maclaurin-series-for-ez}}

A special case of the Taylor series, the Maclaurin expansion for \(e^z\) is:

\[
e^z = 1 + z + \frac{z^2}{2!} + \frac{z^3}{3!} + \cdots
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{eulers-summation-formula}{%
\subsubsection{Euler's Summation Formula}\label{eulers-summation-formula}}

Euler's summation formula connects sums and integrals:

\[
\sum_{a \leq k < b} f(k) = \int_a^b f(x) \, dx + \sum_{k=1}^m \frac{B_k}{k!} \left[f^{(k-1)}(x)\right]_a^b 
+ (-1)^{m+1} \int_a^b \frac{B_m(x-\lfloor x \rfloor)}{m!} f^{(m)}(x) \, dx
\]

Here, \(B_k\) are Bernoulli numbers.

\begin{itemize}
\tightlist
\item
  \textbf{For} \(m=1\) (Trapezoidal Rule):
\end{itemize}

\[
\sum_{a \leq k < b} f(k) \approx \int_a^b f(x) \, dx - \frac{1}{2}(f(b) - f(a))
\]

\hypertarget{taylor-expansion}{%
\subsection{Taylor Expansion}\label{taylor-expansion}}

A differentiable function, \(G(x)\), can be written as an infinite sum of its derivatives. More specifically, if \(G(x)\) is infinitely differentiable and evaluated at \(a\), its Taylor expansion is:

\[
G(x) = G(a) + \frac{G'(a)}{1!} (x-a) + \frac{G''(a)}{2!}(x-a)^2 + \frac{G'''(a)}{3!}(x-a)^3 + \dots
\]

This expansion is valid within the radius of convergence.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{law-of-large-numbers}{%
\subsection{Law of Large Numbers}\label{law-of-large-numbers}}

Let \(X_1, X_2, \ldots\) be an infinite sequence of independent and identically distributed (i.i.d.) random variables with finite mean \(\mu\) and variance \(\sigma^2\). The \textbf{Law of Large Numbers (LLN)} states that the sample average:

\[
\bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i
\]

converges to the expected value \(\mu\) as \(n \rightarrow \infty\). This can be expressed as:

\[
\bar{X}_n \rightarrow \mu \quad \text{(as $n \rightarrow \infty$)}.
\]

\hypertarget{variance-of-the-sample-mean}{%
\subsubsection{Variance of the Sample Mean}\label{variance-of-the-sample-mean}}

The variance of the sample mean decreases as the sample size increases:

\[
Var(\bar{X}_n) = Var\left(\frac{1}{n} \sum_{i=1}^n X_i\right) = \frac{\sigma^2}{n}.
\]

\[ 
\begin{aligned}
Var(\bar{X}_n) &= Var(\frac{1}{n}(X_1 + ... + X_n)) =Var\left(\frac{1}{n} \sum_{i=1}^n X_i\right) \\
&= \frac{1}{n^2}Var(X_1 + ... + X_n) \\
&=\frac{n\sigma^2}{n^2}=\frac{\sigma^2}{n} 
\end{aligned}
\]

\textbf{Note}: The connection between the \protect\hyperlink{law-of-large-numbers}{Law of Large Numbers} and the \protect\hyperlink{normal-distribution}{Normal Distribution} lies in the \protect\hyperlink{central-limit-theorem}{Central Limit Theorem}. The CLT states that, regardless of the original distribution of a dataset, the distribution of the sample means will tend to follow a normal distribution as the sample size becomes larger.

The difference between {[}Weak Law{]} and {[}Strong Law{]} regards the mode of convergence.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{weak-law-of-large-numbers}{%
\subsubsection{Weak Law of Large Numbers}\label{weak-law-of-large-numbers}}

The \textbf{Weak Law of Large Numbers} states that the sample average converges in probability to the expected value:

\[
\bar{X}_n \xrightarrow{p} \mu \quad \text{as } n \rightarrow \infty.
\]

Formally, for any \(\epsilon > 0\):

\[
\lim_{n \to \infty} P(|\bar{X}_n - \mu| > \epsilon) = 0.
\]

Additionally, the sample mean of an i.i.d. random sample (\(\{ X_i \}_{i=1}^n\)) from any population with a finite mean and variance is a consistent estimator of the population mean \(\mu\):

\[
plim(\bar{X}_n) = plim\left(\frac{1}{n}\sum_{i=1}^{n} X_i\right) = \mu.
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{strong-law-of-large-numbers}{%
\subsubsection{Strong Law of Large Numbers}\label{strong-law-of-large-numbers}}

The \textbf{Strong Law of Large Numbers} states that the sample average converges almost surely to the expected value:

\[
\bar{X}_n \xrightarrow{a.s.} \mu \quad \text{as } n \rightarrow \infty.
\]

Equivalently, this can be expressed as:

\[
P\left(\lim_{n \to \infty} \bar{X}_n = \mu\right) = 1.
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{convergence}{%
\subsection{Convergence}\label{convergence}}

\hypertarget{convergence-in-probability}{%
\subsubsection{Convergence in Probability}\label{convergence-in-probability}}

As \(n \rightarrow \infty\), an estimator (random variable) \(\theta_n\) is said to converge in probability to a constant \(c\) if:

\[
\lim_{n \to \infty} P(|\theta_n - c| \geq \epsilon) = 0 \quad \text{for any } \epsilon > 0.
\]

This is denoted as:

\[
plim(\theta_n) = c \quad \text{or equivalently, } \theta_n \xrightarrow{p} c.
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Properties of Convergence in Probability:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Slutsky's Theorem}: For a continuous function \(g(\cdot)\), if \(plim(\theta_n) = \theta\), then:

  \[
  plim(g(\theta_n)) = g(\theta)
  \]
\item
  If \(\gamma_n \xrightarrow{p} \gamma\), then:

  \begin{itemize}
  \tightlist
  \item
    \(plim(\theta_n + \gamma_n) = \theta + \gamma\),
  \item
    \(plim(\theta_n \gamma_n) = \theta \gamma\),
  \item
    \(plim(\theta_n / \gamma_n) = \theta / \gamma\) (if \(\gamma \neq 0\)).
  \end{itemize}
\item
  These properties extend to random vectors and matrices.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{convergence-in-distribution}{%
\subsubsection{Convergence in Distribution}\label{convergence-in-distribution}}

As \(n \rightarrow \infty\), the distribution of a random variable \(X_n\) may converge to another (``fixed'') distribution. Formally, \(X_n\) with CDF \(F_n(x)\) converges in distribution to \(X\) with CDF \(F(x)\) if:

\[
\lim_{n \to \infty} |F_n(x) - F(x)| = 0
\]

at all points of continuity of \(F(x)\). This is denoted as:

\[
X_n \xrightarrow{d} X \quad \text{or equivalently, } F(x) \text{ is the limiting distribution of } X_n.
\]

\textbf{Asymptotic Properties:}

\begin{itemize}
\tightlist
\item
  \(E(X)\): Limiting mean (asymptotic mean).
\item
  \(Var(X)\): Limiting variance (asymptotic variance).
\end{itemize}

\textbf{Note:} Limiting expectations and variances do not necessarily match the expectations and variances of \(X_n\):

\[
\begin{aligned}
E(X) &\neq \lim_{n \to \infty} E(X_n), \\
Avar(X_n) &\neq \lim_{n \to \infty} Var(X_n).
\end{aligned}
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Properties of Convergence in Distribution:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Continuous Mapping Theorem}: For a continuous function \(g(\cdot)\), if \(X_n \xrightarrow{d} X\), then:

  \[
  g(X_n) \xrightarrow{d} g(X).
  \]
\item
  If \(Y_n \xrightarrow{d} c\) (a constant), then:

  \begin{itemize}
  \tightlist
  \item
    \(X_n + Y_n \xrightarrow{d} X + c\),
  \item
    \(Y_n X_n \xrightarrow{d} c X\),
  \item
    \(X_n / Y_n \xrightarrow{d} X / c\) (if \(c \neq 0\)).
  \end{itemize}
\item
  These properties also extend to random vectors and matrices.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{summary-properties-of-convergence}{%
\subsubsection{Summary: Properties of Convergence}\label{summary-properties-of-convergence}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.4861}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5139}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Convergence in Probability
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Convergence in Distribution
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Slutsky's Theorem: For a continuous \(g(\cdot)\), if \(plim(\theta_n) = \theta\), then \(plim(g(\theta_n)) = g(\theta)\) & Continuous Mapping Theorem: For a continuous \(g(\cdot)\), if \(X_n \xrightarrow{d} X\), then \(g(X_n) \xrightarrow{d} g(X)\) \\
If \(\gamma_n \xrightarrow{p} \gamma\), then: & If \(Y_n \xrightarrow{d} c\), then: \\
\(plim(\theta_n + \gamma_n) = \theta + \gamma\) & \(X_n + Y_n \xrightarrow{d} X + c\) \\
\(plim(\theta_n \gamma_n) = \theta \gamma\) & \(Y_n X_n \xrightarrow{d} c X\) \\
\(plim(\theta_n / \gamma_n) = \theta / \gamma\) (if \(\gamma \neq 0\)) & \(X_n / Y_n \xrightarrow{d} X / c\) (if \(c \neq 0\)) \\
\end{longtable}

\textbf{Relationship between Convergence Types:}

\protect\hyperlink{convergence-in-probability}{Convergence in Probability} is stronger than \protect\hyperlink{convergence-in-distribution}{Convergence in Distribution}. Therefore:

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{convergence-in-distribution}{Convergence in Distribution} does not guarantee \protect\hyperlink{convergence-in-probability}{Convergence in Probability}.
\end{itemize}

\hypertarget{sufficient-statistics-and-likelihood}{%
\subsection{Sufficient Statistics and Likelihood}\label{sufficient-statistics-and-likelihood}}

\hypertarget{likelihood}{%
\subsubsection{Likelihood}\label{likelihood}}

The \textbf{likelihood} describes the degree to which the observed data supports a particular value of a parameter \(\theta\).

\begin{itemize}
\tightlist
\item
  The exact value of the likelihood is \textbf{not meaningful}; only relative comparisons matter.
\item
  Likelihood is \textbf{informative} when comparing parameter values, helping identify which values of \(\theta\) are more plausible given the data.
\end{itemize}

For a single observation \(Y = y\), the likelihood function is defined as:

\[
L(\theta_0; y) = P(Y = y \mid \theta = \theta_0) = f_Y(y; \theta_0),
\]

where \(f_Y(y; \theta_0)\) is the probability density (or mass) function of \(Y\) for the parameter \(\theta_0\).

\textbf{Key Insight}: The likelihood tells us how plausible \(\theta\) is, given the data we observed. It is \textbf{not a probability}, but it is proportional to the probability of observing the data under a given parameter value.

\textbf{Example}: Suppose \(Y\) follows a binomial distribution with \(n=10\) trials and probability of success \(p\):

\[
P(Y = y \mid p) = \binom{10}{y} p^y (1-p)^{10-y}.
\]

For \(y=7\) observed successes, the likelihood function becomes:

\[
L(p; y=7) = \binom{10}{7} p^7 (1-p)^3.
\]

We can use this to compare how well different values of \(p\) explain the observed data.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{likelihood-ratio}{%
\subsubsection{Likelihood Ratio}\label{likelihood-ratio}}

The \textbf{likelihood ratio} compares the relative likelihood of two parameter values \(\theta_0\) and \(\theta_1\) given the observed data:

\[
\text{Likelihood Ratio} = \frac{L(\theta_0; y)}{L(\theta_1; y)}.
\]

\begin{itemize}
\tightlist
\item
  A likelihood ratio greater than 1 implies that \(\theta_0\) is more likely than \(\theta_1\), given the observed data.
\item
  Likelihood ratios are widely used in hypothesis testing and model comparison to evaluate the evidence against a null hypothesis.
\end{itemize}

\textbf{Example}: For the binomial example above, consider \(p_0 = 0.7\) and \(p_1 = 0.5\). The likelihood ratio is:

\[
\frac{L(p_0; y=7)}{L(p_1; y=7)} = \frac{\binom{10}{7} (0.7)^7 (0.3)^3}{\binom{10}{7} (0.5)^7 (0.5)^3}.
\]

This simplifies to:

\[
\frac{(0.7)^7 (0.3)^3}{(0.5)^7 (0.5)^3}.
\]

The likelihood ratio quantifies how much more likely \(p_0\) is compared to \(p_1\) given the observed data.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{likelihood-function}{%
\subsubsection{Likelihood Function}\label{likelihood-function}}

For a given sample, the likelihood for all possible values of \(\theta\) forms the \textbf{likelihood function}:

\[
L(\theta) = L(\theta; y) = f_Y(y; \theta).
\]

For a sample of size \(n\), assuming independence among observations:

\[
L(\theta) = \prod_{i=1}^{n} f_Y(y_i; \theta).
\]

Taking the natural logarithm of the likelihood gives the \textbf{log-likelihood function}:

\[
l(\theta) = \sum_{i=1}^{n} \log f_Y(y_i; \theta).
\]

\textbf{Why Log-Likelihood?}

\begin{itemize}
\tightlist
\item
  The log-likelihood simplifies computation by turning products into sums.
\item
  It is particularly useful for optimization, as many numerical methods (e.g., gradient-based algorithms) perform better with sums than products.
\end{itemize}

\textbf{Example}: For \(Y_1, Y_2, \dots, Y_n\) i.i.d. observations from a normal distribution \(N(\mu, \sigma^2)\), the likelihood is:

\[
L(\mu, \sigma^2) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - \mu)^2}{2\sigma^2}\right).
\]

The log-likelihood is:

\[
l(\mu, \sigma^2) = -\frac{n}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - \mu)^2.
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{sufficient-statistics}{%
\subsubsection{Sufficient Statistics}\label{sufficient-statistics}}

A \textbf{sufficient statistic} \(T(y)\) is a summary of the data that retains all information about a parameter \(\theta\). It allows us to focus on this condensed statistic without losing any inferential power regarding \(\theta\).

\textbf{Formal Definition:}

A statistic \(T(y)\) is sufficient for a parameter \(\theta\) if the conditional probability distribution of the data \(y\), given \(T(y)\) and \(\theta\), does not depend on \(\theta\). Mathematically:

\[ P(Y = y \mid T(y), \theta) = P(Y = y \mid T(y)). \]

Alternatively, by the \textbf{Factorization Theorem}, \(T(y)\) is sufficient if the likelihood can be written as:

\[ L(\theta; y) = c(y) L^*(\theta; T(y)), \]

where:

\begin{itemize}
\tightlist
\item
  \(c(y)\) is a function of the data independent of \(\theta\).
\item
  \(L^*(\theta; T(y))\) is a function that depends on \(\theta\) and \(T(y)\).
\end{itemize}

In other words, the likelihood function can be rewritten in terms of \(T(y)\) alone, without loss of information about \(\theta\).

\textbf{Why Sufficient Statistics Matter:}

\begin{itemize}
\tightlist
\item
  They allow us to simplify the analysis by reducing the data without losing inferential power.
\item
  Many inferential procedures (e.g., Maximum Likelihood Estimation, Bayesian methods) are simplified by working with sufficient statistics.
\end{itemize}

\textbf{Example:}

Consider a sample of i.i.d. observations \(Y_1, Y_2, \dots, Y_n\) from a normal distribution \(N(\mu, \sigma^2)\). Here:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The sample mean \(\bar{Y} = \frac{1}{n} \sum_{i=1}^n Y_i\) is sufficient for \(\mu\).
\item
  The sample variance \(S^2 = \frac{1}{n-1} \sum_{i=1}^n (Y_i - \bar{Y})^2\) is sufficient for \(\sigma^2\).
\end{enumerate}

\textbf{Verification:} The joint density of \(y_1, y_2, \dots, y_n\) can be factored as:

\[
f(y_1, \dots, y_n; \mu, \sigma^2) = \underbrace{\frac{1}{(2\pi\sigma^2)^{n/2}} \exp\left(-\frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - \bar{y})^2\right)}_{L^*(\mu, \sigma^2; \bar{y}, s^2)}
\cdot \underbrace{\text{[independent of $\mu$, $\sigma^2$]}}_{c(y)}.
\]

This shows \(\bar{Y}\) and \(S^2\) are sufficient.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Usage of Sufficient Statistics}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Maximum Likelihood Estimation (MLE):} In MLE, sufficient statistics simplify the optimization problem by reducing the data without losing information.

  Example: In the normal distribution case, \(\mu\) can be estimated using the sufficient statistic \(\bar{Y}\): \[
  \hat{\mu}_{MLE} = \bar{Y}.
  \]
\item
  \textbf{Bayesian Inference:} In Bayesian analysis, the posterior distribution depends on the sufficient statistic rather than the entire data set. For the normal case: \[
  P(\mu \mid \bar{Y}) \propto P(\mu) L(\mu; \bar{Y}).
  \]
\item
  \textbf{Data Compression:} In practice, sufficient statistics reduce the complexity of data storage and analysis by condensing all relevant information into a smaller representation.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{nuisance-parameters}{%
\subsubsection{Nuisance Parameters}\label{nuisance-parameters}}

Parameters that are not of direct interest in the analysis but are necessary to model the data are called \textbf{nuisance parameters}.

\textbf{Profile Likelihood}: To handle nuisance parameters, replace them with their maximum likelihood estimates (MLEs) in the likelihood function, creating a \textbf{profile likelihood} for the parameter of interest.

\textbf{Example of Profile Likelihood}:

In a regression model with parameters \(\beta\) (coefficients) and \(\sigma^2\) (error variance), \(\sigma^2\) is often a nuisance parameter. The profile likelihood for \(\beta\) is obtained by substituting the MLE of \(\sigma^2\) into the likelihood:

\[
L_p(\beta) = L(\beta, \hat{\sigma}^2),
\]

where \(\hat{\sigma}^2\) is the MLE of \(\sigma^2\) given \(\beta\).

This simplifies the problem to focus only on the parameter of interest, \(\beta\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{parameter-transformations}{%
\subsection{Parameter Transformations}\label{parameter-transformations}}

Transformations of parameters are often used to improve interpretability or statistical properties of models.

\hypertarget{log-odds-transformation}{%
\subsubsection{Log-Odds Transformation}\label{log-odds-transformation}}

The \textbf{log-odds transformation} is commonly used in logistic regression and binary classification problems. It transforms probabilities (which are bounded between 0 and 1) to the real line:

\[
\text{Log odds} = g(\theta) = \ln\left(\frac{\theta}{1-\theta}\right),
\]

where \(\theta\) represents a probability (e.g., the success probability in a Bernoulli trial).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{general-parameter-transformations}{%
\subsubsection{General Parameter Transformations}\label{general-parameter-transformations}}

For a parameter \(\theta\) and a transformation \(g(\cdot)\):

\begin{itemize}
\tightlist
\item
  If \(\theta \in (a, b)\), \(g(\theta)\) may map \(\theta\) to a different range (e.g., \(\mathbb{R}\)).
\item
  Useful transformations include:

  \begin{itemize}
  \tightlist
  \item
    Logarithmic: \(g(\theta) = \ln(\theta)\) for \(\theta > 0\).
  \item
    Exponential: \(g(\theta) = e^{\theta}\) for unconstrained \(\theta\).
  \item
    Square root: \(g(\theta) = \sqrt{\theta}\) for \(\theta \geq 0\).
  \end{itemize}
\end{itemize}

\textbf{Jacobian Adjustment for Transformations}: If transforming a parameter in Bayesian inference, the Jacobian of the transformation must be included to ensure proper posterior scaling.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{applications-of-parameter-transformations}{%
\subsubsection{Applications of Parameter Transformations}\label{applications-of-parameter-transformations}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Improving Interpretability}:

  \begin{itemize}
  \tightlist
  \item
    Probabilities can be transformed to odds or log-odds for logistic models.
  \item
    Rates can be transformed logarithmically for multiplicative effects.
  \end{itemize}
\item
  \textbf{Statistical Modeling}:

  \begin{itemize}
  \tightlist
  \item
    Variance-stabilizing transformations (e.g., log for Poisson data or arcsine for proportions).
  \item
    Regularization or simplification of complex relationships.
  \end{itemize}
\item
  \textbf{Optimization}:

  \begin{itemize}
  \tightlist
  \item
    Transforming constrained parameters (e.g., probabilities or positive scales) to unconstrained scales simplifies optimization algorithms.
  \end{itemize}
\end{enumerate}

\hypertarget{data-importexport}{%
\section{Data Import/Export}\label{data-importexport}}

\href{https://cran.r-project.org/doc/manuals/r-release/R-data.html}{Extended Manual by R}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}@{}}
\caption{Table by \href{https://cran.r-project.org/web/packages/rio/vignettes/rio.html}{Rio Vignette}}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Format
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Typical Extension
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Import Package
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Export Package
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Installed by Default
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Format
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Typical Extension
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Import Package
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Export Package
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Installed by Default
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Comma-separated data & .csv & \href{https://cran.r-project.org/package=data.table}{\textbf{data.table}} & \href{https://cran.r-project.org/package=data.table}{\textbf{data.table}} & Yes \\
Pipe-separated data & .psv & \href{https://cran.r-project.org/package=data.table}{\textbf{data.table}} & \href{https://cran.r-project.org/package=data.table}{\textbf{data.table}} & Yes \\
Tab-separated data & .tsv & \href{https://cran.r-project.org/package=data.table}{\textbf{data.table}} & \href{https://cran.r-project.org/package=data.table}{\textbf{data.table}} & Yes \\
CSVY (CSV + YAML metadata header) & .csvy & \href{https://cran.r-project.org/package=data.table}{\textbf{data.table}} & \href{https://cran.r-project.org/package=data.table}{\textbf{data.table}} & Yes \\
SAS & .sas7bdat & \href{https://cran.r-project.org/package=haven}{\textbf{haven}} & \href{https://cran.r-project.org/package=haven}{\textbf{haven}} & Yes \\
SPSS & .sav & \href{https://cran.r-project.org/package=haven}{\textbf{haven}} & \href{https://cran.r-project.org/package=haven}{\textbf{haven}} & Yes \\
SPSS (compressed) & .zsav & \href{https://cran.r-project.org/package=haven}{\textbf{haven}} & \href{https://cran.r-project.org/package=haven}{\textbf{haven}} & Yes \\
Stata & .dta & \href{https://cran.r-project.org/package=haven}{\textbf{haven}} & \href{https://cran.r-project.org/package=haven}{\textbf{haven}} & Yes \\
SAS XPORT & .xpt & \href{https://cran.r-project.org/package=haven}{\textbf{haven}} & \href{https://cran.r-project.org/package=haven}{\textbf{haven}} & Yes \\
SPSS Portable & .por & \href{https://cran.r-project.org/package=haven}{\textbf{haven}} & & Yes \\
Excel & .xls & \href{https://cran.r-project.org/package=readxl}{\textbf{readxl}} & & Yes \\
Excel & .xlsx & \href{https://cran.r-project.org/package=readxl}{\textbf{readxl}} & \href{https://cran.r-project.org/package=openxlsx}{\textbf{openxlsx}} & Yes \\
R syntax & .R & \textbf{base} & \textbf{base} & Yes \\
Saved R objects & .RData, .rda & \textbf{base} & \textbf{base} & Yes \\
Serialized R objects & .rds & \textbf{base} & \textbf{base} & Yes \\
Epiinfo & .rec & \href{https://cran.r-project.org/package=foreign}{\textbf{foreign}} & & Yes \\
Minitab & .mtp & \href{https://cran.r-project.org/package=foreign}{\textbf{foreign}} & & Yes \\
Systat & .syd & \href{https://cran.r-project.org/package=foreign}{\textbf{foreign}} & & Yes \\
``XBASE'' database files & .dbf & \href{https://cran.r-project.org/package=foreign}{\textbf{foreign}} & \href{https://cran.r-project.org/package=foreign}{\textbf{foreign}} & Yes \\
Weka Attribute-Relation File Format & .arff & \href{https://cran.r-project.org/package=foreign}{\textbf{foreign}} & \href{https://cran.r-project.org/package=foreign}{\textbf{foreign}} & Yes \\
Data Interchange Format & .dif & \textbf{utils} & & Yes \\
Fortran data & no recognized extension & \textbf{utils} & & Yes \\
Fixed-width format data & .fwf & \textbf{utils} & \textbf{utils} & Yes \\
gzip comma-separated data & .csv.gz & \textbf{utils} & \textbf{utils} & Yes \\
Apache Arrow (Parquet) & .parquet & \href{https://cran.r-project.org/package=arrow}{\textbf{arrow}} & \href{https://cran.r-project.org/package=arrow}{\textbf{arrow}} & No \\
EViews & .wf1 & \href{https://cran.r-project.org/package=hexView}{\textbf{hexView}} & & No \\
Feather R/Python interchange format & .feather & \href{https://cran.r-project.org/package=feather}{\textbf{feather}} & \href{https://cran.r-project.org/package=feather}{\textbf{feather}} & No \\
Fast Storage & .fst & \href{https://cran.r-project.org/package=fst}{\textbf{fst}} & \href{https://cran.r-project.org/package=fst}{\textbf{fst}} & No \\
JSON & .json & \href{https://cran.r-project.org/package=jsonlite}{\textbf{jsonlite}} & \href{https://cran.r-project.org/package=jsonlite}{\textbf{jsonlite}} & No \\
Matlab & .mat & \href{https://cran.r-project.org/package=rmatio}{\textbf{rmatio}} & \href{https://cran.r-project.org/package=rmatio}{\textbf{rmatio}} & No \\
OpenDocument Spreadsheet & .ods & \href{https://cran.r-project.org/package=readODS}{\textbf{readODS}} & \href{https://cran.r-project.org/package=readODS}{\textbf{readODS}} & No \\
HTML Tables & .html & \href{https://cran.r-project.org/package=xml2}{\textbf{xml2}} & \href{https://cran.r-project.org/package=xml2}{\textbf{xml2}} & No \\
Shallow XML documents & .xml & \href{https://cran.r-project.org/package=xml2}{\textbf{xml2}} & \href{https://cran.r-project.org/package=xml2}{\textbf{xml2}} & No \\
YAML & .yml & \href{https://cran.r-project.org/package=yaml}{\textbf{yaml}} & \href{https://cran.r-project.org/package=yaml}{\textbf{yaml}} & No \\
Clipboard & default is tsv & \href{https://cran.r-project.org/package=clipr}{\textbf{clipr}} & \href{https://cran.r-project.org/package=clipr}{\textbf{clipr}} & No \\
\href{https://www.google.com/sheets/about/}{Google Sheets} & as Comma-separated data & & & \\
\end{longtable}

R limitations:

\begin{itemize}
\item
  By default, R use 1 core in CPU
\item
  R puts data into memory (limit around 2-4 GB), while SAS uses data from files on demand
\item
  Categorization

  \begin{itemize}
  \item
    Medium-size file: within RAM limit, around 1-2 GB
  \item
    Large file: 2-10 GB, there might be some workaround solution
  \item
    Very large file \textgreater{} 10 GB, you have to use distributed or parallel computing
  \end{itemize}
\end{itemize}

Solutions:

\begin{itemize}
\item
  buy more RAM
\item
  HPC packages

  \begin{itemize}
  \item
    Explicit Parallelism
  \item
    Implicit Parallelism
  \item
    Large Memory
  \item
    Map/Reduce
  \end{itemize}
\item
  specify number of rows and columns, typically including command \texttt{nrow\ =}
\item
  Use packages that store data differently

  \begin{itemize}
  \item
    \texttt{bigmemory}, \texttt{biganalytics}, \texttt{bigtabulate} , \texttt{synchronicity}, \texttt{bigalgebra}, \texttt{bigvideo} use C++ to store matrices, but also support one class type
  \item
    For multiple class types, use \texttt{ff} package
  \end{itemize}
\item
  Very Large datasets use

  \begin{itemize}
  \tightlist
  \item
    \texttt{RHaddop} package
  \item
    \texttt{HadoopStreaming}
  \item
    \texttt{Rhipe}
  \end{itemize}
\end{itemize}

\hypertarget{medium-size}{%
\subsection{Medium size}\label{medium-size}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"rio"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To import multiple files in a directory

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(}\FunctionTok{import\_list}\NormalTok{(}\FunctionTok{dir}\NormalTok{()), }\AttributeTok{which =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To export a single data file

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{export}\NormalTok{(data, }\StringTok{"data.csv"}\NormalTok{)}
\FunctionTok{export}\NormalTok{(data,}\StringTok{"data.dta"}\NormalTok{)}
\FunctionTok{export}\NormalTok{(data,}\StringTok{"data.txt"}\NormalTok{)}
\FunctionTok{export}\NormalTok{(data,}\StringTok{"data\_cyl.rds"}\NormalTok{)}
\FunctionTok{export}\NormalTok{(data,}\StringTok{"data.rdata"}\NormalTok{)}
\FunctionTok{export}\NormalTok{(data,}\StringTok{"data.R"}\NormalTok{)}
\FunctionTok{export}\NormalTok{(data,}\StringTok{"data.csv.zip"}\NormalTok{)}
\FunctionTok{export}\NormalTok{(data,}\StringTok{"list.json"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To export multiple data files

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{export}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{mtcars =}\NormalTok{ mtcars, }\AttributeTok{iris =}\NormalTok{ iris), }\StringTok{"data\_file\_type"}\NormalTok{) }
\CommentTok{\# where data\_file\_type should substituted with the extension listed above}
\end{Highlighting}
\end{Shaded}

To convert between data file types

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# convert Stata to SPSS}
\FunctionTok{convert}\NormalTok{(}\StringTok{"data.dta"}\NormalTok{, }\StringTok{"data.sav"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{large-size}{%
\subsection{Large size}\label{large-size}}

\hypertarget{cloud-computing-using-aws-for-big-data}{%
\subsubsection{Cloud Computing: Using AWS for Big Data}\label{cloud-computing-using-aws-for-big-data}}

Amazon Web Service (AWS): Compute resources can be rented at approximately \$1/hr. Use AWS to process large datasets without overwhelming your local machine.

\hypertarget{importing-large-files-as-chunks}{%
\subsubsection{Importing Large Files as Chunks}\label{importing-large-files-as-chunks}}

\hypertarget{using-base-r}{%
\paragraph{Using Base R}\label{using-base-r}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{file\_in }\OtherTok{\textless{}{-}} \FunctionTok{file}\NormalTok{(}\StringTok{"in.csv"}\NormalTok{, }\StringTok{"r"}\NormalTok{)  }\CommentTok{\# Open a connection to the file}
\NormalTok{chunk\_size }\OtherTok{\textless{}{-}} \DecValTok{100000}            \CommentTok{\# Define chunk size}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{readLines}\NormalTok{(file\_in, }\AttributeTok{n =}\NormalTok{ chunk\_size)  }\CommentTok{\# Read data in chunks}
\FunctionTok{close}\NormalTok{(file\_in)                  }\CommentTok{\# Close the file connection}
\end{Highlighting}
\end{Shaded}

\hypertarget{using-the-data.table-package}{%
\paragraph{\texorpdfstring{Using the \texttt{data.table} Package}{Using the data.table Package}}\label{using-the-data.table-package}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(data.table)}
\NormalTok{mydata }\OtherTok{\textless{}{-}} \FunctionTok{fread}\NormalTok{(}\StringTok{"in.csv"}\NormalTok{, }\AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{)  }\CommentTok{\# Fast and memory{-}efficient}
\end{Highlighting}
\end{Shaded}

\hypertarget{using-the-ff-package}{%
\paragraph{\texorpdfstring{Using the \texttt{ff} Package}{Using the ff Package}}\label{using-the-ff-package}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ff)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{read.csv.ffdf}\NormalTok{(}
  \AttributeTok{file =} \StringTok{"file.csv"}\NormalTok{,}
  \AttributeTok{nrow =} \DecValTok{10}\NormalTok{,          }\CommentTok{\# Total rows}
  \AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{,      }\CommentTok{\# Include headers}
  \AttributeTok{VERBOSE =} \ConstantTok{TRUE}\NormalTok{,     }\CommentTok{\# Display progress}
  \AttributeTok{first.rows =} \DecValTok{10000}\NormalTok{, }\CommentTok{\# Initial chunk}
  \AttributeTok{next.rows =} \DecValTok{50000}\NormalTok{,  }\CommentTok{\# Subsequent chunks}
  \AttributeTok{colClasses =} \ConstantTok{NA}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{using-the-bigmemory-package}{%
\paragraph{\texorpdfstring{Using the \texttt{bigmemory} Package}{Using the bigmemory Package}}\label{using-the-bigmemory-package}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(bigmemory)}
\NormalTok{my\_data }\OtherTok{\textless{}{-}} \FunctionTok{read.big.matrix}\NormalTok{(}\StringTok{\textquotesingle{}in.csv\textquotesingle{}}\NormalTok{, }\AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{using-the-sqldf-package}{%
\paragraph{\texorpdfstring{Using the \texttt{sqldf} Package}{Using the sqldf Package}}\label{using-the-sqldf-package}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(sqldf)}
\NormalTok{my\_data }\OtherTok{\textless{}{-}} \FunctionTok{read.csv.sql}\NormalTok{(}\StringTok{\textquotesingle{}in.csv\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Example: Filtering during import}
\NormalTok{iris2 }\OtherTok{\textless{}{-}} \FunctionTok{read.csv.sql}\NormalTok{(}\StringTok{"iris.csv"}\NormalTok{, }
    \AttributeTok{sql =} \StringTok{"SELECT * FROM file WHERE Species = \textquotesingle{}setosa\textquotesingle{}"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{using-the-rmysql-package}{%
\paragraph{\texorpdfstring{Using the \texttt{RMySQL} Package}{Using the RMySQL Package}}\label{using-the-rmysql-package}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(RMySQL)}
\end{Highlighting}
\end{Shaded}

\texttt{RQLite} package

\begin{itemize}
\tightlist
\item
  \href{https://sqlite.org/download.html}{Download SQLite}, pick ``A bundle of command-line tools for managing SQLite database files'' for Window 10
\item
  Unzip file, and open \texttt{sqlite3.exe.}
\item
  Type in the prompt

  \begin{itemize}
  \tightlist
  \item
    \texttt{sqlite\textgreater{}\ .cd\ \textquotesingle{}C:\textbackslash{}Users\textbackslash{}data\textquotesingle{}} specify path to your desired directory
  \item
    \texttt{sqlite\textgreater{}\ .open\ database\_name.db} to open a database
  \item
    To import the CSV file into the database

    \begin{itemize}
    \tightlist
    \item
      \texttt{sqlite\textgreater{}\ .mode\ csv} specify to SQLite that the next file is .csv file
    \item
      \texttt{sqlite\textgreater{}\ .import\ file\_name.csv\ datbase\_name} to import the csv file to the database
    \end{itemize}
  \item
    \texttt{sqlite\textgreater{}\ .exit} After you're done, exit the sqlite program
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(DBI)}
\FunctionTok{library}\NormalTok{(dplyr)}
\FunctionTok{library}\NormalTok{(}\StringTok{"RSQLite"}\NormalTok{)}
\FunctionTok{setwd}\NormalTok{(}\StringTok{""}\NormalTok{)}
\NormalTok{con }\OtherTok{\textless{}{-}} \FunctionTok{dbConnect}\NormalTok{(RSQLite}\SpecialCharTok{::}\FunctionTok{SQLite}\NormalTok{(), }\StringTok{"data\_base.db"}\NormalTok{)}
\NormalTok{tbl }\OtherTok{\textless{}{-}} \FunctionTok{tbl}\NormalTok{(con, }\StringTok{"data\_table"}\NormalTok{)}
\NormalTok{tbl }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{filter}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{select}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{collect}\NormalTok{() }\CommentTok{\# to actually pull the data into the workspace}
\FunctionTok{dbDisconnect}\NormalTok{(con)}
\end{Highlighting}
\end{Shaded}

\hypertarget{using-the-arrow-package}{%
\paragraph{\texorpdfstring{Using the \texttt{arrow} Package}{Using the arrow Package}}\label{using-the-arrow-package}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(arrow)}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv\_arrow}\NormalTok{(}\StringTok{"file.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{using-the-vroom-package}{%
\paragraph{\texorpdfstring{Using the \texttt{vroom} Package}{Using the vroom Package}}\label{using-the-vroom-package}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(vroom)}

\CommentTok{\# Import a compressed CSV file}
\NormalTok{compressed }\OtherTok{\textless{}{-}} \FunctionTok{vroom\_example}\NormalTok{(}\StringTok{"mtcars.csv.zip"}\NormalTok{)}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{vroom}\NormalTok{(compressed)}
\end{Highlighting}
\end{Shaded}

\hypertarget{using-the-data.table-package-1}{%
\paragraph{\texorpdfstring{Using the \texttt{data.table} Package}{Using the data.table Package}}\label{using-the-data.table-package-1}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{s }\OtherTok{=} \FunctionTok{fread}\NormalTok{(}\StringTok{"sample.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{comparisons-regarding-storage-space}{%
\paragraph{Comparisons Regarding Storage Space}\label{comparisons-regarding-storage-space}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test }\OtherTok{=}\NormalTok{ ff}\SpecialCharTok{::}\FunctionTok{read.csv.ffdf}\NormalTok{(}\AttributeTok{file =} \StringTok{""}\NormalTok{)}
\FunctionTok{object.size}\NormalTok{(test) }\CommentTok{\# Highest memory usage}

\NormalTok{test1 }\OtherTok{=}\NormalTok{ data.table}\SpecialCharTok{::}\FunctionTok{fread}\NormalTok{(}\AttributeTok{file =} \StringTok{""}\NormalTok{)}
\FunctionTok{object.size}\NormalTok{(test1) }\CommentTok{\# Lowest memory usage}

\NormalTok{test2 }\OtherTok{=}\NormalTok{ readr}\SpecialCharTok{::}\FunctionTok{read\_csv}\NormalTok{(}\AttributeTok{file =} \StringTok{""}\NormalTok{)}
\FunctionTok{object.size}\NormalTok{(test2) }\CommentTok{\# Second lowest memory usage}

\NormalTok{test3 }\OtherTok{=}\NormalTok{ vroom}\SpecialCharTok{::}\FunctionTok{vroom}\NormalTok{(}\AttributeTok{file =} \StringTok{""}\NormalTok{)}
\FunctionTok{object.size}\NormalTok{(test3) }\CommentTok{\# Similar to read\_csv}
\end{Highlighting}
\end{Shaded}

To work with large datasets, you can compress them into \texttt{csv.gz} format. However, typically, R requires loading the entire dataset before exporting it, which can be impractical for data over 10 GB. In such cases, processing the data sequentially becomes necessary. Although \texttt{read.csv} is slower compared to \texttt{readr::read\_csv}, it can handle connections and allows for sequential looping, making it useful for large files.

Currently, \texttt{readr::read\_csv} does not support the \texttt{skip} argument efficiently for large data. Even if you specify \texttt{skip}, the function reads all preceding lines again. For instance, if you run \texttt{read\_csv(file,\ n\_max\ =\ 100,\ skip\ =\ 0)} followed by \texttt{read\_csv(file,\ n\_max\ =\ 200,\ skip\ =\ 100)}, the first 100 rows are re-read. In contrast, \texttt{read.csv} can continue from where it left off without re-reading previous rows.

If you encounter an error such as:

``Error in (function (con, what, n = 1L, size = NA\_integer\_, signed = TRUE): can only read from a binary connection'',

you can modify the connection mode from \texttt{"r"} to \texttt{"rb"} (read binary). Although the \texttt{file} function is designed to detect the appropriate format automatically, this workaround can help resolve the issue when it does not behave as expected.

\hypertarget{sequential-processing-for-large-data}{%
\subsubsection{Sequential Processing for Large Data}\label{sequential-processing-for-large-data}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Open file for sequential reading}
\NormalTok{file\_conn }\OtherTok{\textless{}{-}} \FunctionTok{file}\NormalTok{(}\StringTok{"file.csv"}\NormalTok{, }\AttributeTok{open =} \StringTok{"r"}\NormalTok{)}
\ControlFlowTok{while}\NormalTok{ (}\ConstantTok{TRUE}\NormalTok{) \{}
  \CommentTok{\# Read a chunk of data}
\NormalTok{  data\_chunk }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(file\_conn, }\AttributeTok{nrows =} \DecValTok{1000}\NormalTok{)}
  \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{nrow}\NormalTok{(data\_chunk) }\SpecialCharTok{==} \DecValTok{0}\NormalTok{) }\ControlFlowTok{break}  \CommentTok{\# Stop if no more rows}
  \CommentTok{\# Process the chunk here}
\NormalTok{\}}
\FunctionTok{close}\NormalTok{(file\_conn)  }\CommentTok{\# Close connection}
\end{Highlighting}
\end{Shaded}

\hypertarget{data-manipulation}{%
\section{Data Manipulation}\label{data-manipulation}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load required packages}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(lubridate)}

\CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\# Data Structures in R}
\CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}

\CommentTok{\# Create vectors}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{23}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{45}\NormalTok{)}
\NormalTok{n }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{g }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"M"}\NormalTok{, }\StringTok{"M"}\NormalTok{, }\StringTok{"F"}\NormalTok{)}

\CommentTok{\# Create a data frame}
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(n, g)}
\NormalTok{df  }\CommentTok{\# View the data frame}
\CommentTok{\#\textgreater{}   n g}
\CommentTok{\#\textgreater{} 1 1 M}
\CommentTok{\#\textgreater{} 2 3 M}
\CommentTok{\#\textgreater{} 3 5 F}
\FunctionTok{str}\NormalTok{(df)  }\CommentTok{\# Check its structure}
\CommentTok{\#\textgreater{} \textquotesingle{}data.frame\textquotesingle{}:    3 obs. of  2 variables:}
\CommentTok{\#\textgreater{}  $ n: num  1 3 5}
\CommentTok{\#\textgreater{}  $ g: chr  "M" "M" "F"}

\CommentTok{\# Using tibble for cleaner outputs}
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(n, g)}
\NormalTok{df  }\CommentTok{\# View the tibble}
\CommentTok{\#\textgreater{} \# A tibble: 3 x 2}
\CommentTok{\#\textgreater{}       n g    }
\CommentTok{\#\textgreater{}   \textless{}dbl\textgreater{} \textless{}chr\textgreater{}}
\CommentTok{\#\textgreater{} 1     1 M    }
\CommentTok{\#\textgreater{} 2     3 M    }
\CommentTok{\#\textgreater{} 3     5 F}
\FunctionTok{str}\NormalTok{(df)}
\CommentTok{\#\textgreater{} tibble [3 x 2] (S3: tbl\_df/tbl/data.frame)}
\CommentTok{\#\textgreater{}  $ n: num [1:3] 1 3 5}
\CommentTok{\#\textgreater{}  $ g: chr [1:3] "M" "M" "F"}

\CommentTok{\# Create a list}
\NormalTok{lst }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(x, n, g, df)}
\NormalTok{lst  }\CommentTok{\# Display the list}
\CommentTok{\#\textgreater{} [[1]]}
\CommentTok{\#\textgreater{} [1]  1  4 23  4 45}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} [[2]]}
\CommentTok{\#\textgreater{} [1] 1 3 5}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} [[3]]}
\CommentTok{\#\textgreater{} [1] "M" "M" "F"}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} [[4]]}
\CommentTok{\#\textgreater{} \# A tibble: 3 x 2}
\CommentTok{\#\textgreater{}       n g    }
\CommentTok{\#\textgreater{}   \textless{}dbl\textgreater{} \textless{}chr\textgreater{}}
\CommentTok{\#\textgreater{} 1     1 M    }
\CommentTok{\#\textgreater{} 2     3 M    }
\CommentTok{\#\textgreater{} 3     5 F}

\CommentTok{\# Name list elements}
\NormalTok{lst2 }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{num =}\NormalTok{ x, }\AttributeTok{size =}\NormalTok{ n, }\AttributeTok{sex =}\NormalTok{ g, }\AttributeTok{data =}\NormalTok{ df)}
\NormalTok{lst2  }\CommentTok{\# Named list elements are easier to reference}
\CommentTok{\#\textgreater{} $num}
\CommentTok{\#\textgreater{} [1]  1  4 23  4 45}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $size}
\CommentTok{\#\textgreater{} [1] 1 3 5}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $sex}
\CommentTok{\#\textgreater{} [1] "M" "M" "F"}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $data}
\CommentTok{\#\textgreater{} \# A tibble: 3 x 2}
\CommentTok{\#\textgreater{}       n g    }
\CommentTok{\#\textgreater{}   \textless{}dbl\textgreater{} \textless{}chr\textgreater{}}
\CommentTok{\#\textgreater{} 1     1 M    }
\CommentTok{\#\textgreater{} 2     3 M    }
\CommentTok{\#\textgreater{} 3     5 F}

\CommentTok{\# Another list example with numeric vectors}
\NormalTok{lst3 }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{7}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{),}
  \AttributeTok{z =} \FunctionTok{c}\NormalTok{(}\DecValTok{22}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{)}
\NormalTok{lst3}
\CommentTok{\#\textgreater{} $x}
\CommentTok{\#\textgreater{} [1] 1 3 5 7}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $y}
\CommentTok{\#\textgreater{} [1] 2 2 2 4 5 5 5 6}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $z}
\CommentTok{\#\textgreater{} [1] 22  3  3  3  5 10}

\CommentTok{\# Find means of list elements}
\CommentTok{\# One at a time}
\FunctionTok{mean}\NormalTok{(lst3}\SpecialCharTok{$}\NormalTok{x)}
\CommentTok{\#\textgreater{} [1] 4}
\FunctionTok{mean}\NormalTok{(lst3}\SpecialCharTok{$}\NormalTok{y)}
\CommentTok{\#\textgreater{} [1] 3.875}
\FunctionTok{mean}\NormalTok{(lst3}\SpecialCharTok{$}\NormalTok{z)}
\CommentTok{\#\textgreater{} [1] 7.666667}

\CommentTok{\# Using lapply to calculate means}
\FunctionTok{lapply}\NormalTok{(lst3, mean)}
\CommentTok{\#\textgreater{} $x}
\CommentTok{\#\textgreater{} [1] 4}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $y}
\CommentTok{\#\textgreater{} [1] 3.875}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $z}
\CommentTok{\#\textgreater{} [1] 7.666667}

\CommentTok{\# Simplified output with sapply}
\FunctionTok{sapply}\NormalTok{(lst3, mean)}
\CommentTok{\#\textgreater{}        x        y        z }
\CommentTok{\#\textgreater{} 4.000000 3.875000 7.666667}

\CommentTok{\# Tidyverse alternative: map() function}
\FunctionTok{map}\NormalTok{(lst3, mean)}
\CommentTok{\#\textgreater{} $x}
\CommentTok{\#\textgreater{} [1] 4}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $y}
\CommentTok{\#\textgreater{} [1] 3.875}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $z}
\CommentTok{\#\textgreater{} [1] 7.666667}

\CommentTok{\# Tidyverse with numeric output: map\_dbl()}
\FunctionTok{map\_dbl}\NormalTok{(lst3, mean)}
\CommentTok{\#\textgreater{}        x        y        z }
\CommentTok{\#\textgreater{} 4.000000 3.875000 7.666667}

\CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\# Binding Data Frames}
\CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}

\CommentTok{\# Create tibbles for demonstration}
\NormalTok{dat01 }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{x =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{, }\AttributeTok{y =} \DecValTok{5}\SpecialCharTok{:}\DecValTok{1}\NormalTok{)}
\NormalTok{dat02 }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{x =} \DecValTok{10}\SpecialCharTok{:}\DecValTok{16}\NormalTok{, }\AttributeTok{y =}\NormalTok{ x }\SpecialCharTok{/} \DecValTok{2}\NormalTok{)}
\NormalTok{dat03 }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{z =} \FunctionTok{runif}\NormalTok{(}\DecValTok{5}\NormalTok{))  }\CommentTok{\# 5 random numbers from (0, 1)}

\CommentTok{\# Row binding}
\FunctionTok{bind\_rows}\NormalTok{(dat01, dat02, dat01)}
\CommentTok{\#\textgreater{} \# A tibble: 17 x 2}
\CommentTok{\#\textgreater{}        x     y}
\CommentTok{\#\textgreater{}    \textless{}int\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1     1   5  }
\CommentTok{\#\textgreater{}  2     2   4  }
\CommentTok{\#\textgreater{}  3     3   3  }
\CommentTok{\#\textgreater{}  4     4   2  }
\CommentTok{\#\textgreater{}  5     5   1  }
\CommentTok{\#\textgreater{}  6    10   5  }
\CommentTok{\#\textgreater{}  7    11   5.5}
\CommentTok{\#\textgreater{}  8    12   6  }
\CommentTok{\#\textgreater{}  9    13   6.5}
\CommentTok{\#\textgreater{} 10    14   7  }
\CommentTok{\#\textgreater{} 11    15   7.5}
\CommentTok{\#\textgreater{} 12    16   8  }
\CommentTok{\#\textgreater{} 13     1   5  }
\CommentTok{\#\textgreater{} 14     2   4  }
\CommentTok{\#\textgreater{} 15     3   3  }
\CommentTok{\#\textgreater{} 16     4   2  }
\CommentTok{\#\textgreater{} 17     5   1}

\CommentTok{\# Add a new identifier column with .id}
\FunctionTok{bind\_rows}\NormalTok{(dat01, dat02, }\AttributeTok{.id =} \StringTok{"id"}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 12 x 3}
\CommentTok{\#\textgreater{}    id        x     y}
\CommentTok{\#\textgreater{}    \textless{}chr\textgreater{} \textless{}int\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1 1         1   5  }
\CommentTok{\#\textgreater{}  2 1         2   4  }
\CommentTok{\#\textgreater{}  3 1         3   3  }
\CommentTok{\#\textgreater{}  4 1         4   2  }
\CommentTok{\#\textgreater{}  5 1         5   1  }
\CommentTok{\#\textgreater{}  6 2        10   5  }
\CommentTok{\#\textgreater{}  7 2        11   5.5}
\CommentTok{\#\textgreater{}  8 2        12   6  }
\CommentTok{\#\textgreater{}  9 2        13   6.5}
\CommentTok{\#\textgreater{} 10 2        14   7  }
\CommentTok{\#\textgreater{} 11 2        15   7.5}
\CommentTok{\#\textgreater{} 12 2        16   8}

\CommentTok{\# Use named inputs for better identification}
\FunctionTok{bind\_rows}\NormalTok{(}\StringTok{"dat01"} \OtherTok{=}\NormalTok{ dat01, }\StringTok{"dat02"} \OtherTok{=}\NormalTok{ dat02, }\AttributeTok{.id =} \StringTok{"id"}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 12 x 3}
\CommentTok{\#\textgreater{}    id        x     y}
\CommentTok{\#\textgreater{}    \textless{}chr\textgreater{} \textless{}int\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1 dat01     1   5  }
\CommentTok{\#\textgreater{}  2 dat01     2   4  }
\CommentTok{\#\textgreater{}  3 dat01     3   3  }
\CommentTok{\#\textgreater{}  4 dat01     4   2  }
\CommentTok{\#\textgreater{}  5 dat01     5   1  }
\CommentTok{\#\textgreater{}  6 dat02    10   5  }
\CommentTok{\#\textgreater{}  7 dat02    11   5.5}
\CommentTok{\#\textgreater{}  8 dat02    12   6  }
\CommentTok{\#\textgreater{}  9 dat02    13   6.5}
\CommentTok{\#\textgreater{} 10 dat02    14   7  }
\CommentTok{\#\textgreater{} 11 dat02    15   7.5}
\CommentTok{\#\textgreater{} 12 dat02    16   8}

\CommentTok{\# Bind a list of data frames}
\NormalTok{list01 }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\StringTok{"dat01"} \OtherTok{=}\NormalTok{ dat01, }\StringTok{"dat02"} \OtherTok{=}\NormalTok{ dat02)}
\FunctionTok{bind\_rows}\NormalTok{(list01, }\AttributeTok{.id =} \StringTok{"source"}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 12 x 3}
\CommentTok{\#\textgreater{}    source     x     y}
\CommentTok{\#\textgreater{}    \textless{}chr\textgreater{}  \textless{}int\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1 dat01      1   5  }
\CommentTok{\#\textgreater{}  2 dat01      2   4  }
\CommentTok{\#\textgreater{}  3 dat01      3   3  }
\CommentTok{\#\textgreater{}  4 dat01      4   2  }
\CommentTok{\#\textgreater{}  5 dat01      5   1  }
\CommentTok{\#\textgreater{}  6 dat02     10   5  }
\CommentTok{\#\textgreater{}  7 dat02     11   5.5}
\CommentTok{\#\textgreater{}  8 dat02     12   6  }
\CommentTok{\#\textgreater{}  9 dat02     13   6.5}
\CommentTok{\#\textgreater{} 10 dat02     14   7  }
\CommentTok{\#\textgreater{} 11 dat02     15   7.5}
\CommentTok{\#\textgreater{} 12 dat02     16   8}

\CommentTok{\# Column binding}
\FunctionTok{bind\_cols}\NormalTok{(dat01, dat03)}
\CommentTok{\#\textgreater{} \# A tibble: 5 x 3}
\CommentTok{\#\textgreater{}       x     y      z}
\CommentTok{\#\textgreater{}   \textless{}int\textgreater{} \textless{}int\textgreater{}  \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1     1     5 0.508 }
\CommentTok{\#\textgreater{} 2     2     4 0.824 }
\CommentTok{\#\textgreater{} 3     3     3 0.0374}
\CommentTok{\#\textgreater{} 4     4     2 0.577 }
\CommentTok{\#\textgreater{} 5     5     1 0.988}

\CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\# String Manipulation}
\CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}

\NormalTok{names }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Ford, MS"}\NormalTok{, }\StringTok{"Jones, PhD"}\NormalTok{, }\StringTok{"Martin, Phd"}\NormalTok{, }\StringTok{"Huck, MA, MLS"}\NormalTok{)}

\CommentTok{\# Remove everything after the first comma}
\FunctionTok{str\_remove}\NormalTok{(names, }\AttributeTok{pattern =} \StringTok{", [[:print:]]+"}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] "Ford"   "Jones"  "Martin" "Huck"}

\CommentTok{\# Explanation: [[:print:]]+ matches one or more printable characters}

\CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\# Reshaping Data}
\CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}

\CommentTok{\# Wide format data}
\NormalTok{wide }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{name =} \FunctionTok{c}\NormalTok{(}\StringTok{"Clay"}\NormalTok{, }\StringTok{"Garrett"}\NormalTok{, }\StringTok{"Addison"}\NormalTok{),}
  \AttributeTok{test1 =} \FunctionTok{c}\NormalTok{(}\DecValTok{78}\NormalTok{, }\DecValTok{93}\NormalTok{, }\DecValTok{90}\NormalTok{),}
  \AttributeTok{test2 =} \FunctionTok{c}\NormalTok{(}\DecValTok{87}\NormalTok{, }\DecValTok{91}\NormalTok{, }\DecValTok{97}\NormalTok{),}
  \AttributeTok{test3 =} \FunctionTok{c}\NormalTok{(}\DecValTok{88}\NormalTok{, }\DecValTok{99}\NormalTok{, }\DecValTok{91}\NormalTok{)}
\NormalTok{)}

\CommentTok{\# Long format data}
\NormalTok{long }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{name =} \FunctionTok{rep}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"Clay"}\NormalTok{, }\StringTok{"Garrett"}\NormalTok{, }\StringTok{"Addison"}\NormalTok{), }\AttributeTok{each =} \DecValTok{3}\NormalTok{),}
  \AttributeTok{test =} \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{),}
  \AttributeTok{score =} \FunctionTok{c}\NormalTok{(}\DecValTok{78}\NormalTok{, }\DecValTok{87}\NormalTok{, }\DecValTok{88}\NormalTok{, }\DecValTok{93}\NormalTok{, }\DecValTok{91}\NormalTok{, }\DecValTok{99}\NormalTok{, }\DecValTok{90}\NormalTok{, }\DecValTok{97}\NormalTok{, }\DecValTok{91}\NormalTok{)}
\NormalTok{)}

\CommentTok{\# Summary statistics}
\FunctionTok{aggregate}\NormalTok{(score }\SpecialCharTok{\textasciitilde{}}\NormalTok{ name, }\AttributeTok{data =}\NormalTok{ long, mean)  }\CommentTok{\# Mean score per student}
\CommentTok{\#\textgreater{}      name    score}
\CommentTok{\#\textgreater{} 1 Addison 92.66667}
\CommentTok{\#\textgreater{} 2    Clay 84.33333}
\CommentTok{\#\textgreater{} 3 Garrett 94.33333}
\FunctionTok{aggregate}\NormalTok{(score }\SpecialCharTok{\textasciitilde{}}\NormalTok{ test, }\AttributeTok{data =}\NormalTok{ long, mean)  }\CommentTok{\# Mean score per test}
\CommentTok{\#\textgreater{}   test    score}
\CommentTok{\#\textgreater{} 1    1 87.00000}
\CommentTok{\#\textgreater{} 2    2 91.66667}
\CommentTok{\#\textgreater{} 3    3 92.66667}

\CommentTok{\# Line plot of scores over tests}
\FunctionTok{ggplot}\NormalTok{(long,}
       \FunctionTok{aes}\NormalTok{(}
           \AttributeTok{x =} \FunctionTok{factor}\NormalTok{(test),}
           \AttributeTok{y =}\NormalTok{ score,}
           \AttributeTok{color =}\NormalTok{ name,}
           \AttributeTok{group =}\NormalTok{ name}
\NormalTok{       )) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{xlab}\NormalTok{(}\StringTok{"Test"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Test Scores by Student"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{02.3-data-mani_files/figure-latex/unnamed-chunk-18-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Reshape wide to long}
\FunctionTok{pivot\_longer}\NormalTok{(wide, test1}\SpecialCharTok{:}\NormalTok{test3, }\AttributeTok{names\_to =} \StringTok{"test"}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{"score"}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 9 x 3}
\CommentTok{\#\textgreater{}   name    test  score}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}   \textless{}chr\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Clay    test1    78}
\CommentTok{\#\textgreater{} 2 Clay    test2    87}
\CommentTok{\#\textgreater{} 3 Clay    test3    88}
\CommentTok{\#\textgreater{} 4 Garrett test1    93}
\CommentTok{\#\textgreater{} 5 Garrett test2    91}
\CommentTok{\#\textgreater{} 6 Garrett test3    99}
\CommentTok{\#\textgreater{} 7 Addison test1    90}
\CommentTok{\#\textgreater{} 8 Addison test2    97}
\CommentTok{\#\textgreater{} 9 Addison test3    91}

\CommentTok{\# Use names\_prefix to clean column names}
\FunctionTok{pivot\_longer}\NormalTok{(}
\NormalTok{    wide,}
    \SpecialCharTok{{-}}\NormalTok{name,}
    \AttributeTok{names\_to =} \StringTok{"test"}\NormalTok{,}
    \AttributeTok{values\_to =} \StringTok{"score"}\NormalTok{,}
    \AttributeTok{names\_prefix =} \StringTok{"test"}
\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 9 x 3}
\CommentTok{\#\textgreater{}   name    test  score}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}   \textless{}chr\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Clay    1        78}
\CommentTok{\#\textgreater{} 2 Clay    2        87}
\CommentTok{\#\textgreater{} 3 Clay    3        88}
\CommentTok{\#\textgreater{} 4 Garrett 1        93}
\CommentTok{\#\textgreater{} 5 Garrett 2        91}
\CommentTok{\#\textgreater{} 6 Garrett 3        99}
\CommentTok{\#\textgreater{} 7 Addison 1        90}
\CommentTok{\#\textgreater{} 8 Addison 2        97}
\CommentTok{\#\textgreater{} 9 Addison 3        91}

\CommentTok{\# Reshape long to wide with explicit id\_cols argument}
\FunctionTok{pivot\_wider}\NormalTok{(}
\NormalTok{  long,}
  \AttributeTok{id\_cols =}\NormalTok{ name, }
  \AttributeTok{names\_from =}\NormalTok{ test,}
  \AttributeTok{values\_from =}\NormalTok{ score}
\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 3 x 4}
\CommentTok{\#\textgreater{}   name      \textasciigrave{}1\textasciigrave{}   \textasciigrave{}2\textasciigrave{}   \textasciigrave{}3\textasciigrave{}}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}   \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Clay       78    87    88}
\CommentTok{\#\textgreater{} 2 Garrett    93    91    99}
\CommentTok{\#\textgreater{} 3 Addison    90    97    91}

\CommentTok{\# Add a prefix to the resulting columns}
\FunctionTok{pivot\_wider}\NormalTok{(}
\NormalTok{  long,}
  \AttributeTok{id\_cols =}\NormalTok{ name,  }
  \AttributeTok{names\_from =}\NormalTok{ test,}
  \AttributeTok{values\_from =}\NormalTok{ score,}
  \AttributeTok{names\_prefix =} \StringTok{"test"}
\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 3 x 4}
\CommentTok{\#\textgreater{}   name    test1 test2 test3}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}   \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Clay       78    87    88}
\CommentTok{\#\textgreater{} 2 Garrett    93    91    99}
\CommentTok{\#\textgreater{} 3 Addison    90    97    91}
\end{Highlighting}
\end{Shaded}

The verbs of data manipulation

\begin{itemize}
\tightlist
\item
  \texttt{select}: selecting (or not selecting) columns based on their names (eg: select columns Q1 through Q25)
\item
  \texttt{slice}: selecting (or not selecting) rows based on their position (eg: select rows 1:10)
\item
  \texttt{mutate}: add or derive new columns (or variables) based on existing columns (eg: create a new column that expresses measurement in cm based on existing measure in inches)
\item
  \texttt{rename}: rename variables or change column names (eg: change ``GraduationRate100'' to ``grad100'')
\item
  \texttt{filter}: selecting rows based on a condition (eg: all rows where gender = Male)
\item
  \texttt{arrange}: ordering rows based on variable(s) numeric or alphabetical order (eg: sort in descending order of Income)
\item
  \texttt{sample}: take random samples of data (eg: sample 80\% of data to create a ``training'' set)
\item
  \texttt{summarize}: condense or aggregate multiple values into single summary values (eg: calculate median income by age group)
\item
  \texttt{group\_by}: convert a tbl into a grouped tbl so that operations are performed ``by group''; allows us to summarize data or apply verbs to data by groups (eg, by gender or treatment)
\item
  the pipe: \texttt{\%\textgreater{}\%}

  \begin{itemize}
  \item
    Use Ctrl + Shift + M (Win) or Cmd + Shift + M (Mac) to enter in RStudio
  \item
    The pipe takes the output of a function and ``pipes'' into the first argument of the next function.
  \item
    new pipe is \texttt{\textbar{}\textgreater{}} It should be identical to the old one, except for certain special cases.
  \end{itemize}
\item
  \texttt{:=} (Walrus operator): similar to \texttt{=} , but for cases where you want to use the \texttt{glue} package (i.e., dynamic changes in the variable name in the left-hand side)
\end{itemize}

Writing function in R

Tunneling

\texttt{\{\{} (called curly-curly) allows you to tunnel data-variables through arg-variables (i.e., function arguments)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\# Writing Functions with \{\{ \}\}}
\CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}

\CommentTok{\# Define a custom function using \{\{ \}\}}
\NormalTok{get\_mean }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, group\_var, var\_to\_mean) \{}
\NormalTok{  data }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{group\_by}\NormalTok{(\{\{group\_var\}\}) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{summarize}\NormalTok{(}\AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(\{\{var\_to\_mean\}\}, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{))}
\NormalTok{\}}

\CommentTok{\# Apply the function}
\FunctionTok{data}\NormalTok{(}\StringTok{"mtcars"}\NormalTok{)}
\NormalTok{mtcars }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{get\_mean}\NormalTok{(}\AttributeTok{group\_var =}\NormalTok{ cyl, }\AttributeTok{var\_to\_mean =}\NormalTok{ mpg)}
\CommentTok{\#\textgreater{} \# A tibble: 3 x 2}
\CommentTok{\#\textgreater{}     cyl  mean}
\CommentTok{\#\textgreater{}   \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1     4  26.7}
\CommentTok{\#\textgreater{} 2     6  19.7}
\CommentTok{\#\textgreater{} 3     8  15.1}

\CommentTok{\# Dynamically name the resulting variable}
\NormalTok{get\_mean }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, group\_var, var\_to\_mean, }\AttributeTok{prefix =} \StringTok{"mean\_of"}\NormalTok{) \{}
\NormalTok{  data }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{group\_by}\NormalTok{(\{\{group\_var\}\}) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{summarize}\NormalTok{(}\StringTok{"\{prefix\}\_\{\{var\_to\_mean\}\}"} \SpecialCharTok{:=} \FunctionTok{mean}\NormalTok{(\{\{var\_to\_mean\}\}, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{))}
\NormalTok{\}}

\CommentTok{\# Apply the modified function}
\NormalTok{mtcars }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{get\_mean}\NormalTok{(}\AttributeTok{group\_var =}\NormalTok{ cyl, }\AttributeTok{var\_to\_mean =}\NormalTok{ mpg)}
\CommentTok{\#\textgreater{} \# A tibble: 3 x 2}
\CommentTok{\#\textgreater{}     cyl mean\_of\_mpg}
\CommentTok{\#\textgreater{}   \textless{}dbl\textgreater{}       \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1     4        26.7}
\CommentTok{\#\textgreater{} 2     6        19.7}
\CommentTok{\#\textgreater{} 3     8        15.1}
\end{Highlighting}
\end{Shaded}

\hypertarget{part-i.-basic}{%
\part*{I. BASIC}\label{part-i.-basic}}
\addcontentsline{toc}{part}{I. BASIC}

\hypertarget{descriptive-statistics}{%
\chapter{Descriptive Statistics}\label{descriptive-statistics}}

When you have an area of interest to research, a problem to solve, or a relationship to investigate, theoretical and empirical processes will help you.

\textbf{Estimand}: Defined as ``a quantity of scientific interest that can be calculated in the population and does not change its value depending on the data collection design used to measure it (i.e., it does not vary with sample size, survey design, the number of non-respondents, or follow-up efforts).'' \citep{Rubin_1996}

Examples of estimands include:

\begin{itemize}
\tightlist
\item
  Population means
\item
  Population variances
\item
  Correlations
\item
  Factor loadings
\item
  Regression coefficients
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{numerical-measures}{%
\section{Numerical Measures}\label{numerical-measures}}

There are differences between a population and a sample:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.0924}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2651}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3855}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2490}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Measures of}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Category}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Population}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Sample}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
& What is it? & Reality & A small fraction of reality (inference) \\
& Characteristics described by & Parameters & Statistics \\
\textbf{Central Tendency} & Mean & \(\mu = E(Y)\) & \(\hat{\mu} = \overline{y}\) \\
\textbf{Central Tendency} & Median & 50th percentile & \(y_{(\frac{n+1}{2})}\) \\
\textbf{Dispersion} & Variance & \(\sigma^2 = var(Y) = E[(Y-\mu)^2]\) & \(s^2 = \frac{1}{n-1} \sum_{i=1}^{n} (y_i - \overline{y})^2\) \\
\textbf{Dispersion} & Coefficient of Variation & \(\frac{\sigma}{\mu}\) & \(\frac{s}{\overline{y}}\) \\
\textbf{Dispersion} & Interquartile Range & Difference between 25th and 75th percentiles; robust to outliers & \\
\textbf{Shape} & Skewness

Standardized 3rd central moment (unitless) & \(g_1 = \frac{\mu_3}{\sigma^3}\) & \(\hat{g_1} = \frac{m_3}{m_2^{3/2}}\) \\
\textbf{Shape} & Central moments & \(\mu=E(Y)\), \(\mu_2 = \sigma^2 = E[(Y-\mu)^2]\), \(\mu_3 = E[(Y-\mu)^3]\), \(\mu_4 = E[(Y-\mu)^4]\) & \(m_2 = \frac{1}{n} \sum_{i=1}^{n} (y_i - \overline{y})^2\)

\(m_3 = \frac{1}{n} \sum_{i=1}^{n} (y_i - \overline{y})^3\) \\
\textbf{Shape} & Kurtosis

(peakedness and tail thickness) Standardized 4th central moment & \(g_2^* = \frac{E[(Y-\mu)^4]}{\sigma^4}\) & \(\hat{g_2} = \frac{m_4}{m_2^2} - 3\) \\
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Notes}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Order Statistics}: \(y_{(1)}, y_{(2)}, \ldots, y_{(n)}\), where \(y_{(1)} < y_{(2)} < \ldots < y_{(n)}\).
\item
  \textbf{Coefficient of Variation}:

  \begin{itemize}
  \tightlist
  \item
    Defined as the standard deviation divided by the mean.
  \item
    A stable, unitless statistic useful for comparison.
  \end{itemize}
\item
  \textbf{Symmetry}:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Symmetric distributions}: Mean = Median; Skewness = 0.
  \item
    \textbf{Skewed Right}: Mean \textgreater{} Median; Skewness \textgreater{} 0.
  \item
    \textbf{Skewed Left}: Mean \textless{} Median; Skewness \textless{} 0.
  \end{itemize}
\item
  \textbf{Central Moments}:

  \begin{itemize}
  \tightlist
  \item
    \(\mu = E(Y)\)
  \item
    \(\mu_2 = \sigma^2 = E[(Y-\mu)^2]\)
  \item
    \(\mu_3 = E[(Y-\mu)^3]\)
  \item
    \(\mu_4 = E[(Y-\mu)^4]\)
  \end{itemize}
\end{enumerate}

\textbf{Skewness (}\(\hat{g_1}\)\textbf{)}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Sampling Distribution}:\\
  For samples drawn from a normal population:

  \begin{itemize}
  \tightlist
  \item
    \(\hat{g_1}\) is approximately distributed as \(N(0, \frac{6}{n})\) when \(n > 150\).
  \end{itemize}
\item
  \textbf{Inference}:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Large Samples}: Inference on skewness can be based on the standard normal distribution.\\
    The 95\% confidence interval for \(g_1\) is given by: \[
    \hat{g_1} \pm 1.96 \sqrt{\frac{6}{n}}
    \]
  \item
    \textbf{Small Samples}: For small samples, consult special tables such as:

    \begin{itemize}
    \tightlist
    \item
      \citet{snedecor1989statistical}, Table A 19(i)
    \item
      Monte Carlo test results
    \end{itemize}
  \end{itemize}
\end{enumerate}

\textbf{Kurtosis (}\(\hat{g_2}\)\textbf{)}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Definitions and Relationships}:

  \begin{itemize}
  \tightlist
  \item
    A normal distribution has kurtosis \(g_2^* = 3\).\\
    Kurtosis is often redefined as: \[
    g_2 = \frac{E[(Y - \mu)^4]}{\sigma^4} - 3
    \] where the 4th central moment is estimated by: \[
    m_4 = \frac{\sum_{i=1}^n (y_i - \overline{y})^4}{n}
    \]
  \end{itemize}
\item
  \textbf{Sampling Distribution}:\\
  For large samples (\(n > 1000\)):

  \begin{itemize}
  \tightlist
  \item
    \(\hat{g_2}\) is approximately distributed as \(N(0, \frac{24}{n})\).
  \end{itemize}
\item
  \textbf{Inference}:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Large Samples}: Inference for kurtosis can use standard normal tables.
  \item
    \textbf{Small Samples}: Refer to specialized tables such as:

    \begin{itemize}
    \tightlist
    \item
      \citet{snedecor1989statistical}, Table A 19(ii)
    \item
      \citet{geary1936moments}
    \end{itemize}
  \end{itemize}
\end{enumerate}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2600}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5300}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Kurtosis Value}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Tail Behavior}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Comparison to Normal Distribution}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(g_2 > 0\) (Leptokurtic) & Heavier Tails & Examples: \(t\)-distributions \\
\(g_2 < 0\) (Platykurtic) & Lighter Tails & Examples: Uniform or certain bounded distributions \\
\(g_2 = 0\) (Mesokurtic) & Normal Tails & Exactly matches the normal distribution \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Generate random data from a normal distribution}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{)}

\CommentTok{\# Load the e1071 package for skewness and kurtosis functions}
\FunctionTok{library}\NormalTok{(e1071)}

\CommentTok{\# Calculate skewness}
\NormalTok{skewness\_value }\OtherTok{\textless{}{-}} \FunctionTok{skewness}\NormalTok{(data)}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Skewness:"}\NormalTok{, skewness\_value, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Skewness: 0.1003872}

\CommentTok{\# Calculate kurtosis}
\NormalTok{kurtosis\_value }\OtherTok{\textless{}{-}} \FunctionTok{kurtosis}\NormalTok{(data)}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Kurtosis:"}\NormalTok{, kurtosis\_value, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Kurtosis: {-}0.5484067}
\end{Highlighting}
\end{Shaded}

\hypertarget{graphical-measures}{%
\section{Graphical Measures}\label{graphical-measures}}

The following table summarizes key graphical measures along with guidance on when and why to use each. More detailed explanations, visual examples, and sample code will be discussed after this table.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1366}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4439}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4146}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Graph Type}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{When to Use}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Why It\textquotesingle s Useful}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Histogram} & - Exploring the distribution (shape, center, spread) of a single continuous variable & \begin{minipage}[t]{\linewidth}\raggedright
- Quickly identifies frequency, modes, skewness, and potential outliers\\
- Provides an overview of data "shape"\strut
\end{minipage} \\
\textbf{Box-and-Whisker Plot} & \begin{minipage}[t]{\linewidth}\raggedright
- Comparing the same continuous variable across multiple categories\\
- Identifying median, IQR, and outliers\strut
\end{minipage} & \begin{minipage}[t]{\linewidth}\raggedright
- Shows distribution at a glance (median, quartiles)\\
- Highlights outliers and potential group differences\strut
\end{minipage} \\
\textbf{Stem-and-Leaf Plot} & - Small, single-variable datasets where you want a textual yet visual distribution view & \begin{minipage}[t]{\linewidth}\raggedright
- Reveals the distribution while preserving actual data values\\
- Easy to spot clusters and gaps for small datasets\strut
\end{minipage} \\
\textbf{Notched Boxplot} & - Similar to a standard boxplot but with confidence intervals around the median & \begin{minipage}[t]{\linewidth}\raggedright
- If notches don\textquotesingle t overlap, it suggests the medians differ significantly\\
- Helps clarify whether differences in medians are likely meaningful\strut
\end{minipage} \\
\textbf{Bagplot (2D Boxplot)} & \begin{minipage}[t]{\linewidth}\raggedright
- Bivariate data where you want a 2D "boxplot"-style overview\\
- Identifying outliers in two-dimensional space\strut
\end{minipage} & \begin{minipage}[t]{\linewidth}\raggedright
- Depicts both central region ("bag") and potential outliers\\
- Ideal for discovering clusters or unusual points in two continuous variables\strut
\end{minipage} \\
\textbf{Boxplot Matrix} & - Multiple continuous variables that you want to compare side-by-side & \begin{minipage}[t]{\linewidth}\raggedright
- Quickly compares distributions of many variables simultaneously\\
- Helpful for spotting differences in median, spread, and outliers\strut
\end{minipage} \\
\textbf{Violin Plot} & - Same use case as boxplot but you want \emph{more detail} on the distribution\textquotesingle s shape & \begin{minipage}[t]{\linewidth}\raggedright
- Combines boxplot features with a density plot\\
- Shows where data are concentrated or sparse within each category\strut
\end{minipage} \\
\textbf{Scatterplot} & - Two continuous variables to check for relationships, trends, or outliers & \begin{minipage}[t]{\linewidth}\raggedright
- Visualizes correlation or non-linear patterns\\
- Aids in identifying clusters or extreme values\strut
\end{minipage} \\
\textbf{Pairwise Scatterplots} & - Initial exploration of \emph{several} variables at once & \begin{minipage}[t]{\linewidth}\raggedright
- Enables a quick scan of relationships between \emph{all} variable pairs\\
- Useful for identifying multivariate patterns or potential correlation structures\strut
\end{minipage} \\
\end{longtable}

\textbf{Tips for Selecting the Right Plot:}

\begin{itemize}
\item
  \textbf{Focus on Your Question}: Are you comparing groups, investigating correlations, or just exploring the overall shape of the data?
\item
  \textbf{Match the Plot to Your Data Type}: Continuous vs.~categorical data often dictates your choice of chart.
\item
  \textbf{Mind the Data Size}: Some plots become cluttered or lose clarity with very large datasets (e.g., stem-and-leaf), while others may be less informative with very few data points.
\end{itemize}

\hypertarget{shape}{%
\subsection{Shape}\label{shape}}

Properly labeling your graphs is essential to ensure that viewers can easily understand the data presented. Below are several examples of graphical measures used to assess the shape of a dataset.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Generate random data for demonstration purposes}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{)}

\CommentTok{\# Histogram: A graphical representation of the distribution of a dataset.}
\FunctionTok{hist}\NormalTok{(}
\NormalTok{    data,}
    \AttributeTok{labels =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{col =} \StringTok{"grey"}\NormalTok{,}
    \AttributeTok{breaks =} \DecValTok{12}\NormalTok{,}
    \AttributeTok{main =} \StringTok{"Histogram of Random Data"}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"Value"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"Frequency"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{03-descriptive-stat_files/figure-latex/unnamed-chunk-2-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Interactive Histogram: Using \textquotesingle{}highcharter\textquotesingle{} for a more interactive visualization.}
\CommentTok{\# pacman::p\_load("highcharter")}
\CommentTok{\# hchart(data, type = "column", name = "Random Data Distribution")}

\CommentTok{\# Box{-}and{-}Whisker Plot: Useful for visualizing the distribution and identifying outliers.}
\FunctionTok{boxplot}\NormalTok{(}
\NormalTok{    count }\SpecialCharTok{\textasciitilde{}}\NormalTok{ spray,}
    \AttributeTok{data =}\NormalTok{ InsectSprays,}
    \AttributeTok{col =} \StringTok{"lightgray"}\NormalTok{,}
    \AttributeTok{main =} \StringTok{"Boxplot of Insect Sprays"}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"Spray Type"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"Count"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{03-descriptive-stat_files/figure-latex/unnamed-chunk-2-2} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Notched Boxplot: The notches indicate a confidence interval around the median.}
\FunctionTok{boxplot}\NormalTok{(}
\NormalTok{    len }\SpecialCharTok{\textasciitilde{}}\NormalTok{ supp }\SpecialCharTok{*}\NormalTok{ dose,}
    \AttributeTok{data =}\NormalTok{ ToothGrowth,}
    \AttributeTok{notch =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"gold"}\NormalTok{, }\StringTok{"darkgreen"}\NormalTok{),}
    \AttributeTok{main =} \StringTok{"Tooth Growth by Supplement and Dose"}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"Supplement and Dose"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"Length"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{03-descriptive-stat_files/figure-latex/unnamed-chunk-2-3} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# If the notches of two boxes do not overlap, this suggests that the medians differ significantly.}

\CommentTok{\# Stem{-}and{-}Leaf Plot: Provides a quick way to visualize the distribution of data.}
\FunctionTok{stem}\NormalTok{(data)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   The decimal point is at the |}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   {-}2 | 4300}
\CommentTok{\#\textgreater{}   {-}1 | 8644432221110}
\CommentTok{\#\textgreater{}   {-}0 | 9888887766666433222211000}
\CommentTok{\#\textgreater{}    0 | 0000001111112222233334444445566667778899}
\CommentTok{\#\textgreater{}    1 | 0000111223455889}
\CommentTok{\#\textgreater{}    2 | 28}

\CommentTok{\# Bagplot {-} A 2D Boxplot Extension: Visualizes the spread and identifies outliers in two{-}dimensional data.}
\NormalTok{pacman}\SpecialCharTok{::}\FunctionTok{p\_load}\NormalTok{(aplpack)}
\FunctionTok{attach}\NormalTok{(mtcars)}
\FunctionTok{bagplot}\NormalTok{(wt,}
\NormalTok{        mpg,}
        \AttributeTok{xlab =} \StringTok{"Car Weight"}\NormalTok{,}
        \AttributeTok{ylab =} \StringTok{"Miles Per Gallon"}\NormalTok{,}
        \AttributeTok{main =} \StringTok{"Bagplot of Car Weight vs. Miles Per Gallon"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{03-descriptive-stat_files/figure-latex/unnamed-chunk-2-4} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{detach}\NormalTok{(mtcars)}
\end{Highlighting}
\end{Shaded}

Below are some advanced plot types that can provide deeper insights into data:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# boxplot.matrix(): Creates boxplots for each column in a matrix. Useful for comparing multiple variables.}
\NormalTok{graphics}\SpecialCharTok{::}\FunctionTok{boxplot.matrix}\NormalTok{(}
    \FunctionTok{cbind}\NormalTok{(}
        \AttributeTok{Uni05 =}\NormalTok{ (}\DecValTok{1}\SpecialCharTok{:}\DecValTok{100}\NormalTok{) }\SpecialCharTok{/} \DecValTok{21}\NormalTok{,}
        \AttributeTok{Norm =} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{),}
        \AttributeTok{T5 =} \FunctionTok{rt}\NormalTok{(}\DecValTok{100}\NormalTok{, }\AttributeTok{df =} \DecValTok{5}\NormalTok{),}
        \AttributeTok{Gam2 =} \FunctionTok{rgamma}\NormalTok{(}\DecValTok{100}\NormalTok{, }\AttributeTok{shape =} \DecValTok{2}\NormalTok{)}
\NormalTok{    ),}
    \AttributeTok{main =} \StringTok{"Boxplot Marix"}\NormalTok{,}
    \AttributeTok{notch =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{col =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{4}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{03-descriptive-stat_files/figure-latex/unnamed-chunk-3-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Violin Plot (vioplot()): Combines a boxplot with a density plot, providing more information about the distribution.}
\FunctionTok{library}\NormalTok{(}\StringTok{"vioplot"}\NormalTok{)}
\FunctionTok{vioplot}\NormalTok{(data, }\AttributeTok{col =} \StringTok{"lightblue"}\NormalTok{, }\AttributeTok{main =} \StringTok{"Violin Plot Example"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{03-descriptive-stat_files/figure-latex/unnamed-chunk-3-2} \end{center}

\hypertarget{scatterplot}{%
\subsection{Scatterplot}\label{scatterplot}}

Scatterplots are useful for visualizing relationships between two continuous variables. They help identify patterns, correlations, and outliers.

Pairwise Scatterplots: Visualizes relationships between all pairs of variables in a dataset. This is especially useful for exploring potential correlations.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pairs}\NormalTok{(mtcars,}
      \AttributeTok{main =} \StringTok{"Pairwise Scatterplots"}\NormalTok{,}
      \AttributeTok{pch =} \DecValTok{19}\NormalTok{,}
      \AttributeTok{col =} \StringTok{"blue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{03-descriptive-stat_files/figure-latex/unnamed-chunk-4-1} \end{center}

\hypertarget{normality-assessment}{%
\section{Normality Assessment}\label{normality-assessment}}

The Normal (Gaussian) distribution plays a critical role in statistical analyses due to its theoretical and practical applications. Many statistical methods assume normality in the data, making it essential to assess whether our variable of interest follows a normal distribution. To achieve this, we utilize both \protect\hyperlink{numerical-measures}{Numerical Measures} and \protect\hyperlink{graphical-assessment}{Graphical Assessment}.

\hypertarget{graphical-assessment}{%
\subsection{Graphical Assessment}\label{graphical-assessment}}

Graphical methods provide an intuitive way to visually inspect the normality of a dataset. One of the most common methods is the \textbf{Q-Q plot} (quantile-quantile plot). The Q-Q plot compares the quantiles of the sample data to the quantiles of a theoretical normal distribution. Deviations from the line indicate departures from normality.

Below is an example of using the \texttt{qqnorm} and \texttt{qqline} functions in R to assess the normality of the \texttt{precip} dataset, which contains precipitation data (in inches per year) for 70 U.S. cities:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load the required package}
\NormalTok{pacman}\SpecialCharTok{::}\FunctionTok{p\_load}\NormalTok{(}\StringTok{"car"}\NormalTok{)}

\CommentTok{\# Generate a Q{-}Q plot}
\FunctionTok{qqnorm}\NormalTok{(precip,}
       \AttributeTok{ylab =} \StringTok{"Precipitation [in/yr] for 70 US cities"}\NormalTok{,}
       \AttributeTok{main =} \StringTok{"Q{-}Q Plot of Precipitation Data"}\NormalTok{)}
\FunctionTok{qqline}\NormalTok{(precip, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{03-descriptive-stat_files/figure-latex/unnamed-chunk-5-1} \end{center}

\textbf{Interpretation}

\begin{itemize}
\item
  \textbf{Theoretical Line}: The red line represents the expected relationship if the data were perfectly normally distributed.
\item
  \textbf{Data Points}: The dots represent the actual empirical data.
\end{itemize}

If the points closely align with the theoretical line, we can conclude that the data likely follow a normal distribution. However, noticeable deviations from the line, particularly systematic patterns (e.g., curves or s-shaped patterns), indicate potential departures from normality.

Tips

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Small Deviations}: Minor deviations from the line in small datasets are not uncommon and may not significantly impact analyses that assume normality.
\item
  \textbf{Systematic Patterns}: Look for clear trends, such as clusters or s-shaped curves, which suggest skewness or heavy tails.
\item
  \textbf{Complementary Tests}: Always pair graphical methods with numerical measures (e.g., Shapiro-Wilk test) to make a robust conclusion.
\end{enumerate}

When interpreting a Q-Q plot, it is helpful to see both ideal and non-ideal scenarios. Below is an illustrative example:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Normal Data}: Points fall closely along the line.
\item
  \textbf{Skewed Data}: Points systematically deviate from the line, curving upward or downward.
\item
  \textbf{Heavy Tails}: Points deviate at the extremes (ends) of the distribution.
\end{enumerate}

By combining visual inspection and numerical measures, we can better understand the nature of our data and its alignment with the assumption of normality.

\hypertarget{summary-statistics}{%
\subsection{Summary Statistics}\label{summary-statistics}}

While graphical assessments, such as Q-Q plots, provide a visual indication of normality, they may not always offer a definitive conclusion. To supplement graphical methods, statistical tests are often employed. These tests provide quantitative evidence to support or refute the assumption of normality. The most common methods can be classified into two categories:

\begin{itemize}
\item
  \protect\hyperlink{methods-based-on-normal-probability-plot}{Methods Based on Normal Probability Plot}

  \begin{itemize}
  \tightlist
  \item
    \protect\hyperlink{correlation-coefficient-with-normal-probability-plots}{Correlation Coefficient with Normal Probability Plots}
  \item
    \protect\hyperlink{shapiro-wilk-test}{Shapiro-Wilk Test}
  \end{itemize}
\item
  \protect\hyperlink{methods-based-on-empirical-cumulative-distribution-function}{Methods based on empirical cumulative distribution function}

  \begin{itemize}
  \tightlist
  \item
    \protect\hyperlink{anderson-darling-test}{Anderson-Darling Test}
  \item
    \protect\hyperlink{kolmogorov-smirnov-test}{Kolmogorov-Smirnov Test}
  \item
    \protect\hyperlink{cramer-von-mises-test}{Cramer-von Mises Test}
  \item
    \protect\hyperlink{jarquebera-test}{Jarque--Bera Test}
  \end{itemize}
\end{itemize}

\hypertarget{methods-based-on-normal-probability-plot}{%
\subsubsection{Methods Based on Normal Probability Plot}\label{methods-based-on-normal-probability-plot}}

\hypertarget{correlation-coefficient-with-normal-probability-plots}{%
\paragraph{Correlation Coefficient with Normal Probability Plots}\label{correlation-coefficient-with-normal-probability-plots}}

As described by \citet{Looney_1985} and \citet{Shapiro_1972}, this method evaluates the linearity of a normal probability plot by calculating the correlation coefficient between the ordered sample values \(y_{(i)}\) and their theoretical normal quantiles \(m_i^*\). A perfectly linear relationship suggests that the data follow a normal distribution.

The correlation coefficient, denoted \(W^*\), is given by:

\[
W^* = \frac{\sum_{i=1}^{n}(y_{(i)}-\bar{y})(m_i^* - 0)}{\sqrt{\sum_{i=1}^{n}(y_{(i)}-\bar{y})^2 \cdot \sum_{i=1}^{n}(m_i^* - 0)^2}}
\]

where:

\begin{itemize}
\item
  \(\bar{y}\) is the sample mean,
\item
  \(\bar{m^*} = 0\) under the null hypothesis of normality.
\end{itemize}

The \textbf{Pearson product-moment correlation formula} can also be used to evaluate this relationship:

\[
\hat{\rho} = \frac{\sum_{i=1}^{n}(y_i - \bar{y})(x_i - \bar{x})}{\sqrt{\sum_{i=1}^{n}(y_i - \bar{y})^2 \cdot \sum_{i=1}^{n}(x_i - \bar{x})^2}}
\]

\begin{itemize}
\tightlist
\item
  \textbf{Interpretation}:

  \begin{itemize}
  \tightlist
  \item
    When the correlation is 1, the plot is exactly linear, and normality is assumed.
  \item
    The closer the correlation is to 0, the stronger the evidence to reject normality.
  \item
    Inference on \(W^*\) requires reference to special tables \citep{Looney_1985}.
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"EnvStats"}\NormalTok{)}

\CommentTok{\# Perform Probability Plot Correlation Coefficient (PPCC) Test}
\FunctionTok{gofTest}\NormalTok{(data, }\AttributeTok{test =} \StringTok{"ppcc"}\NormalTok{)}\SpecialCharTok{$}\NormalTok{p.value }\CommentTok{\# Probability Plot Correlation Coefficient}
\CommentTok{\#\textgreater{} [1] 0.7094038}
\end{Highlighting}
\end{Shaded}

\hypertarget{shapiro-wilk-test}{%
\paragraph{Shapiro-Wilk Test}\label{shapiro-wilk-test}}

The Shapiro-Wilk test \citep{Shapiro_1965} is one of the most widely used tests for assessing normality, especially for sample sizes \(n < 2000\). This test evaluates how well the data's order statistics match a theoretical normal distribution. The test statistic, \(W\), is computed as:

\[
W=\frac{\sum_{i=1}^{n}a_i x_{(i)}}{\sum_{i=1}^{n}(x_{(i)}-\bar{x})^2} 
\]

where

\begin{itemize}
\item
  \(n\): The sample size.
\item
  \(x_{(i)}\): The \(i\)-th smallest value in the sample (the ordered data).
\item
  \(\bar{x}\): The sample mean.
\item
  \(a_i\): Weights derived from the expected values and variances of the order statistics of a normal distribution, precomputed based on the sample size \(n\).
\end{itemize}

\textbf{Sensitive} to:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Symmetry}

  \begin{itemize}
  \tightlist
  \item
    The Shapiro-Wilk test assesses whether a sample is drawn from a normal distribution, which assumes \textbf{symmetry} around the mean.
  \item
    If the data exhibit skewness (a lack of symmetry), the test is likely to reject the null hypothesis of normality.
  \end{itemize}
\item
  \textbf{Heavy Tails}

  \begin{itemize}
  \tightlist
  \item
    Heavy tails refer to distributions where extreme values (outliers) are more likely compared to a normal distribution.
  \item
    The Shapiro-Wilk test is also sensitive to such departures from normality because heavy tails affect the spread and variance, which are central to the calculation of the test statistic \(W\).
  \end{itemize}
\end{enumerate}

Hence, the Shapiro-Wilk test's sensitivity to these deviations makes it a powerful tool for detecting non-normality only in small to moderate-sized samples. However:

\begin{itemize}
\item
  It is generally more sensitive to \textbf{symmetry} (skewness) than to \textbf{tail behavior} (kurtosis).
\item
  In very large samples, even small deviations in symmetry or tail behavior may cause the test to reject the null hypothesis, even if the data is practically ``normal'' for the intended analysis.

  \begin{itemize}
  \item
    Small sample sizes may lack power to detect deviations from normality.
  \item
    Large sample sizes may detect minor deviations that are not practically significant.
  \end{itemize}
\end{itemize}

\textbf{Key Steps}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Sort the Data:} Arrange the sample data in ascending order, yielding \(x_{(1)}, x_{(2)}, \dots, x_{(n)}\).
\item
  \textbf{Compute Weights:} The weights \(a_i\) are determined using a covariance matrix of the normal order statistics. These are optimized to maximize the power of the test.
\item
  \textbf{Calculate} \(W\): Use the formula to determine \(W\), which ranges from 0 to 1.
\end{enumerate}

\textbf{Decision Rule}:

\begin{itemize}
\item
  \textbf{Null Hypothesis} (\(H_0\)): The data follows a normal distribution.
\item
  \textbf{Alternative Hypothesis} (\(H_1\)): The data does not follow a normal distribution.
\item
  A small \(W\) value, along with a \(p\)-value below a chosen significance level (e.g., 0.05), leads to rejection of \(H_0\).

  \begin{itemize}
  \item
    Under normality, \(W\) approaches 1.
  \item
    Smaller values of \(W\) indicate deviations from normality.
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Perform Shapiro{-}Wilk Test (Default for gofTest)}
\NormalTok{EnvStats}\SpecialCharTok{::}\FunctionTok{gofTest}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{mpg, }\AttributeTok{test =} \StringTok{"sw"}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Results of Goodness{-}of{-}Fit Test}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Test Method:                     Shapiro{-}Wilk GOF}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Hypothesized Distribution:       Normal}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Estimated Parameter(s):          mean = 20.090625}
\CommentTok{\#\textgreater{}                                  sd   =  6.026948}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Estimation Method:               mvue}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Data:                            mtcars$mpg}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Sample Size:                     32}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Test Statistic:                  W = 0.9475647}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Test Statistic Parameter:        n = 32}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} P{-}value:                         0.1228814}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Alternative Hypothesis:          True cdf does not equal the}
\CommentTok{\#\textgreater{}                                  Normal Distribution.}
\end{Highlighting}
\end{Shaded}

\hypertarget{methods-based-on-empirical-cumulative-distribution-function}{%
\subsubsection{Methods Based on Empirical Cumulative Distribution Function}\label{methods-based-on-empirical-cumulative-distribution-function}}

The \textbf{Empirical Cumulative Distribution Function (ECDF)} is a way to represent the distribution of a sample dataset in cumulative terms. It answers the question:

\begin{quote}
``What fraction of the observations in my dataset are less than or equal to a given value \(x\)?''
\end{quote}

The ECDF is defined as:

\[
F_n(x) = \frac{1}{n} \sum_{i=1}^{n} \mathbb{I}(X_i \leq x)
\]

where:

\begin{itemize}
\item
  \(F_(x)\): ECDF at value \(x\).
\item
  \(n\): Total number of data points.
\item
  \(\mathbb{I}(X_i \leq x)\): Indicator function, equal to 1 if \(X_i \leq x\), otherwise 0.
\end{itemize}

This method is especially useful for large sample sizes and can be applied to distributions beyond the normal (Gaussian) distribution.

Properties of the ECDF

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Step Function}: The ECDF is a step function that increases by \(1/n\) at each data point.
\item
  \textbf{Non-decreasing}: As \(x\) increases, \(F_n(x)\) never decreases.
\item
  \textbf{Range}: The ECDF starts at 0 and ends at 1:

  \begin{itemize}
  \tightlist
  \item
    \(F_n(x) = 0\) for \(x < \min(X)\).
  \item
    \(F_n(x) = 1\) for \(x \geq \max(X)\).
  \end{itemize}
\item
  \textbf{Convergence}: As \(n \to \infty\), the ECDF approaches the true cumulative distribution function (CDF) of the population.
\end{enumerate}

Let's consider a sample dataset \(\{3, 7, 7, 10, 15\}\). The ECDF at different values of \(x\) is calculated as:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1444}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.4667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1889}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1778}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\(x\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(\mathbb{I}(X_i \leq x)\) for each \(X_i\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Count \(\leq x\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ECDF \(F_n(x)\)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(x = 5\) & \(\{1, 0, 0, 0, 0\}\) & 1 & \(1/5 = 0.2\) \\
\(x = 7\) & \(\{1, 1, 1, 0, 0\}\) & 3 & \(3/5 = 0.6\) \\
\(x = 12\) & \(\{1, 1, 1, 1, 0\}\) & 4 & \(4/5 = 0.8\) \\
\(x = 15\) & \(\{1, 1, 1, 1, 1\}\) & 5 & \(5/5 = 1.0\) \\
\end{longtable}

Applications of the ECDF

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Goodness-of-fit Tests}: Compare the ECDF to a theoretical CDF (e.g., using the Kolmogorov-Smirnov test).
\item
  \textbf{Outlier Detection}: Analyze cumulative trends to spot unusual data points.
\item
  \textbf{Visual Data Exploration}: Use the ECDF to understand the spread, skewness, and distribution of the data.
\item
  \textbf{Comparing Distributions}: Compare the ECDFs of two datasets to assess differences in their distributions.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load required libraries}
\FunctionTok{library}\NormalTok{(ggplot2)}

\CommentTok{\# Sample dataset}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{15}\NormalTok{)}

\CommentTok{\# ECDF calculation}
\NormalTok{ecdf\_function }\OtherTok{\textless{}{-}} \FunctionTok{ecdf}\NormalTok{(data)}

\CommentTok{\# Generate a data frame for plotting}
\NormalTok{ecdf\_data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x =} \FunctionTok{sort}\NormalTok{(}\FunctionTok{unique}\NormalTok{(data)),}
                        \AttributeTok{ecdf =} \FunctionTok{sapply}\NormalTok{(}\FunctionTok{sort}\NormalTok{(}\FunctionTok{unique}\NormalTok{(data)), }\ControlFlowTok{function}\NormalTok{(x)}
                          \FunctionTok{mean}\NormalTok{(data }\SpecialCharTok{\textless{}=}\NormalTok{ x)))}

\CommentTok{\# Display ECDF values}
\FunctionTok{print}\NormalTok{(ecdf\_data)}
\CommentTok{\#\textgreater{}    x ecdf}
\CommentTok{\#\textgreater{} 1  3  0.2}
\CommentTok{\#\textgreater{} 2  7  0.6}
\CommentTok{\#\textgreater{} 3 10  0.8}
\CommentTok{\#\textgreater{} 4 15  1.0}

\CommentTok{\# Plot the ECDF}
\FunctionTok{ggplot}\NormalTok{(ecdf\_data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ ecdf)) }\SpecialCharTok{+}
  \FunctionTok{geom\_step}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{title =} \StringTok{"Empirical Cumulative Distribution Function"}\NormalTok{,}
    \AttributeTok{x =} \StringTok{"Data Values"}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"Cumulative Proportion"}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{03-descriptive-stat_files/figure-latex/unnamed-chunk-8-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Alternatively}
\FunctionTok{plot.ecdf}\NormalTok{(}\FunctionTok{as.numeric}\NormalTok{(mtcars[}\DecValTok{1}\NormalTok{, ]),}
          \AttributeTok{verticals =} \ConstantTok{TRUE}\NormalTok{,}
          \AttributeTok{do.points =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{03-descriptive-stat_files/figure-latex/unnamed-chunk-9-1} \end{center}

\hypertarget{anderson-darling-test}{%
\paragraph{Anderson-Darling Test}\label{anderson-darling-test}}

The \textbf{Anderson-Darling test} statistic \citep{Anderson_1952} is given by:

\[
A^2 = \int_{-\infty}^{\infty} \frac{\left(F_n(t) - F(t)\right)^2}{F(t)(1 - F(t))} dF(t)
\]

This test calculates a weighted average of squared deviations between the empirical cumulative distribution function (CDF), \(F_n(t)\), and the theoretical CDF, \(F(t)\). More weight is given to deviations in the tails of the distribution, which makes the test particularly sensitive to these regions.

For a sample of size \(n\), with ordered observations \(y_{(1)}, y_{(2)}, \dots, y_{(n)}\), the Anderson-Darling test statistic can also be written as:

\[
A^2 = -n - \frac{1}{n} \sum_{i=1}^n \left[ (2i - 1) \ln(F(y_{(i)})) + (2n + 1 - 2i) \ln(1 - F(y_{(i)})) \right]
\]

For the \textbf{normal distribution}, the test statistic is further simplified. Using the transformation:

\[
p_i = \Phi\left(\frac{y_{(i)} - \bar{y}}{s}\right),
\]

where:

\begin{itemize}
\item
  \(p_i\) is the cumulative probability under the standard normal distribution,
\item
  \(y_{(i)}\) are the ordered sample values,
\item
  \(\bar{y}\) is the sample mean,
\item
  \(s\) is the sample standard deviation,
\end{itemize}

the formula becomes:

\[
A^2 = -n - \frac{1}{n} \sum_{i=1}^n \left[ (2i - 1) \ln(p_i) + (2n + 1 - 2i) \ln(1 - p_i) \right].
\]

Key Features of the Test

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{CDF-Based Weighting}: The Anderson-Darling test gives more weight to deviations in the tails, which makes it particularly sensitive to detecting non-normality in these regions.
\item
  \textbf{Sensitivity}: Compared to other goodness-of-fit tests, such as the \protect\hyperlink{kolmogorov-smirnov-test}{Kolmogorov-Smirnov Test}, the Anderson-Darling test is better at identifying differences in the tails of the distribution.
\item
  \textbf{Integral Form}: The test statistic can also be expressed as an integral over the theoretical CDF: \[
  A^2 = n \int_{-\infty}^\infty \frac{\left[F_n(t) - F(t)\right]^2}{F(t)(1 - F(t))} dF(t),
  \] where \(F_n(t)\) is the empirical CDF, and \(F(t)\) is the specified theoretical CDF.
\item
  \textbf{Applications}:

  \begin{itemize}
  \tightlist
  \item
    Testing for normality or other distributions (e.g., exponential, Weibull).
  \item
    Validating assumptions in statistical models.
  \item
    Comparing data to theoretical distributions.
  \end{itemize}
\end{enumerate}

Hypothesis Testing

\begin{itemize}
\tightlist
\item
  \textbf{Null Hypothesis (}\(H_0\)): The data follows the specified distribution (e.g., normal distribution).
\item
  \textbf{Alternative Hypothesis (}\(H_1\)): The data does not follow the specified distribution.
\item
  The null hypothesis is rejected if \(A^2\) is too large, indicating a poor fit to the specified distribution.
\end{itemize}

Critical values for the test statistic are provided by \citep{Marsaglia_2004} and \citep{Stephens_1974}.

Applications to Other Distributions

The Anderson-Darling test can be applied to various distributions by using specific transformation methods. Examples include:

\begin{itemize}
\item
  \textbf{Exponential}
\item
  \textbf{Logistic}
\item
  \textbf{Gumbel}
\item
  \textbf{Extreme-value}
\item
  \textbf{Weibull} (after logarithmic transformation: \(\log(\text{Weibull}) = \text{Gumbel}\))
\item
  \textbf{Gamma}
\item
  \textbf{Cauchy}
\item
  \textbf{von Mises}
\item
  \textbf{Log-normal (two-parameter)}
\end{itemize}

For more details on transformations and critical values, consult \citep{Stephens_1974}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Perform Anderson{-}Darling Test}
\FunctionTok{library}\NormalTok{(nortest)}
\NormalTok{ad\_test\_result }\OtherTok{\textless{}{-}} \FunctionTok{ad.test}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{mpg)}

\CommentTok{\# Output the test statistic and p{-}value}
\NormalTok{ad\_test\_result}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Anderson{-}Darling normality test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  mtcars$mpg}
\CommentTok{\#\textgreater{} A = 0.57968, p{-}value = 0.1207}
\end{Highlighting}
\end{Shaded}

Alternatively, for a broader range of distributions, use the \texttt{gofTest} function from the \texttt{gof} package:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# General goodness{-}of{-}fit test with Anderson{-}Darling}
\FunctionTok{library}\NormalTok{(EnvStats)}
\NormalTok{gof\_test\_result }\OtherTok{\textless{}{-}}\NormalTok{ EnvStats}\SpecialCharTok{::}\FunctionTok{gofTest}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{mpg, }\AttributeTok{test =} \StringTok{"ad"}\NormalTok{)}

\CommentTok{\# Extract the p{-}value}
\NormalTok{gof\_test\_result}\SpecialCharTok{$}\NormalTok{p.value}
\CommentTok{\#\textgreater{} [1] 0.1207371}
\end{Highlighting}
\end{Shaded}

\hypertarget{kolmogorov-smirnov-test}{%
\paragraph{Kolmogorov-Smirnov Test}\label{kolmogorov-smirnov-test}}

The \textbf{Kolmogorov-Smirnov (K-S) test} is a nonparametric test that compares the empirical cumulative distribution function (ECDF) of a sample to a theoretical cumulative distribution function (CDF), or compares the ECDFs of two samples. It is used to assess whether a sample comes from a specific distribution (one-sample test) or to compare two samples (two-sample test).

The test statistic \(D_n\) for the one-sample test is defined as:

\[
D_n = \sup_x \left| F_n(x) - F(x) \right|,
\]

where:

\begin{itemize}
\item
  \(F_n(x)\) is the empirical CDF of the sample,
\item
  \(F(x)\) is the theoretical CDF under the null hypothesis,
\item
  \(\sup_x\) denotes the supremum (largest value) over all possible values of \(x\).
\end{itemize}

For the two-sample K-S test, the statistic is:

\[
D_{n,m} = \sup_x \left| F_{n,1}(x) - F_{m,2}(x) \right|,
\]

where \(F_{n,1}(x)\) and \(F_{m,2}(x)\) are the empirical CDFs of the two samples, with sizes \(n\) and \(m\), respectively.

Hypotheses

\begin{itemize}
\tightlist
\item
  \textbf{Null hypothesis (}\(H_0\)): The sample comes from the specified distribution (one-sample) or the two samples are drawn from the same distribution (two-sample).
\item
  \textbf{Alternative hypothesis (}\(H_1\)): The sample does not come from the specified distribution (one-sample) or the two samples are drawn from different distributions (two-sample).
\end{itemize}

Properties

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Based on the Largest Deviation}: The K-S test is sensitive to the largest absolute difference between the empirical and expected CDFs, making it effective for detecting shifts in location or scale.
\item
  \textbf{Distribution-Free}: The test does not assume a specific distribution for the data under the null hypothesis. Its significance level is determined from the distribution of the test statistic under the null hypothesis.
\item
  \textbf{Limitations}:

  \begin{itemize}
  \tightlist
  \item
    The test is more sensitive near the center of the distribution than in the tails.
  \item
    It may not perform well with discrete data or small sample sizes.
  \end{itemize}
\item
  \textbf{Related Tests}:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Kuiper's Test}: A variation of the K-S test that is sensitive to deviations in both the center and tails of the distribution. The Kuiper test statistic is: \[
    V_n = D^+ + D^-,
    \] where \(D^+\) and \(D^-\) are the maximum positive and negative deviations of the empirical CDF from the theoretical CDF.
  \end{itemize}
\end{enumerate}

Applications

\begin{itemize}
\tightlist
\item
  Testing for normality or other specified distributions.
\item
  Comparing two datasets to determine if they are drawn from the same distribution.
\end{itemize}

To perform a one-sample K-S test in R, use the \texttt{ks.test()} function. To check the goodness of fit for a specific distribution, the \texttt{gofTest()} function from a package like \texttt{DescTools} can also be used.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# One{-}sample Kolmogorov{-}Smirnov test for normality}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{50}\NormalTok{)  }\CommentTok{\# Generate random normal data}
\FunctionTok{ks.test}\NormalTok{(data, }\StringTok{"pnorm"}\NormalTok{, }\FunctionTok{mean}\NormalTok{(data), }\FunctionTok{sd}\NormalTok{(data))}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Exact one{-}sample Kolmogorov{-}Smirnov test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  data}
\CommentTok{\#\textgreater{} D = 0.095053, p{-}value = 0.7211}
\CommentTok{\#\textgreater{} alternative hypothesis: two{-}sided}

\CommentTok{\# Goodness{-}of{-}fit test using gofTest}
\FunctionTok{library}\NormalTok{(DescTools)}
\FunctionTok{gofTest}\NormalTok{(data, }\AttributeTok{test =} \StringTok{"ks"}\NormalTok{)}\SpecialCharTok{$}\NormalTok{p.value  }\CommentTok{\# Kolmogorov{-}Smirnov test p{-}value}
\CommentTok{\#\textgreater{} [1] 0.7211441}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  \textbf{Advantages}:

  \begin{itemize}
  \item
    Simple and widely applicable.
  \item
    Distribution-free under the null hypothesis.
  \end{itemize}
\item
  \textbf{Limitations}:

  \begin{itemize}
  \item
    Sensitive to sample size: small deviations may lead to significance in large samples.
  \item
    Reduced sensitivity to differences in the tails compared to the Anderson-Darling test.
  \end{itemize}
\end{itemize}

The Kolmogorov-Smirnov test provides a general-purpose method for goodness-of-fit testing and sample comparison, with particular utility in detecting central deviations.

\hypertarget{cramer-von-mises-test}{%
\paragraph{Cramer-von Mises Test}\label{cramer-von-mises-test}}

The \textbf{Cramer-von Mises (CVM) test} is a nonparametric goodness-of-fit test that evaluates the agreement between the empirical cumulative distribution function (ECDF) of a sample and a specified theoretical cumulative distribution function (CDF). Unlike the \protect\hyperlink{kolmogorov-smirnov-test}{Kolmogorov-Smirnov test}, which focuses on the largest discrepancy, the Cramer-von Mises test considers the \textbf{average squared discrepancy} across the entire distribution. Unlike the \protect\hyperlink{anderson-darling-test}{Anderson-Darling test}, it weights all parts of the distribution equally.

The test statistic \(W^2\) for the one-sample Cramer-von Mises test is defined as:

\[
W^2 = n \int_{-\infty}^\infty \left[ F_n(t) - F(t) \right]^2 dF(t),
\]

where:

\begin{itemize}
\item
  \(F_n(t)\) is the empirical CDF,
\item
  \(F(t)\) is the specified theoretical CDF under the null hypothesis,
\item
  \(n\) is the sample size.
\end{itemize}

In practice, \(W^2\) is computed using the ordered sample values \(y_{(1)}, y_{(2)}, \dots, y_{(n)}\) as:

\[
W^2 = \sum_{i=1}^n \left( F(y_{(i)}) - \frac{2i - 1}{2n} \right)^2 + \frac{1}{12n},
\]

where:

\begin{itemize}
\tightlist
\item
  \(F(y_{(i)})\) is the theoretical CDF evaluated at the ordered sample values \(y_{(i)}\).
\end{itemize}

Hypotheses

\begin{itemize}
\tightlist
\item
  \textbf{Null hypothesis (H0)}: The sample data follow the specified distribution.
\item
  \textbf{Alternative hypothesis (H1)}: The sample data do not follow the specified distribution.
\end{itemize}

Properties

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Focus on Average Discrepancy}: The Cramer-von Mises test measures the overall goodness-of-fit by considering the squared deviations across all points in the distribution, ensuring equal weighting of discrepancies.
\item
  \textbf{Comparison to Anderson-Darling}: Unlike the Anderson-Darling test, which gives more weight to deviations in the tails, the CVM test weights all parts of the distribution equally.
\item
  \textbf{Integral Representation}: The statistic is expressed as an integral over the squared differences between the empirical and theoretical CDFs.
\item
  \textbf{Two-Sample Test}: The Cramer-von Mises framework can also be extended to compare two empirical CDFs. The two-sample statistic is based on the pooled empirical CDF.
\end{enumerate}

Applications

\begin{itemize}
\tightlist
\item
  Assessing goodness-of-fit for a theoretical distribution (e.g., normal, exponential, Weibull).
\item
  Comparing two datasets to determine if they are drawn from similar distributions.
\item
  Validating model assumptions.
\end{itemize}

To perform a Cramer-von Mises test in R, the \texttt{gofTest()} function from the \texttt{DescTools} package can be used. Below is an example:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Generate random normal data}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{50}\NormalTok{)}

\CommentTok{\# Perform the Cramer{-}von Mises test}
\FunctionTok{library}\NormalTok{(DescTools)}
\FunctionTok{gofTest}\NormalTok{(data, }\AttributeTok{test =} \StringTok{"cvm"}\NormalTok{)}\SpecialCharTok{$}\NormalTok{p.value  }\CommentTok{\# Cramer{-}von Mises test p{-}value}
\CommentTok{\#\textgreater{} [1] 0.2425907}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  \textbf{Advantages}:

  \begin{itemize}
  \item
    Considers discrepancies across the entire distribution.
  \item
    Robust to outliers due to equal weighting.
  \item
    Simple to compute and interpret.
  \end{itemize}
\item
  \textbf{Limitations}:

  \begin{itemize}
  \item
    Less sensitive to deviations in the tails compared to the Anderson-Darling test.
  \item
    May be less powerful than the Kolmogorov-Smirnov test in detecting central shifts.
  \end{itemize}
\end{itemize}

\hypertarget{jarquebera-test}{%
\paragraph{Jarque-Bera Test}\label{jarquebera-test}}

The \textbf{Jarque-Bera (JB) test} \citep{Bera_1981} is a goodness-of-fit test used to check whether a dataset follows a normal distribution. It is based on the \textbf{skewness} and \textbf{kurtosis} of the data, which measure the asymmetry and the ``tailedness'' of the distribution, respectively.

The Jarque-Bera test statistic is defined as:

\[
JB = \frac{n}{6}\left(S^2 + \frac{(K - 3)^2}{4}\right),
\]

where:

\begin{itemize}
\item
  \(n\) is the sample size,
\item
  \(S\) is the sample skewness,
\item
  \(K\) is the sample kurtosis.
\end{itemize}

Skewness (\(S\)) is calculated as:

\[
S = \frac{\hat{\mu}_3}{\hat{\sigma}^3} = \frac{\frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^3}{\left(\frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2\right)^{3/2}},
\]

where:

\begin{itemize}
\item
  \(\hat{\mu}_3\) is the third central moment,
\item
  \(\hat{\sigma}\) is the standard deviation,
\item
  \(\bar{x}\) is the sample mean.
\end{itemize}

Kurtosis (\(K\)) is calculated as:

\[
K = \frac{\hat{\mu}_4}{\hat{\sigma}^4} = \frac{\frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^4}{\left(\frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2\right)^2},
\]

where:

\begin{itemize}
\tightlist
\item
  \(\hat{\mu}_4\) is the fourth central moment.
\end{itemize}

Hypothesis

\begin{itemize}
\tightlist
\item
  \textbf{Null hypothesis (}\(H_0\)): The data follow a normal distribution, implying:

  \begin{itemize}
  \tightlist
  \item
    Skewness \(S = 0\),
  \item
    Excess kurtosis \(K - 3 = 0\).
  \end{itemize}
\item
  \textbf{Alternative hypothesis (}\(H_1\)): The data do not follow a normal distribution.
\end{itemize}

\textbf{Distribution of the JB Statistic}

Under the null hypothesis, the Jarque-Bera statistic asymptotically follows a chi-squared distribution with 2 degrees of freedom:

\[
JB \sim \chi^2_2.
\]

Properties

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Sensitivity}:

  \begin{itemize}
  \tightlist
  \item
    Skewness (\(S\)) captures asymmetry in the data.
  \item
    Kurtosis (\(K\)) measures how heavy-tailed or light-tailed the distribution is compared to a normal distribution.
  \end{itemize}
\item
  \textbf{Limitations}:

  \begin{itemize}
  \tightlist
  \item
    The test is sensitive to large sample sizes; even small deviations from normality may result in rejection of \(H_0\).
  \item
    Assumes that the data are independently and identically distributed.
  \end{itemize}
\end{enumerate}

Applications

\begin{itemize}
\tightlist
\item
  Testing normality in regression residuals.
\item
  Validating distributional assumptions in econometrics and time series analysis.
\end{itemize}

The Jarque-Bera test can be performed in R using the \texttt{tseries} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tseries)}

\CommentTok{\# Generate a sample dataset}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{)  }\CommentTok{\# Normally distributed data}

\CommentTok{\# Perform the Jarque{-}Bera test}
\FunctionTok{jarque.bera.test}\NormalTok{(data)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Jarque Bera Test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  data}
\CommentTok{\#\textgreater{} X{-}squared = 1.6324, df = 2, p{-}value = 0.4421}
\end{Highlighting}
\end{Shaded}

\hypertarget{bivariate-statistics}{%
\section{Bivariate Statistics}\label{bivariate-statistics}}

Bivariate statistics involve the analysis of relationships between two variables. Understanding these relationships can provide insights into patterns, associations, or (suggestive of) causal connections. Below, we explore the correlation between different types of variables:

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{two-continuous}{Two Continuous} \textbf{Variables}
\item
  \protect\hyperlink{two-discrete}{Two Discrete} \textbf{Variables}
\item
  \protect\hyperlink{categorical-and-continuous}{Categorical and Continuous} \textbf{Variables}
\end{itemize}

Before delving into the analysis, it is critical to consider the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Is the relationship linear or non-linear?}

  \begin{itemize}
  \tightlist
  \item
    Linear relationships can be modeled with simpler statistical methods such as Pearson's correlation, while non-linear relationships may require alternative approaches, such as Spearman's rank correlation or regression with transformations.
  \end{itemize}
\item
  \textbf{If the variable is continuous, is it normal and homoskedastic?}

  \begin{itemize}
  \tightlist
  \item
    For parametric methods like Pearson's correlation, assumptions such as normality and homoskedasticity (equal variance) must be met. When these assumptions fail, non-parametric methods like Spearman's correlation or robust alternatives are preferred.
  \end{itemize}
\item
  \textbf{How big is your dataset?}

  \begin{itemize}
  \tightlist
  \item
    Large datasets can reveal subtle patterns but may lead to statistically significant results that are not practically meaningful. For smaller datasets, careful selection of statistical methods is essential to ensure reliability and validity.
  \end{itemize}
\end{enumerate}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1406}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4766}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3750}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Categorical
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Continuous
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Categorical} & \protect\hyperlink{chi-squared-test}{Chi-squared Test}

\protect\hyperlink{phi-coefficient}{Phi Coefficient}

\protect\hyperlink{cramers-v}{Cramer's V}

\protect\hyperlink{tschuprows-t}{Tschuprow's T}

\protect\hyperlink{spearmans-rank-correlation}{Spearman's Rank Correlation}

\protect\hyperlink{kendalls-tau}{Kendall's Tau}

\protect\hyperlink{gamma-statistic}{Gamma Statistic}

\protect\hyperlink{freemans-theta}{Freeman's Theta}

\protect\hyperlink{epsilon-squared}{Epsilon-squared}

\protect\hyperlink{goodman-kruskals-gamma}{Goodman Kruskal's Gamma}

\protect\hyperlink{somers-d}{Somers' D}

\protect\hyperlink{kendalls-tau-b}{Kendall's Tau-b}

\protect\hyperlink{yules-q-and-y}{Yule's Q and Y}

\protect\hyperlink{tetrachoric-correlation}{Tetrachoric Correlation}

\protect\hyperlink{polychoric-correlation}{Polychoric Correlation} & \\
\textbf{Continuous} & \protect\hyperlink{point-biserial-correlation}{Point-Biserial Correlation}

\protect\hyperlink{logistic-regression}{Logistic Regression} & \protect\hyperlink{pearson-correlation-1}{Pearson Correlation}

\protect\hyperlink{spearman-correlation}{Spearman Correlation} \\
\end{longtable}

\hypertarget{two-continuous}{%
\subsection{Two Continuous}\label{two-continuous}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{n }\OtherTok{=} \DecValTok{100} \CommentTok{\# (sample size)}

\NormalTok{data }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{A =} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{20}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{size =}\NormalTok{ n),}
                  \AttributeTok{B =} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{30}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{size =}\NormalTok{ n))}
\end{Highlighting}
\end{Shaded}

\hypertarget{pearson-correlation}{%
\subsubsection{Pearson Correlation}\label{pearson-correlation}}

\textbf{Pearson correlation} quantifies the strength and direction of a \textbf{linear relationship} between two continuous variables.

Formula:

\[
r = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum (x_i - \bar{x})^2 \cdot \sum (y_i - \bar{y})^2}}
\] where

\begin{itemize}
\item
  \(x_i, y_i\): Individual data points of variables \(X\) and \(Y\).
\item
  \(\bar{x}, \bar{y}\): Means of \(X\) and \(Y\).
\end{itemize}

\textbf{Assumptions}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The relationship between variables is \textbf{linear}.
\item
  Variables are \textbf{normally distributed}.
\item
  Data exhibits \textbf{homoscedasticity} (equal variance of \(Y\) for all values of \(X\)).
\end{enumerate}

Use Case:

\begin{itemize}
\tightlist
\item
  Use when the relationship is expected to be linear, and assumptions of normality and homoscedasticity are met.
\end{itemize}

Interpretation:

\begin{itemize}
\tightlist
\item
  \(r = +1\): Perfect positive linear relationship.
\item
  \(r = -1\): Perfect negative linear relationship.
\item
  \(r = 0\): No linear relationship.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Pearson correlation}
\NormalTok{pearson\_corr }\OtherTok{\textless{}{-}}\NormalTok{ stats}\SpecialCharTok{::}\FunctionTok{cor}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{A, data}\SpecialCharTok{$}\NormalTok{B, }\AttributeTok{method =} \StringTok{"pearson"}\NormalTok{)}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Pearson Correlation (r):"}\NormalTok{, pearson\_corr, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Pearson Correlation (r): 0.02394939}
\end{Highlighting}
\end{Shaded}

\hypertarget{spearman-correlation}{%
\subsubsection{Spearman Correlation}\label{spearman-correlation}}

\textbf{Spearman correlation} measures the strength of a \textbf{monotonic relationship} between two variables. It ranks the data and calculates correlation based on ranks.

Formula:

\[
\rho = 1 - \frac{6 \sum d_i^2}{n(n^2 -1)}
\]

where

\begin{itemize}
\tightlist
\item
  \(d_i\): Difference between the ranks of \(x_i\) and \(y_i\).
\item
  \(n\): Number of paired observations.
\end{itemize}

\textbf{Assumptions}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Relationship must be \textbf{monotonic}, not necessarily linear.
\item
  No assumptions about the distribution of variables.
\end{enumerate}

Use Case:

\begin{itemize}
\tightlist
\item
  Use when data is ordinal or when normality and linearity assumptions are violated.
\end{itemize}

Interpretation:

\begin{itemize}
\item
  \(\rho = +1\): Perfect positive monotonic relationship.
\item
  \(\rho = -1\): Perfect negative monotonic relationship.
\item
  \(\rho = 0\): No monotonic relationship.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Spearman correlation}
\NormalTok{spearman\_corr }\OtherTok{\textless{}{-}}\NormalTok{ stats}\SpecialCharTok{::}\FunctionTok{cor}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{A, data}\SpecialCharTok{$}\NormalTok{B, }\AttributeTok{method =} \StringTok{"spearman"}\NormalTok{)}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Spearman Correlation (rho):"}\NormalTok{, spearman\_corr, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Spearman Correlation (rho): 0.02304636}
\end{Highlighting}
\end{Shaded}

\hypertarget{kendalls-tau-correlation}{%
\subsubsection{Kendall's Tau Correlation}\label{kendalls-tau-correlation}}

\textbf{Kendall's Tau} measures the strength of a \textbf{monotonic relationship} by comparing concordant and discordant pairs.

Formula:

\[
\tau = \frac{(C- D)}{\binom{n}{2}}
\]

where\hspace{0pt}

\begin{itemize}
\item
  \(C\): Number of concordant pairs (where ranks of \(X\) and \(Y\) increase or decrease together).
\item
  \(D\): Number of discordant pairs (where one rank increases while the other decreases).
\item
  \(\binom{n}{2}\): Total number of possible pairs.
\end{itemize}

\textbf{Assumptions}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  No specific assumptions about the data distribution.
\item
  Measures monotonic relationships.
\end{enumerate}

Use Case:

\begin{itemize}
\tightlist
\item
  Preferred for small datasets or when data contains outliers.
\end{itemize}

Interpretation:

\begin{itemize}
\item
  \(\tau = +1\): Perfect positive monotonic relationship.
\item
  \(\tau = -1\): Perfect negative monotonic relationship.
\item
  \(\tau = 0\): No monotonic relationship.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Kendall\textquotesingle{}s Tau correlation}
\NormalTok{kendall\_corr }\OtherTok{\textless{}{-}}\NormalTok{ stats}\SpecialCharTok{::}\FunctionTok{cor}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{A, data}\SpecialCharTok{$}\NormalTok{B, }\AttributeTok{method =} \StringTok{"kendall"}\NormalTok{)}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Kendall\textquotesingle{}s Tau Correlation (tau):"}\NormalTok{, kendall\_corr, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Kendall\textquotesingle{}s Tau Correlation (tau): 0.02171284}
\end{Highlighting}
\end{Shaded}

\hypertarget{distance-correlation}{%
\subsubsection{Distance Correlation}\label{distance-correlation}}

\textbf{Distance Correlation} measures both \textbf{linear and non-linear relationships} between variables. It does not require monotonicity or linearity.

Formula:

\[
d Cor = \frac{d Cov(X,Y)}{\sqrt{d Var (X) \cdot d Var (Y)}}
\]

where\hspace{0pt}

\begin{itemize}
\item
  \(dCov\): Distance covariance between \(X\) and \(Y\).
\item
  \(dVar\): Distance variances of \(X\) and \(Y\).
\end{itemize}

\textbf{Assumptions:}

\begin{itemize}
\tightlist
\item
  No specific assumptions about the relationship (linear, monotonic, or otherwise).
\end{itemize}

Use Case:

\begin{itemize}
\tightlist
\item
  Use for complex relationships, including non-linear patterns.
\end{itemize}

Interpretation:

\begin{itemize}
\item
  \(dCor = 0\): No association.
\item
  \(dCor = 1\): Perfect association.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Distance correlation}
\NormalTok{distance\_corr }\OtherTok{\textless{}{-}}\NormalTok{ energy}\SpecialCharTok{::}\FunctionTok{dcor}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{A, data}\SpecialCharTok{$}\NormalTok{B)}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Distance Correlation (dCor):"}\NormalTok{, distance\_corr, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Distance Correlation (dCor): 0.1008934}
\end{Highlighting}
\end{Shaded}

\hypertarget{summary-table-of-correlation-methods}{%
\subsubsection{Summary Table of Correlation Methods}\label{summary-table-of-correlation-methods}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1452}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1774}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1505}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1613}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1398}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.2043}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Formula/Approach
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Detects Relationship Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Assumptions
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Sensitivity to Outliers
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Use Case
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Pearson} & Linear covariance & Linear & Normality, homoscedasticity & High & Linear relationships. \\
\textbf{Spearman} & Ranks and monotonicity formula & Monotonic & None & Moderate & Monotonic, non-linear data. \\
\textbf{Kendall's Tau} & Concordance/discordance ratio & Monotonic & None & Low & Small datasets, robust to outliers. \\
\textbf{Distance Correlation} & Distance-based variance & Linear and non-linear & None & Low & Complex, non-linear relationships. \\
\end{longtable}

\hypertarget{categorical-and-continuous}{%
\subsection{Categorical and Continuous}\label{categorical-and-continuous}}

Analyzing the relationship between a \textbf{categorical variable} (binary or multi-class) and a \textbf{continuous variable} requires specialized techniques. These methods assess whether the categorical variable significantly influences the continuous variable or vice versa.

We focus on the following methods:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \protect\hyperlink{point-biserial-correlation}{Point-Biserial Correlation}
\item
  \protect\hyperlink{logistic-regression}{Logistic Regression}
\item
  \protect\hyperlink{sec-analysis-of-variance-anova}{Analysis of Variance (ANOVA)}
\item
  T-test
\end{enumerate}

\hypertarget{point-biserial-correlation}{%
\subsubsection{Point-Biserial Correlation}\label{point-biserial-correlation}}

The \textbf{Point-Biserial Correlation} is a special case of the Pearson correlation used to assess the relationship between a \textbf{binary categorical variable} (coded as 0 and 1) and a continuous variable. It measures the strength and direction of the linear relationship.

Formula:

\[
r_{pb} = \frac{\bar{Y_1} - \bar{Y_0}}{s_Y} \sqrt{\frac{n_1 n_0}{n^2}}
\]

where

\begin{itemize}
\item
  \(\bar{Y_1}\), \(\bar{Y_0}\): Mean of the continuous variable for the groups coded as 1 and 0, respectively.
\item
  \(s_Y\): Standard deviation of the continuous variable.
\item
  \(n_1, n_0\): Number of observations in each group (1 and 0).
\item
  \(n\): Total number of observations.
\end{itemize}

Key Properties:

\begin{itemize}
\tightlist
\item
  \textbf{Range}: \(-1\) to \(1\).

  \begin{itemize}
  \tightlist
  \item
    \(r_{pb} = +1\): Perfect positive correlation.
  \item
    \(r_{pb} = -1\): Perfect negative correlation.
  \item
    \(r_{pb} = 0\): No linear relationship.
  \end{itemize}
\item
  A positive \(r_{pb}\) indicates higher values of the continuous variable are associated with the 1 group, while a negative \(r_{pb}\) indicates the opposite.
\end{itemize}

\textbf{Assumptions}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The binary variable is \textbf{truly dichotomous} (e.g., male/female, success/failure).
\item
  The continuous variable is approximately \textbf{normally distributed}.
\item
  Homogeneity of variance across the two groups (not strictly required but recommended).
\end{enumerate}

Use Case:

\begin{itemize}
\tightlist
\item
  To evaluate the linear relationship between a binary categorical variable and a continuous variable.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ltm)}
\CommentTok{\# Point{-}Biserial Correlation}
\NormalTok{biserial\_corr }\OtherTok{\textless{}{-}}\NormalTok{ ltm}\SpecialCharTok{::}\FunctionTok{biserial.cor}\NormalTok{(}
  \FunctionTok{c}\NormalTok{(}\FloatTok{12.5}\NormalTok{, }\FloatTok{15.3}\NormalTok{, }\FloatTok{10.7}\NormalTok{, }\FloatTok{18.1}\NormalTok{, }\FloatTok{11.2}\NormalTok{, }\FloatTok{16.8}\NormalTok{, }\FloatTok{13.4}\NormalTok{, }\FloatTok{14.9}\NormalTok{), }
  \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }
  \AttributeTok{use =} \StringTok{"all.obs"}\NormalTok{, }
  \AttributeTok{level =} \DecValTok{2}
\NormalTok{)}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Point{-}Biserial Correlation:"}\NormalTok{, biserial\_corr, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Point{-}Biserial Correlation: 0.8792835}
\end{Highlighting}
\end{Shaded}

\hypertarget{logistic-regression}{%
\subsubsection{Logistic Regression}\label{logistic-regression}}

\textbf{Logistic Regression} models the relationship between a \textbf{binary categorical variable} (dependent variable) and one or more independent variables (which may include continuous variables). It predicts the probability of the binary outcome (e.g., success/failure, yes/no).

Refer to \ref{logistic-regression} for more detail.

Formula:

The logistic regression model is represented as:

\[
\text{logit}(p) = \ln \left( \frac{p}{1 - p} \right) = \beta_0 + \beta_1 X
\]

where

\begin{itemize}
\item
  \(p\): Probability of the outcome being 1.
\item
  \(\beta_0\): Intercept.
\item
  \(\beta_1\): Coefficient for the continuous variable \(X\).
\item
  \(\text{logit}(p)\): Log-odds of the probability.
\end{itemize}

Key Features:

\begin{itemize}
\item
  \textbf{Output}: Odds ratio or probability of the binary outcome.
\item
  Can include multiple predictors (continuous and categorical).
\item
  Non-linear transformation ensures predictions are probabilities between 0 and 1.
\end{itemize}

\textbf{Assumptions}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The dependent variable is \textbf{binary}.
\item
  Observations are \textbf{independent}.
\item
  There is a \textbf{linear relationship} between the logit of the dependent variable and the independent variable.
\item
  No \textbf{multicollinearity} between predictors.
\end{enumerate}

Use Case:

\begin{itemize}
\tightlist
\item
  To predict the likelihood of a binary outcome based on a continuous predictor (e.g., probability of success given test scores).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Simulated data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\AttributeTok{mean =} \DecValTok{50}\NormalTok{, }\AttributeTok{sd =} \DecValTok{10}\NormalTok{)  }\CommentTok{\# Continuous predictor}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(x }\SpecialCharTok{\textgreater{}} \DecValTok{55}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)  }\CommentTok{\# Binary outcome based on threshold}

\CommentTok{\# Logistic Regression}
\NormalTok{logistic\_model }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{family =}\NormalTok{ binomial)}
\FunctionTok{summary}\NormalTok{(logistic\_model)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} glm(formula = y \textasciitilde{} x, family = binomial)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Deviance Residuals: }
\CommentTok{\#\textgreater{}        Min          1Q      Median          3Q         Max  }
\CommentTok{\#\textgreater{} {-}2.770e{-}04  {-}2.100e{-}08  {-}2.100e{-}08   2.100e{-}08   2.548e{-}04  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}             Estimate Std. Error z value Pr(\textgreater{}|z|)}
\CommentTok{\#\textgreater{} (Intercept)  {-}3749.9   495083.0  {-}0.008    0.994}
\CommentTok{\#\textgreater{} x               67.9     8966.6   0.008    0.994}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} (Dispersion parameter for binomial family taken to be 1)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Null deviance: 1.2217e+02  on 99  degrees of freedom}
\CommentTok{\#\textgreater{} Residual deviance: 1.4317e{-}07  on 98  degrees of freedom}
\CommentTok{\#\textgreater{} AIC: 4}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Fisher Scoring iterations: 25}

\CommentTok{\# Predicted probabilities}
\NormalTok{predicted\_probs }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(logistic\_model, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\FunctionTok{print}\NormalTok{(}\FunctionTok{head}\NormalTok{(predicted\_probs))}
\CommentTok{\#\textgreater{}         1         2         3         4         5         6 }
\CommentTok{\#\textgreater{} {-}735.6466 {-}511.3844  703.2134 {-}307.2281 {-}267.3187  809.3747}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Visualize logistic regression curve}
\FunctionTok{library}\NormalTok{(ggplot2)}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ y, }\AttributeTok{predicted =}\NormalTok{ predicted\_probs)}

\FunctionTok{ggplot}\NormalTok{(data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ predicted)) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ y), }\AttributeTok{color =} \StringTok{"red"}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{color =} \StringTok{"blue"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Logistic Regression: Continuous vs Binary"}\NormalTok{,}
         \AttributeTok{x =} \StringTok{"Continuous Predictor"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Predicted Probability"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{03-descriptive-stat_files/figure-latex/unnamed-chunk-22-1} \end{center}

\hypertarget{summary-table-of-methods-between-categorical-and-continuous}{%
\subsubsection{Summary Table of Methods (Between Categorical and Continuous)}\label{summary-table-of-methods-between-categorical-and-continuous}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2157}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2614}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2418}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2680}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Type of Variable Relationship
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Key Assumptions
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Use Case
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Point-Biserial Correlation} & Binary Categorical vs Continuous & Linear, normality (continuous) & Assess linear association. \\
\textbf{Logistic Regression} & Continuous  Binary Categorical & Logit-linear relationship & Predict probability of binary outcome. \\
\textbf{ANOVA} & Multi-level Categorical vs Continuous & Normality, homogeneity of variance & Compare means across groups. \\
\textbf{T-Test} & Binary Categorical vs Continuous & Normality, equal variance & Compare means between two groups. \\
\end{longtable}

\hypertarget{two-discrete}{%
\subsection{Two Discrete}\label{two-discrete}}

When analyzing the relationship between two \textbf{discrete variables} (categorical or ordinal), various methods are available to quantify the degree of association or similarity. These methods can broadly be classified into:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \protect\hyperlink{distance-metrics}{Distance Metrics}
\item
  \protect\hyperlink{statistical-metrics}{Statistical Metrics}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{distance-metrics}{%
\subsubsection{Distance Metrics}\label{distance-metrics}}

Distance metrics measure the \textbf{dissimilarity} between two discrete variables and are often used as a proxy for correlation in specific applications like clustering or machine learning.

\hypertarget{euclidean-distance}{%
\paragraph{Euclidean Distance}\label{euclidean-distance}}

\[
d(x, y) = \sqrt{\sum_{i=1}^n (x_i - y_i)^2}
\]

\begin{itemize}
\item
  Measures the straight-line distance between two variables in Euclidean space.
\item
  Sensitive to scaling; variables should be normalized for meaningful comparisons.
\end{itemize}

\hypertarget{manhattan-distance}{%
\paragraph{Manhattan Distance}\label{manhattan-distance}}

\[
d(x, y) = \sum_{i=1}^n |x_i - y_i|
\]

\begin{itemize}
\item
  Measures distance by summing the absolute differences along each dimension.
\item
  Also called \textbf{L1 norm}; often used in grid-based problems.
\end{itemize}

\hypertarget{chebyshev-distance}{%
\paragraph{Chebyshev Distance}\label{chebyshev-distance}}

\[
d(x, y) = \max_{i=1}^n |x_i - y_i|
\]

\begin{itemize}
\item
  Measures the maximum single-step distance along any dimension.
\item
  Useful in discrete, grid-based problems (e.g., chess moves).
\end{itemize}

\hypertarget{minkowski-distance}{%
\paragraph{Minkowski Distance}\label{minkowski-distance}}

\[
d(x, y) = \left( \sum_{i=1}^n |x_i - y_i|^p \right)^{1/p}
\]

\begin{itemize}
\item
  Generalized distance metric. Special cases include:

  \begin{itemize}
  \item
    \(p = 1\): Manhattan Distance.
  \item
    \(p = 2\): Euclidean Distance.
  \item
    \(p \to \infty\): Chebyshev Distance.
  \end{itemize}
\end{itemize}

\hypertarget{canberra-distance}{%
\paragraph{Canberra Distance}\label{canberra-distance}}

\[
d(x, y) = \sum_{i=1}^n \frac{|x_i - y_i|}{|x_i| + |y_i|}
\]

\begin{itemize}
\tightlist
\item
  Emphasizes proportional differences, making it sensitive to smaller values.
\end{itemize}

\hypertarget{hamming-distance}{%
\paragraph{Hamming Distance}\label{hamming-distance}}

\[
d(x, y) = \sum_{i=1}^n I(x_i \neq y_i)
\]

\begin{itemize}
\item
  Counts the number of differing positions between two sequences.
\item
  Widely used in text similarity and binary data.
\end{itemize}

\hypertarget{cosine-similarity-and-distance}{%
\paragraph{Cosine Similarity and Distance}\label{cosine-similarity-and-distance}}

\[
\text{Cosine Similarity} = \frac{\sum_{i=1}^n x_i y_i}{\sqrt{\sum_{i=1}^n x_i^2} \cdot \sqrt{\sum_{i=1}^n y_i^2}}
\]

\[
\text{Cosine Distance} = 1 - \text{Cosine Similarity}
\]

\begin{itemize}
\item
  Measures the angle between two vectors in a high-dimensional space.
\item
  Often used in text and document similarity.
\end{itemize}

\hypertarget{sum-of-absolute-differences}{%
\paragraph{Sum of Absolute Differences}\label{sum-of-absolute-differences}}

\[
d(x, y) = \sum_{i=1}^n |x_i - y_i|
\]

\begin{itemize}
\tightlist
\item
  Equivalent to Manhattan Distance but without coordinate context.
\end{itemize}

\hypertarget{sum-of-squared-differences}{%
\paragraph{Sum of Squared Differences}\label{sum-of-squared-differences}}

\[
d(x, y) = \sum_{i=1}^n (x_i - y_i)^2
\]

\begin{itemize}
\tightlist
\item
  Equivalent to squared Euclidean Distance.
\end{itemize}

\hypertarget{mean-absolute-error}{%
\paragraph{Mean Absolute Error}\label{mean-absolute-error}}

\[
\text{MAE} = \frac{1}{n} \sum_{i=1}^n |x_i - y_i|
\]

\begin{itemize}
\tightlist
\item
  Measures average absolute differences.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example data}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{)}

\CommentTok{\# Compute distances}
\NormalTok{euclidean }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{sum}\NormalTok{((x }\SpecialCharTok{{-}}\NormalTok{ y)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{))}
\NormalTok{manhattan }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(}\FunctionTok{abs}\NormalTok{(x }\SpecialCharTok{{-}}\NormalTok{ y))}
\NormalTok{chebyshev }\OtherTok{\textless{}{-}} \FunctionTok{max}\NormalTok{(}\FunctionTok{abs}\NormalTok{(x }\SpecialCharTok{{-}}\NormalTok{ y))}
\NormalTok{hamming }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(x }\SpecialCharTok{!=}\NormalTok{ y)}
\NormalTok{cosine\_similarity }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(x }\SpecialCharTok{*}\NormalTok{ y) }\SpecialCharTok{/}\NormalTok{ (}\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{sum}\NormalTok{(x}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)) }\SpecialCharTok{*} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{sum}\NormalTok{(y}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)))}
\NormalTok{cosine\_distance }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ cosine\_similarity}

\CommentTok{\# Display results}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Euclidean Distance:"}\NormalTok{, euclidean, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Euclidean Distance: 2.236068}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Manhattan Distance:"}\NormalTok{, manhattan, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Manhattan Distance: 5}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Chebyshev Distance:"}\NormalTok{, chebyshev, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Chebyshev Distance: 1}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Hamming Distance:"}\NormalTok{, hamming, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Hamming Distance: 5}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Cosine Distance:"}\NormalTok{, cosine\_distance, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Cosine Distance: 0.005063324}
\end{Highlighting}
\end{Shaded}

\hypertarget{statistical-metrics}{%
\subsubsection{Statistical Metrics}\label{statistical-metrics}}

\hypertarget{chi-squared-test}{%
\paragraph{Chi-squared Test}\label{chi-squared-test}}

The \textbf{Chi-Squared Test} evaluates whether two categorical variables are \textbf{independent} by comparing observed and expected frequencies in a contingency table.

Formula:

\[
\chi^2 = \sum \frac{(O_i - E_i)^2}{E_i}
\]

where

\begin{itemize}
\item
  \(O_i\): Observed frequency in each cell of the table.
\item
  \(E_i\): Expected frequency under the assumption of independence.
\end{itemize}

Steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Construct a contingency table with observed counts.
\item
  Compute expected frequencies: \(E_{ij} = \frac{\text{Row Total}_i \cdot \text{Column Total}_j}{\text{Grand Total}}\)
\item
  Apply the Chi-squared formula.
\item
  Compare \(\chi^2\) with a critical value from the \protect\hyperlink{chi-squared-distribution}{Chi-squared distribution}.
\end{enumerate}

Assumptions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Observations are \textbf{independent}.
\item
  Expected frequencies should be \(\geq 5\) in at least 80\% of the cells.
\end{enumerate}

Use Case:

\begin{itemize}
\tightlist
\item
  Tests for \textbf{independence} between two nominal variables.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example data}
\NormalTok{dt }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{15}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{40}\NormalTok{), }\AttributeTok{nrow =} \DecValTok{2}\NormalTok{)}
\FunctionTok{rownames}\NormalTok{(dt) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Group A"}\NormalTok{, }\StringTok{"Group B"}\NormalTok{)}
\FunctionTok{colnames}\NormalTok{(dt) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Category 1"}\NormalTok{, }\StringTok{"Category 2"}\NormalTok{)}

\CommentTok{\# Perform Chi{-}Squared Test}
\NormalTok{chi\_sq\_test }\OtherTok{\textless{}{-}} \FunctionTok{chisq.test}\NormalTok{(dt)}
\FunctionTok{print}\NormalTok{(chi\_sq\_test)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Pearson\textquotesingle{}s Chi{-}squared test with Yates\textquotesingle{} continuity correction}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  dt}
\CommentTok{\#\textgreater{} X{-}squared = 0.045788, df = 1, p{-}value = 0.8306}
\end{Highlighting}
\end{Shaded}

\hypertarget{phi-coefficient}{%
\paragraph{Phi Coefficient}\label{phi-coefficient}}

The \textbf{Phi Coefficient} is a measure of association between two \textbf{binary variables}, derived from the Chi-squared statistic.

Formula:

\[
\phi = \frac{\chi^2}{n}
\]

where

\begin{itemize}
\tightlist
\item
  \(n\): Total sample size.
\end{itemize}

Interpretation:

\begin{itemize}
\item
  \(\phi = 0\): No association.
\item
  \(\phi = +1\): Perfect positive association.
\item
  \(\phi = -1\): Perfect negative association.
\end{itemize}

Use Case:

\begin{itemize}
\tightlist
\item
  Suitable for \textbf{2x2 contingency tables}.
\item
  2 binary
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(psych)}

\CommentTok{\# Compute Phi Coefficient}
\NormalTok{phi\_coeff }\OtherTok{\textless{}{-}} \FunctionTok{phi}\NormalTok{(dt)}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Phi Coefficient:"}\NormalTok{, phi\_coeff, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Phi Coefficient: 0.04}
\end{Highlighting}
\end{Shaded}

\hypertarget{cramers-v}{%
\paragraph{Cramer's V}\label{cramers-v}}

\textbf{Cramer's V} generalizes the Phi coefficient to handle contingency tables with more than two rows or columns.

Formula:

\[
V = \sqrt{\frac{\chi^2 / n}{\min(r-1, c-1)}}
\]

where\hspace{0pt}\hspace{0pt}

\begin{itemize}
\item
  \(r\): Number of rows.
\item
  \(c\): Number of columns.
\end{itemize}

\textbf{Assumptions}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Variables are nominal.
\item
  Suitable for \textbf{larger contingency tables}.
\end{enumerate}

Use Case:

\begin{itemize}
\tightlist
\item
  Measures the \textbf{strength of association} between nominal variables with no natural order.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(lsr)}

\CommentTok{\# Simulate data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{A =} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{size =} \DecValTok{100}\NormalTok{),  }\CommentTok{\# Nominal variable}
  \AttributeTok{B =} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{size =} \DecValTok{100}\NormalTok{)  }\CommentTok{\# Nominal variable}
\NormalTok{)}

\CommentTok{\# Compute Cramer\textquotesingle{}s V}
\NormalTok{cramers\_v }\OtherTok{\textless{}{-}} \FunctionTok{cramersV}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{A, data}\SpecialCharTok{$}\NormalTok{B)}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Cramer\textquotesingle{}s V:"}\NormalTok{, cramers\_v, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Cramer\textquotesingle{}s V: 0.1944616}
\end{Highlighting}
\end{Shaded}

Alternatively,

\begin{itemize}
\item
  \texttt{ncchisq} noncentral Chi-square
\item
  \texttt{nchisqadj} Adjusted noncentral Chi-square
\item
  \texttt{fisher} Fisher Z transformation
\item
  \texttt{fisheradj} bias correction Fisher z transformation
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{DescTools}\SpecialCharTok{::}\FunctionTok{CramerV}\NormalTok{(data, }\AttributeTok{conf.level =} \FloatTok{0.95}\NormalTok{,}\AttributeTok{method =} \StringTok{"ncchisqadj"}\NormalTok{)}
\CommentTok{\#\textgreater{}  Cramer V    lwr.ci    upr.ci }
\CommentTok{\#\textgreater{} 0.3472325 0.3929964 0.4033053}
\end{Highlighting}
\end{Shaded}

\hypertarget{adjusted-cramers-v}{%
\paragraph{Adjusted Cramer's V}\label{adjusted-cramers-v}}

Adjusted versions of Cramer's V correct for bias, especially in small samples.

Adjusted formulas account for non-central Chi-squared or bias correction. Examples include:

\begin{itemize}
\item
  \textbf{Non-central Chi-squared}: \(V_{adj} = \sqrt{\frac{\chi^2_{nc} / n}{\min(r-1, c-1)}}\)\hspace{0pt}
\item
  \textbf{Bias Correction}: \(V_{adj} = V - \text{Bias Term}\)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(DescTools)}

\CommentTok{\# Compute Adjusted Cramer\textquotesingle{}s V}
\NormalTok{cramers\_v\_adj }\OtherTok{\textless{}{-}} \FunctionTok{CramerV}\NormalTok{(data, }\AttributeTok{conf.level =} \FloatTok{0.95}\NormalTok{, }\AttributeTok{method =} \StringTok{"ncchisqadj"}\NormalTok{)}
\NormalTok{cramers\_v\_adj}
\CommentTok{\#\textgreater{}  Cramer V    lwr.ci    upr.ci }
\CommentTok{\#\textgreater{} 0.3472325 0.3929964 0.4033053}
\end{Highlighting}
\end{Shaded}

\hypertarget{tschuprows-t}{%
\paragraph{Tschuprow's T}\label{tschuprows-t}}

\textbf{Tschuprow's T} is a symmetric measure of association for nominal variables. It differs from Cramer's V by considering the product of rows and columns, making it less sensitive to asymmetrical tables.

Formula:

\[
T = \sqrt{\frac{\chi^2/n}{\sqrt{(r-1)(c-1)}}}
\]

\textbf{Assumptions}:

\begin{itemize}
\item
  Applicable to nominal variables.
\item
  Suitable for contingency tables with unequal dimensions.
\end{itemize}

Use Case:

\begin{itemize}
\tightlist
\item
  Preferred when table dimensions are \textbf{highly unequal}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Compute Tschuprow\textquotesingle{}s T}
\NormalTok{tschuprow\_t }\OtherTok{\textless{}{-}}\NormalTok{ DescTools}\SpecialCharTok{::}\FunctionTok{TschuprowT}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{A, data}\SpecialCharTok{$}\NormalTok{B)}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Tschuprow\textquotesingle{}s T:"}\NormalTok{, tschuprow\_t, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Tschuprow\textquotesingle{}s T: 0.1839104}
\end{Highlighting}
\end{Shaded}

\hypertarget{ordinal-association-rank-correlation}{%
\paragraph{Ordinal Association (Rank correlation)}\label{ordinal-association-rank-correlation}}

When at least one variable is \textbf{ordinal}, rank-based methods are the most appropriate as they respect the \textbf{order of the categories}. These methods are often used when relationships are monotonic (increasing or decreasing consistently) but not necessarily linear.

\hypertarget{spearmans-rank-correlation}{%
\subparagraph{Spearman's Rank Correlation}\label{spearmans-rank-correlation}}

\textbf{Spearman's Rank Correlation} (\(\rho\)) measures the strength and direction of a \textbf{monotonic relationship} between two variables. It transforms the data into ranks and calculates Pearson correlation on the ranks.

Formula:

\[
\rho = 1 - \frac{6 \sum d_i^2}{n (n^2 -1)}
\]

where\hspace{0pt}\hspace{0pt}

\begin{itemize}
\item
  \(d_i\): Difference between the ranks of the paired observations.
\item
  \(n\): Number of paired observations.
\end{itemize}

\textbf{Assumptions}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Data must be \textbf{ordinal} or \textbf{continuous} but convertible to ranks.
\item
  Relationship is \textbf{monotonic}.
\end{enumerate}

Use Case:

\begin{itemize}
\tightlist
\item
  Suitable for ordinal-ordinal or ordinal-continuous associations.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Simulating ordinal data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{ordinal\_x }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{, }\DecValTok{100}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{ordinal\_y }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{, }\DecValTok{100}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}

\CommentTok{\# Spearman\textquotesingle{}s Correlation}
\NormalTok{spearman\_corr }\OtherTok{\textless{}{-}} \FunctionTok{cor}\NormalTok{(ordinal\_x, ordinal\_y, }\AttributeTok{method =} \StringTok{"spearman"}\NormalTok{)}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Spearman\textquotesingle{}s Correlation (rho):"}\NormalTok{, spearman\_corr, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Spearman\textquotesingle{}s Correlation (rho): 0.08731195}
\end{Highlighting}
\end{Shaded}

\hypertarget{kendalls-tau}{%
\subparagraph{Kendall's Tau}\label{kendalls-tau}}

\textbf{Kendall's Tau} (\(\tau\)) measures the strength of a \textbf{monotonic relationship} by comparing concordant and discordant pairs.

Formula:

\[
\tau = \frac{C - D}{C + D}
\]\hspace{0pt}where

\begin{itemize}
\item
  \(C\): Number of concordant pairs (ranks increase together).
\item
  \(D\): Number of discordant pairs (one rank increases while the other decreases).
\end{itemize}

Variants:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Kendall's Tau-a}: For data with no ties.
\item
  \textbf{Kendall's Tau-b}: Adjusted for ties in ranks.
\item
  \textbf{Kendall's Tau-c}: Adjusted for ties in large tables.
\end{enumerate}

Use Case:

\begin{itemize}
\tightlist
\item
  Ideal for small datasets or when ties are present.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Kendall\textquotesingle{}s Tau}
\NormalTok{kendall\_corr }\OtherTok{\textless{}{-}} \FunctionTok{cor}\NormalTok{(ordinal\_x, ordinal\_y, }\AttributeTok{method =} \StringTok{"kendall"}\NormalTok{)}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Kendall\textquotesingle{}s Tau (tau):"}\NormalTok{, kendall\_corr, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Kendall\textquotesingle{}s Tau (tau): 0.06795076}
\end{Highlighting}
\end{Shaded}

\hypertarget{gamma-statistic}{%
\subparagraph{Gamma Statistic}\label{gamma-statistic}}

The \textbf{Gamma Statistic} measures the strength of association between two ordinal variables by focusing on \textbf{concordant} and \textbf{discordant} pairs, ignoring ties.

Formula:

\[
\gamma = \frac{C- D}{C + D}
\]

Use Case:

\begin{itemize}
\tightlist
\item
  Works well when there are \textbf{many ties} in the data.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(vcd)}

\CommentTok{\# Simulating ordinal data}
\NormalTok{cont\_table }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(ordinal\_x, ordinal\_y)}

\CommentTok{\# Gamma Statistic}
\NormalTok{gamma\_stat }\OtherTok{\textless{}{-}} \FunctionTok{assocstats}\NormalTok{(cont\_table)}\SpecialCharTok{$}\NormalTok{gamma}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Gamma Statistic:"}\NormalTok{, gamma\_stat, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Gamma Statistic:}
\end{Highlighting}
\end{Shaded}

\hypertarget{freemans-theta}{%
\subparagraph{Freeman's Theta}\label{freemans-theta}}

\textbf{Freeman's Theta} measures the association between an \textbf{ordinal variable} and a \textbf{nominal variable}. It quantifies how well the grouping in the nominal variable explains the ordering in the ordinal variable.

Use Case:

\begin{itemize}
\tightlist
\item
  Useful when analyzing relationships between ordinal predictors and nominal responses (or vice versa).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rcompanion}\SpecialCharTok{::}\FunctionTok{freemanTheta}\NormalTok{(ordinal\_x, ordinal\_y)}
\CommentTok{\#\textgreater{} Freeman.theta }
\CommentTok{\#\textgreater{}         0.094}
\end{Highlighting}
\end{Shaded}

\hypertarget{epsilon-squared}{%
\subparagraph{Epsilon-squared}\label{epsilon-squared}}

\textbf{Epsilon-Squared} (\(\epsilon^2\)) measures the proportion of variance in the \textbf{ordinal variable} explained by a \textbf{nominal variable}. It is conceptually similar to the coefficient of determination (\(R^2\)) in linear regression but adapted for ordinal-nominal relationships.

Formula:

\[
\epsilon^2 = \frac{\text{variance explained by group differences}}{\text{total variance}}
\]

where

\begin{itemize}
\item
  The numerator represents the variance between ordinal categories due to differences in nominal groups.
\item
  The denominator is the total variance in the ordinal variable.
\end{itemize}

Use Case:

\begin{itemize}
\tightlist
\item
  Quantifies the effect size when analyzing how well a nominal variable explains an ordinal variable.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{ordinal\_x }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{, }\DecValTok{100}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)  }\CommentTok{\# Ordinal variable}
\NormalTok{nominal\_y }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{, }\DecValTok{100}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{) }\CommentTok{\# Nominal variable}

\CommentTok{\# Compute Epsilon{-}Squared}
\FunctionTok{library}\NormalTok{(rcompanion)}
\NormalTok{epsilon\_squared }\OtherTok{\textless{}{-}}\NormalTok{ rcompanion}\SpecialCharTok{::}\FunctionTok{epsilonSquared}\NormalTok{(ordinal\_x,  nominal\_y)}
\FunctionTok{print}\NormalTok{(epsilon\_squared)}
\CommentTok{\#\textgreater{} epsilon.squared }
\CommentTok{\#\textgreater{}         0.00446}
\end{Highlighting}
\end{Shaded}

\hypertarget{goodman-kruskals-gamma}{%
\subparagraph{Goodman-Kruskal's Gamma}\label{goodman-kruskals-gamma}}

\textbf{Goodman-Kruskal's Gamma} measures the strength of association between two \textbf{ordinal variables}. It is a rank-based measure, focusing only on \textbf{concordant} and \textbf{discordant} pairs while ignoring ties.

Formula:

\[
\gamma = \frac{C - D}{C + D}
\]

where

\begin{itemize}
\item
  \(C\): Number of concordant pairs (where ranks move in the same direction).
\item
  \(D\): Number of discordant pairs (where ranks move in opposite directions).
\end{itemize}

Use Case:

\begin{itemize}
\tightlist
\item
  Suitable for ordinal variables with many ties.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{=} \DecValTok{100} \CommentTok{\# (sample size)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{dt }\OtherTok{=} \FunctionTok{table}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(}
    \AttributeTok{A =} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{size =}\NormalTok{ n), }\CommentTok{\# ordinal}
    \AttributeTok{B =} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{size =}\NormalTok{ n)  }\CommentTok{\# ordinal}
\NormalTok{)) }
\NormalTok{dt}
\CommentTok{\#\textgreater{}    B}
\CommentTok{\#\textgreater{} A    1  2  3}
\CommentTok{\#\textgreater{}   1  7 11  9}
\CommentTok{\#\textgreater{}   2 11  6 14}
\CommentTok{\#\textgreater{}   3  7 11  4}
\CommentTok{\#\textgreater{}   4  6  4 10}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Compute Goodman{-}Kruskal\textquotesingle{}s Gamma}
\FunctionTok{library}\NormalTok{(DescTools)}
\NormalTok{goodman\_kruskal\_gamma }\OtherTok{\textless{}{-}} \FunctionTok{GoodmanKruskalGamma}\NormalTok{(dt, }\AttributeTok{conf.level =} \FloatTok{0.95}\NormalTok{)}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Goodman{-}Kruskal\textquotesingle{}s Gamma:"}\NormalTok{, goodman\_kruskal\_gamma, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Goodman{-}Kruskal\textquotesingle{}s Gamma: 0.006781013 {-}0.2290321 0.2425941}
\end{Highlighting}
\end{Shaded}

\hypertarget{somers-d}{%
\subparagraph{Somers' D}\label{somers-d}}

\textbf{Somers' D} (also called Somers' Delta) extends Kendall's Tau by focusing on \textbf{asymmetric relationships}, where one variable is a predictor and the other is a response.

Formula:

\[
D_{XY} = \frac{C - D}{C + D + T_Y}
\]

where

\begin{itemize}
\tightlist
\item
  \(T_Y\): Tied pairs in the dependent variable.
\end{itemize}

Use Case:

\begin{itemize}
\tightlist
\item
  Appropriate when there is a clear predictor-response relationship between two ordinal variables.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Compute Somers\textquotesingle{} D}
\NormalTok{somers\_d }\OtherTok{\textless{}{-}} \FunctionTok{SomersDelta}\NormalTok{(dt, }\AttributeTok{conf.level =} \FloatTok{0.95}\NormalTok{)}
\NormalTok{somers\_d}
\CommentTok{\#\textgreater{}       somers       lwr.ci       upr.ci }
\CommentTok{\#\textgreater{}  0.005115859 {-}0.172800185  0.183031903}
\end{Highlighting}
\end{Shaded}

\hypertarget{kendalls-tau-b}{%
\subparagraph{Kendall's Tau-b}\label{kendalls-tau-b}}

\textbf{Kendall's Tau-b} is an extension of Kendall's Tau that accounts for \textbf{ties} in the data.

Formula:

\[
\tau_b = \frac{C - D}{\sqrt{(C + D+ T_X) (C + D + T_Y)}}
\]

where

\begin{itemize}
\tightlist
\item
  \(T_X, T_Y\): Tied pairs in each variable.
\end{itemize}

Use Case:

\begin{itemize}
\tightlist
\item
  Use when ordinal data contains ties.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Compute Kendall\textquotesingle{}s Tau{-}b}
\NormalTok{kendalls\_tau\_b }\OtherTok{\textless{}{-}} \FunctionTok{KendallTauB}\NormalTok{(dt, }\AttributeTok{conf.level =} \FloatTok{0.95}\NormalTok{)}
\NormalTok{kendalls\_tau\_b}
\CommentTok{\#\textgreater{}        tau\_b       lwr.ci       upr.ci }
\CommentTok{\#\textgreater{}  0.004839732 {-}0.163472443  0.173151906}
\end{Highlighting}
\end{Shaded}

\hypertarget{yules-q-and-y}{%
\subparagraph{Yule's Q and Y}\label{yules-q-and-y}}

\textbf{Yule's Q} and \textbf{Yule's Y} are specialized measures for \textbf{2x2 contingency tables}. They are simplified versions of Goodman-Kruskal's Gamma, designed for binary ordinal variables.\hspace{0pt}\hspace{0pt}

Use Case:

\begin{itemize}
\tightlist
\item
  Ideal for binary ordinal variables in a 2x2 table.
\end{itemize}

Special version \((2 \times 2)\) of the \protect\hyperlink{goodman-kruskals-gamma}{Goodman Kruskal's Gamma} coefficient.

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
& Variable 1 & \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Variable 2} & a & b \\
& c & d \\
\end{longtable}

\[
\text{Yule's Q} = \frac{ad - bc}{ad + bc}
\]

We typically use Yule's \(Q\) in practice while Yule's Y has the following relationship with \(Q\).

\[
\text{Yule's Y} = \frac{\sqrt{ad} - \sqrt{bc}}{\sqrt{ad} + \sqrt{bc}}
\]

\[
Q = \frac{2Y}{1 + Y^2}
\]

\[
Y = \frac{1 = \sqrt{1-Q^2}}{Q}
\]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create 2x2 table}
\NormalTok{dt\_binary }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{A =} \FunctionTok{sample}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{size =}\NormalTok{ n),}
  \AttributeTok{B =} \FunctionTok{sample}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{size =}\NormalTok{ n)}
\NormalTok{))}

\CommentTok{\# Compute Yule\textquotesingle{}s Q}
\NormalTok{yules\_q }\OtherTok{\textless{}{-}} \FunctionTok{YuleQ}\NormalTok{(dt\_binary)}
\NormalTok{yules\_q}
\CommentTok{\#\textgreater{} [1] {-}0.07667474}
\end{Highlighting}
\end{Shaded}

\hypertarget{tetrachoric-correlation}{%
\subparagraph{Tetrachoric Correlation}\label{tetrachoric-correlation}}

\textbf{Tetrachoric Correlation} measures the association between two \textbf{binary variables} by assuming they represent thresholds of underlying continuous normal distributions. It is a special case of \protect\hyperlink{polychoric-correlation}{Polychoric Correlation} when both variables are binary

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Simulate binary data}
\FunctionTok{library}\NormalTok{(psych)}
\NormalTok{data\_binary }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{A =} \FunctionTok{sample}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{size =}\NormalTok{ n),}
  \AttributeTok{B =} \FunctionTok{sample}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{size =}\NormalTok{ n)}
\NormalTok{)}

\CommentTok{\# Compute Tetrachoric Correlation}
\NormalTok{tetrachoric\_corr }\OtherTok{\textless{}{-}} \FunctionTok{tetrachoric}\NormalTok{(data\_binary)}
\FunctionTok{print}\NormalTok{(tetrachoric\_corr)}
\CommentTok{\#\textgreater{} Call: tetrachoric(x = data\_binary)}
\CommentTok{\#\textgreater{} tetrachoric correlation }
\CommentTok{\#\textgreater{}   A    B   }
\CommentTok{\#\textgreater{} A 1.00     }
\CommentTok{\#\textgreater{} B 0.31 1.00}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  with tau of }
\CommentTok{\#\textgreater{}      A      B }
\CommentTok{\#\textgreater{}  0.126 {-}0.025}
\end{Highlighting}
\end{Shaded}

\hypertarget{polychoric-correlation}{%
\subparagraph{Polychoric Correlation}\label{polychoric-correlation}}

\textbf{Polychoric Correlation} measures the association between \textbf{ordinal variables} by assuming they are discretized versions of latent, normally distributed continuous variables.

Assumptions:

\begin{itemize}
\tightlist
\item
  The ordinal variables represent categories of an underlying normal distribution.
\end{itemize}

Use Case:

\begin{itemize}
\tightlist
\item
  Suitable for ordinal variables with a natural order.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Simulate ordinal data}
\FunctionTok{library}\NormalTok{(polycor)}
\NormalTok{data\_ordinal }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{A =} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{size =}\NormalTok{ n),}
  \AttributeTok{B =} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{size =}\NormalTok{ n)}
\NormalTok{)}

\CommentTok{\# Compute Polychoric Correlation}
\NormalTok{polychoric\_corr }\OtherTok{\textless{}{-}} \FunctionTok{polychor}\NormalTok{(data\_ordinal}\SpecialCharTok{$}\NormalTok{A, data\_ordinal}\SpecialCharTok{$}\NormalTok{B)}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Polychoric Correlation:"}\NormalTok{, polychoric\_corr, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Polychoric Correlation: 0.1908334}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2586}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3103}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4224}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Metric
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Variable Types
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Use Case
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Spearman's Correlation} & Ordinal vs.~Ordinal & Non-linear, monotonic relationships. \\
\textbf{Kendall's Tau} & Ordinal vs.~Ordinal & Non-linear, monotonic relationships with ties. \\
\textbf{Gamma Statistic} & Ordinal vs.~Ordinal & Handles data with many ties effectively. \\
\textbf{Freeman's Theta} & Ordinal vs.~Nominal & Mixed data types (ordinal and nominal). \\
\textbf{Epsilon-Squared} & Ordinal vs.~Nominal & Variance explained by nominal groups. \\
\textbf{Goodman-Kruskal's Gamma} & Ordinal vs.~Ordinal & Strong association; ignores ties. \\
\textbf{Somers' D} & Ordinal Predictor and Response & Asymmetric association. \\
\textbf{Kendall's Tau-b} & Ordinal vs.~Ordinal & Adjusts for ties in data. \\
\textbf{Yule's Q} & Binary Ordinal vs.~Binary Ordinal & Special case for 2x2 tables. \\
\textbf{Tetrachoric Correlation} & Binary vs.~Binary & Binary ordinal variables. \\
\textbf{Polychoric Correlation} & Ordinal vs.~Ordinal & Continuous latent structure. \\
\end{longtable}

\hypertarget{general-approach-to-bivariate-statistics}{%
\subsection{General Approach to Bivariate Statistics}\label{general-approach-to-bivariate-statistics}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\FunctionTok{data}\NormalTok{(}\StringTok{"mtcars"}\NormalTok{)}
\NormalTok{df }\OtherTok{=}\NormalTok{ mtcars }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(cyl, vs, carb)}


\NormalTok{df\_factor }\OtherTok{=}\NormalTok{ df }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}
        \AttributeTok{cyl =} \FunctionTok{factor}\NormalTok{(cyl),}
        \AttributeTok{vs =} \FunctionTok{factor}\NormalTok{(vs),}
        \AttributeTok{carb =} \FunctionTok{factor}\NormalTok{(carb)}
\NormalTok{    )}
\CommentTok{\# summary(df)}
\FunctionTok{str}\NormalTok{(df)}
\CommentTok{\#\textgreater{} \textquotesingle{}data.frame\textquotesingle{}:    32 obs. of  3 variables:}
\CommentTok{\#\textgreater{}  $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...}
\CommentTok{\#\textgreater{}  $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...}
\CommentTok{\#\textgreater{}  $ carb: num  4 4 1 1 2 1 4 2 2 4 ...}
\FunctionTok{str}\NormalTok{(df\_factor)}
\CommentTok{\#\textgreater{} \textquotesingle{}data.frame\textquotesingle{}:    32 obs. of  3 variables:}
\CommentTok{\#\textgreater{}  $ cyl : Factor w/ 3 levels "4","6","8": 2 2 1 2 3 2 3 1 1 2 ...}
\CommentTok{\#\textgreater{}  $ vs  : Factor w/ 2 levels "0","1": 1 1 2 2 1 2 1 2 2 2 ...}
\CommentTok{\#\textgreater{}  $ carb: Factor w/ 6 levels "1","2","3","4",..: 4 4 1 1 2 1 4 2 2 4 ...}
\end{Highlighting}
\end{Shaded}

Get the correlation table for continuous variables only

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(df)}
\CommentTok{\#\textgreater{}             cyl         vs       carb}
\CommentTok{\#\textgreater{} cyl   1.0000000 {-}0.8108118  0.5269883}
\CommentTok{\#\textgreater{} vs   {-}0.8108118  1.0000000 {-}0.5696071}
\CommentTok{\#\textgreater{} carb  0.5269883 {-}0.5696071  1.0000000}

\CommentTok{\# only complete obs}
\CommentTok{\# cor(df, use = "complete.obs")}
\end{Highlighting}
\end{Shaded}

Alternatively, you can also have the

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Hmisc}\SpecialCharTok{::}\FunctionTok{rcorr}\NormalTok{(}\FunctionTok{as.matrix}\NormalTok{(df), }\AttributeTok{type =} \StringTok{"pearson"}\NormalTok{)}
\CommentTok{\#\textgreater{}        cyl    vs  carb}
\CommentTok{\#\textgreater{} cyl   1.00 {-}0.81  0.53}
\CommentTok{\#\textgreater{} vs   {-}0.81  1.00 {-}0.57}
\CommentTok{\#\textgreater{} carb  0.53 {-}0.57  1.00}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} n= 32 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} P}
\CommentTok{\#\textgreater{}      cyl    vs     carb  }
\CommentTok{\#\textgreater{} cyl         0.0000 0.0019}
\CommentTok{\#\textgreater{} vs   0.0000        0.0007}
\CommentTok{\#\textgreater{} carb 0.0019 0.0007}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{modelsummary}\SpecialCharTok{::}\FunctionTok{datasummary\_correlation}\NormalTok{(df)}
\end{Highlighting}
\end{Shaded}

\begin{table}
\centering
\begin{tabular}[t]{lrrr}
\toprule
  & cyl & vs & carb\\
\midrule
cyl & 1 & . & .\\
vs & \num{-.81} & 1 & .\\
carb & \num{.53} & \num{-.57} & 1\\
\bottomrule
\end{tabular}
\end{table}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ggcorrplot}\SpecialCharTok{::}\FunctionTok{ggcorrplot}\NormalTok{(}\FunctionTok{cor}\NormalTok{(df))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{03-descriptive-stat_files/figure-latex/unnamed-chunk-46-1} \end{center}

Comparing correlations between different types of variables (e.g., continuous vs.~categorical) poses unique challenges. One key issue is ensuring that methods are appropriate for the nature of the variables being analyzed. Another challenge lies in detecting non-linear relationships, as traditional correlation measures, such as Pearson's correlation coefficient, are designed to assess linear associations.

To address these challenges, a potential solution is to utilize \textbf{mutual information} from information theory. Mutual information quantifies how much knowing one variable reduces the uncertainty of another, providing a more general measure of association that accommodates both linear and non-linear relationships.

\hypertarget{approximating-mutual-information}{%
\subsubsection{Approximating Mutual Information}\label{approximating-mutual-information}}

We can approximate mutual information using the following relationship:

\[
\downarrow \text{Prediction Error} \approx \downarrow \text{Uncertainty} \approx \uparrow \text{Association Strength}
\]

This principle underpins the \href{https://rviews.rstudio.com/2021/04/15/an-alternative-to-the-correlation-coefficient-that-works-for-numeric-and-categorical-variables/}{X2Y metric}, which is implemented through the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Predict} \(y\) without \(x\) (baseline model):

  \begin{itemize}
  \tightlist
  \item
    If \(y\) is continuous, predict the mean of \(y\).\\
  \item
    If \(y\) is categorical, predict the mode of \(y\).
  \end{itemize}
\item
  \textbf{Predict} \(y\) with \(x\) using a model (e.g., linear regression, random forest, etc.).
\item
  \textbf{Calculate the difference in prediction error} between steps 1 and 2. This difference reflects the reduction in uncertainty about \(y\) when \(x\) is included, serving as a measure of association strength.
\end{enumerate}

\hypertarget{generalizing-across-variable-types}{%
\subsubsection{Generalizing Across Variable Types}\label{generalizing-across-variable-types}}

To construct a comprehensive framework that handles different variable combinations, such as:

\begin{itemize}
\tightlist
\item
  Continuous vs.~continuous
\item
  Categorical vs.~continuous
\item
  Continuous vs.~categorical
\item
  Categorical vs.~categorical
\end{itemize}

a flexible modeling approach is required. \textbf{Classification and Regression Trees (CART)} are particularly well-suited for this purpose, as they can accommodate both continuous and categorical predictors and outcomes. However, other models, such as random forests or generalized additive models (GAMs), may also be employed.

\hypertarget{limitations-of-the-approach}{%
\subsubsection{Limitations of the Approach}\label{limitations-of-the-approach}}

Despite its strengths, this approach has some limitations:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Asymmetry:}\\
  The measure is not symmetric, meaning \((x, y) \neq (y, x)\).
\item
  \textbf{Comparability:}\\
  Different variable pairs may yield metrics that are not directly comparable. For instance, continuous outcomes often use metrics like Mean Absolute Error (MAE), while categorical outcomes use measures like misclassification error.
\end{enumerate}

These limitations should be considered when interpreting results, especially in multi-variable or mixed-data contexts.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ppsr)}
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{iris }\OtherTok{\textless{}{-}}\NormalTok{ iris }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{)}

\CommentTok{\# ppsr::score\_df(iris) \# if you want a dataframe}
\NormalTok{ppsr}\SpecialCharTok{::}\FunctionTok{score\_matrix}\NormalTok{(iris,}
                   \AttributeTok{do\_parallel =} \ConstantTok{TRUE}\NormalTok{,}
                   \AttributeTok{n\_cores =}\NormalTok{ parallel}\SpecialCharTok{::}\FunctionTok{detectCores}\NormalTok{() }\SpecialCharTok{/} \DecValTok{2}\NormalTok{)}
\CommentTok{\#\textgreater{}              Sepal.Length Sepal.Width Petal.Length}
\CommentTok{\#\textgreater{} Sepal.Length   1.00000000  0.04632352    0.5491398}
\CommentTok{\#\textgreater{} Sepal.Width    0.06790301  1.00000000    0.2376991}
\CommentTok{\#\textgreater{} Petal.Length   0.61608360  0.24263851    1.0000000}

\CommentTok{\# if you want a similar correlation matrix}
\NormalTok{ppsr}\SpecialCharTok{::}\FunctionTok{score\_matrix}\NormalTok{(df,}
                   \AttributeTok{do\_parallel =} \ConstantTok{TRUE}\NormalTok{,}
                   \AttributeTok{n\_cores =}\NormalTok{ parallel}\SpecialCharTok{::}\FunctionTok{detectCores}\NormalTok{() }\SpecialCharTok{/} \DecValTok{2}\NormalTok{)}
\CommentTok{\#\textgreater{}             cyl        vs      carb}
\CommentTok{\#\textgreater{} cyl  1.00000000 0.3982789 0.2092533}
\CommentTok{\#\textgreater{} vs   0.02514286 1.0000000 0.2000000}
\CommentTok{\#\textgreater{} carb 0.30798148 0.2537309 1.0000000}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{corrplot}\SpecialCharTok{::}\FunctionTok{corrplot}\NormalTok{(}\FunctionTok{cor}\NormalTok{(df))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{03-descriptive-stat_files/figure-latex/unnamed-chunk-48-1} \end{center}

Alternatively,

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PerformanceAnalytics}\SpecialCharTok{::}\FunctionTok{chart.Correlation}\NormalTok{(df, }\AttributeTok{histogram =}\NormalTok{ T, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{03-descriptive-stat_files/figure-latex/unnamed-chunk-49-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{heatmap}\NormalTok{(}\FunctionTok{as.matrix}\NormalTok{(df))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{03-descriptive-stat_files/figure-latex/unnamed-chunk-50-1} \end{center}

More general form,

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ppsr}\SpecialCharTok{::}\FunctionTok{visualize\_pps}\NormalTok{(}
    \AttributeTok{df =}\NormalTok{ iris,}
    \AttributeTok{do\_parallel =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{n\_cores =}\NormalTok{ parallel}\SpecialCharTok{::}\FunctionTok{detectCores}\NormalTok{() }\SpecialCharTok{/} \DecValTok{2}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{03-descriptive-stat_files/figure-latex/unnamed-chunk-51-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ppsr}\SpecialCharTok{::}\FunctionTok{visualize\_correlations}\NormalTok{(}
    \AttributeTok{df =}\NormalTok{ iris}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{03-descriptive-stat_files/figure-latex/unnamed-chunk-52-1} \end{center}

Both heat map and correlation at the same time

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ppsr}\SpecialCharTok{::}\FunctionTok{visualize\_both}\NormalTok{(}
    \AttributeTok{df =}\NormalTok{ iris,}
    \AttributeTok{do\_parallel =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{n\_cores =}\NormalTok{ parallel}\SpecialCharTok{::}\FunctionTok{detectCores}\NormalTok{() }\SpecialCharTok{/} \DecValTok{2}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{03-descriptive-stat_files/figure-latex/unnamed-chunk-53-1} \end{center}

More elaboration with \texttt{ggplot2}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ppsr}\SpecialCharTok{::}\FunctionTok{visualize\_pps}\NormalTok{(}
    \AttributeTok{df =}\NormalTok{ iris,}
    \AttributeTok{color\_value\_high =} \StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{,}
    \AttributeTok{color\_value\_low =} \StringTok{\textquotesingle{}yellow\textquotesingle{}}\NormalTok{,}
    \AttributeTok{color\_text =} \StringTok{\textquotesingle{}black\textquotesingle{}}
\NormalTok{) }\SpecialCharTok{+}
\NormalTok{    ggplot2}\SpecialCharTok{::}\FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
\NormalTok{    ggplot2}\SpecialCharTok{::}\FunctionTok{theme}\NormalTok{(}\AttributeTok{plot.background =} 
\NormalTok{                       ggplot2}\SpecialCharTok{::}\FunctionTok{element\_rect}\NormalTok{(}\AttributeTok{fill =} \StringTok{"lightgrey"}\NormalTok{)) }\SpecialCharTok{+}
\NormalTok{    ggplot2}\SpecialCharTok{::}\FunctionTok{theme}\NormalTok{(}\AttributeTok{title =}\NormalTok{ ggplot2}\SpecialCharTok{::}\FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{15}\NormalTok{)) }\SpecialCharTok{+}
\NormalTok{    ggplot2}\SpecialCharTok{::}\FunctionTok{labs}\NormalTok{(}
        \AttributeTok{title =} \StringTok{\textquotesingle{}Correlation aand Heatmap\textquotesingle{}}\NormalTok{,}
        \AttributeTok{subtitle =} \StringTok{\textquotesingle{}Subtitle\textquotesingle{}}\NormalTok{,}
        \AttributeTok{caption =} \StringTok{\textquotesingle{}Caption\textquotesingle{}}\NormalTok{,}
        \AttributeTok{x =} \StringTok{\textquotesingle{}More info\textquotesingle{}}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{03-descriptive-stat_files/figure-latex/unnamed-chunk-54-1} \end{center}

\hypertarget{basic-statistical-inference}{%
\chapter{Basic Statistical Inference}\label{basic-statistical-inference}}

Statistical inference involves drawing conclusions about population parameters based on sample data. The two primary goals of inference are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Making inferences} about the true parameter value (\(\beta\)) based on our estimator or estimate:

  \begin{itemize}
  \tightlist
  \item
    This involves interpreting the sample-derived estimate to understand the population parameter.
  \item
    Examples include estimating population means, variances, or proportions.
  \end{itemize}
\item
  \textbf{Testing whether underlying assumptions hold true}, including:

  \begin{itemize}
  \tightlist
  \item
    Assumptions about the true population parameters (e.g., \(\mu\), \(\sigma^2\)).
  \item
    Assumptions about random variables (e.g., independence, normality).
  \item
    Assumptions about the model specification (e.g., linearity in regression).
  \end{itemize}
\end{enumerate}

\textbf{Note}: Statistical testing does not:

\begin{itemize}
\item
  Confirm with absolute certainty that a hypothesis is true or false.
\item
  Interpret the magnitude of the estimated value in economic, practical, or business contexts without additional analysis.

  \begin{itemize}
  \item
    \textbf{Statistical significance}: Refers to whether an observed effect is unlikely due to chance.
  \item
    \textbf{Practical significance}: Focuses on the real-world importance of the effect.
  \end{itemize}
\end{itemize}

\textbf{Example}:

\begin{itemize}
\tightlist
\item
  A marketing campaign increases sales by \(0.5\%\), which is statistically significant (\(p < 0.05\)). However, in a small market, this may lack practical significance.
\end{itemize}

Instead, inference provides a framework for making probabilistic statements about population parameters, given sample data.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{hypothesis-testing-framework}{%
\section{Hypothesis Testing Framework}\label{hypothesis-testing-framework}}

Hypothesis testing is one of the fundamental tools in statistics. It provides a formal procedure to test claims or assumptions (hypotheses) about population parameters using sample data. This process is essential in various fields, including business, medicine, and social sciences, as it helps answer questions like ``Does a new marketing strategy improve sales?'' or ``Is there a significant difference in test scores between two teaching methods?''

The goal of hypothesis testing is to make decisions or draw conclusions about a population based on sample data. This is necessary because we rarely have access to the entire population. For example, if a company wants to determine whether a new advertising campaign increases sales, it might analyze data from a sample of stores rather than every store globally.

\textbf{Key Steps in Hypothesis Testing}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Formulate Hypotheses}: Define the null and alternative hypotheses.
\item
  \textbf{Choose a Significance Level} (\(\alpha\)): Determine the acceptable probability of making a Type I error.
\item
  \textbf{Select a Test Statistic}: Identify the appropriate statistical test based on the data and hypotheses.
\item
  \textbf{Define the Rejection Region}: Specify the range of values for which the null hypothesis will be rejected.
\item
  \textbf{Compute the Test Statistic}: Use sample data to calculate the test statistic.
\item
  \textbf{Make a Decision}: Compare the test statistic to the critical value or use the p-value to decide whether to reject or fail to reject the null hypothesis.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{null-and-alternative-hypotheses}{%
\subsection{Null and Alternative Hypotheses}\label{null-and-alternative-hypotheses}}

At the heart of hypothesis testing lies the formulation of two competing hypotheses:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Null Hypothesis (}\(H_0\)):

  \begin{itemize}
  \tightlist
  \item
    Represents the current state of knowledge, status quo, or no effect.
  \item
    It is assumed true unless there is strong evidence against it.
  \item
    Examples:

    \begin{itemize}
    \tightlist
    \item
      \(H_0: \mu_1 = \mu_2\) (no difference in means between two groups).
    \item
      \(H_0: \beta = 0\) (a predictor variable has no effect in a regression model).
    \end{itemize}
  \item
    Think of \(H_0\) as the ``default assumption.''
  \end{itemize}
\item
  \textbf{Alternative Hypothesis (}\(H_a\) or \(H_1\)):

  \begin{itemize}
  \tightlist
  \item
    Represents a claim that contradicts the null hypothesis.
  \item
    It is what you are trying to prove or find evidence for.
  \item
    Examples:

    \begin{itemize}
    \tightlist
    \item
      \(H_a: \mu_1 \neq \mu_2\) (means of two groups are different).
    \item
      \(H_a: \beta \neq 0\) (a predictor variable has an effect).
    \end{itemize}
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{errors-in-hypothesis-testing}{%
\subsection{Errors in Hypothesis Testing}\label{errors-in-hypothesis-testing}}

Hypothesis testing involves decision-making under uncertainty, meaning there is always a risk of making errors. These errors are classified into two types:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Type I Error} (\(\alpha\)):

  \begin{itemize}
  \tightlist
  \item
    Occurs when the null hypothesis is rejected, even though it is true.
  \item
    Example: Concluding that a medication is effective when it actually has no effect.
  \item
    The probability of making a Type I error is denoted by \(\alpha\), called the \textbf{significance level} (commonly set at 0.05 or 5\%).
  \end{itemize}
\item
  \textbf{Type II Error} (\(\beta\)):

  \begin{itemize}
  \tightlist
  \item
    Occurs when the null hypothesis is not rejected, but the alternative hypothesis is true.
  \item
    Example: Failing to detect that a medication is effective when it actually works.
  \item
    The complement of \(\beta\) is called the \textbf{power} of the test (\(1 - \beta\)), representing the probability of correctly rejecting the null hypothesis.
  \end{itemize}
\end{enumerate}

\textbf{Analogy: The Legal System}

To make this concept more intuitive, consider the analogy of a courtroom:

\begin{itemize}
\tightlist
\item
  \textbf{Null Hypothesis (}\(H_0\)): The defendant is innocent.
\item
  \textbf{Alternative Hypothesis (}\(H_a\)): The defendant is guilty.
\item
  \textbf{Type I Error:} Convicting an innocent person (false positive).
\item
  \textbf{Type II Error:} Letting a guilty person go free (false negative).
\end{itemize}

Balancing \(\alpha\) and \(\beta\) is critical in hypothesis testing, as reducing one often increases the other. For example, if you make it harder to reject \(H_0\) (reducing \(\alpha\)), you increase the chance of failing to detect a true effect (increasing \(\beta\)).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{the-role-of-distributions-in-hypothesis-testing}{%
\subsection{The Role of Distributions in Hypothesis Testing}\label{the-role-of-distributions-in-hypothesis-testing}}

Distributions play a fundamental role in hypothesis testing because they provide a mathematical model for understanding how a test statistic behaves under the null hypothesis (\(H_0\)). Without distributions, it would be impossible to determine whether the observed results are due to random chance or provide evidence to reject the null hypothesis.

\hypertarget{expected-outcomes}{%
\subsubsection{Expected Outcomes}\label{expected-outcomes}}

One of the key reasons distributions are so crucial is that they describe the range of values a test statistic is likely to take when \(H_0\) is true. This helps us understand what is considered ``normal'' variation in the data due to random chance. For example:

\begin{itemize}
\tightlist
\item
  Imagine you are conducting a study to test whether a new marketing strategy increases the average monthly sales. Under the null hypothesis, you assume the new strategy has no effect, and the average sales remain unchanged.
\item
  When you collect a sample and calculate the test statistic, you compare it to the expected distribution (e.g., the normal distribution for a \(z\)-test). This distribution shows the range of test statistic values that are likely to occur purely due to random fluctuations in the data, assuming \(H_0\) is true.
\end{itemize}

By providing this baseline of what is ``normal,'' distributions allow us to identify unusual results that may indicate the null hypothesis is false.

\hypertarget{critical-values-and-rejection-regions}{%
\subsubsection{Critical Values and Rejection Regions}\label{critical-values-and-rejection-regions}}

Distributions also help define critical values and rejection regions in hypothesis testing. Critical values are specific points on the distribution that mark the boundaries of the rejection region. The rejection region is the range of values for the test statistic that lead us to reject \(H_0\).

The location of these critical values depends on:

\begin{itemize}
\item
  The \textbf{level of significance} (\(\alpha\)), which is the probability of rejecting \(H_0\) when it is true (a Type I error).
\item
  The shape of the test statistic's distribution under \(H_0\).
\end{itemize}

For example:

\begin{itemize}
\tightlist
\item
  In a one-tailed \(z\)-test with \(\alpha = 0.05\), the critical value is approximately \(1.645\) for a standard normal distribution. If the calculated test statistic exceeds this value, we reject \(H_0\) because such a result would be very unlikely under \(H_0\).
\end{itemize}

Distributions help us visually and mathematically determine these critical points. By examining the distribution, we can see where the rejection region lies and what the probability is of observing a value in that region by random chance alone.

\hypertarget{p-values}{%
\subsubsection{P-values}\label{p-values}}

The p-value, a central concept in hypothesis testing, is directly derived from the distribution of the test statistic under \(H_0\). The p-value represents the probability of observing a test statistic as extreme as (or more extreme than) the one calculated, assuming \(H_0\) is true.

The \textbf{p-value} quantifies the strength of evidence against \(H_0\). It represents the probability of observing a test statistic as extreme as (or more extreme than) the one calculated, assuming \(H_0\) is true.

\begin{itemize}
\tightlist
\item
  \textbf{Small p-value} (\textbf{\textless{}} \(\alpha\)): Strong evidence against \(H_0\); reject \(H_0\).
\item
  \textbf{Large p-value} (\textbf{\textgreater{}} \(\alpha\)): Weak evidence against \(H_0\); fail to reject \(H_0\).
\end{itemize}

For example:

\begin{itemize}
\item
  Suppose you calculate a \(z\)-test statistic of \(2.1\) in a one-tailed test. Using the standard normal distribution, the p-value is the area under the curve to the right of \(z = 2.1\). This area represents the likelihood of observing a result as extreme as \(z = 2.1\) if \(H_0\) is true.
\item
  In this case, the p-value is approximately \(0.0179\). A small p-value (typically less than \(\alpha = 0.05\)) suggests that the observed result is unlikely under \(H_0\) and provides evidence to reject the null hypothesis.
\end{itemize}

\hypertarget{why-does-all-this-matter}{%
\subsubsection{Why Does All This Matter?}\label{why-does-all-this-matter}}

To summarize, distributions are the backbone of hypothesis testing because they allow us to:

\begin{itemize}
\item
  Define what is expected under \(H_0\) by modeling the behavior of the test statistic.
\item
  Identify results that are unlikely to occur by random chance, which leads to the rejection of \(H_0\).
\item
  Calculate p-values to quantify the strength of evidence against \(H_0\).
\end{itemize}

Distributions provide the framework for understanding the role of chance in statistical analysis. They are essential for determining expected outcomes, setting thresholds for decision-making (critical values and rejection regions), and calculating p-values. A solid grasp of distributions will greatly enhance your ability to interpret and conduct hypothesis tests, making it easier to draw meaningful conclusions from data.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{the-test-statistic}{%
\subsection{The Test Statistic}\label{the-test-statistic}}

The test statistic is a crucial component in hypothesis testing, as it quantifies how far the observed data deviates from what we would expect if the null hypothesis (\(H_0\)) were true. Essentially, it provides a standardized way to compare the observed outcomes against the expectations set by \(H_0\), enabling us to assess whether the observed results are likely due to random chance or indicative of a significant effect.

The general formula for a test statistic is:

\[
\text{Test Statistic} = \frac{\text{Observed Value} - \text{Expected Value under } H_0}{\text{Standard Error}}
\]

Each component of this formula has an important role:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Numerator:}

  \begin{itemize}
  \tightlist
  \item
    The numerator represents the difference between the actual data (observed value) and the hypothetical value (expected value) that is assumed under \(H_0\).
  \item
    This difference quantifies the extent of the deviation. A larger deviation suggests stronger evidence against \(H_0\).
  \end{itemize}
\item
  \textbf{Denominator:}

  \begin{itemize}
  \tightlist
  \item
    The denominator is the \textbf{standard error}, which measures the variability or spread of the data. It accounts for factors such as sample size and the inherent randomness of the data.
  \item
    By dividing the numerator by the standard error, the test statistic is standardized, allowing comparisons across different studies, sample sizes, and distributions.
  \end{itemize}
\end{enumerate}

The test statistic plays a central role in determining whether to reject \(H_0\). Once calculated, it is compared to a known distribution (e.g., standard normal distribution for \(z\)-tests or \(t\)-distribution for \(t\)-tests). This comparison allows us to evaluate the likelihood of observing such a test statistic under \(H_0\):

\begin{itemize}
\tightlist
\item
  \textbf{If the test statistic is close to 0:} This indicates that the observed data is very close to what is expected under \(H_0\). There is little evidence to suggest rejecting \(H_0\).
\item
  \textbf{If the test statistic is far from 0 (in the tails of the distribution):} This suggests that the observed data deviates significantly from the expectations under \(H_0\). Such deviations may provide strong evidence against \(H_0\).
\end{itemize}

\hypertarget{why-standardizing-matters}{%
\subsubsection{Why Standardizing Matters}\label{why-standardizing-matters}}

Standardizing the difference between the observed and expected values ensures that the test statistic is not biased by factors such as the scale of measurement or the size of the sample. For instance:

\begin{itemize}
\item
  A raw difference of 5 might be highly significant in one context but negligible in another, depending on the variability (standard error).
\item
  Standardizing ensures that the magnitude of the test statistic reflects both the size of the difference and the reliability of the sample data.
\end{itemize}

\hypertarget{interpreting-the-test-statistic}{%
\subsubsection{Interpreting the Test Statistic}\label{interpreting-the-test-statistic}}

After calculating the test statistic, it is used to:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compare with a critical value: For example, in a \(z\)-test with \(\alpha = 0.05\), the critical values are \(-1.96\) and \(1.96\) for a two-tailed test. If the test statistic falls beyond these values, \(H_0\) is rejected.
\item
  Calculate the p-value: The p-value is derived from the distribution and reflects the probability of observing a test statistic as extreme as the one calculated if \(H_0\) is true.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{critical-values-and-rejection-regions-1}{%
\subsection{Critical Values and Rejection Regions}\label{critical-values-and-rejection-regions-1}}

The \textbf{critical value} is a point on the distribution that separates the rejection region from the non-rejection region:

\begin{itemize}
\tightlist
\item
  \textbf{Rejection Region}: If the test statistic falls in this region, we reject \(H_0\).
\item
  \textbf{Non-Rejection Region}: If the test statistic falls here, we fail to reject \(H_0\).
\end{itemize}

The rejection region depends on the significance level (\(\alpha\)). For a two-tailed test with \(\alpha = 0.05\), the critical values correspond to the top 2.5\% and bottom 2.5\% of the distribution.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{visualizing-hypothesis-testing}{%
\subsection{Visualizing Hypothesis Testing}\label{visualizing-hypothesis-testing}}

Let's create a visualization to tie these concepts together:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Parameters}
\NormalTok{alpha }\OtherTok{\textless{}{-}} \FloatTok{0.05}  \CommentTok{\# Significance level}
\NormalTok{df }\OtherTok{\textless{}{-}} \DecValTok{29}       \CommentTok{\# Degrees of freedom (for t{-}distribution)}
\NormalTok{t\_critical }\OtherTok{\textless{}{-}}
    \FunctionTok{qt}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alpha }\SpecialCharTok{/} \DecValTok{2}\NormalTok{, df)  }\CommentTok{\# Critical value for two{-}tailed test}

\CommentTok{\# Generate t{-}distribution values}
\NormalTok{t\_values }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{1000}\NormalTok{)}
\NormalTok{density }\OtherTok{\textless{}{-}} \FunctionTok{dt}\NormalTok{(t\_values, df)}

\CommentTok{\# Observed test statistic}
\NormalTok{t\_obs }\OtherTok{\textless{}{-}} \FloatTok{2.5}  \CommentTok{\# Example observed test statistic}

\CommentTok{\# Plot the t{-}distribution}
\FunctionTok{plot}\NormalTok{(}
\NormalTok{    t\_values,}
\NormalTok{    density,}
    \AttributeTok{type =} \StringTok{"l"}\NormalTok{,}
    \AttributeTok{lwd =} \DecValTok{2}\NormalTok{,}
    \AttributeTok{col =} \StringTok{"blue"}\NormalTok{,}
    \AttributeTok{main =} \StringTok{"Hypothesis Testing with Distribution"}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"Test Statistic (t{-}value)"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"Density"}\NormalTok{,}
    \AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.4}\NormalTok{)}
\NormalTok{)}

\CommentTok{\# Shade the rejection regions}
\FunctionTok{polygon}\NormalTok{(}\FunctionTok{c}\NormalTok{(t\_values[t\_values }\SpecialCharTok{\textless{}=} \SpecialCharTok{{-}}\NormalTok{t\_critical], }\SpecialCharTok{{-}}\NormalTok{t\_critical),}
        \FunctionTok{c}\NormalTok{(density[t\_values }\SpecialCharTok{\textless{}=} \SpecialCharTok{{-}}\NormalTok{t\_critical], }\DecValTok{0}\NormalTok{),}
        \AttributeTok{col =} \StringTok{"red"}\NormalTok{,}
        \AttributeTok{border =} \ConstantTok{NA}\NormalTok{)}
\FunctionTok{polygon}\NormalTok{(}\FunctionTok{c}\NormalTok{(t\_values[t\_values }\SpecialCharTok{\textgreater{}=}\NormalTok{ t\_critical], t\_critical),}
        \FunctionTok{c}\NormalTok{(density[t\_values }\SpecialCharTok{\textgreater{}=}\NormalTok{ t\_critical], }\DecValTok{0}\NormalTok{),}
        \AttributeTok{col =} \StringTok{"red"}\NormalTok{,}
        \AttributeTok{border =} \ConstantTok{NA}\NormalTok{)}

\CommentTok{\# Add observed test statistic}
\FunctionTok{points}\NormalTok{(}
\NormalTok{    t\_obs,}
    \FunctionTok{dt}\NormalTok{(t\_obs, df),}
    \AttributeTok{col =} \StringTok{"green"}\NormalTok{,}
    \AttributeTok{pch =} \DecValTok{19}\NormalTok{,}
    \AttributeTok{cex =} \FloatTok{1.5}
\NormalTok{)}
\FunctionTok{text}\NormalTok{(}
\NormalTok{    t\_obs,}
    \FunctionTok{dt}\NormalTok{(t\_obs, df) }\SpecialCharTok{+} \FloatTok{0.02}\NormalTok{,}
    \FunctionTok{paste}\NormalTok{(}\StringTok{"Observed t:"}\NormalTok{, }\FunctionTok{round}\NormalTok{(t\_obs, }\DecValTok{2}\NormalTok{)),}
    \AttributeTok{col =} \StringTok{"green"}\NormalTok{,}
    \AttributeTok{pos =} \DecValTok{3}
\NormalTok{)}

\CommentTok{\# Highlight the critical values}
\FunctionTok{abline}\NormalTok{(}
    \AttributeTok{v =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{t\_critical, t\_critical),}
    \AttributeTok{col =} \StringTok{"black"}\NormalTok{,}
    \AttributeTok{lty =} \DecValTok{2}
\NormalTok{)}
\FunctionTok{text}\NormalTok{(}
    \SpecialCharTok{{-}}\NormalTok{t\_critical,}
    \FloatTok{0.05}\NormalTok{,}
    \FunctionTok{paste}\NormalTok{(}\StringTok{"Critical Value:"}\NormalTok{, }\FunctionTok{round}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{t\_critical, }\DecValTok{2}\NormalTok{)),}
    \AttributeTok{pos =} \DecValTok{4}\NormalTok{,}
    \AttributeTok{col =} \StringTok{"black"}
\NormalTok{)}
\FunctionTok{text}\NormalTok{(}
\NormalTok{    t\_critical,}
    \FloatTok{0.05}\NormalTok{,}
    \FunctionTok{paste}\NormalTok{(}\StringTok{"Critical Value:"}\NormalTok{, }\FunctionTok{round}\NormalTok{(t\_critical, }\DecValTok{2}\NormalTok{)),}
    \AttributeTok{pos =} \DecValTok{4}\NormalTok{,}
    \AttributeTok{col =} \StringTok{"black"}
\NormalTok{)}

\CommentTok{\# Calculate p{-}value}
\NormalTok{p\_value }\OtherTok{\textless{}{-}} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{pt}\NormalTok{(}\FunctionTok{abs}\NormalTok{(t\_obs), df))  }\CommentTok{\# Two{-}tailed p{-}value}
\FunctionTok{text}\NormalTok{(}\DecValTok{0}\NormalTok{,}
     \FloatTok{0.35}\NormalTok{,}
     \FunctionTok{paste}\NormalTok{(}\StringTok{"P{-}value:"}\NormalTok{, }\FunctionTok{round}\NormalTok{(p\_value, }\DecValTok{4}\NormalTok{)),}
     \AttributeTok{col =} \StringTok{"blue"}\NormalTok{,}
     \AttributeTok{pos =} \DecValTok{3}\NormalTok{)}

\CommentTok{\# Annotate regions}
\FunctionTok{text}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{3}\NormalTok{,}
     \FloatTok{0.15}\NormalTok{,}
     \StringTok{"Rejection Region"}\NormalTok{,}
     \AttributeTok{col =} \StringTok{"red"}\NormalTok{,}
     \AttributeTok{pos =} \DecValTok{3}\NormalTok{)}
\FunctionTok{text}\NormalTok{(}\DecValTok{3}\NormalTok{, }\FloatTok{0.15}\NormalTok{, }\StringTok{"Rejection Region"}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{pos =} \DecValTok{3}\NormalTok{)}
\FunctionTok{text}\NormalTok{(}\DecValTok{0}\NormalTok{,}
     \FloatTok{0.05}\NormalTok{,}
     \StringTok{"Non{-}Rejection Region"}\NormalTok{,}
     \AttributeTok{col =} \StringTok{"blue"}\NormalTok{,}
     \AttributeTok{pos =} \DecValTok{3}\NormalTok{)}

\CommentTok{\# Add legend}
\FunctionTok{legend}\NormalTok{(}
    \StringTok{"topright"}\NormalTok{,}
    \AttributeTok{legend =} \FunctionTok{c}\NormalTok{(}\StringTok{"Rejection Region"}\NormalTok{, }\StringTok{"Critical Value"}\NormalTok{, }\StringTok{"Observed Test Statistic"}\NormalTok{),}
    \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"red"}\NormalTok{, }\StringTok{"black"}\NormalTok{, }\StringTok{"green"}\NormalTok{),}
    \AttributeTok{lty =} \FunctionTok{c}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\DecValTok{2}\NormalTok{, }\ConstantTok{NA}\NormalTok{),}
    \AttributeTok{pch =} \FunctionTok{c}\NormalTok{(}\DecValTok{15}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\DecValTok{19}\NormalTok{),}
    \AttributeTok{bty =} \StringTok{"n"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{04-basic-inference_files/figure-latex/unnamed-chunk-1-1} \end{center}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{key-concepts-and-definitions}{%
\section{Key Concepts and Definitions}\label{key-concepts-and-definitions}}

\hypertarget{random-sample}{%
\subsection{Random Sample}\label{random-sample}}

A random sample of size \(n\) consists of \(n\) independent observations, each drawn from the same underlying population distribution. Independence ensures that no observation influences another, and identical distribution guarantees that all observations are governed by the same probability rules.

\hypertarget{sample-statistics}{%
\subsection{Sample Statistics}\label{sample-statistics}}

\hypertarget{sample-mean}{%
\subsubsection{Sample Mean}\label{sample-mean}}

The sample mean is a measure of central tendency:

\[
\bar{X} = \frac{\sum_{i=1}^{n} X_i}{n}
\]

\begin{itemize}
\tightlist
\item
  Example: Suppose we measure the heights of 5 individuals (in cm): \(170, 165, 180, 175, 172\). The sample mean is:
\end{itemize}

\[
\bar{X} = \frac{170 + 165 + 180 + 175 + 172}{5} = 172.4 \, \text{cm}.
\]

\hypertarget{sample-median}{%
\subsubsection{Sample Median}\label{sample-median}}

The sample median is the middle value of ordered data:

\[
\tilde{x} = \begin{cases}
\text{Middle observation,} & \text{if } n \text{ is odd}, \\
\text{Average of two middle observations,} & \text{if } n \text{ is even}.
\end{cases}
\]

\hypertarget{sample-variance}{%
\subsubsection{Sample Variance}\label{sample-variance}}

The sample variance measures data spread:

\[
S^2 = \frac{\sum_{i=1}^{n}(X_i - \bar{X})^2}{n-1}
\]

\hypertarget{sample-standard-deviation}{%
\subsubsection{Sample Standard Deviation}\label{sample-standard-deviation}}

The sample standard deviation is the square root of the variance:

\[
S = \sqrt{S^2}
\]

\hypertarget{sample-proportions}{%
\subsubsection{Sample Proportions}\label{sample-proportions}}

Used for categorical data:

\[
\hat{p} = \frac{X}{n} = \frac{\text{Number of successes}}{\text{Sample size}}
\]

\hypertarget{estimators}{%
\subsubsection{Estimators}\label{estimators}}

\begin{itemize}
\tightlist
\item
  \textbf{Point Estimator}: A statistic (\(\hat{\theta}\)) used to estimate a population parameter (\(\theta\)).
\item
  \textbf{Point Estimate}:The numerical value assumed by \(\hat{\theta}\) when evaluated for a given sample.
\item
  \textbf{Unbiased Estimator}: A point estimator \(\hat{\theta}\) is unbiased if \(E(\hat{\theta}) = \theta\).
\end{itemize}

Examples of unbiased estimators:

\begin{itemize}
\item
  \(\bar{X}\) for \(\mu\) (population mean).
\item
  \(S^2\) for \(\sigma^2\) (population variance).
\item
  \(\hat{p}\) for \(p\) (population proportion).
\item
  \(\widehat{p_1-p_2}\) for \(p_1- p_2\) (population proportion difference)
\item
  \(\bar{X_1} - \bar{X_2}\) for \(\mu_1 - \mu_2\) (population mean difference)
\end{itemize}

\textbf{Note}: While \(S^2\) is unbiased for \(\sigma^2\), \(S\) is a biased estimator of \(\sigma\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{distribution-of-the-sample-mean}{%
\subsection{Distribution of the Sample Mean}\label{distribution-of-the-sample-mean}}

The sampling distribution of the mean \(\bar{X}\) depends on:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Population Distribution}:

  \begin{itemize}
  \tightlist
  \item
    If \(X \sim N(\mu, \sigma^2)\), then \(\bar{X} \sim N\left(\mu, \frac{\sigma^2}{n}\right)\).
  \end{itemize}
\item
  \textbf{Central Limit Theorem}:

  \begin{itemize}
  \tightlist
  \item
    For large \(n\), \(\bar{X}\) approximately follows a normal distribution, regardless of the population's shape.
  \end{itemize}
\end{enumerate}

\hypertarget{standard-error-of-the-mean}{%
\subsubsection{Standard Error of the Mean}\label{standard-error-of-the-mean}}

The standard error quantifies variability in \(\bar{X}\):

\[
\sigma_{\bar{X}} = \frac{\sigma}{\sqrt{n}}
\]

\textbf{Example}: - Suppose \(\sigma = 10\) and \(n = 25\). Then: \[
  \sigma_{\bar{X}} = \frac{10}{\sqrt{25}} = 2.
  \]

The smaller the standard error, the more precise our estimate of the population mean.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{one-sample-inference}{%
\section{One-Sample Inference}\label{one-sample-inference}}

\hypertarget{for-single-mean}{%
\subsection{For Single Mean}\label{for-single-mean}}

Consider a scenario where

\[
Y_i \sim \text{i.i.d. } N(\mu, \sigma^2),
\]

where i.i.d. stands for ``independent and identically distributed.'' This model can be expressed as:

\[
Y_i = \mu + \epsilon_i,
\]

where:

\begin{itemize}
\tightlist
\item
  \(\epsilon_i \sim^{\text{i.i.d.}} N(0, \sigma^2)\),
\item
  \(E(Y_i) = \mu\),
\item
  \(\text{Var}(Y_i) = \sigma^2\),
\item
  \(\bar{y} \sim N(\mu, \sigma^2 / n)\).
\end{itemize}

When \(\sigma^2\) is estimated by \(s^2\), the standardized test statistic follows a \(t\)-distribution:

\[
\frac{\bar{y} - \mu}{s / \sqrt{n}} \sim t_{n-1}.
\]

A \(100(1-\alpha)\%\) confidence interval for \(\mu\) is obtained as:

\[
1 - \alpha = P\left(-t_{\alpha/2;n-1} \leq \frac{\bar{y} - \mu}{s / \sqrt{n}} \leq t_{\alpha/2;n-1}\right),
\]

or equivalently,

\[
P\left(\bar{y} - t_{\alpha/2;n-1}\frac{s}{\sqrt{n}} \leq \mu \leq \bar{y} + t_{\alpha/2;n-1}\frac{s}{\sqrt{n}}\right).
\]

The confidence interval is expressed as:

\[
\bar{y} \pm t_{\alpha/2;n-1}\frac{s}{\sqrt{n}},
\]

where \(s / \sqrt{n}\) is the standard error of \(\bar{y}\).

If the experiment were repeated many times, \(100(1-\alpha)\%\) of these intervals would contain \(\mu\).

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2451}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2549}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2451}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2451}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Case
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Confidence Interval \(100(1-\alpha)\%\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Sample Size (Confidence \(\alpha\), Error \(d\))
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Hypothesis Test Statistic
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(\sigma^2\) known, \(X\) normal (or \(n \geq 25\)) & \(\bar{X} \pm z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\) & \(n \approx \frac{z_{\alpha/2}^2 \sigma^2}{d^2}\) & \(z = \frac{\bar{X} - \mu_0}{\sigma / \sqrt{n}}\) \\
\(\sigma^2\) unknown, \(X\) normal (or \(n \geq 25\)) & \(\bar{X} \pm t_{\alpha/2}\frac{s}{\sqrt{n}}\) & \(n \approx \frac{z_{\alpha/2}^2 s^2}{d^2}\) & \(t = \frac{\bar{X} - \mu_0}{s / \sqrt{n}}\) \\
\end{longtable}

\hypertarget{power-in-hypothesis-testing}{%
\subsubsection{Power in Hypothesis Testing}\label{power-in-hypothesis-testing}}

Power (\(\pi(\mu)\)) of a hypothesis test represents the probability of correctly rejecting the null hypothesis (\(H_0\)) when it is false (i.e., when alternative hypothesis \(H_A\) is true). Formally, it is expressed as:

\[ \begin{aligned} \text{Power} &= \pi(\mu) = 1 - \beta \\ &= P(\text{test rejects } H_0|\mu) \\ &= P(\text{test rejects } H_0| H_A \text{ is true}), \end{aligned} \]

where \(\beta\) is the probability of a Type II error (failing to reject \(H_0\) when it is false).

To calculate this probability:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Under} \(H_0\): The distribution of the test statistic is centered around the null parameter (e.g., \(\mu_0\)).
\item
  \textbf{Under} \(H_A\): The test statistic is distributed differently, shifted according to the true value under \(H_A\) (e.g., \(\mu_1\)).
\end{enumerate}

Hence, to evaluate the power, it is crucial to determine the distribution of the test statistic under the alternative hypothesis, \(H_A\).

Below, we derive the power for both one-sided and two-sided z-tests.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{one-sided-z-test}{%
\paragraph{One-Sided z-Test}\label{one-sided-z-test}}

Consider the hypotheses:

\[ H_0: \mu \leq \mu_0 \quad \text{vs.} \quad H_A: \mu > \mu_0 \]

The power for a one-sided z-test is derived as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The test rejects \(H_0\) if \(\bar{y} > \mu_0 + z_{\alpha} \frac{\sigma}{\sqrt{n}}\), where \(z_{\alpha}\) is the critical value for the test at the significance level \(\alpha\).
\item
  Under the alternative hypothesis, the distribution of \(\bar{y}\) is centered at \(\mu\), with standard deviation \(\frac{\sigma}{\sqrt{n}}\).
\item
  The power is then:
\end{enumerate}

\[
\begin{aligned} 
\pi(\mu) &= P\left(\bar{y} > \mu_0 + z_{\alpha} \frac{\sigma}{\sqrt{n}} \middle| \mu \right) \\ 
&= P\left(Z > z_{\alpha} + \frac{\mu_0 - \mu}{\sigma / \sqrt{n}} \middle| \mu \right), \quad \text{where } Z = \frac{\bar{y} - \mu}{\sigma / \sqrt{n}} \\ 
&= 1 - \Phi\left(z_{\alpha} + \frac{(\mu_0 - \mu)\sqrt{n}}{\sigma}\right) \\ 
&= \Phi\left(-z_{\alpha} + \frac{(\mu - \mu_0)\sqrt{n}}{\sigma}\right).
\end{aligned}
\]

Here, we use the symmetry of the standard normal distribution: \(1 - \Phi(x) = \Phi(-x)\).

Suppose we wish to show that the mean response \(\mu\) under the treatment is higher than the mean response \(\mu_0\) without treatment (i.e., the treatment effect \(\delta = \mu - \mu_0\) is large).

Since power is an increasing function of \(\mu - \mu_0\), it suffices to find the sample size \(n\) that achieves the desired power \(1 - \beta\) at \(\mu = \mu_0 + \delta\). The power at \(\mu = \mu_0 + \delta\) is:

\[
\pi(\mu_0 + \delta) = \Phi\left(-z_{\alpha} + \frac{\delta \sqrt{n}}{\sigma}\right) = 1 - \beta
\]

Given \(\Phi(z_{\beta}) = 1 - \beta\), we have:

\[
-z_{\alpha} + \frac{\delta \sqrt{n}}{\sigma} = z_{\beta}
\]

Solving for \(n\), we obtain:

\[
n = \left(\frac{(z_{\alpha} + z_{\beta})\sigma}{\delta}\right)^2
\]

Larger sample sizes are required when:

\begin{itemize}
\tightlist
\item
  The sample variability is large (\(\sigma\) is large).
\item
  The significance level \(\alpha\) is small (\(z_{\alpha}\) is large).
\item
  The desired power \(1 - \beta\) is large (\(z_{\beta}\) is large).
\item
  The magnitude of the effect is small (\(\delta\) is small).
\end{itemize}

In practice, \(\delta\) and \(\sigma\) are often unknown. To estimate \(\sigma\), you can:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Use prior studies or pilot studies.
\item
  Approximate \(\sigma\) based on the anticipated range of the observations (excluding outliers). For normally distributed data, dividing the range by 4 provides a reasonable estimate of \(\sigma\).
\end{enumerate}

These considerations ensure the test is adequately powered to detect meaningful effects while balancing practical constraints such as sample size.

\hypertarget{two-sided-z-test}{%
\paragraph{Two-Sided z-Test}\label{two-sided-z-test}}

For a two-sided test, the hypotheses are:

\[
H_0: \mu = \mu_0 \quad \text{vs.} \quad H_A: \mu \neq \mu_0
\]

The test rejects \(H_0\) if \(\bar{y}\) lies outside the interval \(\mu_0 \pm z_{\alpha/2} \frac{\sigma}{\sqrt{n}}\). The power of the test is:

\[
\begin{aligned} 
\pi(\mu) &= P\left(\bar{y} < \mu_0 - z_{\alpha/2} \frac{\sigma}{\sqrt{n}} \middle| \mu \right) + P\left(\bar{y} > \mu_0 + z_{\alpha/2} \frac{\sigma}{\sqrt{n}} \middle| \mu \right) \\ 
&= \Phi\left(-z_{\alpha/2} + \frac{(\mu - \mu_0)\sqrt{n}}{\sigma}\right) + \Phi\left(-z_{\alpha/2} - \frac{(\mu - \mu_0)\sqrt{n}}{\sigma}\right).
\end{aligned}
\]

To ensure a power of \(1-\beta\) when the treatment effect \(\delta = |\mu - \mu_0|\) is at least a certain value, we solve for \(n\). Since the power function for a two-sided test is increasing and symmetric in \(|\mu - \mu_0|\), it suffices to find \(n\) such that the power equals \(1-\beta\) when \(\mu = \mu_0 + \delta\). This gives:

\[
n = \left(\frac{(z_{\alpha/2} + z_{\beta}) \sigma}{\delta}\right)^2
\]

Alternatively, the required sample size can be determined using a confidence interval approach. For a two-sided \(\alpha\)-level confidence interval of the form:

\[
\bar{y} \pm D
\]

where \(D = z_{\alpha/2} \frac{\sigma}{\sqrt{n}}\), solving for \(n\) gives:

\[
n = \left(\frac{z_{\alpha/2} \sigma}{D}\right)^2
\]

This value should be rounded up to the nearest integer to ensure the required precision.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Generate random data and compute a 95\% confidence interval}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{) }\CommentTok{\# Generate 100 random values}
\FunctionTok{t.test}\NormalTok{(data, }\AttributeTok{conf.level =} \FloatTok{0.95}\NormalTok{) }\CommentTok{\# Perform t{-}test with 95\% confidence interval}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  One Sample t{-}test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  data}
\CommentTok{\#\textgreater{} t = {-}1.0355, df = 99, p{-}value = 0.303}
\CommentTok{\#\textgreater{} alternative hypothesis: true mean is not equal to 0}
\CommentTok{\#\textgreater{} 95 percent confidence interval:}
\CommentTok{\#\textgreater{}  {-}0.29774458  0.09354984}
\CommentTok{\#\textgreater{} sample estimates:}
\CommentTok{\#\textgreater{}  mean of x }
\CommentTok{\#\textgreater{} {-}0.1020974}
\end{Highlighting}
\end{Shaded}

For a one-sided hypothesis test, such as testing \(H_0: \mu \geq 30\) versus \(H_a: \mu < 30\):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Perform one{-}sided t{-}test}
\FunctionTok{t.test}\NormalTok{(data, }\AttributeTok{mu =} \DecValTok{30}\NormalTok{, }\AttributeTok{alternative =} \StringTok{"less"}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  One Sample t{-}test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  data}
\CommentTok{\#\textgreater{} t = {-}305.29, df = 99, p{-}value \textless{} 2.2e{-}16}
\CommentTok{\#\textgreater{} alternative hypothesis: true mean is less than 30}
\CommentTok{\#\textgreater{} 95 percent confidence interval:}
\CommentTok{\#\textgreater{}        {-}Inf 0.06162006}
\CommentTok{\#\textgreater{} sample estimates:}
\CommentTok{\#\textgreater{}  mean of x }
\CommentTok{\#\textgreater{} {-}0.1020974}
\end{Highlighting}
\end{Shaded}

When \(\sigma\) is unknown, you can estimate it using:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Prior studies or pilot studies.
\item
  The range of observations (excluding outliers) divided by 4, which provides a reasonable approximation for normally distributed data.
\end{enumerate}

\hypertarget{z-test-summary}{%
\paragraph{z-Test Summary}\label{z-test-summary}}

\begin{itemize}
\tightlist
\item
  For one-sided tests:
\end{itemize}

\[ \pi(\mu) = \Phi\left(-z_{\alpha} + \frac{(\mu - \mu_0)\sqrt{n}}{\sigma}\right) \]

\begin{itemize}
\tightlist
\item
  For two-sided tests:
\end{itemize}

\[ \pi(\mu) = \Phi\left(-z_{\alpha/2} + \frac{(\mu - \mu_0)\sqrt{n}}{\sigma}\right) + \Phi\left(-z_{\alpha/2} - \frac{(\mu - \mu_0)\sqrt{n}}{\sigma}\right) \]

\textbf{Factors Affecting Power}

\begin{itemize}
\tightlist
\item
  \textbf{Effect Size (}\(\mu - \mu_0\)): Larger differences between \(\mu\) and \(\mu_0\) increase power.
\item
  \textbf{Sample Size (}\(n\)): Larger \(n\) reduces the standard error, increasing power.
\item
  \textbf{Variance (}\(\sigma^2\)): Smaller variance increases power.
\item
  \textbf{Significance Level (}\(\alpha\)): Increasing \(\alpha\) (making the test more liberal) increases power through \(z_{\alpha}\).
\end{itemize}

\hypertarget{one-sample-t-test}{%
\paragraph{One-Sample t-test}\label{one-sample-t-test}}

In hypothesis testing, calculating the power and determining the required sample size for \textbf{t-tests} are more complex than for \textbf{z-tests}. This complexity arises from the involvement of the \textbf{Student's t-distribution} and its generalized form, the \textbf{non-central t-distribution}.

The power function for a one-sample t-test can be expressed as:

\[
\pi(\mu) = P\left(\frac{\bar{y} - \mu_0}{s / \sqrt{n}} > t_{n-1; \alpha} \mid \mu \right)
\]

Here:

\begin{itemize}
\item
  \(\mu_0\) is the hypothesized population mean under the null hypothesis,
\item
  \(\bar{y}\) is the sample mean,
\item
  \(s\) is the sample standard deviation,
\item
  \(n\) is the sample size,
\item
  \(t_{n-1; \alpha}\) is the critical t-value from the Student's t-distribution with \(n-1\) degrees of freedom at significance level \(\alpha\).
\end{itemize}

When \(\mu > \mu_0\) (i.e., \(\mu - \mu_0 = \delta\)), the random variable

\[
T = \frac{\bar{y} - \mu_0}{s / \sqrt{n}}
\]

does not follow the Student's t-distribution. Instead, it follows a \textbf{non-central t-distribution} with:

\begin{itemize}
\item
  a \textbf{non-centrality parameter} \(\lambda = \delta \sqrt{n} / \sigma\), where \(\sigma\) is the population standard deviation,
\item
  degrees of freedom \(n-1\).
\end{itemize}

\textbf{Key Properties of the Power Function}

\begin{itemize}
\tightlist
\item
  The power \(\pi(\mu)\) is an increasing function of the non-centrality parameter \(\lambda\).
\item
  For \(\delta = 0\) (i.e., when the null hypothesis is true), the non-central t-distribution simplifies to the regular Student's t-distribution.
\end{itemize}

To calculate the power in practice, numerical procedures (see below) or precomputed charts are typically required.

\textbf{Approximate Sample Size Adjustment for t-tests}

When planning a study, researchers often start with an approximation based on \textbf{z-tests} and then adjust for the specifics of the t-test. Here's the process:

1. Start with the Sample Size for a z-test

For a two-sided test: \[
n_z = \frac{\left(z_{\alpha/2} + z_\beta\right)^2 \sigma^2}{\delta^2}
\] where:

\begin{itemize}
\item
  \(z_{\alpha/2}\) is the critical value from the standard normal distribution for a two-tailed test,
\item
  \(z_\beta\) corresponds to the desired power \(1 - \beta\),
\item
  \(\delta\) is the effect size \(\mu - \mu_0\),
\item
  \(\sigma\) is the population standard deviation.
\end{itemize}

2. Adjust for the t-distribution

Let \(v = n - 1\), where \(n\) is the sample size derived from the z-test. For a two-sided t-test, the approximate sample size is:

\[
n^* = \frac{\left(t_{v; \alpha/2} + t_{v; \beta}\right)^2 \sigma^2}{\delta^2}
\]

Here:

\begin{itemize}
\item
  \(t_{v; \alpha/2}\) and \(t_{v; \beta}\) are the critical values from the Student's t-distribution for the significance level \(\alpha\) and desired power, respectively.
\item
  Since \(v\) depends on \(n^*\), this process may require iterative refinement.
\end{itemize}

Notes:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Approximations}: The above formulas provide an intuitive starting point but may require adjustments based on exact numerical solutions.
\item
  \textbf{Insights}: Power is an increasing function of:

  \begin{itemize}
  \tightlist
  \item
    the effect size \(\delta\),
  \item
    the sample size \(n\),
  \item
    and a decreasing function of the population variability \(\sigma\).
  \end{itemize}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example: Power calculation for a one{-}sample t{-}test}
\FunctionTok{library}\NormalTok{(pwr)}

\CommentTok{\# Parameters}
\NormalTok{effect\_size }\OtherTok{\textless{}{-}} \FloatTok{0.5}  \CommentTok{\# Cohen\textquotesingle{}s d}
\NormalTok{alpha }\OtherTok{\textless{}{-}} \FloatTok{0.05}       \CommentTok{\# Significance level}
\NormalTok{power }\OtherTok{\textless{}{-}} \FloatTok{0.8}        \CommentTok{\# Desired power}

\CommentTok{\# Compute sample size}
\NormalTok{sample\_size }\OtherTok{\textless{}{-}}
    \FunctionTok{pwr.t.test}\NormalTok{(}
        \AttributeTok{d =}\NormalTok{ effect\_size,}
        \AttributeTok{sig.level =}\NormalTok{ alpha,}
        \AttributeTok{power =}\NormalTok{ power,}
        \AttributeTok{type =} \StringTok{"one.sample"}
\NormalTok{    )}\SpecialCharTok{$}\NormalTok{n}

\CommentTok{\# Print result}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Required sample size for one{-}sample t{-}test:"}\NormalTok{,}
    \FunctionTok{ceiling}\NormalTok{(sample\_size),}
    \StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Required sample size for one{-}sample t{-}test: 34}

\CommentTok{\# Power calculation for a given sample size}
\NormalTok{calculated\_power }\OtherTok{\textless{}{-}}
    \FunctionTok{pwr.t.test}\NormalTok{(}
        \AttributeTok{n =} \FunctionTok{ceiling}\NormalTok{(sample\_size),}
        \AttributeTok{d =}\NormalTok{ effect\_size,}
        \AttributeTok{sig.level =}\NormalTok{ alpha,}
        \AttributeTok{type =} \StringTok{"one.sample"}
\NormalTok{    )}\SpecialCharTok{$}\NormalTok{power}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Achieved power with computed sample size:"}\NormalTok{,}
\NormalTok{    calculated\_power,}
    \StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Achieved power with computed sample size: 0.8077775}
\end{Highlighting}
\end{Shaded}

\hypertarget{for-difference-of-means-independent-samples}{%
\subsection{For Difference of Means, Independent Samples}\label{for-difference-of-means-independent-samples}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1359}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2306}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2573}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3714}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(100(1-\alpha)%
\) Confidence Interval
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Hypothesis Testing Test Statistic
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
When \(\sigma^2\) is known & \(\bar{X}_1 - \bar{X}_2 \pm z_{\alpha/2}\sqrt{\frac{\sigma^2_1}{n_1}+\frac{\sigma^2_2}{n_2}}\) & \(z= \frac{(\bar{X}_1-\bar{X}_2)-(\mu_1-\mu_2)_0}{\sqrt{\frac{\sigma^2_1}{n_1}+\frac{\sigma^2_2}{n_2}}}\) & \\
When \(\sigma^2\) is unknown, Variances Assumed EQUAL & \(\bar{X}_1 - \bar{X}_2 \pm t_{\alpha/2}\sqrt{s^2_p(\frac{1}{n_1}+\frac{1}{n_2})}\) & \(t = \frac{(\bar{X}_1-\bar{X}_2)-(\mu_1-\mu_2)_0}{\sqrt{s^2_p(\frac{1}{n_1}+\frac{1}{n_2})}}\) & Pooled Variance: \(s_p^2 = \frac{(n_1 -1)s^2_1 - (n_2-1)s^2_2}{n_1 + n_2 -2}\) Degrees of Freedom: \(\gamma = n_1 + n_2 -2\) \\
When \(\sigma^2\) is unknown, Variances Assumed UNEQUAL & \(\bar{X}_1 - \bar{X}_2 \pm t_{\alpha/2}\sqrt{(\frac{s^2_1}{n_1}+\frac{s^2_2}{n_2})}\) & \(t = \frac{(\bar{X}_1-\bar{X}_2)-(\mu_1-\mu_2)_0}{\sqrt{(\frac{s^2_1}{n_1}+\frac{s^2_2}{n_2})}}\) & Degrees of Freedom: \(\gamma = \frac{(\frac{s_1^2}{n_1}+\frac{s^2_2}{n_2})^2}{\frac{(\frac{s_1^2}{n_1})^2}{n_1-1}+\frac{(\frac{s_2^2}{n_2})^2}{n_2-1}}\) \\
\end{longtable}

\hypertarget{for-difference-of-means-paired-samples}{%
\subsection{For Difference of Means, Paired Samples}\label{for-difference-of-means-paired-samples}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.3636}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.6364}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Metric
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Formula
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Confidence Interval & \(\bar{D} \pm t_{\alpha/2}\frac{s_d}{\sqrt{n}}\) \\
Hypothesis Test Statistic & \(t = \frac{\bar{D} - D_0}{s_d / \sqrt{n}}\) \\
\end{longtable}

\hypertarget{for-difference-of-two-proportions}{%
\subsection{For Difference of Two Proportions}\label{for-difference-of-two-proportions}}

The mean of the difference between two sample proportions is given by:

\[
\hat{p_1} - \hat{p_2}
\]

The variance of the difference in proportions is:

\[
\frac{p_1(1-p_1)}{n_1} + \frac{p_2(1-p_2)}{n_2}
\]

A \(100(1-\alpha)\%\) confidence interval for the difference in proportions is calculated as:

\[
\hat{p_1} - \hat{p_2} \pm z_{\alpha/2} \sqrt{\frac{p_1(1-p_1)}{n_1} + \frac{p_2(1-p_2)}{n_2}}
\]

where

\begin{itemize}
\item
  \(z_{\alpha/2}\): The critical value from the standard normal distribution.
\item
  \(\hat{p_1}\), \(\hat{p_2}\): Sample proportions.
\item
  \(n_1\), \(n_2\): Sample sizes.
\end{itemize}

\textbf{Sample Size for a Desired Confidence Level and Margin of Error}

To achieve a margin of error \(d\) for a given confidence level, the required sample size can be estimated as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{With Prior Estimates of} \(\hat{p_1}\) and \(\hat{p_2}\): \[
  n \approx \frac{z_{\alpha/2}^2 \left[p_1(1-p_1) + p_2(1-p_2)\right]}{d^2}
  \]
\item
  \textbf{Without Prior Estimates} (assuming maximum variability, \(\hat{p} = 0.5\)): \[
  n \approx \frac{z_{\alpha/2}^2}{2d^2}
  \]
\end{enumerate}

\textbf{Hypothesis Testing for Difference in Proportions}

The test statistic for hypothesis testing depends on the null hypothesis:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{When} \((p_1 - p_2) \neq 0\): \[
  z = \frac{(\hat{p_1} - \hat{p_2}) - (p_1 - p_2)_0}{\sqrt{\frac{p_1(1-p_1)}{n_1} + \frac{p_2(1-p_2)}{n_2}}}
  \]
\item
  \textbf{When} \((p_1 - p_2)_0 = 0\) (testing equality of proportions): \[
  z = \frac{\hat{p_1} - \hat{p_2}}{\sqrt{\hat{p}(1-\hat{p}) \left(\frac{1}{n_1} + \frac{1}{n_2}\right)}}
  \]
\end{enumerate}

where \(\hat{p}\) is the pooled sample proportion:

\[
\hat{p} = \frac{x_1 + x_2}{n_1 + n_2} = \frac{n_1\hat{p_1} + n_2\hat{p_2}}{n_1 + n_2}
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{for-single-proportion}{%
\subsection{For Single Proportion}\label{for-single-proportion}}

The \(100(1-\alpha)\%\) confidence interval for a population proportion \(p\) is:

\[
\hat{p} \pm z_{\alpha/2} \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}
\]

Sample Size Determination

\begin{itemize}
\item
  \textbf{With Prior Estimate} (\(\hat{p}\)): \[
  n \approx \frac{z_{\alpha/2}^2 \hat{p}(1-\hat{p})}{d^2}
  \]
\item
  \textbf{Without Prior Estimate}: \[
  n \approx \frac{z_{\alpha/2}^2}{4d^2}
  \]
\end{itemize}

The test statistic for \(H_0: p = p_0\) is:

\[
z = \frac{\hat{p} - p_0}{\sqrt{\frac{p_0(1-p_0)}{n}}}
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{for-single-variance}{%
\subsection{For Single Variance}\label{for-single-variance}}

For a sample variance \(s^2\) with \(n\) observations, the \(100(1-\alpha)\%\) confidence interval for the population variance \(\sigma^2\) is:

\[
\begin{aligned}
1 - \alpha &= P( \chi_{1-\alpha/2;n-1}^2) \le (n-1)s^2/\sigma^2 \le \chi_{\alpha/2;n-1}^2)\\
&=P\left(\frac{(n-1)s^2}{\chi^2_{\alpha/2; n-1}} \leq \sigma^2 \leq \frac{(n-1)s^2}{\chi^2_{1-\alpha/2; n-1}}\right)
\end{aligned}
\]

Equivalently, the confidence interval can be written as:

\[
\left(\frac{(n-1)s^2}{\chi^2_{\alpha/2}}, \frac{(n-1)s^2}{\chi^2_{1-\alpha/2}}\right)
\]

To find confidence limits for \(\sigma\), compute the square root of the interval bounds:

\[
\text{Confidence Interval for } \sigma: \quad \left(\sqrt{\frac{(n-1)s^2}{\chi^2_{\alpha/2}}}, \sqrt{\frac{(n-1)s^2}{\chi^2_{1-\alpha/2}}}\right)
\]

\textbf{Hypothesis Testing for Variance}

The test statistic for testing a null hypothesis about a population variance (\(\sigma^2_0\)) is:

\[
\chi^2 = \frac{(n-1)s^2}{\sigma^2_0}
\]

This test statistic follows a chi-squared distribution with \(n-1\) degrees of freedom under the null hypothesis.

\hypertarget{non-parametric-tests}{%
\subsection{Non-parametric Tests}\label{non-parametric-tests}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4961}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2403}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2558}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Method}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Purpose}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Assumptions}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\protect\hyperlink{sign-test}{Sign Test} & Test median & None (ordinal data sufficient) \\
\protect\hyperlink{wilcoxon-signed-rank-test}{Wilcoxon Signed Rank Test} & Test symmetry around a value & Symmetry of distribution \\
\protect\hyperlink{wald-wolfowitz-runs-test}{Wald-Wolfowitz Runs Test} & Test for randomness & Independent observations \\
\protect\hyperlink{quantile-or-percentile-test}{Quantile (or Percentile) Test} & Test specific quantile & None (ordinal data sufficient) \\
\end{longtable}

\hypertarget{sign-test}{%
\subsubsection{Sign Test}\label{sign-test}}

The \textbf{Sign Test} is used to test hypotheses about the median of a population, \(\mu_{(0.5)}\), without assuming a specific distribution for the data. This test is ideal for small sample sizes or when normality assumptions are not met.

To test the population median, consider the hypotheses:

\begin{itemize}
\tightlist
\item
  Null Hypothesis: \(H_0: \mu_{(0.5)} = 0\)
\item
  Alternative Hypothesis: \(H_a: \mu_{(0.5)} > 0\) (one-sided test)
\end{itemize}

Steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Count Positive and Negative Deviations}:

  \begin{itemize}
  \tightlist
  \item
    Count observations (\(y_i\)) greater than 0: \(s_+\) (number of positive signs).
  \item
    Count observations less than 0: \(s_-\) (number of negative signs).
  \item
    \(s_- = n - s_+\).
  \end{itemize}
\item
  \textbf{Decision Rule}:

  \begin{itemize}
  \tightlist
  \item
    Reject \(H_0\) if \(s_+\) is large (or equivalently, \(s_-\) is small).
  \item
    To determine how large \(s_+\) must be, use the distribution of \(S_+\) under \(H_0\), which is \textbf{Binomial} with \(p = 0.5\).
  \end{itemize}
\item
  \textbf{Null Distribution}:\\
  Under \(H_0\), \(S_+\) follows: \[
  S_+ \sim Binomial(n, p = 0.5)
  \]
\item
  \textbf{Critical Value}:\\
  Reject \(H_0\) if: \[
  s_+ \ge b_{n,\alpha}
  \] where \(b_{n,\alpha}\) is the upper \(\alpha\) critical value of the binomial distribution.
\item
  \textbf{p-value Calculation}:\\
  Compute the p-value for the observed (one-tailed) \(s_+\) as: \[
  \text{p-value} = P(S \ge s_+) = \sum_{i=s_+}^{n} \binom{n}{i} \left(\frac{1}{2}\right)^n
  \]

  Alternatively: \[
  P(S \le s_-) = \sum_{i=0}^{s_-} \binom{n}{i} \left(\frac{1}{2}\right)^n
  \]
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Large Sample Normal Approximation

For large \(n\), use a normal approximation for the binomial test. Reject \(H_0\) if: \[
s_+ \ge \frac{n}{2} + \frac{1}{2} + z_{\alpha} \sqrt{\frac{n}{4}}
\] where \(z_\alpha\) is the critical value for a one-sided test.

For two-sided tests, use the maximum or minimum of \(s_+\) and \(s_-\):

\begin{itemize}
\item
  Test statistic: \(s_{\text{max}} = \max(s_+, s_-)\) or \(s_{\text{min}} = \min(s_+, s_-)\)
\item
  Reject \(H_0\) if \(p\)-value is less than \(\alpha\), where: \[
  p\text{-value} = 2 \sum_{i=s_{\text{max}}}^{n} \binom{n}{i} \left(\frac{1}{2}\right)^n = 2 \sum_{i = 0}^{s_{min}} \binom{n}{i} \left( \frac{1}{2} \right)^n
  \]
\end{itemize}

Equivalently, rejecting \(H_0\) if \(s_{max} \ge b_{n,\alpha/2}\).

For large \(n\), the normal approximation uses: \[
z = \frac{s_{\text{max}} - \frac{n}{2} - \frac{1}{2}}{\sqrt{\frac{n}{4}}}
\]\\
Reject \(H_0\) at \(\alpha\) if \(z \ge z_{\alpha/2}\).

Handling zeros in the data is a common issue with the Sign Test:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Random Assignment}: Assign zeros randomly to either \(s_+\) or \(s_-\) (2 researchers might get different results).
\item
  \textbf{Fractional Assignment}: Count each zero as \(0.5\) toward both \(s_+\) and \(s_-\) (but then we could not apply the \protect\hyperlink{binomial-distribution}{Binomial Distribution} afterward).
\item
  \textbf{Ignore Zeros}: Ignore zeros, but note this reduces the sample size and power.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example Data}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{0.76}\NormalTok{, }\FloatTok{0.82}\NormalTok{, }\FloatTok{0.80}\NormalTok{, }\FloatTok{0.79}\NormalTok{, }\FloatTok{1.06}\NormalTok{, }\FloatTok{0.83}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{0.43}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{0.34}\NormalTok{, }\FloatTok{3.34}\NormalTok{, }\FloatTok{2.33}\NormalTok{)}

\CommentTok{\# Count positive signs}
\NormalTok{s\_plus }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(data }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{)}

\CommentTok{\# Sample size excluding zeros}
\NormalTok{n }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(data)}

\CommentTok{\# Perform a one{-}sided binomial test}
\FunctionTok{binom.test}\NormalTok{(s\_plus, n, }\AttributeTok{p =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{alternative =} \StringTok{"greater"}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Exact binomial test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  s\_plus and n}
\CommentTok{\#\textgreater{} number of successes = 8, number of trials = 10, p{-}value = 0.05469}
\CommentTok{\#\textgreater{} alternative hypothesis: true probability of success is greater than 0.5}
\CommentTok{\#\textgreater{} 95 percent confidence interval:}
\CommentTok{\#\textgreater{}  0.4930987 1.0000000}
\CommentTok{\#\textgreater{} sample estimates:}
\CommentTok{\#\textgreater{} probability of success }
\CommentTok{\#\textgreater{}                    0.8}
\end{Highlighting}
\end{Shaded}

\hypertarget{wilcoxon-signed-rank-test}{%
\subsubsection{Wilcoxon Signed Rank Test}\label{wilcoxon-signed-rank-test}}

The \textbf{Wilcoxon Signed Rank Test} is an improvement over the \protect\hyperlink{sign-test}{Sign Test} as it considers both the magnitude and direction of deviations from the null hypothesis value (e.g., 0). However, this test assumes that the data are symmetrically distributed around the median, unlike the Sign Test.

We test the following hypotheses:

\[
H_0: \mu_{(0.5)} = 0 \\
H_a: \mu_{(0.5)} > 0
\]

This example assumes no ties or duplicate observations in the data.

Procedure for the Signed Rank Test

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Rank the Absolute Values}:

  \begin{itemize}
  \tightlist
  \item
    Rank the observations \(y_i\) based on their absolute values.
  \item
    Let \(r_i\) denote the rank of \(y_i\).
  \item
    Since there are no ties, ranks \(r_i\) are uniquely determined and form a permutation of integers \(1, 2, \dots, n\).
  \end{itemize}
\item
  \textbf{Calculate} \(w_+\) and \(w_-\):

  \begin{itemize}
  \tightlist
  \item
    \(w_+\) is the sum of the ranks corresponding to positive values of \(y_i\).
  \item
    \(w_-\) is the sum of the ranks corresponding to negative values of \(y_i\).
  \item
    By definition: \[
    w_+ + w_- = \sum_{i=1}^n r_i = \frac{n(n+1)}{2}
    \]
  \end{itemize}
\item
  \textbf{Decision Rule}:

  \begin{itemize}
  \tightlist
  \item
    Reject \(H_0\) if \(w_+\) is large (or equivalently, if \(w_-\) is small).
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Null Distribution} of \(W_+\)

Under the null hypothesis, the distributions of \(W_+\) and \(W_-\) are identical and symmetric. The p-value for a one-sided test is:

\[
\text{p-value} = P(W \ge w_+) = P(W \le w_-)
\]

An \(\alpha\)-level test rejects \(H_0\) if \(w_+ \ge w_{n,\alpha}\), where \(w_{n,\alpha}\) is the critical value from a table of the null distribution of \(W_+\).

For two-sided tests, use:

\[
p\text{-value} = 2P(W \ge w_{max}) = 2P(W \le w_{min})
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Normal Approximation for Large Samples}

For large \(n\), the null distribution of \(W_+\) can be approximated by a normal distribution:

\[
z = \frac{w_+ - \frac{n(n+1)}{4} - \frac{1}{2}}{\sqrt{\frac{n(n+1)(2n+1)}{24}}}
\]

The test rejects \(H_0\) at level \(\alpha\) if:

\[
w_+ \ge \frac{n(n+1)}{4} + \frac{1}{2} + z_{\alpha} \sqrt{\frac{n(n+1)(2n+1)}{24}} \approx w_{n,\alpha}
\]

For a two-sided test, the decision rule uses the maximum or minimum of \(w_+\) and \(w_-\):

\begin{itemize}
\item
  \(w_{max} = \max(w_+, w_-)\)
\item
  \(w_{min} = \min(w_+, w_-)\)
\end{itemize}

The p-value is computed as:

\[
p\text{-value} = 2P(W \ge w_{max}) = 2P(W \le w_{min})
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Handling Tied Ranks}

If some observations \(|y_i|\) have tied absolute values, assign the average rank (or ``midrank'') to all tied values. For example:

\begin{itemize}
\tightlist
\item
  Suppose \(y_1 = -1\), \(y_2 = 3\), \(y_3 = -3\), and \(y_4 = 5\).
\item
  The ranks for \(|y_i|\) are:

  \begin{itemize}
  \tightlist
  \item
    \(|y_1| = 1\): \(r_1 = 1\)
  \item
    \(|y_2| = |y_3| = 3\): \(r_2 = r_3 = \frac{2+3}{2} = 2.5\)
  \item
    \(|y_4| = 5\): \(r_4 = 4\)
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example Data}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{0.76}\NormalTok{, }\FloatTok{0.82}\NormalTok{, }\FloatTok{0.80}\NormalTok{, }\FloatTok{0.79}\NormalTok{, }\FloatTok{1.06}\NormalTok{, }\FloatTok{0.83}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{0.43}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{0.34}\NormalTok{, }\FloatTok{3.34}\NormalTok{, }\FloatTok{2.33}\NormalTok{)}

\CommentTok{\# Perform Wilcoxon Signed Rank Test (exact test)}
\NormalTok{wilcox\_exact }\OtherTok{\textless{}{-}} \FunctionTok{wilcox.test}\NormalTok{(data, }\AttributeTok{exact =} \ConstantTok{TRUE}\NormalTok{)}

\CommentTok{\# Display results}
\NormalTok{wilcox\_exact}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Wilcoxon signed rank exact test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  data}
\CommentTok{\#\textgreater{} V = 52, p{-}value = 0.009766}
\CommentTok{\#\textgreater{} alternative hypothesis: true location is not equal to 0}
\end{Highlighting}
\end{Shaded}

For large samples, you can use the normal approximation by setting \texttt{exact\ =\ FALSE}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Perform Wilcoxon Signed Rank Test (normal approximation)}
\NormalTok{wilcox\_normal }\OtherTok{\textless{}{-}} \FunctionTok{wilcox.test}\NormalTok{(data, }\AttributeTok{exact =} \ConstantTok{FALSE}\NormalTok{)}

\CommentTok{\# Display results}
\NormalTok{wilcox\_normal}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Wilcoxon signed rank test with continuity correction}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  data}
\CommentTok{\#\textgreater{} V = 52, p{-}value = 0.01443}
\CommentTok{\#\textgreater{} alternative hypothesis: true location is not equal to 0}
\end{Highlighting}
\end{Shaded}

\hypertarget{wald-wolfowitz-runs-test}{%
\subsubsection{Wald-Wolfowitz Runs Test}\label{wald-wolfowitz-runs-test}}

The \textbf{Runs Test} is a non-parametric test used to examine the randomness of a sequence. Specifically, it tests whether the order of observations in a sequence is random. This test is useful in detecting non-random patterns, such as trends, clustering, or periodicity.

The hypotheses for the Runs Test are:

\begin{itemize}
\tightlist
\item
  Null Hypothesis: \(H_0\): The sequence is random.
\item
  Alternative Hypothesis: \(H_a\): The sequence is not random.
\end{itemize}

A \textbf{run} is a sequence of consecutive observations of the same type. For example: - In the binary sequence \texttt{+\ +\ -\ -\ +\ -\ +\ +}, there are \textbf{5 runs}: \texttt{++}, \texttt{-\/-}, \texttt{+}, \texttt{-}, \texttt{++}.

Runs can be formed based on any classification criteria, such as:

\begin{itemize}
\item
  Positive vs.~Negative values
\item
  Above vs.~Below the median
\item
  Success vs.~Failure in binary outcomes
\end{itemize}

\textbf{Test Statistic}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Number of Runs} (\(R\)):\\
  The observed number of runs in the sequence.
\item
  \textbf{Expected Number of Runs} (\(E[R]\)):\\
  Under the null hypothesis of randomness, the expected number of runs is: \[
  E[R] = \frac{2 n_1 n_2}{n_1 + n_2} + 1
  \] where:

  \begin{itemize}
  \tightlist
  \item
    \(n_1\): Number of observations in the first category (e.g., positives).
  \item
    \(n_2\): Number of observations in the second category (e.g., negatives).
  \item
    \(n = n_1 + n_2\): Total number of observations.
  \end{itemize}
\item
  \textbf{Variance of Runs} (\(\text{Var}[R]\)):\\
  The variance of the number of runs is given by: \[
  \text{Var}[R] = \frac{2 n_1 n_2 (2 n_1 n_2 - n)}{n^2 (n - 1)}
  \]
\item
  \textbf{Standardized Test Statistic (}\(z\)):\\
  For large samples (\(n \geq 20\)), the test statistic is approximately normally distributed: \[
  z = \frac{R - E[R]}{\sqrt{\text{Var}[R]}}
  \]
\end{enumerate}

\textbf{Decision Rule}

\begin{itemize}
\tightlist
\item
  Compute the \(z\)-value and compare it to the critical value of the standard normal distribution.
\item
  For a significance level \(\alpha\):

  \begin{itemize}
  \tightlist
  \item
    Reject \(H_0\) if \(|z| \ge z_{\alpha/2}\) (two-sided test).
  \item
    Reject \(H_0\) if \(z \ge z_\alpha\) or \(z \le -z_\alpha\) for one-sided tests.
  \end{itemize}
\end{itemize}

Steps for Conducting a Runs Test:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Classify the data into two groups (e.g., above/below median, positive/negative).
\item
  Count the total number of runs (\(R\)).
\item
  Compute \(E[R]\) and \(\text{Var}[R]\) based on \(n_1\) and \(n_2\).
\item
  Compute the \(z\)-value for the observed number of runs.
\item
  Compare the \(z\)-value to the critical value to decide whether to reject \(H_0\).
\end{enumerate}

For a numerical dataset where the test is based on values above and below the median:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example dataset}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{1.2}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{0.5}\NormalTok{, }\FloatTok{3.4}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{1.1}\NormalTok{, }\FloatTok{2.8}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{0.8}\NormalTok{, }\FloatTok{4.5}\NormalTok{, }\FloatTok{0.7}\NormalTok{)}

\FunctionTok{library}\NormalTok{(randtests)}
\CommentTok{\# Perform Runs Test (above/below median)}
\FunctionTok{runs.test}\NormalTok{(data)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Runs Test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  data}
\CommentTok{\#\textgreater{} statistic = 2.2913, runs = 8, n1 = 4, n2 = 4, n = 8, p{-}value = 0.02195}
\CommentTok{\#\textgreater{} alternative hypothesis: nonrandomness}
\end{Highlighting}
\end{Shaded}

The output of the \texttt{runs.test} function includes:

\begin{itemize}
\item
  \textbf{Observed Runs}: The actual number of runs in the sequence.
\item
  \textbf{Expected Runs}: The expected number of runs under \(H_0\).
\item
  \textbf{p-value}: The probability of observing a number of runs as extreme as the observed one under \(H_0\).
\item
  If the p-value is less than \(\alpha\), reject \(H_0\) and conclude that the sequence is not random.
\end{itemize}

Limitations of the Runs Test

\begin{itemize}
\item
  The test assumes that observations are independent.
\item
  For small sample sizes, the test may have limited power.
\item
  Ties in the data must be resolved by a predefined rule (e.g., treating ties as belonging to one group or excluding them).
\end{itemize}

\hypertarget{quantile-or-percentile-test}{%
\subsubsection{Quantile (or Percentile) Test}\label{quantile-or-percentile-test}}

The \textbf{Quantile Test} (also called the Percentile Test) is a non-parametric test used to evaluate whether the proportion of observations falling within a specific quantile matches the expected proportion under the null hypothesis. This test is useful for assessing the distribution of data when specific quantiles (e.g., medians or percentiles) are of interest.

Suppose we want to test whether the true proportion of data below a specified quantile \(q\) matches a given probability \(p\). The hypotheses are:

\begin{itemize}
\tightlist
\item
  Null Hypothesis: \(H_0\): The true proportion is equal to \(p\).
\item
  Alternative Hypothesis: \(H_a\): The true proportion is not equal to \(p\) (two-sided), greater than \(p\) (right-tailed), or less than \(p\) (left-tailed).
\end{itemize}

\textbf{Test Statistic}

The test statistic is based on the observed count of data points below the specified quantile.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Observed Count} (\(k\)):\\
  The number of data points \(y_i\) such that \(y_i \leq q\).
\item
  \textbf{Expected Count} (\(E[k]\)):\\
  The expected number of observations below the quantile \(q\) under \(H_0\) is: \[
  E[k] = n \cdot p
  \]
\item
  \textbf{Variance}:\\
  Under the binomial distribution, the variance is: \[
  \text{Var}[k] = n \cdot p \cdot (1 - p)
  \]
\item
  \textbf{Standardized Test Statistic} (\(z\)):\\
  For large \(n\), the test statistic is approximately normally distributed: \[
  z = \frac{k - E[k]}{\sqrt{\text{Var}[k]}} = \frac{k - n \cdot p}{\sqrt{n \cdot p \cdot (1 - p)}}
  \]
\end{enumerate}

\textbf{Decision Rule}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute the \(z\)-value for the observed count.
\item
  Compare the \(z\)-value to the critical value of the standard normal distribution:

  \begin{itemize}
  \tightlist
  \item
    For a two-sided test, reject \(H_0\) if \(|z| \geq z_{\alpha/2}\).
  \item
    For a one-sided test, reject \(H_0\) if \(z \geq z_\alpha\) (right-tailed) or \(z \leq -z_\alpha\) (left-tailed).
  \end{itemize}
\end{enumerate}

Alternatively, calculate the p-value and reject \(H_0\) if the p-value \(\leq \alpha\).

Suppose we have a dataset and want to test whether the proportion of observations below the 50th percentile (median) matches the expected value of \(p = 0.5\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example data}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{12}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{14}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{13}\NormalTok{, }\DecValTok{11}\NormalTok{, }\DecValTok{14}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{13}\NormalTok{)}

\CommentTok{\# Define the quantile to test}
\NormalTok{quantile\_value }\OtherTok{\textless{}{-}} \FunctionTok{quantile}\NormalTok{(data, }\FloatTok{0.5}\NormalTok{) }\CommentTok{\# Median}
\NormalTok{p }\OtherTok{\textless{}{-}} \FloatTok{0.5}                             \CommentTok{\# Proportion under H0}

\CommentTok{\# Count observed values below or equal to the quantile}
\NormalTok{k }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(data }\SpecialCharTok{\textless{}=}\NormalTok{ quantile\_value)}

\CommentTok{\# Sample size}
\NormalTok{n }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(data)}

\CommentTok{\# Expected count under H0}
\NormalTok{expected\_count }\OtherTok{\textless{}{-}}\NormalTok{ n }\SpecialCharTok{*}\NormalTok{ p}

\CommentTok{\# Variance}
\NormalTok{variance }\OtherTok{\textless{}{-}}\NormalTok{ n }\SpecialCharTok{*}\NormalTok{ p }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ p)}

\CommentTok{\# Test statistic (z{-}value)}
\NormalTok{z }\OtherTok{\textless{}{-}}\NormalTok{ (k }\SpecialCharTok{{-}}\NormalTok{ expected\_count) }\SpecialCharTok{/} \FunctionTok{sqrt}\NormalTok{(variance)}

\CommentTok{\# Calculate p{-}value for two{-}sided test}
\NormalTok{p\_value }\OtherTok{\textless{}{-}} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{pnorm}\NormalTok{(}\FunctionTok{abs}\NormalTok{(z)))}

\CommentTok{\# Output results}
\FunctionTok{list}\NormalTok{(}
  \AttributeTok{quantile\_value =}\NormalTok{ quantile\_value,}
  \AttributeTok{observed\_count =}\NormalTok{ k,}
  \AttributeTok{expected\_count =}\NormalTok{ expected\_count,}
  \AttributeTok{z\_value =}\NormalTok{ z,}
  \AttributeTok{p\_value =}\NormalTok{ p\_value}
\NormalTok{)}
\CommentTok{\#\textgreater{} $quantile\_value}
\CommentTok{\#\textgreater{}  50\% }
\CommentTok{\#\textgreater{} 13.5 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $observed\_count}
\CommentTok{\#\textgreater{} [1] 5}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $expected\_count}
\CommentTok{\#\textgreater{} [1] 5}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $z\_value}
\CommentTok{\#\textgreater{} [1] 0}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $p\_value}
\CommentTok{\#\textgreater{} [1] 1}
\end{Highlighting}
\end{Shaded}

For a one-sided test (e.g., testing whether the proportion is greater than \(p\)):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate one{-}sided p{-}value}
\NormalTok{p\_value\_one\_sided }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{pnorm}\NormalTok{(z)}

\CommentTok{\# Output one{-}sided p{-}value}
\NormalTok{p\_value\_one\_sided}
\CommentTok{\#\textgreater{} [1] 0.5}
\end{Highlighting}
\end{Shaded}

Interpretation of Results

\begin{itemize}
\item
  \textbf{p-value}: If the p-value is less than \(\alpha\), reject \(H_0\) and conclude that the proportion of observations below the quantile deviates significantly from \(p\).
\item
  \textbf{Quantile Test Statistic (}\(z\)\textbf{)}: The \(z\)-value indicates how many standard deviations the observed count is from the expected count under the null hypothesis. Large positive or negative \(z\) values suggest non-random deviations.
\end{itemize}

Assumptions of the Test

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Observations are independent.
\item
  The sample size is large enough for the normal approximation to the binomial distribution to be valid (\(n \cdot p \geq 5\) and \(n \cdot (1 - p) \geq 5\)).
\end{enumerate}

Limitations of the Test

\begin{itemize}
\item
  For small sample sizes, the normal approximation may not hold. In such cases, exact binomial tests are more appropriate.
\item
  The test assumes that the quantile used (e.g., the median) is well-defined and correctly calculated from the data.
\end{itemize}

\hypertarget{two-sample-inference}{%
\section{Two-Sample Inference}\label{two-sample-inference}}

\hypertarget{for-means}{%
\subsection{For Means}\label{for-means}}

Suppose we have two sets of observations:

\begin{itemize}
\tightlist
\item
  \(y_1, \dots, y_{n_y}\)
\item
  \(x_1, \dots, x_{n_x}\)
\end{itemize}

These are random samples from two independent populations with means \(\mu_y\) and \(\mu_x\) and variances \(\sigma_y^2\) and \(\sigma_x^2\). Our goal is to compare \(\mu_y\) and \(\mu_x\) or test whether \(\sigma_y^2 = \sigma_x^2\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{large-sample-tests}{%
\subsubsection{Large Sample Tests}\label{large-sample-tests}}

If \(n_y\) and \(n_x\) are large (\(\geq 30\)), the \protect\hyperlink{central-limit-theorem}{Central Limit Theorem} allows us to make the following assumptions:

\begin{itemize}
\tightlist
\item
  \textbf{Expectation}: \[
  E(\bar{y} - \bar{x}) = \mu_y - \mu_x
  \]
\item
  \textbf{Variance}: \[
  \text{Var}(\bar{y} - \bar{x}) = \frac{\sigma_y^2}{n_y} + \frac{\sigma_x^2}{n_x}
  \]
\end{itemize}

The test statistic is:

\[
Z = \frac{\bar{y} - \bar{x} - (\mu_y - \mu_x)}{\sqrt{\frac{\sigma_y^2}{n_y} + \frac{\sigma_x^2}{n_x}}} \sim N(0,1)
\]

For large samples, replace variances with their unbiased estimators \(s_y^2\) and \(s_x^2\), yielding the same large sample distribution.

\textbf{Confidence Interval}

An approximate \(100(1-\alpha)\%\) confidence interval for \(\mu_y - \mu_x\) is:

\[
\bar{y} - \bar{x} \pm z_{\alpha/2} \sqrt{\frac{s_y^2}{n_y} + \frac{s_x^2}{n_x}}
\]

\textbf{Hypothesis Test}

Testing:

\[
H_0: \mu_y - \mu_x = \delta_0 \quad \text{vs.} \quad H_a: \mu_y - \mu_x \neq \delta_0
\]

The test statistic:

\[
z = \frac{\bar{y} - \bar{x} - \delta_0}{\sqrt{\frac{s_y^2}{n_y} + \frac{s_x^2}{n_x}}}
\]

Reject \(H_0\) at the \(\alpha\)-level if:

\[
|z| > z_{\alpha/2}
\]

If \(\delta_0 = 0\), this tests whether the two means are equal.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Large sample test}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{14}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{18}\NormalTok{)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{9}\NormalTok{, }\DecValTok{11}\NormalTok{, }\DecValTok{13}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{17}\NormalTok{)}

\CommentTok{\# Mean and variance}
\NormalTok{mean\_y }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(y)}
\NormalTok{mean\_x }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(x)}
\NormalTok{var\_y }\OtherTok{\textless{}{-}} \FunctionTok{var}\NormalTok{(y)}
\NormalTok{var\_x }\OtherTok{\textless{}{-}} \FunctionTok{var}\NormalTok{(x)}
\NormalTok{n\_y }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(y)}
\NormalTok{n\_x }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(x)}

\CommentTok{\# Test statistic}
\NormalTok{z }\OtherTok{\textless{}{-}}\NormalTok{ (mean\_y }\SpecialCharTok{{-}}\NormalTok{ mean\_x) }\SpecialCharTok{/} \FunctionTok{sqrt}\NormalTok{(var\_y }\SpecialCharTok{/}\NormalTok{ n\_y }\SpecialCharTok{+}\NormalTok{ var\_x }\SpecialCharTok{/}\NormalTok{ n\_x)}
\NormalTok{p\_value }\OtherTok{\textless{}{-}} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{pnorm}\NormalTok{(}\FunctionTok{abs}\NormalTok{(z)))}

\FunctionTok{list}\NormalTok{(}\AttributeTok{z =}\NormalTok{ z, }\AttributeTok{p\_value =}\NormalTok{ p\_value)}
\CommentTok{\#\textgreater{} $z}
\CommentTok{\#\textgreater{} [1] 0.5}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $p\_value}
\CommentTok{\#\textgreater{} [1] 0.6170751}
\end{Highlighting}
\end{Shaded}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{small-sample-tests}{%
\subsubsection{Small Sample Tests}\label{small-sample-tests}}

If the samples are small, assume the data come from independent normal distributions:

\begin{itemize}
\item
  \(y_i \sim N(\mu_y, \sigma_y^2)\)
\item
  \(x_i \sim N(\mu_x, \sigma_x^2)\)
\end{itemize}

We can do inference based on the \protect\hyperlink{students-t-distribution}{Student's T Distribution}, where we have 2 cases:

\begin{itemize}
\item
  \protect\hyperlink{equal-variances}{Equal Variances}
\item
  \protect\hyperlink{unequal-variances}{Unequal Variances}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3316}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4796}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1837}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Assumption
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Tests
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Plots
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Independence and Identically Distributed (i.i.d.) Observations & Test for serial correlation & \\
Independence Between Samples & Correlation Coefficient & \protect\hyperlink{scatterplot}{Scatterplot} \\
Normality & See \protect\hyperlink{normality-assessment}{Normality Assessment} & See \protect\hyperlink{normality-assessment}{Normality Assessment} \\
Equality of Variances & \begin{minipage}[t]{\linewidth}\raggedright
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \protect\hyperlink{f-test}{F-Test}
\item
  \protect\hyperlink{levenes-test}{Levene's Test}
\item
  \protect\hyperlink{modified-levene-test-brown-forsythe-test}{Modified Levene Test (Brown-Forsythe Test)}
\item
  \protect\hyperlink{bartletts-test}{Bartlett's Test}
\end{enumerate}
\end{minipage} & \begin{minipage}[t]{\linewidth}\raggedright
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Boxplots with overlayed means
\item
  Residuals spread plots
\end{enumerate}
\end{minipage} \\
\end{longtable}

\hypertarget{equal-variances}{%
\paragraph{Equal Variances}\label{equal-variances}}

\textbf{Assumptions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Independence and Identically Distributed (i.i.d.) Observations
\end{enumerate}

Assume that observations in each sample are i.i.d., which implies:

\[
var(\bar{y}) = \frac{\sigma^2_y}{n_y}, \quad var(\bar{x}) = \frac{\sigma^2_x}{n_x}
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Independence Between Samples
\end{enumerate}

The samples are assumed to be independent, meaning no observation from one sample influences observations from the other. This independence allows us to write:

\[
\begin{aligned}
var(\bar{y} - \bar{x}) &= var(\bar{y}) + var(\bar{x}) - 2cov(\bar{y}, \bar{x}) \\
&= var(\bar{y}) + var(\bar{x}) \\
&= \frac{\sigma^2_y}{n_y} + \frac{\sigma^2_x}{n_x}
\end{aligned}
\]

This calculation assumes \(cov(\bar{y}, \bar{x}) = 0\) due to the independence between the samples.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Normality Assumption
\end{enumerate}

We assume that the underlying populations are normally distributed. This assumption justifies the use of the \protect\hyperlink{students-t-distribution}{Student's T Distribution}, which is critical for hypothesis testing and constructing confidence intervals.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Equality of Variances
\end{enumerate}

If the population variances are equal, i.e., \(\sigma^2_y = \sigma^2_x = \sigma^2\), then \(s^2_y\) and \(s^2_x\) are both unbiased estimators of \(\sigma^2\). This allows us to pool the variances.

The pooled variance estimator is calculated as:

\[
s^2 = \frac{(n_y - 1)s^2_y + (n_x - 1)s^2_x}{(n_y - 1) + (n_x - 1)}
\]

The pooled variance estimate has degrees of freedom equal to:

\[
df = (n_y + n_x - 2)
\]

\textbf{Test Statistic}

The test statistic is: \[
T = \frac{\bar{y} - \bar{x} - (\mu_y - \mu_x)}{s \sqrt{\frac{1}{n_y} + \frac{1}{n_x}}} \sim t_{n_y + n_x - 2}
\]

\textbf{Confidence Interval}

A \(100(1 - \alpha)\%\) confidence interval for \(\mu_y - \mu_x\) is: \[
\bar{y} - \bar{x} \pm t_{n_y + n_x - 2, \alpha/2} \cdot s \sqrt{\frac{1}{n_y} + \frac{1}{n_x}}
\]

\textbf{Hypothesis Test}

Testing: \[
H_0: \mu_y - \mu_x = \delta_0 \quad \text{vs.} \quad H_a: \mu_y - \mu_x \neq \delta_0
\]

Reject \(H_0\) if: \[
|T| > t_{n_y + n_x - 2, \alpha/2}
\]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Small sample test with equal variance}
\NormalTok{t\_test\_equal }\OtherTok{\textless{}{-}} \FunctionTok{t.test}\NormalTok{(y, x, }\AttributeTok{var.equal =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{t\_test\_equal}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Two Sample t{-}test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  y and x}
\CommentTok{\#\textgreater{} t = 0.5, df = 8, p{-}value = 0.6305}
\CommentTok{\#\textgreater{} alternative hypothesis: true difference in means is not equal to 0}
\CommentTok{\#\textgreater{} 95 percent confidence interval:}
\CommentTok{\#\textgreater{}  {-}3.612008  5.612008}
\CommentTok{\#\textgreater{} sample estimates:}
\CommentTok{\#\textgreater{} mean of x mean of y }
\CommentTok{\#\textgreater{}        14        13}
\end{Highlighting}
\end{Shaded}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{unequal-variances}{%
\paragraph{Unequal Variances}\label{unequal-variances}}

\textbf{Assumptions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Independence and Identically Distributed (i.i.d.) Observations
\end{enumerate}

Assume that observations in each sample are i.i.d., which implies:

\[ var(\bar{y}) = \frac{\sigma^2_y}{n_y}, \quad var(\bar{x}) = \frac{\sigma^2_x}{n_x} \]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Independence Between Samples
\end{enumerate}

The samples are assumed to be independent, meaning no observation from one sample influences observations from the other. This independence allows us to write:

\[ \begin{aligned} var(\bar{y} - \bar{x}) &= var(\bar{y}) + var(\bar{x}) - 2cov(\bar{y}, \bar{x}) \\ &= var(\bar{y}) + var(\bar{x}) \\ &= \frac{\sigma^2_y}{n_y} + \frac{\sigma^2_x}{n_x} \end{aligned} \]

This calculation assumes \(cov(\bar{y}, \bar{x}) = 0\) due to the independence between the samples.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Normality Assumption
\end{enumerate}

We assume that the underlying populations are normally distributed. This assumption justifies the use of the \protect\hyperlink{students-t-distribution}{Student's T Distribution}, which is critical for hypothesis testing and constructing confidence intervals.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Unequal Variances
\end{enumerate}

\(\sigma_y^2 \neq \sigma_x^2\)

\textbf{Test Statistic}

The test statistic is:

\[
T = \frac{\bar{y} - \bar{x} - (\mu_y - \mu_x)}{\sqrt{\frac{s_y^2}{n_y} + \frac{s_x^2}{n_x}}}
\]

\textbf{Degrees of Freedom (Welch-Satterthwaite Approximation)} \citep{Satterthwaite_1946}

The degrees of freedom are approximated by:

\[
v = \frac{\left(\frac{s_y^2}{n_y} + \frac{s_x^2}{n_x}\right)^2}{\frac{\left(\frac{s_y^2}{n_y}\right)^2}{n_y - 1} + \frac{\left(\frac{s_x^2}{n_x}\right)^2}{n_x - 1}}
\]

Since \(v\) is fractional, truncate to the nearest integer.

\textbf{Confidence Interval}

A \(100(1 - \alpha)\%\) confidence interval for \(\mu_y - \mu_x\) is:

\[
\bar{y} - \bar{x} \pm t_{v, \alpha/2} \sqrt{\frac{s_y^2}{n_y} + \frac{s_x^2}{n_x}}
\]

\textbf{Hypothesis Test}

Testing:

\[
H_0: \mu_y - \mu_x = \delta_0 \quad \text{vs.} \quad H_a: \mu_y - \mu_x \neq \delta_0
\]

Reject \(H_0\) if:

\[
|T| > t_{v, \alpha/2}
\]

where

\[
t = \frac{\bar{y} - \bar{x}-\delta_0}{\sqrt{s^2_y/n_y + s^2_x /n_x}}
\]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Small sample test with unequal variance}
\NormalTok{t\_test\_unequal }\OtherTok{\textless{}{-}} \FunctionTok{t.test}\NormalTok{(y, x, }\AttributeTok{var.equal =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{t\_test\_unequal}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Welch Two Sample t{-}test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  y and x}
\CommentTok{\#\textgreater{} t = 0.5, df = 8, p{-}value = 0.6305}
\CommentTok{\#\textgreater{} alternative hypothesis: true difference in means is not equal to 0}
\CommentTok{\#\textgreater{} 95 percent confidence interval:}
\CommentTok{\#\textgreater{}  {-}3.612008  5.612008}
\CommentTok{\#\textgreater{} sample estimates:}
\CommentTok{\#\textgreater{} mean of x mean of y }
\CommentTok{\#\textgreater{}        14        13}
\end{Highlighting}
\end{Shaded}

\hypertarget{for-variances}{%
\subsection{For Variances}\label{for-variances}}

To compare the variances of two independent samples, we can use the \textbf{F-test}. The test statistic is defined as:

\[
F_{ndf,ddf} = \frac{s_1^2}{s_2^2}
\]

where \(s_1^2 > s_2^2\), \(ndf = n_1 - 1\), and \(ddf = n_2 - 1\) are the numerator and denominator degrees of freedom, respectively.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{f-test}{%
\subsubsection{F-Test}\label{f-test}}

The hypotheses for the F-test are:

\[
H_0: \sigma_y^2 = \sigma_x^2 \quad \text{(equal variances)} \\
H_a: \sigma_y^2 \neq \sigma_x^2 \quad \text{(unequal variances)}
\]

The test statistic is:

\[
F = \frac{s_y^2}{s_x^2}
\]

where \(s_y^2\) and \(s_x^2\) are the sample variances of the two groups.

\textbf{Decision Rule}

Reject \(H_0\) if:

\begin{itemize}
\item
  \(F > F_{n_y-1, n_x-1, \alpha/2}\) (upper critical value), or
\item
  \(F < F_{n_y-1, n_x-1, 1-\alpha/2}\) (lower critical value).
\end{itemize}

Here:

\begin{itemize}
\tightlist
\item
  \(F_{n_y-1, n_x-1, \alpha/2}\) and \(F_{n_y-1, n_x-1, 1-\alpha/2}\) are the critical points of the \textbf{F-distribution}, with \(n_y - 1\) and \(n_x - 1\) degrees of freedom.
\end{itemize}

\textbf{Assumptions}

\begin{itemize}
\tightlist
\item
  The F-test requires that the data in both groups follow a \textbf{normal distribution}.
\item
  The F-test is sensitive to deviations from normality (e.g., heavy-tailed distributions). If the normality assumption is violated, it may lead to an inflated Type I error rate (false positives).
\end{itemize}

\textbf{Limitations and Alternatives}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Sensitivity to Non-Normality}:

  \begin{itemize}
  \tightlist
  \item
    When data have long-tailed distributions (positive kurtosis), the F-test may produce misleading results.
  \item
    To assess normality, see \protect\hyperlink{normality-assessment}{Normality Assessment}.
  \end{itemize}
\item
  \textbf{Nonparametric Alternatives}:

  \begin{itemize}
  \tightlist
  \item
    If the normality assumption is not met, use robust tests such as the \protect\hyperlink{modified-levene-test-brown-forsythe-test}{Modified Levene Test (Brown-Forsythe Test)}, which compares group variances based on medians instead of means.
  \end{itemize}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load iris dataset}
\FunctionTok{data}\NormalTok{(iris)}

\CommentTok{\# Subset data for two species}
\NormalTok{irisVe }\OtherTok{\textless{}{-}}\NormalTok{ iris}\SpecialCharTok{$}\NormalTok{Petal.Width[iris}\SpecialCharTok{$}\NormalTok{Species }\SpecialCharTok{==} \StringTok{"versicolor"}\NormalTok{]}
\NormalTok{irisVi }\OtherTok{\textless{}{-}}\NormalTok{ iris}\SpecialCharTok{$}\NormalTok{Petal.Width[iris}\SpecialCharTok{$}\NormalTok{Species }\SpecialCharTok{==} \StringTok{"virginica"}\NormalTok{]}

\CommentTok{\# Perform F{-}test}
\NormalTok{f\_test }\OtherTok{\textless{}{-}} \FunctionTok{var.test}\NormalTok{(irisVe, irisVi)}

\CommentTok{\# Display results}
\NormalTok{f\_test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  F test to compare two variances}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  irisVe and irisVi}
\CommentTok{\#\textgreater{} F = 0.51842, num df = 49, denom df = 49, p{-}value = 0.02335}
\CommentTok{\#\textgreater{} alternative hypothesis: true ratio of variances is not equal to 1}
\CommentTok{\#\textgreater{} 95 percent confidence interval:}
\CommentTok{\#\textgreater{}  0.2941935 0.9135614}
\CommentTok{\#\textgreater{} sample estimates:}
\CommentTok{\#\textgreater{} ratio of variances }
\CommentTok{\#\textgreater{}          0.5184243}
\end{Highlighting}
\end{Shaded}

\hypertarget{levenes-test}{%
\subsubsection{Levene's Test}\label{levenes-test}}

\textbf{Levene's Test} is a robust method for testing the equality of variances across multiple groups. Unlike the F-test, it is less sensitive to departures from normality and is particularly useful for handling non-normal distributions and datasets with outliers. The test works by analyzing the deviations of individual observations from their group mean or median.

\textbf{Test Procedure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute the absolute deviations of each observation from its group mean or median:

  \begin{itemize}
  \tightlist
  \item
    For group \(y\): \[
    d_{y,i} = |y_i - \text{Central Value}_y|
    \]
  \item
    For group \(x\): \[
    d_{x,j} = |x_j - \text{Central Value}_x|
    \]
  \item
    The ``central value'' can be either the \textbf{mean} (classic Levene's test) or the \textbf{median} (\protect\hyperlink{modified-levene-test-brown-forsythe-test}{Modified Levene Test (Brown-Forsythe Test)} variation, more robust for non-normal data).
  \end{itemize}
\item
  Perform a one-way ANOVA on the absolute deviations to test for differences in group variances.
\end{enumerate}

\textbf{Hypotheses}

\begin{itemize}
\tightlist
\item
  Null Hypothesis (\(H_0\)): All groups have equal variances.
\item
  Alternative Hypothesis (\(H_a\)): At least one group has a variance different from the others.
\end{itemize}

\textbf{Test Statistic}

The Levene test statistic is calculated as an ANOVA on the absolute deviations. Let:

\begin{itemize}
\item
  \(k\): Number of groups,
\item
  \(n_i\): Number of observations in group \(i\),
\item
  \(n\): Total number of observations.
\end{itemize}

The test statistic is:

\[
W = \frac{(n - k) \sum_{i=1}^k n_i (\bar{d}_i - \bar{d})^2}{(k - 1) \sum_{i=1}^k \sum_{j=1}^{n_i} (d_{i,j} - \bar{d}_i)^2}
\]

where:

\begin{itemize}
\item
  \(d_{i,j}\): Absolute deviations within group \(i\),
\item
  \(\bar{d}_i\): Mean of the absolute deviations for group \(i\),
\item
  \(\bar{d}\): Overall mean of the absolute deviations.
\end{itemize}

Under the null hypothesis, \(W \sim F_{k-1, n - k}\).

\textbf{Decision Rule}

\begin{itemize}
\tightlist
\item
  Compute the test statistic \(W\).
\item
  Reject \(H_0\) at significance level \(\alpha\) if: \[
  W > F_{k-1, n-k, \alpha}
  \]
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load required package}
\FunctionTok{library}\NormalTok{(car)}

\CommentTok{\# Perform Levene\textquotesingle{}s Test (absolute deviations from the mean)}
\NormalTok{levene\_test\_mean }\OtherTok{\textless{}{-}} \FunctionTok{leveneTest}\NormalTok{(Petal.Width }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Species, }\AttributeTok{data =}\NormalTok{ iris)}

\CommentTok{\# Perform Levene\textquotesingle{}s Test (absolute deviations from the median)}
\NormalTok{levene\_test\_median }\OtherTok{\textless{}{-}}
    \FunctionTok{leveneTest}\NormalTok{(Petal.Width }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Species, }\AttributeTok{data =}\NormalTok{ iris, }\AttributeTok{center =}\NormalTok{ median)}

\CommentTok{\# Display results}
\NormalTok{levene\_test\_mean}
\CommentTok{\#\textgreater{} Levene\textquotesingle{}s Test for Homogeneity of Variance (center = median)}
\CommentTok{\#\textgreater{}        Df F value    Pr(\textgreater{}F)    }
\CommentTok{\#\textgreater{} group   2  19.892 2.261e{-}08 ***}
\CommentTok{\#\textgreater{}       147                      }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\NormalTok{levene\_test\_median}
\CommentTok{\#\textgreater{} Levene\textquotesingle{}s Test for Homogeneity of Variance (center = median)}
\CommentTok{\#\textgreater{}        Df F value    Pr(\textgreater{}F)    }
\CommentTok{\#\textgreater{} group   2  19.892 2.261e{-}08 ***}
\CommentTok{\#\textgreater{}       147                      }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

The output includes:

\begin{itemize}
\item
  \textbf{Df}: Degrees of freedom for the numerator and denominator.
\item
  \textbf{F-value}: The computed value of the test statistic \(W\).
\item
  \textbf{p-value}: The probability of observing such a value under the null hypothesis.
\item
  If the p-value is less than \(\alpha\), reject \(H_0\) and conclude that the group variances are significantly different.
\item
  Otherwise, fail to reject \(H_0\) and conclude there is no evidence of a difference in variances.
\end{itemize}

\textbf{Advantages of Levene's Test}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Robustness}:

  \begin{itemize}
  \tightlist
  \item
    Handles non-normal data and outliers better than the F-test.
  \end{itemize}
\item
  \textbf{Flexibility}:

  \begin{itemize}
  \item
    By choosing the center value (mean or median), it can adapt to different data characteristics:

    \begin{itemize}
    \item
      Use the mean for symmetric distributions.
    \item
      Use the median for non-normal or skewed data.
    \end{itemize}
  \end{itemize}
\item
  \textbf{Versatility}:

  \begin{itemize}
  \tightlist
  \item
    Applicable to comparing variances across more than two groups, unlike the \protect\hyperlink{modified-levene-test-brown-forsythe-test}{Modified Levene Test (Brown-Forsythe Test)}, which is limited to two groups.
  \end{itemize}
\end{enumerate}

\hypertarget{modified-levene-test-brown-forsythe-test}{%
\subsubsection{Modified Levene Test (Brown-Forsythe Test)}\label{modified-levene-test-brown-forsythe-test}}

The \textbf{Modified Levene Test} is a robust alternative to the \protect\hyperlink{f-test}{F-test} for comparing variances between two groups. Instead of using squared deviations (as in the F-test), this test considers the \textbf{absolute deviations} from the median, making it less sensitive to non-normal data and long-tailed distributions. It is, however, still appropriate for normally distributed data.

For each sample, compute the absolute deviations from the median:

\[
d_{y,i} = |y_i - y_{.5}| \quad \text{and} \quad d_{x,i} = |x_i - x_{.5}|
\]

Let:

\begin{itemize}
\tightlist
\item
  \(\bar{d}_y\) and \(\bar{d}_x\) be the means of the absolute deviations for groups \(y\) and \(x\), respectively.
\end{itemize}

The test statistic is:

\[
t_L^* = \frac{\bar{d}_y - \bar{d}_x}{s \sqrt{\frac{1}{n_y} + \frac{1}{n_x}}}
\]

where the pooled variance \(s^2\) is:

\[
s^2 = \frac{\sum_{i=1}^{n_y} (d_{y,i} - \bar{d}_y)^2 + \sum_{j=1}^{n_x} (d_{x,j} - \bar{d}_x)^2}{n_y + n_x - 2}
\]

\textbf{Assumptions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Constant Variance of Error Terms}:\\
  The test assumes equal error variances in each group under the null hypothesis.
\item
  \textbf{Moderate Sample Size}:\\
  The approximation \(t_L^* \sim t_{n_y + n_x - 2}\) holds well for moderate or large sample sizes.
\end{enumerate}

\textbf{Decision Rule}

\begin{itemize}
\tightlist
\item
  Compute \(t_L^*\) using the formula above.
\item
  Reject the null hypothesis of equal variances if: \[
  |t_L^*| > t_{n_y + n_x - 2; \alpha/2}
  \]
\end{itemize}

This is equivalent to applying a two-sample t-test to the absolute deviations.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Absolute deviations from the median}
\NormalTok{dVe }\OtherTok{\textless{}{-}} \FunctionTok{abs}\NormalTok{(irisVe }\SpecialCharTok{{-}} \FunctionTok{median}\NormalTok{(irisVe))}
\NormalTok{dVi }\OtherTok{\textless{}{-}} \FunctionTok{abs}\NormalTok{(irisVi }\SpecialCharTok{{-}} \FunctionTok{median}\NormalTok{(irisVi))}

\CommentTok{\# Perform t{-}test on absolute deviations}
\NormalTok{levene\_test }\OtherTok{\textless{}{-}} \FunctionTok{t.test}\NormalTok{(dVe, dVi, }\AttributeTok{var.equal =} \ConstantTok{TRUE}\NormalTok{)}

\CommentTok{\# Display results}
\NormalTok{levene\_test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Two Sample t{-}test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  dVe and dVi}
\CommentTok{\#\textgreater{} t = {-}2.5584, df = 98, p{-}value = 0.01205}
\CommentTok{\#\textgreater{} alternative hypothesis: true difference in means is not equal to 0}
\CommentTok{\#\textgreater{} 95 percent confidence interval:}
\CommentTok{\#\textgreater{}  {-}0.12784786 {-}0.01615214}
\CommentTok{\#\textgreater{} sample estimates:}
\CommentTok{\#\textgreater{} mean of x mean of y }
\CommentTok{\#\textgreater{}     0.154     0.226}
\end{Highlighting}
\end{Shaded}

For small sample sizes, use the unequal variance t-test directly on the original data as a robust alternative:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Small sample t{-}test with unequal variances}
\NormalTok{small\_sample\_test }\OtherTok{\textless{}{-}} \FunctionTok{t.test}\NormalTok{(irisVe, irisVi, }\AttributeTok{var.equal =} \ConstantTok{FALSE}\NormalTok{)}

\CommentTok{\# Display results}
\NormalTok{small\_sample\_test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Welch Two Sample t{-}test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  irisVe and irisVi}
\CommentTok{\#\textgreater{} t = {-}14.625, df = 89.043, p{-}value \textless{} 2.2e{-}16}
\CommentTok{\#\textgreater{} alternative hypothesis: true difference in means is not equal to 0}
\CommentTok{\#\textgreater{} 95 percent confidence interval:}
\CommentTok{\#\textgreater{}  {-}0.7951002 {-}0.6048998}
\CommentTok{\#\textgreater{} sample estimates:}
\CommentTok{\#\textgreater{} mean of x mean of y }
\CommentTok{\#\textgreater{}     1.326     2.026}
\end{Highlighting}
\end{Shaded}

\hypertarget{bartletts-test}{%
\subsubsection{Bartlett's Test}\label{bartletts-test}}

The \textbf{Bartlett's Test} is a statistical procedure for testing the equality of variances across multiple groups. It assumes that the data in each group are normally distributed and is sensitive to deviations from normality. When the assumption of normality holds, Bartlett's Test is more powerful than \protect\hyperlink{levenes-test}{Levene's Test}.

\textbf{Hypotheses for Bartlett's Test}

\begin{itemize}
\tightlist
\item
  Null Hypothesis (\(H_0\)): All groups have equal variances.
\item
  Alternative Hypothesis (\(H_a\)): At least one group has a variance different from the others.
\end{itemize}

The test statistic for Bartlett's Test is:

\[
B = \frac{(n - k) \log(S_p^2) - \sum_{i=1}^k (n_i - 1) \log(S_i^2)}{1 + \frac{1}{3(k - 1)} \left( \sum_{i=1}^k \frac{1}{n_i - 1} - \frac{1}{n - k} \right)}
\]

Where:

\begin{itemize}
\item
  \(k\): Number of groups,
\item
  \(n_i\): Number of observations in group \(i\),
\item
  \(n = \sum_{i=1}^k n_i\): Total number of observations,
\item
  \(S_i^2\): Sample variance of group \(i\),
\item
  \(S_p^2\): Pooled variance, given by: \[
    S_p^2 = \frac{\sum_{i=1}^k (n_i - 1) S_i^2}{n - k}
    \]
\end{itemize}

Under the null hypothesis, the test statistic \(B \sim \chi^2_{k - 1}\).

\textbf{Assumptions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Normality}: The data in each group must follow a normal distribution.
\item
  \textbf{Independence}: Observations within and between groups must be independent.
\item
  \textbf{Equal Sample Sizes (Optional)}: Bartlett's Test is more robust if sample sizes are approximately equal.
\end{enumerate}

\textbf{Decision Rule}

\begin{itemize}
\tightlist
\item
  Compute the test statistic \(B\).
\item
  Compare \(B\) to the critical value of the Chi-Square distribution at \(\alpha\) and \(k - 1\) degrees of freedom.
\item
  Reject \(H_0\) if: \[
  B > \chi^2_{k-1, \alpha}
  \]
\end{itemize}

Alternatively, use the p-value:

\begin{itemize}
\tightlist
\item
  Reject \(H_0\) if the p-value \(\leq \alpha\).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Perform Bartlett\textquotesingle{}s Test}
\NormalTok{bartlett\_test }\OtherTok{\textless{}{-}} \FunctionTok{bartlett.test}\NormalTok{(Petal.Width }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Species, }\AttributeTok{data =}\NormalTok{ iris)}

\CommentTok{\# Display results}
\NormalTok{bartlett\_test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Bartlett test of homogeneity of variances}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  Petal.Width by Species}
\CommentTok{\#\textgreater{} Bartlett\textquotesingle{}s K{-}squared = 39.213, df = 2, p{-}value = 3.055e{-}09}
\end{Highlighting}
\end{Shaded}

The output includes:

\begin{itemize}
\item
  \textbf{Bartlett's K-squared}: The value of the test statistic \(B\).
\item
  \textbf{df}: Degrees of freedom (\(k - 1\)), where \(k\) is the number of groups.
\item
  \textbf{p-value}: The probability of observing such a value of \(B\) under \(H_0\).
\item
  If the p-value is less than \(\alpha\), reject \(H_0\) and conclude that the variances are significantly different across groups.
\item
  If the p-value is greater than \(\alpha\), fail to reject \(H_0\) and conclude that there is no significant evidence of variance differences.
\end{itemize}

\textbf{Limitations of Bartlett's Test}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Sensitivity to Non-Normality}:\\
  Bartlett's Test is highly sensitive to departures from normality. Even slight deviations can lead to misleading results.
\item
  \textbf{Not Robust to Outliers}:\\
  Outliers can disproportionately affect the test result.
\item
  \textbf{Alternatives}:\\
  If the normality assumption is violated, use robust alternatives like:

  \begin{itemize}
  \item
    \protect\hyperlink{levenes-test}{Levene's Test} (absolute deviations)
  \item
    \protect\hyperlink{modified-levene-test-brown-forsythe-test}{Modified Levene Test (Brown-Forsythe Test)} (median-based absolute deviations)
  \end{itemize}
\end{enumerate}

\textbf{Advantages of Bartlett's Test}

\begin{itemize}
\item
  \textbf{High Power}: Bartlett's Test is more powerful than robust alternatives when the normality assumption holds.
\item
  \textbf{Simple Implementation}: The test is easy to perform and interpret.
\end{itemize}

\hypertarget{power}{%
\subsection{Power}\label{power}}

To evaluate the power of a test, we consider the situation where the variances are equal across groups:

\[
\sigma_y^2 = \sigma_x^2 = \sigma^2
\]

Under the assumption of equal variances, we take \textbf{equal sample sizes} from both groups, i.e., \(n_y = n_x = n\).

\textbf{Hypotheses for One-Sided Testing}

We are testing:

\[
H_0: \mu_y - \mu_x \leq 0 \quad \text{vs.} \quad H_a: \mu_y - \mu_x > 0
\]

\textbf{Test Statistic}

The \(\alpha\)-level \textbf{z-test} rejects \(H_0\) if the test statistic:

\[
z = \frac{\bar{y} - \bar{x}}{\sigma \sqrt{\frac{2}{n}}} > z_\alpha
\]

where:

\begin{itemize}
\item
  \(\bar{y}\) and \(\bar{x}\) are the sample means,
\item
  \(\sigma\) is the common standard deviation,
\item
  \(z_\alpha\) is the critical value from the standard normal distribution.
\end{itemize}

\textbf{Power Function}

The \textbf{power} of the test, denoted as \(\pi(\mu_y - \mu_x)\), is the probability of correctly rejecting \(H_0\) when \(\mu_y - \mu_x\) is some specified value. Under the alternative hypothesis, the power function is:

\[
\pi(\mu_y - \mu_x) = \Phi\left(-z_\alpha + \frac{\mu_y - \mu_x}{\sigma} \sqrt{\frac{n}{2}}\right)
\]

where:

\begin{itemize}
\item
  \(\Phi\) is the cumulative distribution function (CDF) of the standard normal distribution,
\item
  \(\frac{\mu_y - \mu_x}{\sigma} \sqrt{\frac{n}{2}}\) represents the standardized effect size.
\end{itemize}

\textbf{Determining the Required Sample Size}

To achieve a desired power of \(1 - \beta\) when the true difference is \(\delta\) (the smallest difference of interest), we solve for the required sample size \(n\). The power equation is:

\[
\Phi\left(-z_\alpha + \frac{\delta}{\sigma} \sqrt{\frac{n}{2}}\right) = 1 - \beta
\]

Rearranging for \(n\), the required sample size is:

\[
n = \frac{2 \sigma^2}{\delta^2} \left(z_\alpha + z_\beta\right)^2
\]

where:

\begin{itemize}
\item
  \(\sigma\): The common standard deviation,
\item
  \(z_{\alpha}\): The critical value for the Type I error rate \(\alpha\) (one-sided test),
\item
  \(z_{\beta}\): The critical value for the Type II error rate \(\beta\) (related to power \(1 - \beta\)),
\item
  \(\delta\): The minimum detectable difference between the means.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Parameters}
\NormalTok{alpha }\OtherTok{\textless{}{-}} \FloatTok{0.05}   \CommentTok{\# Significance level}
\NormalTok{beta }\OtherTok{\textless{}{-}} \FloatTok{0.2}     \CommentTok{\# Type II error rate (1 {-} Power = 0.2)}
\NormalTok{sigma }\OtherTok{\textless{}{-}} \DecValTok{1}      \CommentTok{\# Common standard deviation}
\NormalTok{delta }\OtherTok{\textless{}{-}} \FloatTok{0.5}    \CommentTok{\# Minimum detectable difference}

\CommentTok{\# Critical values}
\NormalTok{z\_alpha }\OtherTok{\textless{}{-}} \FunctionTok{qnorm}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alpha)}
\NormalTok{z\_beta }\OtherTok{\textless{}{-}} \FunctionTok{qnorm}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ beta)}

\CommentTok{\# Sample size calculation}
\NormalTok{n }\OtherTok{\textless{}{-}}\NormalTok{ (}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ sigma }\SpecialCharTok{\^{}} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ (z\_alpha }\SpecialCharTok{+}\NormalTok{ z\_beta) }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{) }\SpecialCharTok{/}\NormalTok{ delta }\SpecialCharTok{\^{}} \DecValTok{2}

\CommentTok{\# Output the required sample size (per group)}
\FunctionTok{ceiling}\NormalTok{(n)}
\CommentTok{\#\textgreater{} [1] 50}
\end{Highlighting}
\end{Shaded}

\textbf{Sample Size for Two-Sided Tests}

For a \textbf{two-sided test}, replace \(z_{\alpha}\) with \(z_{\alpha/2}\) to account for the two-tailed critical region:

\[
n = 2 \left( \frac{\sigma (z_{\alpha/2} + z_{\beta})}{\delta} \right)^2
\]

This ensures that the test has the required power \(1 - \beta\) to detect a difference of size \(\delta\) between the means at significance level \(\alpha\).

\textbf{Adjustment for the Exact t-Test}

When conducting an exact \textbf{two-sample t-test} for small sample sizes, the sample size calculation involves the \textbf{non-central t-distribution}. An approximate correction can be applied using the critical values from the t-distribution instead of the z-distribution.

The adjusted sample size is:

\[
n^* = 2 \left( \frac{\sigma (t_{2n-2; \alpha/2} + t_{2n-2; \beta})}{\delta} \right)^2
\]

Where:

\begin{itemize}
\item
  \(t_{2n-2; \alpha/2}\): The critical value for the t-distribution with \(2n - 2\) degrees of freedom for significance level \(\alpha/2\),
\item
  \(t_{2n-2; \beta}\): The critical value for the t-distribution with \(2n - 2\) degrees of freedom for power \(1 - \beta\).
\end{itemize}

This correction adjusts for the increased variability of the t-distribution, especially important for small sample sizes.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Parameters}
\NormalTok{alpha }\OtherTok{\textless{}{-}} \FloatTok{0.05}    \CommentTok{\# Significance level}
\NormalTok{power }\OtherTok{\textless{}{-}} \FloatTok{0.8}     \CommentTok{\# Desired power}
\NormalTok{sigma }\OtherTok{\textless{}{-}} \DecValTok{1}       \CommentTok{\# Common standard deviation}
\NormalTok{delta }\OtherTok{\textless{}{-}} \FloatTok{0.5}     \CommentTok{\# Minimum detectable difference}

\CommentTok{\# Calculate sample size for two{-}sided test}
\NormalTok{sample\_size }\OtherTok{\textless{}{-}}
    \FunctionTok{power.t.test}\NormalTok{(}
        \AttributeTok{delta =}\NormalTok{ delta,}
        \AttributeTok{sd =}\NormalTok{ sigma,}
        \AttributeTok{sig.level =}\NormalTok{ alpha,}
        \AttributeTok{power =}\NormalTok{ power,}
        \AttributeTok{type =} \StringTok{"two.sample"}\NormalTok{,}
        \AttributeTok{alternative =} \StringTok{"two.sided"}
\NormalTok{    )}

\CommentTok{\# Display results}
\NormalTok{sample\_size}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}      Two{-}sample t test power calculation }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}               n = 63.76576}
\CommentTok{\#\textgreater{}           delta = 0.5}
\CommentTok{\#\textgreater{}              sd = 1}
\CommentTok{\#\textgreater{}       sig.level = 0.05}
\CommentTok{\#\textgreater{}           power = 0.8}
\CommentTok{\#\textgreater{}     alternative = two.sided}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }\AlertTok{NOTE}\CommentTok{: n is number in *each* group}
\end{Highlighting}
\end{Shaded}

\textbf{Key Insights}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Z-Test vs.~T-Test}:\\
  For large samples, the normal approximation (z-test) works well. For small samples, the t-test correction using the t-distribution is essential.
\item
  \textbf{Effect of Power and Significance Level}:

  \begin{itemize}
  \item
    Increasing power (\(1 - \beta\)) or decreasing \(\alpha\) requires larger sample sizes.
  \item
    A smaller minimum detectable difference (\(\delta\)) also requires a larger sample size.
  \end{itemize}
\item
  \textbf{Two-Sided Tests}:\\
  Two-sided tests require larger sample sizes compared to one-sided tests due to the split critical region.
\end{enumerate}

\textbf{Formula Summary}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.1892}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.8108}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Test Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Formula for Sample Size
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
One-Sided Test & \(n = 2 \left( \frac{\sigma (z_{\alpha} + z_{\beta})}{\delta} \right)^2\) \\
Two-Sided Test & \(n = 2 \left( \frac{\sigma (z_{\alpha/2} + z_{\beta})}{\delta} \right)^2\) \\
Approximate t-Test & \(n^* = 2 \left( \frac{\sigma (t_{2n-2; \alpha/2} + t_{2n-2; \beta})}{\delta} \right)^2\) \\
\end{longtable}

\hypertarget{matched-pair-designs}{%
\subsection{Matched Pair Designs}\label{matched-pair-designs}}

In \textbf{matched pair designs}, two treatments are compared by measuring responses for the same subjects under both treatments. This ensures that the effects of subject-to-subject variability are minimized, as each subject serves as their own control.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

We have two treatments, and the data are structured as follows:

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Subject & Treatment A & Treatment B & Difference \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & \(y_1\) & \(x_1\) & \(d_1 = y_1 - x_1\) \\
2 & \(y_2\) & \(x_2\) & \(d_2 = y_2 - x_2\) \\
\ldots{} & \ldots{} & \ldots{} & \ldots{} \\
n & \(y_n\) & \(x_n\) & \(d_n = y_n - x_n\) \\
\end{longtable}

Here:

\begin{itemize}
\item
  \(y_i\) represents the observation under Treatment A,
\item
  \(x_i\) represents the observation under Treatment B,
\item
  \(d_i = y_i - x_i\) is the difference for subject \(i\).
\end{itemize}

\textbf{Assumptions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Observations \(y_i\) and \(x_i\) are measured for the same subjects, inducing correlation.
\item
  The differences \(d_i\) are independent and identically distributed (iid), and follow a normal distribution: \[
  d_i \sim N(\mu_D, \sigma_D^2)
  \]
\end{enumerate}

\textbf{Mean and Variance of the Difference}

The mean difference \(\mu_D\) and the variance \(\sigma_D^2\) are given by:

\[
\mu_D = E(y_i - x_i) = \mu_y - \mu_x
\]

\[
\sigma_D^2 = \text{Var}(y_i - x_i) = \text{Var}(y_i) + \text{Var}(x_i) - 2 \cdot \text{Cov}(y_i, x_i)
\]

\begin{itemize}
\tightlist
\item
  If the \textbf{covariance} between \(y_i\) and \(x_i\) is \textbf{positive} (a typical case), the variance of the differences \(\sigma_D^2\) is \textbf{reduced} compared to the independent sample case.
\item
  This is the key benefit of \textbf{Matched Pair Designs}: reduced variability increases the precision of estimates.
\end{itemize}

\textbf{Sample Statistics}

For the differences \(d_i = y_i - x_i\):

\begin{itemize}
\item
  The sample mean of the differences: \[
  \bar{d} = \frac{1}{n} \sum_{i=1}^n d_i = \bar{y} - \bar{x}
  \]
\item
  The sample variance of the differences: \[
  s_d^2 = \frac{1}{n-1} \sum_{i=1}^n (d_i - \bar{d})^2
  \]
\end{itemize}

Once the data are converted into differences \(d_i\), the problem reduces to \textbf{one-sample inference}. We can use tests and confidence intervals (CIs) for the mean of a single sample.

\textbf{Hypothesis Test}

We test the following hypotheses:

\[
H_0: \mu_D = 0 \quad \text{vs.} \quad H_a: \mu_D \neq 0
\]

The test statistic is:

\[
t = \frac{\bar{d}}{s_d / \sqrt{n}} \sim t_{n-1}
\]

where \(n\) is the number of subjects.

\begin{itemize}
\tightlist
\item
  Reject \(H_0\) at significance level \(\alpha\) if: \[
  |t| > t_{n-1, \alpha/2}
  \]
\end{itemize}

\textbf{Confidence Interval}

A \(100(1 - \alpha)\%\) confidence interval for \(\mu_D\) is:

\[
\bar{d} \pm t_{n-1, \alpha/2} \cdot \frac{s_d}{\sqrt{n}}
\]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Sample data}
\NormalTok{treatment\_a }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{85}\NormalTok{, }\DecValTok{90}\NormalTok{, }\DecValTok{78}\NormalTok{, }\DecValTok{92}\NormalTok{, }\DecValTok{88}\NormalTok{)}
\NormalTok{treatment\_b }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{80}\NormalTok{, }\DecValTok{86}\NormalTok{, }\DecValTok{75}\NormalTok{, }\DecValTok{89}\NormalTok{, }\DecValTok{85}\NormalTok{)}

\CommentTok{\# Compute differences}
\NormalTok{differences }\OtherTok{\textless{}{-}}\NormalTok{ treatment\_a }\SpecialCharTok{{-}}\NormalTok{ treatment\_b}

\CommentTok{\# Perform one{-}sample t{-}test on the differences}
\NormalTok{t\_test }\OtherTok{\textless{}{-}} \FunctionTok{t.test}\NormalTok{(differences, }\AttributeTok{mu =} \DecValTok{0}\NormalTok{, }\AttributeTok{alternative =} \StringTok{"two.sided"}\NormalTok{)}

\CommentTok{\# Display results}
\NormalTok{t\_test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  One Sample t{-}test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  differences}
\CommentTok{\#\textgreater{} t = 9, df = 4, p{-}value = 0.0008438}
\CommentTok{\#\textgreater{} alternative hypothesis: true mean is not equal to 0}
\CommentTok{\#\textgreater{} 95 percent confidence interval:}
\CommentTok{\#\textgreater{}  2.489422 4.710578}
\CommentTok{\#\textgreater{} sample estimates:}
\CommentTok{\#\textgreater{} mean of x }
\CommentTok{\#\textgreater{}       3.6}
\end{Highlighting}
\end{Shaded}

The output includes:

\begin{itemize}
\item
  \textbf{t-statistic}: The calculated test statistic for the matched pairs.
\item
  \textbf{p-value}: The probability of observing such a difference under the null hypothesis.
\item
  \textbf{Confidence Interval}: The range of plausible values for the mean difference \(\mu_D\).
\item
  If the p-value is less than \(\alpha\), reject \(H_0\) and conclude that there is a significant difference between the two treatments.
\item
  If the confidence interval does not include 0, this supports the conclusion of a significant difference.
\end{itemize}

\textbf{Key Insights}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Reduced Variability}: Positive correlation between paired observations reduces the variance of the differences, increasing test power.
\item
  \textbf{Use of Differences}: The paired design converts the data into a single-sample problem for inference.
\item
  \textbf{Robustness}: The paired t-test assumes normality of the differences \(d_i\). For larger \(n\), the \protect\hyperlink{central-limit-theorem}{Central Limit Theorem} ensures robustness to non-normality.
\end{enumerate}

Matched pair designs are a powerful way to control for subject-specific variability, leading to more precise comparisons between treatments.

\hypertarget{nonparametric-tests-for-two-samples}{%
\subsection{Nonparametric Tests for Two Samples}\label{nonparametric-tests-for-two-samples}}

For \protect\hyperlink{matched-pair-designs}{Matched Pair Designs} or independent samples where normality cannot be assumed, we use \textbf{nonparametric tests}. These tests do not assume any specific distribution of the data and are robust alternatives to parametric methods.

\textbf{Stochastic Order and Location Shift}

Suppose \(Y\) and \(X\) are random variables with cumulative distribution functions (CDFs) \(F_Y\) and \(F_X\). Then \(Y\) is \textbf{stochastically larger} than \(X\) if, for all real numbers \(u\):

\[
P(Y > u) \geq P(X > u) \quad \text{(equivalently, } F_Y(u) \leq F_X(u)).
\]

If the two distributions differ only in their \textbf{location parameters}, say \(\theta_y\) and \(\theta_x\), then we can frame the relationship as:

\[
Y > X \quad \text{if} \quad \theta_y > \theta_x.
\]

We test the following hypotheses:

\begin{itemize}
\tightlist
\item
  \textbf{Two-Sided Hypothesis}: \[
  H_0: F_Y = F_X \quad \text{vs.} \quad H_a: F_Y \neq F_X
  \]
\item
  \textbf{Upper One-Sided Hypothesis}: \[
  H_0: F_Y = F_X \quad \text{vs.} \quad H_a: F_Y < F_X
  \]
\item
  \textbf{Lower One-Sided Hypothesis}: \[
  H_0: F_Y = F_X \quad \text{vs.} \quad H_a: F_Y > F_X
  \]
\end{itemize}

We generally avoid the completely non-directional alternative \(H_a: F_Y \neq F_X\) because it allows arbitrary differences between the distributions, without requiring one distribution to be stochastically larger than the other.

\textbf{Nonparametric Tests}

When the focus is on whether the two distributions differ \textbf{only in location parameters}, two equivalent nonparametric tests are commonly used:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \protect\hyperlink{wilcoxon-signed-rank-test}{Wilcoxon Signed Rank Test}
\item
  \protect\hyperlink{mann-whitney-u-test-1}{Mann-Whitney U Test}
\end{enumerate}

Both tests are mathematically equivalent and test whether one sample is systematically larger than the other.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{wilcoxon-rank-sum-test}{%
\subsubsection{Wilcoxon Rank-Sum Test}\label{wilcoxon-rank-sum-test}}

The \textbf{Wilcoxon Rank Test} is a nonparametric test used to compare two independent samples to assess whether their distributions differ in location. It is based on the ranks of the combined observations rather than their actual values.

\textbf{Procedure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Combine and Rank Observations}:\\
  Combine all \(n = n_y + n_x\) observations (from both groups) into a single dataset and rank them in ascending order. If ties exist, assign the average rank to tied values.
\item
  \textbf{Calculate Rank Sums}:\\
  Compute the sum of ranks for each group:

  \begin{itemize}
  \tightlist
  \item
    \(w_y\): Sum of the ranks for group \(y\) (sample 1),
  \item
    \(w_x\): Sum of the ranks for group \(x\) (sample 2).\\
    By definition: \[
    w_y + w_x = \frac{n(n+1)}{2}
    \]
  \end{itemize}
\item
  \textbf{Test Statistic}:\\
  The test focuses on the rank sum \(w_y\). Reject \(H_0\) if \(w_y\) is \textbf{large} (indicating \(y\) systematically has larger values) or equivalently, if \(w_x\) is \textbf{small}.
\item
  \textbf{Null Distribution}:\\
  Under \(H_0\) (no difference between groups), all possible arrangements of ranks among \(y\) and \(x\) are equally likely. The total number of possible rank arrangements is:

  \[
  \frac{(n_y + n_x)!}{n_y! \, n_x!}
  \]
\item
  \textbf{Computational Considerations}:

  \begin{itemize}
  \tightlist
  \item
    For small samples, the exact null distribution of the rank sums can be calculated.\\
  \item
    For large samples, an \textbf{approximate normal distribution} can be used.
  \end{itemize}
\end{enumerate}

\textbf{Hypotheses}

\begin{itemize}
\item
  Null Hypothesis (\(H_0\)): The two samples come from identical distributions.
\item
  Alternative Hypothesis (\(H_a\)): The two samples come from different distributions, or one distribution is systematically larger.
\item
  \textbf{Two-Sided Test}: \[
  H_a: F_Y \neq F_X
  \]
\item
  \textbf{One-Sided Test}: \[
  H_a: F_Y > F_X \quad \text{or} \quad H_a: F_Y < F_X
  \]
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Subset data for two species}
\NormalTok{irisVe }\OtherTok{\textless{}{-}}\NormalTok{ iris}\SpecialCharTok{$}\NormalTok{Petal.Width[iris}\SpecialCharTok{$}\NormalTok{Species }\SpecialCharTok{==} \StringTok{"versicolor"}\NormalTok{]}
\NormalTok{irisVi }\OtherTok{\textless{}{-}}\NormalTok{ iris}\SpecialCharTok{$}\NormalTok{Petal.Width[iris}\SpecialCharTok{$}\NormalTok{Species }\SpecialCharTok{==} \StringTok{"virginica"}\NormalTok{]}

\CommentTok{\# Perform Wilcoxon Rank Test (approximate version, large sample)}
\NormalTok{wilcox\_result }\OtherTok{\textless{}{-}} \FunctionTok{wilcox.test}\NormalTok{(}
\NormalTok{    irisVe,}
\NormalTok{    irisVi,}
    \AttributeTok{alternative =} \StringTok{"two.sided"}\NormalTok{, }\CommentTok{\# Two{-}sided test}
    \AttributeTok{conf.level =} \FloatTok{0.95}\NormalTok{,         }\CommentTok{\# Confidence level}
    \AttributeTok{exact =} \ConstantTok{FALSE}\NormalTok{,             }\CommentTok{\# Approximate test for large samples}
    \AttributeTok{correct =} \ConstantTok{TRUE}             \CommentTok{\# Apply continuity correction}
\NormalTok{)}

\CommentTok{\# Display results}
\NormalTok{wilcox\_result}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Wilcoxon rank sum test with continuity correction}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  irisVe and irisVi}
\CommentTok{\#\textgreater{} W = 49, p{-}value \textless{} 2.2e{-}16}
\CommentTok{\#\textgreater{} alternative hypothesis: true location shift is not equal to 0}
\end{Highlighting}
\end{Shaded}

The output of \texttt{wilcox.test} includes:

\begin{itemize}
\item
  \textbf{W}: The test statistic, which is the smaller of the two rank sums.
\item
  \textbf{p-value}: The probability of observing such a difference in rank sums under \(H_0\).
\item
  \textbf{Alternative Hypothesis}: Specifies whether the test was one-sided or two-sided.
\item
  \textbf{Confidence Interval} (if applicable): Provides a range for the difference in medians.
\end{itemize}

\textbf{Decision Rule}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Reject \(H_0\) at significance level \(\alpha\) if the p-value \(\leq \alpha\).
\item
  For large samples, compare the test statistic to a critical value from the normal approximation.
\end{enumerate}

\textbf{Key Features}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Robustness}:\\
  The test does not require assumptions of normality and is robust to outliers.
\item
  \textbf{Distribution-Free}:\\
  It evaluates whether two samples differ in location without assuming a specific distribution.
\item
  \textbf{Rank-Based}:\\
  It uses the ranks of the observations, which makes it scale-invariant (resistant to data transformation).
\end{enumerate}

\textbf{Computational Considerations}

\begin{itemize}
\item
  For small sample sizes, the \textbf{exact distribution} of the rank sums is used.
\item
  For large sample sizes, the \textbf{normal approximation} with continuity correction is applied for computational efficiency.
\end{itemize}

\hypertarget{mann-whitney-u-test-1}{%
\subsubsection{Mann-Whitney U Test}\label{mann-whitney-u-test-1}}

The \textbf{Mann-Whitney U Test} is a nonparametric test used to compare two independent samples. It evaluates whether one sample tends to produce larger observations than the other, based on pairwise comparisons. The test does not assume normality and is robust to outliers.

\textbf{Procedure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Pairwise Comparisons}:\\
  Compare each observation \(y_i\) from sample \(Y\) with each observation \(x_j\) from sample \(X\).

  \begin{itemize}
  \tightlist
  \item
    Let \(u_y\) be the number of pairs where \(y_i > x_j\).
  \item
    Let \(u_x\) be the number of pairs where \(y_i < x_j\).
  \end{itemize}

  By definition: \[
  u_y + u_x = n_y n_x
  \] where \(n_y\) is the sample size for group \(Y\), and \(n_x\) is the sample size for group \(X\).
\item
  \textbf{Test Statistic}:\\
  Reject \(H_0\) if \(u_y\) is \textbf{large} (or equivalently, if \(u_x\) is \textbf{small}).

  The \textbf{Mann-Whitney U Test} and \textbf{Wilcoxon Rank-Sum Test} are related through the rank sums:

  \[
  u_y = w_y - \frac{n_y (n_y + 1)}{2}, \quad u_x = w_x - \frac{n_x (n_x + 1)}{2}
  \]

  Here, \(w_y\) and \(w_x\) are the rank sums for groups \(Y\) and \(X\), respectively.
\end{enumerate}

\textbf{Hypotheses}

\begin{itemize}
\tightlist
\item
  Null Hypothesis (\(H_0\)): The two samples come from identical distributions.
\item
  Alternative Hypothesis (\(H_a\)):

  \begin{itemize}
  \tightlist
  \item
    \textbf{Upper One-Sided}: \(F_Y < F_X\) (Sample \(Y\) is stochastically larger).
  \item
    \textbf{Lower One-Sided}: \(F_Y > F_X\) (Sample \(X\) is stochastically larger).
  \item
    \textbf{Two-Sided}: \(F_Y \neq F_X\) (Distributions differ in location).
  \end{itemize}
\end{itemize}

\textbf{Test Statistic for Large Samples}

For large sample sizes \(n_y\) and \(n_x\), the null distribution of \(U\) can be approximated by a normal distribution with:

\begin{itemize}
\item
  \textbf{Mean}: \[
  E(U) = \frac{n_y n_x}{2}
  \]
\item
  \textbf{Variance}: \[
  \text{Var}(U) = \frac{n_y n_x (n_y + n_x + 1)}{12}
  \]
\end{itemize}

The standardized test statistic \(z\) is:

\[
z = \frac{u_y - \frac{n_y n_x}{2} - \frac{1}{2}}{\sqrt{\frac{n_y n_x (n_y + n_x + 1)}{12}}}
\]

The test rejects \(H_0\) at level \(\alpha\) if:

\[
z \ge z_{\alpha} \quad \text{(one-sided)} \quad \text{or} \quad |z| \ge z_{\alpha/2} \quad \text{(two-sided)}.
\]

For the \textbf{two-sided test}, we use:

\begin{itemize}
\item
  \(u_{\text{max}} = \max(u_y, u_x)\), and
\item
  \(u_{\text{min}} = \min(u_y, u_x)\).
\end{itemize}

The p-value is given by:

\[
p\text{-value} = 2P(U \ge u_{\text{max}}) = 2P(U \le u_{\text{min}}).
\]

When \(y_i = x_j\) (ties), assign a value of \(1/2\) to both \(u_y\) and \(u_x\) for that pair. While the exact sampling distribution differs slightly when ties exist, the \textbf{large sample normal approximation} remains reasonable.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Subset data for two species}
\NormalTok{irisVe }\OtherTok{\textless{}{-}}\NormalTok{ iris}\SpecialCharTok{$}\NormalTok{Petal.Width[iris}\SpecialCharTok{$}\NormalTok{Species }\SpecialCharTok{==} \StringTok{"versicolor"}\NormalTok{]}
\NormalTok{irisVi }\OtherTok{\textless{}{-}}\NormalTok{ iris}\SpecialCharTok{$}\NormalTok{Petal.Width[iris}\SpecialCharTok{$}\NormalTok{Species }\SpecialCharTok{==} \StringTok{"virginica"}\NormalTok{]}

\CommentTok{\# Perform Mann{-}Whitney U Test}
\NormalTok{mann\_whitney }\OtherTok{\textless{}{-}} \FunctionTok{wilcox.test}\NormalTok{(}
\NormalTok{    irisVe, irisVi, }
    \AttributeTok{alternative =} \StringTok{"two.sided"}\NormalTok{, }
    \AttributeTok{conf.level =} \FloatTok{0.95}\NormalTok{,}
    \AttributeTok{exact =} \ConstantTok{FALSE}\NormalTok{,   }\CommentTok{\# Approximate test for large samples}
    \AttributeTok{correct =} \ConstantTok{TRUE}   \CommentTok{\# Apply continuity correction}
\NormalTok{)}

\CommentTok{\# Display results}
\NormalTok{mann\_whitney}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Wilcoxon rank sum test with continuity correction}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  irisVe and irisVi}
\CommentTok{\#\textgreater{} W = 49, p{-}value \textless{} 2.2e{-}16}
\CommentTok{\#\textgreater{} alternative hypothesis: true location shift is not equal to 0}
\end{Highlighting}
\end{Shaded}

\textbf{Decision Rule}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Reject \(H_0\) if the p-value is less than \(\alpha\).
\item
  For large samples, check whether \$z \textbackslash ge z\_\{\textbackslash alpha\}\$ (one-sided) or \$\textbar z\textbar{} \textbackslash ge z\_\{\textbackslash alpha/2\}\$ (two-sided).
\end{enumerate}

\textbf{Key Insights}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Robustness}: The Mann-Whitney U Test does not assume normality and is robust to outliers.
\item
  \textbf{Relationship to Wilcoxon Test}: The test is equivalent to the \textbf{Wilcoxon Rank-Sum Test} but formulated differently (based on pairwise comparisons).
\item
  \textbf{Large Sample Approximation}: For large \(n_y\) and \(n_x\), the test statistic \(U\) follows an approximate normal distribution, simplifying computation.
\item
  \textbf{Handling Ties}: Ties are accounted for by assigning fractional contributions to \(u_y\) and \(u_x\).
\end{enumerate}

\hypertarget{categorical-data-analysis}{%
\section{Categorical Data Analysis}\label{categorical-data-analysis}}

\textbf{Categorical Data Analysis} is used when the outcome variables are \textbf{categorical}.

\begin{itemize}
\tightlist
\item
  \textbf{Nominal Variables}: Categories have no logical order (e.g., sex: male, female).
\item
  \textbf{Ordinal Variables}: Categories have a logical order, but the relative distances between values are not well defined (e.g., small, medium, large).
\end{itemize}

In categorical data, we often analyze how the distribution of one variable changes with the levels of another variable. For example, row percentages may differ across columns in a contingency table.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{association-tests}{%
\subsection{Association Tests}\label{association-tests}}

\hypertarget{small-samples}{%
\subsubsection{Small Samples}\label{small-samples}}

\hypertarget{fishers-exact-test}{%
\paragraph{Fisher's Exact Test}\label{fishers-exact-test}}

For small samples, the approximate tests based on the asymptotic normality of \(\hat{p}_1 - \hat{p}_2\) (the difference in proportions) do not hold. In such cases, we use \textbf{Fisher's Exact Test} to evaluate:

\begin{itemize}
\tightlist
\item
  Null Hypothesis (\(H_0\)): \(p_1 = p_2\) (no association between variables),
\item
  Alternative Hypothesis (\(H_a\)): \(p_1 \neq p_2\) (an association exists).
\end{itemize}

\textbf{Assumptions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(X_1\) and \(X_2\) are independent \textbf{Binomial random variables}:

  \begin{itemize}
  \tightlist
  \item
    \(X_1 \sim \text{Binomial}(n_1, p_1)\),
  \item
    \(X_2 \sim \text{Binomial}(n_2, p_2)\).
  \end{itemize}
\item
  \(x_1\) and \(x_2\) are the observed values (successes in each sample).
\item
  Total sample size is \(n = n_1 + n_2\).
\item
  Total successes are \(m = x_1 + x_2\).
\end{enumerate}

By conditioning on \(m\), the total number of successes, the number of successes in sample 1 follows a \textbf{Hypergeometric distribution}.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Test Statistic}

To test \(H_0: p_1 = p_2\) against \(H_a: p_1 \neq p_2\), we use the test statistic:

\[
Z^2 = \left( \frac{\hat{p}_1 - \hat{p}_2}{\sqrt{\hat{p}(1 - \hat{p}) \left( \frac{1}{n_1} + \frac{1}{n_2} \right)}} \right)^2 \sim \chi^2_{1, \alpha}
\]

where:

\begin{itemize}
\item
  \(\hat{p}_1\) and \(\hat{p}_2\) are the observed proportions of successes in samples 1 and 2,
\item
  \(\hat{p}\) is the pooled proportion: \[
    \hat{p} = \frac{x_1 + x_2}{n_1 + n_2},
    \]
\item
  \(\chi^2_{1, \alpha}\) is the upper \(\alpha\) critical value of the \textbf{Chi-squared distribution} with 1 degree of freedom.
\end{itemize}

Fisher's Exact Test can be extended to a \textbf{contingency table} setting to test whether the observed frequencies differ significantly from the expected frequencies under the null hypothesis of no association.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a 2x2 contingency table}
\NormalTok{data\_table }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{), }\AttributeTok{nrow =} \DecValTok{2}\NormalTok{, }\AttributeTok{byrow =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{colnames}\NormalTok{(data\_table) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Success"}\NormalTok{, }\StringTok{"Failure"}\NormalTok{)}
\FunctionTok{rownames}\NormalTok{(data\_table) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Group 1"}\NormalTok{, }\StringTok{"Group 2"}\NormalTok{)}

\CommentTok{\# Display the table}
\NormalTok{data\_table}
\CommentTok{\#\textgreater{}         Success Failure}
\CommentTok{\#\textgreater{} Group 1       8       2}
\CommentTok{\#\textgreater{} Group 2       1       5}

\CommentTok{\# Perform Fisher\textquotesingle{}s Exact Test}
\NormalTok{fisher\_result }\OtherTok{\textless{}{-}} \FunctionTok{fisher.test}\NormalTok{(data\_table)}

\CommentTok{\# Display the results}
\NormalTok{fisher\_result}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Fisher\textquotesingle{}s Exact Test for Count Data}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  data\_table}
\CommentTok{\#\textgreater{} p{-}value = 0.03497}
\CommentTok{\#\textgreater{} alternative hypothesis: true odds ratio is not equal to 1}
\CommentTok{\#\textgreater{} 95 percent confidence interval:}
\CommentTok{\#\textgreater{}     1.008849 1049.791446}
\CommentTok{\#\textgreater{} sample estimates:}
\CommentTok{\#\textgreater{} odds ratio }
\CommentTok{\#\textgreater{}   15.46969}
\end{Highlighting}
\end{Shaded}

The output of \texttt{fisher.test()} includes:

\begin{itemize}
\item
  \textbf{p-value}: The probability of observing such a contingency table under the null hypothesis.
\item
  \textbf{Alternative Hypothesis}: Indicates whether the test is two-sided or one-sided.
\item
  If the p-value is less than \(\alpha\), reject \(H_0\) and conclude that there is a significant association between the two variables.
\end{itemize}

\hypertarget{exact-chi-square-test}{%
\paragraph{Exact Chi-Square Test}\label{exact-chi-square-test}}

For small samples where the normal approximation does not apply, we can compute the \textbf{exact Chi-Square test} by using Fisher's Exact Test or Monte Carlo simulation methods.

The Chi-Square test statistic in the 2x2 table is:

\(\chi^2 = \sum_{i=1}^r \sum_{j=1}^c \frac{(O_{ij} - E_{ij})^2}{E_{ij}}\)

where:

\begin{itemize}
\item
  \(O_{ij}\): Observed frequency in cell \((i, j)\),
\item
  \(E_{ij}\): Expected frequency under the null hypothesis,
\item
  \(r\): Number of rows,
\item
  \(c\): Number of columns.
\end{itemize}

\hypertarget{large-samples}{%
\subsubsection{Large Samples}\label{large-samples}}

\hypertarget{pearson-chi-square-test}{%
\paragraph{Pearson Chi-Square Test}\label{pearson-chi-square-test}}

The \textbf{Pearson Chi-Square Test} is commonly used to test whether there is an association between two categorical variables. It compares the observed counts in a contingency table to the expected counts under the null hypothesis.

The test statistic is:

\[
\chi^2 = \sum_{\text{all cells}} \frac{(\text{observed} - \text{expected})^2}{\text{expected}}
\]

The test is applied in settings where multiple proportions or frequencies are compared across independent surveys or experiments.

\begin{itemize}
\tightlist
\item
  \textbf{Null Hypothesis (}\(H_0\)): The observed data are consistent with the expected values (no association or no deviation from a model).
\item
  \textbf{Alternative Hypothesis (}\(H_a\)): The observed data differ significantly from the expected values.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Characteristics of the Test}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Validation of Models}:\\
  In some cases, \(H_0\) represents the model whose validity is being tested. The goal is not necessarily to reject the model but to check whether the data are consistent with it. Deviations may be due to random chance.
\item
  \textbf{Strength of Association}:\\
  The Chi-Square Test detects whether an association exists but does \textbf{not} measure the strength of the association. For measuring strength, metrics like Cramr's V or the Phi coefficient should be used.
\item
  \textbf{Effect of Sample Size}:

  \begin{itemize}
  \tightlist
  \item
    The Chi-Square statistic reflects sample size. If the sample size is doubled (e.g., duplicating observations), the \(\chi^2\) statistic will also double, even though the strength of the association remains unchanged.
  \item
    This sensitivity can sometimes lead to detecting significant results that are not practically meaningful.
  \end{itemize}
\item
  \textbf{Expected Cell Frequencies}:

  \begin{itemize}
  \tightlist
  \item
    The test is not appropriate if more than \textbf{20\% of the cells} in a contingency table have expected frequencies less than 5.
  \item
    For small sample sizes, \protect\hyperlink{fishers-exact-test}{Fisher's Exact Test} or exact p-values should be used instead.
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Test for a Single Proportion}\\
  We test whether the observed proportion of successes equals 0.5.
\end{enumerate}

\[
H_0: p_J = 0.5 \\
H_a: p_J < 0.5
\]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Observed data}
\NormalTok{july.x }\OtherTok{\textless{}{-}} \DecValTok{480}
\NormalTok{july.n }\OtherTok{\textless{}{-}} \DecValTok{1000}
\CommentTok{\# Test for single proportion}
\FunctionTok{prop.test}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ july.x,}
  \AttributeTok{n =}\NormalTok{ july.n,}
  \AttributeTok{p =} \FloatTok{0.5}\NormalTok{,}
  \AttributeTok{alternative =} \StringTok{"less"}\NormalTok{,}
  \AttributeTok{correct =} \ConstantTok{FALSE}
\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  1{-}sample proportions test without continuity correction}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  july.x out of july.n, null probability 0.5}
\CommentTok{\#\textgreater{} X{-}squared = 1.6, df = 1, p{-}value = 0.103}
\CommentTok{\#\textgreater{} alternative hypothesis: true p is less than 0.5}
\CommentTok{\#\textgreater{} 95 percent confidence interval:}
\CommentTok{\#\textgreater{}  0.0000000 0.5060055}
\CommentTok{\#\textgreater{} sample estimates:}
\CommentTok{\#\textgreater{}    p }
\CommentTok{\#\textgreater{} 0.48}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Test for Equality of Proportions Between Two Groups}: We test whether the proportions of successes in July and September are equal.
\end{enumerate}

\[
H_0: p_J = p_S \\
H_a: p_j \neq p_S
\]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Observed data for two groups}
\NormalTok{sept.x }\OtherTok{\textless{}{-}} \DecValTok{704}
\NormalTok{sept.n }\OtherTok{\textless{}{-}} \DecValTok{1600}
\CommentTok{\# Test for equality of proportions}
\FunctionTok{prop.test}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(july.x, sept.x),}
  \AttributeTok{n =} \FunctionTok{c}\NormalTok{(july.n, sept.n),}
  \AttributeTok{correct =} \ConstantTok{FALSE}
\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  2{-}sample test for equality of proportions without continuity correction}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  c(july.x, sept.x) out of c(july.n, sept.n)}
\CommentTok{\#\textgreater{} X{-}squared = 3.9701, df = 1, p{-}value = 0.04632}
\CommentTok{\#\textgreater{} alternative hypothesis: two.sided}
\CommentTok{\#\textgreater{} 95 percent confidence interval:}
\CommentTok{\#\textgreater{}  0.0006247187 0.0793752813}
\CommentTok{\#\textgreater{} sample estimates:}
\CommentTok{\#\textgreater{} prop 1 prop 2 }
\CommentTok{\#\textgreater{}   0.48   0.44}
\end{Highlighting}
\end{Shaded}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Comparison of Proportions for Multiple Groups}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2651}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1807}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1807}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1566}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1807}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Experiment 1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Experiment 2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\ldots{}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Experiment k
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Number of successes & \(x_1\) & \(x_2\) & \ldots{} & \(x_k\) \\
Number of failures & \(n_1 - x_1\) & \(n_2 - x_2\) & \ldots{} & \(n_k - x_k\) \\
Total & \(n_1\) & \(n_2\) & \ldots{} & \(n_k\) \\
\end{longtable}

We test the null hypothesis:

\[
H_0: p_1 = p_2 = \dots = p_k
\]

against the alternative that at least one proportion differs.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Pooled Proportion}

Assuming \(H_0\) is true, we estimate the common value of the probability of success as:

\[
\hat{p} = \frac{x_1 + x_2 + \dots + x_k}{n_1 + n_2 + \dots + n_k}.
\]

The \textbf{expected counts} under \(H_0\) are:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1512}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2209}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2209}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1512}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2209}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Success
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(n_1 \hat{p}\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(n_2 \hat{p}\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\ldots{}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(n_k \hat{p}\)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Failure & \(n_1(1-\hat{p})\) & \(n_2(1-\hat{p})\) & \ldots{} & \(n_k(1-\hat{p})\) \\
& \(n_1\) & \(n_2\) & & \(n_k\) \\
\end{longtable}

The test statistic is:

\[
\chi^2 = \sum_{\text{all cells}} \frac{(\text{observed} - \text{expected})^2}{\text{expected}}
\]

with \(k - 1\) degrees of freedom.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Two-Way Contingency Tables}

When categorical data are cross-classified, we create a two-way table of observed counts.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1613}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1183}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1183}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.0860}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1183}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.0860}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1183}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1290}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\ldots{}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
j
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\ldots{}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
c
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Row Total
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & \(n_{11}\) & \(n_{12}\) & \ldots{} & \(n_{1j}\) & \ldots{} & \(n_{1c}\) & \(n_{1.}\) \\
2 & \(n_{21}\) & \(n_{22}\) & \ldots{} & \(n_{2j}\) & \ldots{} & \(n_{2c}\) & \(n_{2.}\) \\
\ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} \\
r & \(n_{r1}\) & \(n_{r2}\) & \ldots{} & \(n_{rj}\) & \ldots{} & \(n_{rc}\) & \(n_{r.}\) \\
Column Total & \(n_{.1}\) & \(n_{.2}\) & \ldots{} & \(n_{.j}\) & \ldots{} & \(n_{.c}\) & \(n_{..}\) \\
\end{longtable}

\textbf{Sampling Designs}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Design 1: Total Sample Size Fixed}

  \begin{itemize}
  \item
    A single random sample of size \(n\) is drawn from the population.
  \item
    Units are cross-classified into \(r\) rows and \(c\) columns. Both row and column totals are random variables.
  \item
    The cell counts \(n_{ij}\) follow a \textbf{multinomial distribution} with probabilities \(p_{ij}\) such that: \[ \sum_{i=1}^r \sum_{j=1}^c p_{ij} = 1. \]
  \item
    Let \(p_{ij} = P(X = i, Y = j)\) be the joint probability, where \(X\) is the row variable and \(Y\) is the column variable.
  \item
    \textbf{Null Hypothesis of Independence}: \[ H_0: p_{ij} = p_{i.} p_{.j}, \quad \text{where } p_{i.} = P(X = i) \text{ and } p_{.j} = P(Y = j). \]
  \item
    \textbf{Alternative Hypothesis}: \[ H_a: p_{ij} \neq p_{i.} p_{.j}. \]
  \end{itemize}
\item
  \textbf{Design 2: Row Totals Fixed}

  \begin{itemize}
  \item
    Random samples of sizes \(n_1, n_2, \dots, n_r\) are drawn independently from \(r\) row populations.
  \item
    The row totals \(n_{i.}\) are fixed, but column totals are random.
  \item
    Counts in each row follow independent \textbf{multinomial distributions}.
  \item
    The null hypothesis assumes that the conditional probabilities of the column variable \(Y\) are the same across all rows: \[ H_0: p_{ij} = P(Y = j | X = i) = p_j \quad \text{for all } i \text{ and } j. \]
  \item
    Alternatively: \[ H_0: (p_{i1}, p_{i2}, \dots, p_{ic}) = (p_1, p_2, \dots, p_c) \quad \text{for all } i. \]
  \item
    \textbf{Alternative Hypothesis}: \[ H_a: (p_{i1}, p_{i2}, \dots, p_{ic}) \text{ are not the same for all } i. \]
  \end{itemize}
\end{enumerate}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.0853}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4573}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4539}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Design}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Total Sample Size Fixed}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Row Totals Fixed}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Scenario} & A single dataset or experiment where all observations are collected together as one sample. & Observations are collected separately for each row, with fixed totals for each row population. \\
\textbf{Example} & Survey with 100 respondents randomly selected, recording responses based on two categorical variables (e.g., age group and gender). & Stratified survey with specific numbers of individuals sampled from predefined groups (e.g., 30 males, 40 females, 30 non-binary). \\
\textbf{Why This Design?} & \begin{minipage}[t]{\linewidth}\raggedright
- Models situations where the total number of observations is fixed.\\
- Both row and column categories emerge randomly.\\
- Tests for \textbf{independence} between two categorical variables (row and column).\strut
\end{minipage} & \begin{minipage}[t]{\linewidth}\raggedright
- Models scenarios where sampling occurs independently within predefined strata or groups.\\
- Tests for \textbf{homogeneity} of column proportions across rows, ignoring differences in total counts between rows.\strut
\end{minipage} \\
\textbf{Practical Use Case} & \begin{minipage}[t]{\linewidth}\raggedright
- \textbf{Market Research}: Do customer demographics (rows) and purchase behavior (columns) show a dependence?\\
- \textbf{Biology}: Is there an association between species (rows) and habitat types (columns)?\strut
\end{minipage} & \begin{minipage}[t]{\linewidth}\raggedright
- \textbf{Public Health}: Are smoking rates (columns) consistent across age groups (rows)?\\
- \textbf{Education}: Do pass rates (columns) differ across schools (rows), controlling for the number of students in each school?\strut
\end{minipage} \\
\end{longtable}

\textbf{Why Both Designs?}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Real-World Sampling Constraints}:

  \begin{itemize}
  \item
    Sometimes, you have control over row totals (e.g., fixed group sizes in stratified sampling).
  \item
    Other times, you collect data without predefined group sizes, and totals emerge randomly.
  \end{itemize}
\item
  \textbf{Different Null Hypotheses}:

  \begin{itemize}
  \item
    Design 1 tests whether two variables are \textbf{independent} (e.g., does one variable predict the other?).
  \item
    Design 2 tests whether column proportions are \textbf{homogeneous} across groups (e.g., are the groups similar?).
  \end{itemize}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Sampling Design 1: Total Sample Size Fixed}
\CommentTok{\# Parameters for the multinomial distribution}
\NormalTok{r }\OtherTok{\textless{}{-}} \DecValTok{3}  \CommentTok{\# Number of rows}
\NormalTok{c }\OtherTok{\textless{}{-}} \DecValTok{4}  \CommentTok{\# Number of columns}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{100}  \CommentTok{\# Total sample size}
\NormalTok{p }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\FloatTok{0.1}\NormalTok{,}
              \FloatTok{0.05}\NormalTok{, }\FloatTok{0.15}\NormalTok{, }\FloatTok{0.05}\NormalTok{, }\FloatTok{0.1}\NormalTok{,}
              \FloatTok{0.05}\NormalTok{, }\FloatTok{0.05}\NormalTok{, }\FloatTok{0.025}\NormalTok{, }\FloatTok{0.075}\NormalTok{), }\AttributeTok{nrow =}\NormalTok{ r, }\AttributeTok{byrow =} \ConstantTok{TRUE}\NormalTok{)}

\CommentTok{\# Generate a single random sample}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)  }\CommentTok{\# For reproducibility}
\NormalTok{n\_ij }\OtherTok{\textless{}{-}} \FunctionTok{rmultinom}\NormalTok{(}\DecValTok{1}\NormalTok{, }\AttributeTok{size =}\NormalTok{ n, }\AttributeTok{prob =} \FunctionTok{as.vector}\NormalTok{(p))}

\CommentTok{\# Reshape into a contingency table}
\NormalTok{contingency\_table\_fixed\_total }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(n\_ij, }\AttributeTok{nrow =}\NormalTok{ r, }\AttributeTok{ncol =}\NormalTok{ c, }\AttributeTok{byrow =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{rownames}\NormalTok{(contingency\_table\_fixed\_total) }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\StringTok{"Row"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\NormalTok{r)}
\FunctionTok{colnames}\NormalTok{(contingency\_table\_fixed\_total) }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\StringTok{"Col"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\NormalTok{c)}

\CommentTok{\# Hypothesis testing (Chi{-}squared test of independence)}
\NormalTok{chisq\_test\_fixed\_total }\OtherTok{\textless{}{-}} \FunctionTok{chisq.test}\NormalTok{(contingency\_table\_fixed\_total)}

\CommentTok{\# Display results}
\FunctionTok{print}\NormalTok{(}\StringTok{"Contingency Table (Total Sample Size Fixed):"}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] "Contingency Table (Total Sample Size Fixed):"}
\FunctionTok{print}\NormalTok{(contingency\_table\_fixed\_total)}
\CommentTok{\#\textgreater{}      Col1 Col2 Col3 Col4}
\CommentTok{\#\textgreater{} Row1    8    6    4   24}
\CommentTok{\#\textgreater{} Row2   18    1    9    7}
\CommentTok{\#\textgreater{} Row3    2    7    5    9}
\FunctionTok{print}\NormalTok{(}\StringTok{"Chi{-}squared Test Results:"}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] "Chi{-}squared Test Results:"}
\FunctionTok{print}\NormalTok{(chisq\_test\_fixed\_total)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Pearson\textquotesingle{}s Chi{-}squared test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  contingency\_table\_fixed\_total}
\CommentTok{\#\textgreater{} X{-}squared = 28.271, df = 6, p{-}value = 8.355e{-}05}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  All counts in the contingency table come from a single multinomial sample where both row and column totals are random.
\item
  \textbf{Conclusion}: Reject Null\hspace{0pt}. The data suggests significant dependence between row and column variables.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Sampling Design 2: Row Totals Fixed}
\CommentTok{\# Parameters for the fixed row totals}
\NormalTok{n\_row }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{30}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{30}\NormalTok{)  }\CommentTok{\# Row totals}
\NormalTok{c }\OtherTok{\textless{}{-}} \DecValTok{4}  \CommentTok{\# Number of columns}
\NormalTok{p\_col }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{0.25}\NormalTok{, }\FloatTok{0.25}\NormalTok{, }\FloatTok{0.25}\NormalTok{, }\FloatTok{0.25}\NormalTok{)  }\CommentTok{\# Common column probabilities under H0}

\CommentTok{\# Generate independent multinomial samples for each row}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)  }\CommentTok{\# For reproducibility}
\NormalTok{row\_samples }\OtherTok{\textless{}{-}} \FunctionTok{lapply}\NormalTok{(n\_row, }\ControlFlowTok{function}\NormalTok{(size) }\FunctionTok{t}\NormalTok{(}\FunctionTok{rmultinom}\NormalTok{(}\DecValTok{1}\NormalTok{, size, }\AttributeTok{prob =}\NormalTok{ p\_col)))}

\CommentTok{\# Combine into a contingency table}
\NormalTok{contingency\_table\_fixed\_rows }\OtherTok{\textless{}{-}} \FunctionTok{do.call}\NormalTok{(rbind, row\_samples)}
\FunctionTok{rownames}\NormalTok{(contingency\_table\_fixed\_rows) }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\StringTok{"Row"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(n\_row))}
\FunctionTok{colnames}\NormalTok{(contingency\_table\_fixed\_rows) }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\StringTok{"Col"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\NormalTok{c)}

\CommentTok{\# Hypothesis testing (Chi{-}squared test of homogeneity)}
\NormalTok{chisq\_test\_fixed\_rows }\OtherTok{\textless{}{-}} \FunctionTok{chisq.test}\NormalTok{(contingency\_table\_fixed\_rows)}

\CommentTok{\# Display results}
\FunctionTok{print}\NormalTok{(}\StringTok{"Contingency Table (Row Totals Fixed):"}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] "Contingency Table (Row Totals Fixed):"}
\FunctionTok{print}\NormalTok{(contingency\_table\_fixed\_rows)}
\CommentTok{\#\textgreater{}      Col1 Col2 Col3 Col4}
\CommentTok{\#\textgreater{} Row1    6   10    7    7}
\CommentTok{\#\textgreater{} Row2   13   13    4   10}
\CommentTok{\#\textgreater{} Row3    8   10    6    6}
\FunctionTok{print}\NormalTok{(}\StringTok{"Chi{-}squared Test Results:"}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] "Chi{-}squared Test Results:"}
\FunctionTok{print}\NormalTok{(chisq\_test\_fixed\_rows)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Pearson\textquotesingle{}s Chi{-}squared test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  contingency\_table\_fixed\_rows}
\CommentTok{\#\textgreater{} X{-}squared = 3.2069, df = 6, p{-}value = 0.7825}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  Row totals are fixed, and column counts within each row follow independent multinomial distributions.
\item
  \textbf{Conclusion}: Fail to reject the null. The data does not provide evidence to suggest differences in column probabilities across rows.
\end{itemize}

\textbf{Why Are the Results Different?}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Data Generation Differences}:

  \begin{itemize}
  \item
    In \textbf{Design 1}, the entire table is treated as a single multinomial sample. This introduces dependencies between counts in the table.
  \item
    In \textbf{Design 2}, rows are generated independently, and only the column probabilities are tested for consistency across rows.
  \end{itemize}
\item
  \textbf{Null Hypotheses}:

  \begin{itemize}
  \item
    \textbf{Design 1} tests independence between row and column variables (more restrictive).
  \item
    \textbf{Design 2} tests homogeneity of column probabilities across rows (less restrictive).
  \end{itemize}
\end{enumerate}

\textbf{Interpretation}

\begin{itemize}
\item
  The results are \textbf{not directly comparable} because the null hypotheses are different:

  \begin{itemize}
  \item
    \textbf{Design 1} focuses on whether rows and columns are independent across the entire table.
  \item
    \textbf{Design 2} focuses on whether column distributions are consistent across rows.
  \end{itemize}
\item
  \textbf{Real-World Implication}:

  \begin{itemize}
  \item
    If you are testing for independence (e.g., whether two variables are unrelated), use Design 1.
  \item
    If you are testing for consistency across groups (e.g., whether proportions are the same across categories), use Design 2.
  \end{itemize}
\end{itemize}

\textbf{Takeaways}

\begin{itemize}
\item
  The tests use the same statistical machinery (Chi-squared test), but their interpretations differ based on the experimental design and null hypothesis.
\item
  For the same dataset, differences in assumptions can lead to different conclusions.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{chi-square-test-for-independence}{%
\paragraph{Chi-Square Test for Independence}\label{chi-square-test-for-independence}}

The expected frequencies \(\hat{e}_{ij}\) under the null hypothesis are:

\[
\hat{e}_{ij} = \frac{n_{i.} n_{.j}}{n_{..}},
\]

where \(n_{i.}\) and \(n_{.j}\) are the row and column totals, respectively, and \(n_{..}\) is the total sample size.

The test statistic is:

\[
\chi^2 = \sum_{i=1}^r \sum_{j=1}^c \frac{(n_{ij} - \hat{e}_{ij})^2}{\hat{e}_{ij}} \sim \chi^2_{(r-1)(c-1)}.
\]

We reject \(H_0\) at significance level \(\alpha\) if:

\[
\chi^2 > \chi^2_{(r-1)(c-1), \alpha}.
\]

Notes on the Pearson Chi-Square Test

\begin{itemize}
\tightlist
\item
  \textbf{Purpose}: Test for association or independence between two categorical variables.
\item
  \textbf{Sensitivity to Sample Size}: The \(\chi^2\) statistic is proportional to sample size. Doubling the sample size doubles \(\chi^2\) even if the strength of the association remains unchanged.
\item
  \textbf{Assumption on Expected Frequencies}: The test is not valid when more than 20\% of the expected cell counts are less than 5. In such cases, exact tests are preferred.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a contingency table}
\NormalTok{data\_table }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{30}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{40}\NormalTok{), }\AttributeTok{nrow =} \DecValTok{2}\NormalTok{, }\AttributeTok{byrow =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{colnames}\NormalTok{(data\_table) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Category 1"}\NormalTok{, }\StringTok{"Category 2"}\NormalTok{)}
\FunctionTok{rownames}\NormalTok{(data\_table) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Group 1"}\NormalTok{, }\StringTok{"Group 2"}\NormalTok{)}

\CommentTok{\# Display the table}
\FunctionTok{print}\NormalTok{(data\_table)}
\CommentTok{\#\textgreater{}         Category 1 Category 2}
\CommentTok{\#\textgreater{} Group 1         30         10}
\CommentTok{\#\textgreater{} Group 2         20         40}

\CommentTok{\# Perform Chi{-}Square Test}
\NormalTok{chi\_result }\OtherTok{\textless{}{-}} \FunctionTok{chisq.test}\NormalTok{(data\_table)}

\CommentTok{\# Display results}
\NormalTok{chi\_result}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Pearson\textquotesingle{}s Chi{-}squared test with Yates\textquotesingle{} continuity correction}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  data\_table}
\CommentTok{\#\textgreater{} X{-}squared = 15.042, df = 1, p{-}value = 0.0001052}
\end{Highlighting}
\end{Shaded}

The output includes:

\begin{itemize}
\item
  Chi-Square Statistic (\(\chi^2\)): The test statistic measuring the deviation between observed and expected counts.
\item
  \textbf{p-value}: The probability of observing such a deviation under \(H_0\).
\item
  \textbf{Degrees of Freedom}: \((r-1)(c-1)\) for an \(r \times c\) table.
\item
  \textbf{Expected Frequencies}: The table of expected counts under \(H_0\).
\item
  If the p-value is less than \(\alpha\), reject \(H_0\) and conclude that there is a significant association between the row and column variables.
\end{itemize}

\hypertarget{key-takeaways}{%
\subsubsection{Key Takeaways}\label{key-takeaways}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1398}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2832}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1935}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1254}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2473}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Test}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Purpose}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Features}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Sample Size Suitability}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Statistical Assumptions}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Fisher's Exact Test} & Tests association between two categorical variables in a \textbf{2x2 table}. & \begin{minipage}[t]{\linewidth}\raggedright
- Computes exact p-values.\\
- Does not rely on asymptotic assumptions.\\
- Handles small sample sizes.\strut
\end{minipage} & \textbf{Small sample sizes} & \begin{minipage}[t]{\linewidth}\raggedright
- Observations are independent.\\
- Fixed marginal totals.\\
- No normality assumption.\strut
\end{minipage} \\
\textbf{Exact Chi-Square Test} & Tests association in larger contingency tables using exact methods. & \begin{minipage}[t]{\linewidth}\raggedright
- Generalization of Fisher's Exact Test.\\
- Avoids asymptotic assumptions.\\
- Suitable for small to medium datasets.\strut
\end{minipage} & \textbf{Small to medium sample sizes} & \begin{minipage}[t]{\linewidth}\raggedright
- Observations are independent.\\
- Marginal totals may not be fixed.\\
- No normality assumption.\strut
\end{minipage} \\
\textbf{Pearson Chi-Square Test} & Tests discrepancies between observed and expected frequencies. & \begin{minipage}[t]{\linewidth}\raggedright
- Most common chi-square-based test.\\
- Includes independence and goodness-of-fit tests.\\
- Relies on asymptotic assumptions.\strut
\end{minipage} & \textbf{Large sample sizes} & \begin{minipage}[t]{\linewidth}\raggedright
- Observations are independent.\\
- Expected cell frequencies  5.\\
- Test statistic follows a chi-square distribution asymptotically.\strut
\end{minipage} \\
\textbf{Chi-Square Test for Independence} & Tests independence between two categorical variables in a contingency table. & \begin{minipage}[t]{\linewidth}\raggedright
- Application of Pearson Chi-Square Test.\\
- Same assumptions as asymptotic chi-square tests.\\
- Often used for larger contingency tables.\strut
\end{minipage} & \textbf{Medium to large sample sizes} & \begin{minipage}[t]{\linewidth}\raggedright
- Observations are independent.\\
- Expected cell frequencies  5.\\
- Random sampling.\strut
\end{minipage} \\
\end{longtable}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Fisher's Exact Test} is specialized for small samples and fixed margins (2x2 tables).
\item
  \textbf{Exact Chi-Square Test} is a broader version of Fisher's for larger tables but avoids asymptotic approximations.
\item
  \textbf{Pearson Chi-Square Test} is the general framework, and its applications include:

  \begin{itemize}
  \item
    Goodness-of-fit testing.
  \item
    Testing independence (same as the Chi-Square Test for Independence).
  \end{itemize}
\item
  \textbf{Chi-Square Test for Independence} is a specific application of the Pearson Chi-Square Test.
\end{enumerate}

In essence:

\begin{itemize}
\item
  \textbf{Fisher's Exact Test} and \textbf{Exact Chi-Square Test} are precise methods for small datasets.
\item
  \textbf{Pearson Chi-Square Test} and \textbf{Chi-Square Test for Independence} are interchangeable terms in many contexts, focusing on larger datasets.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{ordinal-association}{%
\subsection{Ordinal Association}\label{ordinal-association}}

Ordinal association refers to a relationship between two variables where the levels of one variable exhibit a consistent pattern of increase or decrease in response to the levels of the other variable. This type of association is particularly relevant when dealing with ordinal variables, which have naturally ordered categories, such as ratings (``poor'', ``fair'', ``good'', ``excellent'') or income brackets (``low'', ``medium'', ``high'').

For example:

\begin{itemize}
\item
  As customer satisfaction ratings increase from ``poor'' to ``excellent,'' the likelihood of recommending a product may also increase (positive ordinal association).
\item
  Alternatively, as stress levels move from ``low'' to ``high,'' job performance may tend to decrease (negative ordinal association).
\end{itemize}

\textbf{Key Characteristics of Ordinal Association}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Logical Ordering of Levels}: The levels of both variables must follow a logical sequence. For instance, ``small,'' ``medium,'' and ``large'' are logically ordered, whereas categories like ``blue,'' ``round,'' and ``tall'' lack inherent order and are unsuitable for ordinal association.
\item
  \textbf{Monotonic Trends}: The association is typically monotonic, meaning that as one variable moves in a specific direction, the other variable tends to move in a consistent direction (either increasing or decreasing).
\item
  \textbf{Tests for Ordinal Association}: Specialized statistical tests assess ordinal association, focusing on how the rankings of one variable relate to those of the other. These tests require the data to respect the ordinal structure of both variables.
\end{enumerate}

\textbf{Practical Considerations}

When using these tests, keep in mind:

\begin{itemize}
\item
  \textbf{Ordinal Data Handling}: Ensure that the data respects the ordinal structure (e.g., categories are correctly ranked and coded).
\item
  \textbf{Sample Size}: Larger sample sizes provide more reliable estimates and stronger test power.
\item
  \textbf{Contextual Relevance}: Interpret results within the context of the data and the research question. For example, a significant Spearman's correlation does not imply causation but rather a consistent trend.
\end{itemize}

\hypertarget{mantel-haenszel-chi-square-test}{%
\subsubsection{Mantel-Haenszel Chi-square Test}\label{mantel-haenszel-chi-square-test}}

The Mantel-Haenszel Chi-square Test is a statistical tool for evaluating ordinal associations, particularly when the data consists of multiple \(2 \times 2\) contingency tables that examine the same association under varying conditions or strata. Unlike measures of association such as correlation coefficients, this test does not quantify the strength of the association but rather evaluates whether an association exists after controlling for stratification.

The Mantel-Haenszel Test is applicable to \(2 \times 2 \times K\) contingency tables, where \(K\) represents the number of strata. Each stratum is a \(2 \times 2\) table corresponding to different conditions or subgroups.

For each stratum \(k\), let the marginal totals of the table be:

\begin{itemize}
\item
  \(n_{.1k}\): Total observations in column 1
\item
  \(n_{.2k}\): Total observations in column 2
\item
  \(n_{1.k}\): Total observations in row 1
\item
  \(n_{2.k}\): Total observations in row 2
\item
  \(n_{..k}\): Total observations in the entire table
\end{itemize}

The observed cell count in row 1 and column 1 is denoted \(n_{11k}\). Given the marginal totals, the sampling distribution of \(n_{11k}\) follows a hypergeometric distribution.

Under the assumption of conditional independence:

The expected value of \(n_{11k}\) is: \[ 
  m_{11k} = E(n_{11k}) = \frac{n_{1.k} n_{.1k}}{n_{..k}} 
  \] The variance of \(n_{11k}\) is: \[
  var(n_{11k}) = \frac{n_{1.k} n_{2.k} n_{.1k} n_{.2k}}{n_{..k}^2 (n_{..k} - 1)}
  \]

Mantel and Haenszel proposed the test statistic:

\[
M^2 = \frac{\left(|\sum_k n_{11k} - \sum_k m_{11k}| - 0.5\right)^2}{\sum_k var(n_{11k})} \sim \chi^2_{1}
\]

where

\begin{itemize}
\item
  The 0.5 adjustment, known as a continuity correction, improves the approximation to the \(\chi^2\) distribution.
\item
  The test statistic follows a \(\chi^2\) distribution with 1 degree of freedom under the null hypothesis of conditional independence.
\end{itemize}

This method can be extended to general \(I \times J \times K\) contingency tables, where \(I\) and \(J\) represent the number of rows and columns, respectively, and \(K\) is the number of strata.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Null Hypothesis} (\(H_0\)):

There is no association between the two variables of interest across all strata, after controlling for the confounder.\\
In mathematical terms:

\[
H_0: \text{Odds Ratio (OR)} = 1 \; \text{or} \; \text{Risk Ratio (RR)} = 1
\]

\textbf{Alternative Hypothesis} (\(H_a\)):

There is an association between the two variables of interest across all strata, after controlling for the confounder.\\
In mathematical terms:

\[
H_a: \text{Odds Ratio (OR)} \neq 1 \; \text{or} \; \text{Risk Ratio (RR)} \neq 1
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Let's consider a scenario where a business wants to evaluate the relationship between customer satisfaction (Satisfied vs.~Not Satisfied) and the likelihood of repeat purchases (Yes vs.~No) across different regions (e.g., North, South, and West). The goal is to determine whether this relationship holds consistently across the regions.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a 2 x 2 x 3 contingency table}
\NormalTok{CustomerData }\OtherTok{=} \FunctionTok{array}\NormalTok{(}
    \FunctionTok{c}\NormalTok{(}\DecValTok{40}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{200}\NormalTok{, }\DecValTok{300}\NormalTok{, }\DecValTok{35}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{180}\NormalTok{, }\DecValTok{265}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{250}\NormalTok{, }\DecValTok{275}\NormalTok{),}
    \AttributeTok{dim =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{),}
    \AttributeTok{dimnames =} \FunctionTok{list}\NormalTok{(}
        \AttributeTok{Satisfaction =} \FunctionTok{c}\NormalTok{(}\StringTok{"Satisfied"}\NormalTok{, }\StringTok{"Not Satisfied"}\NormalTok{),}
        \AttributeTok{RepeatPurchase =} \FunctionTok{c}\NormalTok{(}\StringTok{"Yes"}\NormalTok{, }\StringTok{"No"}\NormalTok{),}
        \AttributeTok{Region =} \FunctionTok{c}\NormalTok{(}\StringTok{"North"}\NormalTok{, }\StringTok{"South"}\NormalTok{, }\StringTok{"West"}\NormalTok{)}
\NormalTok{    )}
\NormalTok{)}

\CommentTok{\# View marginal table (summarized across regions)}
\FunctionTok{margin.table}\NormalTok{(CustomerData, }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\CommentTok{\#\textgreater{}                RepeatPurchase}
\CommentTok{\#\textgreater{} Satisfaction    Yes  No}
\CommentTok{\#\textgreater{}   Satisfied     125 630}
\CommentTok{\#\textgreater{}   Not Satisfied  75 840}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Calculate the overall odds ratio (ignoring strata):
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(samplesizeCMH)}
\NormalTok{marginal\_table }\OtherTok{=} \FunctionTok{margin.table}\NormalTok{(CustomerData, }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\FunctionTok{odds.ratio}\NormalTok{(marginal\_table)}
\CommentTok{\#\textgreater{} [1] 2.222222}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Calculate the conditional odds ratios for each region:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{apply}\NormalTok{(CustomerData, }\DecValTok{3}\NormalTok{, odds.ratio)}
\CommentTok{\#\textgreater{}    North    South     West }
\CommentTok{\#\textgreater{} 2.000000 2.576389 2.200000}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  The Mantel-Haenszel Test evaluates whether the relationship between customer satisfaction and repeat purchases remains consistent across regions:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mantelhaen.test}\NormalTok{(CustomerData, }\AttributeTok{correct =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Mantel{-}Haenszel chi{-}squared test with continuity correction}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  CustomerData}
\CommentTok{\#\textgreater{} Mantel{-}Haenszel X{-}squared = 26.412, df = 1, p{-}value = 2.758e{-}07}
\CommentTok{\#\textgreater{} alternative hypothesis: true common odds ratio is not equal to 1}
\CommentTok{\#\textgreater{} 95 percent confidence interval:}
\CommentTok{\#\textgreater{}  1.637116 3.014452}
\CommentTok{\#\textgreater{} sample estimates:}
\CommentTok{\#\textgreater{} common odds ratio }
\CommentTok{\#\textgreater{}          2.221488}
\end{Highlighting}
\end{Shaded}

Interpretation

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Overall Odds Ratio}: This provides an estimate of the overall association between satisfaction and repeat purchases, ignoring regional differences.
\item
  \textbf{Conditional Odds Ratios}: These show whether the odds of repeat purchases given satisfaction are similar across regions.
\item
  \textbf{Mantel-Haenszel Test}: A significant test result (e.g., \(p < 0.05\)) suggests that the relationship between satisfaction and repeat purchases is consistent across regions. Conversely, a non-significant result implies that regional differences may affect the association. By applying the Mantel-Haenszel Test, businesses can determine if a marketing or customer retention strategy should be uniformly applied or customized to account for regional variations.

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    There is strong evidence to suggest that the two variables of interest are associated across the strata (North, South, and West), even after accounting for potential confounding effects of stratification.
  \item
    The common odds ratio of approximately \(2.22\) indicates a substantial association, meaning that the outcome is more likely in the exposed group compared to the unexposed group.
  \item
    The variability in the stratum-specific odds ratios suggests that the strength of the association may differ slightly by region, but the Mantel-Haenszel test assumes the association is consistent (homogeneous).
  \end{enumerate}
\end{enumerate}

\hypertarget{mcnemars-test}{%
\subsubsection{McNemar's Test}\label{mcnemars-test}}

McNemar's Test is a special case of the \protect\hyperlink{mantel-haenszel-chi-square-test}{Mantel-Haenszel Chi-square Test}, designed for paired nominal data. It is particularly useful for evaluating changes in categorical responses before and after a treatment or intervention, or for comparing paired responses in matched samples. Unlike the Mantel-Haenszel Test, which handles stratified data, McNemar's Test is tailored to situations with a single \(2 \times 2\) table derived from paired observations.

McNemar's Test assesses whether the proportions of discordant pairs (off-diagonal elements in a \(2 \times 2\) table) are significantly different. Specifically, it tests the null hypothesis that the probabilities of transitioning from one category to another are equal.

\begin{itemize}
\item
  \textbf{Null Hypothesis (}\(H_0\)): \[
  P(\text{Switch from A to B}) = P(\text{Switch from B to A})
  \] This implies that the probabilities of transitioning from one category to the other are equal, or equivalently, the off-diagonal cell counts (\(n_{12}\) and \(n_{21}\)) are symmetric: \[
  H_0: n_{12} = n_{21}
  \]
\item
  \textbf{Alternative Hypothesis (}\(H_A\)): \[
  P(\text{Switch from A to B}) \neq P(\text{Switch from B to A})
  \] This suggests that the probabilities of transitioning between categories are not equal, or equivalently, the off-diagonal cell counts (\(n_{12}\) and \(n_{21}\)) are asymmetric: \[
  H_A: n_{12} \neq n_{21}
  \]
\end{itemize}

For example, consider a business analyzing whether a new advertising campaign influences customer preference for two products (A and B). Each customer is surveyed before and after the campaign, resulting in the following \(2 \times 2\) contingency table:

\begin{itemize}
\tightlist
\item
  \textbf{Before rows}: Preference for Product A or B before the campaign.
\item
  \textbf{After columns}: Preference for Product A or B after the campaign.
\end{itemize}

Let the table structure be:

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
& After A & After B \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Before A} & \(n_{11}\) & \(n_{12}\) \\
\textbf{Before B} & \(n_{21}\) & \(n_{22}\) \\
\end{longtable}

\begin{itemize}
\tightlist
\item
  \(n_{12}\): Customers who switched from Product A to B.
\item
  \(n_{21}\): Customers who switched from Product B to A.
\end{itemize}

The test focuses on \(n_{12}\) and \(n_{21}\), as they represent the discordant pairs.

The McNemar's Test statistic is: \[
M^2 = \frac{(|n_{12} - n_{21}| - 0.5)^2}{n_{12} + n_{21}}
\] where

\begin{itemize}
\item
  The 0.5 is a continuity correction applied when sample sizes are small.
\item
  Under the null hypothesis of no preference change, \(M^2\) follows a \(\chi^2\) distribution with 1 degree of freedom.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Let's analyze a voting behavior study where participants were surveyed before and after a campaign. The table represents:

\begin{itemize}
\item
  Rows: Voting preference before the campaign (Yes, No).
\item
  Columns: Voting preference after the campaign (Yes, No).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Voting preference before and after a campaign}
\NormalTok{vote }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{682}\NormalTok{, }\DecValTok{22}\NormalTok{, }\DecValTok{86}\NormalTok{, }\DecValTok{810}\NormalTok{), }\AttributeTok{nrow =} \DecValTok{2}\NormalTok{, }\AttributeTok{byrow =} \ConstantTok{TRUE}\NormalTok{,}
              \AttributeTok{dimnames =} \FunctionTok{list}\NormalTok{(}
                \StringTok{"Before"} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"Yes"}\NormalTok{, }\StringTok{"No"}\NormalTok{),}
                \StringTok{"After"} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"Yes"}\NormalTok{, }\StringTok{"No"}\NormalTok{)}
\NormalTok{              ))}

\CommentTok{\# Perform McNemar\textquotesingle{}s Test with continuity correction}
\NormalTok{mcnemar\_result }\OtherTok{\textless{}{-}} \FunctionTok{mcnemar.test}\NormalTok{(vote, }\AttributeTok{correct =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{mcnemar\_result}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  McNemar\textquotesingle{}s Chi{-}squared test with continuity correction}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  vote}
\CommentTok{\#\textgreater{} McNemar\textquotesingle{}s chi{-}squared = 36.75, df = 1, p{-}value = 1.343e{-}09}
\end{Highlighting}
\end{Shaded}

The test provides:

\begin{itemize}
\item
  \textbf{Test statistic (}\(M^2\)\textbf{)}: Quantifies the asymmetry in discordant pairs.
\item
  \textbf{p-value}: Indicates whether there is a significant difference in the discordant proportions.
\end{itemize}

Interpretation

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Test Statistic}: A large \(M^2\) value suggests significant asymmetry in the discordant pairs.
\item
  \textbf{p-value}:

  \begin{itemize}
  \item
    A low p-value (e.g., \(p < 0.05\)) rejects the null hypothesis, indicating that the proportion of participants switching preferences (e.g., from Yes to No) is significantly different from those switching in the opposite direction (e.g., from No to Yes).
  \item
    A high p-value fails to reject the null hypothesis, suggesting no significant preference change.
  \end{itemize}
\end{enumerate}

McNemar's Test is widely used in business and other fields:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Marketing Campaigns}: Evaluating whether a campaign shifts consumer preferences or purchase intentions.
\item
  \textbf{Product Testing}: Determining if a new feature or redesign changes customer ratings.
\item
  \textbf{Healthcare Studies}: Analyzing treatment effects in paired medical trials.
\end{enumerate}

\hypertarget{mcnemar-bowker-test}{%
\subsubsection{McNemar-Bowker Test}\label{mcnemar-bowker-test}}

The McNemar-Bowker Test is an extension of \protect\hyperlink{mcnemars-test}{McNemar's Test}, designed for analyzing paired nominal data with more than two categories. It evaluates the symmetry of the full contingency table by comparing the off-diagonal elements across all categories. This test is particularly useful for understanding whether changes between categories are uniformly distributed or whether significant asymmetries exist.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Let the data be structured in an \(r \times r\) square contingency table, where \(r\) is the number of categories, and the off-diagonal elements represent transitions between categories.

The hypotheses for the McNemar-Bowker Test are:

\begin{itemize}
\item
  \textbf{Null Hypothesis (}\(H_0\)): \[
  P(\text{Switch from Category } i \text{ to Category } j) = P(\text{Switch from Category } j \text{ to Category } i) \quad \forall \, i \neq j
  \] This implies that the off-diagonal elements are symmetric, and there is no directional preference in category transitions.
\item
  \textbf{Alternative Hypothesis (}\(H_A\)): \[
  P(\text{Switch from Category } i \text{ to Category } j) \neq P(\text{Switch from Category } j \text{ to Category } i) \quad \text{for at least one pair } (i, j)
  \] This suggests that the off-diagonal elements are not symmetric, indicating a directional preference in transitions between at least one pair of categories.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The McNemar-Bowker Test statistic is: \[
B^2 = \sum_{i < j} \frac{(n_{ij} - n_{ji})^2}{n_{ij} + n_{ji}}
\]

where

\begin{itemize}
\item
  \(n_{ij}\): Observed count of transitions from category \(i\) to category \(j\).
\item
  \(n_{ji}\): Observed count of transitions from category \(j\) to category \(i\).
\end{itemize}

Under the null hypothesis, the test statistic \(B^2\) approximately follows a \(\chi^2\) distribution with \(\frac{r(r-1)}{2}\) degrees of freedom (corresponding to the number of unique pairs of categories).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

For example, a company surveys customers about their satisfaction before and after implementing a new policy. Satisfaction is rated on a scale of 1 to 3 (1 = Low, 2 = Medium, 3 = High). The paired responses are summarized in the following \(3 \times 3\) contingency table.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Satisfaction ratings before and after the intervention}
\NormalTok{satisfaction\_table }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}
    \DecValTok{30}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{5}\NormalTok{,  }\CommentTok{\# Before: Low}
    \DecValTok{8}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{12}\NormalTok{,  }\CommentTok{\# Before: Medium}
    \DecValTok{6}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{40}   \CommentTok{\# Before: High}
\NormalTok{), }\AttributeTok{nrow =} \DecValTok{3}\NormalTok{, }\AttributeTok{byrow =} \ConstantTok{TRUE}\NormalTok{,}
\AttributeTok{dimnames =} \FunctionTok{list}\NormalTok{(}
    \StringTok{"Before"} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"Low"}\NormalTok{, }\StringTok{"Medium"}\NormalTok{, }\StringTok{"High"}\NormalTok{),}
    \StringTok{"After"} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"Low"}\NormalTok{, }\StringTok{"Medium"}\NormalTok{, }\StringTok{"High"}\NormalTok{)}
\NormalTok{))}

\CommentTok{\# Function to perform McNemar{-}Bowker Test}
\NormalTok{mcnemar\_bowker\_test }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(table) \{}
  \ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\FunctionTok{all}\NormalTok{(}\FunctionTok{dim}\NormalTok{(table)[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{==} \FunctionTok{dim}\NormalTok{(table)[}\DecValTok{2}\NormalTok{])) \{}
    \FunctionTok{stop}\NormalTok{(}\StringTok{"Input must be a square matrix."}\NormalTok{)}
\NormalTok{  \}}
  
  \CommentTok{\# Extract off{-}diagonal elements}
\NormalTok{  n }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(table)}
\NormalTok{  stat }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{  df }\OtherTok{\textless{}{-}} \DecValTok{0}
  
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{(n }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)) \{}
    \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in}\NormalTok{ (i }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{n) \{}
\NormalTok{      nij }\OtherTok{\textless{}{-}}\NormalTok{ table[i, j]}
\NormalTok{      nji }\OtherTok{\textless{}{-}}\NormalTok{ table[j, i]}
\NormalTok{      stat }\OtherTok{\textless{}{-}}\NormalTok{ stat }\SpecialCharTok{+}\NormalTok{ (nij }\SpecialCharTok{{-}}\NormalTok{ nji)}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{/}\NormalTok{ (nij }\SpecialCharTok{+}\NormalTok{ nji)}
\NormalTok{      df }\OtherTok{\textless{}{-}}\NormalTok{ df }\SpecialCharTok{+} \DecValTok{1}
\NormalTok{    \}}
\NormalTok{  \}}
  
\NormalTok{  p\_value }\OtherTok{\textless{}{-}} \FunctionTok{pchisq}\NormalTok{(stat, }\AttributeTok{df =}\NormalTok{ df, }\AttributeTok{lower.tail =} \ConstantTok{FALSE}\NormalTok{)}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{statistic =}\NormalTok{ stat, }\AttributeTok{df =}\NormalTok{ df, }\AttributeTok{p\_value =}\NormalTok{ p\_value))}
\NormalTok{\}}

\CommentTok{\# Run the test}
\NormalTok{result }\OtherTok{\textless{}{-}} \FunctionTok{mcnemar\_bowker\_test}\NormalTok{(satisfaction\_table)}

\CommentTok{\# Print results}
\FunctionTok{cat}\NormalTok{(}\StringTok{"McNemar{-}Bowker Test Results:}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} McNemar{-}Bowker Test Results:}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Test Statistic (B\^{}2):"}\NormalTok{, result}\SpecialCharTok{$}\NormalTok{statistic, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Test Statistic (B\^{}2): 0.4949495}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Degrees of Freedom:"}\NormalTok{, result}\SpecialCharTok{$}\NormalTok{df, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Degrees of Freedom: 3}
\FunctionTok{cat}\NormalTok{(}\StringTok{"p{-}value:"}\NormalTok{, result}\SpecialCharTok{$}\NormalTok{p\_value, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} p{-}value: 0.9199996}
\end{Highlighting}
\end{Shaded}

The output includes:

\begin{itemize}
\item
  \textbf{Test Statistic (}\(B^2\)\textbf{)}: A measure of the asymmetry in the off-diagonal elements.
\item
  \textbf{p-value}: The probability of observing the data under the null hypothesis of symmetry.
\end{itemize}

Interpretation

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Test Statistic}: A large \$B\^{}2\$ value suggests substantial asymmetry in transitions between categories.
\item
  \textbf{p-value}:

  \begin{itemize}
  \item
    If the \textbf{p-value is less than the significance level} (e.g., \$p \textless{} 0.05\$), we reject the null hypothesis, indicating significant asymmetry in the transitions between at least one pair of categories.
  \item
    If the \textbf{p-value is greater than the significance level}, we fail to reject the null hypothesis, suggesting that the category transitions are symmetric.
  \end{itemize}
\end{enumerate}

The McNemar-Bowker Test has broad applications in business and other fields:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Customer Feedback Analysis}: Evaluating changes in customer satisfaction levels before and after interventions.
\item
  \textbf{Marketing Campaigns}: Assessing shifts in brand preferences across multiple brands in response to an advertisement.
\item
  \textbf{Product Testing}: Understanding how user preferences among different product features change after a redesign.
\end{enumerate}

\hypertarget{stuart-maxwell-test}{%
\subsubsection{Stuart-Maxwell Test}\label{stuart-maxwell-test}}

The Stuart-Maxwell Test is used for analyzing changes in paired categorical data with more than two categories. It is a generalization of \protect\hyperlink{mcnemars-test}{McNemar's Test}, applied to square contingency tables where the off-diagonal elements represent transitions between categories. Unlike the \protect\hyperlink{mcnemar-bowker-test}{McNemar-Bowker Test}, which tests for symmetry across all pairs, the \protect\hyperlink{stuart-maxwell-test}{Stuart-Maxwell Test} focuses on overall marginal homogeneity.

The test evaluates whether the marginal distributions of paired data are consistent across categories. This is particularly useful when investigating whether the distribution of responses has shifted between two conditions, such as before and after an intervention.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Hypotheses for the Stuart-Maxwell Test

\begin{itemize}
\item
  \textbf{Null Hypothesis (}\(H_0\)): \[
  \text{The marginal distributions of the paired data are homogeneous (no difference).}
  \]
\item
  \textbf{Alternative Hypothesis (}\(H_A\)): \[
  \text{The marginal distributions of the paired data are not homogeneous (there is a difference).}
  \]
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The Stuart-Maxwell Test statistic is calculated as: \[
M^2 = \mathbf{b}' \mathbf{V}^{-1} \mathbf{b}
\] where:

\begin{itemize}
\item
  \(\mathbf{b}\): Vector of differences between the marginal totals of paired categories.
\item
  \(\mathbf{V}\): Covariance matrix of \(\mathbf{b}\) under the null hypothesis.
\end{itemize}

The test statistic \(M^2\) follows a \(\chi^2\) distribution with \((r - 1)\) degrees of freedom, where \(r\) is the number of categories.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

A company surveys employees about their satisfaction levels (Low, Medium, High) before and after implementing a new workplace policy. The results are summarized in the following \(3 \times 3\) contingency table.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Employee satisfaction data before and after a policy change}
\NormalTok{satisfaction\_table }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}
    \DecValTok{40}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{5}\NormalTok{,  }\CommentTok{\# Before: Low}
    \DecValTok{8}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{12}\NormalTok{,  }\CommentTok{\# Before: Medium}
    \DecValTok{6}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{40}   \CommentTok{\# Before: High}
\NormalTok{), }\AttributeTok{nrow =} \DecValTok{3}\NormalTok{, }\AttributeTok{byrow =} \ConstantTok{TRUE}\NormalTok{,}
\AttributeTok{dimnames =} \FunctionTok{list}\NormalTok{(}
    \StringTok{"Before"} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"Low"}\NormalTok{, }\StringTok{"Medium"}\NormalTok{, }\StringTok{"High"}\NormalTok{),}
    \StringTok{"After"} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"Low"}\NormalTok{, }\StringTok{"Medium"}\NormalTok{, }\StringTok{"High"}\NormalTok{)}
\NormalTok{))}

\CommentTok{\# Function to perform the Stuart{-}Maxwell Test}
\NormalTok{stuart\_maxwell\_test }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(table) \{}
  \ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\FunctionTok{all}\NormalTok{(}\FunctionTok{dim}\NormalTok{(table)[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{==} \FunctionTok{dim}\NormalTok{(table)[}\DecValTok{2}\NormalTok{])) \{}
    \FunctionTok{stop}\NormalTok{(}\StringTok{"Input must be a square matrix."}\NormalTok{)}
\NormalTok{  \}}
  
  \CommentTok{\# Marginal totals for each category}
\NormalTok{  row\_totals }\OtherTok{\textless{}{-}} \FunctionTok{rowSums}\NormalTok{(table)}
\NormalTok{  col\_totals }\OtherTok{\textless{}{-}} \FunctionTok{colSums}\NormalTok{(table)}
  
  \CommentTok{\# Vector of differences between row and column marginal totals}
\NormalTok{  b }\OtherTok{\textless{}{-}}\NormalTok{ row\_totals }\SpecialCharTok{{-}}\NormalTok{ col\_totals}
  
  \CommentTok{\# Covariance matrix under the null hypothesis}
\NormalTok{  total }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(table)}
\NormalTok{  V }\OtherTok{\textless{}{-}} \FunctionTok{diag}\NormalTok{(row\_totals }\SpecialCharTok{+}\NormalTok{ col\_totals) }\SpecialCharTok{{-}} 
\NormalTok{       (}\FunctionTok{outer}\NormalTok{(row\_totals, col\_totals, }\StringTok{"+"}\NormalTok{) }\SpecialCharTok{/}\NormalTok{ total)}
  
  \CommentTok{\# Calculate the test statistic}
\NormalTok{  M2 }\OtherTok{\textless{}{-}} \FunctionTok{t}\NormalTok{(b) }\SpecialCharTok{\%*\%} \FunctionTok{solve}\NormalTok{(V) }\SpecialCharTok{\%*\%}\NormalTok{ b}
\NormalTok{  df }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(table) }\SpecialCharTok{{-}} \DecValTok{1}
\NormalTok{  p\_value }\OtherTok{\textless{}{-}} \FunctionTok{pchisq}\NormalTok{(M2, }\AttributeTok{df =}\NormalTok{ df, }\AttributeTok{lower.tail =} \ConstantTok{FALSE}\NormalTok{)}
  
  \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{statistic =}\NormalTok{ M2, }\AttributeTok{df =}\NormalTok{ df, }\AttributeTok{p\_value =}\NormalTok{ p\_value))}
\NormalTok{\}}

\CommentTok{\# Run the Stuart{-}Maxwell Test}
\NormalTok{result }\OtherTok{\textless{}{-}} \FunctionTok{stuart\_maxwell\_test}\NormalTok{(satisfaction\_table)}

\CommentTok{\# Print the results}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Stuart{-}Maxwell Test Results:}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Stuart{-}Maxwell Test Results:}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Test Statistic (M\^{}2):"}\NormalTok{, result}\SpecialCharTok{$}\NormalTok{statistic, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Test Statistic (M\^{}2): 0.01802387}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Degrees of Freedom:"}\NormalTok{, result}\SpecialCharTok{$}\NormalTok{df, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Degrees of Freedom: 2}
\FunctionTok{cat}\NormalTok{(}\StringTok{"p{-}value:"}\NormalTok{, result}\SpecialCharTok{$}\NormalTok{p\_value, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} p{-}value: 0.9910286}
\end{Highlighting}
\end{Shaded}

Interpretation

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Test Statistic}: Measures the extent of marginal differences in the table.
\item
  \textbf{p-value}:

  \begin{itemize}
  \item
    A \textbf{low p-value} (e.g., \(p < 0.05\)) indicates significant differences between the marginal distributions, suggesting a change in the distribution of responses.
  \item
    A \textbf{high p-value} suggests no evidence of marginal differences, meaning the distribution is consistent across conditions.
  \end{itemize}
\end{enumerate}

Practical Applications of the Stuart-Maxwell Test

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Employee Surveys}: Analyzing shifts in satisfaction levels before and after policy changes.
\item
  \textbf{Consumer Studies}: Evaluating changes in product preferences before and after a marketing campaign.
\item
  \textbf{Healthcare Research}: Assessing changes in patient responses to treatments across categories.
\end{enumerate}

\hypertarget{cochran-mantel-haenszel-cmh-test}{%
\subsubsection{Cochran-Mantel-Haenszel (CMH) Test}\label{cochran-mantel-haenszel-cmh-test}}

The Cochran-Mantel-Haenszel (CMH) Test is a generalization of the \protect\hyperlink{mantel-haenszel-chi-square-test}{Mantel-Haenszel Chi-square Test}. It evaluates the association between two variables while controlling for the effect of a third stratifying variable. This test is particularly suited for ordinal data, allowing researchers to detect trends and associations across strata.

The CMH Test addresses scenarios where:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Two variables (e.g., exposure and outcome) are ordinal or nominal.
\item
  A third variable (e.g., a demographic or environmental factor) stratifies the data into \(K\) independent groups.
\end{enumerate}

The test answers: Is there a consistent association between the two variables across the strata defined by the third variable?

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The CMH Test has three main variations depending on the nature of the data:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Correlation Test for Ordinal Data}: Assesses whether there is a linear association between two ordinal variables across strata.
\item
  \textbf{General Association Test}: Tests for any association (not necessarily ordinal) between two variables while stratifying by a third.
\item
  \textbf{Homogeneity Test}: Checks whether the strength of the association between the two variables is consistent across strata.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Hypotheses

\begin{itemize}
\item
  \textbf{Null Hypothesis (}\(H_0\)): There is no association between the two variables across all strata, or the strength of the association is consistent across strata.
\item
  \textbf{Alternative Hypothesis (}\(H_A\)): There is an association between the two variables in at least one stratum, or the strength of the association varies across strata.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The CMH test statistic is: \[
CMH = \frac{\left( \sum_{k} \left(O_k - E_k \right)\right)^2}{\sum_{k} V_k}
\] Where:

\begin{itemize}
\item
  \(O_k\): Observed counts in stratum \(k\).
\item
  \(E_k\): Expected counts in stratum \(k\), calculated under the null hypothesis.
\item
  \(V_k\): Variance of the observed counts in stratum \(k\).
\end{itemize}

The test statistic follows a \(\chi^2\) distribution with 1 degree of freedom under the null hypothesis.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

A company evaluates whether sales performance (Low, Medium, High) is associated with product satisfaction (Low, Medium, High) across three experience levels (Junior, Mid-level, Senior). The data is organized into a \(3 \times 3 \times 3\) contingency table.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Sales performance data}
\NormalTok{sales\_data }\OtherTok{\textless{}{-}} \FunctionTok{array}\NormalTok{(}
    \FunctionTok{c}\NormalTok{(}\DecValTok{20}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{18}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{20}\NormalTok{,   }\CommentTok{\# Junior}
      \DecValTok{25}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{20}\NormalTok{,  }\CommentTok{\# Mid{-}level}
      \DecValTok{30}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{28}\NormalTok{, }\DecValTok{32}\NormalTok{, }\DecValTok{35}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{30}\NormalTok{), }\CommentTok{\# Senior}
    \AttributeTok{dim =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{),}
    \AttributeTok{dimnames =} \FunctionTok{list}\NormalTok{(}
        \AttributeTok{SalesPerformance =} \FunctionTok{c}\NormalTok{(}\StringTok{"Low"}\NormalTok{, }\StringTok{"Medium"}\NormalTok{, }\StringTok{"High"}\NormalTok{),}
        \AttributeTok{Satisfaction =} \FunctionTok{c}\NormalTok{(}\StringTok{"Low"}\NormalTok{, }\StringTok{"Medium"}\NormalTok{, }\StringTok{"High"}\NormalTok{),}
        \AttributeTok{ExperienceLevel =} \FunctionTok{c}\NormalTok{(}\StringTok{"Junior"}\NormalTok{, }\StringTok{"Mid{-}level"}\NormalTok{, }\StringTok{"Senior"}\NormalTok{)}
\NormalTok{    )}
\NormalTok{)}

\CommentTok{\# Load the vcd package for the CMH test}
\FunctionTok{library}\NormalTok{(vcd)}

\CommentTok{\# Perform CMH Test}
\NormalTok{cmh\_result }\OtherTok{\textless{}{-}} \FunctionTok{mantelhaen.test}\NormalTok{(sales\_data, }\AttributeTok{correct =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{cmh\_result}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Cochran{-}Mantel{-}Haenszel test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  sales\_data}
\CommentTok{\#\textgreater{} Cochran{-}Mantel{-}Haenszel M\^{}2 = 22.454, df = 4, p{-}value = 0.0001627}
\end{Highlighting}
\end{Shaded}

Interpretation

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Test Statistic}: A large CMH statistic suggests a significant association between sales performance and satisfaction after accounting for experience level.
\item
  \textbf{p-value}:

  \begin{itemize}
  \item
    A \textbf{low p-value} (e.g., \(p < 0.05\)) indicates a significant association between the two variables across strata.
  \item
    A \textbf{high p-value} suggests no evidence of association or that the relationship is consistent across all strata.
  \end{itemize}
\end{enumerate}

Practical Applications of the CMH Test

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Business Performance Analysis}: Investigating the relationship between customer satisfaction and sales performance across different demographic groups.
\item
  \textbf{Healthcare Studies}: Assessing the effect of treatment (e.g., dosage) on outcomes while controlling for patient characteristics (e.g., age groups).
\item
  \textbf{Educational Research}: Analyzing the relationship between test scores and study hours, stratified by teaching method.
\end{enumerate}

\hypertarget{summary-table-of-tests}{%
\subsubsection{Summary Table of Tests}\label{summary-table-of-tests}}

The following table provides a concise guide on when and why to use each test:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1610}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3483}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2622}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2210}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Test Name}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{When to Use}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Question Addressed}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Data Requirements}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
{[}\textbf{Mantel-Haenszel Chi-square Test}{]} & When testing for association between two binary variables across multiple strata. & Is there a consistent association across strata? & Binary variables in \(2 \times 2 \times K\) tables. \\
{[}\textbf{McNemar's Test}{]} & When analyzing marginal symmetry in paired binary data. & Are the proportions of discordant pairs equal? & Paired binary responses (\(2 \times 2\) table). \\
{[}\textbf{McNemar-Bowker Test}{]} & When testing for symmetry in paired nominal data with more than two categories. & Are the off-diagonal elements symmetric across all categories? & Paired nominal data in \(r \times r\) tables. \\
{[}\textbf{Cochran-Mantel-Haenszel (CMH) Test}{]} & When testing ordinal or general associations while controlling for a stratifying variable. & Is there an association between two variables after stratification? & Ordinal or nominal data in \(I \times J \times K\) tables. \\
{[}\textbf{Stuart-Maxwell Test}{]} & When analyzing marginal homogeneity in paired nominal data with more than two categories. & Are the marginal distributions of paired data homogeneous? & Paired nominal data in \(r \times r\) tables. \\
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{How to Choose the Right Test}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Paired vs.~Stratified Data}:

  \begin{itemize}
  \tightlist
  \item
    Use \textbf{McNemar's Test} or \textbf{McNemar-Bowker Test} for paired data.
  \item
    Use \textbf{Mantel-Haenszel} or \textbf{CMH Test} for stratified data.
  \end{itemize}
\item
  \textbf{Binary vs.~Multi-category Variables}:

  \begin{itemize}
  \tightlist
  \item
    Use \textbf{McNemar's Test} for binary data.
  \item
    Use \textbf{McNemar-Bowker Test} or \textbf{Stuart-Maxwell Test} for multi-category data.
  \end{itemize}
\item
  \textbf{Ordinal Trends}:

  \begin{itemize}
  \tightlist
  \item
    Use the \textbf{CMH Test} if testing for ordinal associations while controlling for a stratifying variable.
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{ordinal-trend}{%
\subsection{Ordinal Trend}\label{ordinal-trend}}

When analyzing ordinal data, it is often important to determine whether a consistent trend exists between variables. Tests for trend are specifically designed to detect monotonic relationships where changes in one variable are systematically associated with changes in another. These tests are widely used in scenarios involving ordered categories, such as customer satisfaction ratings, income brackets, or educational levels.

The primary objectives of trend tests are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{To detect monotonic relationships}: Determine if higher or lower categories of one variable are associated with higher or lower categories of another variable.
\item
  \textbf{To account for ordinal structure}: Leverage the inherent order in the data to provide more sensitive and interpretable results compared to tests designed for nominal data.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Key Considerations for Trend Tests}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Data Structure}:

  \begin{itemize}
  \tightlist
  \item
    Ensure that the variables have a natural order and are treated as ordinal.
  \item
    Verify that the trend test chosen matches the data structure (e.g., binary outcome vs.~multi-level ordinal variables).
  \end{itemize}
\item
  \textbf{Assumptions}:

  \begin{itemize}
  \tightlist
  \item
    Many tests assume monotonic trends, meaning that the relationship should not reverse direction.
  \end{itemize}
\item
  \textbf{Interpretation}:

  \begin{itemize}
  \tightlist
  \item
    A significant result indicates the presence of a trend but does not imply causality.
  \item
    The direction and strength of the trend should be carefully interpreted in the context of the data.
  \end{itemize}
\end{enumerate}

\hypertarget{cochran-armitage-test}{%
\subsubsection{Cochran-Armitage Test}\label{cochran-armitage-test}}

The Cochran-Armitage Test for Trend is a statistical method designed to detect a linear trend in proportions across ordered categories of a predictor variable. It is particularly useful in \(2 \times J\) contingency tables, where there is a binary outcome (e.g., success/failure) and an ordinal predictor variable with \(J\) ordered levels.

The Cochran-Armitage Test evaluates whether the proportion of a binary outcome changes systematically across the levels of an ordinal predictor. This test leverages the ordinal nature of the predictor to enhance sensitivity and power compared to general chi-square tests.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Hypotheses}

\begin{itemize}
\item
  \textbf{Null Hypothesis (}\(H_0\)): \[
  \text{The proportion of the binary outcome is constant across the levels of the ordinal predictor.}
  \]
\item
  \textbf{Alternative Hypothesis (}\(H_A\)): \[
  \text{There is a linear trend in the proportion of the binary outcome across the levels of the ordinal predictor.}
  \]
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The Cochran-Armitage Test statistic is calculated as:

\[
Z = \frac{\sum_{j=1}^{J} w_j (n_{1j} - N_j \hat{p})}{\sqrt{\hat{p} (1 - \hat{p}) \sum_{j=1}^{J} w_j^2 N_j}}
\]

Where:

\begin{itemize}
\item
  \(n_{1j}\): Count of the binary outcome (e.g., ``success'') in category \(j\).
\item
  \(N_j\): Total number of observations in category \(j\).
\item
  \(\hat{p}\): Overall proportion of the binary outcome, calculated as: \[
    \hat{p} = \frac{\sum_{j=1}^{J} n_{1j}}{\sum_{j=1}^{J} N_j}
    \]
\item
  \(w_j\): Score assigned to the \(j\)th category of the ordinal predictor, often set to \(j\) for equally spaced levels.
\end{itemize}

The test statistic \(Z\) follows a standard normal distribution under the null hypothesis.

\textbf{Key Assumptions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Ordinal Predictor}: The categories of the predictor variable must have a natural order.
\item
  \textbf{Binary Outcome}: The response variable must be dichotomous (e.g., success/failure).
\item
  \textbf{Independent Observations}: Observations within and across categories are independent.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Let's consider a study examining whether the success rate of a marketing campaign varies across three income levels (Low, Medium, High). The data is structured in a \(2 \times 3\) contingency table:

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Income Level & Success & Failure & Total \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Low & 20 & 30 & 50 \\
Medium & 35 & 15 & 50 \\
High & 45 & 5 & 50 \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Data: Success and Failure counts by Income Level}
\NormalTok{income\_levels }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Low"}\NormalTok{, }\StringTok{"Medium"}\NormalTok{, }\StringTok{"High"}\NormalTok{)}
\NormalTok{success }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{20}\NormalTok{, }\DecValTok{35}\NormalTok{, }\DecValTok{45}\NormalTok{)}
\NormalTok{failure }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{30}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{total }\OtherTok{\textless{}{-}}\NormalTok{ success }\SpecialCharTok{+}\NormalTok{ failure}

\CommentTok{\# Scores for ordinal levels (can be custom weights)}
\NormalTok{scores }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(income\_levels)}

\CommentTok{\# Cochran{-}Armitage Test}
\CommentTok{\# Function to calculate Z statistic}
\NormalTok{cochran\_armitage\_test }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(success, failure, scores) \{}
\NormalTok{  N }\OtherTok{\textless{}{-}}\NormalTok{ success }\SpecialCharTok{+}\NormalTok{ failure}
\NormalTok{  p\_hat }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(success) }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(N)}
\NormalTok{  weights }\OtherTok{\textless{}{-}}\NormalTok{ scores}
  
  \CommentTok{\# Calculate numerator}
\NormalTok{  numerator }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(weights }\SpecialCharTok{*}\NormalTok{ (success }\SpecialCharTok{{-}}\NormalTok{ N }\SpecialCharTok{*}\NormalTok{ p\_hat))}
  
  \CommentTok{\# Calculate denominator}
\NormalTok{  denominator }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(p\_hat }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ p\_hat) }\SpecialCharTok{*} \FunctionTok{sum}\NormalTok{(weights}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ N))}
  
  \CommentTok{\# Z statistic}
\NormalTok{  Z }\OtherTok{\textless{}{-}}\NormalTok{ numerator }\SpecialCharTok{/}\NormalTok{ denominator}
\NormalTok{  p\_value }\OtherTok{\textless{}{-}} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{pnorm}\NormalTok{(}\FunctionTok{abs}\NormalTok{(Z)))}
  
  \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{Z\_statistic =}\NormalTok{ Z, }\AttributeTok{p\_value =}\NormalTok{ p\_value))}
\NormalTok{\}}

\CommentTok{\# Perform the test}
\NormalTok{result }\OtherTok{\textless{}{-}} \FunctionTok{cochran\_armitage\_test}\NormalTok{(success, failure, scores)}

\CommentTok{\# Print results}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Cochran{-}Armitage Test for Trend Results:}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Cochran{-}Armitage Test for Trend Results:}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Z Statistic:"}\NormalTok{, result}\SpecialCharTok{$}\NormalTok{Z\_statistic, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Z Statistic: 2.004459}
\FunctionTok{cat}\NormalTok{(}\StringTok{"p{-}value:"}\NormalTok{, result}\SpecialCharTok{$}\NormalTok{p\_value, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} p{-}value: 0.04502088}
\end{Highlighting}
\end{Shaded}

Interpretation

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Test Statistic (}\(Z\)\textbf{)}:

  \begin{itemize}
  \item
    The \(Z\) value indicates the strength and direction of the trend.
  \item
    Positive \(Z\): Proportions increase with higher categories.
  \item
    Negative \(Z\): Proportions decrease with higher categories.
  \end{itemize}
\item
  \textbf{p-value}:

  \begin{itemize}
  \item
    A \textbf{low p-value} (e.g., \(p < 0.05\)) rejects the null hypothesis, indicating a significant linear trend.
  \item
    A \textbf{high p-value} fails to reject the null hypothesis, suggesting no evidence of a trend.
  \end{itemize}
\end{enumerate}

Practical Applications

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Marketing}: Analyzing whether customer success rates vary systematically across income levels or demographics.
\item
  \textbf{Healthcare}: Evaluating the dose-response relationship between medication levels and recovery rates.
\item
  \textbf{Education}: Studying whether pass rates improve with higher levels of educational support.
\end{enumerate}

\hypertarget{jonckheere-terpstra-test}{%
\subsubsection{Jonckheere-Terpstra Test}\label{jonckheere-terpstra-test}}

The Jonckheere-Terpstra Test is a nonparametric test designed to detect ordered differences between groups. It is particularly suited for ordinal data where both the predictor and response variables exhibit a monotonic trend. Unlike general nonparametric tests like the Kruskal-Wallis test, which assess any differences between groups, the Jonckheere-Terpstra Test specifically evaluates whether the data follows a prespecified ordering.

The Jonckheere-Terpstra Test determines whether:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  There is a monotonic trend in the response variable across ordered groups of the predictor.
\item
  The data aligns with an a priori hypothesized order (e.g., group 1 \textless{} group 2 \textless{} group 3).
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Hypotheses}

\begin{itemize}
\item
  \textbf{Null Hypothesis (}\(H_0\)): \[
  \text{There is no trend in the response variable across the ordered groups.}
  \]
\item
  \textbf{Alternative Hypothesis (}\(H_A\)): \[
  \text{The response variable exhibits a monotonic trend across the ordered groups.}
  \]
\end{itemize}

The trend can be increasing, decreasing, or as otherwise hypothesized.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The Jonckheere-Terpstra Test statistic is based on the number of pairwise comparisons (\(U\)) that are consistent with the hypothesized trend. For \(k\) groups:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compare all possible pairs of observations across groups.
\item
  Count the number of pairs where the values are consistent with the hypothesized order.
\end{enumerate}

The test statistic \(T\) is the sum of all pairwise comparisons: \[
T = \sum_{i < j} T_{ij}
\] Where \(T_{ij}\) is the number of concordant pairs between groups \(i\) and \(j\).

Under the null hypothesis, \(T\) follows a normal distribution with:

\begin{itemize}
\item
  Mean: \[
    \mu_T = \frac{N (N - 1)}{4}
    \]
\item
  Variance: \[
    \sigma_T^2 = \frac{N (N - 1) (2N + 1)}{24}
    \] Where \(N\) is the total number of observations.
\end{itemize}

The standardized test statistic is: \[
Z = \frac{T - \mu_T}{\sigma_T}
\]

\textbf{Key Assumptions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Ordinal or Interval Data}: The response variable must be at least ordinal, and the groups must have a logical order.
\item
  \textbf{Independent Groups}: Observations within and between groups are independent.
\item
  \textbf{Consistent Hypothesis}: The trend (e.g., increasing or decreasing) must be specified in advance.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Let's consider a study analyzing whether customer satisfaction ratings (on a scale of 1 to 5) improve with increasing levels of service tiers (Basic, Standard, Premium). The data is grouped by service tier, and we hypothesize that satisfaction ratings increase with higher service tiers.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example Data: Customer Satisfaction Ratings by Service Tier}
\NormalTok{satisfaction }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
  \AttributeTok{Basic =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{3}\NormalTok{),}
  \AttributeTok{Standard =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{4}\NormalTok{),}
  \AttributeTok{Premium =} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{)}

\CommentTok{\# Prepare data}
\NormalTok{ratings }\OtherTok{\textless{}{-}} \FunctionTok{unlist}\NormalTok{(satisfaction)}
\NormalTok{groups }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\FunctionTok{names}\NormalTok{(satisfaction), }\AttributeTok{times =} \FunctionTok{sapply}\NormalTok{(satisfaction, length)))}

\CommentTok{\# Calculate pairwise comparisons}
\NormalTok{manual\_jonckheere }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(ratings, groups) \{}
\NormalTok{  n\_groups }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(}\FunctionTok{unique}\NormalTok{(groups))}
\NormalTok{  pairwise\_comparisons }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{  total\_pairs }\OtherTok{\textless{}{-}} \DecValTok{0}
  
  \CommentTok{\# Iterate over group pairs}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{(n\_groups }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)) \{}
    \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in}\NormalTok{ (i }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{n\_groups) \{}
\NormalTok{      group\_i }\OtherTok{\textless{}{-}}\NormalTok{ ratings[groups }\SpecialCharTok{==} \FunctionTok{levels}\NormalTok{(groups)[i]]}
\NormalTok{      group\_j }\OtherTok{\textless{}{-}}\NormalTok{ ratings[groups }\SpecialCharTok{==} \FunctionTok{levels}\NormalTok{(groups)[j]]}
      
      \CommentTok{\# Count concordant pairs}
      \ControlFlowTok{for}\NormalTok{ (x }\ControlFlowTok{in}\NormalTok{ group\_i) \{}
        \ControlFlowTok{for}\NormalTok{ (y }\ControlFlowTok{in}\NormalTok{ group\_j) \{}
          \ControlFlowTok{if}\NormalTok{ (x }\SpecialCharTok{\textless{}}\NormalTok{ y) pairwise\_comparisons }\OtherTok{\textless{}{-}}\NormalTok{ pairwise\_comparisons }\SpecialCharTok{+} \DecValTok{1}
          \ControlFlowTok{if}\NormalTok{ (x }\SpecialCharTok{==}\NormalTok{ y) pairwise\_comparisons }\OtherTok{\textless{}{-}}\NormalTok{ pairwise\_comparisons }\SpecialCharTok{+} \FloatTok{0.5}
\NormalTok{          total\_pairs }\OtherTok{\textless{}{-}}\NormalTok{ total\_pairs }\SpecialCharTok{+} \DecValTok{1}
\NormalTok{        \}}
\NormalTok{      \}}
\NormalTok{    \}}
\NormalTok{  \}}
  
  \CommentTok{\# Compute test statistic}
\NormalTok{  T }\OtherTok{\textless{}{-}}\NormalTok{ pairwise\_comparisons}
\NormalTok{  N }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(ratings)}
\NormalTok{  mu\_T }\OtherTok{\textless{}{-}}\NormalTok{ total\_pairs }\SpecialCharTok{/} \DecValTok{2}
\NormalTok{  sigma\_T }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(total\_pairs }\SpecialCharTok{*}\NormalTok{ (N }\SpecialCharTok{+} \DecValTok{1}\NormalTok{) }\SpecialCharTok{/} \DecValTok{12}\NormalTok{)}
  
\NormalTok{  Z }\OtherTok{\textless{}{-}}\NormalTok{ (T }\SpecialCharTok{{-}}\NormalTok{ mu\_T) }\SpecialCharTok{/}\NormalTok{ sigma\_T}
\NormalTok{  p\_value }\OtherTok{\textless{}{-}} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{pnorm}\NormalTok{(}\FunctionTok{abs}\NormalTok{(Z)))}
  
  \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{T\_statistic =}\NormalTok{ T, }\AttributeTok{Z\_statistic =}\NormalTok{ Z, }\AttributeTok{p\_value =}\NormalTok{ p\_value))}
\NormalTok{\}}

\CommentTok{\# Perform the test}
\NormalTok{result }\OtherTok{\textless{}{-}} \FunctionTok{manual\_jonckheere}\NormalTok{(ratings, groups)}

\CommentTok{\# Print results}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Jonckheere{-}Terpstra Test Results:}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Jonckheere{-}Terpstra Test Results:}
\FunctionTok{cat}\NormalTok{(}\StringTok{"T Statistic (Sum of Concordant Pairs):"}\NormalTok{, result}\SpecialCharTok{$}\NormalTok{T\_statistic, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} T Statistic (Sum of Concordant Pairs): 49.5}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Z Statistic:"}\NormalTok{, result}\SpecialCharTok{$}\NormalTok{Z\_statistic, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Z Statistic: 1.2}
\FunctionTok{cat}\NormalTok{(}\StringTok{"p{-}value:"}\NormalTok{, result}\SpecialCharTok{$}\NormalTok{p\_value, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} p{-}value: 0.2301393}
\end{Highlighting}
\end{Shaded}

Interpretation

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Test Statistic (}\(T\)\textbf{)}:

  \begin{itemize}
  \tightlist
  \item
    Represents the sum of all pairwise comparisons consistent with the hypothesized order.
  \item
    Includes 0.5 for tied pairs.
  \end{itemize}
\item
  \(Z\) \textbf{Statistic}:

  \begin{itemize}
  \item
    A standardized measure of the strength of the trend.
  \item
    Calculated using \(T\), the expected value of \(T\) under the null hypothesis (\(\mu_T\)), and the variance of \(T\) (\(\sigma_T^2\)).
  \end{itemize}
\item
  \textbf{p-value}:

  \begin{itemize}
  \item
    A \textbf{low p-value} (e.g., \(p < 0.05\)) rejects the null hypothesis, indicating a significant trend in the response variable across ordered groups.
  \item
    A \textbf{high p-value} fails to reject the null hypothesis, suggesting no evidence of a trend.
  \end{itemize}
\end{enumerate}

Practical Applications

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Customer Experience Analysis}: Assessing whether customer satisfaction increases with higher service levels or product tiers.
\item
  \textbf{Healthcare Studies}: Testing whether recovery rates improve with increasing doses of a treatment.
\item
  \textbf{Education Research}: Analyzing whether test scores improve with higher levels of educational intervention.
\end{enumerate}

\hypertarget{mantel-test-for-trend}{%
\subsubsection{Mantel Test for Trend}\label{mantel-test-for-trend}}

The Mantel Test for Trend is a statistical method designed to detect a linear association between two ordinal variables. It is an extension of the \protect\hyperlink{mantel-haenszel-chi-square-test}{Mantel-Haenszel Chi-square Test} and is particularly suited for analyzing trends in ordinal contingency tables, such as \(I \times J\) tables where both variables are ordinal.

The Mantel Test for Trend evaluates whether an increasing or decreasing trend exists between two ordinal variables. It uses the ordering of categories to assess linear relationships, making it more sensitive to trends compared to general association tests like chi-square.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Hypotheses}

\begin{itemize}
\item
  \textbf{Null Hypothesis (}\(H_0\)): \[
  \text{There is no linear association between the two ordinal variables.}
  \]
\item
  \textbf{Alternative Hypothesis (}\(H_A\)): \[
  \text{There is a significant linear association between the two ordinal variables.}
  \]
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The Mantel Test is based on the Pearson correlation between the row and column scores in an ordinal contingency table. The test statistic is: \[
M = \frac{\sum_{i} \sum_{j} w_i w_j n_{ij}}{\sqrt{\sum_{i} w_i^2 n_{i\cdot} \sum_{j} w_j^2 n_{\cdot j}}}
\]

Where:

\begin{itemize}
\item
  \(n_{ij}\): Observed frequency in cell \((i, j)\).
\item
  \(n_{i\cdot}\): Row marginal total for row \(i\).
\item
  \(n_{\cdot j}\): Column marginal total for column \(j\).
\item
  \(w_i\): Score for the \(i\)th row.
\item
  ore for the \(j\)th column.
\end{itemize}

The test statistic \(M\) is asymptotically normally distributed under the null hypothesis.

\textbf{Key Assumptions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Ordinal Variables}: Both variables must have a natural order.
\item
  \textbf{Linear Trend}: Assumes a linear relationship between the scores assigned to the rows and columns.
\item
  \textbf{Independence}: Observations must be independent.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Let's consider a marketing study evaluating whether customer satisfaction levels (Low, Medium, High) are associated with increasing purchase frequency (Low, Medium, High).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Customer satisfaction and purchase frequency data}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}
  \FunctionTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{12}\NormalTok{), }
  \AttributeTok{nrow =} \DecValTok{3}\NormalTok{, }
  \AttributeTok{byrow =} \ConstantTok{TRUE}\NormalTok{,}
  \AttributeTok{dimnames =} \FunctionTok{list}\NormalTok{(}
    \AttributeTok{Satisfaction =} \FunctionTok{c}\NormalTok{(}\StringTok{"Low"}\NormalTok{, }\StringTok{"Medium"}\NormalTok{, }\StringTok{"High"}\NormalTok{),}
    \AttributeTok{Frequency =} \FunctionTok{c}\NormalTok{(}\StringTok{"Low"}\NormalTok{, }\StringTok{"Medium"}\NormalTok{, }\StringTok{"High"}\NormalTok{)}
\NormalTok{  )}
\NormalTok{)}

\CommentTok{\# Assign scores for rows and columns}
\NormalTok{row\_scores }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(data)}
\NormalTok{col\_scores }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{ncol}\NormalTok{(data)}

\CommentTok{\# Compute Mantel statistic manually}
\NormalTok{mantel\_test\_manual }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, row\_scores, col\_scores) \{}
\NormalTok{  numerator }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(}\FunctionTok{outer}\NormalTok{(row\_scores, col\_scores, }\StringTok{"*"}\NormalTok{) }\SpecialCharTok{*}\NormalTok{ data)}
\NormalTok{  row\_marginals }\OtherTok{\textless{}{-}} \FunctionTok{rowSums}\NormalTok{(data)}
\NormalTok{  col\_marginals }\OtherTok{\textless{}{-}} \FunctionTok{colSums}\NormalTok{(data)}
\NormalTok{  row\_variance }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(row\_scores}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ row\_marginals)}
\NormalTok{  col\_variance }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(col\_scores}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ col\_marginals)}
  
\NormalTok{  M }\OtherTok{\textless{}{-}}\NormalTok{ numerator }\SpecialCharTok{/} \FunctionTok{sqrt}\NormalTok{(row\_variance }\SpecialCharTok{*}\NormalTok{ col\_variance)}
\NormalTok{  z\_value }\OtherTok{\textless{}{-}}\NormalTok{ M}
\NormalTok{  p\_value }\OtherTok{\textless{}{-}} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{pnorm}\NormalTok{(}\FunctionTok{abs}\NormalTok{(z\_value))) }\CommentTok{\# Two{-}tailed test}
  
  \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{Mantel\_statistic =}\NormalTok{ M, }\AttributeTok{p\_value =}\NormalTok{ p\_value))}
\NormalTok{\}}

\CommentTok{\# Perform the Mantel Test}
\NormalTok{result }\OtherTok{\textless{}{-}} \FunctionTok{mantel\_test\_manual}\NormalTok{(data, row\_scores, col\_scores)}

\CommentTok{\# Display results}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Mantel Test for Trend Results:}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Mantel Test for Trend Results:}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Mantel Statistic (M):"}\NormalTok{, result}\SpecialCharTok{$}\NormalTok{Mantel\_statistic, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Mantel Statistic (M): 0.8984663}
\FunctionTok{cat}\NormalTok{(}\StringTok{"p{-}value:"}\NormalTok{, result}\SpecialCharTok{$}\NormalTok{p\_value, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} p{-}value: 0.368937}
\end{Highlighting}
\end{Shaded}

Interpretation

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Test Statistic (}\(M\)\textbf{)}:

  \begin{itemize}
  \item
    Represents the strength and direction of the linear association.
  \item
    Positive \(M\): Increasing trend.
  \item
    Negative \(M\): Decreasing trend.
  \end{itemize}
\item
  \textbf{p-value}:

  \begin{itemize}
  \item
    A \textbf{low p-value} (e.g., \(p < 0.05\)) indicates a significant linear association.
  \item
    A \textbf{high p-value} suggests no evidence of a trend.
  \end{itemize}
\end{enumerate}

Practical Applications

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Marketing Analysis}: Investigating whether satisfaction levels are associated with purchase behavior or loyalty.
\item
  \textbf{Healthcare Research}: Testing for a dose-response relationship between treatment levels and outcomes.
\item
  \textbf{Social Sciences}: Analyzing trends in survey responses across ordered categories.
\end{enumerate}

\hypertarget{chi-square-test-for-linear-trend}{%
\subsubsection{Chi-square Test for Linear Trend}\label{chi-square-test-for-linear-trend}}

The Chi-square Test for Linear Trend is a statistical method used to detect a linear relationship between an ordinal predictor and a binary outcome. It is an extension of the chi-square test, designed specifically for ordered categories, making it more sensitive to linear trends in proportions compared to a general chi-square test of independence.

The Chi-square Test for Linear Trend evaluates whether the proportions of a binary outcome (e.g., success/failure) change systematically across ordered categories of a predictor variable. It is widely used in situations such as analyzing dose-response relationships or evaluating trends in survey responses.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Hypotheses}

\begin{itemize}
\item
  \textbf{Null Hypothesis (}\(H_0\)): \[
  \text{There is no linear trend in the proportions of the binary outcome across ordered categories.}
  \]
\item
  \textbf{Alternative Hypothesis (}\(H_A\)): \[
  \text{There is a significant linear trend in the proportions of the binary outcome across ordered categories.}
  \]
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The test statistic is:

\[
X^2_{\text{trend}} = \frac{\left( \sum_{j=1}^J w_j (p_j - \bar{p}) N_j \right)^2}{\sum_{j=1}^J w_j^2 \bar{p} (1 - \bar{p}) N_j}
\]

Where: - \(J\): Number of ordered categories. - \(w_j\): Scores assigned to the \(j\)th category (typically \(j = 1, 2, \dots, J\)). - \(p_j\): Proportion of success in the \(j\)th category. - \(\bar{p}\): Overall proportion of success across all categories. - \(N_j\): Total number of observations in the \(j\)th category.

The test statistic follows a chi-square distribution with 1 degree of freedom under the null hypothesis.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Key Assumptions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Binary Outcome}: The response variable must be binary (e.g., success/failure).
\item
  \textbf{Ordinal Predictor}: The predictor variable must have a natural order.
\item
  \textbf{Independent Observations}: Data across categories must be independent.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Let's consider a study analyzing whether the proportion of customers who recommend a product increases with customer satisfaction levels (Low, Medium, High).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example Data: Customer Satisfaction and Recommendation}
\NormalTok{satisfaction\_levels }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Low"}\NormalTok{, }\StringTok{"Medium"}\NormalTok{, }\StringTok{"High"}\NormalTok{)}
\NormalTok{success }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{20}\NormalTok{, }\DecValTok{35}\NormalTok{, }\DecValTok{50}\NormalTok{)  }\CommentTok{\# Number of customers who recommend the product}
\NormalTok{failure }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{30}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{10}\NormalTok{)  }\CommentTok{\# Number of customers who do not recommend the product}
\NormalTok{total }\OtherTok{\textless{}{-}}\NormalTok{ success }\SpecialCharTok{+}\NormalTok{ failure}

\CommentTok{\# Assign ordinal scores}
\NormalTok{scores }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(satisfaction\_levels)}

\CommentTok{\# Calculate overall proportion of success}
\NormalTok{p\_hat }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(success) }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(total)}
\end{Highlighting}
\end{Shaded}

Interpretation

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Chi-square Statistic (}\(X^2_{\text{trend}}\)\textbf{)}:

  \begin{itemize}
  \tightlist
  \item
    Indicates the strength of the linear trend in the proportions.
  \end{itemize}
\item
  \textbf{p-value}:

  \begin{itemize}
  \item
    A \textbf{low p-value} (e.g., \(p < 0.05\)) rejects the null hypothesis, indicating a significant linear trend.
  \item
    A \textbf{high p-value} suggests no evidence of a linear trend.
  \end{itemize}
\end{enumerate}

Practical Applications

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Marketing}: Analyzing whether customer satisfaction levels predict product recommendations or repurchase intentions.
\item
  \textbf{Healthcare}: Evaluating dose-response relationships in clinical trials.
\item
  \textbf{Education}: Testing whether higher levels of intervention improve success rates.
\end{enumerate}

\hypertarget{key-takeways}{%
\subsubsection{Key Takeways}\label{key-takeways}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1559}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2890}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2129}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3346}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Test}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Purpose}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Assumptions}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Use Cases}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
{[}\textbf{Cochran-Armitage Test}{]} & Tests for a linear trend in proportions across ordinal categories. & \begin{minipage}[t]{\linewidth}\raggedright
- Binary response variable.\\
- Predictor variable is ordinal.\strut
\end{minipage} & Evaluating dose-response relationships, comparing proportions across ordinal groups. \\
{[}\textbf{Jonckheere-Terpstra Test}{]} & Tests for a monotonic trend in a response variable across ordered groups. & \begin{minipage}[t]{\linewidth}\raggedright
- Response variable is continuous or ordinal.\\
- Predictor variable is ordinal.\strut
\end{minipage} & Comparing medians or distributions across ordinal groups, e.g., treatment levels. \\
{[}\textbf{Mantel Test for Trend}{]} & Evaluates a linear association between an ordinal predictor and response. & \begin{minipage}[t]{\linewidth}\raggedright
- Ordinal variables.\\
- Linear trend expected.\strut
\end{minipage} & Determining trends in stratified or grouped data. \\
{[}\textbf{Chi-square Test for Linear Trend}{]} & Tests for linear trends in categorical data using contingency tables. & \begin{minipage}[t]{\linewidth}\raggedright
- Contingency table with ordinal predictor.\\
- Sufficient sample size (expected frequencies \textgreater{} 5).\strut
\end{minipage} & Analyzing trends in frequency data, e.g., examining disease prevalence by age groups. \\
\end{longtable}

\hypertarget{divergence-metrics-and-tests-for-comparing-distributions}{%
\section{Divergence Metrics and Tests for Comparing Distributions}\label{divergence-metrics-and-tests-for-comparing-distributions}}

Divergence metrics are powerful tools used to measure the similarity or dissimilarity between probability distributions. Unlike deviation and deviance statistics, divergence metrics focus on the broader relationships between entire distributions, rather than individual data points or specific model fit metrics. Let's clarify these differences:

\begin{itemize}
\tightlist
\item
  \textbf{Deviation Statistics}: Measure the difference between the realization of a variable and some reference value (e.g., the mean). Common statistics derived from deviations include:

  \begin{itemize}
  \tightlist
  \item
    Standard deviation
  \item
    Average absolute deviation
  \item
    Median absolute deviation
  \item
    Maximum absolute deviation
  \end{itemize}
\item
  \textbf{Deviance Statistics}: Assess the goodness-of-fit of statistical models. These are analogous to the sum of squared residuals in ordinary least squares (OLS) but are generalized for use in cases with maximum likelihood estimation (MLE). Deviance statistics are frequently employed in generalized linear models (GLMs).
\end{itemize}

Divergence statistics differ fundamentally by focusing on statistical distances between entire probability distributions, rather than on individual data points or model errors.

\textbf{1. Divergence Metrics}

\begin{itemize}
\item
  \textbf{Definition}: Divergence metrics measure how much one probability distribution differs from another.
\item
  \textbf{Key Properties}:

  \begin{itemize}
  \item
    \textbf{Asymmetry}: Many divergence metrics, such as Kullback-Leibler (KL) divergence, are not symmetric (i.e., \(D(P \|\| Q) \neq D(Q \|\| P)\)).
  \item
    \textbf{Non-Metric}: They don't necessarily satisfy the properties of a metric (e.g., symmetry, triangle inequality).
  \item
    \textbf{Unitless}: Divergences are often expressed in terms of information (e.g., bits or nats).
  \end{itemize}
\item
  \textbf{When to Use}:

  \begin{itemize}
  \tightlist
  \item
    Use divergence metrics to assess the degree of mismatch between two probability distributions, especially in machine learning, statistical inference, or model evaluation.
  \end{itemize}
\end{itemize}

\textbf{2. Distance Metrics}

\begin{itemize}
\item
  \textbf{Definition}: Distance metrics measure the ``distance'' or dissimilarity between two objects, including probability distributions, datasets, or points in space.
\item
  \textbf{Key Properties}:

  \begin{itemize}
  \item
    \textbf{Symmetry}: \(D(P, Q) = D(Q, P)\).
  \item
    \textbf{Triangle Inequality}: \(D(P, R) \leq D(P, Q) + D(Q, R)\).
  \item
    \textbf{Non-Negativity}: \(D(P, Q) \geq 0\), with \(D(P, Q) = 0\) only if \(P=Q\).
  \end{itemize}
\item
  \textbf{When to Use}:

  \begin{itemize}
  \tightlist
  \item
    Use distance metrics to compare datasets, distributions, or clustering outcomes where symmetry and geometric properties are important.
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1955}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3534}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4436}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Divergence Metrics}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Distance Metrics}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Symmetry} & Often asymmetric (e.g., KL divergence). & Always symmetric (e.g., Wasserstein). \\
\textbf{Triangle Inequality} & Not satisfied. & Satisfied. \\
\textbf{Use Case} & Quantifying how different distributions are. & Measuring the dissimilarity or ``cost'' of transformation. \\
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Applications of Divergence Metrics}

Divergence metrics have found wide utility across domains, including:

\begin{itemize}
\tightlist
\item
  \textbf{Detecting Data Drift in Machine Learning}: Used to monitor whether the distribution of incoming data differs significantly from training data.
\item
  \textbf{Feature Selection}: Employed to identify features with the most distinguishing power by comparing their distributions across different classes.
\item
  \textbf{Variational Autoencoders (VAEs)}: Divergence metrics (such as Kullback-Leibler divergence) are central to the loss functions used in training VAEs.
\item
  \textbf{Reinforcement Learning}: Measure the similarity between policy distributions to improve decision-making processes.
\item
  \textbf{Assessing Consistency}: Compare the distributions of two variables representing constructs to test their relationship or agreement.
\end{itemize}

Divergence metrics are also highly relevant in business settings, providing insights and solutions for a variety of applications, such as:

\begin{itemize}
\item
  \textbf{Customer Segmentation and Targeting}: Compare the distributions of customer demographics or purchase behavior across market segments to identify key differences and target strategies more effectively.
\item
  \textbf{Market Basket Analysis}: Measure divergence between distributions of product co-purchases across regions or customer groups to optimize product bundling and cross-selling strategies.
\item
  \textbf{Marketing Campaign Effectiveness}: Evaluate whether the distribution of customer responses (e.g., click-through rates or conversions) differs significantly before and after a marketing campaign, providing insights into its success.
\item
  \textbf{Fraud Detection}: Monitor divergence in transaction patterns over time to detect anomalies that may indicate fraudulent activities.
\item
  \textbf{Supply Chain Optimization}: Compare demand distributions across time periods or regions to optimize inventory allocation and reduce stock-outs or overstocking.
\item
  \textbf{Pricing Strategy Evaluation}: Analyze the divergence between pricing and purchase distributions across products or customer segments to refine pricing models and improve profitability.
\item
  \textbf{Churn Prediction}: Compare distributions of engagement metrics (e.g., frequency of transactions or usage time) between customers likely to churn and those who stay, to design retention strategies.
\item
  \textbf{Financial Portfolio Analysis}: Assess divergence between the expected returns and actual performance distributions of different asset classes to adjust investment strategies.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{kolmogorov-smirnov-test-1}{%
\subsection{Kolmogorov-Smirnov Test}\label{kolmogorov-smirnov-test-1}}

The Kolmogorov-Smirnov (KS) test is a \textbf{non-parametric test} used to determine whether two distributions differ significantly or whether a sample distribution matches a reference distribution. It is applicable to \textbf{continuous distributions} and is widely used in hypothesis testing and model evaluation.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Mathematical Definition}

The KS statistic is defined as:

\[
D = \max |F_P(x) - F_Q(x)|
\]

Where:

\begin{itemize}
\item
  \(F_P(x)\) is the cumulative distribution function (CDF) of the first distribution (or sample).
\item
  \(F_Q(x)\) is the CDF of the second distribution (or theoretical reference distribution).
\item
  \(D\) measures the maximum vertical distance between the two CDFs.
\end{itemize}

\textbf{Hypotheses}

\begin{itemize}
\tightlist
\item
  \textbf{Null Hypothesis} (\(H_0\)): The empirical distribution follows a specified distribution (or the two samples are drawn from the same distribution).
\item
  \textbf{Alternative Hypothesis} (\(H_1\)): The empirical distribution does not follow the specified distribution (or the two samples are drawn from different distributions).
\end{itemize}

\textbf{Properties of the KS Statistic}

\begin{itemize}
\tightlist
\item
  \textbf{Range}: \[
  D \in [0, 1]
  \]

  \begin{itemize}
  \tightlist
  \item
    \(D = 0\): Perfect match between the distributions.
  \item
    \(D = 1\): Maximum dissimilarity between the distributions.
  \end{itemize}
\item
  \textbf{Non-parametric Nature}: The KS test makes no assumptions about the underlying distribution of the data.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The KS test is useful in various scenarios, including:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Comparing two empirical distributions to evaluate similarity.
\item
  Testing goodness-of-fit for a sample against a theoretical distribution.
\item
  Detecting data drift or shifts in distributions over time.
\item
  Validating simulation outputs by comparing them to real-world data.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Example 1: Continuous Distributions}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load necessary libraries}
\FunctionTok{library}\NormalTok{(stats)}

\CommentTok{\# Generate two sample distributions}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{sample\_1 }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{)        }\CommentTok{\# Sample from a standard normal distribution}
\NormalTok{sample\_2 }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\AttributeTok{mean =} \DecValTok{1}\NormalTok{)  }\CommentTok{\# Sample with mean shifted to 1}

\CommentTok{\# Perform Kolmogorov{-}Smirnov test}
\NormalTok{ks\_test\_result }\OtherTok{\textless{}{-}} \FunctionTok{ks.test}\NormalTok{(sample\_1, sample\_2)}
\FunctionTok{print}\NormalTok{(ks\_test\_result)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Asymptotic two{-}sample Kolmogorov{-}Smirnov test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  sample\_1 and sample\_2}
\CommentTok{\#\textgreater{} D = 0.36, p{-}value = 4.705e{-}06}
\CommentTok{\#\textgreater{} alternative hypothesis: two{-}sided}
\end{Highlighting}
\end{Shaded}

This compares the CDFs of the two samples. The p-value indicates whether the null hypothesis (that the samples come from the same distribution) can be rejected.

\textbf{Example 2: Discrete Data with Bootstrapped KS Test}

For discrete data, a bootstrapped version of the KS test is often used to bypass the continuity requirement.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(Matching)}

\CommentTok{\# Define two discrete samples}
\NormalTok{discrete\_sample\_1 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\SpecialCharTok{:}\DecValTok{10}\NormalTok{)}
\NormalTok{discrete\_sample\_2 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\SpecialCharTok{:}\DecValTok{10}\NormalTok{)}

\CommentTok{\# Perform bootstrapped KS test}
\NormalTok{ks\_boot\_result }\OtherTok{\textless{}{-}} \FunctionTok{ks.boot}\NormalTok{(}\AttributeTok{Tr =}\NormalTok{ discrete\_sample\_1, }\AttributeTok{Co =}\NormalTok{ discrete\_sample\_2)}
\FunctionTok{print}\NormalTok{(ks\_boot\_result)}
\CommentTok{\#\textgreater{} $ks.boot.pvalue}
\CommentTok{\#\textgreater{} [1] 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $ks}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Exact two{-}sample Kolmogorov{-}Smirnov test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  Tr and Co}
\CommentTok{\#\textgreater{} D = 0, p{-}value = 1}
\CommentTok{\#\textgreater{} alternative hypothesis: two{-}sided}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $nboots}
\CommentTok{\#\textgreater{} [1] 1000}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} attr(,"class")}
\CommentTok{\#\textgreater{} [1] "ks.boot"}
\end{Highlighting}
\end{Shaded}

This method performs a bootstrapped version of the KS test, suitable for discrete data. The p-value indicates whether the null hypothesis (that the samples come from the same distribution) can be rejected.

\textbf{Example 3: Comparing Multiple Distributions with KL Divergence (Optional Enhancement)}

If you wish to extend the analysis to include divergence measures like KL divergence, use the following:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(entropy)}
\FunctionTok{library}\NormalTok{(tidyverse)}

\CommentTok{\# Define multiple samples}
\NormalTok{lst }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{sample\_1 =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{20}\NormalTok{), }\AttributeTok{sample\_2 =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\SpecialCharTok{:}\DecValTok{30}\NormalTok{), }\AttributeTok{sample\_3 =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\SpecialCharTok{:}\DecValTok{30}\NormalTok{))}

\CommentTok{\# Compute KL divergence between all pairs of distributions}
\NormalTok{result }\OtherTok{\textless{}{-}} \FunctionTok{expand.grid}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(lst), }\DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(lst)) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{rowwise}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{KL =} \FunctionTok{KL.empirical}\NormalTok{(lst[[Var1]], lst[[Var2]]))}

\FunctionTok{print}\NormalTok{(result)}
\CommentTok{\#\textgreater{} \# A tibble: 9 x 3}
\CommentTok{\#\textgreater{} \# Rowwise: }
\CommentTok{\#\textgreater{}    Var1  Var2     KL}
\CommentTok{\#\textgreater{}   \textless{}int\textgreater{} \textless{}int\textgreater{}  \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1     1     1 0     }
\CommentTok{\#\textgreater{} 2     2     1 0.150 }
\CommentTok{\#\textgreater{} 3     3     1 0.183 }
\CommentTok{\#\textgreater{} 4     1     2 0.704 }
\CommentTok{\#\textgreater{} 5     2     2 0     }
\CommentTok{\#\textgreater{} 6     3     2 0.0679}
\CommentTok{\#\textgreater{} 7     1     3 0.622 }
\CommentTok{\#\textgreater{} 8     2     3 0.0870}
\CommentTok{\#\textgreater{} 9     3     3 0}
\end{Highlighting}
\end{Shaded}

This calculates the KL divergence for all pairs of distributions in the list, offering additional insights into the relationships between the distributions.

\hypertarget{anderson-darling-test-1}{%
\subsection{Anderson-Darling Test}\label{anderson-darling-test-1}}

The Anderson-Darling (AD) test is a \textbf{goodness-of-fit test} that evaluates whether a sample of data comes from a specific distribution. It is an enhancement of the \protect\hyperlink{kolmogorov-smirnov-test}{Kolmogorov-Smirnov test}, with greater sensitivity to deviations in the tails of the distribution.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The Anderson-Darling test statistic is defined as:

\[
A^2 = -n - \frac{1}{n} \sum_{i=1}^n \left[ (2i - 1) \left( \log F(Y_i) + \log(1 - F(Y_{n+1-i})) \right) \right]
\]

Where:

\begin{itemize}
\item
  \(n\) is the sample size.
\item
  \(F\) is the cumulative distribution function (CDF) of the theoretical distribution being tested.
\item
  \(Y_i\) are the ordered sample values.
\end{itemize}

The AD test modifies the basic framework of the KS test by giving more weight to the tails of the distribution, making it particularly sensitive to tail discrepancies.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Hypotheses}

\begin{itemize}
\tightlist
\item
  \textbf{Null Hypothesis (}\(H_0\)): The sample data follows the specified distribution.
\item
  \textbf{Alternative Hypothesis (}\(H_1\)): The sample data does not follow the specified distribution.
\end{itemize}

\textbf{Key Properties}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Tail Sensitivity}: Unlike the Kolmogorov-Smirnov test, the Anderson-Darling test emphasizes discrepancies in the tails of the distribution.
\item
  \textbf{Distribution-Specific Critical Values}: The AD test provides critical values tailored to the specific distribution being tested (e.g., normal, exponential).
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The Anderson-Darling test is commonly used in:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Testing goodness-of-fit for a sample against theoretical distributions such as normal, exponential, or uniform.
\item
  Evaluating the appropriateness of parametric models in hypothesis testing.
\item
  Assessing distributional assumptions in quality control and reliability analysis.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Example: Testing Normality with the Anderson-Darling Test}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(nortest)}

\CommentTok{\# Generate a sample from a normal distribution}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{sample\_data }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)}

\CommentTok{\# Perform the Anderson{-}Darling test for normality}
\NormalTok{ad\_test\_result }\OtherTok{\textless{}{-}} \FunctionTok{ad.test}\NormalTok{(sample\_data)}
\FunctionTok{print}\NormalTok{(ad\_test\_result)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Anderson{-}Darling normality test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  sample\_data}
\CommentTok{\#\textgreater{} A = 0.16021, p{-}value = 0.9471}
\end{Highlighting}
\end{Shaded}

If the p-value is below a chosen significance level (e.g., 0.05), the null hypothesis that the data is normally distributed is rejected.

\textbf{Example: Comparing Two Empirical Distributions}

The AD test can also be applied to compare two empirical distributions using resampling techniques.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Define two samples}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{sample\_1 }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)}
\NormalTok{sample\_2 }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\AttributeTok{mean =} \DecValTok{1}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)}

\CommentTok{\# Perform resampling{-}based Anderson{-}Darling test (custom implementation or packages like twosamples)}
\FunctionTok{library}\NormalTok{(twosamples)}
\NormalTok{ad\_test\_result\_empirical }\OtherTok{\textless{}{-}} \FunctionTok{ad\_test}\NormalTok{(sample\_1, sample\_2)}
\FunctionTok{print}\NormalTok{(ad\_test\_result\_empirical)}
\CommentTok{\#\textgreater{}  Test Stat    P{-}Value }
\CommentTok{\#\textgreater{} 6796.70454    0.00025}
\end{Highlighting}
\end{Shaded}

This evaluates whether the two empirical distributions differ significantly.

\hypertarget{chi-square-goodness-of-fit-test}{%
\subsection{Chi-Square Goodness-of-Fit Test}\label{chi-square-goodness-of-fit-test}}

The \textbf{Chi-Square Goodness-of-Fit Test} is a non-parametric statistical test used to evaluate whether a sample data set comes from a population with a specific distribution. It compares observed frequencies with expected frequencies under a hypothesized distribution.

\begin{itemize}
\tightlist
\item
  \textbf{Null Hypothesis (}\(H_0\)): The data follow the specified distribution.
\item
  \textbf{Alternative Hypothesis (}\(H_a\)): The data do not follow the specified distribution.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The Chi-Square test statistic is computed as:

\[
\chi^2 = \sum_{i=1}^k \frac{(O_i - E_i)^2}{E_i}
\]

Where:

\begin{itemize}
\item
  \(O_i\): Observed frequency for category \(i\).
\item
  \(E_i\): Expected frequency for category \(i\).
\item
  \(k\): Number of categories.
\end{itemize}

The test statistic follows a Chi-Square distribution with degrees of freedom:

\[
\nu = k - 1 - p
\]

Where \(p\) is the number of parameters estimated from the data.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Assumptions of the Test}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Random Sampling}: The sample data are drawn randomly from the population.
\item
  \textbf{Minimum Expected Frequency}: The expected frequencies \(E_i\) are sufficiently large (typically \(E_i \geq 5\)).
\item
  \textbf{Independence}: Observations in the sample are independent of each other.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Decision Rule

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute the test statistic \(\chi^2\) using the observed and expected frequencies.
\item
  Determine the critical value \(\chi^2_{\alpha, \nu}\) for the chosen significance level \(\alpha\) and degrees of freedom \(\nu\).
\item
  Compare \(\chi^2\) to \(\chi^2_{\alpha, \nu}\):

  \begin{itemize}
  \tightlist
  \item
    Reject \(H_0\) if \(\chi^2 > \chi^2_{\alpha, \nu}\).
  \item
    Alternatively, use the p-value approach:

    \begin{itemize}
    \tightlist
    \item
      Reject \(H_0\) if \(p \leq \alpha\).
    \item
      Fail to reject \(H_0\) if \(p > \alpha\).
    \end{itemize}
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Steps for the Chi-Square Goodness-of-Fit Test

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Define the expected frequencies based on the hypothesized distribution.
\item
  Compute the observed frequencies from the data.
\item
  Calculate the test statistic \(\chi^2\).
\item
  Determine the degrees of freedom \(\nu\).
\item
  Compare \(\chi^2\) with the critical value or use the p-value for decision-making.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Example: Testing a Fair Die}

Suppose you are testing whether a six-sided die is fair. The die is rolled 60 times, and the observed frequencies of the outcomes are:

\begin{itemize}
\tightlist
\item
  \textbf{Observed Frequencies}: \([10, 12, 8, 11, 9, 10]\)
\item
  \textbf{Expected Frequencies}: A fair die has equal probability for each face, so \(E_i = 60 / 6 = 10\) for each face.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Observed frequencies}
\NormalTok{observed }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{11}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{10}\NormalTok{)}

\CommentTok{\# Expected frequencies under a fair die}
\NormalTok{expected }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{6}\NormalTok{)}

\CommentTok{\# Perform Chi{-}Square Goodness{-}of{-}Fit Test}
\NormalTok{chisq\_test }\OtherTok{\textless{}{-}} \FunctionTok{chisq.test}\NormalTok{(}\AttributeTok{x =}\NormalTok{ observed, }\AttributeTok{p =}\NormalTok{ expected }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(expected))}

\CommentTok{\# Display results}
\NormalTok{chisq\_test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Chi{-}squared test for given probabilities}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  observed}
\CommentTok{\#\textgreater{} X{-}squared = 1, df = 5, p{-}value = 0.9626}
\end{Highlighting}
\end{Shaded}

\textbf{Example: Testing a Loaded Die}

For a die with unequal probabilities (e.g., a loaded die), the expected probabilities are defined explicitly:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Observed frequencies}
\NormalTok{observed }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{11}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{10}\NormalTok{)}

\CommentTok{\# Expected probabilities (e.g., for a loaded die)}
\NormalTok{probabilities }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\FloatTok{0.1}\NormalTok{)}

\CommentTok{\# Expected frequencies}
\NormalTok{expected }\OtherTok{\textless{}{-}}\NormalTok{ probabilities }\SpecialCharTok{*} \FunctionTok{sum}\NormalTok{(observed)}

\CommentTok{\# Perform Chi{-}Square Goodness{-}of{-}Fit Test}
\NormalTok{chisq\_test\_loaded }\OtherTok{\textless{}{-}} \FunctionTok{chisq.test}\NormalTok{(}\AttributeTok{x =}\NormalTok{ observed, }\AttributeTok{p =}\NormalTok{ probabilities)}

\CommentTok{\# Display results}
\NormalTok{chisq\_test\_loaded}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Chi{-}squared test for given probabilities}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  observed}
\CommentTok{\#\textgreater{} X{-}squared = 15.806, df = 5, p{-}value = 0.007422}
\end{Highlighting}
\end{Shaded}

Limitations of the Chi-Square Test

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Minimum Expected Frequency}: If \(E_i < 5\) for any category, the test may lose power. Consider merging categories to meet this criterion.
\item
  \textbf{Independence}: Assumes observations are independent. Violations of this assumption can invalidate the test.
\item
  \textbf{Sample Size Sensitivity}: Large sample sizes may result in significant \(\chi\^2\) values even for minor deviations from the expected distribution.
\end{enumerate}

The Chi-Square Goodness-of-Fit Test is a versatile tool for evaluating the fit of observed data to a hypothesized distribution, widely used in fields like quality control, genetics, and market research.

\hypertarget{cramuxe9r-von-mises-test}{%
\subsection{Cramr-von Mises Test}\label{cramuxe9r-von-mises-test}}

The \textbf{Cramr-von Mises (CvM) Test} is a goodness-of-fit test that evaluates whether a sample data set comes from a specified distribution. Similar to the \protect\hyperlink{kolmogorov-smirnov-test}{Kolmogorov-Smirnov Test} (KS) and \protect\hyperlink{anderson-darling-test}{Anderson-Darling Test} (AD), it assesses the discrepancy between the empirical and theoretical cumulative distribution functions (CDFs). However, the CvM test has \textbf{equal sensitivity across the entire distribution}, unlike the KS test (focused on the maximum difference) or the AD test (emphasizing the tails).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The Cramr-von Mises test statistic is defined as:

\[
W^2 = n \int_{-\infty}^{\infty} \left( F_n(x) - F(x) \right)^2 dF(x)
\]

Where:

\begin{itemize}
\item
  \(n\) is the sample size.
\item
  \(F_n(x)\) is the empirical cumulative distribution function (ECDF) of the sample.
\item
  \(F(x)\) is the CDF of the specified theoretical distribution.
\end{itemize}

For practical implementation, the test statistic is often computed as:

\[
W^2 = \sum_{i=1}^n \left[ F(X_i) - \frac{2i - 1}{2n} \right]^2 + \frac{1}{12n}
\]

Where \(X_i\) are the ordered sample values.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Hypotheses}

\begin{itemize}
\tightlist
\item
  \textbf{Null Hypothesis (}\(H_0\)): The sample data follow the specified distribution.
\item
  \textbf{Alternative Hypothesis (}\(H_a\)): The sample data do not follow the specified distribution.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Key Properties}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Equal Sensitivity}:

  \begin{itemize}
  \tightlist
  \item
    The CvM test gives equal weight to discrepancies across all parts of the distribution, unlike the AD test, which emphasizes the tails.
  \end{itemize}
\item
  \textbf{Non-parametric}:

  \begin{itemize}
  \tightlist
  \item
    The test makes no strong parametric assumptions about the data, aside from the specified distribution.
  \end{itemize}
\item
  \textbf{Complementary to KS and AD Tests}:

  \begin{itemize}
  \tightlist
  \item
    While the KS test focuses on the maximum distance between CDFs and the AD test emphasizes tails, the CvM test provides a balanced sensitivity across the entire range of the distribution.
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The Cramr-von Mises test is widely used in:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Goodness-of-Fit Testing}: Assessing whether data follow a specified theoretical distribution (e.g., normal, exponential).
\item
  \textbf{Model Validation}: Evaluating the fit of probabilistic models in statistical and machine learning contexts.
\item
  \textbf{Complementary Testing}: Used alongside KS and AD tests for a comprehensive analysis of distributional assumptions.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Example 1: Testing Normality}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(nortest)}

\CommentTok{\# Generate a sample from a normal distribution}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{sample\_data }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)}

\CommentTok{\# Perform the Cramr{-}von Mises test for normality}
\NormalTok{cvm\_test\_result }\OtherTok{\textless{}{-}} \FunctionTok{cvm.test}\NormalTok{(sample\_data)}
\FunctionTok{print}\NormalTok{(cvm\_test\_result)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Cramer{-}von Mises normality test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  sample\_data}
\CommentTok{\#\textgreater{} W = 0.026031, p{-}value = 0.8945}
\end{Highlighting}
\end{Shaded}

The test evaluates whether the sample data follow a normal distribution.

\textbf{Example 2: Goodness-of-Fit for Custom Distributions}

For distributions other than normal, you can use resampling techniques or custom implementations. Here's a pseudo-implementation for a custom theoretical distribution:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Custom ECDF and theoretical CDF comparison}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{sample\_data }\OtherTok{\textless{}{-}}
    \FunctionTok{rexp}\NormalTok{(}\DecValTok{100}\NormalTok{, }\AttributeTok{rate =} \DecValTok{1}\NormalTok{)  }\CommentTok{\# Sample from exponential distribution}
\NormalTok{theoretical\_cdf }\OtherTok{\textless{}{-}}
    \ControlFlowTok{function}\NormalTok{(x) \{}
        \FunctionTok{pexp}\NormalTok{(x, }\AttributeTok{rate =} \DecValTok{1}\NormalTok{)}
\NormalTok{    \}  }\CommentTok{\# Exponential CDF}

\CommentTok{\# Compute empirical CDF}
\NormalTok{empirical\_cdf }\OtherTok{\textless{}{-}} \FunctionTok{ecdf}\NormalTok{(sample\_data)}

\CommentTok{\# Compute CvM statistic}
\NormalTok{cvm\_statistic }\OtherTok{\textless{}{-}}
    \FunctionTok{sum}\NormalTok{((}\FunctionTok{empirical\_cdf}\NormalTok{(sample\_data) }\SpecialCharTok{{-}} \FunctionTok{theoretical\_cdf}\NormalTok{(sample\_data)) }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{) }\SpecialCharTok{/} \FunctionTok{length}\NormalTok{(sample\_data)}
\FunctionTok{print}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"Cramr{-}von Mises Statistic (Custom):"}\NormalTok{, }\FunctionTok{round}\NormalTok{(cvm\_statistic, }\DecValTok{4}\NormalTok{)))}
\CommentTok{\#\textgreater{} [1] "Cramr{-}von Mises Statistic (Custom): 0.0019"}
\end{Highlighting}
\end{Shaded}

This demonstrates a custom calculation of the CvM statistic for testing goodness-of-fit to an exponential distribution.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Normality Test}:

  \begin{itemize}
  \item
    The \texttt{cvm.test} function evaluates whether the sample data follow a normal distribution.
  \item
    A small p-value indicates significant deviation from normality.
  \end{itemize}
\item
  \textbf{Custom Goodness-of-Fit}:

  \begin{itemize}
  \item
    Custom implementation allows testing for distributions other than normal.
  \item
    The statistic measures the squared differences between the empirical and theoretical CDFs.
  \end{itemize}
\end{enumerate}

\textbf{Advantages and Limitations}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Advantages}:

  \begin{itemize}
  \item
    Balanced sensitivity across the entire distribution.
  \item
    Complements KS and AD tests by providing a different perspective on goodness-of-fit.
  \end{itemize}
\item
  \textbf{Limitations}:

  \begin{itemize}
  \item
    Critical values are distribution-specific.
  \item
    The test may be less sensitive to tail deviations compared to the AD test.
  \end{itemize}
\end{enumerate}

The Cramr-von Mises test is a robust and versatile goodness-of-fit test, offering balanced sensitivity across the entire distribution. Its complementarity to KS and AD tests makes it an essential tool for validating distributional assumptions in both theoretical and applied contexts.

\hypertarget{kullback-leibler-divergence}{%
\subsection{Kullback-Leibler Divergence}\label{kullback-leibler-divergence}}

Kullback-Leibler (KL) divergence, also known as \textbf{relative entropy}, is a measure used to quantify the similarity between two probability distributions. It plays a critical role in statistical inference, machine learning, and information theory. However, KL divergence is not a true metric as it does not satisfy the triangle inequality.

\textbf{Key Properties of KL Divergence}

\begin{itemize}
\item
  \textbf{Not a Metric}: KL divergence fails to meet the triangle inequality requirement, and it is not symmetric, meaning: \[
  D_{KL}(P \| Q) \neq D_{KL}(Q \| P)
  \]
\item
  \textbf{Generalization to Multivariate Case}: KL divergence can be extended for multivariate distributions, making it flexible for complex analyses.
\item
  \textbf{Quantifies Information Loss}: It measures the ``information loss'' when approximating the true distribution \(P\) with the predicted distribution \(Q\). Thus, smaller values indicate closer similarity between the distributions.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Mathematical Definitions}

KL divergence is defined differently for discrete and continuous distributions.

\textbf{1. Discrete Case}\\
For two discrete probability distributions \(P = \{P_i\}\) and \(Q = \{Q_i\}\), the KL divergence is given by: \[
D_{KL}(P \| Q) = \sum_i P_i \log\left(\frac{P_i}{Q_i}\right)
\]

\textbf{2. Continuous Case}\\
For continuous probability density functions \(P(x)\) and \(Q(x)\): \[
D_{KL}(P \| Q) = \int P(x) \log\left(\frac{P(x)}{Q(x)}\right) dx
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{itemize}
\tightlist
\item
  \textbf{Range}: \[
  D_{KL}(P \| Q) \in [0, \infty)
  \]

  \begin{itemize}
  \tightlist
  \item
    \(D_{KL} = 0\) indicates identical distributions (\(P = Q\)).
  \item
    Larger values indicate greater dissimilarity between \(P\) and \(Q\).
  \end{itemize}
\item
  \textbf{Non-Symmetric Nature}: As noted, \(D_{KL}(P \| Q)\) and \(D_{KL}(Q \| P)\) are not equal, emphasizing its directed nature.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(philentropy)}

\CommentTok{\# Example 1: Continuous case}
\CommentTok{\# Define two continuous probability distributions with distinct patterns}
\NormalTok{X\_continuous }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\FloatTok{0.4}\NormalTok{)  }\CommentTok{\# Normalized to sum to 1}
\NormalTok{Y\_continuous }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{0.4}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\FloatTok{0.1}\NormalTok{)  }\CommentTok{\# Normalized to sum to 1}

\CommentTok{\# Calculate KL divergence (logarithm base 2)}
\NormalTok{KL\_continuous }\OtherTok{\textless{}{-}} \FunctionTok{KL}\NormalTok{(}\FunctionTok{rbind}\NormalTok{(X\_continuous, Y\_continuous), }\AttributeTok{unit =} \StringTok{"log2"}\NormalTok{)}
\FunctionTok{print}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"KL divergence (continuous):"}\NormalTok{, }\FunctionTok{round}\NormalTok{(KL\_continuous, }\DecValTok{2}\NormalTok{)))}
\CommentTok{\#\textgreater{} [1] "KL divergence (continuous): 0.66"}

\CommentTok{\# Example 2: Discrete case}
\CommentTok{\# Define two discrete probability distributions}
\NormalTok{X\_discrete }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{20}\NormalTok{)  }\CommentTok{\# Counts for events}
\NormalTok{Y\_discrete }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{20}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{5}\NormalTok{)  }\CommentTok{\# Counts for events}

\CommentTok{\# Estimate probabilities empirically and compute KL divergence}
\NormalTok{KL\_discrete }\OtherTok{\textless{}{-}} \FunctionTok{KL}\NormalTok{(}\FunctionTok{rbind}\NormalTok{(X\_discrete, Y\_discrete), }\AttributeTok{est.prob =} \StringTok{"empirical"}\NormalTok{)}
\FunctionTok{print}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"KL divergence (discrete):"}\NormalTok{, }\FunctionTok{round}\NormalTok{(KL\_discrete, }\DecValTok{2}\NormalTok{)))}
\CommentTok{\#\textgreater{} [1] "KL divergence (discrete): 0.66"}
\end{Highlighting}
\end{Shaded}

Insights:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Continuous case uses normalized probability values explicitly provided.
\item
  Discrete case relies on empirical estimation of probabilities from counts.
\item
  Observe how KL divergence quantifies the ``distance'' between the two distributions.
\end{enumerate}

\hypertarget{jensen-shannon-divergence}{%
\subsection{Jensen-Shannon Divergence}\label{jensen-shannon-divergence}}

Jensen-Shannon (JS) divergence is a symmetric and bounded measure of the similarity between two probability distributions. It is derived from the \protect\hyperlink{kullback-leibler-divergence}{Kullback-Leibler Divergence} (KL) but addresses its asymmetry and unboundedness by incorporating a mixed distribution.

The Jensen-Shannon divergence is defined as: \[
D_{JS}(P \| Q) = \frac{1}{2} \left( D_{KL}(P \| M) + D_{KL}(Q \| M) \right)
\] where:

\begin{itemize}
\item
  \(M = \frac{1}{2}(P + Q)\) is the \textbf{mixed distribution}, representing the average of \(P\) and \(Q\).
\item
  \(D_{KL}\) is the Kullback-Leibler divergence.
\end{itemize}

\textbf{Key Properties}

\begin{itemize}
\item
  \textbf{Symmetry}: Unlike KL divergence, JS divergence is symmetric: \[
  D_{JS}(P \| Q) = D_{JS}(Q \| P)
  \]
\item
  \textbf{Boundedness}:

  \begin{itemize}
  \tightlist
  \item
    For base-2 logarithms: \[
    D_{JS} \in [0, 1]
    \]
  \item
    For natural logarithms (base-\(e\)): \[
    D_{JS} \in [0, \ln(2)]
    \]
  \end{itemize}
\item
  \textbf{Interpretability}: The JS divergence measures the average information gain when moving from the mixed distribution \(M\) to either \(P\) or \(Q\). Its bounded nature makes it easier to compare across datasets.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load the required library}
\FunctionTok{library}\NormalTok{(philentropy)}

\CommentTok{\# Example 1: Continuous case}
\CommentTok{\# Define two continuous distributions}
\NormalTok{X\_continuous }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10}  \CommentTok{\# Continuous sequence}
\NormalTok{Y\_continuous }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\DecValTok{20}  \CommentTok{\# Continuous sequence}

\CommentTok{\# Compute JS divergence (logarithm base 2)}
\NormalTok{JS\_continuous }\OtherTok{\textless{}{-}} \FunctionTok{JSD}\NormalTok{(}\FunctionTok{rbind}\NormalTok{(X\_continuous, Y\_continuous), }\AttributeTok{unit =} \StringTok{"log2"}\NormalTok{)}
\FunctionTok{print}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"JS divergence (continuous):"}\NormalTok{, }\FunctionTok{round}\NormalTok{(JS\_continuous, }\DecValTok{2}\NormalTok{)))}
\CommentTok{\#\textgreater{} [1] "JS divergence (continuous): 20.03"}

\CommentTok{\# X\_continuous and Y\_continuous represent continuous distributions.}
\CommentTok{\# The mixed distribution (M) is computed internally as the average of the two distributions.}

\CommentTok{\# Example 2: Discrete case}
\CommentTok{\# Define two discrete distributions}
\NormalTok{X\_discrete }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{20}\NormalTok{)  }\CommentTok{\# Observed counts for events}
\NormalTok{Y\_discrete }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{20}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{5}\NormalTok{)  }\CommentTok{\# Observed counts for events}

\CommentTok{\# Compute JS divergence with empirical probability estimation}
\NormalTok{JS\_discrete }\OtherTok{\textless{}{-}} \FunctionTok{JSD}\NormalTok{(}\FunctionTok{rbind}\NormalTok{(X\_discrete, Y\_discrete), }\AttributeTok{est.prob =} \StringTok{"empirical"}\NormalTok{)}
\FunctionTok{print}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"JS divergence (discrete):"}\NormalTok{, }\FunctionTok{round}\NormalTok{(JS\_discrete, }\DecValTok{2}\NormalTok{)))}
\CommentTok{\#\textgreater{} [1] "JS divergence (discrete): 0.15"}

\CommentTok{\# X\_discrete and Y\_discrete represent event counts.}
\CommentTok{\# Probabilities are estimated empirically before calculating the divergence.}
\end{Highlighting}
\end{Shaded}

\hypertarget{hellinger-distance}{%
\subsection{Hellinger Distance}\label{hellinger-distance}}

The Hellinger distance is a bounded and symmetric measure of similarity between two probability distributions. It is widely used in statistics and machine learning to quantify how ``close'' two distributions are, with values ranging between 0 (identical distributions) and 1 (completely disjoint distributions).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Mathematical Definition}

The Hellinger distance between two probability distributions \(P\) and \(Q\) is defined as:

\[
H(P, Q) = \frac{1}{\sqrt{2}} \sqrt{\sum_x \left(\sqrt{P(x)} - \sqrt{Q(x)}\right)^2}
\]

Where:

\begin{itemize}
\item
  \(P(x)\) and \(Q(x)\) are the probability densities or probabilities at point \(x\) for the distributions \(P\) and \(Q\).
\item
  The term \(\sqrt{P(x)}\) is the square root of the probabilities, emphasizing geometric comparisons between the distributions.
\end{itemize}

Alternatively, for continuous distributions, the Hellinger distance can be expressed as:

\[
H(P, Q) = \frac{1}{\sqrt{2}} \sqrt{\int \left(\sqrt{P(x)} - \sqrt{Q(x)}\right)^2 dx}
\]

\textbf{Key Properties}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Symmetry}: \[
  H(P, Q) = H(Q, P)
  \] The distance is symmetric, unlike Kullback-Leibler divergence.
\item
  \textbf{Boundedness}: \[
  H(P, Q) \in [0, 1]
  \]

  \begin{itemize}
  \tightlist
  \item
    \(H = 0\): The distributions are identical (\(P(x) = Q(x)\) for all \(x\)).
  \item
    \(H = 1\): The distributions have no overlap (\(P(x) \neq Q(x)\)).
  \end{itemize}
\item
  \textbf{Interpretability}:

  \begin{itemize}
  \tightlist
  \item
    Hellinger distance provides a scale-invariant measure, making it suitable for comparing distributions in various contexts.
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Hellinger distance is widely used in:

\begin{itemize}
\tightlist
\item
  \textbf{Hypothesis Testing}: Comparing empirical distributions to theoretical models.
\item
  \textbf{Machine Learning}: Feature selection, classification, and clustering tasks.
\item
  \textbf{Bayesian Analysis}: Quantifying differences between prior and posterior distributions.
\item
  \textbf{Economics and Ecology}: Measuring dissimilarity in distributions like income, species abundance, or geographical data.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(philentropy)}

\CommentTok{\# Example 1: Compute Hellinger Distance for Discrete Distributions}
\CommentTok{\# Define two discrete distributions as probabilities}
\NormalTok{P\_discrete }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\FloatTok{0.4}\NormalTok{)  }\CommentTok{\# Normalized probabilities}
\NormalTok{Q\_discrete }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{0.3}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\FloatTok{0.2}\NormalTok{)  }\CommentTok{\# Normalized probabilities}

\CommentTok{\# Compute Hellinger distance}
\NormalTok{hellinger\_discrete }\OtherTok{\textless{}{-}} \FunctionTok{distance}\NormalTok{(}\FunctionTok{rbind}\NormalTok{(P\_discrete, Q\_discrete), }\AttributeTok{method =} \StringTok{"hellinger"}\NormalTok{)}
\FunctionTok{print}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"Hellinger Distance (Discrete):"}\NormalTok{, }\FunctionTok{round}\NormalTok{(hellinger\_discrete, }\DecValTok{4}\NormalTok{)))}
\CommentTok{\#\textgreater{} [1] "Hellinger Distance (Discrete): 0.465"}

\CommentTok{\# Example 2: Compute Hellinger Distance for Empirical Distributions}
\CommentTok{\# Define two empirical distributions (counts)}
\NormalTok{P\_empirical }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{40}\NormalTok{)  }\CommentTok{\# Counts for distribution P}
\NormalTok{Q\_empirical }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{30}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{20}\NormalTok{)  }\CommentTok{\# Counts for distribution Q}

\CommentTok{\# Normalize counts to probabilities}
\NormalTok{P\_normalized }\OtherTok{\textless{}{-}}\NormalTok{ P\_empirical }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(P\_empirical)}
\NormalTok{Q\_normalized }\OtherTok{\textless{}{-}}\NormalTok{ Q\_empirical }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(Q\_empirical)}

\CommentTok{\# Compute Hellinger distance}
\NormalTok{hellinger\_empirical }\OtherTok{\textless{}{-}} \FunctionTok{distance}\NormalTok{(}\FunctionTok{rbind}\NormalTok{(P\_normalized, Q\_normalized), }\AttributeTok{method =} \StringTok{"hellinger"}\NormalTok{)}
\FunctionTok{print}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"Hellinger Distance (Empirical):"}\NormalTok{, }\FunctionTok{round}\NormalTok{(hellinger\_empirical, }\DecValTok{4}\NormalTok{)))}
\CommentTok{\#\textgreater{} [1] "Hellinger Distance (Empirical): 0.465"}
\end{Highlighting}
\end{Shaded}

\hypertarget{bhattacharyya-distance}{%
\subsection{Bhattacharyya Distance}\label{bhattacharyya-distance}}

The \textbf{Bhattacharyya Distance} is a statistical measure used to quantify the similarity or overlap between two probability distributions. It is commonly used in pattern recognition, signal processing, and statistics to evaluate how closely related two distributions are. The Bhattacharyya distance is particularly effective for comparing both discrete and continuous distributions.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The Bhattacharyya distance between two probability distributions \(P\) and \(Q\) is defined as:

\[
D_B(P, Q) = -\ln \left( \sum_x \sqrt{P(x) Q(x)} \right)
\]

For continuous distributions, the Bhattacharyya distance is expressed as:

\[
D_B(P, Q) = -\ln \left( \int \sqrt{P(x) Q(x)} dx \right)
\]

Where:

\begin{itemize}
\item
  \(P(x)\) and \(Q(x)\) are the probability densities or probabilities for the distributions \(P\) and \(Q\).
\item
  The term \(\int \sqrt{P(x) Q(x)} dx\) is known as the \textbf{Bhattacharyya coefficient}.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Key Properties}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Symmetry}: \[
  D_B(P, Q) = D_B(Q, P)
  \]
\item
  \textbf{Range}: \[
  D_B(P, Q) \in [0, \infty)
  \]

  \begin{itemize}
  \tightlist
  \item
    \(D_B = 0\): The distributions are identical (\(P = Q\)).
  \item
    Larger values indicate less overlap and greater dissimilarity between \(P\) and \(Q\).
  \end{itemize}
\item
  \textbf{Relation to Hellinger Distance}:

  \begin{itemize}
  \tightlist
  \item
    The Bhattacharyya coefficient is related to the Hellinger distance: \[
    H(P, Q) = \sqrt{1 - \sum_x \sqrt{P(x) Q(x)}}
    \]
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The Bhattacharyya distance is widely used in:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Classification}: Measuring the similarity between feature distributions in machine learning.
\item
  \textbf{Hypothesis Testing}: Evaluating the closeness of observed data to a theoretical model.
\item
  \textbf{Image Processing}: Comparing pixel intensity distributions or color histograms.
\item
  \textbf{Economics and Ecology}: Assessing similarity in income distributions or species abundance.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Example 1: Discrete Distributions}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Define two discrete probability distributions}
\NormalTok{P\_discrete }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\FloatTok{0.4}\NormalTok{)  }\CommentTok{\# Normalized probabilities}
\NormalTok{Q\_discrete }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{0.3}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\FloatTok{0.2}\NormalTok{)  }\CommentTok{\# Normalized probabilities}

\CommentTok{\# Compute Bhattacharyya coefficient}
\NormalTok{bhattacharyya\_coefficient }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(P\_discrete }\SpecialCharTok{*}\NormalTok{ Q\_discrete))}

\CommentTok{\# Compute Bhattacharyya distance}
\NormalTok{bhattacharyya\_distance }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\FunctionTok{log}\NormalTok{(bhattacharyya\_coefficient)}

\CommentTok{\# Display results}
\FunctionTok{print}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}
    \StringTok{"Bhattacharyya Coefficient:"}\NormalTok{,}
    \FunctionTok{round}\NormalTok{(bhattacharyya\_coefficient, }\DecValTok{4}\NormalTok{)}
\NormalTok{))}
\CommentTok{\#\textgreater{} [1] "Bhattacharyya Coefficient: 0.9459"}
\FunctionTok{print}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}
    \StringTok{"Bhattacharyya Distance (Discrete):"}\NormalTok{,}
    \FunctionTok{round}\NormalTok{(bhattacharyya\_distance, }\DecValTok{4}\NormalTok{)}
\NormalTok{))}
\CommentTok{\#\textgreater{} [1] "Bhattacharyya Distance (Discrete): 0.0556"}
\end{Highlighting}
\end{Shaded}

A smaller Bhattacharyya distance indicates greater similarity between the two distributions.

\textbf{Example 2: Continuous Distributions (Approximation)}

For continuous distributions, the Bhattacharyya distance can be approximated using numerical integration or discretization.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Generate two continuous distributions}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{P\_continuous }\OtherTok{\textless{}{-}}
    \FunctionTok{rnorm}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)  }\CommentTok{\# Standard normal distribution}
\NormalTok{Q\_continuous }\OtherTok{\textless{}{-}}
    \FunctionTok{rnorm}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\AttributeTok{mean =} \DecValTok{1}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)  }\CommentTok{\# Normal distribution with mean 1}

\CommentTok{\# Create histograms to approximate probabilities}
\NormalTok{hist\_P }\OtherTok{\textless{}{-}} \FunctionTok{hist}\NormalTok{(P\_continuous, }\AttributeTok{breaks =} \DecValTok{50}\NormalTok{, }\AttributeTok{plot =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{hist\_Q }\OtherTok{\textless{}{-}} \FunctionTok{hist}\NormalTok{(Q\_continuous, }\AttributeTok{breaks =} \DecValTok{50}\NormalTok{, }\AttributeTok{plot =} \ConstantTok{FALSE}\NormalTok{)}

\CommentTok{\# Normalize histograms to probabilities}
\NormalTok{prob\_P }\OtherTok{\textless{}{-}}\NormalTok{ hist\_P}\SpecialCharTok{$}\NormalTok{counts }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(hist\_P}\SpecialCharTok{$}\NormalTok{counts)}
\NormalTok{prob\_Q }\OtherTok{\textless{}{-}}\NormalTok{ hist\_Q}\SpecialCharTok{$}\NormalTok{counts }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(hist\_Q}\SpecialCharTok{$}\NormalTok{counts)}

\CommentTok{\# Compute Bhattacharyya coefficient}
\NormalTok{bhattacharyya\_coefficient\_continuous }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(prob\_P }\SpecialCharTok{*}\NormalTok{ prob\_Q))}

\CommentTok{\# Compute Bhattacharyya distance}
\NormalTok{bhattacharyya\_distance\_continuous }\OtherTok{\textless{}{-}}
    \SpecialCharTok{{-}}\FunctionTok{log}\NormalTok{(bhattacharyya\_coefficient\_continuous)}

\CommentTok{\# Display results}
\FunctionTok{print}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}
    \StringTok{"Bhattacharyya Coefficient (Continuous):"}\NormalTok{,}
    \FunctionTok{round}\NormalTok{(bhattacharyya\_coefficient\_continuous, }\DecValTok{4}\NormalTok{)}
\NormalTok{))}
\CommentTok{\#\textgreater{} [1] "Bhattacharyya Coefficient (Continuous): 0.9823"}
\FunctionTok{print}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}
    \StringTok{"Bhattacharyya Distance (Continuous Approximation):"}\NormalTok{,}
    \FunctionTok{round}\NormalTok{(bhattacharyya\_distance\_continuous, }\DecValTok{4}\NormalTok{)}
\NormalTok{))}
\CommentTok{\#\textgreater{} [1] "Bhattacharyya Distance (Continuous Approximation): 0.0178"}
\end{Highlighting}
\end{Shaded}

Continuous distributions are discretized into histograms to compute the Bhattacharyya coefficient and distance.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Discrete Case:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    The Bhattacharyya coefficient quantifies the overlap between \(P\) and \(Q\).
  \item
    The Bhattacharyya distance translates this overlap into a logarithmic measure of dissimilarity.
  \end{enumerate}
\item
  Continuous Case:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Distributions are discretized into histograms to approximate the Bhattacharyya coefficient and distance.
  \end{enumerate}
\end{enumerate}

\hypertarget{wasserstein-distance}{%
\subsection{Wasserstein Distance}\label{wasserstein-distance}}

The Wasserstein distance, also known as the \textbf{Earth Mover's Distance (EMD)}, is a measure of similarity between two probability distributions. It quantifies the ``cost'' of transforming one distribution into another, making it particularly suitable for continuous data and applications where the geometry of the data matters.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Mathematical Definition}

The Wasserstein distance between two probability distributions \(P\) and \(Q\) over a domain \(\mathcal{X}\) is defined as:

\[
W_p(P, Q) = \left( \int_{\mathcal{X}} |F_P(x) - F_Q(x)|^p dx \right)^{\frac{1}{p}}
\]

Where:

\begin{itemize}
\item
  \(F_P(x)\) and \(F_Q(x)\) are the cumulative distribution functions (CDFs) of \(P\) and \(Q\).
\item
  \(p \geq 1\) is the order of the Wasserstein distance (commonly \(p = 1\)).
\item
  \(|\cdot|^p\) is the absolute difference raised to the power \(p\).
\end{itemize}

For the case of \(p = 1\), the formula simplifies to:

\[
W_1(P, Q) = \int_{\mathcal{X}} |F_P(x) - F_Q(x)| dx
\]

This represents the minimum ``cost'' of transforming the distribution \(P\) into \(Q\), where cost is proportional to the distance a ``unit of mass'' must move.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Key Properties}

\begin{itemize}
\tightlist
\item
  \textbf{Interpretability}: Represents the ``effort'' required to morph one distribution into another.
\item
  \textbf{Metric}: Wasserstein distance satisfies the properties of a metric, including symmetry, non-negativity, and the triangle inequality.
\item
  \textbf{Flexibility}: Can handle both empirical and continuous distributions.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Wasserstein distance is widely used in various fields, including:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Machine Learning}:

  \begin{itemize}
  \tightlist
  \item
    Training generative models such as Wasserstein GANs.
  \item
    Monitoring data drift in online systems.
  \end{itemize}
\item
  \textbf{Statistics}:

  \begin{itemize}
  \tightlist
  \item
    Comparing empirical distributions derived from observed data.
  \item
    Robustness testing under distributional shifts.
  \end{itemize}
\item
  \textbf{Economics}:

  \begin{itemize}
  \tightlist
  \item
    Quantifying disparities in income or wealth distributions.
  \end{itemize}
\item
  \textbf{Image Processing}:

  \begin{itemize}
  \tightlist
  \item
    Measuring structural differences between image distributions.
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(transport)}
\FunctionTok{library}\NormalTok{(twosamples)}

\CommentTok{\# Example 1: Compute Wasserstein Distance (1D case)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{dist\_1 }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{)               }\CommentTok{\# Generate a sample from a standard normal distribution}
\NormalTok{dist\_2 }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\AttributeTok{mean =} \DecValTok{1}\NormalTok{)     }\CommentTok{\# Generate a sample with mean shifted to 1}

\CommentTok{\# Calculate the Wasserstein distance}
\NormalTok{wass\_distance }\OtherTok{\textless{}{-}} \FunctionTok{wasserstein1d}\NormalTok{(dist\_1, dist\_2)}
\FunctionTok{print}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"1D Wasserstein Distance:"}\NormalTok{, }\FunctionTok{round}\NormalTok{(wass\_distance, }\DecValTok{4}\NormalTok{)))}
\CommentTok{\#\textgreater{} [1] "1D Wasserstein Distance: 0.8533"}

\CommentTok{\# Example 2: Wasserstein Metric as a Statistic}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{wass\_stat\_value }\OtherTok{\textless{}{-}} \FunctionTok{wass\_stat}\NormalTok{(dist\_1, dist\_2)}
\FunctionTok{print}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"Wasserstein Statistic:"}\NormalTok{, }\FunctionTok{round}\NormalTok{(wass\_stat\_value, }\DecValTok{4}\NormalTok{)))}
\CommentTok{\#\textgreater{} [1] "Wasserstein Statistic: 0.8533"}

\CommentTok{\# Example 3: Wasserstein Test (Permutation{-}based Two{-}sample Test)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{wass\_test\_result }\OtherTok{\textless{}{-}} \FunctionTok{wass\_test}\NormalTok{(dist\_1, dist\_2)}
\FunctionTok{print}\NormalTok{(wass\_test\_result)}
\CommentTok{\#\textgreater{} Test Stat   P{-}Value }
\CommentTok{\#\textgreater{} 0.8533046 0.0002500}

\CommentTok{\# {-} Example 1 calculates the simple Wasserstein distance between two distributions.}
\CommentTok{\# {-} Example 2 computes the Wasserstein distance as a statistical metric.}
\CommentTok{\# {-} Example 3 performs a permutation{-}based two{-}sample test using the Wasserstein metric.}
\end{Highlighting}
\end{Shaded}

\hypertarget{energy-distance}{%
\subsection{Energy Distance}\label{energy-distance}}

The \textbf{Energy Distance} is a statistical metric used to quantify the similarity between two probability distributions. It is particularly effective for comparing multi-dimensional distributions.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The Energy Distance between two distributions \(P\) and \(Q\) is defined as:

\[
E(P, Q) = 2 \mathbb{E}[||X - Y||] - \mathbb{E}[||X - X'||] - \mathbb{E}[||Y - Y'||]
\]

Where:

\begin{itemize}
\item
  \(X\) and \(X'\) are independent and identically distributed (i.i.d.) random variables from \(P\).
\item
  \(Y\) and \(Y'\) are i.i.d. random variables from \(Q\).
\item
  \(||\cdot||\) denotes the Euclidean distance.
\end{itemize}

Alternatively, for empirical distributions, the Energy Distance can be approximated as:

\[
E(P, Q) = \frac{2}{mn} \sum_{i=1}^m \sum_{j=1}^n ||X_i - Y_j|| - \frac{1}{m^2} \sum_{i=1}^m \sum_{j=1}^m ||X_i - X_j|| - \frac{1}{n^2} \sum_{i=1}^n \sum_{j=1}^n ||Y_i - Y_j||
\]

Where:

\begin{itemize}
\item
  \(m\) and \(n\) are the sample sizes from distributions \(P\) and \(Q\) respectively.
\item
  \(X_i\) and \(Y_j\) are samples from \(P\) and \(Q\).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Key Properties}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Metric}:

  \begin{itemize}
  \tightlist
  \item
    Energy distance satisfies the properties of a metric: symmetry, non-negativity, and the triangle inequality.
  \end{itemize}
\item
  \textbf{Range}: \[
  E(P, Q) \geq 0
  \]

  \begin{itemize}
  \tightlist
  \item
    \(E(P, Q) = 0\): The distributions are identical.
  \item
    Larger values indicate greater dissimilarity.
  \end{itemize}
\item
  \textbf{Effectiveness for Multi-dimensional Data}:

  \begin{itemize}
  \tightlist
  \item
    Energy distance is designed to work well in higher-dimensional spaces, unlike some traditional metrics.
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The Energy Distance is widely used in:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Hypothesis Testing}: Testing whether two distributions are the same.
\item
  \textbf{Energy Test} for equality of distributions.
\item
  \textbf{Clustering}: Measuring dissimilarity between clusters in multi-dimensional data.
\item
  \textbf{Feature Selection}: Comparing distributions of features across different classes to evaluate their discriminative power.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Example 1: Comparing Two Distributions}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load the \textquotesingle{}energy\textquotesingle{} package}
\FunctionTok{library}\NormalTok{(energy)}

\CommentTok{\# Generate two sample distributions}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{X }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{), }\AttributeTok{ncol =} \DecValTok{2}\NormalTok{)  }\CommentTok{\# Distribution P}
\NormalTok{Y }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\AttributeTok{mean =} \DecValTok{1}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{), }\AttributeTok{ncol =} \DecValTok{2}\NormalTok{)  }\CommentTok{\# Distribution Q}

\CommentTok{\# Combine X and Y and create a group identifier}
\NormalTok{combined }\OtherTok{\textless{}{-}} \FunctionTok{rbind}\NormalTok{(X, Y)}
\NormalTok{groups }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FunctionTok{nrow}\NormalTok{(X)), }\FunctionTok{rep}\NormalTok{(}\DecValTok{2}\NormalTok{, }\FunctionTok{nrow}\NormalTok{(Y)))}

\CommentTok{\# Compute Energy Distance}
\NormalTok{energy\_dist }\OtherTok{\textless{}{-}} \FunctionTok{edist}\NormalTok{(combined, }\AttributeTok{sizes =} \FunctionTok{table}\NormalTok{(groups))}

\CommentTok{\# Print the Energy Distance}
\FunctionTok{print}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"Energy Distance:"}\NormalTok{, }\FunctionTok{round}\NormalTok{(energy\_dist, }\DecValTok{4}\NormalTok{)))}
\CommentTok{\#\textgreater{} [1] "Energy Distance: 201.9202"}
\end{Highlighting}
\end{Shaded}

This calculates the energy distance between two multi-dimensional distributions.

Example 2: Energy Test for Equality of Distributions

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Perform the Energy Test}
\NormalTok{energy\_test }\OtherTok{\textless{}{-}}
    \FunctionTok{eqdist.etest}\NormalTok{(}\FunctionTok{rbind}\NormalTok{(X, Y), }\AttributeTok{sizes =} \FunctionTok{c}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(X), }\FunctionTok{nrow}\NormalTok{(Y)), }\AttributeTok{R =} \DecValTok{999}\NormalTok{)}
\FunctionTok{print}\NormalTok{(energy\_test)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Multivariate 2{-}sample E{-}test of equal distributions}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  sample sizes 500 500, replicates 999}
\CommentTok{\#\textgreater{} E{-}statistic = 201.92, p{-}value = 0.001}
\end{Highlighting}
\end{Shaded}

The energy test evaluates the null hypothesis that the two distributions are identical.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Energy Distance}:

  \begin{itemize}
  \tightlist
  \item
    Provides a single metric to quantify the dissimilarity between two distributions, considering all dimensions of the data.
  \end{itemize}
\item
  \textbf{Energy Test}:

  \begin{itemize}
  \item
    Tests for equality of distributions using Energy Distance.
  \item
    The p-value indicates whether the distributions are significantly different.
  \end{itemize}
\end{enumerate}

Advantages of Energy Distance

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Multi-dimensional Applicability}:

  \begin{itemize}
  \tightlist
  \item
    Works seamlessly with high-dimensional data, unlike some divergence metrics which may suffer from dimensionality issues.
  \end{itemize}
\item
  \textbf{Non-parametric}:

  \begin{itemize}
  \tightlist
  \item
    Makes no assumptions about the form of the distributions.
  \end{itemize}
\item
  \textbf{Robustness}:

  \begin{itemize}
  \tightlist
  \item
    Effective even with complex data structures.
  \end{itemize}
\end{enumerate}

\hypertarget{total-variation-distance}{%
\subsection{Total Variation Distance}\label{total-variation-distance}}

The \textbf{Total Variation (TV) Distance} is a measure of the maximum difference between two probability distributions. It is widely used in probability theory, statistics, and machine learning to quantify how dissimilar two distributions are.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The Total Variation Distance between two probability distributions \(P\) and \(Q\) is defined as:

\[
D_{TV}(P, Q) = \frac{1}{2} \sum_x |P(x) - Q(x)|
\]

Where:

\begin{itemize}
\item
  \(P(x)\) and \(Q(x)\) are the probabilities assigned to the outcome \(x\) by the distributions \(P\) and \(Q\).
\item
  The factor \(\frac{1}{2}\) ensures that the distance lies within the range \([0, 1]\).
\end{itemize}

Alternatively, for continuous distributions, the TV distance can be expressed as:

\[
D_{TV}(P, Q) = \frac{1}{2} \int |P(x) - Q(x)| dx
\]

\textbf{Key Properties}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Range}: \[
  D_{TV}(P, Q) \in [0, 1]
  \]

  \begin{itemize}
  \tightlist
  \item
    \(D_{TV} = 0\): The distributions are identical (\(P = Q\)).
  \item
    \(D_{TV} = 1\): The distributions are completely disjoint (no overlap).
  \end{itemize}
\item
  \textbf{Symmetry}: \[
  D_{TV}(P, Q) = D_{TV}(Q, P)
  \]
\item
  \textbf{Interpretability}:

  \begin{itemize}
  \tightlist
  \item
    \(D_{TV}(P, Q)\) represents the maximum probability mass that needs to be shifted to transform \(P\) into \(Q\).
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The Total Variation Distance is used in:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Hypothesis Testing}: Quantifying the difference between observed and expected distributions.
\item
  \textbf{Machine Learning}: Evaluating similarity between predicted and true distributions.
\item
  \textbf{Information Theory}: Comparing distributions in contexts like communication and cryptography.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Example 1: Discrete Distributions}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Define two discrete probability distributions}
\NormalTok{P\_discrete }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\FloatTok{0.4}\NormalTok{)  }\CommentTok{\# Normalized probabilities}
\NormalTok{Q\_discrete }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{0.3}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\FloatTok{0.2}\NormalTok{)  }\CommentTok{\# Normalized probabilities}

\CommentTok{\# Compute Total Variation Distance}
\NormalTok{tv\_distance }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(}\FunctionTok{abs}\NormalTok{(P\_discrete }\SpecialCharTok{{-}}\NormalTok{ Q\_discrete)) }\SpecialCharTok{/} \DecValTok{2}
\FunctionTok{print}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"Total Variation Distance (Discrete):"}\NormalTok{, }\FunctionTok{round}\NormalTok{(tv\_distance, }\DecValTok{4}\NormalTok{)))}
\CommentTok{\#\textgreater{} [1] "Total Variation Distance (Discrete): 0.3"}
\end{Highlighting}
\end{Shaded}

This calculates the maximum difference between the two distributions, scaled to lie between 0 and 1.

\textbf{Example 2: Continuous Distributions (Approximation)}

For continuous distributions, the TV distance can be approximated using discretization or numerical integration. Here's an example using random samples:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Generate two continuous distributions}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{P\_continuous }\OtherTok{\textless{}{-}}
    \FunctionTok{rnorm}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)  }\CommentTok{\# Standard normal distribution}
\NormalTok{Q\_continuous }\OtherTok{\textless{}{-}}
    \FunctionTok{rnorm}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\AttributeTok{mean =} \DecValTok{1}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)  }\CommentTok{\# Normal distribution with mean 1}

\CommentTok{\# Create histograms to approximate probabilities}
\NormalTok{hist\_P }\OtherTok{\textless{}{-}} \FunctionTok{hist}\NormalTok{(P\_continuous, }\AttributeTok{breaks =} \DecValTok{50}\NormalTok{, }\AttributeTok{plot =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{hist\_Q }\OtherTok{\textless{}{-}} \FunctionTok{hist}\NormalTok{(Q\_continuous, }\AttributeTok{breaks =} \DecValTok{50}\NormalTok{, }\AttributeTok{plot =} \ConstantTok{FALSE}\NormalTok{)}

\CommentTok{\# Normalize histograms to probabilities}
\NormalTok{prob\_P }\OtherTok{\textless{}{-}}\NormalTok{ hist\_P}\SpecialCharTok{$}\NormalTok{counts }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(hist\_P}\SpecialCharTok{$}\NormalTok{counts)}
\NormalTok{prob\_Q }\OtherTok{\textless{}{-}}\NormalTok{ hist\_Q}\SpecialCharTok{$}\NormalTok{counts }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(hist\_Q}\SpecialCharTok{$}\NormalTok{counts)}

\CommentTok{\# Compute Total Variation Distance}
\NormalTok{tv\_distance\_continuous }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(}\FunctionTok{abs}\NormalTok{(prob\_P }\SpecialCharTok{{-}}\NormalTok{ prob\_Q)) }\SpecialCharTok{/} \DecValTok{2}
\FunctionTok{print}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}
    \StringTok{"Total Variation Distance (Continuous Approximation):"}\NormalTok{,}
    \FunctionTok{round}\NormalTok{(tv\_distance\_continuous, }\DecValTok{4}\NormalTok{)}
\NormalTok{))}
\CommentTok{\#\textgreater{} [1] "Total Variation Distance (Continuous Approximation): 0.125"}
\end{Highlighting}
\end{Shaded}

The continuous distributions are discretized into histograms, and TV distance is computed based on the resulting probabilities.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Discrete Case}:

  \begin{itemize}
  \item
    The TV distance quantifies the maximum difference between \(P\) and \(Q\) in terms of probability mass.
  \item
    In this example, it highlights how much \(P\) and \(Q\) diverge.
  \end{itemize}
\item
  \textbf{Continuous Case}:

  \begin{itemize}
  \item
    For continuous distributions, TV distance is approximated using discretized probabilities from histograms.
  \item
    This approach provides an intuitive measure of similarity for large samples.
  \end{itemize}
\end{enumerate}

The Total Variation Distance provides an intuitive and interpretable measure of the maximum difference between two distributions. Its symmetry and bounded nature make it a versatile tool for comparing both discrete and continuous distributions.

\hypertarget{summary}{%
\subsection{Summary}\label{summary}}

\textbf{1. Tests for Comparing Distributions}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1284}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.3074}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1318}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1554}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2669}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Test Name}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Purpose}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Type of Data}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Advantages}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Limitations}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Kolmogorov-Smirnov Test} & Tests if two distributions are the same or if a sample matches a reference distribution. & Empirical Distributions (Continuous) & Non-parametric, detects global differences. & Less sensitive to tail differences, limited to one-dimensional data. \\
\textbf{Anderson-Darling Test} & Tests goodness-of-fit with emphasis on the tails. & Continuous Data & Strong sensitivity to tail behavior. & Requires specifying a reference distribution. \\
\textbf{Chi-Square Goodness-of-Fit Test} & Tests if observed frequencies match expected frequencies. & Categorical Data & Simple, intuitive for discrete data. & Requires large sample sizes and sufficiently large expected frequencies. \\
\textbf{Cramr-von Mises Test} & Evaluates goodness-of-fit using cumulative distribution functions. & Empirical Distributions (Continuous) & Sensitive across the entire distribution. & Limited to one-dimensional data; requires cumulative distribution functions. \\
\end{longtable}

\textbf{2. Divergence Metrics}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1298}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2786}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1947}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1947}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1908}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Metric Name}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Purpose}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Type of Data}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Advantages}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Limitations}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Kullback-Leibler Divergence} & Measures how one probability distribution diverges from another. & Probability Distributions (Continuous/Discrete) & Provides a clear measure of information loss. & Asymmetric, sensitive to zero probabilities. \\
\textbf{Jensen-Shannon Divergence} & Symmetric measure of similarity between two probability distributions. & Probability Distributions (Continuous/Discrete) & Symmetric and bounded; intuitive for comparison. & Less sensitive to tail differences. \\
\textbf{Hellinger Distance} & Measures geometric similarity between two probability distributions. & Discrete or Continuous Probability Distributions & Easy to interpret; bounded between 0 and 1. & Computationally expensive for large datasets. \\
\textbf{Bhattacharyya Distance} & Quantifies overlap between two statistical distributions. & Probability Distributions (Continuous/Discrete) & Useful for classification and clustering tasks. & Less interpretable in large-scale applications. \\
\end{longtable}

\textbf{3. Distance Metrics}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1069}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.3000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1724}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1931}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2172}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Metric Name}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Purpose}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Type of Data}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Advantages}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Limitations}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Wasserstein Distance} & Measures the ``effort'' or ``cost'' to transform one distribution into another. & Continuous or Empirical Distributions & Provides geometric interpretation; versatile. & Computationally expensive for large-scale data. \\
\textbf{Energy Distance} & Measures statistical dissimilarity between multivariate distributions. & Multivariate Empirical Distributions & Non-parametric, works well for high-dimensional data. & Requires pairwise calculations; sensitive to outliers. \\
\textbf{Total Variation Distance} & Measures the maximum absolute difference between probabilities of two distributions. & Probability Distributions (Discrete/Continuous) & Intuitive and strict divergence measure. & Ignores structural differences beyond the largest deviation. \\
\end{longtable}

\hypertarget{part-ii.-regression}{%
\part*{II. REGRESSION}\label{part-ii.-regression}}
\addcontentsline{toc}{part}{II. REGRESSION}

\hypertarget{linear-regression}{%
\chapter{Linear Regression}\label{linear-regression}}

\includegraphics[width=4.6875in,height=2.08333in]{images/econometrics.PNG}

Linear regression is one of the most fundamental tools in statistics and econometrics, widely used for modeling relationships between variables. It forms the cornerstone of predictive analysis, enabling us to understand and quantify how changes in one or more explanatory variables are associated with a dependent variable. Its simplicity and versatility make it an essential tool in fields ranging from economics and marketing to healthcare and environmental studies.

At its core, linear regression addresses questions about associations rather than causation. For example:

\begin{itemize}
\tightlist
\item
  How are advertising expenditures associated with sales performance?
\item
  What is the relationship between a company's revenue and its stock price?
\item
  How does the level of education correlate with income?
\end{itemize}

These questions are about patterns in data---not necessarily causal effects. While regression can provide insights into potential causal relationships, establishing causality requires more than just regression analysis. It requires careful consideration of the study design, assumptions, and potential confounding factors.

So, why is it called ``linear''? The term refers to the structure of the model, where the dependent variable (outcome) is modeled as a linear combination of one or more independent variables (predictors). For example, in simple linear regression, the relationship is represented as:

\[Y = \beta_0 + \beta_1 X + \epsilon,\]

where \(Y\) is the dependent variable, \(X\) is the independent variable, \(\beta_0\) and \(\beta_1\) are parameters to be estimated, and \(\epsilon\) is the error term capturing randomness or unobserved factors.

Linear regression serves as a foundation for much of applied data analysis because of its wide-ranging applications:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Understanding Patterns in Data:} Regression provides a framework to summarize and explore relationships between variables. It allows us to identify patterns such as trends or associations, which can guide further analysis or decision-making.
\item
  \textbf{Prediction:} Beyond exploring relationships, regression is widely used for making predictions. For instance, given historical data, we can use a regression model to predict future outcomes like sales, prices, or demand.
\item
  \textbf{Building Blocks for Advanced Techniques:} Linear regression is foundational for many advanced statistical and machine learning models, such as logistic regression, ridge regression, and neural networks. Mastering linear regression equips you with the skills to tackle more complex methods.
\end{enumerate}

\textbf{Regression and Causality: A Crucial Distinction}

It's essential to remember that regression alone does not establish causation. For instance, a regression model might show a strong association between advertising and sales, but this does not prove that advertising directly causes sales to increase. Other factors---such as seasonality, market trends, or unobserved variables---could also influence the results.

Establishing causality requires additional steps, such as controlled experiments, instrumental variable techniques, or careful observational study designs. As we work through the details of linear regression, we'll revisit this distinction and highlight scenarios where causality might or might not be inferred.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{What is an Estimator?}

At the heart of regression lies the process of \textbf{estimation}---the act of using data to determine the unknown characteristics of a population or model.

An \textbf{estimator} is a mathematical rule or formula used to calculate an estimate of an unknown quantity based on observed data. For example, when we calculate the average height of a sample to estimate the average height of a population, the sample mean is the estimator.

In the context of regression, the quantities we typically estimate are:

\begin{itemize}
\tightlist
\item
  \textbf{Parameters}: Fixed, unknown values that describe the relationship between variables (e.g., coefficients in a regression equation).

  \begin{itemize}
  \tightlist
  \item
    \textbf{Estimating parameters}  Parametric models (finite parameters, e.g., coefficients in regression).
  \end{itemize}
\item
  \textbf{Functions}: Unknown relationships or patterns in the data, often modeled without assuming a fixed functional form.

  \begin{itemize}
  \tightlist
  \item
    \textbf{Estimating functions}  Non-parametric models (focus on shapes or trends, not a fixed number of parameters).
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Types of Estimators}

To better understand the estimation process, let's introduce two broad categories of estimators that we'll work with:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Parametric Estimators}\\
  Parametric estimation focuses on a finite set of parameters that define a model. For example, in a simple linear regression:\\
  \[Y = \beta_0 + \beta_1 X + \epsilon,\]\\
  the task is to estimate the parameters \(\beta_0\) (intercept) and \(\beta_1\) (slope). Parametric estimators rely on specific assumptions about the form of the model (e.g., linearity) and the distribution of the error term (e.g., normality).
\item
  \textbf{Non-Parametric Estimators}\\
  Non-parametric estimation avoids assuming a specific functional form for the relationship between variables. Instead, it focuses on estimating patterns or trends directly from the data. For example, using a scatterplot smoothing technique to visualize how sales vary with advertising spend without imposing a linear or quadratic relationship.
\end{enumerate}

These two categories reflect a fundamental trade-off in statistical analysis: \textbf{parametric models} are often simpler and more interpretable but require strong assumptions, while \textbf{non-parametric models} are more flexible but may require more data and computational resources.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Desirable Properties of Estimators}

Regardless of whether we are estimating parameters or functions, we want our estimators to possess certain desirable properties. Think of these as the ``golden standards'' that help us judge whether an estimator is reliable:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Unbiasedness}\\
  An estimator is unbiased if it hits the true value of the parameter, on average, over repeated samples. Mathematically:\\
  \[E[\hat{\beta}] = \beta.\]\\
  This means that, across multiple samples, the estimator does not systematically overestimate or underestimate the true parameter.
\item
  \textbf{Consistency}\\
  Consistency ensures that as the sample size increases, the estimator converges to the true value of the parameter. Formally:\\
  \[plim\ \hat{\beta_n} = \beta.\]\\
  This property relies on the \protect\hyperlink{law-of-large-numbers}{Law of Large Numbers}, which guarantees that larger samples reduce random fluctuations, leading to more precise estimates.
\item
  \textbf{Efficiency}\\
  Among all unbiased estimators, an efficient estimator has the smallest variance.

  \begin{itemize}
  \tightlist
  \item
    The \protect\hyperlink{ordinary-least-squares}{Ordinary Least Squares} method is efficient because it is the \textbf{Best Linear Unbiased Estimator (BLUE)} under the \protect\hyperlink{gauss-markov-theorem}{Gauss-Markov Theorem}.
  \item
    For estimators that meet specific distributional assumptions (e.g., normality), \textbf{Maximum Likelihood Estimators (MLE)} are asymptotically efficient, meaning they achieve the lowest possible variance as the sample size grows.
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Why These Properties Matter}

Understanding these properties is crucial because they ensure that the methods we use for estimation are reliable, precise, and robust. Whether we are estimating coefficients in a regression model or uncovering a complex pattern in the data, these properties provide the foundation for statistical inference and decision-making.

Now that we've established what estimators are, the types we'll encounter, and their desirable properties, we can move on to understanding how these concepts apply specifically to the \protect\hyperlink{ordinary-least-squares}{Ordinary Least Squares} method---the backbone of linear regression.

Reference Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1161}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3244}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2857}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2679}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Estimator}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Assumptions}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Strengths}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Limitations}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\protect\hyperlink{ordinary-least-squares}{Ordinary Least Squares} & Errors are independent, identically distributed (i.i.d.) with mean 0 and constant variance.

Linear relationship between predictors and response. & Simple, well-understood method.

Minimizes residual sum of squares (easy to interpret coefficients). & Sensitive to outliers and violations of normality.

Can perform poorly if predictors are highly correlated (multicollinearity). \\
\protect\hyperlink{generalized-least-squares}{Generalized Least Squares} & Errors have a known correlation structure or heteroscedasticity structure that can be modeled. & Handles correlated or non-constant-variance errors.

More flexible than OLS when noise structure is known. & Requires specifying (or estimating) the error covariance structure.

Misspecification can lead to biased estimates. \\
\protect\hyperlink{maximum-likelihood-estimator}{Maximum Likelihood} & Underlying probability distribution (e.g., normal) must be specified correctly. & Provides a general framework for estimating parameters under well-defined probability models.

Can extend to complex likelihoods. & Highly sensitive to model misspecification.

May require more computation than OLS or GLS. \\
\protect\hyperlink{penalized-regularized-estimators}{Penalized (Regularized) Estimators} & Coefficients assumed to be shrinkable; model typically allows coefficient penalization. & Controls overfitting via regularization.

Handles high-dimensional data or many predictors.

Can perform feature selection (e.g., Lasso). & Requires choosing tuning parameter(s) (e.g., ).

Interpretation of coefficients becomes less straightforward. \\
\protect\hyperlink{robust-estimators}{Robust Estimators} & Less sensitive to heavy-tailed or outlier-prone distributions (weaker assumptions on the error structure). & Resistant to large deviations or outliers in data.

Often maintains good performance under mild model misspecifications. & Less efficient if errors are truly normal.

Choice of robust method and tuning can be subjective. \\
\protect\hyperlink{partial-least-squares}{Partial Least Squares} & Predictors may be highly correlated; dimension reduction is desired. & Simultaneously reduces dimensionality and fits regression.

Works well with collinear, high-dimensional data. & Can be harder to interpret than OLS (latent components instead of original predictors).

Requires choosing the number of components. \\
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{ordinary-least-squares}{%
\section{Ordinary Least Squares}\label{ordinary-least-squares}}

Ordinary Least Squares (OLS) is the backbone of statistical modeling, a method so foundational that it often serves as the starting point for understanding data relationships. Whether predicting sales, estimating economic trends, or uncovering patterns in scientific research, OLS remains a critical tool. Its appeal lies in simplicity: OLS models the relationship between a dependent variable and one or more predictors by minimizing the squared differences between observed and predicted values.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Why OLS Works: Linear and Nonlinear Relationships}

OLS rests on the Conditional Expectation Function (CEF), \(E[Y | X]\), which describes the expected value of \(Y\) given \(X\). Regression shines in two key scenarios:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Perfect Fit (Linear CEF):}\\
  If \(E[Y_i | X_{1i}, \dots, X_{Ki}] = a + \sum_{k=1}^K b_k X_{ki}\), the regression of \(Y_i\) on \(X_{1i}, \dots, X_{Ki}\) exactly equals the CEF. In other words, the regression gives the true average relationship between \(Y\) and \(X\).\\
  If the true relationship is linear, regression delivers the exact CEF. For instance, imagine you're estimating the relationship between advertising spend and sales revenue. If the true impact is linear, OLS will perfectly capture it.
\item
  \textbf{Approximation (Nonlinear CEF):}\\
  If \(E[Y_i | X_{1i}, \dots, X_{Ki}]\) is nonlinear, OLS provides the best linear approximation to this relationship. Specifically, it minimizes the expected squared deviation between the linear regression line and the nonlinear CEF.\\
  For example, the effect of advertising diminishes at higher spending levels? OLS still works, providing the best linear approximation to this nonlinear relationship by minimizing the squared deviations between predictions and the true (but unknown) CEF.
\end{enumerate}

In other words, regression is not just a tool for ``linear'' relationships---it's a workhorse that adapts remarkably well to messy, real-world data.

\includegraphics[width=0.8\textwidth,height=\textheight]{images/meme-linear-regerssion.jpg}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{simple-regression-basic-model}{%
\subsection{Simple Regression (Basic) Model}\label{simple-regression-basic-model}}

The simplest form of regression is a straight line:

\[
Y_i = \beta_0 + \beta_1 X_i + \epsilon_i
\]

where

\begin{itemize}
\tightlist
\item
  \(Y_i\): The dependent variable or outcome we're trying to predict (e.g., sales, temperature).
\item
  \(X_i\): The independent variable or predictor (e.g., advertising spend, time).
\item
  \(\beta_0\): The intercept---where the line crosses the \(Y\)-axis when \(X = 0\).
\item
  \(\beta_1\): The slope, representing the change in \(Y\) for a one-unit increase in \(X\).
\item
  \(\epsilon_i\): The error term, accounting for random factors that \(X\) cannot explain.
\end{itemize}

Assumptions About the Error Term (\(\epsilon_i\)):

\[
\begin{aligned}
E(\epsilon_i) &= 0 \\
\text{Var}(\epsilon_i) &= \sigma^2 \\
\text{Cov}(\epsilon_i, \epsilon_j) &= 0 \quad \text{for all } i \neq j
\end{aligned}
\]

Since \(\epsilon_i\) is random, \(Y_i\) is also random:

\[
\begin{aligned}
E(Y_i) &= E(\beta_0 + \beta_1 X_i + \epsilon_i) \\
&= \beta_0 + \beta_1 X_i
\end{aligned}
\]

\[
\begin{aligned}
\text{Var}(Y_i) &= \text{Var}(\beta_0 + \beta_1 X_i + \epsilon_i) \\
&= \text{Var}(\epsilon_i) \\
&= \sigma^2
\end{aligned}
\]

Since \(\text{Cov}(\epsilon_i, \epsilon_j) = 0\), the outcomes across observations are independent. Hence, \(Y_i\) and \(Y_j\) are uncorrelated as well, conditioned on the \(X\)'s.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{estimation-in-ordinary-least-squares}{%
\subsubsection{Estimation in Ordinary Least Squares}\label{estimation-in-ordinary-least-squares}}

The goal of OLS is to estimate the regression parameters (\(\beta_0\), \(\beta_1\)) that best describe the relationship between the dependent variable \(Y\) and the independent variable \(X\). To achieve this, we minimize the sum of squared deviations between observed values of \(Y_i\) and their expected values predicted by the model.

The deviation of an observed value \(Y_i\) from its expected value, based on the regression model, is:

\[
Y_i - E(Y_i) = Y_i - (\beta_0 + \beta_1 X_i).
\]

This deviation represents the error in prediction for the \(i\)-th observation.

To ensure that the errors don't cancel each other out and to prioritize larger deviations, we consider the squared deviations. The sum of squared deviations, denoted by \(Q\), is defined as:

\[
Q = \sum_{i=1}^{n} (Y_i - \beta_0 - \beta_1 X_i)^2.
\]

The goal of OLS is to find the values of \(\beta_0\) and \(\beta_1\) that minimize \(Q\). These values are called the \textbf{OLS estimators}.

To minimize \(Q\), we take partial derivatives with respect to \(\beta_0\) and \(\beta_1\), set them to zero, and solve the resulting system of equations. After simplifying, the estimators for the slope (\(b_1\)) and intercept (\(b_0\)) are obtained as follows:

Slope (\(b_1\)):

\[
b_1 = \frac{\sum_{i=1}^{n} (X_i - \bar{X})(Y_i - \bar{Y})}{\sum_{i=1}^{n} (X_i - \bar{X})^2}.
\]

Here, \(\bar{X}\) and \(\bar{Y}\) represent the means of \(X\) and \(Y\), respectively. This formula reveals that the slope is proportional to the covariance between \(X\) and \(Y\), scaled by the variance of \(X\).

Intercept (\(b_0\)):

\[
b_0 = \frac{1}{n} \left( \sum_{i=1}^{n} Y_i - b_1 \sum_{i=1}^{n} X_i \right) = \bar{Y} - b_1 \bar{X}.
\]

The intercept is determined by aligning the regression line with the center of the data.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Intuition Behind the Estimators}

\begin{itemize}
\item
  \(b_1\) (Slope): This measures the average change in \(Y\) for a one-unit increase in \(X\). The formula uses deviations from the mean to ensure that the relationship captures the joint variability of \(X\) and \(Y\).
\item
  \(b_0\) (Intercept): This ensures that the regression line passes through the mean of the data points \((\bar{X}, \bar{Y})\), anchoring the model in the center of the observed data.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Equivalently, we can also write these parameters in terms of covariances.

The covariance between two variables is defined as:

\[ \text{Cov}(X_i, Y_i) = E[(X_i - E[X_i])(Y_i - E[Y_i])] \]

Properties of Covariance:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(\text{Cov}(X_i, X_i) = \sigma^2_X\)
\item
  If \(E(X_i) = 0\) or \(E(Y_i) = 0\), then \(\text{Cov}(X_i, Y_i) = E[X_i Y_i]\)
\item
  For \(W_i = a + b X_i\) and \(Z_i = c + d Y_i\),\\
  \(\text{Cov}(W_i, Z_i) = bd \cdot \text{Cov}(X_i, Y_i)\)
\end{enumerate}

For a bivariate regression, the slope \(\beta\) in a bivariate regression is given by:

\[ \beta = \frac{\text{Cov}(Y_i, X_i)}{\text{Var}(X_i)} \]

For a multivariate case, the slope for \(X_k\) is:

\[ \beta_k = \frac{\text{Cov}(Y_i, \tilde{X}_{ki})}{\text{Var}(\tilde{X}_{ki})} \]

Where \(\tilde{X}_{ki}\) represents the residual from a regression of \(X_{ki}\) on the \(K-1\) other covariates in the model.

The intercept is:

\[ \beta_0 = E[Y_i] - \beta_1 E(X_i) \]

Note:

\begin{itemize}
\tightlist
\item
  OLS does not require the assumption of a specific distribution for the variables. Its robustness is based on the minimization of squared errors (i.e., no distributional assumptions).
\end{itemize}

\hypertarget{properties-of-least-squares-estimators}{%
\subsubsection{Properties of Least Squares Estimators}\label{properties-of-least-squares-estimators}}

The properties of the Ordinary Least Squares estimators (\(b_0\) and \(b_1\)) are derived based on their statistical behavior. These properties provide insights into the accuracy, variability, and reliability of the estimates.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{expectation-of-the-ols-estimators}{%
\paragraph{Expectation of the OLS Estimators}\label{expectation-of-the-ols-estimators}}

The OLS estimators \(b_0\) (intercept) and \(b_1\) (slope) are unbiased. This means their expected values equal the true population parameters:

\[
\begin{aligned}
E(b_1) &= \beta_1, \\
E(b_0) &= E(\bar{Y}) - \bar{X}\beta_1.
\end{aligned}
\]

Since the expected value of the sample mean of \(Y\), \(E(\bar{Y})\), is:

\[
E(\bar{Y}) = \beta_0 + \beta_1 \bar{X},
\]

the expected value of \(b_0\) simplifies to:

\[
E(b_0) = \beta_0.
\]

Thus, \(b_0\) and \(b_1\) are unbiased estimators of their respective population parameters \(\beta_0\) and \(\beta_1\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{variance-of-the-ols-estimators}{%
\paragraph{Variance of the OLS Estimators}\label{variance-of-the-ols-estimators}}

The variability of the OLS estimators depends on the spread of the predictor variable \(X\) and the error variance \(\sigma^2\). The variances are given by:

Variance of \(b_1\) (Slope):

\[
\text{Var}(b_1) = \frac{\sigma^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2}.
\]

Variance of \(b_0\) (Intercept):

\[
\text{Var}(b_0) = \sigma^2 \left( \frac{1}{n} + \frac{\bar{X}^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2} \right).
\]

These formulas highlight that:

\begin{itemize}
\tightlist
\item
  \(\text{Var}(b_1) \to 0\) as the number of observations increases, provided \(X_i\) values are distributed around their mean \(\bar{X}\).
\item
  \(\text{Var}(b_0) \to 0\) as \(n\) increases, assuming \(X_i\) values are appropriately selected (i.e., not all clustered near the mean).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{mean-square-error-mse}{%
\subsubsection{Mean Square Error (MSE)}\label{mean-square-error-mse}}

The Mean Square Error (MSE) quantifies the average squared residual (error) in the model:

\[
MSE = \frac{SSE}{n-2} = \frac{\sum_{i=1}^{n} e_i^2}{n-2} = \frac{\sum_{i=1}^{n} (Y_i - \hat{Y}_i)^2}{n-2},
\]

where \(SSE\) is the Sum of Squared Errors and \(n-2\) represents the degrees of freedom for a simple linear regression model (two parameters estimated: \(\beta_0\) and \(\beta_1\)).

The expected value of the MSE equals the error variance (i.e., unbiased Estimator of MSE:):

\[
E(MSE) = \sigma^2.
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{estimating-variance-of-the-ols-coefficients}{%
\subsubsection{Estimating Variance of the OLS Coefficients}\label{estimating-variance-of-the-ols-coefficients}}

The sample-based estimates of the variances of \(b_0\) and \(b_1\) are expressed as follows:

Estimated Variance of \(b_1\) (Slope):

\[
s^2(b_1) = \widehat{\text{Var}}(b_1) = \frac{MSE}{\sum_{i=1}^{n} (X_i - \bar{X})^2}.
\]

Estimated Variance of \(b_0\) (Intercept):

\[
s^2(b_0) = \widehat{\text{Var}}(b_0) = MSE \left( \frac{1}{n} + \frac{\bar{X}^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2} \right).
\]

These estimates rely on the MSE to approximate \(\sigma^2\).

The variance estimates are unbiased:

\[
\begin{aligned}
E(s^2(b_1)) &= \text{Var}(b_1), \\
E(s^2(b_0)) &= \text{Var}(b_0).
\end{aligned}
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Implications of These Properties}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Unbiasedness:} The unbiased nature of \(b_0\) and \(b_1\) ensures that, on average, the regression model accurately reflects the true relationship in the population.
\item
  \textbf{Decreasing Variance:} As the sample size \(n\) increases or as the spread of \(X_i\) values grows, the variances of \(b_0\) and \(b_1\) decrease, leading to more precise estimates.
\item
  \textbf{Error Estimation with MSE:} MSE provides a reliable estimate of the error variance \(\sigma^2\), which feeds directly into assessing the reliability of \(b_0\) and \(b_1\).
\end{enumerate}

\hypertarget{residuals-in-ordinary-least-squares}{%
\subsubsection{Residuals in Ordinary Least Squares}\label{residuals-in-ordinary-least-squares}}

Residuals are the differences between observed values (\(Y_i\)) and their predicted counterparts (\(\hat{Y}_i\)). They play a central role in assessing model fit and ensuring the assumptions of OLS are met.

The residual for the \(i\)-th observation is defined as:

\[
e_i = Y_i - \hat{Y}_i = Y_i - (b_0 + b_1 X_i),
\]

where:

\begin{itemize}
\tightlist
\item
  \(e_i\): Residual for the \(i\)-th observation.
\item
  \(\hat{Y}_i\): Predicted value based on the regression model.
\item
  \(Y_i\): Actual observed value.
\end{itemize}

Residuals estimate the unobservable error terms \(\epsilon_i\):

\begin{itemize}
\tightlist
\item
  \(e_i\) is an estimate of \(\epsilon_i = Y_i - E(Y_i)\).
\item
  \(\epsilon_i\) is always unknown because we do not know the true values of \(\beta_0\) and \(\beta_1\).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{key-properties-of-residuals}{%
\paragraph{Key Properties of Residuals}\label{key-properties-of-residuals}}

Residuals exhibit several mathematical properties that align with the OLS estimation process:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Sum of Residuals}:\\
  The residuals sum to zero:

  \[
  \sum_{i=1}^{n} e_i = 0.
  \]

  This ensures that the regression line passes through the centroid of the data, \((\bar{X}, \bar{Y})\).
\item
  \textbf{Orthogonality of Residuals to Predictors}:\\
  The residuals are orthogonal (uncorrelated) to the predictor variable \(X\):

  \[
  \sum_{i=1}^{n} X_i e_i = 0.
  \]

  This reflects the fact that the OLS minimizes the squared deviations of residuals along the \(Y\)-axis, not the \(X\)-axis.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{expected-values-of-residuals}{%
\paragraph{Expected Values of Residuals}\label{expected-values-of-residuals}}

The expected values of residuals reinforce the unbiased nature of OLS:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Mean of Residuals}:\\
  The residuals have an expected value of zero:

  \[
  E[e_i] = 0.
  \]
\item
  \textbf{Orthogonality to Predictors and Fitted Values}:\\
  Residuals are uncorrelated with both the predictor variables and the fitted values:

  \[
  \begin{aligned}
  E[X_i e_i] &= 0, \\
  E[\hat{Y}_i e_i] &= 0.
  \end{aligned}
  \]
\end{enumerate}

These properties highlight that residuals do not contain systematic information about the predictors or the fitted values, reinforcing the idea that the model has captured the underlying relationship effectively.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{practical-importance-of-residuals}{%
\paragraph{Practical Importance of Residuals}\label{practical-importance-of-residuals}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Model Diagnostics:}\\
  Residuals are analyzed to check the assumptions of OLS, including linearity, homoscedasticity (constant variance), and independence of errors. Patterns in residual plots can signal issues such as nonlinearity or heteroscedasticity.
\item
  \textbf{Goodness-of-Fit:}\\
  The sum of squared residuals, \(\sum e_i^2\), measures the total unexplained variation in \(Y\). A smaller sum indicates a better fit.
\item
  \textbf{Influence Analysis:}\\
  Large residuals may indicate outliers or influential points that disproportionately affect the regression line.
\end{enumerate}

\hypertarget{inference-in-ordinary-least-squares}{%
\subsubsection{Inference in Ordinary Least Squares}\label{inference-in-ordinary-least-squares}}

Inference allows us to make probabilistic statements about the regression parameters (\(\beta_0\), \(\beta_1\)) and predictions (\(Y_h\)). To perform valid inference, certain assumptions about the distribution of errors are necessary.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Normality Assumption

\begin{itemize}
\tightlist
\item
  OLS estimation itself does \textbf{not} require the assumption of normality.
\item
  However, to conduct hypothesis tests or construct confidence intervals for \(\beta_0\), \(\beta_1\), and predictions, distributional assumptions are necessary.
\item
  Inference on \(\beta_0\) and \(\beta_1\) is \textbf{robust} to moderate departures from normality, especially in large samples due to the \protect\hyperlink{central-limit-theorem}{Central Limit Theorem}.
\item
  Inference on predicted values, \(Y_{pred}\), is more sensitive to normality violations.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

When we assume a \textbf{normal error model}, the response variable \(Y_i\) is modeled as:

\[
Y_i \sim N(\beta_0 + \beta_1 X_i, \sigma^2),
\]

where:

\begin{itemize}
\tightlist
\item
  \(\beta_0 + \beta_1 X_i\): Mean response
\item
  \(\sigma^2\): Variance of the errors
\end{itemize}

Under this model, the sampling distributions of the OLS estimators, \(b_0\) and \(b_1\), can be derived.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{inference-for-beta_1-slope}{%
\paragraph{\texorpdfstring{Inference for \(\beta_1\) (Slope)}{Inference for \textbackslash beta\_1 (Slope)}}\label{inference-for-beta_1-slope}}

Under the normal error model:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Sampling Distribution of} \(b_1\):

  \[ 
  b_1 \sim N\left(\beta_1, \frac{\sigma^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2}\right).
  \]

  This indicates that \(b_1\) is an unbiased estimator of \(\beta_1\) with variance proportional to \(\sigma^2\).
\item
  \textbf{Test Statistic:}

  \[ 
  t = \frac{b_1 - \beta_1}{s(b_1)} \sim t_{n-2},
  \]

  where \(s(b_1)\) is the standard error of \(b_1\): \[
  s(b_1) = \sqrt{\frac{MSE}{\sum_{i=1}^{n} (X_i - \bar{X})^2}}.
  \]
\item
  \textbf{Confidence Interval:}

  A \((1-\alpha) 100\%\) confidence interval for \(\beta_1\) is:

  \[ 
  b_1 \pm t_{1-\alpha/2; n-2} \cdot s(b_1).
  \]
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{inference-for-beta_0-intercept}{%
\paragraph{\texorpdfstring{Inference for \(\beta_0\) (Intercept)}{Inference for \textbackslash beta\_0 (Intercept)}}\label{inference-for-beta_0-intercept}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Sampling Distribution of} \(b_0\):

  Under the normal error model, the sampling distribution of \(b_0\) is:

  \[ 
  b_0 \sim N\left(\beta_0, \sigma^2 \left(\frac{1}{n} + \frac{\bar{X}^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2}\right)\right).
  \]
\item
  \textbf{Test Statistic:}

  \[ 
  t = \frac{b_0 - \beta_0}{s(b_0)} \sim t_{n-2},
  \]

  where \(s(b_0)\) is the standard error of \(b_0\): \[
  s(b_0) = \sqrt{MSE \left(\frac{1}{n} + \frac{\bar{X}^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2}\right)}.
  \]
\item
  \textbf{Confidence Interval:}

  A \((1-\alpha) 100\%\) confidence interval for \(\beta_0\) is:

  \[ 
  b_0 \pm t_{1-\alpha/2; n-2} \cdot s(b_0).
  \]
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{mean-response}{%
\paragraph{Mean Response}\label{mean-response}}

In regression, we often estimate the mean response of the dependent variable \(Y\) for a given level of the predictor variable \(X\), denoted as \(X_h\). This estimation provides a predicted average outcome for a specific value of \(X\) based on the fitted regression model.

\begin{itemize}
\tightlist
\item
  Let \(X_h\) represent the level of \(X\) for which we want to estimate the mean response.
\item
  The mean response when \(X = X_h\) is denoted as \(E(Y_h)\).
\item
  A point estimator for \(E(Y_h)\) is \(\hat{Y}_h\), which is the predicted value from the regression model:
\end{itemize}

\[
\hat{Y}_h = b_0 + b_1 X_h.
\]

The estimator \(\hat{Y}_h\) is unbiased because its expected value equals the true mean response \(E(Y_h)\):

\[
\begin{aligned}
E(\hat{Y}_h) &= E(b_0 + b_1 X_h) \\
&= \beta_0 + \beta_1 X_h \\
&= E(Y_h).
\end{aligned}
\]

Thus, \(\hat{Y}_h\) provides a reliable estimate of the mean response at \(X_h\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The variance of \(\hat{Y}_h\) reflects the uncertainty in the estimate of the mean response:

\[
\begin{aligned}
\text{Var}(\hat{Y}_h) &= \text{Var}(b_0 + b_1 X_h) \quad\text{(definition of }\hat{Y}_h\text{)}\\[6pt]&= \text{Var}\bigl((\bar{Y} - b_1 \bar{X}) + b_1 X_h\bigr)\quad\text{(since } b_0 = \bar{Y} - b_1 \bar{X}\text{)}\\[6pt]&= \text{Var}\bigl(\bar{Y} + b_1(X_h - \bar{X})\bigr)\quad\text{(factor out } b_1\text{)}\\[6pt]&= \text{Var}\bigl(\bar{Y} + b_1 (X_h - \bar{X}) \bigr) \\
&= \text{Var}(\bar{Y}) + (X_h - \bar{X})^2 \text{Var}(b_1) + 2(X_h - \bar{X}) \text{Cov}(\bar{Y}, b_1).
\end{aligned}
\]

Since \(\text{Cov}(\bar{Y}, b_1) = 0\) (due to the independence of the errors, \(\epsilon_i\)), the variance simplifies to:

\[
\text{Var}(\hat{Y}_h) = \frac{\sigma^2}{n} + (X_h - \bar{X})^2 \frac{\sigma^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2}.
\]

This can also be expressed as:

\[
\text{Var}(\hat{Y}_h) = \sigma^2 \left( \frac{1}{n} + \frac{(X_h - \bar{X})^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2} \right).
\]

To estimate the variance of \(\hat{Y}_h\), we replace \(\sigma^2\) with \(MSE\), the mean squared error from the regression:

\[
s^2(\hat{Y}_h) = MSE \left( \frac{1}{n} + \frac{(X_h - \bar{X})^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2} \right).
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Under the normal error model, the sampling distribution of \(\hat{Y}_h\) is:

\[
\begin{aligned}
\hat{Y}_h &\sim N\left(E(Y_h), \text{Var}(\hat{Y}_h)\right), \\
\frac{\hat{Y}_h - E(Y_h)}{s(\hat{Y}_h)} &\sim t_{n-2}.
\end{aligned}
\]

This result follows because \(\hat{Y}_h\) is a linear combination of normally distributed random variables, and its variance is estimated using \(s^2(\hat{Y}_h)\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

A \(100(1-\alpha)\%\) confidence interval for the mean response \(E(Y_h)\) is given by:

\[
\hat{Y}_h \pm t_{1-\alpha/2; n-2} \cdot s(\hat{Y}_h),
\]

where:

\begin{itemize}
\tightlist
\item
  \(\hat{Y}_h\): Point estimate of the mean response,
\item
  \(s(\hat{Y}_h)\): Estimated standard error of the mean response,
\item
  \(t_{1-\alpha/2; n-2}\): Critical value from the \(t\)-distribution with \(n-2\) degrees of freedom.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{prediction-of-a-new-observation}{%
\paragraph{Prediction of a New Observation}\label{prediction-of-a-new-observation}}

When analyzing regression results, it is important to distinguish between:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Estimating the mean response} at a particular value of \(X\).
\item
  \textbf{Predicting an individual outcome} for a particular value of \(X\).
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Mean Response vs.~Individual Outcome

\begin{itemize}
\item
  \textbf{Same Point Estimate}\\
  The formula for both the estimated mean response and the predicted individual outcome at \(X = X_h\) is identical:\\
  \[
  \hat{Y}_{pred} = \hat{Y}_h = b_0 + b_1 X_h.
  \]
\item
  \textbf{Different Variance}\\
  Although the point estimates are the same, the level of uncertainty differs. When predicting an individual outcome, we must consider not only the uncertainty in estimating the mean response (\(\hat{Y}_h\)) but also the additional random variation within the distribution of \(Y\).
\end{itemize}

Therefore, \textbf{prediction intervals} (for individual outcomes) account for more uncertainty and are consequently wider than \textbf{confidence intervals} (for the mean response).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

To predict an individual outcome for a given \(X_h\), we combine the mean response with the random error:

\[
Y_{pred} = \beta_0 + \beta_1 X_h + \epsilon.
\]

Using the least squares predictor:

\[
\hat{Y}_{pred} = b_0 + b_1 X_h,
\]

since \(E(\epsilon) = 0\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The variance of the predicted value for a new observation, \(Y_{pred}\), includes both:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Variance of the estimated mean response: \[
  \sigma^2 \left( \frac{1}{n} + \frac{(X_h - \bar{X})^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2} \right),
  \]
\item
  Variance of the error term, \(\epsilon\), which is \(\sigma^2\).
\end{enumerate}

Thus, the total variance is:

\[
\begin{aligned}
\text{Var}(Y_{pred}) &= \text{Var}(b_0 + b_1 X_h + \epsilon) \\
&= \text{Var}(b_0 + b_1 X_h) + \text{Var}(\epsilon) \\
&= \sigma^2 \left( \frac{1}{n} + \frac{(X_h - \bar{X})^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2} \right) + \sigma^2 \\
&= \sigma^2 \left( 1 + \frac{1}{n} + \frac{(X_h - \bar{X})^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2} \right).
\end{aligned}
\]

We estimate the variance of the prediction using \(MSE\), the mean squared error:

\[
s^2(pred) = MSE \left( 1 + \frac{1}{n} + \frac{(X_h - \bar{X})^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2} \right).
\]

Under the normal error model, the standardized predicted value follows a \(t\)-distribution with \(n-2\) degrees of freedom:

\[
\frac{Y_{pred} - \hat{Y}_h}{s(pred)} \sim t_{n-2}.
\]

A \(100(1-\alpha)\%\) prediction interval for \(Y_{pred}\) is:

\[
\hat{Y}_{pred} \pm t_{1-\alpha/2; n-2} \cdot s(pred).
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{confidence-band}{%
\paragraph{Confidence Band}\label{confidence-band}}

In regression analysis, we often want to evaluate the uncertainty around the entire regression line, not just at a single value of the predictor variable \(X\). This is achieved using a \textbf{confidence band}, which provides a confidence interval for the mean response, \(E(Y) = \beta_0 + \beta_1 X\), over the entire range of \(X\) values.

The Working-Hotelling confidence band is a method to construct simultaneous confidence intervals for the regression line. For a given \(X_h\), the confidence band is expressed as:

\[
\hat{Y}_h \pm W s(\hat{Y}_h),
\]

where:

\begin{itemize}
\item
  \(W^2 = 2F_{1-\alpha; 2, n-2}\),

  \begin{itemize}
  \tightlist
  \item
    \(F_{1-\alpha; 2, n-2}\) is the critical value from the \(F\)-distribution with 2 and \(n-2\) degrees of freedom.
  \end{itemize}
\item
  \(s(\hat{Y}_h)\) is the standard error of the estimated mean response at \(X_h\):

  \[
  s^2(\hat{Y}_h) = MSE \left( \frac{1}{n} + \frac{(X_h - \bar{X})^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2} \right).
  \]
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Key Properties of the Confidence Band}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Width of the Interval:}

  \begin{itemize}
  \tightlist
  \item
    The width of the confidence band changes with \(X_h\) because \(s(\hat{Y}_h)\) depends on how far \(X_h\) is from the mean of \(X\) (\(\bar{X}\)).
  \item
    The interval is narrowest at \(X = \bar{X}\), where the variance of the estimated mean response is minimized.
  \end{itemize}
\item
  \textbf{Shape of the Band:}

  \begin{itemize}
  \tightlist
  \item
    The boundaries of the confidence band form a hyperbolic shape around the regression line.
  \item
    This reflects the increasing uncertainty in the mean response as \(X_h\) moves farther from \(\bar{X}\).
  \end{itemize}
\item
  \textbf{Simultaneous Coverage:}

  \begin{itemize}
  \tightlist
  \item
    The Working-Hotelling band ensures that the true regression line \(E(Y) = \beta_0 + \beta_1 X\) lies within the band across all values of \(X\) with a specified confidence level (e.g., \(95\%\)).
  \end{itemize}
\end{enumerate}

\hypertarget{analysis-of-variance-anova-in-regression}{%
\subsubsection{Analysis of Variance (ANOVA) in Regression}\label{analysis-of-variance-anova-in-regression}}

ANOVA in regression decomposes the total variability in the response variable (\(Y\)) into components attributed to the regression model and residual error. In the context of regression, ANOVA provides a mechanism to assess the fit of the model and test hypotheses about the relationship between \(X\) and \(Y\).

The \textbf{corrected Total Sum of Squares (SSTO)} quantifies the total variation in \(Y\):

\[
SSTO = \sum_{i=1}^n (Y_i - \bar{Y})^2,
\]

where \(\bar{Y}\) is the mean of the response variable. The term ``corrected'' refers to the fact that the sum of squares is calculated relative to the mean (i.e., the uncorrected total sum of squares is given by \(\sum Y_i^2\))

Using the fitted regression model \(\hat{Y}_i = b_0 + b_1 X_i\), we estimate the conditional mean of \(Y\) at \(X_i\). The total sum of squares can be decomposed as:

\[
\begin{aligned}
\sum_{i=1}^n (Y_i - \bar{Y})^2 &= \sum_{i=1}^n (Y_i - \hat{Y}_i + \hat{Y}_i - \bar{Y})^2 \\
&= \sum_{i=1}^n (Y_i - \hat{Y}_i)^2 + \sum_{i=1}^n (\hat{Y}_i - \bar{Y})^2 + 2 \sum_{i=1}^n (Y_i - \hat{Y}_i)(\hat{Y}_i - \bar{Y}) \\
&= \sum_{i=1}^n (Y_i - \hat{Y}_i)^2 + \sum_{i=1}^n (\hat{Y}_i - \bar{Y})^2
\end{aligned}
\]

\begin{itemize}
\tightlist
\item
  The cross-product term is zero, as shown below.
\item
  This decomposition simplifies to:
\end{itemize}

\[
SSTO = SSE + SSR,
\]

where:

\begin{itemize}
\item
  \(SSE = \sum_{i=1}^n (Y_i - \hat{Y}_i)^2\): Error Sum of Squares (variation unexplained by the model).
\item
  \(SSR = \sum_{i=1}^n (\hat{Y}_i - \bar{Y})^2\): Regression Sum of Squares (variation explained by the model), which measure how the conditional mean varies about a central value.
\end{itemize}

Degrees of freedom are partitioned as:

\[
\begin{aligned}
SSTO &= SSR + SSE \\
(n-1) &= (1) + (n-2) \\
\end{aligned}
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

To confirm that the cross-product term is zero:

\[
\begin{aligned}
\sum_{i=1}^n (Y_i - \hat{Y}_i)(\hat{Y}_i - \bar{Y}) 
&= \sum_{i=1}^{n}(Y_i - \bar{Y} -b_1 (X_i - \bar{X}))(\bar{Y} + b_1 (X_i - \bar{X})-\bar{Y}) \quad \text{(Expand } Y_i - \hat{Y}_i \text{ and } \hat{Y}_i - \bar{Y}\text{)} \\
&=\sum_{i=1}^{n}(Y_i - \bar{Y} -b_1 (X_i - \bar{X}))( b_1 (X_i - \bar{X}))  \\
&= b_1 \sum_{i=1}^n (Y_i - \bar{Y})(X_i - \bar{X}) - b_1^2 \sum_{i=1}^n (X_i - \bar{X})^2 \quad \text{(Distribute terms in the product)} \\
&= b_1 \frac{\sum_{i=1}^n (Y_i - \bar{Y})(X_i - \bar{X})}{\sum_{i=1}^n (X_i - \bar{X})^2} \sum_{i=1}^n (X_i - \bar{X})^2 - b_1^2 \sum_{i=1}^n (X_i - \bar{X})^2 \quad \text{(Substitute } b_1 \text{ definition)} \\
&= b_1^2 \sum_{i=1}^n (X_i - \bar{X})^2 - b_1^2 \sum_{i=1}^n (X_i - \bar{X})^2  \\
&= 0 
\end{aligned}
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The ANOVA table summarizes the partitioning of variability:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1944}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1944}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1944}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2222}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1944}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Source of Variation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Sum of Squares
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
df
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Mean Square
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(F\) Statistic
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Regression (Model) & \(SSR\) & \(1\) & \(MSR = \frac{SSR}{1}\) & \(F = \frac{MSR}{MSE}\) \\
Error & \(SSE\) & \(n-2\) & \(MSE = \frac{SSE}{n-2}\) & \\
Total (Corrected) & \(SSTO\) & \(n-1\) & & \\
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The expected values of the mean squares are:

\[
\begin{aligned}
E(MSE) &= \sigma^2, \\
E(MSR) &= \sigma^2 + \beta_1^2 \sum_{i=1}^n (X_i - \bar{X})^2.
\end{aligned}
\]

\begin{itemize}
\tightlist
\item
  \textbf{If} \(\beta_1 = 0\):

  \begin{itemize}
  \tightlist
  \item
    The regression model does not explain any variation in \(Y\) beyond the mean, and \(E(MSR) = E(MSE) = \sigma^2\).
  \item
    This condition corresponds to the null hypothesis, \(H_0: \beta_1 = 0\).
  \end{itemize}
\item
  \textbf{If} \(\beta_1 \neq 0\):

  \begin{itemize}
  \tightlist
  \item
    The regression model explains some variation in \(Y\), and \(E(MSR) > E(MSE)\).
  \item
    The additional term \(\beta_1^2 \sum_{i=1}^{n} (X_i - \bar{X})^2\) represents the variance explained by the predictor \(X\).
  \end{itemize}
\end{itemize}

The difference between \(E(MSR)\) and \(E(MSE)\) allows us to infer whether \(\beta_1 \neq 0\) by comparing their ratio.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Assuming the errors \(\epsilon_i\) are independent and identically distributed as \(N(0, \sigma^2)\), and under the null hypothesis \(H_0: \beta_1 = 0\), we have:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The scaled \(MSE\) follows a chi-square distribution with \(n-2\) degrees of freedom:

  \[
  \frac{MSE}{\sigma^2} \sim \chi_{n-2}^2.
  \]
\item
  The scaled \(MSR\) follows a chi-square distribution with \(1\) degree of freedom:

  \[
  \frac{MSR}{\sigma^2} \sim \chi_{1}^2.
  \]
\item
  These two chi-square random variables are independent.
\end{enumerate}

The ratio of two independent chi-square random variables, scaled by their respective degrees of freedom, follows an \(F\)-distribution. Therefore, under \(H_0\):

\[
F = \frac{MSR}{MSE} \sim F_{1, n-2}.
\]

The \(F\)-statistic tests whether the regression model provides a significant improvement over the null model (constant \(E(Y)\)).

The hypotheses for the \(F\)-test are:

\begin{itemize}
\tightlist
\item
  \textbf{Null Hypothesis} (\(H_0\)): \(\beta_1 = 0\) (no relationship between \(X\) and \(Y\)).
\item
  \textbf{Alternative Hypothesis} (\(H_a\)): \(\beta_1 \neq 0\) (a significant relationship exists between \(X\) and \(Y\)).
\end{itemize}

The rejection rule for \(H_0\) at significance level \(\alpha\) is:

\[
F > F_{1-\alpha;1,n-2},
\]

where \(F_{1-\alpha;1,n-2}\) is the critical value from the \(F\)-distribution with \(1\) and \(n-2\) degrees of freedom.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{If} \(F \leq F_{1-\alpha;1,n-2}\):

  \begin{itemize}
  \tightlist
  \item
    Fail to reject \(H_0\). There is insufficient evidence to conclude that \(X\) significantly explains variation in \(Y\).
  \end{itemize}
\item
  \textbf{If} \(F > F_{1-\alpha;1,n-2}\):

  \begin{itemize}
  \tightlist
  \item
    Reject \(H_0\). There is significant evidence that \(X\) explains some of the variation in \(Y\).
  \end{itemize}
\end{enumerate}

\hypertarget{coefficient-of-determination-r2}{%
\subsubsection{\texorpdfstring{Coefficient of Determination (\(R^2\))}{Coefficient of Determination (R\^{}2)}}\label{coefficient-of-determination-r2}}

The \textbf{Coefficient of Determination} (\(R^2\)) measures how well the linear regression model accounts for the variability in the response variable \(Y\). It is defined as:

\[
R^2 = \frac{SSR}{SSTO} = 1 - \frac{SSE}{SSTO},
\]

where:

\begin{itemize}
\tightlist
\item
  \(SSR\): Regression Sum of Squares (variation explained by the model).
\item
  \(SSTO\): Total Sum of Squares (total variation in \(Y\) about its mean).
\item
  \(SSE\): Error Sum of Squares (variation unexplained by the model).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Properties of} \(R^2\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Range}: \[
  0 \leq R^2 \leq 1.
  \]

  \begin{itemize}
  \tightlist
  \item
    \(R^2 = 0\): The model explains none of the variability in \(Y\) (e.g., \(\beta_1 = 0\)).
  \item
    \(R^2 = 1\): The model explains all the variability in \(Y\) (perfect fit).
  \end{itemize}
\item
  \textbf{Proportionate Reduction in Variance}: \(R^2\) represents the proportionate reduction in the total variation of \(Y\) after fitting the model. It quantifies how much better the model predicts \(Y\) compared to simply using \(\bar{Y}\).
\item
  \textbf{Potential Misinterpretation}: It is not really correct to say \(R^2\) is the ``variation in \(Y\) explained by \(X\).'' The term ``variation explained'' assumes a causative or deterministic explanation, which is not always correct. For example:

  \begin{itemize}
  \item
    \(R^2\) shows how much variance in \(Y\) is accounted for by the regression model, but it does not imply causation.
  \item
    In cases with confounding variables or spurious correlations, \(R^2\) can still be high, even if there's no direct causal link between \(X\) and \(Y\).
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

For simple linear regression, \(R^2\) is the square of the Pearson correlation coefficient, \(r\):

\[
R^2 = (r)^2,
\]

where:

\begin{itemize}
\tightlist
\item
  \(r = \text{corr}(X, Y)\) is the sample correlation coefficient.
\end{itemize}

The relationship between \(b_1\) (the slope of the regression line) and \(r\) is given by:

\[
b_1 = \left(\frac{\sum_{i=1}^n (Y_i - \bar{Y})^2}{\sum_{i=1}^n (X_i - \bar{X})^2}\right)^{1/2}.
\]

Additionally, \(r\) can be expressed as:

\[
r = \frac{s_y}{s_x} \cdot r,
\]

where \(s_y\) and \(s_x\) are the sample standard deviations of \(Y\) and \(X\), respectively.

\hypertarget{lack-of-fit-in-regression}{%
\subsubsection{Lack of Fit in Regression}\label{lack-of-fit-in-regression}}

The \textbf{lack of fit} test evaluates whether the chosen regression model adequately captures the relationship between the predictor variable \(X\) and the response variable \(Y\). When there are repeated observations at specific values of \(X\), we can partition the Error Sum of Squares (\(SSE\)) into two components:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Pure Error}
\item
  \textbf{Lack of Fit}.
\end{enumerate}

Given the observations:

\begin{itemize}
\tightlist
\item
  \(Y_{ij}\): The \(j\)-th replicate for the \(i\)-th distinct value of \(X\),

  \begin{itemize}
  \tightlist
  \item
    \(Y_{11}, Y_{21}, \dots, Y_{n_1, 1}\): \(n_1\) repeated observations of \(X_1\)
  \item
    \(Y_{1c}, Y_{2c}, \dots, Y_{n_c,c}\): \(n_c\) repeated observations of \(X_c\)
  \end{itemize}
\item
  \(\bar{Y}_j\): The mean response for replicates at \(X_j\),
\item
  \(\hat{Y}_{ij}\): The predicted value from the regression model for \(X_j\),
\end{itemize}

the Error Sum of Squares (\(SSE\)) can be decomposed as:

\[
\begin{aligned}
\sum_{i} \sum_{j} (Y_{ij} - \hat{Y}_{ij})^2 &= \sum_{i} \sum_{j} (Y_{ij} - \bar{Y}_j + \bar{Y}_j - \hat{Y}_{ij})^2 \\
&= \sum_{i} \sum_{j} (Y_{ij} - \bar{Y}_j)^2 + \sum_{j} n_j (\bar{Y}_j - \hat{Y}_{ij})^2 + \text{cross product term} \\
&= \sum_{i} \sum_{j} (Y_{ij} - \bar{Y}_j)^2 + \sum_{j} n_j (\bar{Y}_j - \hat{Y}_{ij})^2
\end{aligned}
\]

\begin{itemize}
\tightlist
\item
  The \textbf{cross product term} is zero because the deviations within replicates and the deviations between replicates are orthogonal.
\item
  This simplifies to:
\end{itemize}

\[
SSE = SSPE + SSLF,
\]

where:

\begin{itemize}
\tightlist
\item
  \(SSPE\) (Pure Error Sum of Squares): Variation within replicates for the same \(X_j\), reflecting natural variability in the response.

  \begin{itemize}
  \tightlist
  \item
    Degrees of freedom: \(df_{pe} = n - c\), where \(n\) is the total number of observations, and \(c\) is the number of distinct \(X\) values.
  \end{itemize}
\item
  \(SSLF\) (Lack of Fit Sum of Squares): Variation between the replicate means \(\bar{Y}_j\) and the model-predicted values \(\hat{Y}_{ij}\). If SSLF is large, it suggests the model may not adequately describe the relationship between \(X\) and \(Y\).

  \begin{itemize}
  \tightlist
  \item
    Degrees of freedom: \(df_{lf} = c - 2\), where 2 accounts for the parameters in the linear regression model (\(\beta_0\) and \(\beta_1\)).
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{itemize}
\item
  \textbf{Mean Square for Pure Error (MSPE):}\\
  \[
  MSPE = \frac{SSPE}{df_{pe}} = \frac{SSPE}{n-c}.
  \]
\item
  \textbf{Mean Square for Lack of Fit (MSLF):}\\
  \[
  MSLF = \frac{SSLF}{df_{lf}} = \frac{SSLF}{c-2}.
  \]
\end{itemize}

\hypertarget{the-f-test-for-lack-of-fit}{%
\paragraph{The F-Test for Lack of Fit}\label{the-f-test-for-lack-of-fit}}

The \textbf{F-test for lack of fit} evaluates whether the chosen regression model adequately captures the relationship between the predictor variable \(X\) and the response variable \(Y\). Specifically, it tests whether any systematic deviations from the model exist that are not accounted for by random error.

\begin{itemize}
\item
  \textbf{Null Hypothesis (}\(H_0\)):\\
  The regression model is adequate: \[
  H_0: Y_{ij} = \beta_0 + \beta_1 X_i + \epsilon_{ij}, \quad \epsilon_{ij} \sim \text{i.i.d. } N(0, \sigma^2).
  \]
\item
  \textbf{Alternative Hypothesis (}\(H_a\)):\\
  The regression model is not adequate and includes an additional function \(f(X_i, Z_1, \dots)\) to account for the lack of fit: \[
  H_a: Y_{ij} = \alpha_0 + \alpha_1 X_i + f(X_i, Z_1, \dots) + \epsilon_{ij}^*, \quad \epsilon_{ij}^* \sim \text{i.i.d. } N(0, \sigma^2).
  \]
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Expected Mean Squares}

\begin{itemize}
\item
  The expected Mean Square for Pure Error (MSPE) is the same under both \(H_0\) and \(H_a\):

  \[
  E(MSPE) = \sigma^2.
  \]
\item
  The expected Mean Square for Lack of Fit (MSLF) depends on whether \(H_0\) is true:

  \begin{itemize}
  \tightlist
  \item
    Under \(H_0\) (model is adequate): \[
    E(MSLF) = \sigma^2.
    \]
  \item
    Under \(H_a\) (model is not adequate): \[
    E(MSLF) = \sigma^2 + \frac{\sum n_j f(X_i, Z_1, \dots)^2}{n-2}.
    \]
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The test statistic for the lack-of-fit test is:

\[
F = \frac{MSLF}{MSPE},
\]

where:

\begin{itemize}
\tightlist
\item
  \(MSLF = \frac{SSLF}{c-2}\),\\
  and \(SSLF\) is the Lack of Fit Sum of Squares.
\item
  \(MSPE = \frac{SSPE}{n-c}\),\\
  and \(SSPE\) is the Pure Error Sum of Squares.
\end{itemize}

Under \(H_0\), the \(F\)-statistic follows an \(F\)-distribution:

\[
F \sim F_{c-2, n-c}.
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Decision Rule}

\begin{itemize}
\item
  Reject \(H_0\) at significance level \(\alpha\) if: \[
  F > F_{1-\alpha; c-2, n-c}.
  \]
\item
  \textbf{Failing to reject} \(H_0\):

  \begin{itemize}
  \tightlist
  \item
    Indicates that there is no evidence of lack of fit.
  \item
    Does not imply the model is ``true,'' but it suggests that the model provides a reasonable approximation to the true relationship.
  \end{itemize}
\end{itemize}

To summarize, when repeat observations exist at some levels of \(X\), the \textbf{Error Sum of Squares (SSE)} can be further partitioned into \textbf{Lack of Fit (SSLF)} and \textbf{Pure Error (SSPE)}. This leads to an extended ANOVA table:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1918}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1918}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1918}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2192}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2055}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Source of Variation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Sum of Squares
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
df
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Mean Square
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
F Statistic
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Regression & SSR & \(1\) & \(MSR = \frac{SSR}{1}\) & \(F = \frac{MSR}{MSE}\) \\
Error & SSE & \(n-2\) & \(MSE = \frac{SSE}{n-2}\) & \\
Lack of fit & SSLF & \(c-2\) & \(MSLF = \frac{SSLF}{c-2}\) & \(F = \frac{MSLF}{MSPE}\) \\
Pure Error & SSPE & \(n-c\) & \(MSPE = \frac{SSPE}{n-c}\) & \\
Total (Corrected) & SSTO & \(n-1\) & & \\
\end{longtable}

Repeat observations have important implications for the coefficient of determination (\(R^2\)):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(R^2\) Can't Attain 1 with Repeat Observations:

  \begin{itemize}
  \tightlist
  \item
    With repeat observations, \(SSE\) (Error Sum of Squares) cannot be reduced to 0 because \(SSPE > 0\) (variability within replicates).
  \end{itemize}
\item
  \textbf{Maximum} \(R^2\):

  \begin{itemize}
  \item
    The maximum attainable \(R^2\) in the presence of repeat observations is:

    \[
    R^2_{\text{max}} = \frac{SSTO - SSPE}{SSTO}.
    \]
  \end{itemize}
\item
  \textbf{Importance of Repeat Observations:}

  \begin{itemize}
  \tightlist
  \item
    Not all levels of \(X\) need repeat observations, but their presence enables the separation of pure error from lack of fit, making the \(F\)-test for lack of fit possible.
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Estimation of \(\sigma^2\) with Repeat Observations

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Use of MSE:}

  \begin{itemize}
  \tightlist
  \item
    When \(H_0\) is appropriate (the model fits well), \(MSE\) is typically used as the estimate of \(\sigma^2\) instead of \(MSPE\) because it has more degrees of freedom and provides a more reliable estimate.
  \end{itemize}
\item
  \textbf{Pooling Estimates:}

  \begin{itemize}
  \tightlist
  \item
    In practice, \(MSE\) and \(MSPE\) may be pooled if \(H_0\) holds, resulting in a more precise estimate of \(\sigma^2\).
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{joint-inference-for-regression-parameters}{%
\subsubsection{Joint Inference for Regression Parameters}\label{joint-inference-for-regression-parameters}}

Joint inference considers the simultaneous coverage of confidence intervals for multiple regression parameters, such as \(\beta_0\) (intercept) and \(\beta_1\) (slope). Ensuring adequate confidence for both parameters together requires adjustments to maintain the desired family-wise confidence level.

Let:

\begin{itemize}
\tightlist
\item
  \(\bar{A}_1\): The event that the confidence interval for \(\beta_0\) covers its true value.
\item
  \(\bar{A}_2\): The event that the confidence interval for \(\beta_1\) covers its true value.
\end{itemize}

The individual confidence levels are:

\[
\begin{aligned}
P(\bar{A}_1) &= 1 - \alpha, \\
P(\bar{A}_2) &= 1 - \alpha.
\end{aligned}
\]

The joint confidence coefficient, \(P(\bar{A}_1 \cap \bar{A}_2)\), is:

\[
\begin{aligned}
P(\bar{A}_1 \cap \bar{A}_2) &= 1 - P(\bar{A}_1 \cup \bar{A}_2), \\
&= 1 - P(A_1) - P(A_2) + P(A_1 \cap A_2), \\
&\geq 1 - P(A_1) - P(A_2), \\
&= 1 - 2\alpha.
\end{aligned}
\]

This means that if \(\alpha\) is the significance level for each parameter, the joint confidence coefficient is at least \(1 - 2\alpha\). This inequality is known as the \textbf{Bonferroni Inequality}.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Bonferroni Confidence Intervals}

To ensure a desired joint confidence level of \((1-\alpha)\) for both \(\beta_0\) and \(\beta_1\), the Bonferroni method adjusts the confidence level for each parameter by dividing \(\alpha\) by the number of parameters. For two parameters:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The confidence level for each parameter is \((1-\alpha/2)\).
\item
  The resulting \textbf{Bonferroni-adjusted confidence intervals} are:

  \[
  \begin{aligned}
  b_0 &\pm B \cdot s(b_0), \\
  b_1 &\pm B \cdot s(b_1),
  \end{aligned}
  \]

  where \(B = t_{1-\alpha/4; n-2}\) is the critical value from the \(t\)-distribution with \(n-2\) degrees of freedom.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Interpretation of Bonferroni Confidence Intervals}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Coverage Probability:}

  \begin{itemize}
  \tightlist
  \item
    If repeated samples were taken, \((1-\alpha)100\%\) of the joint intervals would contain the true values of \((\beta_0, \beta_1)\).
  \item
    This implies that \(\alpha \times 100\%\) of the samples would miss at least one of the true parameter values.
  \end{itemize}
\item
  \textbf{Conservatism:}

  \begin{itemize}
  \tightlist
  \item
    The Bonferroni method ensures the family-wise confidence level but is \textbf{conservative}. The actual joint confidence level is often higher than \((1-\alpha)100\%\).
  \item
    This conservatism reduces statistical power.
  \end{itemize}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load necessary libraries}
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(MASS)}

\CommentTok{\# Set seed for reproducibility}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\# Generate synthetic data}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{100}  \CommentTok{\# Number of observations}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)  }\CommentTok{\# Predictor}
\NormalTok{beta\_0 }\OtherTok{\textless{}{-}} \DecValTok{2}  \CommentTok{\# True intercept}
\NormalTok{beta\_1 }\OtherTok{\textless{}{-}} \DecValTok{3}  \CommentTok{\# True slope}
\NormalTok{sigma }\OtherTok{\textless{}{-}} \DecValTok{1}  \CommentTok{\# Standard deviation of error}
\NormalTok{y }\OtherTok{\textless{}{-}}
\NormalTok{    beta\_0 }\SpecialCharTok{+}\NormalTok{ beta\_1 }\SpecialCharTok{*}\NormalTok{ x }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =}\NormalTok{ sigma)  }\CommentTok{\# Response}

\CommentTok{\# Fit linear model}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x)}
\FunctionTok{summary}\NormalTok{(model)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = y \textasciitilde{} x)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}     Min      1Q  Median      3Q     Max }
\CommentTok{\#\textgreater{} {-}1.9073 {-}0.6835 {-}0.0875  0.5806  3.2904 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)  1.89720    0.09755   19.45   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} x            2.94753    0.10688   27.58   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 0.9707 on 98 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.8859, Adjusted R{-}squared:  0.8847 }
\CommentTok{\#\textgreater{} F{-}statistic: 760.6 on 1 and 98 DF,  p{-}value: \textless{} 2.2e{-}16}

\CommentTok{\# Extract coefficients and standard errors}
\NormalTok{b0\_hat }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(model)[}\DecValTok{1}\NormalTok{]}
\NormalTok{b1\_hat }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(model)[}\DecValTok{2}\NormalTok{]}
\NormalTok{s\_b0 }\OtherTok{\textless{}{-}}
    \FunctionTok{summary}\NormalTok{(model)}\SpecialCharTok{$}\NormalTok{coefficients[}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{]  }\CommentTok{\# Standard error of intercept}
\NormalTok{s\_b1 }\OtherTok{\textless{}{-}} \FunctionTok{summary}\NormalTok{(model)}\SpecialCharTok{$}\NormalTok{coefficients[}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{]  }\CommentTok{\# Standard error of slope}

\CommentTok{\# Desired confidence level}
\NormalTok{alpha }\OtherTok{\textless{}{-}} \FloatTok{0.05}  \CommentTok{\# Overall significance level}

\CommentTok{\# Bonferroni correction}
\NormalTok{adjusted\_alpha }\OtherTok{\textless{}{-}}\NormalTok{ alpha }\SpecialCharTok{/} \DecValTok{2}  \CommentTok{\# Adjusted alpha for each parameter}

\CommentTok{\# Critical t{-}value for Bonferroni adjustment}
\NormalTok{t\_crit }\OtherTok{\textless{}{-}}
    \FunctionTok{qt}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ adjusted\_alpha, }\AttributeTok{df =}\NormalTok{ n }\SpecialCharTok{{-}} \DecValTok{2}\NormalTok{)  }\CommentTok{\# n{-}2 degrees of freedom}

\CommentTok{\# Bonferroni confidence intervals}
\NormalTok{ci\_b0 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(b0\_hat }\SpecialCharTok{{-}}\NormalTok{ t\_crit }\SpecialCharTok{*}\NormalTok{ s\_b0, b0\_hat }\SpecialCharTok{+}\NormalTok{ t\_crit }\SpecialCharTok{*}\NormalTok{ s\_b0)}
\NormalTok{ci\_b1 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(b1\_hat }\SpecialCharTok{{-}}\NormalTok{ t\_crit }\SpecialCharTok{*}\NormalTok{ s\_b1, b1\_hat }\SpecialCharTok{+}\NormalTok{ t\_crit }\SpecialCharTok{*}\NormalTok{ s\_b1)}

\CommentTok{\# Print results}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Bonferroni Confidence Intervals:}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Bonferroni Confidence Intervals:}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Intercept (beta\_0): ["}\NormalTok{,}
    \FunctionTok{round}\NormalTok{(ci\_b0[}\DecValTok{1}\NormalTok{], }\DecValTok{2}\NormalTok{),}
    \StringTok{","}\NormalTok{,}
    \FunctionTok{round}\NormalTok{(ci\_b0[}\DecValTok{2}\NormalTok{], }\DecValTok{2}\NormalTok{),}
    \StringTok{"]}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Intercept (beta\_0): [ 1.7 , 2.09 ]}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Slope (beta\_1): ["}\NormalTok{,}
    \FunctionTok{round}\NormalTok{(ci\_b1[}\DecValTok{1}\NormalTok{], }\DecValTok{2}\NormalTok{),}
    \StringTok{","}\NormalTok{,}
    \FunctionTok{round}\NormalTok{(ci\_b1[}\DecValTok{2}\NormalTok{], }\DecValTok{2}\NormalTok{),}
    \StringTok{"]}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Slope (beta\_1): [ 2.74 , 3.16 ]}

\CommentTok{\# Calculate the covariance matrix of coefficients}
\NormalTok{cov\_matrix }\OtherTok{\textless{}{-}} \FunctionTok{vcov}\NormalTok{(model)}

\CommentTok{\# Generate points for confidence ellipse}
\NormalTok{ellipse\_points }\OtherTok{\textless{}{-}}
\NormalTok{    MASS}\SpecialCharTok{::}\FunctionTok{mvrnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{1000}\NormalTok{,}
                  \AttributeTok{mu =} \FunctionTok{coef}\NormalTok{(model),}
                  \AttributeTok{Sigma =}\NormalTok{ cov\_matrix)}

\CommentTok{\# Convert to data frame for plotting}
\NormalTok{ellipse\_df }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(ellipse\_points)}
\FunctionTok{colnames}\NormalTok{(ellipse\_df) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"beta\_0"}\NormalTok{, }\StringTok{"beta\_1"}\NormalTok{)}

\CommentTok{\# Plot confidence intervals and ellipse}
\NormalTok{p }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
    \CommentTok{\# Confidence ellipse}
    \FunctionTok{geom\_point}\NormalTok{(}
        \AttributeTok{data =}\NormalTok{ ellipse\_df,}
        \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ beta\_0, }\AttributeTok{y =}\NormalTok{ beta\_1),}
        \AttributeTok{alpha =} \FloatTok{0.1}\NormalTok{,}
        \AttributeTok{color =} \StringTok{"grey"}
\NormalTok{    ) }\SpecialCharTok{+}
    \CommentTok{\# Point estimate}
    \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ b0\_hat, }\AttributeTok{y =}\NormalTok{ b1\_hat),}
               \AttributeTok{color =} \StringTok{"red"}\NormalTok{,}
               \AttributeTok{size =} \DecValTok{3}\NormalTok{) }\SpecialCharTok{+}
    \CommentTok{\# Bonferroni confidence intervals}
    \FunctionTok{geom\_errorbar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ b0\_hat, }\AttributeTok{ymin =}\NormalTok{ ci\_b1[}\DecValTok{1}\NormalTok{], }\AttributeTok{ymax =}\NormalTok{ ci\_b1[}\DecValTok{2}\NormalTok{]),}
                  \AttributeTok{width =} \FloatTok{0.1}\NormalTok{,}
                  \AttributeTok{color =} \StringTok{"blue"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_errorbarh}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ b1\_hat, }\AttributeTok{xmin =}\NormalTok{ ci\_b0[}\DecValTok{1}\NormalTok{], }\AttributeTok{xmax =}\NormalTok{ ci\_b0[}\DecValTok{2}\NormalTok{]),}
                   \AttributeTok{height =} \FloatTok{0.1}\NormalTok{,}
                   \AttributeTok{color =} \StringTok{"blue"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Bonferroni Confidence Intervals and Joint Confidence Region"}\NormalTok{,}
         \AttributeTok{x =} \StringTok{"Intercept (beta\_0)"}\NormalTok{,}
         \AttributeTok{y =} \StringTok{"Slope (beta\_1)"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{theme\_minimal}\NormalTok{()}

\FunctionTok{print}\NormalTok{(p)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{05-linear-regression_files/figure-latex/unnamed-chunk-1-1} \end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The red point represents the estimated coefficients (b0\_hat, b1\_hat).
\item
  The blue lines represent the Bonferroni-adjusted confidence intervals for beta\_0 and beta\_1.
\item
  The grey points represent the joint confidence region based on the covariance matrix of coefficients.
\item
  The Bonferroni intervals ensure family-wise confidence level but are conservative.
\item
  Simulation results demonstrate how often the true values are captured in the intervals when repeated samples are drawn.
\end{enumerate}

\textbf{Notes}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Conservatism of Bonferroni Intervals

  \begin{itemize}
  \tightlist
  \item
    The \textbf{Bonferroni interval is conservative}:

    \begin{itemize}
    \tightlist
    \item
      The joint confidence level is a lower bound, ensuring family-wise coverage of at least \((1-\alpha)100\%\).
    \item
      This conservatism results in wider intervals, reducing the statistical power of the test.
    \end{itemize}
  \item
    Adjustments for Conservatism:

    \begin{itemize}
    \tightlist
    \item
      Practitioners often choose a larger \(\alpha\) (e.g., \(\alpha = 0.1\)) to reduce the width of the intervals in Bonferroni joint tests.
    \item
      A higher \(\alpha\) allows for a better balance between confidence and precision, especially for exploratory analyses.
    \end{itemize}
  \end{itemize}
\item
  \textbf{Extending Bonferroni to Multiple Parameters}: The Bonferroni method is not limited to two parameters. For testing \(g\) parameters, such as \(\beta_0, \beta_1, \dots, \beta_{g-1}\):

  \begin{itemize}
  \tightlist
  \item
    \textbf{Adjusted Confidence Level for Each Parameter:}

    \begin{itemize}
    \tightlist
    \item
      The confidence level for each individual parameter is \((1-\alpha/g)\).
    \end{itemize}
  \item
    \textbf{Critical} \(t\)-Value:

    \begin{itemize}
    \tightlist
    \item
      For two-sided intervals, the critical value for each parameter is: \[
      t_{1-\frac{\alpha}{2g}; n-p},
      \] where \(p\) is the total number of parameters in the regression model.
    \end{itemize}
  \item
    \textbf{Example:}

    \begin{itemize}
    \tightlist
    \item
      If \(\alpha = 0.05\) and \(g = 10\), each individual confidence interval is constructed at the: \[
      (1 - \frac{0.05}{10}) = 99.5\% \text{ confidence level}.
      \]
    \item
      This corresponds to using \(t_{1-\frac{0.005}{2}; n-p}\) in the formula for the confidence intervals.
    \end{itemize}
  \end{itemize}
\item
  \textbf{Limitations} for Large \(g\)

  \begin{itemize}
  \tightlist
  \item
    \textbf{Wide Intervals:}

    \begin{itemize}
    \tightlist
    \item
      As \(g\) increases, the intervals become excessively wide, often leading to reduced usefulness in practical applications.
    \item
      This issue stems from the conservatism of the Bonferroni method, which prioritizes family-wise error control.
    \end{itemize}
  \item
    \textbf{Suitability for Small} \(g\):

    \begin{itemize}
    \tightlist
    \item
      The Bonferroni procedure works well when \(g\) is relatively small (e.g., \(g \leq 5\)).
    \item
      For larger \(g\), alternative methods (discussed below) are more efficient.
    \end{itemize}
  \end{itemize}
\item
  Correlation Between Parameters: Correlation of \(b_0\) and \(b_1\):

  \begin{itemize}
  \tightlist
  \item
    The estimated regression coefficients \(b_0\) and \(b_1\) are often correlated:

    \begin{itemize}
    \tightlist
    \item
      \textbf{Negative correlation} if \(\bar{X} > 0\).
    \item
      \textbf{Positive correlation} if \(\bar{X} < 0\).
    \end{itemize}
  \item
    This correlation can complicate joint inference but does not affect the validity of Bonferroni-adjusted intervals.
  \end{itemize}
\item
  Alternatives to Bonferroni
\end{enumerate}

Several alternative procedures provide more precise joint inference, especially for larger \(g\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Scheff's Method:}

  \begin{itemize}
  \tightlist
  \item
    Constructs simultaneous confidence regions for all possible linear combinations of parameters.
  \item
    Suitable for exploratory analyses but may result in even wider intervals than Bonferroni.
  \end{itemize}
\item
  \textbf{Tukey's Honest Significant Difference:}

  \begin{itemize}
  \tightlist
  \item
    Designed for pairwise comparisons in ANOVA but can be adapted for regression parameters.
  \end{itemize}
\item
  \textbf{Holm's Step-Down Procedure:}

  \begin{itemize}
  \tightlist
  \item
    A sequential testing procedure that is less conservative than Bonferroni while still controlling the family-wise error rate.
  \end{itemize}
\item
  \textbf{Likelihood Ratio Tests:}

  \begin{itemize}
  \tightlist
  \item
    Construct joint confidence regions based on the likelihood function, offering more precision for large \(g\).
  \end{itemize}
\end{enumerate}

\hypertarget{assumptions-of-linear-regression}{%
\subsubsection{Assumptions of Linear Regression}\label{assumptions-of-linear-regression}}

To ensure valid inference and reliable predictions in linear regression, the following assumptions must hold. We'll cover them in depth in the next section.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.6667}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Assumption}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Description}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Linearity} & Linear relationship between predictors and response. \\
\textbf{Independence of Errors} & Errors are independent (important in time-series/clustering). \\
\textbf{Homoscedasticity} & Constant variance of residuals across predictors. \\
\textbf{Normality of Errors} & Residuals are normally distributed. \\
\textbf{No Multicollinearity} & Predictors are not highly correlated. \\
\textbf{No Outliers/Leverage Points} & No undue influence from outliers or high-leverage points. \\
\textbf{Exogeneity} & Predictors are uncorrelated with the error term (no endogeneity). \\
\textbf{Full Rank} & Predictors are linearly independent (no perfect multicollinearity). \\
\end{longtable}

\hypertarget{diagnostics-for-model-assumptions}{%
\subsubsection{Diagnostics for Model Assumptions}\label{diagnostics-for-model-assumptions}}

\textbf{Constant Variance}

\begin{itemize}
\tightlist
\item
  To check homoscedasticity:

  \begin{itemize}
  \tightlist
  \item
    Plot residuals vs.~fitted values or residuals vs.~predictors.
  \item
    Look for patterns or a funnel-shaped spread indicating heteroscedasticity.
  \end{itemize}
\end{itemize}

\textbf{Outliers}

\begin{itemize}
\tightlist
\item
  Detect outliers using:

  \begin{itemize}
  \tightlist
  \item
    Residuals vs.~predictors plot.
  \item
    Box plots.
  \item
    Stem-and-leaf plots.
  \item
    Scatter plots.
  \end{itemize}
\end{itemize}

\textbf{Standardized Residuals}:

Residuals can be standardized to have unit variance, known as \textbf{studentized residuals}: \[
  r_i = \frac{e_i}{s(e_i)}.
  \]

\textbf{Semi-Studentized Residuals}:

A simplified standardization using the mean squared error (MSE): \[
  e_i^* = \frac{e_i}{\sqrt{MSE}}.
  \]

\textbf{Non-Independent Error Terms}

\begin{itemize}
\tightlist
\item
  To detect non-independence:

  \begin{itemize}
  \tightlist
  \item
    Plot residuals vs.~time for time-series data.
  \item
    Residuals \(e_i\) are not independent because they depend on \(\hat{Y}_i\), which is derived from the same regression function.
  \end{itemize}
\item
  Detect dependency by plotting the residual for the \(i\)-th response vs.~the \((i-1)\)-th.
\end{itemize}

\textbf{Non-Normality of Error Terms}

\begin{itemize}
\tightlist
\item
  To assess normality:

  \begin{itemize}
  \tightlist
  \item
    Plot distribution of residuals.
  \item
    Create box plots, stem-and-leaf plots, or normal probability plots.
  \end{itemize}
\item
  Issues such as an incorrect regression function or non-constant error variance can distort the residual distribution.
\item
  Normality tests require relatively large sample sizes to detect deviations.
\end{itemize}

\textbf{Normality of Residuals}

\begin{itemize}
\tightlist
\item
  Use tests based on the empirical cumulative distribution function (ECDF) (check \protect\hyperlink{normality-assessment}{Normality Assessment})
\end{itemize}

\textbf{Constancy of Error Variance}

\begin{itemize}
\tightlist
\item
  Statistical tests for homoscedasticity:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Brown-Forsythe Test (Modified Levene Test)}:

    \begin{itemize}
    \tightlist
    \item
      Robust against non-normality, examines the variance of residuals across levels of predictors.
    \end{itemize}
  \item
    \textbf{Breusch-Pagan Test (Cook-Weisberg Test)}:

    \begin{itemize}
    \tightlist
    \item
      Tests for heteroscedasticity by regressing squared residuals on predictors.
    \end{itemize}
  \end{itemize}
\end{itemize}

\hypertarget{remedial-measures-for-violations-of-assumptions}{%
\subsubsection{Remedial Measures for Violations of Assumptions}\label{remedial-measures-for-violations-of-assumptions}}

When the assumptions of simple linear regression are violated, appropriate remedial measures can be applied to address these issues. Below is a list of measures for specific deviations from the assumptions.

\hypertarget{general-remedies}{%
\paragraph{General Remedies}\label{general-remedies}}

\begin{itemize}
\tightlist
\item
  Use more \textbf{complicated models} (e.g., non-linear models, generalized linear models).
\item
  Apply \textbf{transformations} (see \protect\hyperlink{variable-transformation}{Variable Transformation}) on \(X\) and/or \(Y\) to stabilize variance, linearize relationships, or normalize residuals. Note that transformations may not always yield ``optimal'' results.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{specific-remedies-for-assumption-violations}{%
\paragraph{Specific Remedies for Assumption Violations}\label{specific-remedies-for-assumption-violations}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2222}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3611}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4167}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Issue}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Remedy}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Explanation}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Non-Linearity} & - Apply transformations (e.g., log, square root). & Transformation of variables can help linearize the relationship between \(X\) and \(Y\). \\
& - Use more complicated models (e.g., polynomial regression, splines). & Higher-order terms or non-linear models can capture non-linear relationships. \\
\textbf{Non-Constant Error Variance} & - Apply \textbf{Weighted Least Squares}. & WLS assigns weights to observations based on the inverse of their variance. \\
& - Use transformations (e.g., log, square root). & Transformations can stabilize error variance. \\
\textbf{Correlated Errors} & - Use serially correlated error models (e.g., ARIMA for time-series data). & Time-series models account for serial dependence in the errors. \\
\textbf{Non-Normality of Errors} & - Transform \(Y\) or use non-parametric methods. & Transformations can normalize residuals; non-parametric methods do not assume normality. \\
\textbf{Omitted Variables} & - Use multiple regression to include additional relevant predictors. & Adding relevant variables reduces omitted variable bias and improves model accuracy. \\
\textbf{Outliers} & - Apply robust estimation techniques (e.g., Huber regression, M-estimation). & Robust methods reduce the influence of outliers on parameter estimates. \\
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{remedies-in-detail}{%
\paragraph{Remedies in Detail}\label{remedies-in-detail}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Non-Linearity:}

  \begin{itemize}
  \tightlist
  \item
    Transformations: Apply transformations to the response variable \(Y\) or the predictor variable \(X\). Common transformations include:

    \begin{itemize}
    \tightlist
    \item
      Logarithmic transformation: \(Y' = \log(Y)\) or \(X' = \log(X)\).
    \item
      Polynomial terms: Include \(X^2\), \(X^3\), etc., to capture curvature.
    \end{itemize}
  \item
    Alternative Models:

    \begin{itemize}
    \tightlist
    \item
      Polynomial regression or splines for flexibility in modeling non-linear relationships.
    \end{itemize}
  \end{itemize}
\item
  \textbf{Non-Constant Error Variance:}

  \begin{itemize}
  \tightlist
  \item
    Weighted Least Squares:

    \begin{itemize}
    \tightlist
    \item
      Assigns weights to observations inversely proportional to their variance.
    \end{itemize}
  \item
    Transformations:

    \begin{itemize}
    \tightlist
    \item
      Use a log or square root transformation to stabilize variance.
    \end{itemize}
  \end{itemize}
\item
  \textbf{Correlated Errors:}

  \begin{itemize}
  \tightlist
  \item
    For time-series data:

    \begin{itemize}
    \tightlist
    \item
      Use serially correlated error models such as AR(1) or ARIMA.
    \item
      These models explicitly account for dependency in residuals over time.
    \end{itemize}
  \end{itemize}
\item
  \textbf{Non-Normality:}

  \begin{itemize}
  \tightlist
  \item
    Transformations:

    \begin{itemize}
    \tightlist
    \item
      Apply a transformation to \(Y\) (e.g., log or square root) to make the residuals approximately normal.
    \end{itemize}
  \item
    Non-parametric regression:

    \begin{itemize}
    \tightlist
    \item
      Methods like LOESS or Theil-Sen regression do not require the normality assumption.
    \end{itemize}
  \end{itemize}
\item
  \textbf{Omitted Variables:}

  \begin{itemize}
  \tightlist
  \item
    Introduce additional predictors:

    \begin{itemize}
    \tightlist
    \item
      Use multiple regression to include all relevant independent variables.
    \end{itemize}
  \item
    Check for multicollinearity when adding new variables.
  \end{itemize}
\item
  \textbf{Outliers:}

  \begin{itemize}
  \tightlist
  \item
    Robust Regression:

    \begin{itemize}
    \tightlist
    \item
      Use methods such as Huber regression or M-estimation to reduce the impact of outliers on model coefficients.
    \end{itemize}
  \item
    Diagnostics:

    \begin{itemize}
    \tightlist
    \item
      Identify outliers using Cook's Distance, leverage statistics, or studentized residuals.
    \end{itemize}
  \end{itemize}
\end{enumerate}

\hypertarget{transformations-in-regression-analysis}{%
\subsubsection{Transformations in Regression Analysis}\label{transformations-in-regression-analysis}}

Transformations involve modifying one or both variables to address issues such as non-linearity, non-constant variance, or non-normality. However, it's important to note that the properties of least-squares estimates apply to the \textbf{transformed model}, not the original variables.

When transforming the dependent variable \(Y\), we fit the model as:

\[
g(Y_i) = b_0 + b_1 X_i,
\]

where \(g(Y_i)\) is the transformed response. To interpret the regression results in terms of the original \(Y\), we need to \textbf{transform back}:

\[
\hat{Y}_i = g^{-1}(b_0 + b_1 X_i).
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Direct back-transformation of predictions can introduce \textbf{bias}. For example, in a log-transformed model:

\[
\log(Y_i) = b_0 + b_1 X_i,
\]

the unbiased back-transformed prediction of \(Y_i\) is:

\[
\hat{Y}_i = \exp(b_0 + b_1 X_i + \frac{\sigma^2}{2}),
\]

where \(\frac{\sigma^2}{2}\) accounts for the bias correction due to the log transformation.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{box-cox-family-of-transformations}{%
\paragraph{Box-Cox Family of Transformations}\label{box-cox-family-of-transformations}}

The \textbf{Box-Cox transformation} is a versatile family of transformations defined as:

\[
Y' =
\begin{cases}
\frac{Y^\lambda - 1}{\lambda}, & \text{if } \lambda \neq 0, \\
\ln(Y), & \text{if } \lambda = 0.
\end{cases}
\]

This transformation introduces a parameter \(\lambda\) that is estimated from the data. Common transformations include:

\begin{longtable}[]{@{}cc@{}}
\toprule\noalign{}
\(\lambda\) & Transformation \(Y'\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
2 & \(Y^2\) \\
0.5 & \(\sqrt{Y}\) \\
0 & \(\ln(Y)\) \\
-0.5 & \(1/\sqrt{Y}\) \\
-1 & \(1/Y\) \\
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Choosing the Transformation Parameter \(\lambda\)

The value of \(\lambda\) can be selected using one of the following methods:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Trial and Error}:

  \begin{itemize}
  \tightlist
  \item
    Apply different transformations and compare the residual plots or model fit statistics (e.g., \(R^2\) or AIC).
  \end{itemize}
\item
  \textbf{Maximum Likelihood Estimation}:

  \begin{itemize}
  \tightlist
  \item
    Choose \(\lambda\) to maximize the likelihood function under the assumption of normally distributed errors.
  \end{itemize}
\item
  \textbf{Numerical Search}:

  \begin{itemize}
  \tightlist
  \item
    Use computational optimization techniques to minimize the residual sum of squares (RSS) or another goodness-of-fit criterion.
  \end{itemize}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Install and load the necessary library}
\ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(}\StringTok{"MASS"}\NormalTok{)) }\FunctionTok{install.packages}\NormalTok{(}\StringTok{"MASS"}\NormalTok{)}
\FunctionTok{library}\NormalTok{(MASS)}

\CommentTok{\# Fit a linear model}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{50}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =} \DecValTok{5}\NormalTok{, }\AttributeTok{sd =} \DecValTok{2}\NormalTok{)}
\NormalTok{y }\OtherTok{\textless{}{-}} \DecValTok{3} \SpecialCharTok{+} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ x }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{2}\NormalTok{)}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x)}

\CommentTok{\# Apply Box{-}Cox Transformation}
\NormalTok{boxcox\_result }\OtherTok{\textless{}{-}} \FunctionTok{boxcox}\NormalTok{(model, }\AttributeTok{lambda =} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\FloatTok{0.1}\NormalTok{), }\AttributeTok{plotit =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{05-linear-regression_files/figure-latex/unnamed-chunk-2-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Find the optimal lambda}
\NormalTok{optimal\_lambda }\OtherTok{\textless{}{-}}\NormalTok{ boxcox\_result}\SpecialCharTok{$}\NormalTok{x[}\FunctionTok{which.max}\NormalTok{(boxcox\_result}\SpecialCharTok{$}\NormalTok{y)]}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Optimal lambda for Box{-}Cox transformation:"}\NormalTok{, optimal\_lambda, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Optimal lambda for Box{-}Cox transformation: 0.8686869}
\end{Highlighting}
\end{Shaded}

\textbf{Notes}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Benefits of Transformations}:

  \begin{itemize}
  \item
    \textbf{Stabilize Variance}: Helps address heteroscedasticity.
  \item
    \textbf{Linearize Relationships}: Useful for non-linear data.
  \item
    \textbf{Normalize Residuals}: Addresses non-normality issues.
  \end{itemize}
\item
  \textbf{Caveats}:

  \begin{itemize}
  \item
    Interpretability: Transformed variables may complicate interpretation.
  \item
    Over-Transformation: Excessive transformations can distort the relationship between variables.
  \end{itemize}
\item
  \textbf{Applicability}:

  \begin{itemize}
  \tightlist
  \item
    Transformations are most effective for issues like non-linearity or non-constant variance. They are less effective for correcting independence violations or omitted variables.
  \end{itemize}
\end{enumerate}

\hypertarget{variance-stabilizing-transformations}{%
\paragraph{Variance Stabilizing Transformations}\label{variance-stabilizing-transformations}}

Variance stabilizing transformations are used when the standard deviation of the response variable depends on its mean. The \textbf{delta method}, which applies a Taylor series expansion, provides a systematic approach to find such transformations.

Given that the standard deviation of \(Y\) is a function of its mean:

\[
\sigma = \sqrt{\text{var}(Y)} = f(\mu),
\]

where \(\mu = E(Y)\) and \(f(\mu)\) is a smooth function of the mean, we aim to find a transformation \(h(Y)\) such that the variance of the transformed variable \(h(Y)\) is constant for all values of \(\mu\).

Expanding \(h(Y)\) in a \protect\hyperlink{taylor-expansion}{Taylor Expansion} series around \(\mu\):

\[
h(Y) = h(\mu) + h'(\mu)(Y - \mu) + \text{higher-order terms}.
\]

Ignoring higher-order terms, the variance of \(h(Y)\) can be approximated as:

\[
\text{var}(h(Y)) = \text{var}(h(\mu) + h'(\mu)(Y - \mu)).
\]

Since \(h(\mu)\) is a constant:

\[
\text{var}(h(Y)) = \left(h'(\mu)\right)^2 \text{var}(Y).
\]

Substituting \(\text{var}(Y) = \left(f(\mu)\right)^2\), we get:

\[
\text{var}(h(Y)) = \left(h'(\mu)\right)^2 \left(f(\mu)\right)^2.
\]

To stabilize the variance (make it constant for all \(\mu\)), we require:

\[
\left(h'(\mu)\right)^2 \left(f(\mu)\right)^2 = \text{constant}.
\]

Thus, the derivative of \(h(\mu)\) must be proportional to the inverse of \(f(\mu)\):

\[
h'(\mu) \propto \frac{1}{f(\mu)}.
\]

Integrating both sides gives:

\[
h(\mu) = \int \frac{1}{f(\mu)} \, d\mu.
\]

The specific form of \(h(\mu)\) depends on the function \(f(\mu)\), which describes the relationship between the standard deviation and the mean.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Examples of Variance Stabilizing Transformations

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2083}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4583}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\(f(\mu)\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Transformation} \(h(Y)\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Purpose}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(\sqrt{\mu}\) & \(\int \frac{1}{\sqrt{\mu}} d\mu = 2\sqrt{Y}\) & Stabilizes variance for Poisson data. \\
\(\mu\) & \(\int \frac{1}{\mu} d\mu = \ln(Y)\) & Stabilizes variance for exponential or multiplicative models. \\
\(\mu^2\) & \(\int \frac{1}{\mu^2} d\mu = -\frac{1}{Y}\) & Stabilizes variance for certain power law data. \\
\end{longtable}

Variance stabilizing transformations are particularly useful for:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Poisson-distributed data}: Use \(h(Y) = 2\sqrt{Y}\) to stabilize variance.
\item
  \textbf{Exponential or multiplicative models}: Use \(h(Y) = \ln(Y)\) for stabilization.
\item
  \textbf{Power law relationships}: Use transformations like \(h(Y) = Y^{-1}\) or other forms derived from \(f(\mu)\).
\end{enumerate}

Example: Variance Stabilizing Transformation for the Poisson Distribution

For a \textbf{Poisson distribution}, the variance of \(Y\) is equal to its mean:

\[
\sigma^2 = \text{var}(Y) = E(Y) = \mu.
\]

Thus, the standard deviation is:

\[
\sigma = f(\mu) = \sqrt{\mu}.
\]

Using the relationship for variance stabilizing transformations:

\[
h'(\mu) \propto \frac{1}{f(\mu)} = \mu^{-0.5}.
\]

Integrating \(h'(\mu)\) gives the variance stabilizing transformation:

\[
h(\mu) = \int \mu^{-0.5} \, d\mu = 2\sqrt{\mu}.
\]

Hence, the variance stabilizing transformation is:

\[
h(Y) = \sqrt{Y}.
\]

This transformation is widely used in Poisson regression to stabilize the variance of the response variable.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Simulate Poisson data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{500}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =} \DecValTok{5}\NormalTok{, }\AttributeTok{sd =} \DecValTok{2}\NormalTok{)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{rpois}\NormalTok{(n, }\AttributeTok{lambda =} \FunctionTok{exp}\NormalTok{(}\DecValTok{1} \SpecialCharTok{+} \FloatTok{0.3} \SpecialCharTok{*}\NormalTok{ x))  }\CommentTok{\# Poisson{-}distributed Y}

\CommentTok{\# Fit linear model without transformation}
\NormalTok{model\_raw }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x)}

\CommentTok{\# Apply square root transformation}
\NormalTok{y\_trans }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(y)}
\NormalTok{model\_trans }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y\_trans }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x)}

\CommentTok{\# Compare residual plots}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\CommentTok{\# Residual plot for raw data}
\FunctionTok{plot}\NormalTok{(}
    \FunctionTok{fitted}\NormalTok{(model\_raw),}
    \FunctionTok{resid}\NormalTok{(model\_raw),}
    \AttributeTok{main =} \StringTok{"Residuals: Raw Data"}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"Fitted Values"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"Residuals"}
\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{h =} \DecValTok{0}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{)}

\CommentTok{\# Residual plot for transformed data}
\FunctionTok{plot}\NormalTok{(}
    \FunctionTok{fitted}\NormalTok{(model\_trans),}
    \FunctionTok{resid}\NormalTok{(model\_trans),}
    \AttributeTok{main =} \StringTok{"Residuals: Transformed Data (sqrt(Y))"}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"Fitted Values"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"Residuals"}
\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{h =} \DecValTok{0}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{05-linear-regression_files/figure-latex/unnamed-chunk-3-1} \end{center}

\hypertarget{general-strategy-when-fmu-is-unknown}{%
\paragraph{\texorpdfstring{General Strategy When \(f(\mu)\) Is Unknown}{General Strategy When f(\textbackslash mu) Is Unknown}}\label{general-strategy-when-fmu-is-unknown}}

If the relationship between \(\text{var}(Y)\) and \(\mu\) (i.e., \(f(\mu)\)) is unknown, the following steps can help:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Trial and Error}:

  \begin{itemize}
  \tightlist
  \item
    Apply common transformations (e.g., \(\log(Y)\), \(\sqrt{Y}\)) and examine residual plots.
  \item
    Select the transformation that results in stabilized variance (residuals show no pattern in plots).
  \end{itemize}
\item
  \textbf{Leverage Prior Research}:

  \begin{itemize}
  \tightlist
  \item
    Consult researchers or literature on similar experiments to determine the transformations typically used.
  \end{itemize}
\item
  \textbf{Analyze Observations with the Same Predictor Value}:

  \begin{itemize}
  \tightlist
  \item
    If multiple observations \(Y_{ij}\) are available at the same \(X\) value:

    \begin{itemize}
    \tightlist
    \item
      Compute the mean \(\bar{Y}_i\) and standard deviation \(s_i\) for each group.
    \item
      Check if \(s_i \propto \bar{Y}_i^{\lambda}\).

      \begin{itemize}
      \tightlist
      \item
        For example, assume: \[
        s_i = a \bar{Y}_i^{\lambda}.
        \]
      \item
        Taking the natural logarithm: \[
        \ln(s_i) = \ln(a) + \lambda \ln(\bar{Y}_i).
        \]
      \end{itemize}
    \item
      Perform a regression of \(\ln(s_i)\) on \(\ln(\bar{Y}_i)\) to estimate \(\lambda\) and suggest the form of \(f(\mu)\).
    \end{itemize}
  \end{itemize}
\item
  \textbf{Group Observations}:

  \begin{itemize}
  \tightlist
  \item
    If individual observations are sparse, try grouping similar observations by \(X\) values to compute \(\bar{Y}_i\) and \(s_i\) for each group.
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{common-transformations-and-their-applications}{%
\paragraph{Common Transformations and Their Applications}\label{common-transformations-and-their-applications}}

The table below summarizes common transformations used to stabilize variance under various conditions, along with their appropriate contexts and comments:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2361}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3056}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4583}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Transformation}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Situation}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Comments}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(\sqrt{Y}\) & \(var(\epsilon_i) = k \, E(Y_i)\) & For counts following a Poisson distribution. \\
\(\sqrt{Y} + \sqrt{Y+1}\) & \(var(\epsilon_i) = k \, E(Y_i)\) & Useful for small counts or datasets with zeros. \\
\(\log(Y)\) & \(var(\epsilon_i) = k \, (E(Y_i))^2\) & Appropriate for positive integers with a wide range. \\
\(\log(Y+1)\) & \(var(\epsilon_i) = k \, (E(Y_i))^2\) & Used when the data includes zero counts. \\
\(1/Y\) & \(var(\epsilon_i) = k \, (E(Y_i))^4\) & For responses mostly near zero, with occasional large values. \\
\(\arcsin(\sqrt{Y})\) & \(var(\epsilon_i) = k \, E(Y_i)(1-E(Y_i))\) & Suitable for binomial proportions or percentage data. \\
\end{longtable}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Choosing the Transformation}:

  \begin{itemize}
  \tightlist
  \item
    Start by identifying the relationship between the variance of the residuals (\(var(\epsilon_i)\)) and the mean of the response variable (\(E(Y_i)\)).
  \item
    Select the transformation that matches the identified variance structure.
  \end{itemize}
\item
  \textbf{Transformations for Zero Values}:

  \begin{itemize}
  \tightlist
  \item
    For data with zeros, transformations like \(\sqrt{Y+1}\) or \(\log(Y+1)\) can be used to avoid undefined values. But this will seriously jeopardize model assumption \citep{chen2024logs}.
  \end{itemize}
\item
  \textbf{Use in Regression Models}:

  \begin{itemize}
  \tightlist
  \item
    Apply these transformations to the dependent variable \(Y\) in the regression model.
  \item
    Always check residual plots to confirm that the transformation stabilizes variance and resolves non-linearity.
  \end{itemize}
\item
  \textbf{Interpretation After Transformation}:

  \begin{itemize}
  \tightlist
  \item
    After transforming \(Y\), interpret the results in terms of the transformed variable.
  \item
    For practical interpretation, back-transform predictions and account for any associated bias.
  \end{itemize}
\end{enumerate}

\hypertarget{multiple-linear-regression}{%
\subsection{Multiple Linear Regression}\label{multiple-linear-regression}}

The geometry of least squares regression involves projecting the response vector \(\mathbf{y}\) onto the space spanned by the columns of the design matrix \(\mathbf{X}\). The fitted values \(\mathbf{\hat{y}}\) can be expressed as:

\[
\begin{aligned}
\mathbf{\hat{y}} &= \mathbf{Xb} \\
&= \mathbf{X(X'X)^{-1}X'y} \\
&= \mathbf{Hy},
\end{aligned}
\]

where:

\begin{itemize}
\tightlist
\item
  \(\mathbf{H} = \mathbf{X(X'X)^{-1}X'}\) is the projection operator (sometimes denoted as \(\mathbf{P}\)).
\item
  \(\mathbf{\hat{y}}\) is the projection of \(\mathbf{y}\) onto the linear space spanned by the columns of \(\mathbf{X}\) (the model space).
\end{itemize}

The dimension of the model space is equal to the rank of \(\mathbf{X}\) (i.e., the number of linearly independent columns in \(\mathbf{X}\)).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Properties of the Projection Matrix \(\mathbf{H}\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Symmetry:

  \begin{itemize}
  \tightlist
  \item
    The projection matrix \(\mathbf{H}\) is symmetric: \[
    \mathbf{H} = \mathbf{H}'.
    \]
  \end{itemize}
\item
  Idempotence:

  \begin{itemize}
  \tightlist
  \item
    Applying \(\mathbf{H}\) twice gives the same result: \[
    \mathbf{HH} = \mathbf{H}.
    \] Proof: \[
    \begin{aligned}
    \mathbf{HH} &= \mathbf{X(X'X)^{-1}X'X(X'X)^{-1}X'} \\
    &= \mathbf{X(X'X)^{-1}IX'} \\
    &= \mathbf{X(X'X)^{-1}X'} \\
    &= \mathbf{H}.
    \end{aligned}
    \]
  \end{itemize}
\item
  Dimensionality:

  \begin{itemize}
  \tightlist
  \item
    \(\mathbf{H}\) is an \(n \times n\) matrix (where \(n\) is the number of observations).
  \item
    The rank of \(\mathbf{H}\) is equal to the rank of \(\mathbf{X}\), which is typically the number of predictors (including the intercept).
  \end{itemize}
\item
  Orthogonal Complement:

  \begin{itemize}
  \tightlist
  \item
    The matrix \(\mathbf{(I - H)}\), where: \[
    \mathbf{I - H} = \mathbf{I - X(X'X)^{-1}X'},
    \] is also a projection operator.
  \item
    It projects onto the orthogonal complement of the space spanned by the columns of \(\mathbf{X}\) (i.e., the space orthogonal to the model space).
  \end{itemize}
\item
  Orthogonality of Projections:

  \begin{itemize}
  \tightlist
  \item
    \(\mathbf{H}\) and \(\mathbf{(I - H)}\) are orthogonal: \[
    \mathbf{H(I - H)} = \mathbf{0}.
    \]
  \item
    Similarly: \[
    \mathbf{(I - H)H} = \mathbf{0}.
    \]
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Intuition for \(\mathbf{H}\) and \(\mathbf{(I - H)}\)

\begin{itemize}
\tightlist
\item
  \(\mathbf{H}\): Projects \(\mathbf{y}\) onto the model space, giving the fitted values \(\mathbf{\hat{y}}\).
\item
  \(\mathbf{I - H}\): Projects \(\mathbf{y}\) onto the residual space, giving the residuals \(\mathbf{e}\): \[
  \mathbf{e} = \mathbf{(I - H)y}.
  \]
\item
  \(\mathbf{H}\) and \(\mathbf{(I - H)}\) divide the response vector \(\mathbf{y}\) into two components: \[
  \mathbf{y} = \mathbf{\hat{y}} + \mathbf{e}.
  \]

  \begin{itemize}
  \tightlist
  \item
    \(\mathbf{\hat{y}} = \mathbf{Hy}\) (fitted values).
  \item
    \(\mathbf{e} = \mathbf{(I - H)y}\) (residuals).
  \end{itemize}
\item
  The properties of \(\mathbf{H}\) (symmetry, idempotence, dimensionality) reflect its role as a linear transformation that projects vectors onto the model space.
\end{itemize}

This geometric perspective provides insight into the mechanics of least squares regression, particularly how the response variable \(\mathbf{y}\) is decomposed into fitted values and residuals.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Similar to simple regression, the total sum of squares in multiple regression analysis can be partitioned into components corresponding to the regression (model fit) and the residuals (errors).

The uncorrected total sum of squares is:

\[
\mathbf{y'y} = \mathbf{\hat{y}'\hat{y} + e'e},
\]

where:

\begin{itemize}
\item
  \(\mathbf{\hat{y} = Hy}\) (fitted values, projected onto the model space).
\item
  \(\mathbf{e = (I - H)y}\) (residuals, projected onto the orthogonal complement of the model space).
\end{itemize}

Expanding this using projection matrices:

\[
\begin{aligned}
\mathbf{y'y} &= \mathbf{(Hy)'(Hy) + ((I-H)y)'((I-H)y)} \\
&= \mathbf{y'H'Hy + y'(I-H)'(I-H)y} \\
&= \mathbf{y'Hy + y'(I-H)y}.
\end{aligned}
\]

This equation shows the partition of \(\mathbf{y'y}\) into components explained by the model (\(\mathbf{\hat{y}}\)) and the unexplained variation (residuals).

For the corrected total sum of squares, we adjust for the mean (using the projection matrix \(\mathbf{H_1}\)):

\[
\mathbf{y'(I-H_1)y = y'(H-H_1)y + y'(I-H)y}.
\]

Here:

\begin{itemize}
\item
  \(\mathbf{H_1} = \frac{1}{n} \mathbf{J}\), where \(\mathbf{J}\) is an \(n \times n\) matrix of ones.
\item
  \(\mathbf{H - H_1}\) projects onto the subspace explained by the predictors after centering.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1272}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3988}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4682}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Uncorrected Total Sum of Squares (\(\mathbf{y'y}\))
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Corrected Total Sum of Squares (\(\mathbf{y'(I-H_1)y}\))
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Definition & Total variation in \(y\) relative to the origin. & Total variation in \(y\) relative to its mean (centered data). \\
Adjustment & No adjustment for the mean of \(y\). & Adjusts for the mean of \(y\) by centering it. \\
Equation & \(\mathbf{y'y} = \mathbf{\hat{y}'\hat{y}} + \mathbf{e'e}\) & \(\mathbf{y'(I-H_1)y} = \mathbf{y'(H-H_1)y} + \mathbf{y'(I-H)y}\) \\
Projection Matrices & \(\mathbf{H}\): Projects onto model space.

\(\mathbf{I-H}\): Projects onto residuals. & \(\mathbf{H_1} = \frac{1}{n} \mathbf{J}\): Adjusts for the mean.

\(\mathbf{H-H_1}\): Projects onto predictors after centering.

\(\mathbf{I-H}\): Projects onto residuals. \\
Interpretation & Includes variation due to the mean of \(y\). & Focuses on variation in \(y\) around its mean. \\
Usage & Suitable for raw, uncentered data. & Common in regression and ANOVA to isolate variability explained by predictors. \\
Application & Measures total variability in \(y\), including overall level (mean). & Measures variability explained by predictors relative to the mean. \\
\end{longtable}

Why the Correction Matters

\begin{itemize}
\item
  In ANOVA and regression, removing the contribution of the mean helps isolate the variability explained by predictors from the overall level of the response variable.
\item
  Corrected sums of squares are more common when comparing models or computing \(R^2\), which requires centering to ensure consistency in proportionate variance explained.
\end{itemize}

The corrected total sum of squares can be decomposed into the sum of squares for regression (SSR) and the sum of squares for error (SSE):

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1238}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.4000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1048}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2095}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Source
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SS
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
df
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MS
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
F
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Regression & \(SSR = \mathbf{y'(H - \frac{1}{n} J)y}\) & \(p - 1\) & \(MSR = SSR / (p-1)\) & \(MSR / MSE\) \\
Error & \(SSE = \mathbf{y'(I - H)y}\) & \(n - p\) & \(MSE = SSE / (n-p)\) & \\
Total & \(SST = \mathbf{y'(I - H_1)y}\) & \(n - 1\) & & \\
\end{longtable}

Where:

\begin{itemize}
\item
  \(p\): Number of parameters (including intercept).
\item
  \(n\): Number of observations.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Alternatively, the regression model can be expressed as:

\[
\mathbf{Y = X\hat{\beta} + (Y - X\hat{\beta})},
\]

where:

\begin{itemize}
\item
  \(\mathbf{\hat{Y} = X\hat{\beta}}\): Vector of fitted values (in the subspace spanned by \(\mathbf{X}\)).
\item
  \(\mathbf{e = Y - X\hat{\beta}}\): Vector of residuals (in the orthogonal complement of the subspace spanned by \(\mathbf{X}\)).
\item
  \(\mathbf{Y}\) is an \(n \times 1\) vector in the \(n\)-dimensional space \(\mathbb{R}^n\).
\item
  \(\mathbf{X}\) is an \(n \times p\) full-rank matrix, with its columns generating a \(p\)-dimensional subspace of \(\mathbb{R}^n\). Hence, any estimator \(\mathbf{X\hat{\beta}}\) is also in this subspace.
\end{itemize}

In linear regression, the Ordinary Least Squares estimator \(\hat{\beta}\) minimizes the squared Euclidean distance \(\|\mathbf{Y} - \mathbf{X}\beta\|^2\) between the observed response vector \(\mathbf{Y}\) and the fitted values \(\mathbf{X}\beta\). This minimization corresponds to the orthogonal projection of \(\mathbf{Y}\) onto the column space of \(\mathbf{X}\).

We solve the optimization problem:

\[
\min_{\beta} \|\mathbf{Y} - \mathbf{X}\beta\|^2.
\]

The objective function can be expanded as:

\[
\|\mathbf{Y} - \mathbf{X}\beta\|^2 
= (\mathbf{Y} - \mathbf{X}\beta)^\top (\mathbf{Y} - \mathbf{X}\beta).
\]

Perform the multiplication:

\[
\begin{aligned}
(\mathbf{Y} - \mathbf{X}\beta)^\top (\mathbf{Y} - \mathbf{X}\beta) 
&= \mathbf{Y}^\top \mathbf{Y} 
- \mathbf{Y}^\top \mathbf{X}\beta 
- \beta^\top \mathbf{X}^\top \mathbf{Y} 
+ \beta^\top (\mathbf{X}^\top \mathbf{X}) \beta.
\end{aligned}
\]

Since \(\mathbf{Y}^\top \mathbf{X}\beta\) is a scalar, it equals \(\beta^\top \mathbf{X}^\top \mathbf{Y}\). Therefore, the expanded expression becomes:

\[
\|\mathbf{Y} - \mathbf{X}\beta\|^2 
= \mathbf{Y}^\top \mathbf{Y} 
- 2\beta^\top \mathbf{X}^\top \mathbf{Y} 
+ \beta^\top (\mathbf{X}^\top \mathbf{X}) \beta.
\]

To find the \(\beta\) that minimizes this expression, take the derivative with respect to \(\beta\) and set it to 0:

\[
\frac{\partial}{\partial \beta} 
\Bigl[
  \mathbf{Y}^\top \mathbf{Y} 
  - 2\beta^\top \mathbf{X}^\top \mathbf{Y} 
  + \beta^\top (\mathbf{X}^\top \mathbf{X}) \beta
\Bigr]
= 0.
\]

Computing the gradient:

\[
\frac{\partial}{\partial \beta} = -2\mathbf{X}^\top \mathbf{Y} + 2(\mathbf{X}^\top \mathbf{X})\beta.
\]

Setting this to zero:

\[
-2\mathbf{X}^\top \mathbf{Y} + 2\mathbf{X}^\top \mathbf{X}\beta = 0.
\]

Simplify:

\[
\mathbf{X}^\top \mathbf{X}\beta = \mathbf{X}^\top \mathbf{Y}.
\]

If \(\mathbf{X}^\top \mathbf{X}\) is invertible, the solution is:

\[
\hat{\beta} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{Y}.
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Orthogonal Projection Interpretation

The fitted values are:

\[
\hat{\mathbf{Y}} = \mathbf{X}\hat{\beta}.
\]

From the normal equations, \(\mathbf{X}^\top(\mathbf{Y} - \mathbf{X}\hat{\beta}) = 0\), which implies that the residual vector \(\mathbf{Y} - \hat{\mathbf{Y}}\) is orthogonal to every column of \(\mathbf{X}\). Therefore:

\begin{itemize}
\tightlist
\item
  \(\hat{\mathbf{Y}} = \mathbf{X}\hat{\beta}\) is the orthogonal projection of \(\mathbf{Y}\) onto \(\mathrm{Col}(\mathbf{X})\).
\item
  \(\mathbf{Y} - \mathbf{X}\hat{\beta}\) lies in the orthogonal complement of \(\mathrm{Col}(\mathbf{X})\).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Pythagoras Decomposition

The geometric interpretation gives us the decomposition:

\[
\mathbf{Y} = \mathbf{X}\hat{\beta} + (\mathbf{Y} - \mathbf{X}\hat{\beta}),
\]

where:

\begin{itemize}
\item
  \(\mathbf{X}\hat{\beta}\) is the projection of \(\mathbf{Y}\) onto the column space of \(\mathbf{X}\).
\item
  \((\mathbf{Y} - \mathbf{X}\hat{\beta})\) is the residual vector, orthogonal to \(\mathbf{X}\hat{\beta}\).
\end{itemize}

Since the two components are orthogonal, their squared norms satisfy:

\[
\begin{aligned}\|\mathbf{Y}\|^2 &= \mathbf{Y}^\top \mathbf{Y}&& \text{(definition of norm squared)} \\[6pt]&= (\mathbf{Y} - \mathbf{X}\hat{\beta} + \mathbf{X}\hat{\beta})^\top(\mathbf{Y} - \mathbf{X}\hat{\beta} + \mathbf{X}\hat{\beta})&& \text{(add and subtract the same term } \mathbf{X}\hat{\beta}\text{)} \\[6pt]&= (\mathbf{Y} - \mathbf{X}\hat{\beta})^\top(\mathbf{Y} - \mathbf{X}\hat{\beta})\;+\; 2\,(\mathbf{X}\hat{\beta})^\top(\mathbf{Y} - \mathbf{X}\hat{\beta})\;+\; (\mathbf{X}\hat{\beta})^\top(\mathbf{X}\hat{\beta})&& \text{(expand }(a+b)^\top(a+b)\text{)} \\[6pt]&= \|\mathbf{Y} - \mathbf{X}\hat{\beta}\|^2\;+\; 2\,(\mathbf{X}\hat{\beta})^\top(\mathbf{Y} - \mathbf{X}\hat{\beta})\;+\; \|\mathbf{X}\hat{\beta}\|^2&& \text{(rewrite each quadratic form as a norm)} \\[6pt]&= \|\mathbf{Y} - \mathbf{X}\hat{\beta}\|^2 + \|\mathbf{X}\hat{\beta}\|^2&& \text{(use that }(\mathbf{X}\hat{\beta})^\top(\mathbf{Y}-\mathbf{X}\hat{\beta}) = 0\text{, i.e. orthogonality)} \\[6pt]& \quad = \|\mathbf{X}\hat{\beta}\|^2 \;+\; \|\mathbf{Y} - \mathbf{X}\hat{\beta}\|^2.
\end{aligned}
\]

where the norm of a vector \(\mathbf{a}\) in \(\mathbb{R}^p\) is defined as:

\[
\|\mathbf{a}\| = \sqrt{\mathbf{a}^\top \mathbf{a}} = \sqrt{\sum_{i=1}^p a_i^2}.
\]

We are saying that \(\mathbf{Y}\) is decomposed into two orthogonal components:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \(\mathbf{X}\hat{\beta}\) (the projection onto \(\mathrm{Col}(\mathbf{X})\)

  \begin{itemize}
  \tightlist
  \item
    \(\|\mathbf{X}\hat{\beta}\|\) measures the part of \(\mathbf{Y}\) explained by the model.
  \end{itemize}
\item
  \(\mathbf{Y} - \mathbf{X}\hat{\beta}\) (the residual lying in the orthogonal complement).

  \begin{itemize}
  \tightlist
  \item
    \(\|\mathbf{Y} - \mathbf{X}\hat{\beta}\|\) measures the residual error.
  \end{itemize}
\end{enumerate}

This geometric interpretation (projection plus orthogonal remainder) is exactly why we call \(\mathbf{X}\hat{\beta}\) the \emph{orthogonal projection} of \(\mathbf{Y}\) onto the column space of \(\mathbf{X}\). This decomposition also underlies the analysis of variance (ANOVA) in regression.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The coefficient of multiple determination, denoted \(R^2\), measures the proportion of the total variation in the response variable (\(\mathbf{Y}\)) that is explained by the regression model. It is defined as:

\[
R^2 = \frac{SSR}{SSTO} = 1 - \frac{SSE}{SSTO},
\]

where:

\begin{itemize}
\item
  \(SSR\): Regression sum of squares (variation explained by the model).
\item
  \(SSE\): Error sum of squares (unexplained variation).
\item
  \(SSTO\): Total sum of squares (total variation in \(\mathbf{Y}\)).
\end{itemize}

The adjusted \(R^2\) adjusts \(R^2\) for the number of predictors in the model, penalizing for adding predictors that do not improve the model's fit substantially. It is defined as:

\[
R^2_a = 1 - \frac{SSE/(n-p)}{SSTO/(n-1)} = 1 - \frac{(n-1)SSE}{(n-p)SSTO},
\]

where:

\begin{itemize}
\item
  \(n\): Number of observations.
\item
  \(p\): Number of parameters (including the intercept).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Key Differences Between \(R^2\) and \(R^2_a\)

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.0993}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4449}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4522}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(R^2\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(R^2_a\)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Behavior with Predictors & Always increases (or remains constant) when more predictors are added, even if they are not statistically significant. & Includes a penalty for the number of predictors. May decrease if added predictors do not improve the model sufficiently. \\
Interpretation & Proportion of the total variation in \(\mathbf{Y}\) explained by the regression model. & Adjusted measure of explained variance, accounting for model complexity. \\
Range & Ranges between \(0\) and \(1\). & Can be lower than \(R^2\), particularly when the model includes irrelevant predictors. \\
Usefulness & Useful for understanding the overall fit of the model. & Useful for comparing models with different numbers of predictors. \\
\end{longtable}

In multiple regression, \(R^2_a\) provides a more reliable measure of model fit, especially when comparing models with different numbers of predictors.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

In a regression model with coefficients \(\beta = (\beta_0, \beta_1, \dots, \beta_{p-1})^\top\), the sums of squares are used to evaluate the contribution of predictors to explaining the variation in the response variable.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Model Sums of Squares:

  \begin{itemize}
  \tightlist
  \item
    \(SSM\): Total model sum of squares, capturing the variation explained by all predictors: \[
    SSM = SS(\beta_0, \beta_1, \dots, \beta_{p-1}).
    \]
  \end{itemize}
\item
  Marginal Contribution:

  \begin{itemize}
  \tightlist
  \item
    \(SSM_m\): Conditional model sum of squares, capturing the variation explained by predictors after accounting for others: \[
    SSM_m = SS(\beta_0, \beta_1, \dots, \beta_{p-1} | \beta_0).
    \]
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Decompositions of \(SSM_m\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Sequential Sums of Squares (Type I SS)
\end{enumerate}

\begin{itemize}
\item
  Definition:

  \begin{itemize}
  \tightlist
  \item
    Sequential SS depends on the order in which predictors are added to the model.
  \item
    It represents the additional contribution of each predictor given only the predictors that precede it in the sequence.
  \end{itemize}
\item
  Formula: \[
  SSM_m = SS(\beta_1 | \beta_0) + SS(\beta_2 | \beta_0, \beta_1) + \dots + SS(\beta_{p-1} | \beta_0, \dots, \beta_{p-2}).
  \]
\item
  Key Points:

  \begin{itemize}
  \tightlist
  \item
    Sequential SS is not unique; it depends on the order of the predictors.
  \item
    Default in many statistical software functions (e.g., \texttt{anova()} in R).
  \end{itemize}
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Marginal Sums of Squares (Type II SS)
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Definition:

  \begin{itemize}
  \tightlist
  \item
    Marginal SS evaluates the contribution of a predictor after accounting for all other predictors except those with which it is collinear.
  \item
    It ignores hierarchical relationships or interactions, focusing on independent contributions.
  \end{itemize}
\item
  Formula: \(SSM_m = SS(\beta_j | \beta_1, \dots, \beta_{j-1}, \beta_{j + 1}, \dots, \beta_{p-1})\) where Type II SS evaluates the contribution of \(\beta_j\) while excluding any terms collinear with \(\beta_j\).
\item
  Key Points:

  \begin{itemize}
  \tightlist
  \item
    Type II SS is independent of predictor order.
  \item
    Suitable for models without interaction terms or when predictors are balanced.
  \end{itemize}
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Partial Sums of Squares (Type III SS)
\end{enumerate}

\begin{itemize}
\item
  Definition:

  \begin{itemize}
  \tightlist
  \item
    Partial SS evaluates the contribution of each predictor after accounting for all other predictors in the model.
  \item
    It quantifies the unique contribution of a predictor, controlling for the presence of others.
  \end{itemize}
\item
  Formula: \[
  SSM_m = SS(\beta_1 | \beta_0, \beta_2, \dots, \beta_{p-1}) + \dots + SS(\beta_{p-1} | \beta_0, \beta_1, \dots, \beta_{p-2}).
  \]
\item
  Key Points:

  \begin{itemize}
  \tightlist
  \item
    Partial SS is unique for a given model.
  \item
    More commonly used in practice for assessing individual predictor importance.
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Comparison of Sequential, Marginal, and Partial SS

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.0806}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3128}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3318}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2654}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Sequential SS (Type I)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Marginal SS (Type II)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Partial SS (Type III)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Dependency & Depends on the order in which predictors are entered. & Independent of order; adjusts for non-collinear predictors. & Independent of order; evaluates unique contributions. \\
Usage & Default in software functions like \texttt{anova()} (Type I SS). & Models without interactions or hierarchical dependencies. & Commonly used for hypothesis testing. \\
Interpretation & Measures the additional contribution of predictors in sequence. & Measures the contribution of a predictor, ignoring collinear terms. & Measures the unique contribution of each predictor. \\
Uniqueness & Not unique; changes with predictor order. & Unique for a given model without interactions. & Unique for a given model. \\
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Practical Notes

\begin{itemize}
\tightlist
\item
  Use Type III SS (Partial SS) when:

  \begin{itemize}
  \tightlist
  \item
    The focus is on individual predictor contributions while accounting for all others.
  \item
    Conducting hypothesis tests on predictors in complex models with interactions or hierarchical structures.
  \end{itemize}
\item
  Use Type II SS (Marginal SS) when:

  \begin{itemize}
  \tightlist
  \item
    Working with balanced datasets or models without interaction terms.
  \item
    Ignoring interactions and focusing on independent effects.
  \end{itemize}
\item
  Use Type I SS (Sequential SS) when:

  \begin{itemize}
  \tightlist
  \item
    Interested in understanding the incremental contribution of predictors based on a specific order of entry (e.g., stepwise regression).
  \end{itemize}
\end{itemize}

\hypertarget{ols-assumptions}{%
\subsubsection{OLS Assumptions}\label{ols-assumptions}}

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{a1-linearity}{A1 Linearity}
\item
  \protect\hyperlink{a2-full-rank}{A2 Full Rank}
\item
  \protect\hyperlink{a3-exogeneity-of-independent-variables}{A3 Exogeneity of Independent Variables}
\item
  \protect\hyperlink{a4-homoskedasticity}{A4 Homoskedasticity}
\item
  \protect\hyperlink{a5-data-generation-random-sampling}{A5 Data Generation (Random Sampling)}
\item
  \protect\hyperlink{a6-normal-distribution}{A6 Normal Distribution}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{a1-linearity}{%
\paragraph{A1 Linearity}\label{a1-linearity}}

The linear regression model is expressed as:

\[
A1: y = \mathbf{x}\beta + \epsilon
\]

This assumption is not restrictive since \(x\) can include nonlinear transformations (e.g., interactions, natural logarithms, quadratic terms).

However, when combined with A3 (Exogeneity of Independent Variables), linearity can become restrictive.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{log-model-variants}{%
\subparagraph{Log Model Variants}\label{log-model-variants}}

Logarithmic transformations of variables allow for flexible modeling of nonlinear relationships. Common log model forms include:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.0791}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2768}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2260}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.4068}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Model
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Form
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Interpretation of \(\beta\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
In Words
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Level-Level & \(y = \beta_0 + \beta_1x + \epsilon\) & \(\Delta y = \beta_1 \Delta x\) & A unit change in \(x\) results in a \(\beta_1\) unit change in \(y\). \\
Log-Level & \(\ln(y) = \beta_0 + \beta_1x + \epsilon\) & \(\% \Delta y = 100 \beta_1 \Delta x\) & A unit change in \(x\) results in a \(100 \beta_1 \%\) change in \(y\). \\
Level-Log & \(y = \beta_0 + \beta_1 \ln(x) + \epsilon\) & \(\Delta y = (\beta_1/100)\% \Delta x\) & A 1\% change in \(x\) results in a \((\beta_1 / 100)\) unit change in \(y\). \\
Log-Log & \(\ln(y) = \beta_0 + \beta_1 \ln(x) + \epsilon\) & \(\% \Delta y = \beta_1 \% \Delta x\) & A 1\% change in \(x\) results in a \(\beta_1 \%\) change in \(y\). \\
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{higher-order-models}{%
\subparagraph{Higher-Order Models}\label{higher-order-models}}

Higher-order terms allow the effect of \(x_1\) on \(y\) to depend on the level of \(x_1\). For example:

\[
y = \beta_0 + x_1\beta_1 + x_1^2\beta_2 + \epsilon
\]

The partial derivative of \(y\) with respect to \(x_1\) is:

\[
\frac{\partial y}{\partial x_1} = \beta_1 + 2x_1\beta_2
\]

\begin{itemize}
\tightlist
\item
  The effect of \(x_1\) on \(y\) depends on the value of \(x_1\).
\item
  Partial Effect at the Average: \(\beta_1 + 2E(x_1)\beta_2\).
\item
  Average Partial Effect: \(E(\beta_1 + 2x_1\beta_2)\).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{interaction-terms}{%
\subparagraph{Interaction Terms}\label{interaction-terms}}

Interactions capture the joint effect of two variables. For example:

\[
y = \beta_0 + x_1\beta_1 + x_2\beta_2 + x_1x_2\beta_3 + \epsilon
\]

\begin{itemize}
\tightlist
\item
  \(\beta_1\) is the average effect of a unit change in \(x_1\) on \(y\) when \(x_2 = 0\).
\item
  The partial effect of \(x_1\) on \(y\), which depends on the level of \(x_2\), is:
\end{itemize}

\[
\beta_1 + x_2\beta_3.
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{a2-full-rank}{%
\paragraph{A2 Full Rank}\label{a2-full-rank}}

The full rank assumption ensures the uniqueness and existence of the parameter estimates in the population regression equation. It is expressed as:

\[
A2: \text{rank}(E(\mathbf{x'x})) = k
\]

This assumption is also known as the identification condition.

Key Points

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  No Perfect Multicollinearity:

  \begin{itemize}
  \tightlist
  \item
    The columns of \(\mathbf{x}\) (the matrix of predictors) must be linearly independent.
  \item
    No column in \(\mathbf{x}\) can be written as a linear combination of other columns.
  \end{itemize}
\item
  Implications:

  \begin{itemize}
  \tightlist
  \item
    Ensures that each parameter in the regression equation is identifiable and unique.
  \item
    Prevents computational issues, such as the inability to invert \(\mathbf{x'x}\), which is required for estimating \(\hat{\beta}\).
  \end{itemize}
\end{enumerate}

Example of Violation

If two predictors, \(x_1\) and \(x_2\), are perfectly correlated (e.g., \(x_2 = 2x_1\)), the rank of \(\mathbf{x}\) is reduced, and \(\mathbf{x'x}\) becomes singular. In such cases:

\begin{itemize}
\item
  The regression coefficients cannot be uniquely estimated.
\item
  The model fails to satisfy the full rank assumption.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{a3-exogeneity-of-independent-variables}{%
\paragraph{A3 Exogeneity of Independent Variables}\label{a3-exogeneity-of-independent-variables}}

The exogeneity assumption ensures that the independent variables (\(\mathbf{x}\)) are not systematically related to the error term (\(\epsilon\)). It is expressed as:

\[
A3: E[\epsilon | x_1, x_2, \dots, x_k] = E[\epsilon | \mathbf{x}] = 0
\]

This assumption is often referred to as strict exogeneity or mean independence (see \protect\hyperlink{correlation-and-independence}{Correlation and Independence}.

Key Points

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Strict Exogeneity:

  \begin{itemize}
  \tightlist
  \item
    Independent variables carry no information about the error term \(\epsilon\).
  \item
    By the {[}Law of Iterated Expectations{]}, \(E(\epsilon) = 0\), which can be satisfied by always including an intercept in the regression model.
  \end{itemize}
\item
  Implication:

  \begin{itemize}
  \tightlist
  \item
    A3 implies: \[
    E(y | \mathbf{x}) = \mathbf{x}\beta,
    \] meaning the conditional mean function is a linear function of \(\mathbf{x}\). This aligns with \protect\hyperlink{a1-linearity}{A1 Linearity}.
  \end{itemize}
\item
  Relationship with Independence:

  \begin{itemize}
  \tightlist
  \item
    Also referred to as mean independence, which is a weaker condition than full independence (see \protect\hyperlink{correlation-and-independence}{Correlation and Independence}).
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{a3a-weak-exogeneity}{%
\subparagraph{A3a: Weaker Exogeneity Assumption}\label{a3a-weak-exogeneity}}

A weaker version of the exogeneity assumption is:

\[
A3a: E(\mathbf{x_i'}\epsilon_i) = 0
\]

This implies:

\begin{itemize}
\item
  The independent variables (\(\mathbf{x}_i\)) are uncorrelated with the error term (\(\epsilon_i\)).
\item
  Weaker than mean independence in A3.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Comparison Between A3 and A3a

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4538}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3950}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
A3 (Strict Exogeneity)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
A3a (Weaker Exogeneity)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Definition & \(E(\epsilon | \mathbf{x}) = 0\). & \(E(\mathbf{x}_i'\epsilon_i) = 0\). \\
Strength & Stronger assumption; implies A3a. & Weaker assumption; does not imply A3. \\
Interpretation & Predictors provide no information about \(\epsilon\). & Predictors are uncorrelated with \(\epsilon\). \\
Causality & Enables causal interpretation. & Does not allow causal interpretations. \\
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Notes on Practical Relevance

\begin{itemize}
\tightlist
\item
  Checking for Exogeneity:

  \begin{itemize}
  \tightlist
  \item
    Strict exogeneity cannot be tested directly, but violations can manifest as omitted variable bias, endogeneity, or measurement error.
  \item
    Including all relevant predictors and ensuring accurate measurement can help satisfy this assumption.
  \end{itemize}
\item
  Violations of Exogeneity:

  \begin{itemize}
  \tightlist
  \item
    If A3 is violated, standard OLS estimates are biased and inconsistent.
  \item
    In such cases, instrumental variable (IV) methods or other approaches may be required to correct for endogeneity.
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{a4-homoskedasticity}{%
\paragraph{A4 Homoskedasticity}\label{a4-homoskedasticity}}

The homoskedasticity assumption ensures that the variance of the error term (\(\epsilon\)) is constant across all levels of the independent variables (\(\mathbf{x}\)). It is expressed as:

\[
A4: \text{Var}(\epsilon | \mathbf{x}) = \text{Var}(\epsilon) = \sigma^2
\]

Key Points

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Definition:

  \begin{itemize}
  \tightlist
  \item
    The variance of the disturbance term \(\epsilon\) is the same for all observations, regardless of the values of the predictors \(\mathbf{x}\).
  \end{itemize}
\item
  Practical Implication:

  \begin{itemize}
  \tightlist
  \item
    Homoskedasticity ensures that the errors do not systematically vary with the predictors.
  \item
    This is critical for valid inference, as the standard errors of the coefficients rely on this assumption.
  \end{itemize}
\item
  Violation (Heteroskedasticity):

  \begin{itemize}
  \tightlist
  \item
    If the variance of \(\epsilon\) depends on \(\mathbf{x}\), the assumption is violated.
  \item
    Common signs include funnel-shaped patterns in residual plots or varying error sizes.
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{a5-data-generation-random-sampling}{%
\paragraph{A5 Data Generation (Random Sampling)}\label{a5-data-generation-random-sampling}}

The random sampling assumption ensures that the observations \((y_i, x_{i1}, \dots, x_{ik-1})\) are drawn independently and identically distributed (iid) from the joint distribution of \((y, \mathbf{x})\). It is expressed as:

\[
A5: \{y_i, x_{i1}, \dots, x_{ik-1} : i = 1, \dots, n\}
\]

Key Points

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Random Sampling:

  \begin{itemize}
  \tightlist
  \item
    The dataset is assumed to be a random sample from the population.
  \item
    Each observation is independent of others and follows the same probability distribution.
  \end{itemize}
\item
  Implications:

  \begin{itemize}
  \tightlist
  \item
    With \protect\hyperlink{a3-exogeneity-of-independent-variables}{A3 (Exogeneity of Independent Variables)} and \protect\hyperlink{a4-homoskedasticity}{A4 (Homoskedasticity)}, random sampling implies:

    \begin{itemize}
    \tightlist
    \item
      Strict Exogeneity: \[
      E(\epsilon_i | x_1, \dots, x_n) = 0
      \] Independent variables do not contain information for predicting \(\epsilon\).
    \item
      Non-Autocorrelation: \[
      E(\epsilon_i \epsilon_j | x_1, \dots, x_n) = 0 \quad \text{for } i \neq j
      \] The error terms are uncorrelated across observations, conditional on the independent variables.
    \item
      Variance of Errors: \[
      \text{Var}(\epsilon | \mathbf{X}) = \text{Var}(\epsilon) = \sigma^2 \mathbf{I}_n
      \]
    \end{itemize}
  \end{itemize}
\item
  When A5 May Not Hold:

  \begin{itemize}
  \tightlist
  \item
    In time series data, where observations are often autocorrelated.
  \item
    In spatial data, where neighboring observations may not be independent.
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Practical Considerations

\begin{itemize}
\tightlist
\item
  Time Series Data:

  \begin{itemize}
  \tightlist
  \item
    Use methods such as autoregressive models or generalized least squares (GLS) to address dependency in observations.
  \end{itemize}
\item
  Spatial Data:

  \begin{itemize}
  \tightlist
  \item
    Spatial econometric models may be required to handle correlation across geographic locations.
  \end{itemize}
\item
  Checking Random Sampling:

  \begin{itemize}
  \tightlist
  \item
    While true randomness cannot always be verified, exploratory analysis of the residuals (e.g., for patterns or autocorrelation) can help detect violations.
  \end{itemize}
\end{itemize}

\hypertarget{a5a-stationarity-in-stochastic-processes}{%
\subparagraph{A5a: Stationarity in Stochastic Processes}\label{a5a-stationarity-in-stochastic-processes}}

A stochastic process \(\{x_t\}_{t=1}^T\) is stationary if, for every collection of time indices \(\{t_1, t_2, \dots, t_m\}\), the joint distribution of:

\[
x_{t_1}, x_{t_2}, \dots, x_{t_m}
\]

is the same as the joint distribution of:

\[
x_{t_1+h}, x_{t_2+h}, \dots, x_{t_m+h}
\]

for any \(h \geq 1\).

Key Points on Stationarity

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Definition:

  \begin{itemize}
  \tightlist
  \item
    A stationary process has statistical properties (mean, variance, and covariance) that are invariant over time.
  \item
    For example, the joint distribution for the first ten observations is identical to the joint distribution for the next ten observations, regardless of their position in time.
  \end{itemize}
\item
  Implication:

  \begin{itemize}
  \tightlist
  \item
    Stationarity ensures that the relationships observed in the data remain consistent over time.
  \item
    Independent draws automatically satisfy the stationarity condition.
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Weak Stationarity

A stochastic process \(\{x_t\}_{t=1}^T\) is weakly stationary if:

\begin{itemize}
\tightlist
\item
  The covariance between \(x_t\) and \(x_{t+h}\) depends only on the lag \(h\) and not on \(t\).
\item
  As \(h \to \infty\), the covariance diminishes, meaning \(x_t\) and \(x_{t+h}\) become ``almost independent.''
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Differences Between Stationarity and Weak Stationarity

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1608}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3986}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4336}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Stationarity
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Weak Stationarity
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Joint Distribution & Entire joint distribution remains unchanged over time. & Focuses only on the first two moments: mean and covariance. \\
Dependence Over Time & Observations at all lags are equally distributed. & Observations far apart are ``almost independent.'' \\
Application & Ensures strong consistency in time-series processes. & More practical for many time-series applications. \\
\end{longtable}

Weak stationarity is often sufficient for many time-series analyses, especially when focusing on correlations and trends rather than full distributions.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Common Weakly Dependent Processes

1. Moving Average Process of Order 1 (MA(1))

An MA(1) process models the dependent variable \(y_t\) as a function of the current and one-period lagged stochastic error term:

\[
y_t = u_t + \alpha_1 u_{t-1},
\]

where \(u_t\) is white noise, independently and identically distributed (iid) with variance \(\sigma^2\).

Key Properties

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Mean: \[
  E(y_t) = E(u_t) + \alpha_1E(u_{t-1}) = 0
  \]
\item
  Variance: \[
  \begin{aligned}
  \text{Var}(y_t) &= \text{Var}(u_t) + \alpha_1^2 \text{Var}(u_{t-1}) \\
  &= \sigma^2 + \alpha_1^2 \sigma^2 \\
  &= \sigma^2 (1 + \alpha_1^2)
  \end{aligned}
  \] An increase in the absolute value of \(\alpha_1\) increases the variance.
\item
  Autocovariance:

  \begin{itemize}
  \tightlist
  \item
    For lag 1: \[
    \text{Cov}(y_t, y_{t-1}) = \alpha_1 \text{Var}(u_{t-1}) = \alpha_1 \sigma^2.
    \]
  \item
    For lag 2 or greater: \[
    \text{Cov}(y_t, y_{t-2}) = 0.
    \]
  \end{itemize}
\end{enumerate}

The MA(1) process is invertible if \(|\alpha_1| < 1\), allowing it to be rewritten as an autoregressive (AR) representation:

\[
u_t = y_t - \alpha_1 u_{t-1}.
\]

Invertibility implies that we can express the current observation in terms of past observations.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

An MA(q) process generalizes the MA(1) process to include \(q\) lags:

\[
y_t = u_t + \alpha_1 u_{t-1} + \dots + \alpha_q u_{t-q},
\]

where \(u_t \sim WN(0, \sigma^2)\) (white noise with mean 0 and variance \(\sigma^2\)).

Key Characteristics:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Covariance Stationary:

  \begin{itemize}
  \tightlist
  \item
    An MA(q) process is covariance stationary irrespective of the parameter values.
  \end{itemize}
\item
  Invertibility:

  \begin{itemize}
  \tightlist
  \item
    An MA(q) process is invertible if the parameters satisfy certain conditions (e.g., \(|\alpha_i| < 1\)).
  \end{itemize}
\item
  Autocorrelations:

  \begin{itemize}
  \tightlist
  \item
    The autocorrelations are nonzero for lags up to \(q\) but are 0 for lags beyond \(q\).
  \end{itemize}
\item
  Conditional Mean:

  \begin{itemize}
  \tightlist
  \item
    The conditional mean of \(y_t\) depends on the \(q\) lags, indicating ``long-term memory.''
  \end{itemize}
\end{enumerate}

Example: Autocovariance of an MA(1)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Lag 1: \[
  \begin{aligned}
  \text{Cov}(y_t, y_{t-1}) &= \text{Cov}(u_t + \alpha_1 u_{t-1}, u_{t-1} + \alpha_1 u_{t-2}) \\
  &= \alpha_1 \text{Var}(u_{t-1}) \\
  &= \alpha_1 \sigma^2.
  \end{aligned}
  \]
\item
  Lag 2: \[
  \begin{aligned}
  \text{Cov}(y_t, y_{t-2}) &= \text{Cov}(u_t + \alpha_1 u_{t-1}, u_{t-2} + \alpha_1 u_{t-3}) \\
  &= 0.
  \end{aligned}
  \]
\end{enumerate}

An MA process captures a linear relationship between the dependent variable \(y_t\) and the current and past values of a stochastic error term \(u_t\). Its properties make it useful for modeling time-series data with limited memory and short-term dependencies.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Auto-Regressive Process of Order 1 (AR(1))
\end{enumerate}

An auto-regressive process of order 1 (AR(1)) is defined as:

\[
y_t = \rho y_{t-1} + u_t, \quad |\rho| < 1
\]

where \(u_t\) represents independent and identically distributed (i.i.d.) random noise over \(t\) with variance \(\sigma^2\).

Covariance at lag 1:

\[
\begin{aligned}
Cov(y_t, y_{t-1}) &= Cov(\rho y_{t-1} + u_t, y_{t-1}) \\
&= \rho Var(y_{t-1}) \\
&= \rho \frac{\sigma^2}{1-\rho^2}.
\end{aligned}
\]

Covariance at lag \(h\):

\[
Cov(y_t, y_{t-h}) = \rho^h \frac{\sigma^2}{1-\rho^2}.
\]

Stationarity implies that the distribution of \(y_t\) does not change over time, requiring constant mean and variance. For this process:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Mean Stationarity:\\
  Assuming \(E(y_t) = 0\), we have: \[
  y_t = \rho^t y_0 + \rho^{t-1}u_1 + \rho^{t-2}u_2 + \dots + \rho u_{t-1} + u_t.
  \]

  If the initial observation \(y_0 = 0\), then \(y_t\) is simply a weighted sum of the random shocks \(u_t\) from all prior time periods. Thus, \(E(y_t) = 0\) for all \(t\).
\item
  Variance Stationarity:\\
  The variance is computed as: \[
  Var(y_t) = Var(\rho y_{t-1} + u_t).
  \]

  Expanding and simplifying: \[
  Var(y_t) = \rho^2 Var(y_{t-1}) + Var(u_t) + 2\rho Cov(y_{t-1}, u_t).
  \] Since \(u_t\) is independent of \(y_{t-1}\), \(Cov(y_{t-1}, u_t) = 0\), giving: \[
  Var(y_t) = \rho^2 Var(y_{t-1}) + \sigma^2.
  \]

  Solving recursively, we find: \[
  Var(y_t) = \frac{\sigma^2}{1-\rho^2}.
  \]
\end{enumerate}

For the variance to remain constant over time, it is required that \(|\rho| < 1\) and \(p \notin \{1,-1\}\).

Key Insights on Stationarity

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Stationarity Requirement:\\
  The condition \(|\rho| < 1\) ensures stationarity, as this guarantees that both the mean and variance of the process are constant over time.
\item
  Weak Dependence:\\
  As \(|\rho| < 1\), the dependency between observations diminishes with increasing lag \(h\), as seen from the covariance: \[
  Cov(y_t, y_{t-h}) = \rho^h \frac{\sigma^2}{1-\rho^2}.
  \]
\end{enumerate}

To estimate an AR(1) process, we utilize the Yule-Walker equations, which relate the autocovariances of the process to its parameters.

Starting with the AR(1) process: \[
y_t = \epsilon_t + \phi y_{t-1},
\] multiplying both sides by \(y_{t-\tau}\) and taking expectations, we get: \[
y_t y_{t-\tau} = \epsilon_t y_{t-\tau} + \phi y_{t-1} y_{t-\tau}.
\]

For \(\tau \geq 1\), the autocovariance \(\gamma(\tau)\) satisfies: \[
\gamma(\tau) = \phi \gamma(\tau - 1).
\]

Dividing through by the variance \(\gamma(0)\), we obtain the autocorrelation: \[
\rho_\tau = \phi^\tau.
\]

Thus, the autocorrelations decay geometrically as \(\phi^\tau\), where \(|\phi| < 1\) ensures stationarity and decay over time.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Generalizing to AR(p)

An AR(p) process extends the AR(1) structure to include \(p\) lags: \[
y_t = \phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_p y_{t-p} + \epsilon_t.
\]

Here, \(\epsilon_t\) is white noise with \(E(\epsilon_t) = 0\) and \(Var(\epsilon_t) = \sigma^2\).

The AR(p) process is covariance stationary if the roots of the characteristic equation lie outside the unit circle: \[
1 - \phi_1 z - \phi_2 z^2 - \dots - \phi_p z^p = 0.
\]

For the AR(p) process, the autocorrelations \(\rho_\tau\) decay more complexly compared to the AR(1). However, they still diminish over time, ensuring weak dependence among distant observations.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The Yule-Walker equations for an AR(p) process provide a system of linear equations to estimate the parameters \(\phi_1, \phi_2, \dots, \phi_p\): \[
\gamma(\tau) = \phi_1 \gamma(\tau - 1) + \phi_2 \gamma(\tau - 2) + \dots + \phi_p \gamma(\tau - p), \quad \tau \geq 1.
\]

This system can be written in matrix form for \(\tau = 1, \dots, p\) as:

\[
\begin{bmatrix}
\gamma(1) \\
\gamma(2) \\
\vdots \\
\gamma(p)
\end{bmatrix}
=
\begin{bmatrix}
\gamma(0) & \gamma(1) & \dots & \gamma(p-1) \\
\gamma(1) & \gamma(0) & \dots & \gamma(p-2) \\
\vdots & \vdots & \ddots & \vdots \\
\gamma(p-1) & \gamma(p-2) & \dots & \gamma(0)
\end{bmatrix}
\begin{bmatrix}
\phi_1 \\
\phi_2 \\
\vdots \\
\phi_p
\end{bmatrix}.
\]

This system is solved to estimate the coefficients \(\phi_1, \phi_2, \dots, \phi_p\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  ARMA(p, q) Process
\end{enumerate}

An ARMA(p, q) process combines autoregressive (AR) and moving average (MA) components to model time series data effectively. The general form is:

\[
y_t = \phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_p y_{t-p} + \epsilon_t + \alpha_1 \epsilon_{t-1} + \alpha_2 \epsilon_{t-2} + \dots + \alpha_q \epsilon_{t-q}.
\]

A simple case of ARMA(1, 1) is given by: \[
y_t = \phi y_{t-1} + \epsilon_t + \alpha \epsilon_{t-1},
\] where:

\begin{itemize}
\item
  \(\phi\) captures the autoregressive behavior,
\item
  \(\alpha\) controls the moving average component,
\item
  \(\epsilon_t\) represents white noise.
\end{itemize}

ARMA processes can capture seasonality and more complex dependencies than pure AR or MA models.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  \textbf{Random Walk Process}
\end{enumerate}

A \textbf{random walk} is a non-stationary process defined as: \[
y_t = y_0 + \sum_{s=1}^t u_s,
\] where \(u_s\) are i.i.d. random variables.

Properties:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Non-Stationarity}:

  \begin{itemize}
  \tightlist
  \item
    If \(y_0 = 0\), then \(E(y_t) = 0\), but the variance increases over time: \[
    Var(y_t) = t \sigma^2.
    \]
  \end{itemize}
\item
  \textbf{Not Weakly Dependent}:

  \begin{itemize}
  \tightlist
  \item
    The covariance of the process does not diminish with increasing lag \(h\): \[
    Cov\left(\sum_{s=1}^t u_s, \sum_{s=1}^{t-h} u_s\right) = (t-h)\sigma^2.
    \]
  \end{itemize}

  As \(h\) increases, the covariance remains large, violating the condition for weak dependence.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{a5a-stationarity-and-weak-dependence-in-time-series}{%
\subparagraph{A5a: Stationarity and Weak Dependence in Time Series}\label{a5a-stationarity-and-weak-dependence-in-time-series}}

For time series data, the set \(\{y_t, x_{t1}, \dots, x_{tk-1}\}\), where \(t = 1, \dots, T\), must satisfy the conditions of \textbf{stationarity} and \textbf{weak dependence}. These properties are essential to ensure the consistency and efficiency of estimators in time-series regression models.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Stationarity}: A stationary process has statistical properties (e.g., mean, variance, autocovariance) that remain constant over time. This ensures that the relationships in the data do not change as \(t\) progresses, making it possible to draw meaningful inferences.
\item
  \textbf{Weak Dependence}: Weak dependence implies that observations far apart in time are ``almost independent.'' While there may be short-term correlations, these diminish as the time lag increases. This property ensures that the sample averages are representative of the population mean.
\end{enumerate}

The \protect\hyperlink{weak-law-of-large-numbers}{Weak Law of Large Numbers} provides a foundation for the consistency of sample means. If \(\{z_t\}\) is a weakly dependent, stationary process with \(E(|z_t|) < \infty\) and \(E(z_t) = \mu\), then:

\[
\frac{1}{T} \sum_{t=1}^T z_t \xrightarrow{p} \mu.
\]

Interpretation:

\begin{itemize}
\item
  As the sample size \(T \to \infty\), the sample mean \(\bar{z} = \frac{1}{T} \sum_{t=1}^T z_t\) converges in probability to the true mean \(\mu\).
\item
  This ensures the \textbf{consistency} of estimators based on time-series data.
\end{itemize}

The \protect\hyperlink{central-limit-theorem}{Central Limit Theorem} extends the WLLN by describing the distribution of the sample mean. Under additional regularity conditions (e.g., finite variance) \citep{greene1990gamma}, the sample mean \(\bar{z}\) satisfies:

\[
\sqrt{T}(\bar{z} - \mu) \xrightarrow{d} N(0, B),
\]

where:

\[
B = \text{Var}(z_t) + 2\sum_{h=1}^\infty \text{Cov}(z_t, z_{t-h}).
\]

\textbf{Interpretation}:

\begin{itemize}
\item
  The sample mean \(\bar{z}\) is approximately normally distributed for large \(T\).
\item
  The variance of the limiting distribution, \(B\), depends not only on the variance of \(z_t\) but also on the covariances between \(z_t\) and its past values.
\end{itemize}

\hypertarget{a6-normal-distribution}{%
\paragraph{A6 Normal Distribution}\label{a6-normal-distribution}}

A6: \(\epsilon|\mathbf{x} \sim N(0, \sigma^2 I_n)\)

The assumption here implies that the error term \(\epsilon\) is normally distributed with mean zero and variance \(\sigma^2 I_n\). This assumption is fundamental for statistical inference in linear regression models.

Using assumptions \protect\hyperlink{a1-linearity}{A1 Linearity}, \protect\hyperlink{a2-full-rank}{A2 Full Rank}, and \protect\hyperlink{a3-exogeneity-of-independent-variables}{A3 Exogeneity of Independent Variables}, we derive the identification (or orthogonality condition) for the population parameter \(\beta\):

\[
\begin{aligned}
y &= x\beta + \epsilon && \text{(A1: Model Specification)} \\
x'y &= x'x\beta + x'\epsilon && \text{(Multiply both sides by $x'$)} \\
E(x'y) &= E(x'x)\beta + E(x'\epsilon) && \text{(Taking expectation)} \\
E(x'y) &= E(x'x)\beta && \text{(A3: Exogeneity, $E(x'\epsilon) = 0$)} \\
[E(x'x)]^{-1}E(x'y) &= [E(x'x)]^{-1}E(x'x)\beta && \text{(Invertibility of $E(x'x)$, A2)} \\
[E(x'x)]^{-1}E(x'y) &= \beta && \text{(Simplified solution for $\beta$)}
\end{aligned}
\]

Thus, \(\beta\) is identified as the vector of parameters that minimizes the expected squared error.

To find \(\beta\), we minimize the expected value of the squared error:

\[
\underset{\gamma}{\operatorname{argmin}} \ E\big((y - x\gamma)^2\big)
\]

The first-order condition is derived by taking the derivative of the objective function with respect to \(\gamma\) and setting it to zero:

\[
\begin{aligned}
\frac{\partial E\big((y - x\gamma)^2\big)}{\partial \gamma} &= 0 \\
-2E(x'(y - x\gamma)) &= 0 \\
E(x'y) - E(x'x\gamma) &= 0 \\
E(x'y) &= E(x'x)\gamma \\
(E(x'x))^{-1}E(x'y) &= \gamma
\end{aligned}
\]

This confirms that \(\gamma = \beta\).

The second-order condition ensures that the solution minimizes the objective function. Taking the second derivative:

\[
\frac{\partial^2 E\big((y - x\gamma)^2\big)}{\partial \gamma'^2} = 0 = 2E(x'x) 
\]

If assumption \protect\hyperlink{a3-exogeneity-of-independent-variables}{A3 Exogeneity of Independent Variables} holds, \(E(x'x)\) is positive semi-definite (PSD). Thus, \(2E(x'x)\) is also PSD, ensuring a minimum.

\hypertarget{hierarchy-of-ols-assumptions}{%
\paragraph{Hierarchy of OLS Assumptions}\label{hierarchy-of-ols-assumptions}}

This table summarizes the hierarchical nature of assumptions required to derive different properties of the OLS estimator.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2710}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1226}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.3097}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1871}}@{}}
\caption{Usage of Assumptions}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Assumption
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Identification Data Description}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Unbiasedness Consistency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\protect\hyperlink{gauss-markov-theorem}{\textbf{Gauss-Markov}} \textbf{(BLUE) Asymptotic Inference (z and Chi-squared)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Classical LM (BUE) Small-sample Inference (t and F)}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Assumption
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Identification Data Description}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Unbiasedness Consistency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\protect\hyperlink{gauss-markov-theorem}{\textbf{Gauss-Markov}} \textbf{(BLUE) Asymptotic Inference (z and Chi-squared)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Classical LM (BUE) Small-sample Inference (t and F)}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\protect\hyperlink{a2-full-rank}{A2 Full Rank}

Variation in \(\mathbf{X}\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) \\
\protect\hyperlink{a5-data-generation-random-sampling}{A5 Data Generation (Random Sampling)}

Random Sampling & & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) \\
\protect\hyperlink{a1-linearity}{A1 Linearity}

Linearity in Parameters & & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) \\
\protect\hyperlink{a3-exogeneity-of-independent-variables}{A3 Exogeneity of Independent Variables}

Zero Conditional Mean & & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) \\
\protect\hyperlink{a4-homoskedasticity}{A4 Homoskedasticity}

\(\mathbf{H}\) homoskedasticity & & & \(\checkmark\) & \(\checkmark\) \\
\protect\hyperlink{a6-normal-distribution}{A6 Normal Distribution}

Normality of Errors & & & & \(\checkmark\) \\
\end{longtable}

\begin{itemize}
\tightlist
\item
  \textbf{Identification Data Description:} Ensures the model is identifiable and coefficients can be estimated.
\item
  \textbf{Unbiasedness Consistency:} Guarantees that OLS estimates are unbiased and converge to the true parameter values as the sample size increases.
\item
  \textbf{Gauss-Markov (BLUE) and Asymptotic Inference:} Requires additional assumptions (e.g., homoskedasticity) to ensure minimum variance of estimators and valid inference using large-sample tests (z and chi-squared).
\item
  \textbf{Classical LM (BUE) Small-sample Inference:} Builds on all previous assumptions and adds normality of errors for valid t and F tests in finite samples.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{theorems}{%
\subsubsection{Theorems}\label{theorems}}

\hypertarget{frischwaughlovell-theorem}{%
\paragraph{Frisch--Waugh--Lovell Theorem}\label{frischwaughlovell-theorem}}

The Frisch--Waugh--Lovell (FWL) Theorem is a fundamental result in linear regression that allows for a deeper understanding of how coefficients are computed in a multiple regression setting \citep{lovell2008simple}. Informally, it states:

\begin{quote}
When estimating the effect of a subset of variables (\(X_1\)) on \(y\) in the presence of other variables (\(X_2\)), you can ``partial out'' the influence of \(X_2\) from both \(y\) and \(X_1\). Then, regressing the residuals of \(y\) on the residuals of \(X_1\) produces coefficients for \(X_1\) that are identical to those obtained from the full multiple regression.
\end{quote}

Consider the multiple linear regression model:

\[ \mathbf{y = X\beta + \epsilon = X_1\beta_1 + X_2\beta_2 + \epsilon} \]

where:

\begin{itemize}
\tightlist
\item
  \(y\) is an \(n \times 1\) vector of the dependent variable.
\item
  \(X_1\) is an \(n \times k_1\) matrix of regressors of interest.
\item
  \(X_2\) is an \(n \times k_2\) matrix of additional regressors.
\item
  \(\beta_1\) and \(\beta_2\) are coefficient vectors of sizes \(k_1 \times 1\) and \(k_2 \times 1\), respectively.
\item
  \(\epsilon\) is an \(n \times 1\) error term vector.
\end{itemize}

This can be equivalently represented in partitioned matrix form as:

\[ \left( \begin{array}{cc} X_1'X_1 & X_1'X_2 \\ X_2'X_1 & X_2'X_2 \end{array} \right) \left( \begin{array}{c} \hat{\beta_1} \\ \hat{\beta_2} \end{array} \right) = \left( \begin{array}{c} X_1'y \\ X_2'y \end{array} \right) \]

The OLS estimator for the vector \(\begin{pmatrix} \beta_1 \\ \beta_2 \end{pmatrix}\) is:

\[
\begin{pmatrix}
\hat{\beta}_1 \\
\hat{\beta}_2
\end{pmatrix}
=
\begin{pmatrix}
X_1'X_1 & X_1'X_2 \\
X_2'X_1 & X_2'X_2
\end{pmatrix}^{-1}
\begin{pmatrix}
X_1'y \\
X_2'y
\end{pmatrix}.
\]

If we only want the coefficients on \(X_1\), a known result from partitioned-inversion gives:

\[
\hat{\beta}_1 
= 
\bigl(X_1' M_2\, X_1\bigr)^{-1} 
\,X_1' M_2\, y,
\]

where

\[
M_2 
= 
I 
- 
X_2 \bigl(X_2'X_2\bigr)^{-1} X_2'.
\]

The matrix \(M_2\) is often called the \textbf{residual-maker} or \textbf{annihilator} matrix for \(X_2\). It is an \(n \times n\) symmetric, idempotent projection matrix that projects any vector in \(\mathbb{R}^n\) onto the orthogonal complement of the column space of \(X_2\). \(M_2\) satisfies \(M_2^2 = M_2\), and \(M_2 = M_2'\).

Intuitively, \(M_2\) captures the part of \(y\) (and any other vector) that is orthogonal to the columns of \(X_2\). This ``partialling out'' of \(X_2\) from both \(y\) and \(X_1\) lets us isolate \(\hat{\beta}_1\).

Equivalently, we can also represent \(\hat{\beta_1}\) as:

\[ \mathbf{\hat{\beta_1} = (X_1'X_1)^{-1}X_1'y - (X_1'X_1)^{-1}X_1'X_2\hat{\beta_2}} \]

From this equation, we can see that

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Betas from Multiple vs.~Simple Regressions:}

  \begin{itemize}
  \tightlist
  \item
    The coefficients (\(\beta\)) from a multiple regression are generally \textbf{not the same} as the coefficients from separate individual simple regressions.
  \end{itemize}
\item
  \textbf{Impact of Additional Variables (}\(X_2\)):

  \begin{itemize}
  \tightlist
  \item
    The inclusion of different sets of explanatory variables (\(X_2\)) affects all coefficient estimates, even for those in \(X_1\).
  \end{itemize}
\item
  \textbf{Special Cases:}

  \begin{itemize}
  \tightlist
  \item
    If \(X_1'X_2 = 0\) (orthogonality between \(X_1\) and \(X_2\)) or \(\hat{\beta_2} = 0\), the above points (1 and 2) do not hold. In such cases, there is no interaction between the coefficients in \(X_1\) and \(X_2\), making the coefficients in \(X_1\) unaffected by \(X_2\).
  \end{itemize}
\end{enumerate}

\textbf{Steps in FWL:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Partial Out} \(X_2\) from \(y\): Regress \(y\) on \(X_2\) to obtain residuals:

  \[
  \tilde{y} = M_2y.
  \]
\item
  \textbf{Partial Out} \(X_2\) from \(X_1\): For each column of \(X_1\), regress it on \(X_2\) to obtain residuals:

  \[
  \tilde{X}_1 = M_2X_1.
  \]
\item
  \textbf{Regression of Residuals:} Regress \(\tilde{y}\) on \(\tilde{X}_1\):

  \[
  \tilde{y} = \tilde{X}_1\beta_1 + \text{error}.
  \]
\end{enumerate}

The coefficients \(\beta_1\) obtained here are identical to those from the full model regression:

\[
y = X_1\beta_1 + X_2\beta_2 + \epsilon.
\]

Why It Matters

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Interpretation of Partial Effects:} The FWL Theorem provides a way to interpret \(\beta_1\) as the effect of \(X_1\) on \(y\) after removing any linear dependence on \(X_2\).
\item
  \textbf{Computational Simplicity:} It allows the decomposition of a large regression problem into smaller, computationally simpler pieces.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Simulate data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{100}
\NormalTok{X1 }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(n }\SpecialCharTok{*} \DecValTok{2}\NormalTok{), n, }\DecValTok{2}\NormalTok{)  }\CommentTok{\# Two regressors of interest}
\NormalTok{X2 }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(n }\SpecialCharTok{*} \DecValTok{2}\NormalTok{), n, }\DecValTok{2}\NormalTok{)  }\CommentTok{\# Two additional regressors}
\NormalTok{beta1 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{)  }\CommentTok{\# Coefficients for X1}
\NormalTok{beta2 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{)  }\CommentTok{\# Coefficients for X2undefined}
\NormalTok{u }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)  }\CommentTok{\# Error term}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ X1 }\SpecialCharTok{\%*\%}\NormalTok{ beta1 }\SpecialCharTok{+}\NormalTok{ X2 }\SpecialCharTok{\%*\%}\NormalTok{ beta2 }\SpecialCharTok{+}\NormalTok{ u  }\CommentTok{\# Generate dependent variable}

\CommentTok{\# Full regression}
\NormalTok{full\_model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X1 }\SpecialCharTok{+}\NormalTok{ X2)}
\FunctionTok{summary}\NormalTok{(full\_model)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = y \textasciitilde{} X1 + X2)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}      Min       1Q   Median       3Q      Max }
\CommentTok{\#\textgreater{} {-}2.47336 {-}0.58010  0.07461  0.68778  2.46552 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)  0.11614    0.10000   1.161    0.248    }
\CommentTok{\#\textgreater{} X11          1.77575    0.10899  16.293  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} X12         {-}1.14151    0.10204 {-}11.187  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} X21          0.94954    0.10468   9.071 1.60e{-}14 ***}
\CommentTok{\#\textgreater{} X22          0.47667    0.09506   5.014 2.47e{-}06 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 0.9794 on 95 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.8297, Adjusted R{-}squared:  0.8225 }
\CommentTok{\#\textgreater{} F{-}statistic: 115.7 on 4 and 95 DF,  p{-}value: \textless{} 2.2e{-}16}

\CommentTok{\# Step 1: Partial out X2 from y}
\NormalTok{y\_residual }\OtherTok{\textless{}{-}} \FunctionTok{residuals}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X2))}

\CommentTok{\# Step 2: Partial out X2 from X1}
\NormalTok{X1\_residual }\OtherTok{\textless{}{-}} \FunctionTok{residuals}\NormalTok{(}\FunctionTok{lm}\NormalTok{(X1 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X2))}

\CommentTok{\# Step 3: Regress residuals}
\NormalTok{fwl\_model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y\_residual }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X1\_residual }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(fwl\_model)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = y\_residual \textasciitilde{} X1\_residual {-} 1)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}      Min       1Q   Median       3Q      Max }
\CommentTok{\#\textgreater{} {-}2.47336 {-}0.58010  0.07461  0.68778  2.46552 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}              Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} X1\_residual1   1.7758     0.1073   16.55   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} X1\_residual2  {-}1.1415     0.1005  {-}11.36   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 0.9643 on 98 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.8109, Adjusted R{-}squared:  0.807 }
\CommentTok{\#\textgreater{} F{-}statistic: 210.1 on 2 and 98 DF,  p{-}value: \textless{} 2.2e{-}16}

\CommentTok{\# Comparison of coefficients}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Full model coefficients (X1):"}\NormalTok{, }\FunctionTok{coef}\NormalTok{(full\_model)[}\DecValTok{2}\SpecialCharTok{:}\DecValTok{3}\NormalTok{], }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Full model coefficients (X1): 1.775754 {-}1.141514}
\FunctionTok{cat}\NormalTok{(}\StringTok{"FWL model coefficients:"}\NormalTok{, }\FunctionTok{coef}\NormalTok{(fwl\_model), }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} FWL model coefficients: 1.775754 {-}1.141514}
\end{Highlighting}
\end{Shaded}

\hypertarget{gauss-markov-theorem}{%
\paragraph{Gauss-Markov Theorem}\label{gauss-markov-theorem}}

For a linear regression model:

\[
\mathbf{y = X\beta + \epsilon},
\]

under the assumptions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \protect\hyperlink{a1-linearity}{\textbf{A1}}: Linearity of the model.
\item
  \protect\hyperlink{a2-full-rank}{\textbf{A2}}: Full rank of \(\mathbf{X}\).
\item
  \protect\hyperlink{a3-exogeneity-of-independent-variables}{\textbf{A3}}: Exogeneity of \(\mathbf{X}\).
\item
  \protect\hyperlink{a4-homoskedasticity}{\textbf{A4}}: Homoskedasticity of \(\epsilon\).
\end{enumerate}

The \protect\hyperlink{ordinary-least-squares}{Ordinary Least Squares} estimator:

\[
\hat{\beta} = \mathbf{(X'X)^{-1}X'y},
\]

is the \textbf{Best Linear Unbiased Estimator (BLUE)}. This means that \(\hat{\beta}\) has the minimum variance among all linear unbiased estimators of \(\beta\).

\textbf{1. Unbiasedness}

Suppose we consider \textbf{any} linear estimator of \(\beta\) of the form:

\[
\tilde{\beta} = \mathbf{C\,y},
\]

where

\begin{itemize}
\tightlist
\item
  \(\mathbf{y}\) is the \(n \times 1\) vector of observations,
\item
  \(\mathbf{C}\) is a \(k \times n\) matrix (with \(k\) the dimension of \(\beta\)) that depends \textbf{only} on the design matrix \(\mathbf{X}\).
\end{itemize}

Our regression model is

\[
\mathbf{y} = \mathbf{X}\beta + \boldsymbol{\epsilon}, 
\quad 
E[\boldsymbol{\epsilon} \mid \mathbf{X}] = \mathbf{0}, 
\quad 
\mathrm{Var}(\boldsymbol{\epsilon} \mid \mathbf{X}) = \sigma^2 \mathbf{I}.
\]

We say \(\tilde{\beta}\) is \textbf{unbiased} if its conditional expectation (given \(\mathbf{X}\)) equals the true parameter \(\beta\):

\[
E(\tilde{\beta} \mid \mathbf{X})
= 
E(\mathbf{C\,y} \mid \mathbf{X})
= 
\beta.
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Substitute \(\mathbf{y} = \mathbf{X}\beta + \boldsymbol{\epsilon}\):
\end{enumerate}

\[
E(\mathbf{C\,y} \mid \mathbf{X})
=
E\bigl(\mathbf{C}(\mathbf{X}\beta + \boldsymbol{\epsilon}) \mid \mathbf{X}\bigr)
=
\mathbf{C\,X}\,\beta 
+
\mathbf{C}\,E(\boldsymbol{\epsilon} \mid \mathbf{X})
=
\mathbf{C\,X}\,\beta.
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  For this to hold for \emph{all} \(\beta\), we require
\end{enumerate}

\[
\mathbf{C\,X} = \mathbf{I}.
\]

In other words, \(\mathbf{C}\) must be a ``right-inverse'' of \(\mathbf{X}\).

On the other hand, the OLS estimator \(\hat{\beta}\) is given by

\[
\hat{\beta} 
= 
(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\,\mathbf{y}.
\]

You can verify:

\begin{itemize}
\tightlist
\item
  Let \(\mathbf{C}_{\text{OLS}} = (\mathbf{X}'\mathbf{X})^{-1} \mathbf{X}'\).
\item
  Then \[
  \mathbf{C}_{\text{OLS}}\,\mathbf{X}
  =
  (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\,\mathbf{X}
  =
  \mathbf{I}.
  \]
\item
  By the argument above, this makes \(\hat{\beta}\) unbiased.
\end{itemize}

Hence, any linear estimator \(\tilde{\beta} = \mathbf{C\,y}\) that is unbiased must satisfy \(\mathbf{C\,X} = \mathbf{I}\).

\textbf{2. Minimum Variance (Gauss--Markov Part)}

Among all estimators of the form \(\tilde{\beta} = \mathbf{C\,y}\) that are unbiased (so \(\mathbf{C\,X} = \mathbf{I}\)), \textbf{OLS} achieves the smallest covariance matrix.

\begin{itemize}
\tightlist
\item
  Variance of a General Unbiased Estimator
\end{itemize}

If \(\tilde{\beta} = \mathbf{C\,y}\) with \(\mathbf{C\,X} = \mathbf{I}\), then:

\[
\mathrm{Var}(\tilde{\beta} \mid \mathbf{X})
=
\mathrm{Var}(\mathbf{C\,y} \mid \mathbf{X})
=
\mathbf{C}\,\mathrm{Var}(\mathbf{y} \mid \mathbf{X})\,\mathbf{C}'
=
\mathbf{C}\bigl(\sigma^2 \mathbf{I}\bigr)\mathbf{C}'
=
\sigma^2\,\mathbf{C}\,\mathbf{C}'.
\]

\begin{itemize}
\tightlist
\item
  Variance of the OLS Estimator
\end{itemize}

For OLS, \(\hat{\beta} = (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\,\mathbf{y}\). Thus,

\[
\mathrm{Var}(\hat{\beta} \mid \mathbf{X})
=
\sigma^2\,(\mathbf{X}'\mathbf{X})^{-1}.
\]

\begin{itemize}
\tightlist
\item
  Comparing \(\mathrm{Var}(\tilde{\beta})\) to \(\mathrm{Var}(\hat{\beta})\)
\end{itemize}

We want to show:

\[
\mathrm{Var}(\tilde{\beta} \mid \mathbf{X}) 
- 
\mathrm{Var}(\hat{\beta} \mid \mathbf{X})
\;\;\text{is positive semi-definite.}
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Since both \(\tilde{\beta}\) and \(\hat{\beta}\) are unbiased, we know: \[
  \mathbf{C\,X} = \mathbf{I},
  \quad
  (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\,\mathbf{X} = \mathbf{I}.
  \]
\item
  One can show algebraically (as in the proof provided above) that \[
  \mathrm{Var}(\tilde{\beta} \mid \mathbf{X})
  -
  \mathrm{Var}(\hat{\beta} \mid \mathbf{X})
  =
  \sigma^2 \bigl[\mathbf{C}\mathbf{C}' - (\mathbf{X}'\mathbf{X})^{-1}\bigr].
  \] Under the condition \(\mathbf{C\,X} = \mathbf{I}\), the difference \(\mathbf{C}\mathbf{C}' - (\mathbf{X}'\mathbf{X})^{-1}\) is positive semi-definite.
\item
  Positive semi-definite difference means
\end{enumerate}

\[
\mathbf{v}' \Bigl(\mathbf{C}\mathbf{C}' 
- 
(\mathbf{X}'\mathbf{X})^{-1}\Bigr)\mathbf{v} 
\ge 
0 
\quad 
\text{for all vectors } \mathbf{v}.
\]

Hence, \(\hat{\beta}\) has the smallest variance (in the sense of covariance matrices) among all linear unbiased estimators \(\tilde{\beta} = \mathbf{C\,y}\).

\textbf{Summary of the Key Points}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Unbiasedness:}\\
  A linear estimator \(\tilde{\beta} = \mathbf{C\,y}\) is unbiased if \(E(\tilde{\beta}\mid \mathbf{X}) = \beta\).\\
  This forces \(\mathbf{C\,X} = \mathbf{I}\).
\item
  \textbf{OLS is Unbiased:}\\
  The OLS estimator \(\hat{\beta} = (X'X)^{-1} X' \, y\) satisfies \((X'X)^{-1} X' \, X = I\), hence is unbiased.
\item
  \textbf{OLS has Minimum Variance:}\\
  Among all \(\mathbf{C}\) that satisfy \(\mathbf{C\,X} = \mathbf{I}\), the matrix \((\mathbf{X}'\mathbf{X})^{-1}\) gives the smallest possible \(\mathrm{Var}(\tilde{\beta})\).\\
  In matrix form, \(\mathrm{Var}(\tilde{\beta}) - \mathrm{Var}(\hat{\beta})\) is positive semi-definite, showing OLS is optimal (the \textbf{Best Linear Unbiased Estimator}, BLUE).
\end{enumerate}

\hypertarget{finite-sample-properties}{%
\subsubsection{Finite Sample Properties}\label{finite-sample-properties}}

The finite sample properties of an estimator are considered when the sample size \(n\) is fixed (not asymptotically large). Key properties include \textbf{bias}, \textbf{distribution}, and \textbf{standard deviation} of the estimator.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Bias} measures how close an estimator is, on average, to the true parameter value \(\beta\). It is defined as:

\[
\text{Bias} = E(\hat{\beta}) - \beta
\]

Where:

\begin{itemize}
\tightlist
\item
  \(\beta\): True parameter value.
\item
  \(\hat{\beta}\): Estimator for \(\beta\).
\end{itemize}

\textbf{Unbiased Estimator}: An estimator is unbiased if:

\[
\text{Bias} = E(\hat{\beta}) - \beta = 0 \quad \text{or equivalently} \quad E(\hat{\beta}) = \beta
\]

This means the estimator will produce estimates that are, on average, equal to the value it is trying to estimate.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

An estimator is a function of random variables (data). Its \textbf{distribution} describes how the estimates vary across repeated samples. Key aspects include:

\begin{itemize}
\tightlist
\item
  \textbf{Center}: Mean of the distribution, which relates to bias.
\item
  \textbf{Spread}: Variability of the estimator, captured by its standard deviation or variance.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The \textbf{standard deviation} of an estimator measures the spread of its sampling distribution. It indicates the variability of the estimator across different samples.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{ordinary-least-squares-properties}{%
\paragraph{Ordinary Least Squares Properties}\label{ordinary-least-squares-properties}}

Under the standard assumptions for OLS:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \protect\hyperlink{a1-linearity}{A1}: The relationship between \(Y\) and \(X\) is linear.
\item
  \protect\hyperlink{a2-full-rank}{A2}: The matrix \(\mathbf{X'X}\) is invertible.
\item
  \protect\hyperlink{a3-exogeneity-of-independent-variables}{A3}: \(E(\epsilon|X) = 0\) (errors are uncorrelated with predictors).
\end{enumerate}

\textbf{OLS is unbiased} under these assumptions. The proof is as follows:

\[
\begin{aligned}
E(\hat{\beta}) &= E(\mathbf{(X'X)^{-1}X'y}) && \text{A2}\\
               &= E(\mathbf{(X'X)^{-1}X'(X\beta + \epsilon)}) && \text{A1}\\
               &= E(\mathbf{(X'X)^{-1}X'X\beta + (X'X)^{-1}X'\epsilon}) \\
               &= E(\beta + \mathbf{(X'X)^{-1}X'\epsilon}) \\
               &= \beta + E(\mathbf{(X'X)^{-1}X'\epsilon}) \\
               &= \beta + E(E(\mathbf{(X'X)^{-1}X'\epsilon}|X)) && \text{LIE (Law of Iterated Expectation)} \\
               &= \beta + E(\mathbf{(X'X)^{-1}X'}E(\epsilon|X)) \\
               &= \beta + E(\mathbf{(X'X)^{-1}X'} \cdot 0) && \text{A3}\\
               &= \beta
\end{aligned}
\]

\textbf{Key Points}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Linearity of Expectation}: Used to separate terms involving \(\beta\) and \(\epsilon\).
\item
  \protect\hyperlink{law-of-iterated-expectation}{Law of Iterated Expectation} \textbf{(LIE)}: Simplifies nested expectations.
\item
  \textbf{Exogeneity of Errors (A3)}: Ensures \(E(\epsilon|X) = 0\), eliminating bias.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Implications of Unbiasedness

\begin{itemize}
\tightlist
\item
  OLS estimators are centered around the true value \(\beta\) across repeated samples.
\item
  In small samples, OLS estimators may exhibit variability, but their expected value remains \(\beta\).
\end{itemize}

If the assumption of exogeneity (\protect\hyperlink{a3-exogeneity-of-independent-variables}{A3}) is violated, the OLS estimator becomes \textbf{biased}. Specifically, omitted variables or endogeneity can introduce systematic errors into the estimation.

From the \textbf{Frisch-Waugh-Lovell Theorem}:

\begin{itemize}
\tightlist
\item
  If an omitted variable \(\hat{\beta}_2 \neq 0\) (non-zero effect) and the omitted variable is correlated with the included predictors (\(\mathbf{X_1'X_2} \neq 0\)), then the OLS estimator will be biased.
\item
  This bias arises because the omitted variable contributes to the variation in the dependent variable, but its effect is incorrectly attributed to other predictors.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{conditional-variance-of-ols-estimator}{%
\paragraph{Conditional Variance of OLS Estimator}\label{conditional-variance-of-ols-estimator}}

Under assumptions \protect\hyperlink{a1-linearity}{A1}, \protect\hyperlink{a2-full-rank}{A2}, \protect\hyperlink{a3-exogeneity-of-independent-variables}{A3}, and \protect\hyperlink{a4-homoskedasticity}{A4}, the \textbf{conditional variance of the OLS estimator} is:

\[
\begin{aligned}
Var(\hat{\beta}|\mathbf{X}) &= Var(\beta + \mathbf{(X'X)^{-1}X'\epsilon|X}) && \text{A1-A2} \\
    &= Var((\mathbf{X'X)^{-1}X'\epsilon|X}) \\
    &= \mathbf{(X'X)^{-1}X'} Var(\epsilon|\mathbf{X})\mathbf{X(X'X)^{-1}} \\
    &= \mathbf{(X'X)^{-1}X'} \sigma^2 I \mathbf{X(X'X)^{-1}} && \text{A4} \\
    &= \sigma^2 \mathbf{(X'X)^{-1}}
\end{aligned}
\]

This result shows that the variance of \(\hat{\beta}\) depends on:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(\sigma^2\): The variance of the errors.
\item
  \(\mathbf{X'X}\): The information content in the design matrix \(\mathbf{X}\).
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{sources-of-variation-in-ols-estimator}{%
\paragraph{Sources of Variation in OLS Estimator}\label{sources-of-variation-in-ols-estimator}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Unexplained Variation in the Dependent Variable}: \(\sigma^2 = Var(\epsilon_i|\mathbf{X})\)
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Large \(\sigma^2\) indicates that the amount of unexplained variation (noise) is high relative to the explained variation (\(\mathbf{x_i \beta}\)).
\item
  This increases the variance of the OLS estimator.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Small Variation in Predictor Variables}
\end{enumerate}

\begin{itemize}
\tightlist
\item
  If the variance of predictors (\(Var(x_{i1}), Var(x_{i2}), \dots\)) is small, the design matrix \(\mathbf{X}\) lacks information, leading to:

  \begin{itemize}
  \tightlist
  \item
    High variability in \(\hat{\beta}\).
  \item
    Potential issues in estimating coefficients accurately.
  \end{itemize}
\item
  \textbf{Small sample size} exacerbates this issue, as fewer observations reduce the robustness of parameter estimates.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Correlation Between Explanatory Variables (Collinearity)}
\end{enumerate}

\begin{itemize}
\tightlist
\item
  \textbf{Strong correlation} among explanatory variables creates problems:

  \begin{itemize}
  \tightlist
  \item
    \(x_{i1}\) being highly correlated with a linear combination of \(1, x_{i2}, x_{i3}, \dots\) contributes to inflated standard errors for \(\hat{\beta}_1\).
  \item
    Including many irrelevant variables exacerbates this issue.
  \end{itemize}
\item
  \textbf{Perfect Collinearity}:

  \begin{itemize}
  \tightlist
  \item
    If \(x_1\) is perfectly determined by a linear combination of other predictors, the matrix \(\mathbf{X'X}\) becomes singular.
  \item
    This violates \protect\hyperlink{a2-full-rank}{A2}, making OLS impossible to compute.
  \end{itemize}
\item
  \textbf{Multicollinearity}:

  \begin{itemize}
  \tightlist
  \item
    If \(x_1\) is highly correlated (but not perfectly) with a linear combination of other variables, the variance of \(\hat{\beta}_1\) increases.
  \item
    Multicollinearity does not violate OLS assumptions but weakens inference by inflating standard errors.
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{standard-errors}{%
\paragraph{Standard Errors}\label{standard-errors}}

Standard errors measure the variability of an estimator, specifically the standard deviation of \(\hat{\beta}\). They are crucial for inference, as they quantify the uncertainty associated with parameter estimates.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The variance of the OLS estimator \(\hat{\beta}\) is:

\[
Var(\hat{\beta}|\mathbf{X}) = \sigma^2 \mathbf{(X'X)^{-1}}
\]

Where:

\begin{itemize}
\tightlist
\item
  \(\sigma^2\): Variance of the error terms.
\item
  \(\mathbf{(X'X)^{-1}}\): Inverse of the design matrix product, capturing the geometry of the predictors.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Estimation of} \(\sigma^2\)

Under assumptions \protect\hyperlink{a1-linearity}{A1} through \protect\hyperlink{a5-homoskedasticity}{A5}, we can estimate \(\sigma^2\) as:

\[
s^2 = \frac{1}{n-k} \sum_{i=1}^{n} e_i^2 = \frac{1}{n-k} SSR
\]

Where:

\begin{itemize}
\tightlist
\item
  \(n\): Number of observations.
\item
  \(k\): Number of predictors, including the intercept.
\item
  \(e_i\): Residuals from the regression model (\(e_i = y_i - \hat{y}_i\)).
\item
  \(SSR\): Sum of squared residuals (\(\sum e_i^2\)).
\end{itemize}

The degrees of freedom adjustment (\(n-k\)) accounts for the fact that residuals \(e_i\) are not true errors \(\epsilon_i\). Since the regression model uses \(k\) parameters, we lose \(k\) degrees of freedom in estimating variance.

The standard error for \(\sigma\) is:

\[
s = \sqrt{s^2}
\]

However, \(s\) is a biased estimator of \(\sigma\) due to \protect\hyperlink{jensens-inequality}{Jensen's Inequality}.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The standard error of each regression coefficient \(\hat{\beta}_{j-1}\) is:

\[
SE(\hat{\beta}_{j-1}) = s \sqrt{[(\mathbf{X'X})^{-1}]_{jj}}
\]

Alternatively, it can be expressed in terms of \(SST_{j-1}\) and \(R_{j-1}^2\):

\[
SE(\hat{\beta}_{j-1}) = \frac{s}{\sqrt{SST_{j-1}(1 - R_{j-1}^2)}}
\]

Where:

\begin{itemize}
\tightlist
\item
  \(SST_{j-1}\): Total sum of squares for \(x_{j-1}\) from a regression of \(x_{j-1}\) on all other predictors.
\item
  \(R_{j-1}^2\): Coefficient of determination for the same regression.
\end{itemize}

This formulation highlights the role of multicollinearity, as \(R_{j-1}^2\) reflects the correlation between \(x_{j-1}\) and other predictors.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{summary-of-finite-sample-properties-of-ols-under-different-assumptions}{%
\paragraph{Summary of Finite Sample Properties of OLS Under Different Assumptions}\label{summary-of-finite-sample-properties-of-ols-under-different-assumptions}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Under \protect\hyperlink{a1-linearity}{A1-A3}}: OLS is unbiased. \[ E(\hat{\beta}) = \beta \]
\item
  \textbf{Under \protect\hyperlink{a4-homoskedasticity}{A1-A4}}: The variance of the OLS estimator is: \[ Var(\hat{\beta}|\mathbf{X}) = \sigma^2 \mathbf{(X'X)^{-1}} \]
\item
  \textbf{Under \protect\hyperlink{a6-normality-of-errors}{A1-A4, A6}}: The OLS estimator is normally distributed: \[ \hat{\beta} \sim N(\beta, \sigma^2 \mathbf{(X'X)^{-1}}) \]
\item
  \textbf{Under \protect\hyperlink{gauss-markov-theorem}{A1-A4, Gauss-Markov Theorem}}: OLS is BLUE (Best Linear Unbiased Estimator).
\item
  \textbf{Under \protect\hyperlink{a5-homoskedasticity}{A1-A5}}: The standard errors for \(\hat{\beta}\) are unbiased estimators of the standard deviation of \(\hat{\beta}\).
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{large-sample-properties}{%
\subsubsection{Large Sample Properties}\label{large-sample-properties}}

Large sample properties provide a framework to evaluate the quality of estimators when finite sample properties are either uninformative or computationally infeasible. This perspective becomes crucial in modern data analysis, especially for methods like GLS or MLE, where assumptions for finite sample analysis may not hold.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1720}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4268}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3949}}@{}}
\caption{When to Use Finite vs.~Large Sample Analysis}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Finite Sample Properties}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Large Sample Properties}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Finite Sample Properties}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Large Sample Properties}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Applicability} & Limited to fixed sample sizes & Relevant for \(n \to \infty\) \\
\textbf{Exactness} & Exact results (e.g., distributions, unbiasedness) & Approximate results \\
\textbf{Assumptions} & May require stronger assumptions (e.g., normality, independence) & Relies on asymptotic approximations (e.g., CLT, LLN) \\
\textbf{Estimator Behavior} & Performance may vary significantly & Estimators stabilize and improve in accuracy \\
\textbf{Ease of Use} & Often complex due to reliance on exact distributions & Simplifies analysis by leveraging asymptotic approximations \\
\textbf{Real-World Relevance} & More realistic for small datasets & More relevant for large datasets \\
\end{longtable}

\begin{itemize}
\item
  \textbf{Finite Sample Analysis}:

  \begin{itemize}
  \item
    Small sample sizes (e.g., \(n < 30\)).
  \item
    Critical for studies where exact results are needed.
  \item
    Useful in experimental designs and case studies.
  \end{itemize}
\item
  \textbf{Large Sample Analysis}:

  \begin{itemize}
  \item
    Large datasets (e.g., \(n > 100\)).
  \item
    Necessary when asymptotic approximations improve computational simplicity.
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Key Concepts:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Consistency}:

  \begin{itemize}
  \tightlist
  \item
    Consistency ensures that an estimator converges in probability to the true parameter value as the sample size increases.
  \item
    Mathematically, an estimator \(\hat{\theta}\) is consistent for \(\theta\) if: \[
    \hat{\theta}_n \to^p \theta \quad \text{as } n \to \infty.
    \]
  \item
    Consistency does not imply unbiasedness, and unbiasedness does not guarantee consistency.
  \end{itemize}
\item
  \textbf{Asymptotic Distribution}:

  \begin{itemize}
  \tightlist
  \item
    The limiting distribution describes the shape of the scaled estimator as \(n \to \infty\).
  \item
    Asymptotic distributions often follow normality due to the \protect\hyperlink{central-limit-theorem}{Central Limit Theorem}, which underpins much of inferential statistics.
  \end{itemize}
\item
  \textbf{Asymptotic Variance}:

  \begin{itemize}
  \tightlist
  \item
    Represents the spread of the estimator with respect to its limiting distribution.
  \item
    Smaller asymptotic variance implies greater precision in large samples.
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Motivation}

\protect\hyperlink{finite-sample-properties}{Finite Sample Properties}, such as unbiasedness, rely on strong assumptions like:

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{a1-linearity}{A1 Linearity}
\item
  \protect\hyperlink{a3-exogeneity-of-independent-variables}{A3 Exogeneity of Independent Variables}
\item
  \protect\hyperlink{a4-homoskedasticity}{A4 Homoskedasticity}
\item
  \protect\hyperlink{a6-normal-distribution}{A6 Normal Distribution}
\end{itemize}

When these assumptions are violated or impractical to verify, finite sample properties lose relevance. In such cases, \protect\hyperlink{large-sample-properties}{Large Sample Properties}serve as an essential alternative for evaluating estimators.

For example, let the conditional expectation function (CEF) be: \[
\mu(\mathbf{X}) = E(y | \mathbf{X}),
\] which represents the \textbf{minimum mean squared predictor} over all possible functions \(f(\mathbf{X})\): \[
\min_f E((y - f(\mathbf{X}))^2).
\]

Under the assumptions A1 and A3, the CEF simplifies to: \[
\mu(\mathbf{X}) = \mathbf{X}\beta.
\]

The \textbf{linear projection} is given by: \[
L(y | 1, \mathbf{X}) = \gamma_0 + \mathbf{X}\text{Var}(\mathbf{X})^{-1}\text{Cov}(\mathbf{X}, y),
\] where: \[
\gamma = \mathbf{X}\text{Var}(\mathbf{X})^{-1}\text{Cov}(\mathbf{X}, y).
\]

This linear projection minimizes the mean squared error: \[
(\gamma_0, \gamma) = \arg\min_{(a, b)} E\left[\left(E(y|\mathbf{X}) - \left(a + \mathbf{X}b\right)\right)^2\right].
\]

Implications for OLS

\begin{itemize}
\tightlist
\item
  \textbf{Consistency}: OLS is always consistent for the linear projection, ensuring convergence to the true parameter value as \(n \to \infty\).
\item
  \textbf{Causal Interpretation}: The linear projection has no inherent causal interpretation---it approximates the conditional mean function.
\item
  \textbf{Assumption Independence}: Unlike the CEF, the linear projection does not depend on assumptions A1 and A3.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Evaluating Estimators via Large Sample Properties}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Consistency}:

  \begin{itemize}
  \tightlist
  \item
    Measures the estimator's centrality to the true value.
  \item
    A consistent estimator ensures that with larger samples, estimates become arbitrarily close to the population parameter.
  \end{itemize}
\item
  \textbf{Limiting Distribution}:

  \begin{itemize}
  \tightlist
  \item
    Helps infer the sampling behavior of the estimator as \(n\) grows.
  \item
    Often approximated by a normal distribution for practical use in hypothesis testing and confidence interval construction.
  \end{itemize}
\item
  \textbf{Asymptotic Variance}:

  \begin{itemize}
  \tightlist
  \item
    Quantifies the dispersion of the estimator around its limiting distribution.
  \item
    Smaller variance is desirable for greater reliability.
  \end{itemize}
\end{enumerate}

An estimator \(\hat{\theta}\) is \textbf{consistent} for a parameter \(\theta\) if, as the sample size \(n\) increases, \(\hat{\theta}\) converges in probability to \(\theta\):

\[
\hat{\theta}_n \to^p \theta \quad \text{as } n \to \infty.
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Convergence in Probability}:

  \begin{itemize}
  \tightlist
  \item
    The probability that \(\hat{\theta}\) deviates from \(\theta\) by more than a small margin (no matter how small) approaches zero as \(n\) increases.
  \end{itemize}

  Formally: \[
  \forall \epsilon > 0, \quad P(|\hat{\theta}_n - \theta| > \epsilon) \to 0 \quad \text{as } n \to \infty.
  \]
\item
  \textbf{Interpretation}: Consistency ensures that the estimator becomes arbitrarily close to the true population parameter \(\theta\) as the sample size grows.
\item
  \textbf{Asymptotic Behavior}: Large sample properties rely on consistency to provide valid approximations of an estimator's behavior in finite samples.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Relationship Between Consistency and Unbiasedness}

\begin{itemize}
\tightlist
\item
  \textbf{Unbiasedness}:

  \begin{itemize}
  \tightlist
  \item
    An estimator \(\hat{\theta}\) is unbiased if its expected value equals the true parameter: \[
    E(\hat{\theta}) = \theta.
    \]
  \item
    Unbiasedness is a finite-sample property and does not depend on the sample size.
  \end{itemize}
\item
  \textbf{Consistency}:

  \begin{itemize}
  \tightlist
  \item
    Consistency is a large-sample property and requires \(\hat{\theta}\) to converge to \(\theta\) as \(n \to \infty\).
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Important Distinctions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Unbiasedness Does Not Imply Consistency}:

  \begin{itemize}
  \tightlist
  \item
    Example: Consider an unbiased estimator with extremely high variance that does not diminish as \(n\) increases. Such an estimator does not converge to \(\theta\) in probability.
  \end{itemize}
\item
  \textbf{Consistency Does Not Imply Unbiasedness}:

  \begin{itemize}
  \tightlist
  \item
    Example: \(\hat{\theta}_n = \frac{n-1}{n}\theta\) is biased for all finite \(n\), but as \(n \to \infty\), \(\hat{\theta}_n \to^p \theta\), making it consistent.
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

From the OLS formula: \[
\hat{\beta} = \mathbf{(X'X)^{-1}X'y},
\] we can expand as: \[
\hat{\beta} = \mathbf{(\sum_{i=1}^{n}x_i'x_i)^{-1} \sum_{i=1}^{n}x_i'y_i},
\] or equivalently: \[
\hat{\beta} = (n^{-1}\mathbf{\sum_{i=1}^{n}x_i'x_i)^{-1}} n^{-1}\mathbf{\sum_{i=1}^{n}x_i'y_i}.
\]

Taking the probability limit under the assumptions \protect\hyperlink{a2-full-rank}{A2} and \protect\hyperlink{a5-data-generation-random-sampling}{A5}, we apply the \protect\hyperlink{weak-law-of-large-numbers}{Weak Law of Large Numbers} (for a random sample, averages converge to expectations as \(n \to \infty\)): \[
plim(\hat{\beta}) = plim((n^{-1}\mathbf{\sum_{i=1}^{n}x_i'x_i)^{-1}})plim(n^{-1}\mathbf{\sum_{i=1}^{n}x_i'y_i}),
\] which simplifies to: \[
\begin{aligned}
plim(\hat{\beta}) 
  &= (E(\mathbf{x_i'x_i}))^{-1}E(\mathbf{x_i'y_i}) & \text{A1}\\
  &= (E(\mathbf{x_i'x_i}))^{-1} \bigl(E(\mathbf{x_i'x_i} \,\beta) + E(\mathbf{x_i\,\epsilon_i})\bigr) & (A3a) \\
  &= (E(\mathbf{x_i' x_i}))^{-1}E(\mathbf{x_i' x_i})\, \beta & (A2)\\
  &= \beta 
\end{aligned}
\]

\textbf{Proof:}

For a model of \(y_i = x_i \beta + \epsilon_i\), where \(\beta\) is the true parameter vector, \(\epsilon_i\) is the random error:

Expanding \(E(x_i y_i)\):

\[
E(x_i'y_i) = E(x_i'(x_i\beta + \epsilon_i))
\]

By the linearity of expectation:

\[
E(x_i'y_i) = E(x_i'x_i \beta) + E(x_i \epsilon_i)
\]

The second term \(E(x_i \epsilon_i) = 0\) under assumption \protect\hyperlink{a3a}{A3a}.

Thus, \[
E(x_i'y_i) = E(x_i'x_i)\beta.
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Consistency of OLS}

Hence, in short, under the assumptions:

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{a1-linearity}{A1 Linearity}
\item
  \protect\hyperlink{a2-full-rank}{A2 Full Rank}
\item
  \protect\hyperlink{a3a-weak-exogeneity}{A3a: Weaker Exogeneity Assumption}
\item
  \protect\hyperlink{a5-data-generation-random-sampling}{A5 Data Generation (Random Sampling)}
\end{itemize}

the term \(E(\mathbf{x_i'\epsilon_i}) = 0\), and the OLS estimator is \textbf{consistent}: \[
plim(\hat{\beta}) = \beta.
\]

However, OLS consistency does not guarantee unbiasedness in small samples.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{asymptotic-distribution-of-ols}{%
\paragraph{Asymptotic Distribution of OLS}\label{asymptotic-distribution-of-ols}}

Under the same assumptions :

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{a1-linearity}{A1 Linearity}
\item
  \protect\hyperlink{a2-full-rank}{A2 Full Rank}
\item
  \protect\hyperlink{a3a-weak-exogeneity}{A3a: Weaker Exogeneity Assumption}
\item
  \protect\hyperlink{a5-data-generation-random-sampling}{A5 Data Generation (Random Sampling)}
\end{itemize}

and if \(\mathbf{x_i'x_i}\) has finite first and second moments (required for the \protect\hyperlink{central-limit-theorem}{Central Limit Theorem}), we have:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Convergence of \(n^{-1}\sum_{i=1}^n \mathbf{x_i'x_i}\): \[
  n^{-1}\sum_{i=1}^n \mathbf{x_i'x_i} \to^p E(\mathbf{x_i'x_i}).
  \]
\item
  Asymptotic normality of \(\sqrt{n}(n^{-1}\sum_{i=1}^n \mathbf{x_i'\epsilon_i})\): \[
  \sqrt{n}(n^{-1}\sum_{i=1}^n \mathbf{x_i'\epsilon_i}) \to^d N(0, \mathbf{B}),
  \] where \(\mathbf{B} = Var(\mathbf{x_i'\epsilon_i})\).
\end{enumerate}

From these results, the scaled difference between \(\hat{\beta}\) and \(\beta\) follows: \[
\sqrt{n}(\hat{\beta} - \beta) = (n^{-1}\sum_{i=1}^n \mathbf{x_i'x_i})^{-1} \sqrt{n}(n^{-1}\sum_{i=1}^n \mathbf{x_i'\epsilon_i}).
\]

By the \protect\hyperlink{central-limit-theorem}{Central Limit Theorem}: \[
\sqrt{n}(\hat{\beta} - \beta) \to^d N(0, \Sigma),
\] where: \[
\Sigma = (E(\mathbf{x_i'x_i}))^{-1} \mathbf{B} (E(\mathbf{x_i'x_i}))^{-1}.
\]

The sandwich form is \(\Sigma\) is standard.

\textbf{Implications for Homoskedasticity (A4) vs.~Heteroskedasticity}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  No Homoskedasticity (\protect\hyperlink{a4-homoskedasticity}{A4 Homoskedasticity}) needed:

  \begin{itemize}
  \tightlist
  \item
    the CLT and the large-sample distribution of \(\hat{\beta}\) do \emph{not} require homoskedasticity. The only place homoskedasticity would simplify things is that \[
    \mathbf{B} = Var(\mathbf{x_i'\epsilon_i}) = \sigma^2 E(\mathbf{x_i'x_i}),
    \]
  \end{itemize}

  only if \(Var(\epsilon_i | \mathbf{x}_i) \sigma^2\)

  Then \[
  \Sigma = \sigma^2 (E(\mathbf{x_i'x_i}))^{-1}.
  \]
\item
  Adjusting for Heteroskedasticity:

  \begin{itemize}
  \tightlist
  \item
    In practice, \(\sigma_i^2\) can vary with \(\mathbf{x}_i\)\hspace{0pt}, leading to heteroskedasticity.
  \item
    The standard OLS formula for \(Var(\hat{\beta})\) is inconsistent under heteroskedasticity, so one uses \emph{robust (White) standard errors}.
  \item
    Heteroskedasticity can arise from (but not limited to):

    \begin{itemize}
    \tightlist
    \item
      Limited dependent variables.
    \item
      Dependent variables with large or skewed ranges.
    \end{itemize}
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{derivation-of-asymptotic-variance}{%
\paragraph{Derivation of Asymptotic Variance}\label{derivation-of-asymptotic-variance}}

The asymptotic variance of the OLS estimator is derived as follows:

\[
\Sigma = (E(\mathbf{x_i'x_i}))^{-1}\mathbf{B}(E(\mathbf{x_i'x_i}))^{-1}.
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Substituting \(\mathbf{B} = Var(\mathbf{x_i'}\epsilon_i)\): \[
  \Sigma = (E(\mathbf{x_i'x_i}))^{-1}Var(\mathbf{x_i'}\epsilon_i)(E(\mathbf{x_i'x_i}))^{-1}.
  \]
\item
  Using the definition of variance: \[
  \Sigma = (E(\mathbf{x_i'x_i}))^{-1}E[(\mathbf{x_i'}\epsilon_i - 0)(\mathbf{x_i'}\epsilon_i - 0)'](E(\mathbf{x_i'x_i}))^{-1}.
  \]
\item
  By the {[}Law of Iterated Expectations{]} and \protect\hyperlink{a3a-weak-exogeneity}{A3a: Weaker Exogeneity Assumption}, we have: \[
  E[(\mathbf{x_i'}\epsilon_i)(\mathbf{x_i'}\epsilon_i)'] = E[E(\epsilon_i^2|\mathbf{x_i})\mathbf{x_i'x_i}],
  \]
\item
  Assuming homoskedasticity (\protect\hyperlink{a4-homoskedasticity}{A4 Homoskedasticity}), \(E(\epsilon_i^2|\mathbf{x_i}) = \sigma^2\), so: \[
  \Sigma = (E(\mathbf{x_i'x_i}))^{-1}\sigma^2E(\mathbf{x_i'x_i})(E(\mathbf{x_i'x_i}))^{-1}.
  \]
\item
  Simplifying: \[
  \Sigma = \sigma^2(E(\mathbf{x_i'x_i}))^{-1}.
  \]
\end{enumerate}

Hence, under the assumptions:

\begin{itemize}
\item
  \protect\hyperlink{a1-linearity}{A1 Linearity}
\item
  \protect\hyperlink{a2-full-rank}{A2 Full Rank}
\item
  \protect\hyperlink{a3a-weak-exogeneity}{A3a: Weaker Exogeneity Assumption}
\item
  \protect\hyperlink{a4-homoskedasticity}{A4 Homoskedasticity}
\item
  \protect\hyperlink{a5-data-generation-random-sampling}{A5 Data Generation (Random Sampling)}
\end{itemize}

we have\[
\sqrt{n}(\hat{\beta} - \beta) \to^d N(0, \sigma^2(E(\mathbf{x_i'x_i}))^{-1}).
\]

The asymptotic variance provides an approximation of the scaled estimator's variance for large \(n\). This leads to:

\[
Avar(\sqrt{n}(\hat{\beta} - \beta)) = \sigma^2(E(\mathbf{x_i'x_i}))^{-1}.
\]

The finite sample variance of an estimator can be approximated using the asymptotic variance for large sample sizes:

\[
\begin{aligned}
Avar(\sqrt{n}(\hat{\beta}-\beta)) &\approx Var(\sqrt{n}(\hat{\beta}-\beta)) \\
Avar(\sqrt{n}(\hat{\beta}-\beta))/n &\approx Var(\sqrt{n}(\hat{\beta}-\beta))/n = Var(\hat{\beta})
\end{aligned}
\]

However, it is critical to note that \textbf{asymptotic variance} (\(Avar(.)\)) does not behave in the same manner as finite sample variance (\(Var(.)\)). This distinction is evident in the following expressions:

\[
\begin{aligned}
Avar(\sqrt{n}(\hat{\beta}-\beta))/n &\neq Avar(\sqrt{n}(\hat{\beta}-\beta)/\sqrt{n}) \\
&\neq Avar(\hat{\beta})
\end{aligned}
\]

Thus, while \(Avar(.)\) provides a useful approximation for large samples, its conceptual properties differ from finite sample variance.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

In \protect\hyperlink{finite-sample-properties}{Finite Sample Properties}, the standard errors are calculated as estimates of the conditional standard deviation:

\[
SE_{fs}(\hat{\beta}_{j-1}) = \sqrt{\hat{Var}(\hat{\beta}_{j-1}|\mathbf{X})} = \sqrt{s^2 \cdot [\mathbf{(X'X)}^{-1}]_{jj}},
\]

where:

\begin{itemize}
\item
  \(s^2\) is the estimator of the error variance,
\item
  \([\mathbf{(X'X)}^{-1}]_{jj}\) represents the \(j\)th diagonal element of the inverse design matrix.
\end{itemize}

In contrast, in \protect\hyperlink{large-sample-properties}{Large Sample Properties}, the standard errors are calculated as estimates of the square root of the asymptotic variance:

\[
SE_{ls}(\hat{\beta}_{j-1}) = \sqrt{\hat{Avar}(\sqrt{n} \hat{\beta}_{j-1})/n} = \sqrt{s^2 \cdot [\mathbf{(X'X)}^{-1}]_{jj}}.
\]

Interestingly, the standard error estimator is \textbf{identical} for both finite and large samples:

\begin{itemize}
\tightlist
\item
  The expressions for \(SE_{fs}\) and \(SE_{ls}\) are mathematically the same.
\item
  However, they are conceptually estimating two \textbf{different quantities}:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Finite Sample Standard Error}: An estimate of the conditional standard deviation of \(\hat{\beta}_{j-1}\) given \(\mathbf{X}\).
  \item
    \textbf{Large Sample Standard Error}: An estimate of the square root of the asymptotic variance of \(\hat{\beta}_{j-1}\).
  \end{itemize}
\end{itemize}

The assumptions required for these estimators to be valid differ in their stringency:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Finite Sample Variance (Conditional Variance):}

  \begin{itemize}
  \tightlist
  \item
    Requires stronger assumptions (A1-A5).
  \end{itemize}
\item
  \textbf{Asymptotic Variance:}

  \begin{itemize}
  \tightlist
  \item
    Valid under weaker assumptions (A1, A2, A3a, A4, A5).
  \end{itemize}
\end{enumerate}

This distinction highlights the utility of asymptotic properties in providing robust approximations when finite sample assumptions may not hold.

\hypertarget{diagnostics}{%
\subsubsection{Diagnostics}\label{diagnostics}}

\hypertarget{normality-of-errors}{%
\paragraph{Normality of Errors}\label{normality-of-errors}}

Ensuring the normality of errors is a critical assumption in many regression models. Deviations from this assumption can impact inference and model interpretation. For diagnoses assessing normality, see \protect\hyperlink{normality-assessment}{Normality Assessment}.

Plots are invaluable for visual inspection of normality. One common approach is the Q-Q plot, which compares the quantiles of the residuals against those of a standard normal distribution:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example Q{-}Q plot}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{) }\CommentTok{\# For reproducibility}
\NormalTok{y }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\DecValTok{100}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{) }\CommentTok{\# Generating random normal data}
\FunctionTok{qqplot}\NormalTok{(x,}
\NormalTok{       y,}
       \AttributeTok{main =} \StringTok{"Q{-}Q Plot"}\NormalTok{,}
       \AttributeTok{xlab =} \StringTok{"Theoretical Quantiles"}\NormalTok{,}
       \AttributeTok{ylab =} \StringTok{"Sample Quantiles"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{05-linear-regression_files/figure-latex/unnamed-chunk-5-1} \end{center}

\hypertarget{influential-observations-and-outliers}{%
\paragraph{Influential Observations and Outliers}\label{influential-observations-and-outliers}}

Identifying influential observations or outliers is essential for robust regression modeling. The \textbf{hat matrix} (\(\mathbf{H}\)) plays a key role in diagnosing influence.

\hypertarget{hat-matrix-outliers-in-x-space}{%
\subparagraph{Hat Matrix: Outliers in X-Space}\label{hat-matrix-outliers-in-x-space}}

The hat matrix is primarily concerned with \textbf{leverage}, which reflects how far an observation's predictor values (\(X\)-space) are from the centroid of the predictor space.

\begin{itemize}
\item
  \textbf{What it measures:} The diagonal elements of the hat matrix quantify \textbf{leverage}, not residual size or model fit. High leverage suggests that an observation has an unusual predictor configuration and might disproportionately influence the regression line, irrespective of the response variable.
\item
  \textbf{What it doesn't measure:} It doesn't directly account for outliers in \(Y\)-space or residuals.
\end{itemize}

The hat matrix, defined as:

\[
\mathbf{H} = \mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}
\]

has the following properties:

\begin{itemize}
\item
  \textbf{Fitted Values}: \(\hat{\mathbf{Y}} = \mathbf{H}\mathbf{Y}\).
\item
  \textbf{Residuals}: \(\mathbf{e} = (\mathbf{I} - \mathbf{H})\mathbf{Y}\).
\item
  \textbf{Variance of Residuals}: \(\text{var}(\mathbf{e}) = \sigma^2 (\mathbf{I} - \mathbf{H})\).
\end{itemize}

Diagonal Elements of the Hat Matrix (\(h_{ii}\))

\begin{itemize}
\item
  \(h_{ii}\) is the \(i\)-th element on the main diagonal of \(\mathbf{H}\). It must satisfy \(0 \leq h_{ii} \leq 1\).
\item
  \(\sum_{i=1}^{n} h_{ii} = p\), where \(p\) is the number of parameters (including the intercept).
\item
  The variance of residuals for observation \(i\) is given by \(\sigma^2(e_i) = \sigma^2 (1 - h_{ii})\).
\item
  The covariance between residuals for observations \(i\) and \(j\) (\(i \neq j\)) is \(-h_{ij}\sigma^2\).
\end{itemize}

For large datasets, the off-diagonal elements (\(h_{ij}\)) tend to have small covariance if model assumptions hold.

Estimations Using MSE

\begin{itemize}
\item
  Variance of residuals: \(s^2(e_i) = MSE (1 - h_{ii})\), where \(MSE\) is the mean squared error.
\item
  Covariance of residuals: \(\hat{\text{cov}}(e_i, e_j) = -h_{ij}(MSE)\).
\end{itemize}

Interpretation of \(h_{ii}\)

If \(\mathbf{x}_i = [1, X_{i,1}, \ldots, X_{i,p-1}]'\) represents the vector of predictor values for observation \(i\), then:

\[
h_{ii} = \mathbf{x}_i' (\mathbf{X}'\mathbf{X})^{-1} \mathbf{x}_i
\]

The value of \(h_{ii}\) depends on the relative positions of the design points \(X_{i,1}, \ldots, X_{i,p-1}\). Observations with high \(h_{ii}\) are more influential and warrant closer inspection for leverage or outlier behavior.

\hypertarget{studentized-residuals-outliers-in-y-space}{%
\subparagraph{Studentized Residuals: Outliers in Y-Space}\label{studentized-residuals-outliers-in-y-space}}

Residuals focus on discrepancies between observed (\(Y\)) and predicted (\(\hat{Y}\)) values, helping to identify outliers in \(Y\)-space.

\begin{itemize}
\item
  \textbf{What they measure:}

  \begin{itemize}
  \item
    Standardized or studentized residuals assess how far an observation's response is from the regression line, adjusted for variance.
  \item
    \textbf{Externally studentized residuals} are more robust because they exclude the \(i\)-th observation when estimating variance.
  \item
    Large studentized residuals (e.g., \(>2\) or \(>3\)) indicate observations that are unusual in \(Y\)-space.
  \end{itemize}
\item
  \textbf{What they don't measure:} They do not consider leverage or the \(X\)-space configuration. A point with a large residual could have low leverage, making it less influential overall.
\end{itemize}

Studentized residuals, also known as standardized residuals, adjust for the variance of residuals by dividing the residuals by their standard error:

\[
\begin{aligned}
r_i &= \frac{e_i}{s(e_i)} \\
r_i &\sim N(0,1),
\end{aligned}
\]

where \(s(e_i) = \sqrt{MSE(1-h_{ii})}\), and \(r_i\) accounts for the varying variances of residuals. These residuals allow for a better comparison of model fit across observations.

\begin{itemize}
\tightlist
\item
  \textbf{Semi-Studentized Residuals}: In contrast, the semi-studentized residuals are defined as:
\end{itemize}

\[
e_i^* = \frac{e_i}{\sqrt{MSE}}
\]

This approach does not adjust for the heterogeneity in variances of residuals, as \(e_i^*\) assumes equal variance for all residuals.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

To assess the influence of individual observations, we consider the model without a particular value. When the \(i\)-th observation is removed, the \textbf{deleted residual} is defined as:

\[
\begin{aligned}
d_i &= Y_i - \hat{Y}_{i(i)} \\
    &= \frac{e_i}{1-h_{ii}},
\end{aligned}
\]

where \(Y_i\) is the actual observation, and \(\hat{Y}_{i(i)}\) is the predicted value for the \(i\)-th observation, computed using the regression model fitted to the remaining \(n-1\) observations. Importantly, we do not need to refit the regression model for each observation to compute \(d_i\).

\begin{itemize}
\tightlist
\item
  As \(h_{ii}\) (leverage) increases, \(d_i\) also increases, indicating higher influence of the observation.
\end{itemize}

The variance of the deleted residual is given by:

\[
s^2(d_i) = \frac{MSE_{(i)}}{1-h_{ii}},
\]

where \(MSE_{(i)}\) is the mean squared error when the \(i\)-th case is omitted.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The \textbf{studentized deleted residual} accounts for variability and follows a \(t\)-distribution with \(n-p-1\) degrees of freedom:

\[
t_i = \frac{d_i}{s(d_i)} = \frac{e_i}{\sqrt{MSE_{(i)}(1-h_{ii})}},
\]

where \(t_i\) helps identify outliers more effectively.

We can compute \(t_i\) without fitting the regression model multiple times. Using the relationship:

\[
(n-p)MSE = (n-p-1)MSE_{(i)} + \frac{e_i^2}{1-h_{ii}},
\]

we derive:

\[
t_i = e_i \sqrt{\frac{n-p-1}{SSE(1-h_{ii}) - e_i^2}}.
\]

This formulation avoids the need for recalculating \(MSE_{(i)}\) explicitly for each case.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Outlying \(Y\)-observations are those with large studentized deleted residuals in absolute value. To handle multiple testing when there are many residuals, we use a Bonferroni-adjusted critical value:

\[
t_{1-\alpha/2n; n-p-1},
\]

where \(\alpha\) is the desired significance level, \(n\) is the sample size, and \(p\) is the number of parameters in the model. Observations exceeding this threshold are flagged as potential outliers.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example R Code for Demonstrating Residual Diagnostics}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{) }\CommentTok{\# For reproducibility}

\CommentTok{\# Simulate some data}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{100}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =} \DecValTok{10}\NormalTok{, }\AttributeTok{sd =} \DecValTok{3}\NormalTok{)}
\NormalTok{y }\OtherTok{\textless{}{-}} \DecValTok{5} \SpecialCharTok{+} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ x }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{2}\NormalTok{)}

\CommentTok{\# Fit a linear regression model}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x)}


\CommentTok{\# Extract residuals, fitted values, and hat values}
\NormalTok{residuals }\OtherTok{\textless{}{-}} \FunctionTok{resid}\NormalTok{(model)}
\NormalTok{hat\_values }\OtherTok{\textless{}{-}} \FunctionTok{lm.influence}\NormalTok{(model)}\SpecialCharTok{$}\NormalTok{hat}
\NormalTok{mse }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(residuals }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{)}
\NormalTok{n }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(residuals)}
\NormalTok{p }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(}\FunctionTok{coefficients}\NormalTok{(model)) }\CommentTok{\# Number of parameters}

\CommentTok{\# Compute studentized residuals}
\NormalTok{studentized\_residuals }\OtherTok{\textless{}{-}}\NormalTok{ residuals }\SpecialCharTok{/} \FunctionTok{sqrt}\NormalTok{(mse }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ hat\_values))}

\CommentTok{\# Compute deleted residuals}
\NormalTok{deleted\_residuals }\OtherTok{\textless{}{-}}\NormalTok{ residuals }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ hat\_values)}

\CommentTok{\# Compute studentized deleted residuals}
\NormalTok{studentized\_deleted\_residuals }\OtherTok{\textless{}{-}}
\NormalTok{    residuals }\SpecialCharTok{*} \FunctionTok{sqrt}\NormalTok{((n }\SpecialCharTok{{-}}\NormalTok{ p }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)}
                     \SpecialCharTok{/}\NormalTok{ (}\FunctionTok{sum}\NormalTok{(residuals }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{) }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ hat\_values) }\SpecialCharTok{{-}}\NormalTok{ residuals }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{))}

\CommentTok{\# Flag potential outliers using Bonferroni{-}adjusted critical value}
\NormalTok{alpha }\OtherTok{\textless{}{-}} \FloatTok{0.05}
\NormalTok{bonferroni\_threshold }\OtherTok{\textless{}{-}} \FunctionTok{qt}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alpha }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ n), }\AttributeTok{df =}\NormalTok{ n }\SpecialCharTok{{-}}\NormalTok{ p }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)}
\NormalTok{outliers }\OtherTok{\textless{}{-}}
    \FunctionTok{abs}\NormalTok{(studentized\_deleted\_residuals) }\SpecialCharTok{\textgreater{}}\NormalTok{ bonferroni\_threshold}

\CommentTok{\# Print results}
\NormalTok{results }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
    \AttributeTok{Residuals                     =}\NormalTok{ residuals,}
    \AttributeTok{Hat\_Values                    =}\NormalTok{ hat\_values,}
    \AttributeTok{Studentized\_Residuals         =}\NormalTok{ studentized\_residuals,}
    \AttributeTok{Deleted\_Residuals             =}\NormalTok{ deleted\_residuals,}
    \AttributeTok{Studentized\_Deleted\_Residuals =}\NormalTok{ studentized\_deleted\_residuals,}
    \AttributeTok{Outlier                       =}\NormalTok{ outliers}
\NormalTok{)}

\NormalTok{causalverse}\SpecialCharTok{::}\FunctionTok{nice\_tab}\NormalTok{(}\FunctionTok{head}\NormalTok{(results))}
\CommentTok{\#\textgreater{}   Residuals Hat\_Values Studentized\_Residuals Deleted\_Residuals}
\CommentTok{\#\textgreater{} 1     {-}1.27       0.02                 {-}0.67             {-}1.29}
\CommentTok{\#\textgreater{} 2      0.70       0.01                  0.36              0.70}
\CommentTok{\#\textgreater{} 3     {-}0.12       0.04                 {-}0.07             {-}0.13}
\CommentTok{\#\textgreater{} 4     {-}0.48       0.01                 {-}0.25             {-}0.49}
\CommentTok{\#\textgreater{} 5     {-}1.68       0.01                 {-}0.88             {-}1.70}
\CommentTok{\#\textgreater{} 6      0.30       0.04                  0.16              0.31}
\CommentTok{\#\textgreater{}   Studentized\_Deleted\_Residuals Outlier}
\CommentTok{\#\textgreater{} 1                         {-}0.66   FALSE}
\CommentTok{\#\textgreater{} 2                          0.36   FALSE}
\CommentTok{\#\textgreater{} 3                         {-}0.06   FALSE}
\CommentTok{\#\textgreater{} 4                         {-}0.25   FALSE}
\CommentTok{\#\textgreater{} 5                         {-}0.87   FALSE}
\CommentTok{\#\textgreater{} 6                          0.15   FALSE}

\CommentTok{\# Plot studentized deleted residuals for visualization}
\FunctionTok{plot}\NormalTok{(}
\NormalTok{    studentized\_deleted\_residuals,}
    \AttributeTok{main =} \StringTok{"Studentized Deleted Residuals"}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"Observation"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"Studentized Deleted Residuals"}\NormalTok{,}
    \AttributeTok{pch =} \DecValTok{16}\NormalTok{,}
    \AttributeTok{col =} \FunctionTok{ifelse}\NormalTok{(outliers, }\StringTok{"red"}\NormalTok{, }\StringTok{"black"}\NormalTok{)}
\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}
    \AttributeTok{h =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{bonferroni\_threshold, bonferroni\_threshold),}
    \AttributeTok{col =} \StringTok{"blue"}\NormalTok{,}
    \AttributeTok{lty =} \DecValTok{2}
\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}
    \StringTok{"topright"}\NormalTok{,}
    \AttributeTok{legend =} \FunctionTok{c}\NormalTok{(}\StringTok{"Potential Outliers"}\NormalTok{, }\StringTok{"Threshold"}\NormalTok{),}
    \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"red"}\NormalTok{, }\StringTok{"blue"}\NormalTok{),}
    \AttributeTok{pch =} \FunctionTok{c}\NormalTok{(}\DecValTok{16}\NormalTok{, }\ConstantTok{NA}\NormalTok{),}
    \AttributeTok{lty =} \FunctionTok{c}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{05-linear-regression_files/figure-latex/unnamed-chunk-6-1} \end{center}

\hypertarget{identifying-influential-cases}{%
\paragraph{Identifying Influential Cases}\label{identifying-influential-cases}}

By \textbf{influential}, we refer to observations whose exclusion causes major changes in the fitted regression model. Note that not all outliers are influential.

Types of Influence Measures

\begin{itemize}
\tightlist
\item
  \textbf{Influence on Single Fitted Values}: \protect\hyperlink{dffits}{DFFITS}
\item
  \textbf{Influence on All Fitted Values}: \protect\hyperlink{cooks-d}{Cook's D}
\item
  \textbf{Influence on the Regression Coefficients}: \protect\hyperlink{dfbetas}{DFBETAS}
\end{itemize}

Measures like Cook's D, DFFITS, and DFBETAS combine leverage (from the hat matrix) and residual size (from studentized residuals) to assess the \textbf{influence} of an observation on the model as a whole. Hence, these effectively combine impact of \(X\)-space and \(Y\)-space.

\hypertarget{dffits}{%
\subparagraph{DFFITS}\label{dffits}}

DFFITS measures the \textbf{influence on single fitted values}. It is defined as:

\[
\begin{aligned}
(DFFITS)_i &= \frac{\hat{Y}_i - \hat{Y}_{i(i)}}{\sqrt{MSE_{(i)}h_{ii}}} \\
&= t_i \left(\frac{h_{ii}}{1-h_{ii}}\right)^{1/2}
\end{aligned}
\]

Where:

\begin{itemize}
\tightlist
\item
  \(\hat{Y}_i\) = fitted value for observation \(i\) using all data.
\item
  \(\hat{Y}_{i(i)}\) = fitted value for observation \(i\) with the \(i\)th case removed.
\item
  \(MSE_{(i)}\) = mean squared error with observation \(i\) excluded.
\item
  \(h_{ii}\) = leverage of the \(i\)th observation.
\item
  \(t_i\) = studentized deleted residual.
\end{itemize}

High DFFITS values occur when leverage and residuals are jointly significant.

DFFITS captures the \textbf{standardized difference between the fitted value for observation} \(i\) with and without the \(i\)th case in the model. It is a product of:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The \textbf{studentized deleted residual}.
\item
  A scaling factor based on the leverage of the \(i\)th observation, \(h_{ii}\).
\end{enumerate}

An observation is considered influential based on DFFITS if:

\begin{itemize}
\tightlist
\item
  \textbf{Small to medium data sets}: \(|DFFITS| > 1\)
\item
  \textbf{Large data sets}: \(|DFFITS| > 2 \sqrt{p/n}\)
\end{itemize}

Where:

\begin{itemize}
\tightlist
\item
  \(p\) = number of predictors (including the intercept).
\item
  \(n\) = total number of observations.
\end{itemize}

This provides a practical threshold for detecting influential observations in different dataset sizes.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load necessary package}
\FunctionTok{library}\NormalTok{(car)}

\CommentTok{\# Fit a linear model}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt, }\AttributeTok{data =}\NormalTok{ mtcars)}

\CommentTok{\# Compute DFFITS}
\NormalTok{dffits\_values }\OtherTok{\textless{}{-}} \FunctionTok{dffits}\NormalTok{(model)}

\CommentTok{\# Display influential observations based on the threshold for a large dataset}
\NormalTok{threshold }\OtherTok{\textless{}{-}} \DecValTok{2} \SpecialCharTok{*} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{length}\NormalTok{(}\FunctionTok{coefficients}\NormalTok{(model)) }\SpecialCharTok{/} \FunctionTok{nrow}\NormalTok{(mtcars))}
\NormalTok{influential\_obs }\OtherTok{\textless{}{-}} \FunctionTok{which}\NormalTok{(}\FunctionTok{abs}\NormalTok{(dffits\_values) }\SpecialCharTok{\textgreater{}}\NormalTok{ threshold)}

\CommentTok{\# Results}
\FunctionTok{list}\NormalTok{(}
    \AttributeTok{DFFITS                   =}\NormalTok{ dffits\_values,}
    \AttributeTok{Threshold                =}\NormalTok{ threshold,}
    \AttributeTok{Influential\_Observations =}\NormalTok{ influential\_obs}
\NormalTok{)}
\CommentTok{\#\textgreater{} $DFFITS}
\CommentTok{\#\textgreater{}           Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive }
\CommentTok{\#\textgreater{}        {-}0.218494101        {-}0.126664789        {-}0.249103400         0.011699160 }
\CommentTok{\#\textgreater{}   Hornet Sportabout             Valiant          Duster 360           Merc 240D }
\CommentTok{\#\textgreater{}         0.028162679        {-}0.253806124        {-}0.191618944         0.221917842 }
\CommentTok{\#\textgreater{}            Merc 230            Merc 280           Merc 280C          Merc 450SE }
\CommentTok{\#\textgreater{}         0.079763706        {-}0.067222732        {-}0.190099538         0.064280875 }
\CommentTok{\#\textgreater{}          Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental }
\CommentTok{\#\textgreater{}         0.020560728        {-}0.135714533         0.008984366         0.227919348 }
\CommentTok{\#\textgreater{}   Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla }
\CommentTok{\#\textgreater{}         1.231668760         0.749153703         0.165329646         0.865985851 }
\CommentTok{\#\textgreater{}       Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28 }
\CommentTok{\#\textgreater{}        {-}0.292008465        {-}0.253389811        {-}0.294709853        {-}0.170476763 }
\CommentTok{\#\textgreater{}    Pontiac Firebird           Fiat X1{-}9       Porsche 914{-}2        Lotus Europa }
\CommentTok{\#\textgreater{}         0.207813200        {-}0.041423665        {-}0.004054382         0.471518032 }
\CommentTok{\#\textgreater{}      Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E }
\CommentTok{\#\textgreater{}        {-}0.161026362        {-}0.129395315         0.907521354        {-}0.128232538 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Threshold}
\CommentTok{\#\textgreater{} [1] 0.6123724}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Influential\_Observations}
\CommentTok{\#\textgreater{} Chrysler Imperial          Fiat 128    Toyota Corolla     Maserati Bora }
\CommentTok{\#\textgreater{}                17                18                20                31}
\end{Highlighting}
\end{Shaded}

\hypertarget{cooks-d}{%
\subparagraph{Cook's D}\label{cooks-d}}

\textbf{Cook's D} measures the \textbf{influence of the} \(i\)th case on all fitted values in a regression model. It is defined as:

\[
\begin{aligned}
D_i &= \frac{\sum_{j=1}^{n}(\hat{Y}_j - \hat{Y}_{j(i)})^2}{p(MSE)} \\
&= \frac{e^2_i}{p(MSE)}\left(\frac{h_{ii}}{(1-h_{ii})^2}\right)
\end{aligned}
\]

Where:

\begin{itemize}
\tightlist
\item
  \(\hat{Y}_j\) = fitted value for observation \(j\) using all data.
\item
  \(\hat{Y}_{j(i)}\) = fitted value for observation \(j\) with the \(i\)th case removed.
\item
  \(e_i\) = residual for observation \(i\).
\item
  \(h_{ii}\) = leverage of the \(i\)th observation.
\item
  \(p\) = number of predictors (including the intercept).
\item
  \(MSE\) = mean squared error of the model.
\end{itemize}

Key Insights

\begin{itemize}
\tightlist
\item
  \textbf{Cook's D quantifies the overall influence} of the \(i\)th observation on the entire set of fitted values.
\item
  If either the residual \(e_i\) increases or the leverage \(h_{ii}\) increases, then \(D_i\) also increases, indicating higher influence.
\item
  Observations with both high leverage and large residuals are flagged as influential.
\end{itemize}

Threshold for Influence

\begin{itemize}
\tightlist
\item
  \(D_i\) can be interpreted as a percentile of an \(F_{(p,n-p)}\) distribution.
\item
  Practical thresholds:

  \begin{itemize}
  \tightlist
  \item
    \textbf{If} \(D_i > 4/n\), the \(i\)th case has major influence.
  \item
    Alternatively, cases where \(D_i\) exceeds the 50th percentile of the \(F\)-distribution may also be considered influential.
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load necessary package}
\FunctionTok{library}\NormalTok{(car)}

\CommentTok{\# Fit a linear model}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt, }\AttributeTok{data =}\NormalTok{ mtcars)}

\CommentTok{\# Compute Cook\textquotesingle{}s D}
\NormalTok{cooks\_d\_values }\OtherTok{\textless{}{-}} \FunctionTok{cooks.distance}\NormalTok{(model)}

\CommentTok{\# Display influential observations based on the threshold}
\NormalTok{threshold }\OtherTok{\textless{}{-}} \DecValTok{4} \SpecialCharTok{/} \FunctionTok{nrow}\NormalTok{(mtcars)}
\NormalTok{influential\_obs }\OtherTok{\textless{}{-}} \FunctionTok{which}\NormalTok{(cooks\_d\_values }\SpecialCharTok{\textgreater{}}\NormalTok{ threshold)}

\CommentTok{\# Results}
\FunctionTok{list}\NormalTok{(}
  \AttributeTok{Cooks\_D                  =}\NormalTok{ cooks\_d\_values,}
  \AttributeTok{Threshold                =}\NormalTok{ threshold,}
  \AttributeTok{Influential\_Observations =}\NormalTok{ influential\_obs}
\NormalTok{)}
\CommentTok{\#\textgreater{} $Cooks\_D}
\CommentTok{\#\textgreater{}           Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive }
\CommentTok{\#\textgreater{}        1.589652e{-}02        5.464779e{-}03        2.070651e{-}02        4.724822e{-}05 }
\CommentTok{\#\textgreater{}   Hornet Sportabout             Valiant          Duster 360           Merc 240D }
\CommentTok{\#\textgreater{}        2.736184e{-}04        2.155064e{-}02        1.255218e{-}02        1.677650e{-}02 }
\CommentTok{\#\textgreater{}            Merc 230            Merc 280           Merc 280C          Merc 450SE }
\CommentTok{\#\textgreater{}        2.188702e{-}03        1.554996e{-}03        1.215737e{-}02        1.423008e{-}03 }
\CommentTok{\#\textgreater{}          Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental }
\CommentTok{\#\textgreater{}        1.458960e{-}04        6.266049e{-}03        2.786686e{-}05        1.780910e{-}02 }
\CommentTok{\#\textgreater{}   Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla }
\CommentTok{\#\textgreater{}        4.236109e{-}01        1.574263e{-}01        9.371446e{-}03        2.083933e{-}01 }
\CommentTok{\#\textgreater{}       Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28 }
\CommentTok{\#\textgreater{}        2.791982e{-}02        2.087419e{-}02        2.751510e{-}02        9.943527e{-}03 }
\CommentTok{\#\textgreater{}    Pontiac Firebird           Fiat X1{-}9       Porsche 914{-}2        Lotus Europa }
\CommentTok{\#\textgreater{}        1.443199e{-}02        5.920440e{-}04        5.674986e{-}06        7.353985e{-}02 }
\CommentTok{\#\textgreater{}      Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E }
\CommentTok{\#\textgreater{}        8.919701e{-}03        5.732672e{-}03        2.720397e{-}01        5.600804e{-}03 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Threshold}
\CommentTok{\#\textgreater{} [1] 0.125}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Influential\_Observations}
\CommentTok{\#\textgreater{} Chrysler Imperial          Fiat 128    Toyota Corolla     Maserati Bora }
\CommentTok{\#\textgreater{}                17                18                20                31}
\end{Highlighting}
\end{Shaded}

\hypertarget{dfbetas}{%
\subparagraph{DFBETAS}\label{dfbetas}}

\textbf{DFBETAS} measures the \textbf{influence of the} \(i\)th observation on each regression coefficient in a regression model. It is defined as:

\[
(DFBETAS)_{k(i)} = \frac{b_k - b_{k(i)}}{\sqrt{MSE_{(i)}c_{kk}}}
\]

Where:

\begin{itemize}
\tightlist
\item
  \(b_k\) = regression coefficient for the \(k\)th predictor using all observations.
\item
  \(b_{k(i)}\) = regression coefficient for the \(k\)th predictor with the \(i\)th observation omitted.
\item
  \(MSE_{(i)}\) = mean squared error with the \(i\)th observation excluded.
\item
  \(c_{kk}\) = \(k\)th diagonal element of \(\mathbf{X'X}^{-1}\), representing the variance of \(b_k\).
\end{itemize}

Key Insights

\begin{itemize}
\tightlist
\item
  \textbf{DFBETAS quantifies the change in each regression coefficient (}\(b_k\)) caused by omitting the \(i\)th observation.
\item
  The sign of DFBETAS indicates whether the inclusion of an observation increases or decreases the regression coefficient.

  \begin{itemize}
  \tightlist
  \item
    Positive DFBETAS: Inclusion increases \(b_k\).
  \item
    Negative DFBETAS: Inclusion decreases \(b_k\).
  \end{itemize}
\item
  High DFBETAS indicate that a single observation disproportionately affects one or more predictors.
\end{itemize}

The thresholds for identifying influential observations based on DFBETAS are:

\begin{itemize}
\tightlist
\item
  \textbf{Small data sets}: \(|DFBETAS| > 1\)
\item
  \textbf{Large data sets}: \(|DFBETAS| > 2 / \sqrt{n}\)
\end{itemize}

Where \(n\) is the total number of observations.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load necessary package}
\FunctionTok{library}\NormalTok{(car)}

\CommentTok{\# Fit a linear model}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt, }\AttributeTok{data =}\NormalTok{ mtcars)}

\CommentTok{\# Compute DFBETAS}
\NormalTok{dfbetas\_values }\OtherTok{\textless{}{-}} \FunctionTok{dfbetas}\NormalTok{(model)}

\CommentTok{\# Display influential observations based on the threshold for each predictor}
\NormalTok{threshold }\OtherTok{\textless{}{-}} \DecValTok{2} \SpecialCharTok{/} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(mtcars))}
\NormalTok{influential\_obs }\OtherTok{\textless{}{-}}
    \FunctionTok{apply}\NormalTok{(dfbetas\_values, }\DecValTok{2}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x)}
        \FunctionTok{which}\NormalTok{(}\FunctionTok{abs}\NormalTok{(x) }\SpecialCharTok{\textgreater{}}\NormalTok{ threshold))}

\CommentTok{\# Results}
\FunctionTok{list}\NormalTok{(}
    \AttributeTok{DFBETAS                  =}\NormalTok{ dfbetas\_values,}
    \AttributeTok{Threshold                =}\NormalTok{ threshold,}
    \AttributeTok{Influential\_Observations =}\NormalTok{ influential\_obs}
\NormalTok{)}
\CommentTok{\#\textgreater{} $DFBETAS}
\CommentTok{\#\textgreater{}                      (Intercept)           hp            wt}
\CommentTok{\#\textgreater{} Mazda RX4           {-}0.161347204  0.032966471  0.0639304305}
\CommentTok{\#\textgreater{} Mazda RX4 Wag       {-}0.069324050  0.045785122 {-}0.0004066495}
\CommentTok{\#\textgreater{} Datsun 710          {-}0.211199646  0.043374926  0.0972314374}
\CommentTok{\#\textgreater{} Hornet 4 Drive       0.002672687 {-}0.006839301  0.0044886906}
\CommentTok{\#\textgreater{} Hornet Sportabout    0.001784844  0.009208434 {-}0.0015536931}
\CommentTok{\#\textgreater{} Valiant             {-}0.005985946  0.180374447 {-}0.1516565139}
\CommentTok{\#\textgreater{} Duster 360           0.004705177 {-}0.159988770  0.0781031774}
\CommentTok{\#\textgreater{} Merc 240D            0.034255292 {-}0.189552940  0.1224118752}
\CommentTok{\#\textgreater{} Merc 230             0.019788247 {-}0.055075623  0.0332570461}
\CommentTok{\#\textgreater{} Merc 280            {-}0.003198686  0.036709039 {-}0.0337297820}
\CommentTok{\#\textgreater{} Merc 280C           {-}0.009045583  0.103809696 {-}0.0953846390}
\CommentTok{\#\textgreater{} Merc 450SE          {-}0.026973686 {-}0.005712458  0.0356973740}
\CommentTok{\#\textgreater{} Merc 450SL          {-}0.003961562  0.003399822  0.0049302300}
\CommentTok{\#\textgreater{} Merc 450SLC          0.031572445 {-}0.016800308 {-}0.0400515832}
\CommentTok{\#\textgreater{} Cadillac Fleetwood  {-}0.006420656 {-}0.002577897  0.0075499557}
\CommentTok{\#\textgreater{} Lincoln Continental {-}0.168791258 {-}0.058242601  0.1903129995}
\CommentTok{\#\textgreater{} Chrysler Imperial   {-}0.924056752 {-}0.148009806  0.9355996760}
\CommentTok{\#\textgreater{} Fiat 128             0.605181396 {-}0.311246566 {-}0.1672758566}
\CommentTok{\#\textgreater{} Honda Civic          0.156388333 {-}0.034026915 {-}0.0819144214}
\CommentTok{\#\textgreater{} Toyota Corolla       0.804669969 {-}0.170934240 {-}0.4114605894}
\CommentTok{\#\textgreater{} Toyota Corona       {-}0.231328587  0.066064464  0.0882138248}
\CommentTok{\#\textgreater{} Dodge Challenger     0.003923967  0.049775308 {-}0.0888481611}
\CommentTok{\#\textgreater{} AMC Javelin         {-}0.019610048  0.037837437 {-}0.0734203131}
\CommentTok{\#\textgreater{} Camaro Z28           0.029920076 {-}0.128670440  0.0390740055}
\CommentTok{\#\textgreater{} Pontiac Firebird    {-}0.058806962 {-}0.002278294  0.0868742949}
\CommentTok{\#\textgreater{} Fiat X1{-}9           {-}0.037559007  0.010208853  0.0174261386}
\CommentTok{\#\textgreater{} Porsche 914{-}2       {-}0.003655931  0.000316321  0.0020588013}
\CommentTok{\#\textgreater{} Lotus Europa         0.423409344  0.188396749 {-}0.4072338373}
\CommentTok{\#\textgreater{} Ford Pantera L      {-}0.022536462 {-}0.148176049  0.0999346699}
\CommentTok{\#\textgreater{} Ferrari Dino        {-}0.065508308 {-}0.085182962  0.0869804902}
\CommentTok{\#\textgreater{} Maserati Bora       {-}0.007482815  0.865763737 {-}0.4999048760}
\CommentTok{\#\textgreater{} Volvo 142E          {-}0.080001907  0.038406565  0.0127537553}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Threshold}
\CommentTok{\#\textgreater{} [1] 0.3535534}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Influential\_Observations}
\CommentTok{\#\textgreater{} $Influential\_Observations$\textasciigrave{}(Intercept)\textasciigrave{}}
\CommentTok{\#\textgreater{} Chrysler Imperial          Fiat 128    Toyota Corolla      Lotus Europa }
\CommentTok{\#\textgreater{}                17                18                20                28 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Influential\_Observations$hp}
\CommentTok{\#\textgreater{} Maserati Bora }
\CommentTok{\#\textgreater{}            31 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Influential\_Observations$wt}
\CommentTok{\#\textgreater{} Chrysler Imperial    Toyota Corolla      Lotus Europa     Maserati Bora }
\CommentTok{\#\textgreater{}                17                20                28                31}
\end{Highlighting}
\end{Shaded}

\hypertarget{collinearity}{%
\paragraph{Collinearity}\label{collinearity}}

\textbf{Collinearity} (or multicollinearity) refers to the correlation among explanatory variables in a regression model. It can lead to various issues, including:

\begin{itemize}
\tightlist
\item
  Large changes in the estimated regression coefficients when a predictor variable is added or removed, or when observations are altered.
\item
  Non-significant results for individual tests on regression coefficients of important predictor variables.
\item
  Regression coefficients with signs opposite to theoretical expectations or prior experience.
\item
  Large coefficients of simple correlation between pairs of predictor variables in the correlation matrix.
\item
  Wide confidence intervals for the regression coefficients representing important predictor variables.
\end{itemize}

When some \(X\) variables are highly correlated, the inverse \((X'X)^{-1}\) either does not exist or is computationally unstable. This can result in:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Non-interpretability of parameters}: \[\mathbf{b = (X'X)^{-1}X'y}\]
\item
  \textbf{Infinite sampling variability}: \[\mathbf{s^2(b) = MSE (X'X)^{-1}}\]
\end{enumerate}

If some predictor variables (\(X\)) are ``perfectly'' correlated, the system becomes undetermined, leading to an infinite number of models that fit the data. Specifically:

\begin{itemize}
\tightlist
\item
  If \(X'X\) is singular, then \((X'X)^{-1}\) does not exist.
\item
  This results in poor parameter estimation and invalid statistical inference.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{variance-inflation-factors-vifs}{%
\subparagraph{Variance Inflation Factors (VIFs)}\label{variance-inflation-factors-vifs}}

The \textbf{Variance Inflation Factor (VIF)} quantifies how much the variance of a regression coefficient is inflated due to multicollinearity. It is defined as:

\[
VIF_k = \frac{1}{1-R^2_k}
\]

Where:

\begin{itemize}
\tightlist
\item
  \(R^2_k\) is the coefficient of multiple determination when \(X_k\) is regressed on the other \(p-2\) predictor variables in the model.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Interpretation of VIFs}

\begin{itemize}
\tightlist
\item
  Large \(VIF_k\) values indicate that near collinearity is inflating the variance of \(b_k\). The relationship is given by: \[ var(b_k) \propto \sigma^2 (VIF_k) \]
\item
  \textbf{Thresholds}:

  \begin{itemize}
  \tightlist
  \item
    \(VIF > 4\): Investigate the cause of multicollinearity.
  \item
    \(VIF_k > 10\): Serious multicollinearity problem that can lead to poor parameter estimates.
  \end{itemize}
\item
  The \textbf{mean VIF} provides an estimate of the degree of multicollinearity:

  \begin{itemize}
  \tightlist
  \item
    If \(avg(VIF) >> 1\), serious multicollinearity is present.
  \end{itemize}
\item
  \textbf{Multicollinearity and VIF:}

  \begin{itemize}
  \item
    High VIFs with indicator variables are normal and not problematic.
  \item
    VIF is generally not useful for detecting multicollinearity concerns in models with fixed effects.
  \end{itemize}
\item
  \textbf{Overemphasis on Multicollinearity:}

  \begin{itemize}
  \item
    Multicollinearity inflates standard errors and widens confidence intervals but does not bias results.
  \item
    If key variables have narrow confidence intervals, multicollinearity is not an issue.
  \end{itemize}
\item
  \textbf{Goldberger's Insight} \citep{goldberger1991course}\textbf{:}

  \begin{itemize}
  \item
    Multicollinearity is akin to small sample size (``micronumerosity'').
  \item
    Large standard errors are expected with highly correlated independent variables.
  \end{itemize}
\item
  \textbf{Practical Implications:}

  \begin{itemize}
  \item
    Evaluate whether confidence intervals for key variables are sufficiently narrow.
  \item
    If not, the study is inconclusive, and a larger dataset or redesigned study is needed.
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load necessary packages}
\FunctionTok{library}\NormalTok{(car)}

\CommentTok{\# Fit a regression model}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ disp, }\AttributeTok{data =}\NormalTok{ mtcars)}

\CommentTok{\# Compute Variance Inflation Factors}
\NormalTok{vif\_values }\OtherTok{\textless{}{-}} \FunctionTok{vif}\NormalTok{(model)}

\CommentTok{\# Check for high multicollinearity}
\NormalTok{threshold }\OtherTok{\textless{}{-}} \DecValTok{10}
\NormalTok{high\_vif }\OtherTok{\textless{}{-}} \FunctionTok{which}\NormalTok{(vif\_values }\SpecialCharTok{\textgreater{}}\NormalTok{ threshold)}

\CommentTok{\# Results}
\FunctionTok{list}\NormalTok{(}
  \AttributeTok{VIFs =}\NormalTok{ vif\_values,}
  \AttributeTok{High\_VIF\_Threshold =}\NormalTok{ threshold,}
  \AttributeTok{High\_VIF\_Indices =}\NormalTok{ high\_vif}
\NormalTok{)}
\CommentTok{\#\textgreater{} $VIFs}
\CommentTok{\#\textgreater{}       hp       wt     disp }
\CommentTok{\#\textgreater{} 2.736633 4.844618 7.324517 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $High\_VIF\_Threshold}
\CommentTok{\#\textgreater{} [1] 10}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $High\_VIF\_Indices}
\CommentTok{\#\textgreater{} named integer(0)}
\end{Highlighting}
\end{Shaded}

\hypertarget{condition-number}{%
\subparagraph{Condition Number}\label{condition-number}}

\textbf{Condition Number} is a diagnostic measure for detecting multicollinearity, derived from the spectral decomposition of the matrix \(\mathbf{X'X}\).

The spectral decomposition of \(\mathbf{X'X}\) is:

\[
\mathbf{X'X}= \sum_{i=1}^{p} \lambda_i \mathbf{u_i u_i'}
\]

Where:

\begin{itemize}
\tightlist
\item
  \(\lambda_i\): Eigenvalue associated with the \(i\)th eigenvector.
\item
  \(\mathbf{u}_i\): Eigenvector associated with \(\lambda_i\).
\item
  \(\lambda_1 > \lambda_2 > \dots > \lambda_p\) (ordered eigenvalues).
\item
  The eigenvectors are orthogonal: \[ 
  \begin{cases}
  \mathbf{u_i'u_j} = 0 & \text{for } i \neq j \\
  \mathbf{u_i'u_j} = 1 & \text{for } i = j
  \end{cases}
  \]
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Definition of the Condition Number

The \textbf{Condition Number} is defined as:

\[
k = \sqrt{\frac{\lambda_{\text{max}}}{\lambda_{\text{min}}}}
\]

Where:

\begin{itemize}
\tightlist
\item
  \(\lambda_{\text{max}}\): Largest eigenvalue.
\item
  \(\lambda_{\text{min}}\): Smallest eigenvalue.
\end{itemize}

Interpretation

\begin{itemize}
\tightlist
\item
  \(k > 30\): Cause for concern.
\item
  \(30 < k < 100\): Moderate dependencies among predictors.
\item
  \(k > 100\): Strong collinearity, indicating serious multicollinearity.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The \textbf{Condition Index} for the \(i\)th eigenvalue is defined as:

\[
\delta_i = \sqrt{\frac{\lambda_{\text{max}}}{\lambda_i}}
\]

Where \(i = 1, \dots, p\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Variance proportions} can be used to identify collinearity issues. The proportion of total variance associated with the \(k\)th regression coefficient and the \(i\)th eigen mode is given by:

\[
\frac{u_{ik}^2/\lambda_i}{\sum_j \left(u^2_{jk}/\lambda_j\right)}
\]

\textbf{Key Indicators}:

\begin{itemize}
\tightlist
\item
  A \textbf{large condition index} \(\delta_i\) suggests potential collinearity.
\item
  Variance proportions \(> 0.5\) for at least two regression coefficients indicate serious collinearity problems.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load necessary package}
\FunctionTok{library}\NormalTok{(car)}

\CommentTok{\# Fit a regression model}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ disp, }\AttributeTok{data =}\NormalTok{ mtcars)}

\CommentTok{\# Compute eigenvalues and eigenvectors of the correlation matrix of predictors}
\NormalTok{cor\_matrix }\OtherTok{\textless{}{-}} \FunctionTok{cor}\NormalTok{(mtcars[, }\FunctionTok{c}\NormalTok{(}\StringTok{"hp"}\NormalTok{, }\StringTok{"wt"}\NormalTok{, }\StringTok{"disp"}\NormalTok{)])}
\NormalTok{eigen\_decomp }\OtherTok{\textless{}{-}} \FunctionTok{eigen}\NormalTok{(cor\_matrix)}

\CommentTok{\# Extract eigenvalues and compute the Condition Number}
\NormalTok{eigenvalues }\OtherTok{\textless{}{-}}\NormalTok{ eigen\_decomp}\SpecialCharTok{$}\NormalTok{values}
\NormalTok{condition\_number }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{max}\NormalTok{(eigenvalues) }\SpecialCharTok{/} \FunctionTok{min}\NormalTok{(eigenvalues))}

\CommentTok{\# Compute Condition Indices}
\NormalTok{condition\_indices }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{max}\NormalTok{(eigenvalues) }\SpecialCharTok{/}\NormalTok{ eigenvalues)}

\CommentTok{\# Results}
\FunctionTok{list}\NormalTok{(}
  \AttributeTok{Condition\_Number =}\NormalTok{ condition\_number,}
  \AttributeTok{Condition\_Indices =}\NormalTok{ condition\_indices}
\NormalTok{)}
\CommentTok{\#\textgreater{} $Condition\_Number}
\CommentTok{\#\textgreater{} [1] 5.469549}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Condition\_Indices}
\CommentTok{\#\textgreater{} [1] 1.000000 2.697266 5.469549}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Condition Number}: A single value indicating the degree of multicollinearity.
\item
  \textbf{Condition Indices}: A vector showing the relative dependency associated with each eigenvalue.
\end{enumerate}

\hypertarget{constancy-of-error-variance}{%
\paragraph{Constancy of Error Variance}\label{constancy-of-error-variance}}

Testing for the constancy of error variance (homoscedasticity) ensures that the assumptions of linear regression are met.

Below are two commonly used tests to assess error variance.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{brown-forsythe-test-modified-levene-test}{%
\subparagraph{Brown-Forsythe Test (Modified Levene Test)}\label{brown-forsythe-test-modified-levene-test}}

The \textbf{Brown-Forsythe Test} does not depend on the normality of errors and is suitable when error variance increases or decreases with \(X\).

Procedure

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Split the residuals into two groups: \[ e_{i1}, i = 1, \dots, n_1 \quad \text{and} \quad e_{j2}, j = 1, \dots, n_2 \]
\item
  Compute absolute deviations from the group medians: \[ d_{i1} = |e_{i1} - \tilde{e}_1| \quad \text{and} \quad d_{j2} = |e_{j2} - \tilde{e}_2| \] where \(\tilde{e}_1\) and \(\tilde{e}_2\) are the medians of groups 1 and 2, respectively.
\item
  Perform a two-sample t-test on \(d_{i1}\) and \(d_{j2}\): \[
  t_L = \frac{\bar{d}_1 - \bar{d}_2}{s\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}
  \] where \[
  s^2 = \frac{\sum_i (d_{i1} - \bar{d}_1)^2 + \sum_j (d_{j2} - \bar{d}_2)^2}{n - 2}
  \]
\item
  Reject the null hypothesis of constant error variance if: \[ |t_L| > t_{1-\alpha/2; n-2} \]
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load necessary package}
\FunctionTok{library}\NormalTok{(car)}

\CommentTok{\# Fit a linear model}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt, }\AttributeTok{data =}\NormalTok{ mtcars)}

\CommentTok{\# Perform the Brown{-}Forsythe Test}
\NormalTok{levene\_test }\OtherTok{\textless{}{-}} \FunctionTok{leveneTest}\NormalTok{(model}\SpecialCharTok{$}\NormalTok{residuals }\SpecialCharTok{\textasciitilde{}} \FunctionTok{cut}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{hp, }\DecValTok{2}\NormalTok{))  }\CommentTok{\# Split HP into 2 groups}

\CommentTok{\# Results}
\NormalTok{levene\_test}
\CommentTok{\#\textgreater{} Levene\textquotesingle{}s Test for Homogeneity of Variance (center = median)}
\CommentTok{\#\textgreater{}       Df F value Pr(\textgreater{}F)}
\CommentTok{\#\textgreater{} group  1  0.0851 0.7724}
\CommentTok{\#\textgreater{}       30}
\end{Highlighting}
\end{Shaded}

The p-value determines whether to reject the null hypothesis of constant variance.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{breusch-pagan-test-cook-weisberg-test}{%
\subparagraph{Breusch-Pagan Test (Cook-Weisberg Test)}\label{breusch-pagan-test-cook-weisberg-test}}

The \textbf{Breusch-Pagan Test} assumes independent and normally distributed errors. It tests the hypothesis:

\begin{itemize}
\tightlist
\item
  \(H_0: \gamma_1 = 0\) (Constant variance).
\item
  \(H_1: \gamma_1 \neq 0\) (Non-constant variance).
\end{itemize}

Procedure

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Assume the variance of the error terms is modeled as: \[ \sigma^2_i = \gamma_0 + \gamma_1 X_i \]
\item
  Regress the squared residuals (\(e_i^2\)) on \(X_i\):

  \begin{itemize}
  \tightlist
  \item
    Obtain the \textbf{regression sum of squares} (\(SSR^*\)).
  \end{itemize}
\item
  Compute the Breusch-Pagan statistic: \[
  X^2_{BP} = \frac{SSR^*/2}{\left(\frac{SSE}{n}\right)^2}
  \] where \(SSE\) is the error sum of squares from the regression of \(Y\) on \(X\).
\item
  Compare \(X^2_{BP}\) to the critical value from the \(\chi^2\) distribution with 1 degree of freedom:

  \begin{itemize}
  \tightlist
  \item
    Reject \(H_0\) (homogeneous variance) if: \[ X^2_{BP} > \chi^2_{1-\alpha;1} \]
  \end{itemize}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load necessary package}
\FunctionTok{library}\NormalTok{(lmtest)}

\CommentTok{\# Perform the Breusch{-}Pagan Test}
\NormalTok{bp\_test }\OtherTok{\textless{}{-}} \FunctionTok{bptest}\NormalTok{(model)}

\CommentTok{\# Results}
\NormalTok{bp\_test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  studentized Breusch{-}Pagan test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  model}
\CommentTok{\#\textgreater{} BP = 0.88072, df = 2, p{-}value = 0.6438}
\end{Highlighting}
\end{Shaded}

If the p-value is below the chosen significance level (e.g., 0.05), reject \$H\_0\$ and conclude non-constant variance.

\hypertarget{independence}{%
\paragraph{Independence}\label{independence}}

Testing for independence ensures that the residuals of a regression model are uncorrelated. Violation of this assumption may lead to biased or inefficient parameter estimates. Below, we discuss three primary methods for diagnosing dependence: plots, the Durbin-Watson test, and specific methods for time-series and spatial data.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{plots}{%
\subparagraph{Plots}\label{plots}}

A residual plot can help detect dependence in the residuals:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Plot residuals (\(e_i\)) versus the predicted values (\(\hat{Y}_i\)).
\item
  Patterns such as systematic waves, trends, or clusters indicate possible dependence.
\item
  Independence is suggested if residuals are randomly scattered without clear patterns.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fit a regression model}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt, }\AttributeTok{data =}\NormalTok{ mtcars)}

\CommentTok{\# Residual plot}
\FunctionTok{plot}\NormalTok{(model}\SpecialCharTok{$}\NormalTok{fitted.values, model}\SpecialCharTok{$}\NormalTok{residuals,}
     \AttributeTok{xlab =} \StringTok{"Fitted Values"}\NormalTok{,}
     \AttributeTok{ylab =} \StringTok{"Residuals"}\NormalTok{,}
     \AttributeTok{main =} \StringTok{"Residual Plot"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{h =} \DecValTok{0}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{05-linear-regression_files/figure-latex/unnamed-chunk-14-1} \end{center}

\hypertarget{durbin-watson-test}{%
\subparagraph{Durbin-Watson Test}\label{durbin-watson-test}}

The \textbf{Durbin-Watson test} specifically detects autocorrelation in residuals, especially in time-series data.

\begin{itemize}
\item
  \textbf{Hypotheses}:

  \begin{itemize}
  \item
    \(H_0\): Residuals are uncorrelated.
  \item
    \(H_1\): Residuals are autocorrelated.
  \end{itemize}
\item
  The Durbin-Watson statistic (\(d\)) is calculated as: \(d = \frac{\sum_{t=2}^n (e_t - e_{t-1})^2}{\sum_{t=1}^n e_t^2}\)

  \begin{itemize}
  \item
    \(d \approx 2\): No autocorrelation.
  \item
    \(d < 2\): Positive autocorrelation.
  \item
    \(d > 2\): Negative autocorrelation.
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load necessary package}
\FunctionTok{library}\NormalTok{(lmtest)}

\CommentTok{\# Perform the Durbin{-}Watson test}
\NormalTok{dw\_test }\OtherTok{\textless{}{-}} \FunctionTok{dwtest}\NormalTok{(model)}

\CommentTok{\# Results}
\NormalTok{dw\_test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Durbin{-}Watson test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  model}
\CommentTok{\#\textgreater{} DW = 1.3624, p{-}value = 0.02061}
\CommentTok{\#\textgreater{} alternative hypothesis: true autocorrelation is greater than 0}
\end{Highlighting}
\end{Shaded}

\hypertarget{time-series-autocorrelation}{%
\subparagraph{Time-Series Autocorrelation}\label{time-series-autocorrelation}}

For time-series data, autocorrelation often occurs due to the temporal structure. Key diagnostics include:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Autocorrelation Function (ACF)}:

  \begin{itemize}
  \item
    Shows the correlation of residuals with their lagged values.
  \item
    Significant spikes in the ACF plot indicate autocorrelation.
  \end{itemize}
\item
  \textbf{Partial Autocorrelation Function (PACF)}:

  \begin{itemize}
  \tightlist
  \item
    Identifies the correlation of residuals after removing the influence of intermediate lags.
  \end{itemize}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load necessary package}
\FunctionTok{library}\NormalTok{(forecast)}

\CommentTok{\# Residuals from a time{-}series model}
\NormalTok{time\_series\_res }\OtherTok{\textless{}{-}} \FunctionTok{ts}\NormalTok{(model}\SpecialCharTok{$}\NormalTok{residuals)}

\CommentTok{\# Plot ACF and PACF}
\FunctionTok{acf}\NormalTok{(time\_series\_res, }\AttributeTok{main =} \StringTok{"ACF of Residuals"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{05-linear-regression_files/figure-latex/unnamed-chunk-16-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pacf}\NormalTok{(time\_series\_res, }\AttributeTok{main =} \StringTok{"PACF of Residuals"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{05-linear-regression_files/figure-latex/unnamed-chunk-16-2} \end{center}

\hypertarget{spatial-statistics}{%
\subparagraph{Spatial Statistics}\label{spatial-statistics}}

Spatial dependence occurs when residuals are correlated across geographical or spatial locations. Two primary tests are used to diagnose spatial autocorrelation:

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Moran's I} measures global spatial autocorrelation. It determines whether similar values cluster spatially. The statistic is defined as:

\[
I = \frac{n}{W} \cdot \frac{\sum_i \sum_j w_{ij} (x_i - \bar{x})(x_j - \bar{x})}{\sum_i (x_i - \bar{x})^2}
\]

Where:

\begin{itemize}
\tightlist
\item
  \(n\): Number of observations.
\item
  \(W\): Sum of all spatial weights \(w_{ij}\).
\item
  \(w_{ij}\): Spatial weight between locations \(i\) and \(j\).
\item
  \(x_i\): Residual value at location \(i\).
\item
  \(\bar{x}\): Mean of the residuals.
\end{itemize}

Spatial Weight Matrix (\(W\)): \(W\) defines the spatial relationship between observations. It is often derived from:

\begin{itemize}
\tightlist
\item
  \textbf{Distance-based methods}: E.g., k-nearest neighbors or distance bands.
\item
  \textbf{Adjacency methods}: Based on shared boundaries in geographic data.
\end{itemize}

\textbf{Interpretation}:

\begin{itemize}
\item
  \(I > 0\): Positive spatial autocorrelation (similar values cluster together).
\item
  \(I < 0\): Negative spatial autocorrelation (dissimilar values are neighbors).
\item
  \(I \approx 0\): Random spatial distribution.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load necessary packages}
\FunctionTok{library}\NormalTok{(spdep)}

\CommentTok{\# Simulate spatial data (example with mtcars dataset)}
\NormalTok{coords }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{hp, mtcars}\SpecialCharTok{$}\NormalTok{wt)  }\CommentTok{\# Coordinates based on two variables}
\NormalTok{neighbors }\OtherTok{\textless{}{-}} \FunctionTok{knn2nb}\NormalTok{(}\FunctionTok{knearneigh}\NormalTok{(coords, }\AttributeTok{k =} \DecValTok{3}\NormalTok{))  }\CommentTok{\# Nearest neighbors}
\NormalTok{weights }\OtherTok{\textless{}{-}} \FunctionTok{nb2listw}\NormalTok{(neighbors, }\AttributeTok{style =} \StringTok{"W"}\NormalTok{)  }\CommentTok{\# Spatial weights}

\CommentTok{\# Compute Moran\textquotesingle{}s I for residuals}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt, }\AttributeTok{data =}\NormalTok{ mtcars)}
\NormalTok{moran\_test }\OtherTok{\textless{}{-}} \FunctionTok{moran.test}\NormalTok{(model}\SpecialCharTok{$}\NormalTok{residuals, weights)}

\CommentTok{\# Results}
\NormalTok{moran\_test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Moran I test under randomisation}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  model$residuals  }
\CommentTok{\#\textgreater{} weights: weights    }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Moran I statistic standard deviate = 1.5146, p{-}value = 0.06494}
\CommentTok{\#\textgreater{} alternative hypothesis: greater}
\CommentTok{\#\textgreater{} sample estimates:}
\CommentTok{\#\textgreater{} Moran I statistic       Expectation          Variance }
\CommentTok{\#\textgreater{}        0.15425069       {-}0.03225806        0.01516371}

\CommentTok{\# Moran\textquotesingle{}s scatterplot}
\FunctionTok{moran.plot}\NormalTok{(model}\SpecialCharTok{$}\NormalTok{residuals, weights, }\AttributeTok{main =} \StringTok{"Moran\textquotesingle{}s Scatterplot"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{05-linear-regression_files/figure-latex/unnamed-chunk-17-1} \end{center}

Significant Moran's I: Indicates global clustering of similar residuals, suggesting spatial dependence.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Geary's C} measures spatial autocorrelation at a local level, emphasizing differences between neighboring observations. The statistic is defined as:

\[
C = \frac{n - 1}{2W} \cdot \frac{\sum_i \sum_j w_{ij} (x_i - x_j)^2}{\sum_i (x_i - \bar{x})^2}
\]

Where:

\begin{itemize}
\tightlist
\item
  \(C\) ranges from 0 to 2:

  \begin{itemize}
  \tightlist
  \item
    \(C \approx 0\): High positive spatial autocorrelation.
  \item
    \(C \approx 1\): Spatial randomness.
  \item
    \(C \approx 2\): High negative spatial autocorrelation.
  \end{itemize}
\end{itemize}

\textbf{Comparison of Moran's I and Geary's C}:

\begin{itemize}
\item
  Moran's I is more global and measures the overall pattern of autocorrelation.
\item
  Geary's C is more sensitive to local spatial autocorrelation, detecting small-scale patterns.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Compute Geary\textquotesingle{}s C for residuals}
\NormalTok{geary\_test }\OtherTok{\textless{}{-}} \FunctionTok{geary.test}\NormalTok{(model}\SpecialCharTok{$}\NormalTok{residuals, weights)}

\CommentTok{\# Results}
\NormalTok{geary\_test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Geary C test under randomisation}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  model$residuals }
\CommentTok{\#\textgreater{} weights: weights }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Geary C statistic standard deviate = 1.0592, p{-}value = 0.1447}
\CommentTok{\#\textgreater{} alternative hypothesis: Expectation greater than statistic}
\CommentTok{\#\textgreater{} sample estimates:}
\CommentTok{\#\textgreater{} Geary C statistic       Expectation          Variance }
\CommentTok{\#\textgreater{}        0.84535708        1.00000000        0.02131399}
\end{Highlighting}
\end{Shaded}

Significant Geary's C: Highlights local spatial autocorrelation, useful for identifying specific regions or groups of observations where dependence is strong.

\hypertarget{generalized-least-squares}{%
\section{Generalized Least Squares}\label{generalized-least-squares}}

\hypertarget{infeasible-generalized-least-squares}{%
\subsection{Infeasible Generalized Least Squares}\label{infeasible-generalized-least-squares}}

Motivation for a More Efficient Estimator

\begin{itemize}
\tightlist
\item
  The \protect\hyperlink{gauss-markov-theorem}{Gauss-Markov Theorem} guarantees that OLS is the \textbf{Best Linear Unbiased Estimator (BLUE)} under assumptions A1-A4:

  \begin{itemize}
  \tightlist
  \item
    \textbf{A4:} \(Var(\epsilon | \mathbf{X}) = \sigma^2 \mathbf{I}_n\) (homoscedasticity and no autocorrelation).
  \end{itemize}
\item
  When \textbf{A4} does not hold:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Heteroskedasticity:} \(Var(\epsilon_i | \mathbf{X}) \neq \sigma^2\).
  \item
    \textbf{Serial Correlation:} \(Cov(\epsilon_i, \epsilon_j | \mathbf{X}) \neq 0\) for (\(i \neq j\)).
  \end{itemize}
\end{itemize}

Without \textbf{A4}, OLS is \textbf{unbiased} but no longer \textbf{efficient}. This motivates the need for an alternative approach to identify the most efficient estimator.

The unweighted (standard) regression model is given by:

\[
\mathbf{y} = \mathbf{X \beta} + \boldsymbol{\epsilon}
\]

Assuming \textbf{A1-A3} hold (linearity, full rank, exogeneity), but \textbf{A4} does not, the variance of the error term is no longer proportional to an identity

\[
Var(\boldsymbol{\epsilon} | \mathbf{X}) = \boldsymbol{\Omega} \neq \sigma^2 \mathbf{I}_n.
\]

To address the violation of \textbf{A4} (\(\boldsymbol{\Omega} \neq \sigma^2 \mathbf{I}_n\)), one can transform the model by premultiplying both sides by a full-rank matrix \(\mathbf{w}\) to have a weighted (transformed) regression model:

\[
\mathbf{w y} = \mathbf{w X \beta} + \mathbf{w \epsilon},
\]

where \(\mathbf{w}\) is a full-rank matrix chosen such that:

\[
\mathbf{w'w} = \boldsymbol{\Omega}^{-1}.
\]

\begin{itemize}
\tightlist
\item
  \(\mathbf{w}\) is the \protect\hyperlink{cholesky-decomposition}{Cholesky Decomposition} of \(\boldsymbol{\Omega}^{-1}\).
\item
  The Cholesky decomposition ensures \(\mathbf{w}\) satisfies \(\mathbf{w'w = \Omega^{-1}}\), where \(\mathbf{w}\) is the ``square root'' of \(\boldsymbol{\Omega}^{-1}\) in matrix terms.
\end{itemize}

By transforming the original model, the variance of the transformed errors becomes:

\[
\begin{aligned}
\boldsymbol{\Omega} &= Var(\boldsymbol{\epsilon} | \mathbf{X}), \\
\boldsymbol{\Omega}^{-1} &= Var(\boldsymbol{\epsilon} | \mathbf{X})^{-1}.
\end{aligned}
\]

The transformed equation allows us to compute a more efficient estimator.

Using the transformed model, the \textbf{Infeasible Generalized Least Squares (IGLS)} estimator is:

\[
\begin{aligned}
\mathbf{\hat{\beta}_{IGLS}} &= \mathbf{(X'w'wX)^{-1}X'w'wy} \\
&= \mathbf{(X' \boldsymbol{\Omega}^{-1} X)^{-1} X' \boldsymbol{\Omega}^{-1} y} \\
&= \mathbf{\beta + (X' \boldsymbol{\Omega}^{-1} X)^{-1} X' \boldsymbol{\Omega}^{-1} \boldsymbol{\epsilon}}.
\end{aligned}
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Unbiasedness}
\end{enumerate}

Since assumptions \textbf{A1-A3} hold for the unweighted model:

\[
\begin{aligned}
\mathbf{E(\hat{\beta}_{IGLS}|\mathbf{X})} 
&= \mathbf{E(\beta + (X'\Omega^{-1}X'\Omega^{-1}\epsilon)|\mathbf{X})} \\
&= \mathbf{\beta + E(X'\Omega^{-1}X'\Omega^{-1}\epsilon|\mathbf{X})} \\
&= \mathbf{\beta + X'\Omega^{-1}X'\Omega^{-1}E(\epsilon|\mathbf{X})}  && \text{since A3: } E(\epsilon|\mathbf{X})=0, \\
&= \mathbf{\beta}.
\end{aligned}
\]

Thus, the \textbf{IGLS estimator is unbiased}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Variance}
\end{enumerate}

The variance of the transformed errors is given by:

\[
\begin{aligned}
\mathbf{Var(w\epsilon|\mathbf{X})} 
&= \mathbf{wVar(\epsilon|\mathbf{X})w'} \\
&= \mathbf{w\Omega w'} \\
&= \mathbf{w(w'w)^{-1}w'} && \text{since } \mathbf{w} \text{ is full-rank,} \\
&= \mathbf{ww^{-1}(w')^{-1}w'} \\
&= \mathbf{I_n}.
\end{aligned}
\]

Hence, \textbf{A4} holds for the transformed (weighted) equation, satisfying the Gauss-Markov conditions.

The variance of the IGLS estimator is:

\[
\begin{aligned}
\mathbf{Var(\hat{\beta}_{IGLS}|\mathbf{X})} 
&= \mathbf{Var(\beta + (X'\Omega^{-1}X)^{-1}X'\Omega^{-1}\epsilon|\mathbf{X})} \\
&= \mathbf{Var((X'\Omega^{-1}X)^{-1}X'\Omega^{-1}\epsilon|\mathbf{X})} \\
&= \mathbf{(X'\Omega^{-1}X)^{-1}X'\Omega^{-1} Var(\epsilon|\mathbf{X}) \Omega^{-1}X(X'\Omega^{-1}X)^{-1}} && \text{because A4 holds}, \\
&= \mathbf{(X'\Omega^{-1}X)^{-1}X'\Omega^{-1} \Omega \Omega^{-1} \Omega^{-1}X(X'\Omega^{-1}X)^{-1}}, \\
&= \mathbf{(X'\Omega^{-1}X)^{-1}}.
\end{aligned}
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Efficiency}
\end{enumerate}

The difference in variances between OLS and IGLS is:

\[
\mathbf{Var(\hat{\beta}_{OLS}|\mathbf{X}) - Var(\hat{\beta}_{IGLS}|\mathbf{X})} = \mathbf{A\Omega A'},
\]

where:

\[
\mathbf{A = (X'X)^{-1}X' - (X'\Omega^{-1}X)^{-1}X'\Omega^{-1}}.
\]

Since \(\mathbf{\Omega}\) is positive semi-definite, \(\mathbf{A\Omega A'}\) is also positive semi-definite. Thus, the \textbf{IGLS estimator is more efficient than OLS} under heteroskedasticity or autocorrelation.

In short, properties of \(\mathbf{\hat{\beta}_{IGLS}}\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Unbiasedness}: \(\mathbf{\hat{\beta}_{IGLS}}\) remains unbiased as long as \textbf{A1-A3} hold.
\item
  \textbf{Efficiency}: \(\mathbf{\hat{\beta}_{IGLS}}\) is more efficient than OLS under heteroskedasticity or serial correlation since it accounts for the structure of \(\boldsymbol{\Omega}\).
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Why Is IGLS ``Infeasible''?}

The name \textbf{infeasible} arises because it is generally impossible to compute the estimator directly due to the structure of \(\mathbf{w}\) (or equivalently \(\boldsymbol{\Omega}^{-1}\)). The matrix \(\mathbf{w}\) is defined as:

\[
\mathbf{w} = 
\begin{pmatrix}
w_{11} & 0 & 0 & \cdots & 0 \\
w_{21} & w_{22} & 0 & \cdots & 0 \\
w_{31} & w_{32} & w_{33} & \cdots & \cdots \\
w_{n1} & w_{n2} & w_{n3} & \cdots & w_{nn} \\
\end{pmatrix},
\]

with \(n(n+1)/2\) unique elements for \(n\) observations. This results in more parameters than data points, making direct estimation infeasible.

To make the estimation feasible, assumptions about the structure of \(\mathbf{\Omega}\) are required. Common approaches include:

\begin{itemize}
\item
  \protect\hyperlink{heteroskedasticity-errors}{Heteroskedasticity Errors}: Assume a multiplicative exponential model for the variance, such as \(Var(\epsilon_i|\mathbf{X}) = \sigma_i^2\).

  \begin{itemize}
  \item
    Assume no correlation between errors, but allow heterogeneous variances: \[ \mathbf{\Omega} = \begin{pmatrix} \sigma_1^2 & 0          & \cdots & 0 \\ 0          & \sigma_2^2 & \cdots & 0 \\ \vdots     & \vdots     & \ddots & \vdots \\ 0          & 0          & \cdots & \sigma_n^2 \end{pmatrix}. \]
  \item
    Estimate \(\sigma_i^2\) using methods such as:

    \begin{itemize}
    \tightlist
    \item
      Modeling \(\sigma_i^2\) as a function of predictors (e.g., \(\sigma_i^2 = \exp(\mathbf{x}_i \gamma)\)).
    \end{itemize}
  \end{itemize}
\item
  \protect\hyperlink{serial-correlation}{Serial Correlation}: Assume serial correlation follows an autoregressive process \protect\hyperlink{ar1}{AR(1) Model}, e.g., \(\epsilon_t = \rho \epsilon_{t-1} + u_t\) and \(Cov(\epsilon_t, \epsilon_{t -h}) = \rho^h \sigma^2\), where we have a variance-covariance matrix with off-diagonal elements decaying geometrically: \[ \mathbf{\Omega} = \frac{\sigma^2}{1-\rho^2} \begin{pmatrix} 1      & \rho    & \rho^2 & \cdots & \rho^{n-1} \\ \rho   & 1       & \rho   & \cdots & \rho^{n-2} \\ \rho^2 & \rho    & 1      & \cdots & \rho^{n-3} \\ \vdots & \vdots  & \vdots & \ddots & \vdots \\ \rho^{n-1} & \rho^{n-2} & \rho^{n-3} & \cdots & 1 \end{pmatrix}. \]
\item
  \protect\hyperlink{cluster-errors}{Cluster Errors}: Assume block-diagonal structure for \(\mathbf{\Omega}\) to account for grouped or panel data.
\end{itemize}

Each assumption simplifies the estimation of \(\mathbf{\Omega}\) and thus \(\mathbf{w}\), enabling \href{FGLS}{Feasible Generalized Least Squares} with fewer unknown parameters to estimate.

\hypertarget{feasible-generalized-least-squares}{%
\subsection{Feasible Generalized Least Squares}\label{feasible-generalized-least-squares}}

\hypertarget{heteroskedasticity-errors}{%
\subsubsection{Heteroskedasticity Errors}\label{heteroskedasticity-errors}}

Heteroskedasticity occurs when the variance of the error term is not constant across observations. Specifically:

\[
Var(\epsilon_i | x_i) = E(\epsilon_i^2 | x_i) \neq \sigma^2,
\]

but instead depends on a function of \(x_i\):

\[
Var(\epsilon_i | x_i) = h(x_i) = \sigma_i^2
\]

This violates the assumption of homoscedasticity (constant variance), impacting the efficiency of OLS estimates.

For the model:

\[
y_i = x_i\beta + \epsilon_i,
\]

we apply a transformation to standardize the variance:

\[
\frac{y_i}{\sigma_i} = \frac{x_i}{\sigma_i} \beta + \frac{\epsilon_i}{\sigma_i}.
\]

By scaling each observation with \(1/\sigma_i\), the variance of the transformed error term becomes:

\[
\begin{aligned}
Var\left(\frac{\epsilon_i}{\sigma_i} \bigg| X \right) &= \frac{1}{\sigma_i^2} Var(\epsilon_i | X) \\
&= \frac{1}{\sigma_i^2} \sigma_i^2 \\
&= 1.
\end{aligned}
\]

Thus, the heteroskedasticity is corrected in the transformed model.

In matrix notation, the transformed model is:

\[
\mathbf{w y} = \mathbf{w X \beta + w \epsilon},
\]

where \(\mathbf{w}\) is the weight matrix used to standardize the variance. The weight matrix \(\mathbf{w}\) is defined as:

\[
\mathbf{w} =
\begin{pmatrix}
1/\sigma_1 & 0          & 0          & \cdots & 0 \\
0          & 1/\sigma_2 & 0          & \cdots & 0 \\
0          & 0          & 1/\sigma_3 & \cdots & 0 \\
\vdots     & \vdots     & \vdots     & \ddots & \vdots \\
0          & 0          & 0          & \cdots & 1/\sigma_n
\end{pmatrix}.
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

In the presence of heteroskedasticity, the variance of the error term, \(Var(\epsilon_i|\mathbf{x}_i)\), is not constant across observations. This leads to inefficient OLS estimates.

\textbf{Infeasible Weighted Least Squares (IWLS)} assumes that the variances \(\sigma_i^2 = Var(\epsilon_i|\mathbf{x}_i)\) are \textbf{known}. This allows us to adjust the regression equation to correct for heteroskedasticity.

The model is transformed as follows:

\[
y_i = \mathbf{x}_i\beta + \epsilon_i \quad \text{(original equation)},
\]

where \(\epsilon_i\) has variance \(\sigma_i^2\). To make the errors homoskedastic, we divide through by \(\sigma_i\):

\[
\frac{y_i}{\sigma_i} = \frac{\mathbf{x}_i}{\sigma_i}\beta + \frac{\epsilon_i}{\sigma_i}.
\]

Now, the transformed error term \(\epsilon_i / \sigma_i\) has a constant variance of 1:

\[
Var\left(\frac{\epsilon_i}{\sigma_i} | \mathbf{x}_i \right) = 1.
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The IWLS estimator minimizes the \textbf{weighted sum of squared residuals} for the transformed model:

\[
\text{Minimize: } \sum_{i=1}^n \left( \frac{y_i - \mathbf{x}_i\beta}{\sigma_i} \right)^2.
\]

In matrix form, the IWLS estimator is:

\[
\hat{\beta}_{IWLS} = (\mathbf{X}'\mathbf{W}\mathbf{X})^{-1}\mathbf{X}'\mathbf{W}\mathbf{y},
\]

where \(\mathbf{W}\) is a diagonal matrix of weights:

\[
\mathbf{W} = 
\begin{pmatrix}
1/\sigma_1^2 & 0 & \cdots & 0 \\
0 & 1/\sigma_2^2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 1/\sigma_n^2
\end{pmatrix}.
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Properties of IWLS

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Valid Standard Errors}:

  \begin{itemize}
  \tightlist
  \item
    If \(Var(\epsilon_i | \mathbf{X}) = \sigma_i^2\), the usual standard errors from IWLS are valid.
  \end{itemize}
\item
  \textbf{Robustness}:

  \begin{itemize}
  \tightlist
  \item
    If the variance assumption is incorrect (\(Var(\epsilon_i | \mathbf{X}) \neq \sigma_i^2\)), heteroskedasticity-robust standard errors must be used instead.
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The primary issue with IWLS is that \(\sigma_i^2 = Var(\epsilon_i|\mathbf{x}_i)\) is generally unknown. Specifically, we do not know:

\[
\sigma_i^2 = Var(\epsilon_i|\mathbf{x}_i) = E(\epsilon_i^2|\mathbf{x}_i).
\]

The challenges are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Single Observation}:

  \begin{itemize}
  \tightlist
  \item
    For each observation \(i\), there is only one \(\epsilon_i\), which is insufficient to estimate the variance \(\sigma_i^2\) directly.
  \end{itemize}
\item
  \textbf{Dependence on Assumptions}:

  \begin{itemize}
  \tightlist
  \item
    To estimate \(\sigma_i^2\), we must impose assumptions about its relationship to \(\mathbf{x}_i\).
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

To make IWLS feasible, we model \(\sigma_i^2\) as a function of the predictors \(\mathbf{x}_i\). A common approach is:

\[
\epsilon_i^2 = v_i \exp(\mathbf{x}_i\gamma),
\]

where:

\begin{itemize}
\item
  \(v_i\) is an independent error term with strictly positive values, representing random noise.
\item
  \(\exp(\mathbf{x}_i\gamma)\) is a deterministic function of the predictors \(\mathbf{x}_i\).
\end{itemize}

Taking the natural logarithm of both sides linearizes the model:

\[
\ln(\epsilon_i^2) = \mathbf{x}_i\gamma + \ln(v_i),
\]

where \(\ln(v_i)\) is independent of \(\mathbf{x}_i\). This transformation enables us to estimate \(\gamma\) using standard OLS techniques.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Estimation Procedure for Feasible GLS (FGLS)}

Since we do not observe the true errors \(\epsilon_i\), we approximate them using the OLS residuals \(e_i\). Here's the step-by-step process:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Compute OLS Residuals}: First, fit the original model using OLS and calculate the residuals:

  \[
  e_i = y_i - \mathbf{x}_i\hat{\beta}_{OLS}.
  \]
\item
  \textbf{Approximate} \(\epsilon_i^2\) with \(e_i^2\): Use the squared residuals as a proxy for the squared errors:

  \[
  e_i^2 \approx \epsilon_i^2.
  \]
\item
  \textbf{Log-Linear Model}: Fit the log-transformed model to estimate \(\gamma\):

  \[
  \ln(e_i^2) = \mathbf{x}_i\gamma + \ln(v_i).
  \]

  Estimate \(\gamma\) using OLS, where \(\ln(v_i)\) is treated as the error term.
\item
  \textbf{Estimate Variances}: Use the fitted values \(\hat{\gamma}\) to estimate \(\sigma_i^2\) for each observation:

  \[
  \hat{\sigma}_i^2 = \exp(\mathbf{x}_i\hat{\gamma}).
  \]
\item
  \textbf{Perform Weighted Least Squares}: Use the estimated variances \(\hat{\sigma}_i^2\) to construct the weight matrix \(\mathbf{\hat{W}}\):

  \[
  \mathbf{\hat{W}} = 
  \begin{pmatrix}
  1/\hat{\sigma}_1^2 & 0 & \cdots & 0 \\
  0 & 1/\hat{\sigma}_2^2 & \cdots & 0 \\
  \vdots & \vdots & \ddots & \vdots \\
  0 & 0 & \cdots & 1/\hat{\sigma}_n^2
  \end{pmatrix}.
  \]

  Then, compute the \textbf{Feasible GLS (FGLS)} estimator:

  \[
  \hat{\beta}_{FGLS} = (\mathbf{X}'\mathbf{\hat{W}}\mathbf{X})^{-1}\mathbf{X}'\mathbf{\hat{W}}\mathbf{y}.
  \]
\end{enumerate}

\hypertarget{serial-correlation}{%
\subsubsection{Serial Correlation}\label{serial-correlation}}

Serial correlation (also called autocorrelation) occurs when the error terms in a regression model are correlated across observations. Formally:

\[
Cov(\epsilon_i, \epsilon_j | \mathbf{X}) \neq 0 \quad \text{for } i \neq j.
\]

This violates the Gauss-Markov assumption that \(Cov(\epsilon_i, \epsilon_j | \mathbf{X}) = 0\), leading to inefficiencies in OLS estimates.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{covariance-stationarity}{%
\paragraph{Covariance Stationarity}\label{covariance-stationarity}}

If the errors are \textbf{covariance stationary}, the covariance between errors depends only on their relative time or positional difference (\(h\)), not their absolute position:

\[
Cov(\epsilon_i, \epsilon_j | \mathbf{X}) = Cov(\epsilon_i, \epsilon_{i+h} | \mathbf{x}_i, \mathbf{x}_{i+h}) = \gamma_h,
\]

where \(\gamma_h\) represents the covariance at lag \(h\).

Under covariance stationarity, the variance-covariance matrix of the error term \(\boldsymbol{\epsilon}\) takes the following form:

\[
Var(\boldsymbol{\epsilon}|\mathbf{X}) = \boldsymbol{\Omega} =
\begin{pmatrix}
\sigma^2 & \gamma_1 & \gamma_2 & \cdots & \gamma_{n-1} \\
\gamma_1 & \sigma^2 & \gamma_1 & \cdots & \gamma_{n-2} \\
\gamma_2 & \gamma_1 & \sigma^2 & \cdots & \vdots \\
\vdots & \vdots & \vdots & \ddots & \gamma_1 \\
\gamma_{n-1} & \gamma_{n-2} & \cdots & \gamma_1 & \sigma^2
\end{pmatrix}.
\]

Key Points:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The diagonal elements represent the variance of the error term: \(\sigma^2\).
\item
  The off-diagonal elements \(\gamma_h\) represent covariances at different lags \(h\).
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Why Serial Correlation Is a Problem?}

The matrix \(\boldsymbol{\Omega}\) introduces \(n\) parameters to estimate (e.g., \(\sigma^2, \gamma_1, \gamma_2, \ldots, \gamma_{n-1}\)). Estimating such a large number of parameters becomes impractical, especially for large datasets. To address this, we impose additional structure to reduce the number of parameters.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{ar1}{%
\paragraph{AR(1) Model}\label{ar1}}

In the \textbf{AR(1)} process, the errors follow a first-order autoregressive process:

\[
\begin{aligned}
y_t &= \beta_0 + x_t\beta_1 + \epsilon_t, \\
\epsilon_t &= \rho \epsilon_{t-1} + u_t,
\end{aligned}
\]

where:

\begin{itemize}
\item
  \(\rho\) is the first-order autocorrelation coefficient, capturing the relationship between consecutive errors.
\item
  \(u_t\) is white noise, satisfying \(Var(u_t) = \sigma_u^2\) and \(Cov(u_t, u_{t-h}) = 0\) for \(h \neq 0\).
\end{itemize}

Under the AR(1) assumption, the variance-covariance matrix of the error term \(\boldsymbol{\epsilon}\) becomes:

\[
Var(\boldsymbol{\epsilon} | \mathbf{X}) = \frac{\sigma_u^2}{1-\rho^2}
\begin{pmatrix}
1 & \rho & \rho^2 & \cdots & \rho^{n-1} \\
\rho & 1 & \rho & \cdots & \rho^{n-2} \\
\rho^2 & \rho & 1 & \cdots & \rho^{n-3} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\rho^{n-1} & \rho^{n-2} & \cdots & \rho & 1
\end{pmatrix}.
\]

Key Features:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The diagonal elements represent the variance: \(Var(\epsilon_t | \mathbf{X}) = \sigma_u^2 / (1-\rho^2)\).
\item
  The off-diagonal elements decay exponentially with lag \(h\): \(Cov(\epsilon_t, \epsilon_{t-h} | \mathbf{X}) = \rho^h \cdot Var(\epsilon_t | \mathbf{X})\).
\end{enumerate}

Under AR(1), only \textbf{one parameter} \(\rho\) needs to be estimated (in addition to \(\sigma_u^2\)), greatly simplifying the structure of \(\boldsymbol{\Omega}\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{OLS Properties Under AR(1)}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Consistency}: If assumptions A1, A2, A3a, and A5a hold, OLS remains consistent.
\item
  \textbf{Asymptotic Normality}: OLS estimates are asymptotically normal.
\item
  \textbf{Inference with Serial Correlation}:

  \begin{itemize}
  \tightlist
  \item
    Standard OLS errors are invalid.
  \item
    Use \textbf{Newey-West standard errors} to obtain robust inference.
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{infeasible-cochrane-orcutt}{%
\paragraph{Infeasible Cochrane-Orcutt}\label{infeasible-cochrane-orcutt}}

The \textbf{Infeasible} \textbf{Cochrane-Orcutt} procedure addresses serial correlation in the error terms by assuming an \textbf{AR(1)} process for the errors:

\[
\epsilon_t = \rho \epsilon_{t-1} + u_t,
\]

where \(u_t\) is white noise and \(\rho\) is the autocorrelation coefficient.

By transforming the original regression equation:

\[
y_t = \beta_0 + x_t\beta_1 + \epsilon_t,
\]

we subtract \(\rho\) times the lagged equation:

\[
\rho y_{t-1} = \rho (\beta_0 + x_{t-1}\beta_1 + \epsilon_{t-1}),
\]

to obtain the weighted first-difference equation:

\[
y_t - \rho y_{t-1} = (1-\rho)\beta_0 + (x_t - \rho x_{t-1})\beta_1 + u_t.
\]

Key Points:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Dependent Variable}: \(y_t - \rho y_{t-1}\).
\item
  \textbf{Independent Variable}: \(x_t - \rho x_{t-1}\).
\item
  \textbf{Error Term}: \(u_t\), which satisfies the Gauss-Markov assumptions (A3, A4, A5).
\end{enumerate}

The ICO estimator minimizes the sum of squared residuals for this transformed equation.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Standard Errors}:

  \begin{itemize}
  \tightlist
  \item
    If the errors truly follow an AR(1) process, the standard errors for the transformed equation are valid.
  \item
    For more complex error structures, \textbf{Newey-West HAC standard errors} are required.
  \end{itemize}
\item
  \textbf{Loss of Observations}:

  \begin{itemize}
  \tightlist
  \item
    The transformation involves first differences, which means the first observation (\(y_1\)) cannot be used. This reduces the effective sample size by one.
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{The Problem:} \(\rho\) \textbf{Is Unknown}

The ICO procedure is \textbf{infeasible} because it requires knowledge of \(\rho\), the autocorrelation coefficient. In practice, we estimate \(\rho\) from the data.

To estimate \(\rho\), we use the OLS residuals (\(e_t\)) as a proxy for the errors (\(\epsilon_t\)). The estimate \(\hat{\rho}\) is given by:

\[
\hat{\rho} = \frac{\sum_{t=2}^{T} e_t e_{t-1}}{\sum_{t=2}^{T} e_t^2}.
\]

Estimation via OLS:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Regress the OLS residuals \(e_t\) on their lagged values \(e_{t-1}\), without an intercept: \[
  e_t = \rho e_{t-1} + u_t.
  \]
\item
  The slope of this regression is the estimate \(\hat{\rho}\).
\end{enumerate}

This estimation is efficient under the AR(1) assumption and provides a practical approximation for \(\rho\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{feasiable-prais-winsten}{%
\paragraph{Feasible Prais-Winsten}\label{feasiable-prais-winsten}}

The \textbf{Feasible Prais-Winsten (FPW)} method addresses \textbf{AR(1) serial correlation} in regression models by transforming the data to eliminate serial dependence in the errors. Unlike the \protect\hyperlink{infeasible-cochrane-orcutt}{Infeasible Cochrane-Orcutt} procedure, which discards the first observation, the Prais-Winsten method retains it using a weighted transformation.

The FPW transformation uses the following weighting matrix \(\mathbf{w}\):

\[
\mathbf{w} =
\begin{pmatrix}
\sqrt{1 - \hat{\rho}^2} & 0 & 0 & \cdots & 0 \\
-\hat{\rho} & 1 & 0 & \cdots & 0 \\
0 & -\hat{\rho} & 1 & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & -\hat{\rho} & 1
\end{pmatrix}.
\]

where

\begin{itemize}
\tightlist
\item
  The first row accounts for the transformation of the first observation, using \(\sqrt{1 - \hat{\rho}^2}\).
\item
  Subsequent rows represent the AR(1) transformation for the remaining observations.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Step-by-Step Procedure}

Step 1: Initial OLS Estimation

Estimate the regression model using OLS:

\[
y_t = \mathbf{x}_t \beta + \epsilon_t,
\]

and compute the residuals:

\[
e_t = y_t - \mathbf{x}_t \hat{\beta}.
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Step 2: Estimate the AR(1) Correlation Coefficient

Estimate the AR(1) correlation coefficient \(\rho\) by regressing \(e_t\) on \(e_{t-1}\) without an intercept:

\[
e_t = \rho e_{t-1} + u_t.
\]

The slope of this regression is the estimated \(\hat{\rho}\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Step 3: Transform the Data

Apply the transformation using the weighting matrix \(\mathbf{w}\) to transform both the dependent variable \(\mathbf{y}\) and the independent variables \(\mathbf{X}\):

\[
\mathbf{wy} = \mathbf{wX} \beta + \mathbf{w\epsilon}.
\]

Specifically: 1. For \(t=1\), the transformed dependent and independent variables are: \[
   \tilde{y}_1 = \sqrt{1 - \hat{\rho}^2} \cdot y_1, \quad \tilde{\mathbf{x}}_1 = \sqrt{1 - \hat{\rho}^2} \cdot \mathbf{x}_1.
   \] 2. For \(t=2, \dots, T\), the transformed variables are: \[
   \tilde{y}_t = y_t - \hat{\rho} \cdot y_{t-1}, \quad \tilde{\mathbf{x}}_t = \mathbf{x}_t - \hat{\rho} \cdot \mathbf{x}_{t-1}.
   \]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Step 4: Feasible Prais-Winsten Estimation

Run OLS on the transformed equation:

\[
\mathbf{wy} = \mathbf{wX} \beta + \mathbf{w\epsilon}.
\]

The resulting estimator is the \textbf{Feasible Prais-Winsten (FPW) estimator}:

\[
\hat{\beta}_{FPW} = (\mathbf{X}'\mathbf{w}'\mathbf{w}\mathbf{X})^{-1} \mathbf{X}'\mathbf{w}'\mathbf{w}\mathbf{y}.
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Properties of Feasible \protect\hyperlink{feasiable-prais-winsten}{Prais-Winsten} Estimator

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Infeasible Prais-Winsten Estimator}:

  \begin{itemize}
  \tightlist
  \item
    The infeasible Prais-Winsten (PW) estimator assumes the AR(1) parameter \(\rho\) is known.
  \item
    Under assumptions \protect\hyperlink{a1-linearity}{A1}, \protect\hyperlink{a2-full-rank}{A2}, and \protect\hyperlink{a3-exogeneity-of-independent-variables}{A3} for the unweighted equation, the infeasible PW estimator is \textbf{unbiased} and efficient.
  \end{itemize}
\item
  \textbf{Feasible Prais-Winsten (FPW) Estimator}: The FPW estimator replaces the unknown \(\rho\) with an estimate \(\hat{\rho}\) derived from the OLS residuals, introducing bias in small samples.

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    \textbf{Bias}:

    \begin{itemize}
    \tightlist
    \item
      The FPW estimator is \textbf{biased} due to the estimation of \(\hat{\rho}\), which introduces an additional layer of approximation.
    \end{itemize}
  \item
    \textbf{Consistency}:

    \begin{enumerate}
    \def\labelenumiii{\arabic{enumiii}.}
    \tightlist
    \item
      The FPW estimator is \textbf{consistent} under the following assumptions:

      \begin{itemize}
      \tightlist
      \item
        \protect\hyperlink{a1-linearity}{A1}: The model is linear in parameters.
      \item
        \protect\hyperlink{a2-full-rank}{A2}: The independent variables are linearly independent.
      \item
        \protect\hyperlink{a5-data-generation-random-sampling}{A5}: The data is generated through random sampling.
      \item
        Additionally: \[
        E\big((\mathbf{x_t - \rho x_{t-1}})'\big(\epsilon_t - \rho \epsilon_{t-1}\big)\big) = 0.
        \] This condition ensures the transformed error term \(\epsilon_t - \rho \epsilon_{t-1}\) is uncorrelated with the transformed regressors \(\mathbf{x_t - \rho x_{t-1}}\).
      \end{itemize}
    \item
      \textbf{Note}: \protect\hyperlink{a3a-weak-exogeneity}{A3a} (zero conditional mean of the error term, \(E(\epsilon_t|\mathbf{x}_t) = 0\)) is \textbf{not sufficient} for the above condition. Full exogeneity of the independent variables (\protect\hyperlink{a3-exogeneity-of-independent-variables}{A3}) is required.
    \end{enumerate}
  \item
    \textbf{Efficiency}

    \begin{enumerate}
    \def\labelenumiii{\arabic{enumiii}.}
    \tightlist
    \item
      \textbf{Asymptotic Efficiency}: The FPW estimator is asymptotically \textbf{more efficient than OLS} if the errors are truly generated by an AR(1) process: \[
      \epsilon_t = \rho \epsilon_{t-1} + u_t, \quad Var(u_t) = \sigma^2.
      \]
    \item
      \textbf{Standard Errors}:

      \begin{enumerate}
      \def\labelenumiv{\arabic{enumiv}.}
      \tightlist
      \item
        \textbf{Usual Standard Errors}: If the errors are correctly specified as an AR(1) process, the usual standard errors from FPW are valid.
      \item
        \textbf{Robust Standard Errors}: If there is concern about a more complex dependence structure (e.g., higher-order autocorrelation or heteroskedasticity), use \textbf{Newey-West Standard Errors} for inference. These are robust to both serial correlation and heteroskedasticity.
      \end{enumerate}
    \end{enumerate}
  \end{enumerate}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{cluster-errors}{%
\subsubsection{Cluster Errors}\label{cluster-errors}}

Consider the regression model with clustered errors:

\[
y_{gi} = \mathbf{x}_{gi}\beta + \epsilon_{gi},
\]

where:

\begin{itemize}
\item
  \(g\) indexes the group (e.g., households, firms, schools).
\item
  \(i\) indexes the individual within the group.
\end{itemize}

The covariance structure for the errors \(\epsilon_{gi}\) is defined as:

\[
Cov(\epsilon_{gi}, \epsilon_{hj}) 
\begin{cases}
= 0 & \text{if } g \neq h \text{ (independent across groups)}, \\
\neq 0 & \text{for any pair } (i,j) \text{ within group } g.
\end{cases}
\]

Within each group, individuals' errors may be correlated (i.e., intra-group correlation), while errors are independent across groups. This violates \protect\hyperlink{a4-homoskedasticity}{A4} (constant variance and no correlation of errors).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Suppose there are three groups with varying sizes. The variance-covariance matrix \(\boldsymbol{\Omega}\) for the errors \(\boldsymbol{\epsilon}\) is:

\[
Var(\boldsymbol{\epsilon}| \mathbf{X}) = \boldsymbol{\Omega} =
\begin{pmatrix}
\sigma^2 & \delta_{12}^1 & \delta_{13}^1 & 0 & 0 & 0 \\
\delta_{12}^1 & \sigma^2 & \delta_{23}^1 & 0 & 0 & 0 \\
\delta_{13}^1 & \delta_{23}^1 & \sigma^2 & 0 & 0 & 0 \\
0 & 0 & 0 & \sigma^2 & \delta_{12}^2 & 0 \\
0 & 0 & 0 & \delta_{12}^2 & \sigma^2 & 0 \\
0 & 0 & 0 & 0 & 0 & \sigma^2
\end{pmatrix}.
\]

where

\begin{itemize}
\tightlist
\item
  \(\delta_{ij}^g = Cov(\epsilon_{gi}, \epsilon_{gj})\) is the covariance between errors for individuals \(i\) and \(j\) in group \(g\).
\item
  \(Cov(\epsilon_{gi}, \epsilon_{hj}) = 0\) for \(g \neq h\) (independent groups).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Infeasible Generalized Least Squares (Cluster)}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Assume Known Variance-Covariance Matrix}: If \(\sigma^2\) and \(\delta_{ij}^g\) are known, construct \(\boldsymbol{\Omega}\) and compute its inverse \(\boldsymbol{\Omega}^{-1}\).
\item
  \textbf{Infeasible GLS Estimator}: The infeasible generalized least squares (IGLS) estimator is:

  \[
  \hat{\beta}_{IGLS} = (\mathbf{X}'\boldsymbol{\Omega}^{-1}\mathbf{X})^{-1}\mathbf{X}'\boldsymbol{\Omega}^{-1}\mathbf{y}.
  \]
\end{enumerate}

Problem:

\begin{itemize}
\tightlist
\item
  We do not know \(\sigma^2\) and \(\delta_{ij}^g\), making this approach \textbf{infeasible}.
\item
  Even if \(\boldsymbol{\Omega}\) is estimated, incorrect assumptions about its structure may lead to invalid inference.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

To make the estimation feasible, we assume a \textbf{group-level random effects} specification for the error:

\[
\begin{aligned}
y_{gi} &= \mathbf{x}_{gi}\beta + c_g + u_{gi}, \\
Var(c_g|\mathbf{x}_i) &= \sigma_c^2, \\
Var(u_{gi}|\mathbf{x}_i) &= \sigma_u^2,
\end{aligned}
\]

where:

\begin{itemize}
\item
  \(c_g\) represents the \textbf{group-level random effect} (common shocks within each group, independent across groups).
\item
  \(u_{gi}\) represents the \textbf{individual-level error} (idiosyncratic shocks within each group, independent across individuals and groups).
\item
  \(\epsilon_{gi} = c_g + u_{gi}\)
\end{itemize}

Independence Assumptions:

\begin{itemize}
\tightlist
\item
  \(c_g\) and \(u_{gi}\) are independent of each other.
\item
  Both are mean-independent of \(\mathbf{x}_i\).
\end{itemize}

Under this specification, the variance-covariance matrix \(\boldsymbol{\Omega}\) becomes block diagonal, where each block corresponds to a group:

\[
Var(\boldsymbol{\epsilon}| \mathbf{X}) = \boldsymbol{\Omega} =
\begin{pmatrix}
\sigma_c^2 + \sigma_u^2 & \sigma_c^2 & \sigma_c^2 & 0 & 0 & 0 \\
\sigma_c^2 & \sigma_c^2 + \sigma_u^2 & \sigma_c^2 & 0 & 0 & 0 \\
\sigma_c^2 & \sigma_c^2  & \sigma_c^2 + \sigma_u^2 & 0 & 0 & 0 \\
0 & 0 & 0 & \sigma_c^2 + \sigma_u^2 & \sigma_c^2 & 0 \\
0 & 0 & 0 & \sigma_c^2 & \sigma_c^2 + \sigma_u^2 & 0 \\
0 & 0 & 0 & 0 & 0 & \sigma_c^2 + \sigma_u^2
\end{pmatrix}.
\]

When the variance components \(\sigma_c^2\) and \(\sigma_u^2\) are unknown, we can use the \textbf{Feasible Group-Level Random Effects (RE)} estimator to simultaneously estimate these variances and the regression coefficients \(\beta\). This practical approach allows us to account for intra-group correlation in the errors and still obtain consistent and efficient estimates of the parameters.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Step-by-Step Procedure}

Step 1: Initial OLS Estimation

Estimate the regression model using OLS:

\[ y_{gi} = \mathbf{x}_{gi}\beta + \epsilon_{gi}, \]

and compute the residuals:

\[ e_{gi} = y_{gi} - \mathbf{x}_{gi}\hat{\beta}. \]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Step 2: Estimate Variance Components

Use the standard OLS variance estimator \(s^2\) to estimate the \textbf{total variance}:

\[ s^2 = \frac{1}{n - k} \sum_{i=1}^{n} e_i^2, \]

where \(n\) is the total number of observations and \(k\) is the number of regressors (including the intercept).

Estimate the \textbf{between-group variance} \(\hat{\sigma}_c^2\) using:

\[ \hat{\sigma}_c^2 = \frac{1}{G} \sum_{g=1}^{G} \left( \frac{1}{\sum_{i=1}^{n_g - 1} i} \sum_{i \neq j} \sum_{j=1}^{n_g} e_{gi} e_{gj} \right), \]

where:

\begin{itemize}
\item
  \(G\) is the total number of groups,
\item
  \(n_g\) is the size of group \(g\),
\item
  The term \(\sum_{i \neq j} e_{gi} e_{gj}\) accounts for within-group covariance.
\end{itemize}

Estimate the \textbf{within-group variance} as:

\[ \hat{\sigma}_u^2 = s^2 - \hat{\sigma}_c^2. \]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Step 3: Construct the Variance-Covariance Matrix

Use the estimated variances \(\hat{\sigma}_c^2\) and \(\hat{\sigma}_u^2\) to construct the variance-covariance matrix \(\hat{\Omega}\) for the error term:

\[ \hat{\Omega}_{gi,gj} = \begin{cases} \hat{\sigma}_c^2 + \hat{\sigma}_u^2 & \text{if } i = j \text{ (diagonal elements)}, \\ \hat{\sigma}_c^2 & \text{if } i \neq j \text{ (off-diagonal elements within group)}, \\ 0 & \text{if } g \neq h \text{ (across groups)}. \end{cases} \]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Step 4: Feasible GLS Estimation

With \(\hat{\Omega}\) in hand, perform Feasible Generalized Least Squares (FGLS) to estimate \(\beta\):

\[ \hat{\beta}_{RE} = (\mathbf{X}'\hat{\Omega}^{-1}\mathbf{X})^{-1} \mathbf{X}'\hat{\Omega}^{-1}\mathbf{y}. \]

If the assumptions about \(\boldsymbol{\Omega}\) are incorrect or infeasible, use \textbf{cluster-robust standard errors} to account for intra-group correlation without explicitly modeling the variance-covariance structure. These standard errors remain valid under arbitrary within-cluster dependence, provided clusters are independent.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Properties of the Feasible Group-Level Random Effects Estimator}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Infeasible Group RE Estimator}
\end{enumerate}

\begin{itemize}
\tightlist
\item
  The infeasible RE estimator (assuming known variances) is \textbf{unbiased} under assumptions \protect\hyperlink{a1-linearity}{A1}, \protect\hyperlink{a2-full-rank}{A2}, and \protect\hyperlink{a3-exogeneity-of-independent-variables}{A3} for the unweighted equation.
\item
  \protect\hyperlink{a3-exogeneity-of-independent-variables}{A3} requires: \[ E(\epsilon_{gi}|\mathbf{x}_i) = E(c_g|\mathbf{x}_i) + E(u_{gi}|\mathbf{x}_i) = 0. \] This assumes:

  \begin{itemize}
  \tightlist
  \item
    \(E(c_g|\mathbf{x}_i) = 0\): The \textbf{random effects assumption} (group-level effects are uncorrelated with the regressors).
  \item
    \(E(u_{gi}|\mathbf{x}_i) = 0\): No endogeneity at the individual level.
  \end{itemize}
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Feasible Group RE Estimator}
\end{enumerate}

\begin{itemize}
\tightlist
\item
  The feasible RE estimator is \textbf{biased} because the variances \(\sigma_c^2\) and \(\sigma_u^2\) are estimated, introducing approximation errors.
\item
  However, the estimator is \textbf{consistent} under \protect\hyperlink{a1-linearity}{A1}, \protect\hyperlink{a2-full-rank}{A2}, \protect\hyperlink{a3a-weak-exogeneity}{A3a} (\(E(\mathbf{x}_i'\epsilon_{gi}) = E(\mathbf{x}_i'c_g) + E(\mathbf{x}_i'u_{gi}) = 0\)), \protect\hyperlink{a5a-random-sampling}{A5a}.
\item
  \textbf{Efficiency}

  \begin{itemize}
  \tightlist
  \item
    \textbf{Asymptotic Efficiency}:

    \begin{itemize}
    \tightlist
    \item
      The feasible RE estimator is asymptotically more efficient than OLS if the errors follow the random effects specification.
    \end{itemize}
  \item
    \textbf{Standard Errors}:

    \begin{itemize}
    \tightlist
    \item
      If the random effects specification is correct, the usual standard errors are consistent.
    \item
      If there is concern about more complex dependence structures or heteroskedasticity, use \textbf{cluster robust standard errors}.
    \end{itemize}
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{weighted-least-squares}{%
\subsection{Weighted Least Squares}\label{weighted-least-squares}}

In the presence of heteroskedasticity, the errors \(\epsilon_i\) have non-constant variance \(Var(\epsilon_i|\mathbf{x}_i) = \sigma_i^2\). This violates the Gauss-Markov assumption of homoskedasticity, leading to inefficient OLS estimates.

\textbf{Weighted Least Squares (WLS)} addresses this by applying weights inversely proportional to the variance of the errors, ensuring that observations with larger variances have less influence on the estimation.

\begin{itemize}
\item
  Weighted Least Squares is essentially \protect\hyperlink{generalized-least-squares}{Generalized Least Squares} in the special case that \(\mathbf{\Omega}\) is a \emph{diagonal} matrix with variances \(\sigma_i^2\) on the diagonal (i.e., errors are uncorrelated but have non-constant variance).

  \begin{itemize}
  \item
    That is, assume the errors are uncorrelated but \textbf{heteroskedastic}: \(\mathbf{\Omega} = \text{diag}\bigl(\sigma_1^2, \ldots, \sigma_n^2\bigr)\)
  \item
    Then \(\mathbf{\Omega}^{-1} = \text{diag}\bigl(1/\sigma_1^2, \ldots, 1/\sigma_n^2\bigr)\)
  \end{itemize}
\end{itemize}

\textbf{Steps for Feasible Weighted Least Squares (FWLS)}

1. Initial OLS Estimation

First, estimate the model using OLS:

\[ y_i = \mathbf{x}_i\beta + \epsilon_i, \]

and compute the residuals:

\[ e_i = y_i - \mathbf{x}_i \hat{\beta}. \]

2. Model the Error Variance

Transform the residuals to model the variance as a function of the predictors:

\[ \ln(e_i^2) = \mathbf{x}_i\gamma + \ln(v_i), \]

where:

\begin{itemize}
\item
  \(e_i^2\) approximates \(\epsilon_i^2\),
\item
  \(\ln(v_i)\) is the error term in this auxiliary regression, assumed independent of \(\mathbf{x}_i\).
\end{itemize}

Estimate this equation using OLS to obtain the predicted values:

\[ \hat{g}_i = \mathbf{x}_i \hat{\gamma}. \]

3. Estimate Weights

Use the predicted values from the auxiliary regression to compute the weights:

\[ \hat{\sigma}_i = \sqrt{\exp(\hat{g}_i)}. \]

These weights approximate the standard deviation of the errors.

4. Weighted Regression

Transform the original equation by dividing through by \(\hat{\sigma}_i\):

\[ \frac{y_i}{\hat{\sigma}_i} = \frac{\mathbf{x}_i}{\hat{\sigma}_i}\beta + \frac{\epsilon_i}{\hat{\sigma}_i}. \]

Estimate the transformed equation using OLS. The resulting estimator is the \textbf{Feasible Weighted Least Squares (FWLS)} estimator:

\[ \hat{\beta}_{FWLS} = (\mathbf{X}'\mathbf{\hat{W}}\mathbf{X})^{-1}\mathbf{X}'\mathbf{\hat{W}}\mathbf{y}, \]

where \(\mathbf{\hat{W}}\) is a diagonal weight matrix with elements \(1/\hat{\sigma}_i^2\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Properties of the FWLS Estimator}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Unbiasedness}:

  \begin{itemize}
  \tightlist
  \item
    The infeasible WLS estimator (where \(\sigma_i\) is known) is unbiased under assumptions A1-A3 for the unweighted model.
  \item
    The \textbf{FWLS estimator} is \textbf{not unbiased} due to the approximation of \(\sigma_i\) using \(\hat{\sigma}_i\).
  \end{itemize}
\item
  \textbf{Consistency}:

  \begin{itemize}
  \tightlist
  \item
    The FWLS estimator is \textbf{consistent} under the following assumptions:

    \begin{itemize}
    \tightlist
    \item
      \protect\hyperlink{a1-linearity}{A1} (for the unweighted equation): The model is linear in parameters.
    \item
      \protect\hyperlink{a2-full-rank}{A2} (for the unweighted equation): The independent variables are linearly independent.
    \item
      \protect\hyperlink{a5-data-generation-random-sampling}{A5}: The data is randomly sampled.
    \item
      \(E(\mathbf{x}_i'\epsilon_i/\sigma_i^2) = 0\). \protect\hyperlink{a3a-weak-exogeneity}{A3a: Weaker Exogeneity Assumption} is not sufficient, but \protect\hyperlink{a3-exogeneity-of-independent-variables}{A3} is.
    \end{itemize}
  \end{itemize}
\item
  \textbf{Efficiency}:

  \begin{itemize}
  \tightlist
  \item
    The FWLS estimator is asymptotically more efficient than OLS if the errors have multiplicative exponential heteroskedasticity: \[ Var(\epsilon_i|\mathbf{x}_i) = \sigma_i^2 = \exp(\mathbf{x}_i\gamma). \]
  \end{itemize}
\end{enumerate}

The FWLS estimator is asymptotically more efficient than OLS if the errors have multiplicative exponential heteroskedasticity.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Usual Standard Errors}:

  \begin{itemize}
  \tightlist
  \item
    If the errors are truly multiplicative exponential heteroskedastic, the usual standard errors for FWLS are valid.
  \end{itemize}
\item
  \textbf{Heteroskedastic Robust Standard Errors}:

  \begin{itemize}
  \tightlist
  \item
    If there is potential mis-specification of the multiplicative exponential model for \(\sigma_i^2\), heteroskedastic-robust standard errors should be reported to ensure valid inference.
  \end{itemize}
\end{enumerate}

\hypertarget{maximum-likelihood-estimator}{%
\section{Maximum Likelihood}\label{maximum-likelihood-estimator}}

The \textbf{Maximum Likelihood Estimation (MLE)} is a statistical method used to estimate the parameters of a model by maximizing the likelihood of observing the given data. The premise is to find the parameter values that maximize the probability (or likelihood) of the observed data.

The likelihood function, denoted as \(L(\theta)\), is expressed as:

\[
L(\theta) = \prod_{i=1}^{n} f(y_i|\theta)
\]

where:

\begin{itemize}
\tightlist
\item
  \(f(y|\theta)\) is the probability density or mass function of observing a single value of \(Y\) given the parameter \(\theta\).
\item
  The product runs over all \(n\) observations.
\end{itemize}

For different types of data, \(f(y|\theta)\) can take different forms. For example, if \(y\) is dichotomous (e.g., success/failure), then the likelihood function becomes:

\[
L(\theta) = \prod_{i=1}^{n} \theta^{y_i} (1-\theta)^{1-y_i}
\]

Here, \(\hat{\theta}\) is the Maximum Likelihood Estimator (MLE) if:

\[
L(\hat{\theta}) > L(\theta_0), \quad \forall \theta_0 \text{ in the parameter space.}
\]

See \protect\hyperlink{distributions}{Distributions} for a review on variable distributions.

\hypertarget{motivation-for-mle}{%
\subsection{Motivation for MLE}\label{motivation-for-mle}}

Suppose we know the \textbf{conditional distribution} of \(Y\) given \(X\), denoted as:

\[
f_{Y|X}(y, x; \theta)
\]

where \(\theta\) is an unknown parameter of the distribution. Sometimes, we are only concerned with the unconditional distribution \(f_Y(y; \theta)\).

For a sample of independent and identically distributed (i.i.d.) data, the joint probability of the sample is:

\[
f_{Y_1, \ldots, Y_n|X_1, \ldots, X_n}(y_1, \ldots, y_n, x_1, \ldots, x_n; \theta) = \prod_{i=1}^{n} f_{Y|X}(y_i, x_i; \theta)
\]

The \textbf{joint distribution}, evaluated at the observed data, defines the likelihood function. The goal of MLE is to find the parameter \(\theta\) that maximizes this likelihood.

To estimate \(\theta\), we maximize the likelihood function:

\[
\max_{\theta} \prod_{i=1}^{n} f_{Y|X}(y_i, x_i; \theta)
\]

In practice, it is easier to work with the natural logarithm of the likelihood (log-likelihood), as it transforms the product into a sum:

\[
\max_{\theta} \sum_{i=1}^{n} \ln(f_{Y|X}(y_i, x_i; \theta))
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Solving for the Maximum Likelihood Estimator

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{First-Order Condition}: Solve the first derivative of the log-likelihood function with respect to \(\theta\):

  \[
  \frac{\partial}{\partial \theta} \ell(\theta) \;=\; \frac{\partial}{\partial \theta} \ln L(\theta) \;=\; \frac{\partial}{\partial \theta} \sum_{i=1}^{n} \ln(f_{Y|X}(y_i, x_i; \hat{\theta}_{MLE})) = 0
  \]

  This yields the critical points where the likelihood is maximized. This derivative, sometimes written as \(U(\theta)\), is called the \textbf{score}. Intuitively, the log-likelihood's ``peak'' indicates the parameter value(s) that make the observed data ``most likely.''
\item
  \textbf{Second-Order Condition}: Verify that the second derivative of the log-likelihood function is negative at the critical point:

  \[
  \frac{\partial^2}{\partial \theta^2} \sum_{i=1}^{n} \ln(f_{Y|X}(y_i, x_i; \hat{\theta}_{MLE})) < 0
  \]

  This ensures that the solution corresponds to a maximum.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Examples of Likelihood Functions

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Unconditional \protect\hyperlink{poisson-distribution}{Poisson Distribution}
\end{enumerate}

The Poisson distribution models count data, such as the number of website visits in a day or product orders per hour. Its likelihood function is:

\[
L(\theta) = \prod_{i=1}^{n} \frac{\theta^{y_i} e^{-\theta}}{y_i!}
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \protect\hyperlink{exponential-distribution}{Exponential Distribution}
\end{enumerate}

The exponential distribution is often used to model the time between events, such as the time until a machine fails. Its probability density function (PDF) is:

\[
f_{Y|X}(y, x; \theta) = \frac{\exp(-y / (x \theta))}{x \theta}
\]

The joint likelihood for \(n\) observations is:

\[
L(\theta) = \prod_{i=1}^{n} \frac{\exp(-y_i / (x_i \theta))}{x_i \theta}
\]

By taking the logarithm, we obtain the log-likelihood for ease of maximization.

\hypertarget{key-quantities-for-inference}{%
\subsection{Key Quantities for Inference}\label{key-quantities-for-inference}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Score Function}\\
  The \textbf{score} is given by\\
  \[
  U(\theta) \;=\; \frac{d}{d\theta} \ell(\theta).
  \]\\
  Setting \(U(\hat{\theta}_{\mathrm{MLE}}) = 0\) yields the critical points of the log-likelihood, from which we can find \(\hat{\theta}_{\mathrm{MLE}}\).
\item
  \textbf{Observed Information}\\
  The second derivative of the log-likelihood, taken at the MLE, is called the \textbf{observed information}:

  \[
  I_O(\theta) \;=\; - \frac{d^2}{d\theta^2} \ell(\theta).
  \]

  (The negative sign is often included so that \(I_O(\theta)\) is \emph{positive} if \(\ell(\theta)\) is concave near its maximum. In some texts, you will see it defined without the negative sign, but the idea is the same: it measures the ``pointedness'' or curvature of \(\ell(\theta)\) at its maximum.)
\item
  \textbf{Fisher Information}\\
  The \textbf{Fisher Information} (or \textbf{expected information}) is the expectation of the observed information over the distribution of the data:

  \[
  I(\theta) \;=\; \mathbb{E}\bigl[I_O(\theta)\bigr].
  \]

  It quantifies how much information the data carry about the parameter \(\theta\). A larger Fisher information suggests that you can estimate \(\theta\) more precisely.
\item
  \textbf{Approximate Variance of} \(\hat{\theta}_{\mathrm{MLE}}\)\\
  One of the key results from standard asymptotic theory is that, for large \(n\), the variance of \(\hat{\theta}_{\mathrm{MLE}}\) can be approximated by the inverse of the Fisher information:

  \[
  \mathrm{Var}\bigl(\hat{\theta}_{\mathrm{MLE}}\bigr) \;\approx\; I(\theta)^{-1}.
  \]

  This also lays the groundwork for constructing confidence intervals for \(\theta\) in large samples.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{assumptions-of-mle}{%
\subsection{Assumptions of MLE}\label{assumptions-of-mle}}

MLE has desirable properties---\emph{consistency}, \emph{asymptotic normality}, and \emph{efficiency}---but these do not come ``for free.'' Instead, they rely on certain assumptions. Below is a breakdown of the main \textbf{regularity conditions}. These conditions are typically mild in many practical settings (for example, in exponential families, such as the normal distribution), but need to be checked in more complex models.

\textbf{High-Level Regulatory Assumptions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Independence and Identical Distribution (iid)}\\
  The sample \(\{(x_i, y_i)\}\) is usually assumed to be composed of independent and identically distributed observations. This independence assumption simplifies the likelihood to a product of individual densities: \[
  L(\theta) = \prod_{i=1}^n f_{Y\mid X}(y_i, x_i; \theta).
  \] In practice, if you have dependent data (e.g., time series, spatial data), modifications are required in the likelihood function.
\item
  \textbf{Same Density Function}\\
  All observations must come from the \emph{same} conditional probability density function \(f_{Y\mid X}(\cdot,\cdot;\theta)\). If the model changes across observations, you cannot simply multiply all of them together in one unified likelihood.
\item
  \textbf{Multivariate Normality (for certain models)}\\
  In many practical cases---especially for continuous outcomes---you might assume (multivariate) normal distributions with finite second or fourth moments \citep{little1988test}. Under these assumptions, the MLE for the mean vector and covariance matrix is consistent and (under further conditions) asymptotically normal. This assumption is quite common in regression, ANOVA, and other classical statistical frameworks.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{large-sample-properties-of-mle}{%
\subsubsection{Large Sample Properties of MLE}\label{large-sample-properties-of-mle}}

\hypertarget{consistency-of-mle}{%
\paragraph{Consistency of MLE}\label{consistency-of-mle}}

\textbf{Definition:} An estimator \(\hat{\theta}_n\) is \emph{consistent} if it converges in probability to the true parameter value \(\theta_0\) as the sample size \(n \to \infty\):

\[
\hat{\theta}_n \;\to^p\; \theta_0.
\]

For the MLE, a set of regularity conditions \(R1\)--\(R4\) is commonly used to ensure consistency:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{R1}\\
  If \(\theta \neq \theta_0\), then\\
  \[
  f_{Y\mid X}(y_i, x_i; \theta) \;\neq\; f_{Y\mid X}(y_i, x_i; \theta_0).
  \]

  In simpler terms, the model is identifiable: no two distinct parameter values generate the \emph{exact} same distribution for the data.
\item
  \textbf{R2}\\
  The parameter space \(\Theta\) is compact (closed and bounded), and it contains the true parameter \(\theta_0\). This ensures that \(\theta\) lies in a ``nice'' region (no parameter going to infinity, etc.), making it easier to prove that a maximum in that space indeed exists.
\item
  \textbf{R3}\\
  The log-likelihood function \(\ln(f_{Y\mid X}(y_i, x_i; \theta))\) is continuous in \(\theta\) with probability \(1\). Continuity is important so that we can apply theorems (like the Continuous Mapping Theorem or the Extreme Value Theorem) to find maxima.
\item
  \textbf{R4}\\
  The expected supremum of the absolute value of the log-likelihood is finite:

  \[
  \mathbb{E}\!\Bigl(\sup_{\theta \in \Theta} \bigl|\ln(f_{Y\mid X}(y_i, x_i; \theta))\bigr|\Bigr) \;<\;\infty.
  \]

  This is a technical condition that helps ensure we can ``exchange'' expectations and suprema, a step needed in many consistency proofs.
\end{enumerate}

When these conditions are satisfied, you can show via standard arguments (e.g., the \protect\hyperlink{law-of-large-numbers}{Law of Large Numbers}, uniform convergence of the log-likelihood) that:

\[
\hat{\theta}_{\mathrm{MLE}} \;\to^p\; \theta_0 \quad (\text{consistency}).
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{asymptotic-normality-of-mle}{%
\paragraph{Asymptotic Normality of MLE}\label{asymptotic-normality-of-mle}}

\textbf{Definition:} An estimator \(\hat{\theta}_n\) is \emph{asymptotically normal} if

\[
\sqrt{n}\,(\hat{\theta}_n - \theta_0) \;\to^d\; \mathcal{N}\bigl(0,\Sigma\bigr),
\]

where \(\to^d\) denotes convergence in distribution and \(\Sigma\) is some covariance matrix. For the MLE, \(\Sigma\) is typically \(I(\theta_0)^{-1}\), where \(I(\theta_0)\) is the Fisher information evaluated at the true parameter.

Beyond \(R1\)--\(R4\), we need the following additional assumptions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{R5}\\
  The true parameter \(\theta_0\) is in the \emph{interior} of the parameter space \(\Theta\). If \(\theta_0\) sits on the boundary, different arguments are required to handle edge effects.
\item
  \textbf{R6}\\
  The pdf \(f_{Y\mid X}(y_i, x_i; \theta)\) is \emph{twice} continuously differentiable (in \(\theta\)) and strictly positive in a neighborhood \(N\) of \(\theta_0\). This allows us to use second-order Taylor expansions around \(\theta_0\) to get the approximate distribution of \(\hat{\theta}_{\mathrm{MLE}}\).
\item
  \textbf{R7}\\
  The following integrals are finite in some neighborhood \(N\) of \(\theta_0\):

  \begin{itemize}
  \tightlist
  \item
    \(\displaystyle \int \sup_{\theta \in N} \Bigl\|\partial f_{Y\mid X}(y_i, x_i; \theta)/\partial \theta \Bigr\|\; d(y,x) < \infty\).
  \item
    \(\displaystyle \int \sup_{\theta \in N} \Bigl\|\partial^2 f_{Y\mid X}(y_i, x_i; \theta)/\partial \theta \partial \theta' \Bigr\|\; d(y,x) < \infty\).
  \item
    \(\displaystyle \mathbb{E}\Bigl(\sup_{\theta \in N} \Bigl\|\partial^2 \ln(f_{Y\mid X}(y_i, x_i; \theta))/\partial \theta \partial \theta' \Bigr\|\Bigr) < \infty\).
  \end{itemize}

  These conditions ensure that differentiating inside integrals is justified (via the dominated convergence theorem) and that we can expand the log-likelihood in a Taylor series safely.
\item
  \textbf{R8}\\
  The information matrix \(I(\theta_0)\) exists and is nonsingular:

  \[
  I(\theta_0) \;=\; \mathrm{Var}\Bigl(\frac{\partial}{\partial \theta} \ln\bigl(f_{Y\mid X}(y_i, x_i; \theta_0)\bigr)\Bigr) \;\neq\; 0.
  \]

  Nonsingularity implies there is enough information in the data to estimate \(\theta\) uniquely.
\end{enumerate}

Under \(R1\)--\(R8\), you can show that

\[
\sqrt{n}\,(\hat{\theta}_{\mathrm{MLE}} - \theta_0) \;\to^d\; \mathcal{N}\Bigl(0,\,I(\theta_0)^{-1}\Bigr).
\]

This result is central to frequentist inference, allowing you to construct approximate confidence intervals and hypothesis tests using the normal approximation for large \(n\).

\hypertarget{properties-of-mle}{%
\subsection{Properties of MLE}\label{properties-of-mle}}

Having established in earlier sections that Maximum Likelihood Estimators (MLEs) are \textbf{consistent} (\protect\hyperlink{consistency-of-mle}{Consistency of MLE}) and \textbf{asymptotically normal} (\protect\hyperlink{asymptotic-normality-of-mle}{Asymptotic Normality of MLE}) under standard regularity conditions, we now highlight additional properties that make MLE a powerful estimation technique.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Asymptotic Efficiency}
\end{enumerate}

\begin{itemize}
\tightlist
\item
  \textbf{Definition}: An estimator is \emph{asymptotically efficient} if it attains the smallest possible asymptotic variance among all consistent estimators (i.e., it achieves the \emph{Cramr-Rao Lower Bound}).
\item
  \textbf{Interpretation}: In large samples, MLE typically has smaller standard errors than other consistent estimators that do not fully use the assumed distributional form.
\item
  \textbf{Implication}: When the true model is correctly specified, MLE is the \emph{most efficient} among a broad class of estimators, leading to more precise inference for \(\theta\).

  \begin{itemize}
  \tightlist
  \item
    \textbf{Cramr-Rao Lower Bound (CRLB)}: A theoretical lower limit on the variance of any unbiased (or asymptotically unbiased) estimator \citep[\citet{rao1992information}]{cramer1999mathematical}.
  \item
    \textbf{When MLE Meets CRLB}: Under correct specification and standard regularity conditions, the asymptotic variance of the MLE matches the CRLB, making it \emph{asymptotically efficient}.
  \item
    \textbf{Interpretation}: Achieving CRLB means no other unbiased estimator can consistently outperform MLE in terms of variance for large \(n\).
  \end{itemize}
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Invariance}
\end{enumerate}

\begin{itemize}
\tightlist
\item
  \textbf{Core Idea}: If \(\hat{\theta}\) is the MLE for \(\theta\), then for any \emph{smooth} transformation \(g(\theta)\), the MLE for \(g(\theta)\) is simply \(g(\hat{\theta})\).
\item
  \textbf{Example}: If \(\theta\) is a mean parameter and you want the MLE for the \emph{variance} \(\theta^2\), you can just square the MLE for \(\theta\).
\item
  \textbf{Key Point}: This \emph{invariance property} saves considerable effort---there is no need to re-derive a new likelihood for the transformed parameter.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Explicit vs.~Implicit MLE
\end{enumerate}

\begin{itemize}
\tightlist
\item
  \textbf{Explicit MLE}:\\
  Occurs when the score equation can be solved in \emph{closed form}. A classic example is the MLE for the mean and variance in a normal distribution.
\item
  \textbf{Implicit MLE}:\\
  Happens when no closed-form solution exists. Iterative numerical methods, such as \textbf{Newton-Raphson}, \textbf{Expectation-Maximization (EM)}, or other optimization algorithms, are used to find \(\hat{\theta}\).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Distributional Mis-Specification}

\begin{itemize}
\tightlist
\item
  \textbf{Definition}: If you assume a distribution for \(f_{Y|X}(\cdot;\theta)\) that does \emph{not} reflect the true data-generating process, the MLE may become \emph{inconsistent} or biased in finite samples.
\item
  \textbf{Quasi-MLE}:

  \begin{itemize}
  \tightlist
  \item
    A strategy to handle certain forms of mis-specification.
  \item
    If the chosen distribution belongs to a flexible class or meets certain conditions (e.g., generalized linear models with a robust link), the resulting parameter estimates can remain consistent \emph{for some parameters of interest}.
  \end{itemize}
\item
  \textbf{Nonparametric \& Semiparametric Approaches}:

  \begin{itemize}
  \tightlist
  \item
    Require minimal or no distributional assumptions.
  \item
    More robust to mis-specification but can be \emph{harder to implement} and may exhibit higher variance or require larger sample sizes to achieve comparable precision.
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{practical-considerations}{%
\subsection{Practical Considerations}\label{practical-considerations}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Use Cases}

  \begin{itemize}
  \tightlist
  \item
    MLE is extremely popular for:

    \begin{itemize}
    \tightlist
    \item
      \textbf{Binary Outcomes} (logistic regression)
    \item
      \textbf{Count Data} (Poisson regression)
    \item
      \textbf{Strictly Positive Outcomes} (Gamma regression)
    \item
      \textbf{Heteroskedastic Settings} (models with variance related to mean, e.g., GLMs)
    \end{itemize}
  \end{itemize}
\item
  \textbf{Distributional Assumptions}

  \begin{itemize}
  \tightlist
  \item
    The efficiency gains of MLE stem from using a specific probability model.
  \item
    If the assumed model closely reflects the data-generating process, MLE gives accurate parameter estimates and reliable standard errors.
  \item
    MLE assumes knowledge of the conditional distribution of the outcome variable. This assumption parallels the normality assumption in linear regression models (e.g., \protect\hyperlink{a6-normal-distribution}{A6 Normal Distribution}).
  \item
    If severely mis-specified, consider robust or semi-/nonparametric methods.
  \end{itemize}
\item
  \textbf{Comparison with OLS}: See \protect\hyperlink{comparison-of-mle-and-ols}{Comparison of MLE and OLS} for more details.

  \begin{itemize}
  \tightlist
  \item
    \protect\hyperlink{ordinary-least-squares}{Ordinary Least Squares} is a special case of MLE when errors are normally distributed and homoscedastic.
  \item
    In more general settings (e.g., non-Gaussian or heteroskedastic data), MLE can outperform OLS in terms of smaller standard errors and better inference.
  \end{itemize}
\item
  \textbf{Numerical Stability \& Computation}

  \begin{itemize}
  \tightlist
  \item
    For complex likelihoods, iterative methods can fail to converge or converge to local maxima.
  \item
    Proper initialization and diagnostics (e.g., checking multiple start points) are crucial.
  \end{itemize}
\end{enumerate}

\hypertarget{comparison-of-mle-and-ols}{%
\subsection{Comparison of MLE and OLS}\label{comparison-of-mle-and-ols}}

While \protect\hyperlink{maximum-likelihood-estimator}{Maximum Likelihood} Estimation is a powerful estimation method, it does not solve all of the challenges associated with \protect\hyperlink{ordinary-least-squares}{Ordinary Least Squares}. Below is a detailed comparison highlighting similarities, differences, and limitations.

\textbf{Key Points of Comparison}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Inference Methods}:

  \begin{itemize}
  \tightlist
  \item
    \textbf{MLE}:

    \begin{itemize}
    \tightlist
    \item
      Joint inference is typically conducted using \textbf{log-likelihood calculations}, such as likelihood ratio tests or information criteria (e.g., AIC, BIC).
    \item
      These methods replace the use of F-statistics commonly associated with OLS.
    \end{itemize}
  \item
    \textbf{OLS}:

    \begin{itemize}
    \tightlist
    \item
      Relies on the \textbf{F-statistic} for hypothesis testing and joint inference.
    \end{itemize}
  \end{itemize}
\item
  \textbf{Sensitivity to Functional Form}:

  \begin{itemize}
  \tightlist
  \item
    Both MLE and OLS are sensitive to the \textbf{functional form} of the model. Incorrect specification (e.g., linear vs.~nonlinear relationships) can lead to biased or inefficient estimates in both cases.
  \end{itemize}
\item
  \textbf{Perfect Collinearity and Multicollinearity}:

  \begin{itemize}
  \tightlist
  \item
    Both methods are affected by collinearity:

    \begin{itemize}
    \tightlist
    \item
      \textbf{Perfect collinearity} (e.g., two identical predictors) makes parameter estimation impossible.
    \item
      \textbf{Multicollinearity} (highly correlated predictors) inflates standard errors, reducing the precision of estimates.
    \end{itemize}
  \item
    Neither MLE nor OLS directly resolves these issues without additional measures, such as regularization or variable selection.
  \end{itemize}
\item
  \textbf{Endogeneity}:

  \begin{itemize}
  \tightlist
  \item
    Problems like \textbf{omitted variable bias} or \textbf{simultaneous equations} affect both MLE and OLS:

    \begin{itemize}
    \tightlist
    \item
      If relevant predictors are omitted, estimates from both methods are likely to be biased and inconsistent.
    \item
      Similarly, in systems of simultaneous equations, both methods yield biased results unless endogeneity is addressed through instrumental variables or other approaches.
    \end{itemize}
  \item
    \textbf{MLE}, while efficient under correct model specification, does not inherently address endogeneity.
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Situations Where MLE and OLS Differ}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1879}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4364}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3697}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{MLE}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{OLS}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Estimator Efficiency} & Efficient for correctly specified distributions. & Efficient under Gauss-Markov assumptions. \\
\textbf{Assumptions about Errors} & Requires specifying a distribution (e.g., normal, binomial). & Requires only mean-zero errors and homoscedasticity. \\
\textbf{Use of Likelihood} & Based on maximizing the likelihood function for parameter estimation. & Based on minimizing the sum of squared residuals. \\
\textbf{Model Flexibility} & More flexible (supports various distributions, non-linear models). & Primarily linear models (extensions for non-linear exist). \\
\textbf{Interpretation} & Log-likelihood values guide model comparison (AIC/BIC). & R-squared and adjusted R-squared measure fit. \\
\end{longtable}

\textbf{Practical Considerations}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{When to Use MLE}:

  \begin{itemize}
  \tightlist
  \item
    Situations where the dependent variable is:

    \begin{itemize}
    \tightlist
    \item
      Binary (e.g., logistic regression)
    \item
      Count data (e.g., Poisson regression)
    \item
      Skewed or bounded (e.g., survival models)
    \end{itemize}
  \item
    When the model naturally arises from a probabilistic framework.
  \end{itemize}
\item
  \textbf{When to Use OLS}:

  \begin{itemize}
  \tightlist
  \item
    Suitable for continuous dependent variables with approximately linear relationships between predictors and outcomes.
  \item
    Simpler to implement and interpret when the assumptions of linear regression are reasonably met.
  \end{itemize}
\end{enumerate}

\hypertarget{applications-of-mle}{%
\subsection{Applications of MLE}\label{applications-of-mle}}

MLE is widely used across various applications to estimate parameters in models tailored for specific data structures. Below are key applications of MLE, categorized by problem type and estimation method.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.0899}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1236}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2275}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1236}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.4270}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Model Type}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Examples}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Characteristics}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Common Estimation Methods}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Additional Notes}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Corner Solution Models} & Hours worked

Donations to charity

Household consumption of a good & Dependent variable is often \textbf{censored at zero} (or another threshold).

Large fraction of observations at the corner (e.g., 0 hours, 0 donations). & Tobit regression

(latent variable approach with censoring) & Useful when a continuous outcome has a \textbf{mass point} at zero but also positive values (e.g., 30\% of individuals donate \$0, the rest donate \textgreater{} \$0). \\
\textbf{Non-Negative Count Models} & Number of arrests

Number of cigarettes smoked

Doctor visits per year & Dependent variable consists of \textbf{non-negative integer counts}.

Possible \textbf{overdispersion} (variance \textgreater{} mean). & Poisson regression,

Negative Binomial regression & Poisson assumes mean = variance, so often Negative Binomial is preferred for real data.

Zero-inflated models (ZIP/ZINB) may be used for data with \textbf{excess zeros}. \\
\textbf{Multinomial Choice Models} & Demand for different car brands

Votes in a primary election

Choice of travel mode & Dependent variable is a \textbf{categorical choice} among \textbf{3+ alternatives}.

Each category is distinct, with no inherent ordering (e.g., brand A, B, or C). & Multinomial logit,

Multinomial probit & Extension of binary choice (logit/probit) to multiple categories.

\textbf{Independence of Irrelevant Alternatives (IIA)} can be a concern for the multinomial logit. \\
\textbf{Ordinal Choice Models} & Self-reported happiness (low/medium/high)

Income level brackets

Likert-scale surveys & Dependent variable is \textbf{ordered} (e.g., \textbf{low \textless{} medium \textless{} high}).

Distances between categories are not necessarily equal. & Ordered logit,

Ordered probit & Probit/logit framework adapted to preserve \textbf{ordinal information}.

Interprets latent continuous variable mapped to discrete ordered categories. \\
\end{longtable}

\hypertarget{binary-response-models}{%
\subsubsection{Binary Response Models}\label{binary-response-models}}

A binary response variable (\(y_i\)) follows a \protect\hyperlink{bernoulli-distribution}{Bernoulli} distribution:

\[
f_Y(y_i; p) = p^{y_i}(1-p)^{(1-y_i)}
\]

where \(p\) is the probability of success. For conditional models, the likelihood becomes:

\[
f_{Y|X}(y_i, x_i; p(.)) = p(x_i)^{y_i}(1 - p(x_i))^{(1-y_i)}
\]

To model \(p(x_i)\), we use a function of \(x_i\) and unknown parameters \(\theta\). A common approach involves a \textbf{latent variable model}:

\[
\begin{aligned}
y_i &= 1\{y_i^* > 0 \}, \\
y_i^* &= x_i \beta - \epsilon_i,
\end{aligned}
\]

where:

\begin{itemize}
\tightlist
\item
  \(y_i^*\) is an unobserved (latent) variable.
\item
  \(\epsilon_i\) is a random variable with mean 0, representing unobserved noise.
\end{itemize}

Rewriting in terms of observed data:

\[
y_i = 1\{x_i \beta > \epsilon_i\}.
\]

The probability function becomes:

\[
\begin{aligned}
p(x_i) &= P(y_i = 1 | x_i) \\
&= P(x_i \beta > \epsilon_i | x_i) \\
&= F_{\epsilon|X}(x_i \beta | x_i),
\end{aligned}
\]

where \(F_{\epsilon|X}(.)\) is the cumulative distribution function (CDF) of \(\epsilon_i\). Assuming independence of \(\epsilon_i\) and \(x_i\), the probability function simplifies to:

\[
p(x_i) = F_\epsilon(x_i \beta).
\]

The conditional expectation function is equivalent:

\[
E(y_i | x_i) = P(y_i = 1 | x_i) = F_\epsilon(x_i \beta).
\]

Common Distributional Assumptions

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Probit Model}:

  \begin{itemize}
  \tightlist
  \item
    Assumes \(\epsilon_i\) follows a standard normal distribution.
  \item
    \(F_\epsilon(.) = \Phi(.)\), where \(\Phi(.)\) is the standard normal CDF.
  \end{itemize}
\item
  \textbf{Logit Model}:

  \begin{itemize}
  \tightlist
  \item
    Assumes \(\epsilon_i\) follows a standard logistic distribution.
  \item
    \(F_\epsilon(.) = \Lambda(.)\), where \(\Lambda(.)\) is the logistic CDF.
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Steps to Derive MLE for Binary Models}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Specify the Log-Likelihood}:

  \begin{itemize}
  \item
    For a chosen distribution (e.g., normal for Probit or logistic for Logit), the log-likelihood is:

    \[
    \ln(f_{Y|X}(y_i, x_i; \beta)) = y_i \ln(F_\epsilon(x_i \beta)) + (1 - y_i) \ln(1 - F_\epsilon(x_i \beta)).
    \]
  \end{itemize}
\item
  \textbf{Maximize the Log-Likelihood}:

  \begin{itemize}
  \item
    Find the parameter estimates that maximize the log-likelihood:

    \[
    \hat{\beta}_{MLE} = \underset{\beta}{\text{argmax}} \sum_{i=1}^{n} \ln(f_{Y|X}(y_i, x_i; \beta)).
    \]
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Properties of Probit and Logit Estimators}

\begin{itemize}
\tightlist
\item
  \textbf{Consistency and Asymptotic Normality}:

  \begin{itemize}
  \tightlist
  \item
    Probit and Logit estimators are consistent and asymptotically normal if:

    \begin{itemize}
    \tightlist
    \item
      \protect\hyperlink{a2-full-rank}{A2 Full Rank}: \(E(x_i' x_i)\) exists and is non-singular.
    \item
      \protect\hyperlink{a5-data-generation-random-sampling}{A5 Data Generation (Random Sampling)}: \(\{y_i, x_i\}\) are iid (or stationary and weakly dependent).
    \item
      Distributional assumptions on \(\epsilon_i\) hold (e.g., normal or logistic, independent of \(x_i\)).
    \end{itemize}
  \end{itemize}
\item
  \textbf{Asymptotic Efficiency}:

  \begin{itemize}
  \item
    Under these assumptions, Probit and Logit estimators are asymptotically efficient with variance:

    \[
    I(\beta_0)^{-1} = \left[E\left(\frac{(f_\epsilon(x_i \beta_0))^2}{F_\epsilon(x_i \beta_0)(1 - F_\epsilon(x_i \beta_0))} x_i' x_i \right)\right]^{-1},
    \]

    where \(f_\epsilon(x_i \beta_0)\) is the PDF (derivative of the CDF).
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Interpretation of Binary Response Models}

Binary response models, such as Probit and Logit, estimate the probability of an event occurring (\(y_i = 1\)) given predictor variables \(x_i\). However, interpreting the estimated coefficients (\(\beta\)) in these models differs significantly from linear models. Below, we explore how to interpret these coefficients and the concept of \textbf{partial effects}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Interpreting \(\beta\) in Binary Response Models
\end{enumerate}

In binary response models, the coefficient \(\beta_j\) represents the average change in the \textbf{latent variable} \(y_i^*\) (an unobserved variable) for a one-unit change in \(x_{ij}\). While this provides insight into the direction of the relationship:

\begin{itemize}
\tightlist
\item
  \textbf{Magnitudes} of \(\beta_j\) do not have a direct, meaningful interpretation in terms of \(y_i\).
\item
  \textbf{Direction} of \(\beta_j\) is meaningful:

  \begin{itemize}
  \tightlist
  \item
    \(\beta_j > 0\): A positive association between \(x_{ij}\) and the probability of \(y_i = 1\).
  \item
    \(\beta_j < 0\): A negative association between \(x_{ij}\) and the probability of \(y_i = 1\).
  \end{itemize}
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Partial Effects in Nonlinear Binary Models
\end{enumerate}

To interpret the effect of a change in a predictor \(x_{ij}\) on the probability of an event occurring (\(P(y_i = 1|x_i)\)), we use the \textbf{partial effect}:

\[
E(y_i | x_i) = F_\epsilon(x_i \beta),
\]

where \(F_\epsilon(.)\) is the cumulative distribution function (CDF) of the error term \(\epsilon_i\) (e.g., standard normal for Probit, logistic for Logit). The \textbf{partial effect} is the derivative of the expected probability with respect to \(x_{ij}\):

\[
PE(x_{ij}) = \frac{\partial E(y_i | x_i)}{\partial x_{ij}} = f_\epsilon(x_i \beta) \beta_j,
\]

where:

\begin{itemize}
\item
  \(f_\epsilon(.)\) is the probability density function (PDF) of the error term \(\epsilon_i\).
\item
  \(\beta_j\) is the coefficient associated with \(x_{ij}\).
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Key Characteristics of Partial Effects
\end{enumerate}

\begin{itemize}
\tightlist
\item
  \textbf{Scaling Factor}:

  \begin{itemize}
  \tightlist
  \item
    The partial effect depends on a \textbf{scaling factor}, \(f_\epsilon(x_i \beta)\), which is derived from the density function \(f_\epsilon(.)\).
  \item
    The scaling factor varies depending on the values of \(x_i\), making the partial effect nonlinear and \textbf{context-dependent}.
  \end{itemize}
\item
  \textbf{Non-Constant Partial Effects}:

  \begin{itemize}
  \tightlist
  \item
    Unlike linear models where coefficients directly represent constant marginal effects, the partial effect in binary models changes based on \(x_i\).
  \item
    For example, in a Logit model, the partial effect is largest when \(P(y_i = 1 | x_i)\) is around 0.5 (the midpoint of the S-shaped logistic curve) and smaller at the extremes (close to 0 or 1).
  \end{itemize}
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Single Values for Partial Effects
\end{enumerate}

In practice, researchers often summarize partial effects using either:

\begin{itemize}
\tightlist
\item
  \textbf{Partial Effect at the Average (PEA)}:

  \begin{itemize}
  \tightlist
  \item
    The partial effect is calculated for an ``average individual,'' where \(x_i = \bar{x}\) (the sample mean of predictors): \[
    PEA = f_\epsilon(\bar{x}\hat{\beta}) \hat{\beta}_j.
    \]
  \item
    This provides a single, interpretable value but assumes the average effect applies to all individuals.
  \end{itemize}
\item
  \textbf{Average Partial Effect (APE)}:

  \begin{itemize}
  \tightlist
  \item
    The average of all individual-level partial effects across the sample: \[
    APE = \frac{1}{n} \sum_{i=1}^{n} f_\epsilon(x_i \hat{\beta}) \hat{\beta}_j.
    \]
  \item
    This accounts for the nonlinearity of the partial effects and provides a more accurate summary of the marginal effect in the population.
  \end{itemize}
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Comparing Partial Effects in Linear and Nonlinear Models
\end{enumerate}

\begin{itemize}
\tightlist
\item
  \textbf{Linear Models}:

  \begin{itemize}
  \tightlist
  \item
    Partial effects are constant: \(APE = PEA\).
  \item
    The coefficients directly represent the marginal effects on \(E(y_i | x_i)\).
  \end{itemize}
\item
  \textbf{Nonlinear Models}:

  \begin{itemize}
  \tightlist
  \item
    Partial effects are \textbf{not constant} due to the dependence on \(f_\epsilon(x_i \beta)\).
  \item
    As a result, \(APE \neq PEA\) in general.
  \end{itemize}
\end{itemize}

\hypertarget{penalized-regularized-estimators}{%
\section{Penalized (Regularized) Estimators}\label{penalized-regularized-estimators}}

Penalized or regularized estimators are extensions of \protect\hyperlink{ordinary-least-squares}{Ordinary Least Squares} designed to address its limitations, particularly in high-dimensional settings. Regularization methods introduce a penalty term to the loss function to prevent overfitting, handle multicollinearity, and improve model interpretability.

There are three popular regularization techniques (but not limited to):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \protect\hyperlink{ridge-regression}{Ridge Regression}
\item
  \protect\hyperlink{lasso-regression}{Lasso Regression}
\item
  \protect\hyperlink{elastic-net}{Elastic Net}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{motivation-for-penalized-estimators}{%
\subsection{Motivation for Penalized Estimators}\label{motivation-for-penalized-estimators}}

OLS minimizes the Residual Sum of Squares (RSS):

\[
RSS = \sum_{i=1}^n \left( y_i - \hat{y}_i \right)^2 = \sum_{i=1}^n \left( y_i - x_i'\beta \right)^2,
\]

where:

\begin{itemize}
\item
  \(y_i\) is the observed outcome,
\item
  \(x_i\) is the vector of predictors for observation \(i\),
\item
  \(\beta\) is the vector of coefficients.
\end{itemize}

While OLS works well under ideal conditions (e.g., low dimensionality, no multicollinearity), it struggles when:

\begin{itemize}
\item
  \textbf{Multicollinearity}: Predictors are highly correlated, leading to large variances in \(\beta\) estimates.
\item
  \textbf{High Dimensionality}: The number of predictors (\(p\)) exceeds or approaches the sample size (\(n\)), making OLS inapplicable or unstable.
\item
  \textbf{Overfitting}: When \(p\) is large, OLS fits noise in the data, reducing generalizability.
\end{itemize}

To address these issues, \textbf{penalized regression} modifies the OLS loss function by adding a \textbf{penalty term} that shrinks the coefficients toward zero. This discourages overfitting and improves predictive performance.

The general form of the penalized loss function is:

\[
L(\beta) = \sum_{i=1}^n \left( y_i - x_i'\beta \right)^2 + \lambda P(\beta),
\]

where:

\begin{itemize}
\item
  \(\lambda \geq 0\): Tuning parameter controlling the strength of regularization.
\item
  \(P(\beta)\): Penalty term that quantifies model complexity.
\end{itemize}

Different choices of \(P(\beta)\) lead to ridge regression, lasso regression, or elastic net.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{ridge-regression}{%
\subsection{Ridge Regression}\label{ridge-regression}}

Ridge regression, also known as \textbf{L2 regularization}, penalizes the sum of squared coefficients:

\[
P(\beta) = \sum_{j=1}^p \beta_j^2.
\]

The ridge objective function becomes:

\[
L_{ridge}(\beta) = \sum_{i=1}^n \left( y_i - x_i'\beta \right)^2 + \lambda \sum_{j=1}^p \beta_j^2,
\]

where:

\begin{itemize}
\tightlist
\item
  \(\lambda \geq 0\) controls the degree of shrinkage. Larger \(\lambda\) leads to greater shrinkage.
\end{itemize}

Ridge regression has a closed-form solution:

\[
\hat{\beta}_{ridge} = \left( X'X + \lambda I \right)^{-1} X'y,
\]

where \(I\) is the \(p \times p\) identity matrix.

\textbf{Key Features}

\begin{itemize}
\tightlist
\item
  Shrinks coefficients but does \textbf{not set them exactly to zero}.
\item
  Handles multicollinearity effectively by stabilizing the coefficient estimates \citep{hoerl1970}.
\item
  Works well when all predictors contribute to the response.
\end{itemize}

\textbf{Example Use Case}

Ridge regression is ideal for applications with many correlated predictors, such as:

\begin{itemize}
\tightlist
\item
  Predicting housing prices based on a large set of features (e.g., size, location, age of the house).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{lasso-regression}{%
\subsection{Lasso Regression}\label{lasso-regression}}

Lasso regression, or \textbf{L1 regularization}, penalizes the sum of absolute coefficients:

\[
P(\beta) = \sum_{j=1}^p |\beta_j|.
\]

The lasso objective function is:

\[
L_{lasso}(\beta) = \sum_{i=1}^n \left( y_i - x_i'\beta \right)^2 + \lambda \sum_{j=1}^p |\beta_j|.
\]

\textbf{Key Features}

\begin{itemize}
\tightlist
\item
  Unlike ridge regression, lasso can set coefficients to \textbf{exactly zero}, performing automatic feature selection.
\item
  Encourages sparse models, making it suitable for high-dimensional data \citep{tibshirani1996}.
\end{itemize}

\textbf{Optimization}

Lasso does not have a closed-form solution due to the non-differentiability of \(|\beta_j|\) at \(\beta_j = 0\). It requires iterative algorithms, such as:

\begin{itemize}
\item
  \textbf{Coordinate Descent},
\item
  \textbf{Least Angle Regression (LARS)}.
\end{itemize}

\textbf{Example Use Case}

Lasso regression is useful when many predictors are irrelevant, such as:

\begin{itemize}
\tightlist
\item
  Genomics, where only a subset of genes are associated with a disease outcome.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{elastic-net}{%
\subsection{Elastic Net}\label{elastic-net}}

Elastic Net combines the penalties of ridge and lasso regression:

\[
P(\beta) = \alpha \sum_{j=1}^p |\beta_j| + \frac{1 - \alpha}{2} \sum_{j=1}^p \beta_j^2,
\]

where:

\begin{itemize}
\item
  \(0 \leq \alpha \leq 1\) determines the balance between lasso (L1) and ridge (L2) penalties.
\item
  \(\lambda\) controls the overall strength of regularization.
\end{itemize}

The elastic net objective function is:

\[
L_{elastic\ net}(\beta) = \sum_{i=1}^n \left( y_i - x_i'\beta \right)^2 + \lambda \left( \alpha \sum_{j=1}^p |\beta_j| + \frac{1 - \alpha}{2} \sum_{j=1}^p \beta_j^2 \right).
\]

\textbf{Key Features}

\begin{itemize}
\tightlist
\item
  Combines the strengths of lasso (sparse models) and ridge (stability with correlated predictors) \citep{zou2005a}.
\item
  Effective when predictors are highly correlated or when \(p > n\).
\end{itemize}

\textbf{Example Use Case}

Elastic net is ideal for high-dimensional datasets with correlated predictors, such as:

\begin{itemize}
\tightlist
\item
  Predicting customer churn using demographic and behavioral features.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{tuning-parameter-selection}{%
\subsection{Tuning Parameter Selection}\label{tuning-parameter-selection}}

Choosing the regularization parameter \(\lambda\) (and \(\alpha\) for elastic net) is critical for balancing model complexity (fit) and regularization (parsimony). If \(\lambda\) is too large, coefficients are overly shrunk (or even set to zero in the case of L1 penalty), leading to underfitting. If \(\lambda\) is too small, the model might overfit because coefficients are not penalized sufficiently. Hence, a systematic approach is needed to determine the optimal \(\lambda\). For elastic net, we also choose an appropriate \(\alpha\) to balance the L1 and L2 penalties.

\hypertarget{cross-validation}{%
\subsubsection{Cross-Validation}\label{cross-validation}}

A common approach to selecting \(\lambda\) (and \(\alpha\)) is \(K\)-Fold Cross-Validation:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Partition the data} into \(K\) roughly equal-sized ``folds.''
\item
  \textbf{Train the model} on \(K-1\) folds and \textbf{validate} on the remaining fold, computing a validation error.
\item
  Repeat this process \textbf{for all folds}, and compute the average validation error across the \(K\) folds.
\item
  \textbf{Select} the value of \(\lambda\) (and \(\alpha\) if tuning it) that \textbf{minimizes} the cross-validated error.
\end{enumerate}

This method helps us maintain a good bias-variance trade-off because every point is used for both training and validation exactly once.

\hypertarget{information-criteria}{%
\subsubsection{Information Criteria}\label{information-criteria}}

Alternatively, one can use \textbf{information criteria}---like the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC)---to guide model selection. These criteria reward goodness-of-fit while penalizing model complexity, thereby helping in selecting an appropriately regularized model.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{properties-of-penalized-estimators}{%
\subsection{Properties of Penalized Estimators}\label{properties-of-penalized-estimators}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Bias-Variance Tradeoff}:

  \begin{itemize}
  \tightlist
  \item
    Regularization introduces some bias in exchange for reducing variance, often resulting in better predictive performance on new data.
  \end{itemize}
\item
  \textbf{Shrinkage}:

  \begin{itemize}
  \tightlist
  \item
    Ridge shrinks coefficients toward zero but usually retains all predictors.
  \item
    Lasso shrinks some coefficients exactly to zero, performing inherent feature selection.
  \end{itemize}
\item
  \textbf{Flexibility}:

  \begin{itemize}
  \tightlist
  \item
    Elastic net allows for a continuum between ridge and lasso, so it can adapt to different data structures (e.g., many correlated features or very high-dimensional feature spaces).
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load required libraries}
\FunctionTok{library}\NormalTok{(glmnet)}

\CommentTok{\# Simulate data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{100}   \CommentTok{\# Number of observations}
\NormalTok{p }\OtherTok{\textless{}{-}} \DecValTok{20}    \CommentTok{\# Number of predictors}
\NormalTok{X }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(n }\SpecialCharTok{*}\NormalTok{ p), }\AttributeTok{nrow =}\NormalTok{ n, }\AttributeTok{ncol =}\NormalTok{ p)  }\CommentTok{\# Predictor matrix}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)                                 }\CommentTok{\# Response vector}

\CommentTok{\# Ridge regression (alpha = 0)}
\NormalTok{ridge\_fit }\OtherTok{\textless{}{-}} \FunctionTok{glmnet}\NormalTok{(X, y, }\AttributeTok{alpha =} \DecValTok{0}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(ridge\_fit, }\AttributeTok{xvar =} \StringTok{"lambda"}\NormalTok{, }\AttributeTok{label =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{title}\NormalTok{(}\StringTok{"Coefficient Paths for Ridge Regression"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{05-linear-regression_files/figure-latex/unnamed-chunk-19-1} \end{center}

\begin{itemize}
\item
  In this plot, each curve represents a coefficient's value as a function of \(\lambda\).
\item
  As \(\lambda\) increases (moving from left to right on a log-scale by default), coefficients shrink toward zero but typically stay non-zero.
\item
  Ridge regression tends to shrink coefficients but does not force them to be exactly zero.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Lasso regression (alpha = 1)}
\NormalTok{lasso\_fit }\OtherTok{\textless{}{-}} \FunctionTok{glmnet}\NormalTok{(X, y, }\AttributeTok{alpha =} \DecValTok{1}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(lasso\_fit, }\AttributeTok{xvar =} \StringTok{"lambda"}\NormalTok{, }\AttributeTok{label =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{title}\NormalTok{(}\StringTok{"Coefficient Paths for Lasso Regression"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{05-linear-regression_files/figure-latex/unnamed-chunk-20-1} \end{center}

Here, as \(\lambda\) grows, several coefficient paths \textbf{hit zero exactly}, illustrating the variable selection property of lasso.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Elastic net (alpha = 0.5)}
\NormalTok{elastic\_net\_fit }\OtherTok{\textless{}{-}} \FunctionTok{glmnet}\NormalTok{(X, y, }\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(elastic\_net\_fit, }\AttributeTok{xvar =} \StringTok{"lambda"}\NormalTok{, }\AttributeTok{label =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{title}\NormalTok{(}\StringTok{"Coefficient Paths for Elastic Net (alpha = 0.5)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{05-linear-regression_files/figure-latex/unnamed-chunk-21-1} \end{center}

\begin{itemize}
\item
  Elastic net combines ridge and lasso penalties. At \(\lambda = 0.5\), we see partial shrinkage and some coefficients going to zero.
\item
  This model is often helpful when you suspect both group-wise shrinkage (like ridge) and sparse solutions (like lasso) might be beneficial.
\end{itemize}

We can further refine our choice of \(\lambda\) by performing cross-validation on the lasso model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cv\_lasso }\OtherTok{\textless{}{-}} \FunctionTok{cv.glmnet}\NormalTok{(X, y, }\AttributeTok{alpha =} \DecValTok{1}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(cv\_lasso)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{05-linear-regression_files/figure-latex/unnamed-chunk-22-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{best\_lambda }\OtherTok{\textless{}{-}}\NormalTok{ cv\_lasso}\SpecialCharTok{$}\NormalTok{lambda.min}
\NormalTok{best\_lambda}
\CommentTok{\#\textgreater{} [1] 0.1449586}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  The plot displays the cross-validated error (often mean-squared error or deviance) on the y-axis versus \(\log(\lambda)\) on the x-axis.
\item
  Two vertical dotted lines typically appear:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \item
    \(\lambda.min\): The \(\lambda\) that achieves the minimum cross-validated error.
  \item
    \(\lambda.1se\): The largest \(\lambda\) such that the cross-validated error is still within one standard error of the minimum. This is a more conservative choice that favors higher regularization (simpler models).
  \end{enumerate}
\item
  \texttt{best\_lambda} above prints the numeric value of \(\lambda.min\). This is the \(\lambda\) that gave the lowest cross-validation error for the lasso model.
\end{itemize}

\textbf{Interpretation}:

\begin{itemize}
\item
  By using \texttt{cv.glmnet}, we systematically compare different values of \(\lambda\) in terms of their predictive performance (cross-validation error).
\item
  The selected \(\lambda\) typically balances having a smaller model (due to regularization) with retaining sufficient predictive power.
\item
  If we used real-world data, we might also look at performance metrics on a hold-out test set to ensure that the chosen \(\lambda\) generalizes well.
\end{itemize}

\hypertarget{robust-estimators}{%
\section{Robust Estimators}\label{robust-estimators}}

Robust estimators are statistical techniques designed to provide reliable parameter estimates even when the assumptions underlying classical methods, such as \protect\hyperlink{ordinary-least-squares}{Ordinary Least Squares}, are violated. Specifically, they address issues caused by outliers, non-normal errors, or heavy-tailed distributions, which can render OLS inefficient or biased.

The goal of robust estimation is to reduce the sensitivity of the estimator to extreme or aberrant data points, thereby ensuring a more reliable and accurate fit to the majority of the data.

We will cover the key robust estimation techniques, their properties, and applications, along with practical examples and mathematical derivations. The focus will include \(M\)-estimators, \(R\)-estimators, \(L\)-estimators, \(LTS\), \(S\)-estimators, \(MM\)-estimators, and more.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{motivation-for-robust-estimation}{%
\subsection{Motivation for Robust Estimation}\label{motivation-for-robust-estimation}}

OLS seeks to minimize the Residual Sum of Squares (RSS):

\[
RSS = \sum_{i=1}^n (y_i - x_i'\beta)^2,
\]

where:

\begin{itemize}
\tightlist
\item
  \(y_i\) is the observed response for the \(i\)th observation,
\item
  \(x_i\) is the vector of predictors for the \(i\)th observation,
\item
  \(\beta\) is the vector of coefficients.
\end{itemize}

OLS assumes:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Errors are normally distributed and no outliers in the data (\protect\hyperlink{a6-normal-distribution}{A6 Normal Distribution}).
\item
  Homoscedasticity (constant variance of errors) (\protect\hyperlink{a4-homoskedasticity}{A4 Homoskedasticity}).
\end{enumerate}

In real-world scenarios:

\begin{itemize}
\item
  \textbf{Outliers} in \(y\) or \(x\) can disproportionately affect the estimates, leading to biased or inefficient results.
\item
  \textbf{Heavy-tailed distributions} (e.g., Cauchy) violate the normality assumption, making OLS inappropriate.
\end{itemize}

For example, \citet{huber1964} demonstrates that a single extreme observation can arbitrarily distort OLS estimates, while \citet{hampel2005} define the breakdown point as a measure of robustness. Robust estimators aim to mitigate these problems by limiting the influence of problematic observations.

OLS inherently squares the residuals \(e_i = y_i - x_i'\beta\), amplifying the influence of large residuals. For example, if a single residual is much larger than the others, its squared value can dominate the RSS, distorting the estimated coefficients.

Consider a simple case where \(y_i = \beta_0 + \beta_1 x_i + e_i\), with \(e_i \sim N(0, \sigma^2)\) under the classical assumptions. Now introduce an outlier: a single observation with an unusually large \(e_i\). The squared residual for this point will dominate the RSS and pull the estimated regression line towards it, leading to biased estimates of \(\beta_0\) and \(\beta_1\).

The \textbf{breakdown point} of an estimator is the proportion of contamination (e.g., outliers) that the estimator can tolerate before yielding arbitrarily large or incorrect results. For OLS, the breakdown point is \(1/n\), meaning even one outlier can cause substantial distortion in the estimates.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{m-estimators}{%
\subsection{\texorpdfstring{\(M\)-Estimators}{M-Estimators}}\label{m-estimators}}

To address the sensitivity of OLS, robust estimators minimize a different objective function:

\[
\sum_{i=1}^n \rho\left(\frac{y_i - x_i'\beta}{\sigma}\right),
\]

where:

\begin{itemize}
\tightlist
\item
  \(\rho(\cdot)\) is a robust loss function that grows slower than the quadratic function used in OLS,
\item
  \(\sigma\) is a scale parameter to normalize residuals.
\end{itemize}

In OLS, the quadratic loss function \(\rho(z) = z^2\) penalizes large residuals disproportionately. Robust estimators replace this with alternative \(\rho\) functions that limit the penalty for large residuals, thus reducing their influence on the parameter estimates.

A robust \(\rho\) function should satisfy the following properties:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Bounded Influence}: Large residuals contribute a finite amount to the objective function.
\item
  \textbf{Symmetry}: \(\rho(z) = \rho(-z)\) ensures that positive and negative residuals are treated equally.
\item
  \textbf{Differentiability}: For computational tractability, \(\rho\) should be smooth and differentiable.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{examples-of-robust-rho-functions}{%
\subsubsection{\texorpdfstring{Examples of Robust \(\rho\) Functions}{Examples of Robust \textbackslash rho Functions}}\label{examples-of-robust-rho-functions}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Huber's Loss Function} \citep{huber1964}
\end{enumerate}

Huber's loss function transitions between quadratic and linear growth:

\[
\rho(z) = 
\begin{cases} 
\frac{z^2}{2} & \text{if } |z| \leq c, \\
c|z| - \frac{c^2}{2} & \text{if } |z| > c.
\end{cases}
\]

Key features:

\begin{itemize}
\tightlist
\item
  For small residuals (\(|z| \leq c\)), the loss is quadratic, mimicking OLS.
\item
  For large residuals (\(|z| > c\)), the loss grows linearly, limiting their influence.
\end{itemize}

The parameter \(c\) controls the threshold at which the loss function transitions from quadratic to linear. Smaller values of \(c\) make the estimator more robust but potentially less efficient under normality.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Tukey's Bisquare Function} \citep{beaton1974}
\end{enumerate}

Tukey's bisquare function completely bounds the influence of large residuals:

\[
\rho(z) = 
\begin{cases} 
c^2 \left(1 - \left(1 - \left(\frac{z}{c}\right)^2\right)^3\right)/6 & \text{if } |z| \leq c, \\
c^2/6 & \text{if } |z| > c.
\end{cases}
\]

Key features:

\begin{itemize}
\tightlist
\item
  Residuals larger than \(c\) contribute a constant value to the objective function, effectively excluding them from the estimation process.
\item
  This approach achieves high robustness at the cost of lower efficiency for small residuals.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Andrews' Sine Function} \citep{andrews1974}:

  \begin{itemize}
  \tightlist
  \item
    Smoothly downweights extreme residuals: \[ \rho(z) = \begin{cases}  c^2 \left(1 - \cos\left(\frac{z}{c}\right)\right)/2 & \text{if } |z| \leq \pi c, \\ c^2/2 & \text{if } |z| > \pi c. \end{cases} \]
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{weighting-scheme-influence-functions}{%
\subsubsection{Weighting Scheme: Influence Functions}\label{weighting-scheme-influence-functions}}

A critical concept in robust estimation is the \textbf{influence function}, which describes the sensitivity of the estimator to individual observations. For \(M\)-estimators, the influence function is derived as the derivative of the loss function \(\rho(z)\) with respect to \(z\):

\[
\psi(z) = \frac{d}{dz} \rho(z).
\]

This function plays a crucial role in downweighting large residuals. The weight assigned to each residual is proportional to \(\psi(z)/z\), which decreases as \(|z|\) increases for robust estimators.

For Huber's loss function, the influence function is:

\[
\psi(z) = 
\begin{cases} 
z & \text{if } |z| \leq c, \\
c \cdot \text{sign}(z) & \text{if } |z| > c.
\end{cases}
\]

\begin{itemize}
\tightlist
\item
  For small residuals, \(\psi(z) = z\), matching OLS.
\item
  For large residuals, \(\psi(z)\) is constant, ensuring bounded influence.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

A key consideration when selecting a robust estimator is the trade-off between \textbf{robustness} (resistance to outliers) and \textbf{efficiency} (performance under ideal conditions). The tuning parameters in \(\rho\) functions (e.g., \(c\) in Huber's loss) directly affect this balance:

\begin{itemize}
\tightlist
\item
  Smaller \(c\) increases robustness but reduces efficiency under normality.
\item
  Larger \(c\) improves efficiency under normality but decreases robustness to outliers.
\end{itemize}

This trade-off reflects the fundamental goal of robust estimation: to achieve a balance between reliability and precision across a wide range of data scenarios.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{properties-of-m-estimators}{%
\subsubsection{\texorpdfstring{Properties of \(M\)-Estimators}{Properties of M-Estimators}}\label{properties-of-m-estimators}}

Robust estimators, particularly \(M\)-estimators, possess the following mathematical properties:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Asymptotic Normality}: Under mild regularity conditions, \(M\)-estimators are asymptotically normal: \[
  \sqrt{n} (\hat{\beta} - \beta) \xrightarrow{d} N(0, \Sigma),
  \] where \(\Sigma\) depends on the choice of \(\rho\) and the distribution of residuals.
\item
  \textbf{Consistency}: As \(n \to \infty\), \(\hat{\beta} \to \beta\) in probability, provided the majority of the data satisfies the model assumptions.
\item
  \textbf{Breakdown Point}: \(M\)-estimators typically have a moderate breakdown point, sufficient to handle a reasonable proportion of contamination.
\end{enumerate}

\hypertarget{r-estimators}{%
\subsection{\texorpdfstring{\(R\)-Estimators}{R-Estimators}}\label{r-estimators}}

\(R\)-estimators are a class of robust estimators that rely on the ranks of residuals rather than their raw magnitudes. This approach makes them naturally resistant to the influence of outliers and highly effective in scenarios involving ordinal data or heavy-tailed error distributions. By leveraging rank-based methods, \(R\)-estimators are particularly useful in situations where classical assumptions about the data, such as normality or homoscedasticity, do not hold.

The general form of an \(R\)-estimator can be expressed as:

\[
\hat{\beta}_R = \arg\min_\beta \sum_{i=1}^n w_i R_i \left(y_i - x_i'\beta\right),
\]

where:

\begin{itemize}
\tightlist
\item
  \(R_i\) are the ranks of residuals \(e_i = y_i - x_i'\beta\),
\item
  \(w_i\) are rank-based weights determined by a chosen scoring function,
\item
  \(y_i\) are observed responses, \(x_i\) are predictor values, and \(\beta\) is the vector of coefficients.
\end{itemize}

This formulation differs from \(M\)-estimators, which directly minimize a loss function \(\rho\), by instead using the \textbf{ordering of residuals} to drive the estimation.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{ranks-and-scoring-function}{%
\subsubsection{Ranks and Scoring Function}\label{ranks-and-scoring-function}}

\hypertarget{definition-of-ranks}{%
\paragraph{Definition of Ranks}\label{definition-of-ranks}}

The rank \(R_i\) of a residual \(e_i\) is its position in the sorted sequence of all residuals:

\[
R_i = \text{rank}(e_i) = \sum_{j=1}^n \mathbb{I}(e_j \leq e_i),
\]

where \(\mathbb{I}(\cdot)\) is the indicator function, equal to 1 if the condition is true and 0 otherwise. This step transforms the residuals into an ordinal scale, eliminating their dependency on magnitude.

\hypertarget{scoring-function}{%
\paragraph{Scoring Function}\label{scoring-function}}

The weights \(w_i\) are derived from a scoring function \(S(R_i)\), which assigns importance to each rank. A common choice is the \textbf{Wilcoxon scoring function}, defined as:

\[
S(R_i) = \frac{R_i}{n + 1},
\]

which gives equal weight to all ranks, scaled by their position relative to the total number of observations \(n\).

Other scoring functions can emphasize different parts of the rank distribution:

\begin{itemize}
\tightlist
\item
  \textbf{Normal Scores}: Derived from the quantiles of a standard normal distribution.
\item
  \textbf{Logarithmic Scores}: Weight lower ranks more heavily.
\end{itemize}

The flexibility of the scoring function allows \(R\)-estimators to adapt to various data structures and assumptions.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{properties-of-r-estimators}{%
\subsubsection{\texorpdfstring{Properties of \(R\)-Estimators}{Properties of R-Estimators}}\label{properties-of-r-estimators}}

\hypertarget{influence-function-and-robustness}{%
\paragraph{Influence Function and Robustness}\label{influence-function-and-robustness}}

A key feature of \(R\)-estimators is their \textbf{bounded influence function}, which ensures robustness. Because the estimator depends only on the ranks of the residuals, extreme values in \(y\) or \(x\) do not disproportionately affect the results.

For \(R\)-estimators, the influence function \(\psi(e_i)\) is proportional to the derivative of the rank-based objective function:

\[
\psi(e_i) = S'(R_i),
\]

where \(S'(R_i)\) is the derivative of the scoring function. Since \(R_i\) depends only on the ordering of residuals, outliers in the data cannot produce excessive changes in \(R_i\), resulting in bounded influence.

\hypertarget{breakdown-point}{%
\paragraph{Breakdown Point}\label{breakdown-point}}

The \textbf{breakdown point} of \(R\)-estimators is higher than that of OLS and comparable to other robust methods. This means they can tolerate a larger proportion of contaminated data without yielding unreliable results.

\hypertarget{asymptotic-efficiency}{%
\paragraph{Asymptotic Efficiency}\label{asymptotic-efficiency}}

Under specific scoring functions, \(R\)-estimators achieve high asymptotic efficiency. For example, the Wilcoxon \(R\)-estimator performs nearly as well as OLS under normality while retaining robustness to non-normality.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{derivation-of-r-estimators-for-simple-linear-regression}{%
\subsubsection{\texorpdfstring{Derivation of \(R\)-Estimators for Simple Linear Regression}{Derivation of R-Estimators for Simple Linear Regression}}\label{derivation-of-r-estimators-for-simple-linear-regression}}

Consider the simple linear regression model:

\[
y_i = \beta_0 + \beta_1 x_i + e_i,
\]

where \(e_i = y_i - (\beta_0 + \beta_1 x_i)\) are the residuals.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Rank the Residuals}: Compute the residuals \(e_i\) for all observations and rank them from smallest to largest.
\item
  \textbf{Assign Weights}: Compute weights \(w_i\) for each residual rank based on the scoring function \(S(R_i)\).
\item
  \textbf{Minimize the Rank-Based Objective}: Solve the following optimization problem:

  \[
  \hat{\beta}_R = \arg\min_{\beta_0, \beta_1} \sum_{i=1}^n w_i R_i \left( y_i - (\beta_0 + \beta_1 x_i) \right).
  \]

  This minimization can be performed iteratively using numerical methods, as the rank-based nature of the function makes direct analytic solutions challenging.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{comparison-to-m-estimators}{%
\subsubsection{\texorpdfstring{Comparison to \(M\)-Estimators}{Comparison to M-Estimators}}\label{comparison-to-m-estimators}}

While \(M\)-estimators downweight large residuals using robust loss functions, \(R\)-estimators completely avoid reliance on the magnitude of residuals by using their ranks. This distinction has important implications:

\begin{itemize}
\tightlist
\item
  \(R\)-estimators are naturally robust to leverage points and extreme outliers.
\item
  The performance of \(R\)-estimators is less sensitive to the choice of scale parameter compared to \(M\)-estimators.
\item
  However, \(R\)-estimators may be less efficient than \(M\)-estimators under normality because they do not use the full information contained in the residual magnitudes.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{l-estimators}{%
\subsection{\texorpdfstring{\(L\)-Estimators}{L-Estimators}}\label{l-estimators}}

\(L\)-estimators are a class of robust estimators constructed as \textbf{linear combinations of order statistics}, where order statistics are simply the sorted values of a dataset. These estimators are particularly appealing due to their intuitive nature and computational simplicity. By using the relative ranks of observations, \(L\)-estimators offer robustness against outliers and heavy-tailed distributions.

Order statistics are denoted as \(y_{(1)}, y_{(2)}, \dots, y_{(n)}\), where \(y_{(i)}\) is the \(i\)th smallest observation in the sample.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The general form of an \(L\)-estimator is:

\[
\hat{\theta}_L = \sum_{i=1}^n c_i y_{(i)},
\]

where:

\begin{itemize}
\tightlist
\item
  \(y_{(i)}\) are the order statistics (sorted observations),
\item
  \(c_i\) are coefficients (weights) that determine the contribution of each order statistic to the estimator.
\end{itemize}

By appropriately choosing the weights \(c_i\), different types of \(L\)-estimators can be constructed to suit specific needs, such as handling outliers or capturing central tendencies robustly.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Examples of \(L\)-Estimators

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Sample Median}: The sample median is a simple \(L\)-estimator where only the middle order statistic contributes (for odd \(n\)) or the average of the two middle order statistics contributes (for even \(n\)):

  \[
  \hat{\mu}_{\text{median}} = 
  \begin{cases} 
  y_{\left(\frac{n+1}{2}\right)} & \text{if } n \text{ is odd}, \\
  \frac{1}{2}\left(y_{\left(\frac{n}{2}\right)} + y_{\left(\frac{n}{2} + 1\right)}\right) & \text{if } n \text{ is even}.
  \end{cases}
  \]

  \begin{itemize}
  \tightlist
  \item
    \textbf{Robustness}: The median has a breakdown point of \(50\%\), meaning it remains unaffected unless more than half the data are corrupted.
  \item
    \textbf{Efficiency}: Under normality, the efficiency of the median is lower than that of the mean (about \(64\%\)).
  \end{itemize}
\item
  \textbf{Trimmed Mean}: The trimmed mean excludes the smallest and largest \(k\%\) of observations before averaging the remaining values:

  \[
  \hat{\mu}_T = \frac{1}{n - 2k} \sum_{i=k+1}^{n-k} y_{(i)},
  \]

  where:

  \begin{itemize}
  \item
    \(k\) is the number of observations trimmed from each tail,
  \item
    \(n\) is the sample size.
  \item
    \textbf{Robustness}: The trimmed mean is less sensitive to extreme values than the sample mean.
  \item
    \textbf{Efficiency}: By retaining most observations, the trimmed mean achieves a good balance between robustness and efficiency.
  \end{itemize}
\item
  \textbf{Winsorized Mean}: Similar to the trimmed mean, but instead of excluding extreme values, it replaces them with the nearest remaining observations:

  \[
  \hat{\mu}_W = \frac{1}{n} \sum_{i=1}^n y_{(i)}^*,
  \]

  where \(y_{(i)}^*\) are ``Winsorized'' values: \[
  y_{(i)}^* = 
  \begin{cases} 
  y_{(k+1)} & \text{if } i \leq k, \\
  y_{(i)} & \text{if } k+1 \leq i \leq n-k, \\
  y_{(n-k)} & \text{if } i > n-k.
  \end{cases}
  \]

  \begin{itemize}
  \tightlist
  \item
    \textbf{Robustness}: The Winsorized mean reduces the influence of outliers without discarding data.
  \item
    \textbf{Efficiency}: Slightly less efficient than the trimmed mean under normality.
  \end{itemize}
\item
  \textbf{Midrange}: The midrange is the average of the smallest and largest observations:

  \[
  \hat{\mu}_{\text{midrange}} = \frac{y_{(1)} + y_{(n)}}{2}.
  \]

  \begin{itemize}
  \tightlist
  \item
    \textbf{Robustness}: Poor robustness, as it depends entirely on the extreme observations.
  \item
    \textbf{Simplicity}: Highly intuitive and computationally trivial.
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{properties-of-l-estimators}{%
\subsubsection{\texorpdfstring{Properties of \(L\)-Estimators}{Properties of L-Estimators}}\label{properties-of-l-estimators}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Robustness to Outliers}: \(L\)-estimators gain robustness by downweighting or excluding extreme observations. For instance:

  \begin{itemize}
  \tightlist
  \item
    The trimmed mean completely removes outliers from the estimation process.
  \item
    The Winsorized mean limits the influence of outliers by bounding their values.
  \end{itemize}
\item
  \textbf{Breakdown Point}:

  \begin{itemize}
  \tightlist
  \item
    The breakdown point of an \(L\)-estimator depends on how many extreme observations are excluded or replaced.
  \item
    The median has the highest possible breakdown point (\(50\%\)), while the trimmed and Winsorized means have breakdown points proportional to the trimming percentage.
  \end{itemize}
\item
  \textbf{Efficiency}:

  \begin{itemize}
  \tightlist
  \item
    The efficiency of \(L\)-estimators varies depending on the underlying data distribution and the specific estimator.
  \item
    For symmetric distributions, the trimmed mean and Winsorized mean approach the efficiency of the sample mean while being much more robust.
  \end{itemize}
\item
  \textbf{Computational Simplicity}:

  \begin{itemize}
  \tightlist
  \item
    \(L\)-estimators involve simple operations like sorting and averaging, making them computationally efficient even for large datasets.
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{derivation-of-the-trimmed-mean}{%
\subsubsection{Derivation of the Trimmed Mean}\label{derivation-of-the-trimmed-mean}}

To understand the robustness of the trimmed mean, consider a dataset with \(n\) observations. Sorting the data gives \(y_{(1)} \leq y_{(2)} \leq \dots \leq y_{(n)}\). After trimming the smallest \(k\) and largest \(k\) observations, the remaining \(n - 2k\) observations are used to compute the mean:

\[
\hat{\mu}_T = \frac{1}{n - 2k} \sum_{i=k+1}^{n-k} y_{(i)}.
\]

Key observations:

\begin{itemize}
\tightlist
\item
  \textbf{Impact of} \(k\): Larger \(k\) increases robustness by removing more extreme values but reduces efficiency by discarding more data.
\item
  \textbf{Choosing} \(k\): In practice, \(k\) is often chosen as a percentage of the total sample size, such as \(10\%\) trimming (\(k = 0.1n\)).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{least-trimmed-squares-lts}{%
\subsection{Least Trimmed Squares (LTS)}\label{least-trimmed-squares-lts}}

Least Trimmed Squares (LTS) is a robust regression method that minimizes the sum of the smallest \(h\) squared residuals, rather than using all residuals as in \protect\hyperlink{ordinary-least-squares}{Ordinary Least Squares}. This approach ensures that large residuals, often caused by outliers or leverage points, have no influence on the parameter estimation.

The LTS estimator is defined as:

\[
\hat{\beta}_{LTS} = \arg\min_\beta \sum_{i=1}^h r_{[i]}^2,
\]

where:

\begin{itemize}
\tightlist
\item
  \(r_{[i]}^2\) are the ordered squared residuals, ranked from smallest to largest,
\item
  \(h\) is the subset size of residuals to include in the minimization, typically chosen as \(h = \lfloor n/2 \rfloor + 1\) (where \(n\) is the sample size).
\end{itemize}

This trimming process ensures robustness by focusing on the best-fitting \(h\) observations and ignoring the most extreme residuals.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{motivation-for-lts}{%
\subsubsection{Motivation for LTS}\label{motivation-for-lts}}

In OLS regression, the objective is to minimize the Residual Sum of Squares (RSS):

\[
RSS = \sum_{i=1}^n r_i^2,
\]

where \(r_i = y_i - x_i'\beta\) are the residuals. However, this method is highly sensitive to outliers because even one large residual (\(r_i^2\)) can dominate the RSS, distorting the parameter estimates \(\beta\).

LTS addresses this issue by trimming the largest residuals and focusing only on the \(h\) smallest ones, thus preventing extreme values from affecting the fit. This approach provides a more robust estimate of the regression coefficients \(\beta\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{properties-of-lts}{%
\subsubsection{Properties of LTS}\label{properties-of-lts}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Objective Function}: The LTS objective function is non-differentiable because it involves ordering the squared residuals. Formally, the ordered residuals are denoted as:

  \[
  r_{[1]}^2 \leq r_{[2]}^2 \leq \dots \leq r_{[n]}^2,
  \]

  and the objective is to minimize:

  \[
  \sum_{i=1}^h r_{[i]}^2.
  \]

  This requires sorting the squared residuals, making the computation more complex than OLS.
\item
  \textbf{Choice of} \(h\): The parameter \(h\) determines the number of residuals included in the minimization. A common choice is:

  \[
  h = \lfloor n/2 \rfloor + 1,
  \]

  which ensures a high breakdown point (discussed below). Smaller values of \(h\) increase robustness but reduce efficiency, while larger \(h\) values improve efficiency but decrease robustness.
\item
  \textbf{Breakdown Point}: LTS has a \textbf{breakdown point} of approximately \(50\%\), the highest possible for a regression estimator. This means that LTS can handle up to \(50\%\) of contaminated data (e.g., outliers) without yielding unreliable estimates.
\item
  \textbf{Robustness}: By focusing only on the \(h\) best-fitting observations, LTS naturally excludes outliers from the estimation process, making it highly robust to both vertical outliers (extreme values in \(y\)) and leverage points (extreme values in \(x\)).
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{algorithm-for-lts}{%
\subsubsection{Algorithm for LTS}\label{algorithm-for-lts}}

Computing the LTS estimator involves the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Initialization}: Select an initial subset of \(h\) observations to compute a preliminary fit for \(\beta\).
\item
  \textbf{Residual Calculation}: For each observation, compute the squared residuals:

  \[
  r_i^2 = \left(y_i - x_i'\beta\right)^2.
  \]
\item
  \textbf{Trimming}: Rank the residuals from smallest to largest and retain only the \(h\) smallest residuals.
\item
  \textbf{Refitting}: Use the \(h\) retained observations to recompute the regression coefficients \(\beta\).
\item
  \textbf{Iterative Refinement}: Repeat the process (residual calculation, trimming, refitting) until convergence, typically when \(\beta\) stabilizes.
\end{enumerate}

Efficient algorithms, such as the \textbf{Fast-LTS} algorithm, are used in practice to reduce computational complexity.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{comparison-of-lts-with-ols}{%
\subsubsection{Comparison of LTS with OLS}\label{comparison-of-lts-with-ols}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2778}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2963}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4167}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Property
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
OLS
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
LTS
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Objective} & Minimize \(\sum_{i=1}^n r_i^2\) & Minimize \(\sum_{i=1}^h r_{[i]}^2\) \\
\textbf{Sensitivity to Outliers} & High & Low \\
\textbf{Breakdown Point} & \(1/n\) & \(\approx 50\%\) \\
\textbf{Computational Cost} & Low & Moderate (requires sorting and iterations) \\
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{s-estimators}{%
\subsection{\texorpdfstring{\(S\)-Estimators}{S-Estimators}}\label{s-estimators}}

\(S\)-estimators are a class of robust estimators that focus on minimizing a robust measure of the \textbf{dispersion} of residuals. Unlike methods such as \(M\)-estimators, which directly minimize a loss function based on residuals, \(S\)-estimators aim to find the parameter values \(\beta\) that produce residuals with the smallest robust scale. These estimators are particularly useful in handling datasets with outliers, heavy-tailed distributions, or other violations of classical assumptions.

The scale \(\sigma\) is estimated by solving the following minimization problem:

\[
\hat{\sigma}_S = \arg\min_\sigma \frac{1}{n} \sum_{i=1}^n \rho\left(\frac{y_i - x_i'\beta}{\sigma}\right),
\]

where:

\begin{itemize}
\tightlist
\item
  \(\rho\) is a robust loss function that controls the influence of residuals,
\item
  \(y_i\) are observed responses, \(x_i\) are predictors, \(\beta\) is the vector of regression coefficients,
\item
  \(\sigma\) represents the robust scale of the residuals.
\end{itemize}

Once \(\sigma\) is estimated, the \(S\)-estimator of \(\beta\) is obtained by solving:

\[
\hat{\beta}_S = \arg\min_\beta \hat{\sigma}_S.
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{motivation-for-s-estimators}{%
\subsubsection{\texorpdfstring{Motivation for \(S\)-Estimators}{Motivation for S-Estimators}}\label{motivation-for-s-estimators}}

In regression analysis, classical methods such as Ordinary Least Squares rely on minimizing the Residual Sum of Squares (RSS). However, OLS is highly sensitive to outliers because even a single extreme residual can dominate the sum of squared residuals, leading to biased estimates of \(\beta\).

\(S\)-estimators address this limitation by using a robust scale \(\sigma\) to evaluate the dispersion of residuals. By minimizing this scale, \(S\)-estimators effectively downweight the influence of outliers, resulting in parameter estimates that are more resistant to contamination in the data.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{key-concepts-in-s-estimators}{%
\subsubsection{\texorpdfstring{Key Concepts in \(S\)-Estimators}{Key Concepts in S-Estimators}}\label{key-concepts-in-s-estimators}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Robust Scale Function}: The key idea of \(S\)-estimators is to minimize a robust measure of scale. The scale \(\sigma\) is computed such that the residuals normalized by \(\sigma\) produce a value close to the expected contribution of well-behaved observations.

  Formally, \(\sigma\) satisfies:

  \[
  \frac{1}{n} \sum_{i=1}^n \rho\left(\frac{y_i - x_i'\beta}{\sigma}\right) = \delta,
  \]

  where \(\delta\) is a constant that depends on the choice of \(\rho\) and ensures consistency under normality. This equation balances the residuals and controls their influence on the scale estimate.
\item
  \textbf{Choice of} \(\rho\)-Function: The choice of the robust \(\rho\) function is critical in determining the behavior of \(S\)-estimators. Common \(\rho\) functions include:

  \begin{itemize}
  \item
    \textbf{Huber's} \(\rho\)-Function: \[
    \rho(z) = 
    \begin{cases} 
    z^2/2 & \text{if } |z| \leq c, \\
    c|z| - c^2/2 & \text{if } |z| > c.
    \end{cases}
    \]
  \item
    \textbf{Tukey's Bisquare}: \[
    \rho(z) = 
    \begin{cases} 
    c^2 \left(1 - \left(1 - \left(\frac{z}{c}\right)^2\right)^3\right)/6 & \text{if } |z| \leq c, \\
    c^2/6 & \text{if } |z| > c.
    \end{cases}
    \]
  \item
    \textbf{Andrews' Sine}: \[
    \rho(z) = 
    \begin{cases} 
    c^2 \left(1 - \cos\left(\frac{z}{c}\right)\right)/2 & \text{if } |z| \leq \pi c, \\
    c^2/2 & \text{if } |z| > \pi c.
    \end{cases}
    \]
  \end{itemize}

  Robust \(\rho\) functions grow more slowly than the quadratic function used in OLS, limiting the impact of large residuals.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{properties-of-s-estimators}{%
\subsubsection{\texorpdfstring{Properties of \(S\)-Estimators}{Properties of S-Estimators}}\label{properties-of-s-estimators}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Breakdown Point}: \(S\)-estimators have a breakdown point of up to \(50\%\), meaning they can tolerate up to half the data being contaminated (e.g., outliers) without yielding unreliable estimates.
\item
  \textbf{Efficiency}: The efficiency of \(S\)-estimators depends on the choice of \(\rho\). While they are highly robust, their efficiency under ideal conditions (e.g., normality) may be lower than that of OLS. Proper tuning of \(\rho\) can balance robustness and efficiency.
\item
  \textbf{Influence Function}: The \textbf{influence function} measures the sensitivity of the estimator to a small perturbation in the data. For \(S\)-estimators, the influence function is bounded, ensuring robustness to outliers.
\item
  \textbf{Consistency}: Under mild regularity conditions, \(S\)-estimators are consistent, meaning \(\hat{\beta}_S \to \beta\) as the sample size \(n \to \infty\).
\item
  \textbf{Asymptotic Normality}: \(S\)-estimators are asymptotically normal, with:

  \[
  \sqrt{n}(\hat{\beta}_S - \beta) \xrightarrow{d} N(0, \Sigma),
  \]

  where \(\Sigma\) depends on the choice of \(\rho\) and the distribution of residuals.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{algorithm-for-computing-s-estimators}{%
\subsubsection{\texorpdfstring{Algorithm for Computing \(S\)-Estimators}{Algorithm for Computing S-Estimators}}\label{algorithm-for-computing-s-estimators}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Initial Guess}: Compute an initial estimate of \(\beta\) using a robust method (e.g., LTS or an \(M\)-estimator).
\item
  \textbf{Scale Estimation}: Compute a robust estimate of scale \(\hat{\sigma}\) by solving:

  \[
  \frac{1}{n} \sum_{i=1}^n \rho\left(\frac{y_i - x_i'\beta}{\sigma}\right) = \delta.
  \]
\item
  \textbf{Iterative Refinement}:

  \begin{itemize}
  \tightlist
  \item
    Recalculate residuals \(r_i = y_i - x_i'\beta\).
  \item
    Update \(\beta\) and \(\sigma\) iteratively until convergence, typically using numerical optimization techniques.
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{mm-estimators}{%
\subsection{\texorpdfstring{\(MM\)-Estimators}{MM-Estimators}}\label{mm-estimators}}

\(MM\)-estimators are a robust regression method that combines the strengths of two powerful techniques: \(S\)-estimators and \(M\)-estimators. They are designed to achieve both a \textbf{high breakdown point} (up to \(50\%\)) and \textbf{high efficiency} under ideal conditions (e.g., normality). This combination makes \(MM\)-estimators one of the most versatile and widely used robust regression methods.

The process of computing \(MM\)-estimators involves three main steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute an initial robust estimate of scale using an \(S\)-estimator.
\item
  Use this robust scale to define weights for an \(M\)-estimator.
\item
  Estimate regression coefficients by solving the weighted \(M\)-estimation problem.
\end{enumerate}

This stepwise approach ensures robustness in the initial scale estimation while leveraging the efficiency of \(M\)-estimators for the final parameter estimates.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Step 1: Robust Scale Estimation

The first step is to estimate the robust scale \(\sigma\) using an \(S\)-estimator. This involves solving:

\[
\hat{\sigma}_S = \arg\min_\sigma \frac{1}{n} \sum_{i=1}^n \rho_S\left(\frac{y_i - x_i'\beta}{\sigma}\right),
\]

where \(\rho_S\) is a robust loss function chosen to control the influence of extreme residuals. Common choices for \(\rho_S\) include Huber's or Tukey's bisquare functions. This scale estimation provides a robust baseline for weighting residuals in the subsequent \(M\)-estimation step.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Step 2: Weight Definition for \(M\)-Estimation

Using the robust scale \(\hat{\sigma}_S\) obtained in Step 1, the weights for the \(M\)-estimator are defined based on a second loss function, \(\rho_M\). The weights downweight residuals proportional to their deviation relative to \(\hat{\sigma}_S\). For each residual \(r_i = y_i - x_i'\beta\), the weight is computed as:

\[
w_i = \psi_M\left(\frac{r_i}{\hat{\sigma}_S}\right) / \frac{r_i}{\hat{\sigma}_S},
\]

where:

\begin{itemize}
\tightlist
\item
  \(\psi_M\) is the derivative of the robust \(\rho_M\) function, known as the \textbf{influence function}.
\item
  \(\rho_M\) is often chosen to provide high efficiency under normality, such as Huber's or Hampel's function.
\end{itemize}

These weights reduce the impact of large residuals while preserving the influence of small, well-behaved residuals.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Step 3: Final \(M\)-Estimation

The final step involves solving the \(M\)-estimation problem using the weights defined in Step 2. The coefficients \(\hat{\beta}_{MM}\) are estimated by minimizing the weighted residuals:

\[
\hat{\beta}_{MM} = \arg\min_\beta \sum_{i=1}^n w_i \rho_M\left(\frac{y_i - x_i'\beta}{\hat{\sigma}_S}\right).
\]

This ensures that the final estimates combine the robustness of the initial \(S\)-estimator with the efficiency of the \(M\)-estimator.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{properties-of-mm-estimators}{%
\subsubsection{\texorpdfstring{Properties of \(MM\)-Estimators}{Properties of MM-Estimators}}\label{properties-of-mm-estimators}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{High Breakdown Point}:

  \begin{itemize}
  \tightlist
  \item
    The \(S\)-estimator in the first step ensures a breakdown point of up to \(50\%\), meaning the estimator can handle up to half the data being contaminated without producing unreliable results.
  \end{itemize}
\item
  \textbf{Asymptotic Efficiency}:

  \begin{itemize}
  \tightlist
  \item
    The use of an efficient \(\rho_M\) function in the final \(M\)-estimation step ensures that \(MM\)-estimators achieve high asymptotic efficiency under normality, often close to that of OLS.
  \end{itemize}
\item
  \textbf{Robustness}:

  \begin{itemize}
  \tightlist
  \item
    The combination of robust scale estimation and downweighting of large residuals makes \(MM\)-estimators highly robust to outliers and leverage points.
  \end{itemize}
\item
  \textbf{Influence Function}:

  \begin{itemize}
  \tightlist
  \item
    The influence function of \(MM\)-estimators is bounded, ensuring that no single observation can exert disproportionate influence on the parameter estimates.
  \end{itemize}
\item
  \textbf{Consistency}:

  \begin{itemize}
  \tightlist
  \item
    \(MM\)-estimators are consistent, converging to the true parameter values as the sample size increases, provided the majority of the data satisfies the model assumptions.
  \end{itemize}
\item
  \textbf{Asymptotic Normality}:

  \begin{itemize}
  \item
    \(MM\)-estimators are asymptotically normal, with:

    \[
    \sqrt{n} (\hat{\beta}_{MM} - \beta) \xrightarrow{d} N(0, \Sigma),
    \]

    where \(\Sigma\) depends on the choice of \(\rho_M\) and the distribution of residuals.
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{choice-of-rho-functions-for-mm-estimators}{%
\subsubsection{\texorpdfstring{Choice of \(\rho\)-Functions for \(MM\)-Estimators}{Choice of \textbackslash rho-Functions for MM-Estimators}}\label{choice-of-rho-functions-for-mm-estimators}}

The robustness and efficiency of \(MM\)-estimators depend on the choice of \(\rho_S\) (for scale) and \(\rho_M\) (for final estimation). Common choices include:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Huber's} \(\rho\)-Function: Combines quadratic and linear growth to balance robustness and efficiency:

  \[
  \rho(z) = 
  \begin{cases} 
  \frac{z^2}{2} & \text{if } |z| \leq c, \\
  c|z| - \frac{c^2}{2} & \text{if } |z| > c.
  \end{cases}
  \]
\item
  \textbf{Tukey's Bisquare Function}: Provides high robustness by completely bounding large residuals:

  \[
  \rho(z) = 
  \begin{cases} 
  c^2 \left(1 - \left(1 - \left(\frac{z}{c}\right)^2\right)^3\right)/6 & \text{if } |z| \leq c, \\
  c^2/6 & \text{if } |z| > c.
  \end{cases}
  \]
\item
  \textbf{Hampel's Three-Part Redescending Function}: Further limits the influence of large residuals by assigning a constant penalty beyond a certain threshold.

  \[
  \rho(z) = 
  \begin{cases} 
  z^2/2 & \text{if } |z| \leq a, \\
  a|z| - a^2/2 & \text{if } a < |z| \leq b, \\
  \text{constant} & \text{if } |z| > b.
  \end{cases}
  \]
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{practical-considerations-1}{%
\subsection{Practical Considerations}\label{practical-considerations-1}}

The following table summarizes the key properties, advantages, and limitations of the robust estimators discussed:
+-----------------+----------------------------------------------------------------------+-------------------------+------------------------------+--------------------------------------------------------------------------+-----------------------------------------------+
\textbar{} Estimator \textbar{} Key Features \textbar{} Breakdown Point \textbar{} Efficiency (Under Normality) \textbar{} Applications \textbar{} Advantages \textbar{}
+=================+======================================================================+=========================+==============================+==========================================================================+===============================================+
\textbar{} \(M\)-Estimators \textbar{} Generalization of OLS Robust \(\rho\) reduces large residual influence \textbar{} Moderate (up to \(0.29\)) \textbar{} High with proper tuning \textbar{} Wide applicability in regression with moderate robustness \textbar{} Balances robustness and efficiency \textbar{}
\textbar{} \textbar{} \textbar{} \textbar{} \textbar{} \textbar{} \textbar{}
\textbar{} \textbar{} \textbar{} \textbar{} \textbar{} \textbar{} Flexible tuning via \(\rho\)-function \textbar{}
+-----------------+----------------------------------------------------------------------+-------------------------+------------------------------+--------------------------------------------------------------------------+-----------------------------------------------+
\textbar{} \(R\)-Estimators \textbar{} Rank-based method \textbar{} High (depends on ranks) \textbar{} Moderate \textbar{} Ordinal data or heavily skewed distributions \textbar{} Handles both predictor and response outliers \textbar{}
\textbar{} \textbar{} \textbar{} \textbar{} \textbar{} \textbar{} \textbar{}
\textbar{} \textbar{} Immune to outliers in \(x\) and \(y\) \textbar{} \textbar{} \textbar{} \textbar{} Suitable for ordinal or rank-based data \textbar{}
+-----------------+----------------------------------------------------------------------+-------------------------+------------------------------+--------------------------------------------------------------------------+-----------------------------------------------+
\textbar{} \(L\)-Estimators \textbar{} Linear combination of order statistics \textbar{} High (up to \(50\%\)) \textbar{} Moderate \textbar{} Descriptive statistics, robust averages \textbar{} Simple and intuitive \textbar{}
\textbar{} \textbar{} \textbar{} \textbar{} \textbar{} \textbar{} \textbar{}
\textbar{} \textbar{} \textbar{} \textbar{} \textbar{} \textbar{} Easy to compute, even for large datasets \textbar{}
+-----------------+----------------------------------------------------------------------+-------------------------+------------------------------+--------------------------------------------------------------------------+-----------------------------------------------+
\textbar{} \textbf{LTS} \textbar{} Minimizes smallest \(h\) squared residuals \textbar{} High (up to \(50\%\)) \textbar{} Moderate \textbar{} Data with high contamination, fault detection \textbar{} High robustness to outliers \textbar{}
\textbar{} \textbar{} \textbar{} \textbar{} \textbar{} \textbar{} \textbar{}
\textbar{} \textbar{} \textbar{} \textbar{} \textbar{} \textbar{} Resistant to leverage points \textbar{}
+-----------------+----------------------------------------------------------------------+-------------------------+------------------------------+--------------------------------------------------------------------------+-----------------------------------------------+
\textbar{} \(S\)-Estimators \textbar{} Minimizes robust scale of residuals \textbar{} High (up to \(50\%\)) \textbar{} Low to moderate \textbar{} Outlier detection, data with heavy-tailed distributions \textbar{} Focus on robust scale estimation \textbar{}
\textbar{} \textbar{} \textbar{} \textbar{} \textbar{} \textbar{} \textbar{}
\textbar{} \textbar{} \textbar{} \textbar{} \textbar{} \textbar{} Effective at detecting extreme outliers \textbar{}
+-----------------+----------------------------------------------------------------------+-------------------------+------------------------------+--------------------------------------------------------------------------+-----------------------------------------------+
\textbar{} \(MM\)-Estimators \textbar{} High robustness (scale) + high efficiency (coefficients) \textbar{} High (up to \(50\%\)) \textbar{} High \textbar{} Real-world applications with mixed contamination and heavy-tailed errors \textbar{} Combinesrobustness and efficiency effectively \textbar{}
\textbar{} \textbar{} \textbar{} \textbar{} \textbar{} \textbar{} \textbar{}
\textbar{} \textbar{} \textbar{} \textbar{} \textbar{} \textbar{} Versatile and flexible \textbar{}
+-----------------+----------------------------------------------------------------------+-------------------------+------------------------------+--------------------------------------------------------------------------+-----------------------------------------------+

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Notes on Choosing an Estimator

\begin{itemize}
\tightlist
\item
  \(M\)-Estimators: Best suited for general-purpose robust regression, offering a balance between robustness and efficiency with moderate contamination.
\item
  \(R\)-Estimators: Ideal for rank-based data or ordinal data, especially when outliers are present in both predictors and responses.
\item
  \(L\)-Estimators: Simple and effective for descriptive statistics or data cleaning with limited computational resources.
\item
  \textbf{LTS}: Recommended for datasets with significant contamination or leverage points due to its high breakdown point.
\item
  \(S\)-Estimators: Focus on robust scale estimation, suitable for identifying and mitigating the influence of extreme residuals.
\item
  \(MM\)-Estimators: Combines the robustness of \(S\)-estimators with the efficiency of \(M\)-estimators, making it the most versatile choice for heavily contaminated data.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load necessary libraries}
\FunctionTok{library}\NormalTok{(MASS)       }\CommentTok{\# For robust regression functions like rlm}
\FunctionTok{library}\NormalTok{(robustbase) }\CommentTok{\# For LTS regression and MM{-}estimators}
\FunctionTok{library}\NormalTok{(dplyr)      }\CommentTok{\# For data manipulation}
\FunctionTok{library}\NormalTok{(ggplot2)    }\CommentTok{\# For visualization}

\CommentTok{\# Simulate dataset}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{100}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =} \DecValTok{5}\NormalTok{, }\AttributeTok{sd =} \DecValTok{2}\NormalTok{)   }\CommentTok{\# Predictor}
\NormalTok{y }\OtherTok{\textless{}{-}} \DecValTok{3} \SpecialCharTok{+} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ x }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{) }\CommentTok{\# Response}

\CommentTok{\# Introduce outliers}
\NormalTok{y[}\DecValTok{95}\SpecialCharTok{:}\DecValTok{100}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ y[}\DecValTok{95}\SpecialCharTok{:}\DecValTok{100}\NormalTok{] }\SpecialCharTok{+} \DecValTok{20}  \CommentTok{\# Vertical outliers}
\NormalTok{x[}\DecValTok{90}\SpecialCharTok{:}\DecValTok{95}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ x[}\DecValTok{90}\SpecialCharTok{:}\DecValTok{95}\NormalTok{] }\SpecialCharTok{+} \DecValTok{10}    \CommentTok{\# Leverage points}

\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(x, y)}

\CommentTok{\# Visualize the data}
\FunctionTok{ggplot}\NormalTok{(data, }\FunctionTok{aes}\NormalTok{(x, y)) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Scatterplot of Simulated Data with Outliers"}\NormalTok{,}
         \AttributeTok{x =} \StringTok{"Predictor (x)"}\NormalTok{,}
         \AttributeTok{y =} \StringTok{"Response (y)"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{05-linear-regression_files/figure-latex/unnamed-chunk-23-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Ordinary Least Squares}
\NormalTok{ols\_model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{data =}\NormalTok{ data)}
\FunctionTok{summary}\NormalTok{(ols\_model)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = y \textasciitilde{} x, data = data)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}      Min       1Q   Median       3Q      Max }
\CommentTok{\#\textgreater{} {-}12.6023  {-}2.4590  {-}0.5717   0.9247  24.4024 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)   8.8346     1.1550   7.649 1.41e{-}11 ***}
\CommentTok{\#\textgreater{} x             0.9721     0.1749   5.558 2.36e{-}07 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 5.583 on 98 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.2396, Adjusted R{-}squared:  0.2319 }
\CommentTok{\#\textgreater{} F{-}statistic: 30.89 on 1 and 98 DF,  p{-}value: 2.358e{-}07}
\end{Highlighting}
\end{Shaded}

OLS coefficients are highly influenced by the presence of outliers. For example, the slope (x coefficient) and intercept are shifted to fit the outliers, resulting in a poor fit to the majority of the data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# $M${-}Estimators}
\NormalTok{m\_model }\OtherTok{\textless{}{-}} \FunctionTok{rlm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{data =}\NormalTok{ data, }\AttributeTok{psi =}\NormalTok{ psi.huber)}
\FunctionTok{summary}\NormalTok{(m\_model)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call: rlm(formula = y \textasciitilde{} x, data = data, psi = psi.huber)}
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}       Min        1Q    Median        3Q       Max }
\CommentTok{\#\textgreater{} {-}18.43919  {-}0.97575  {-}0.03297   0.76967  21.85546 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}             Value   Std. Error t value}
\CommentTok{\#\textgreater{} (Intercept)  4.3229  0.2764    15.6421}
\CommentTok{\#\textgreater{} x            1.7250  0.0419    41.2186}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 1.349 on 98 degrees of freedom}
\end{Highlighting}
\end{Shaded}

The \(M\)-estimator reduces the influence of large residuals using Huber's psi function. This results in coefficients that are less affected by outliers compared to OLS.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Least Trimmed Squares (LTS)}
\NormalTok{lts\_model }\OtherTok{\textless{}{-}} \FunctionTok{ltsReg}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{data =}\NormalTok{ data)}
\NormalTok{lts\_coefficients }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(lts\_model)}
\end{Highlighting}
\end{Shaded}

LTS minimizes the smallest squared residuals, ignoring extreme residuals. This results in a more robust fit, particularly in the presence of both vertical outliers and leverage points.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# $MM${-}Estimators}
\NormalTok{mm\_model }\OtherTok{\textless{}{-}} \FunctionTok{lmrob}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{data =}\NormalTok{ data, }\AttributeTok{setting =} \StringTok{"KS2014"}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(mm\_model)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lmrob(formula = y \textasciitilde{} x, data = data, setting = "KS2014")}
\CommentTok{\#\textgreater{}  \textbackslash{}{-}{-}\textgreater{} method = "SMDM"}
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}       Min        1Q    Median        3Q       Max }
\CommentTok{\#\textgreater{} {-}20.45989  {-}0.69436  {-}0.01455   0.73614  22.10173 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)  3.02192    0.25850   11.69   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} x            1.96672    0.04538   43.34   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Robust residual standard error: 0.9458 }
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.9562, Adjusted R{-}squared:  0.9558 }
\CommentTok{\#\textgreater{} Convergence in 7 IRWLS iterations}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Robustness weights: }
\CommentTok{\#\textgreater{}  10 observations c(90,91,92,93,94,96,97,98,99,100)}
\CommentTok{\#\textgreater{}   are outliers with |weight| = 0 ( \textless{} 0.001); }
\CommentTok{\#\textgreater{}  67 weights are \textasciitilde{}= 1. The remaining 23 ones are summarized as}
\CommentTok{\#\textgreater{}    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. }
\CommentTok{\#\textgreater{}  0.2496  0.7969  0.9216  0.8428  0.9548  0.9943 }
\CommentTok{\#\textgreater{} Algorithmic parameters: }
\CommentTok{\#\textgreater{}       tuning.chi1       tuning.chi2       tuning.chi3       tuning.chi4 }
\CommentTok{\#\textgreater{}        {-}5.000e{-}01         1.500e+00                NA         5.000e{-}01 }
\CommentTok{\#\textgreater{}                bb       tuning.psi1       tuning.psi2       tuning.psi3 }
\CommentTok{\#\textgreater{}         5.000e{-}01        {-}5.000e{-}01         1.500e+00         9.500e{-}01 }
\CommentTok{\#\textgreater{}       tuning.psi4        refine.tol           rel.tol         scale.tol }
\CommentTok{\#\textgreater{}                NA         1.000e{-}07         1.000e{-}07         1.000e{-}10 }
\CommentTok{\#\textgreater{}         solve.tol          zero.tol       eps.outlier             eps.x }
\CommentTok{\#\textgreater{}         1.000e{-}07         1.000e{-}10         1.000e{-}03         3.223e{-}11 }
\CommentTok{\#\textgreater{} warn.limit.reject warn.limit.meanrw }
\CommentTok{\#\textgreater{}         5.000e{-}01         5.000e{-}01 }
\CommentTok{\#\textgreater{}      nResample         max.it       best.r.s       k.fast.s          k.max }
\CommentTok{\#\textgreater{}           1000            500             20              2           2000 }
\CommentTok{\#\textgreater{}    maxit.scale      trace.lev            mts     compute.rd      numpoints }
\CommentTok{\#\textgreater{}            200              0           1000              0             10 }
\CommentTok{\#\textgreater{} fast.s.large.n }
\CommentTok{\#\textgreater{}           2000 }
\CommentTok{\#\textgreater{}               setting                   psi           subsampling }
\CommentTok{\#\textgreater{}              "KS2014"                 "lqq"         "nonsingular" }
\CommentTok{\#\textgreater{}                   cov compute.outlier.stats }
\CommentTok{\#\textgreater{}             ".vcov.w"                "SMDM" }
\CommentTok{\#\textgreater{} seed : int(0)}
\end{Highlighting}
\end{Shaded}

\(MM\)-estimators combine robust scale estimation (from \(S\)-estimators) with efficient coefficient estimation (from \(M\)-estimators).
This achieves both high robustness and high efficiency under normal conditions.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Visualizing results}
\NormalTok{data }\OtherTok{\textless{}{-}}\NormalTok{ data }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}
        \AttributeTok{ols\_fit =} \FunctionTok{predict}\NormalTok{(ols\_model, }\AttributeTok{newdata =}\NormalTok{ data),}
        \AttributeTok{m\_fit =} \FunctionTok{predict}\NormalTok{(m\_model, }\AttributeTok{newdata =}\NormalTok{ data),}
        \AttributeTok{lts\_fit =} \FunctionTok{fitted}\NormalTok{(lts\_model),}
        \CommentTok{\# Use \textasciigrave{}fitted()\textasciigrave{} for ltsReg objects}
        \AttributeTok{mm\_fit =} \FunctionTok{predict}\NormalTok{(mm\_model, }\AttributeTok{newdata =}\NormalTok{ data)}
\NormalTok{    )}

\FunctionTok{ggplot}\NormalTok{(data, }\FunctionTok{aes}\NormalTok{(x, y)) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{geom\_line}\NormalTok{(}
        \FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ ols\_fit),}
        \AttributeTok{color =} \StringTok{"red"}\NormalTok{,}
        \AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{,}
        \AttributeTok{size =} \DecValTok{1}\NormalTok{,}
        \AttributeTok{label =} \StringTok{"OLS"}
\NormalTok{    ) }\SpecialCharTok{+}
    \FunctionTok{geom\_line}\NormalTok{(}
        \FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ m\_fit),}
        \AttributeTok{color =} \StringTok{"blue"}\NormalTok{,}
        \AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{,}
        \AttributeTok{size =} \DecValTok{1}\NormalTok{,}
        \AttributeTok{label =} \StringTok{"$M${-}Estimator"}
\NormalTok{    ) }\SpecialCharTok{+}
    \FunctionTok{geom\_line}\NormalTok{(}
        \FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ lts\_fit),}
        \AttributeTok{color =} \StringTok{"green"}\NormalTok{,}
        \AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{,}
        \AttributeTok{size =} \DecValTok{1}\NormalTok{,}
        \AttributeTok{label =} \StringTok{"LTS"}
\NormalTok{    ) }\SpecialCharTok{+}
    \FunctionTok{geom\_line}\NormalTok{(}
        \FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ mm\_fit),}
        \AttributeTok{color =} \StringTok{"purple"}\NormalTok{,}
        \AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{,}
        \AttributeTok{size =} \DecValTok{1}\NormalTok{,}
        \AttributeTok{label =} \StringTok{"$MM${-}Estimator"}
\NormalTok{    ) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Comparison of Regression Fits"}\NormalTok{,}
         \AttributeTok{x =} \StringTok{"Predictor (x)"}\NormalTok{,}
         \AttributeTok{y =} \StringTok{"Response (y)"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{05-linear-regression_files/figure-latex/unnamed-chunk-27-1} \end{center}

Visualization shows the differences in regression fits:
- OLS is heavily influenced by outliers and provides a poor fit to the majority of the data.
)
- The \(M\)-estimator downweights large residuals, resulting in a better fit.
- LTS regression ignores the extreme residuals entirely, providing the most robust fit.
- \(MM\)-estimators balance robustness and efficiency, producing coefficients close to the LTS but with improved efficiency under normality.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Comparing Coefficients}
\NormalTok{comparison }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
    \AttributeTok{Method =} \FunctionTok{c}\NormalTok{(}\StringTok{"OLS"}\NormalTok{, }\StringTok{"$M${-}Estimator"}\NormalTok{, }\StringTok{"LTS"}\NormalTok{, }\StringTok{"$MM${-}Estimator"}\NormalTok{),}
    \AttributeTok{Intercept =} \FunctionTok{c}\NormalTok{(}
        \FunctionTok{coef}\NormalTok{(ols\_model)[}\DecValTok{1}\NormalTok{],}
        \FunctionTok{coef}\NormalTok{(m\_model)[}\DecValTok{1}\NormalTok{],}
\NormalTok{        lts\_coefficients[}\DecValTok{1}\NormalTok{],}
        \FunctionTok{coef}\NormalTok{(mm\_model)[}\DecValTok{1}\NormalTok{]}
\NormalTok{    ),}
    \AttributeTok{Slope =} \FunctionTok{c}\NormalTok{(}
        \FunctionTok{coef}\NormalTok{(ols\_model)[}\DecValTok{2}\NormalTok{],}
        \FunctionTok{coef}\NormalTok{(m\_model)[}\DecValTok{2}\NormalTok{],}
\NormalTok{        lts\_coefficients[}\DecValTok{2}\NormalTok{],}
        \FunctionTok{coef}\NormalTok{(mm\_model)[}\DecValTok{2}\NormalTok{]}
\NormalTok{    )}
\NormalTok{)}

\FunctionTok{print}\NormalTok{(comparison)}
\CommentTok{\#\textgreater{}           Method Intercept     Slope}
\CommentTok{\#\textgreater{} 1            OLS  8.834553 0.9720994}
\CommentTok{\#\textgreater{} 2  $M${-}Estimator  4.322869 1.7250441}
\CommentTok{\#\textgreater{} 3            LTS  2.954960 1.9777635}
\CommentTok{\#\textgreater{} 4 $MM${-}Estimator  3.021923 1.9667208}
\end{Highlighting}
\end{Shaded}

The table above shows how the coefficients vary across methods:
- OLS coefficients are the most distorted by outliers.
- \(M\)-estimators and \(MM\)-estimators provide coefficients that are less influenced by extreme values.
- LTS regression, with its trimming mechanism, produces the most robust coefficients by excluding the largest residuals.

\hypertarget{partial-least-squares}{%
\section{Partial Least Squares}\label{partial-least-squares}}

Partial Least Squares (PLS) is a dimensionality reduction technique used for regression and predictive modeling. It is particularly useful when predictors are highly collinear or when the number of predictors (\(p\)) exceeds the number of observations (\(n\)). Unlike methods such as Principal Component Regression (PCR), PLS simultaneously considers the relationship between predictors and the response variable.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{motivation-for-pls}{%
\subsection{Motivation for PLS}\label{motivation-for-pls}}

Limitations of Classical Methods

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Multicollinearity}:

  \begin{itemize}
  \tightlist
  \item
    OLS fails when predictors are highly correlated because the design matrix \(X'X\) becomes nearly singular, leading to unstable estimates.
  \end{itemize}
\item
  \textbf{High-Dimensional Data}:

  \begin{itemize}
  \tightlist
  \item
    When \(p > n\), OLS cannot be directly applied as \(X'X\) is not invertible.
  \end{itemize}
\item
  \textbf{Principal Component Regression (PCR)}:

  \begin{itemize}
  \tightlist
  \item
    While PCR addresses multicollinearity by using principal components of \(X\), it does not account for the relationship between predictors and the response variable \(y\) when constructing components.
  \end{itemize}
\end{enumerate}

PLS overcomes these limitations by constructing components that maximize the covariance between predictors \(X\) and the response \(y\). It finds a compromise between explaining the variance in \(X\) and predicting \(y\), making it particularly suited for regression in high-dimensional or collinear datasets.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Let:

\begin{itemize}
\item
  \(X\) be the \(n \times p\) matrix of predictors,
\item
  \(y\) be the \(n \times 1\) response vector,
\item
  \(t_k\) be the \(k\)-th latent component derived from \(X\),
\item
  \(p_k\) and \(q_k\) be the loadings for \(X\) and \(y\), respectively.
\end{itemize}

PLS aims to construct latent components \(t_1, t_2, \ldots, t_K\) such that:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Each \(t_k\) is a linear combination of the predictors: \(t_k = X w_k\), where \(w_k\) is a weight vector. 2
\item
  The covariance between \(t_k\) and \(y\) is maximized: \[
     \text{Maximize } Cov(t_k, y) = w_k' X' y.
     \]
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{steps-to-construct-pls-components}{%
\subsection{Steps to Construct PLS Components}\label{steps-to-construct-pls-components}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Compute Weights}:

  \begin{itemize}
  \tightlist
  \item
    The weights \(w_k\) for the \(k\)-th component are obtained by solving: \[
    w_k = \frac{X'y}{\|X'y\|}.
    \]
  \end{itemize}
\item
  \textbf{Construct Latent Component}:

  \begin{itemize}
  \tightlist
  \item
    Form the \(k\)-th latent component: \[
    t_k = X w_k.
    \]
  \end{itemize}
\item
  \textbf{Deflate the Predictors}:

  \begin{itemize}
  \tightlist
  \item
    After extracting \(t_k\), the predictors are deflated to remove the information explained by \(t_k\): \[
    X \leftarrow X - t_k p_k',
    \] where \(p_k = \frac{X't_k}{t_k't_k}\) are the loadings for \(X\).
  \end{itemize}
\item
  \textbf{Deflate the Response}:

  \begin{itemize}
  \tightlist
  \item
    Similarly, deflate \(y\) to remove the variance explained by \(t_k\): \[
    y \leftarrow y - t_k q_k,
    \] where \(q_k = \frac{t_k'y}{t_k't_k}\).
  \end{itemize}
\item
  \textbf{Repeat for All Components}:

  \begin{itemize}
  \tightlist
  \item
    Repeat the steps above until \(K\) components are extracted.
  \end{itemize}
\end{enumerate}

After constructing \(K\) components, the response \(y\) is modeled as:

\[
y = T C + \epsilon,
\]

where:

\begin{itemize}
\item
  \(T = [t_1, t_2, \ldots, t_K]\) is the matrix of latent components,
\item
  \(C\) is the vector of regression coefficients.
\end{itemize}

The estimated coefficients for the original predictors are then:

\[
\hat{\beta} = W (P' W)^{-1} C,
\]

where \(W = [w_1, w_2, \ldots, w_K]\) and \(P = [p_1, p_2, \ldots, p_K]\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{properties-of-pls}{%
\subsection{Properties of PLS}\label{properties-of-pls}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Dimensionality Reduction}:

  \begin{itemize}
  \tightlist
  \item
    PLS reduces \(X\) to \(K\) components, where \(K \leq \min(n, p)\).
  \end{itemize}
\item
  \textbf{Handles Multicollinearity}:

  \begin{itemize}
  \tightlist
  \item
    By constructing uncorrelated components, PLS avoids the instability caused by multicollinearity in OLS.
  \end{itemize}
\item
  \textbf{Supervised Dimensionality Reduction}:

  \begin{itemize}
  \tightlist
  \item
    Unlike PCR, PLS considers the relationship between \(X\) and \(y\) when constructing components.
  \end{itemize}
\item
  \textbf{Efficiency}:

  \begin{itemize}
  \tightlist
  \item
    PLS requires fewer components than PCR to achieve a similar level of predictive accuracy.
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Practical Considerations

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Number of Components}:

  \begin{itemize}
  \tightlist
  \item
    The optimal number of components \(K\) can be determined using cross-validation.
  \end{itemize}
\item
  \textbf{Preprocessing}:

  \begin{itemize}
  \tightlist
  \item
    Standardizing predictors is essential for PLS, as it ensures that all variables are on the same scale.
  \end{itemize}
\item
  \textbf{Comparison with Other Methods}:

  \begin{itemize}
  \tightlist
  \item
    PLS outperforms OLS and PCR in cases of multicollinearity or when \(p > n\), but it may be less interpretable than sparse methods like Lasso.
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load required library}
\FunctionTok{library}\NormalTok{(pls)}

\CommentTok{\# Step 1: Simulate data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)  }\CommentTok{\# Ensure reproducibility}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{100}       \CommentTok{\# Number of observations}
\NormalTok{p }\OtherTok{\textless{}{-}} \DecValTok{10}        \CommentTok{\# Number of predictors}
\NormalTok{X }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(n }\SpecialCharTok{*}\NormalTok{ p), }\AttributeTok{nrow =}\NormalTok{ n, }\AttributeTok{ncol =}\NormalTok{ p)  }\CommentTok{\# Design matrix (predictors)}
\NormalTok{beta }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(p)                               }\CommentTok{\# True coefficients}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ X }\SpecialCharTok{\%*\%}\NormalTok{ beta }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)                     }\CommentTok{\# Response variable with noise}

\CommentTok{\# Step 2: Fit Partial Least Squares (PLS) Regression}
\NormalTok{pls\_fit }\OtherTok{\textless{}{-}} \FunctionTok{plsr}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X, }\AttributeTok{ncomp =} \DecValTok{5}\NormalTok{, }\AttributeTok{validation =} \StringTok{"CV"}\NormalTok{)}

\CommentTok{\# Step 3: Summarize the PLS Model}
\FunctionTok{summary}\NormalTok{(pls\_fit)}
\CommentTok{\#\textgreater{} Data:    X dimension: 100 10 }
\CommentTok{\#\textgreater{}  Y dimension: 100 1}
\CommentTok{\#\textgreater{} Fit method: kernelpls}
\CommentTok{\#\textgreater{} Number of components considered: 5}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} VALIDATION: RMSEP}
\CommentTok{\#\textgreater{} Cross{-}validated using 10 random segments.}
\CommentTok{\#\textgreater{}        (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps}
\CommentTok{\#\textgreater{} CV           1.339    1.123    1.086    1.090    1.088    1.087}
\CommentTok{\#\textgreater{} adjCV        1.339    1.112    1.078    1.082    1.080    1.080}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} TRAINING: \% variance explained}
\CommentTok{\#\textgreater{}    1 comps  2 comps  3 comps  4 comps  5 comps}
\CommentTok{\#\textgreater{} X    10.88    20.06    30.80    42.19    51.61}
\CommentTok{\#\textgreater{} y    44.80    48.44    48.76    48.78    48.78}

\CommentTok{\# Step 4: Perform Cross{-}Validation and Select Optimal Components}
\FunctionTok{validationplot}\NormalTok{(pls\_fit, }\AttributeTok{val.type =} \StringTok{"MSEP"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{05-linear-regression_files/figure-latex/unnamed-chunk-29-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Step 5: Extract Coefficients for Predictors}
\NormalTok{pls\_coefficients }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(pls\_fit)}
\FunctionTok{print}\NormalTok{(pls\_coefficients)}
\CommentTok{\#\textgreater{} , , 5 comps}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}               y}
\CommentTok{\#\textgreater{} X1   0.30192935}
\CommentTok{\#\textgreater{} X2  {-}0.03161151}
\CommentTok{\#\textgreater{} X3   0.22392538}
\CommentTok{\#\textgreater{} X4   0.42315637}
\CommentTok{\#\textgreater{} X5   0.33000198}
\CommentTok{\#\textgreater{} X6   0.66228763}
\CommentTok{\#\textgreater{} X7   0.40452691}
\CommentTok{\#\textgreater{} X8  {-}0.05704037}
\CommentTok{\#\textgreater{} X9  {-}0.02699757}
\CommentTok{\#\textgreater{} X10  0.05944765}

\CommentTok{\# Step 6: Evaluate Model Performance}
\NormalTok{predicted\_y }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(pls\_fit, X)}
\NormalTok{actual\_vs\_predicted }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{Actual =}\NormalTok{ y,}
  \AttributeTok{Predicted =}\NormalTok{ predicted\_y[, , }\DecValTok{5}\NormalTok{]  }\CommentTok{\# Predicted values using 5 components}
\NormalTok{)}

\CommentTok{\# Plot Actual vs Predicted}
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{ggplot}\NormalTok{(actual\_vs\_predicted, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Actual, }\AttributeTok{y =}\NormalTok{ Predicted)) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{geom\_abline}\NormalTok{(}
        \AttributeTok{intercept =} \DecValTok{0}\NormalTok{,}
        \AttributeTok{slope =} \DecValTok{1}\NormalTok{,}
        \AttributeTok{color =} \StringTok{"red"}\NormalTok{,}
        \AttributeTok{linetype =} \StringTok{"dashed"}
\NormalTok{    ) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Actual vs Predicted Values (PLS Regression)"}\NormalTok{,}
         \AttributeTok{x =} \StringTok{"Actual Values"}\NormalTok{,}
         \AttributeTok{y =} \StringTok{"Predicted Values"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{05-linear-regression_files/figure-latex/unnamed-chunk-29-2} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Step 7: Extract and Interpret Variable Importance (Loadings)}
\NormalTok{loadings\_matrix }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{unclass}\NormalTok{(}\FunctionTok{loadings}\NormalTok{(pls\_fit)))}
\NormalTok{variable\_importance }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(loadings\_matrix)}
\FunctionTok{colnames}\NormalTok{(variable\_importance) }\OtherTok{\textless{}{-}}
    \FunctionTok{paste0}\NormalTok{(}\StringTok{"Component\_"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\FunctionTok{ncol}\NormalTok{(variable\_importance))}
\FunctionTok{rownames}\NormalTok{(variable\_importance) }\OtherTok{\textless{}{-}}
    \FunctionTok{paste0}\NormalTok{(}\StringTok{"X"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(variable\_importance))}

\CommentTok{\# Print variable importance}
\FunctionTok{print}\NormalTok{(variable\_importance)}
\CommentTok{\#\textgreater{}     Component\_1 Component\_2 Component\_3 Component\_4 Component\_5}
\CommentTok{\#\textgreater{} X1  {-}0.04991097   0.5774569  0.24349681 {-}0.41550345 {-}0.02098351}
\CommentTok{\#\textgreater{} X2   0.08913192  {-}0.1139342 {-}0.17582957 {-}0.05709948 {-}0.06707863}
\CommentTok{\#\textgreater{} X3   0.13773357   0.1633338  0.07622919 {-}0.07248620 {-}0.61962875}
\CommentTok{\#\textgreater{} X4   0.40369572  {-}0.2730457  0.69994206 {-}0.07949013  0.35239113}
\CommentTok{\#\textgreater{} X5   0.50562681  {-}0.1788131 {-}0.27936562  0.36197480 {-}0.41919645}
\CommentTok{\#\textgreater{} X6   0.57044281   0.3358522 {-}0.38683260  0.17656349  0.31154275}
\CommentTok{\#\textgreater{} X7   0.36258623   0.1202109 {-}0.01753715 {-}0.12980483 {-}0.06919411}
\CommentTok{\#\textgreater{} X8   0.12975452  {-}0.1164935 {-}0.30479310 {-}0.65654861  0.49948167}
\CommentTok{\#\textgreater{} X9  {-}0.29521786   0.6170234 {-}0.32082508 {-}0.01041860  0.04904396}
\CommentTok{\#\textgreater{} X10  0.23930055  {-}0.3259554  0.20006888 {-}0.53547258 {-}0.17963372}
\end{Highlighting}
\end{Shaded}

The loadings provide the contribution of each predictor to the PLS components. Higher absolute values indicate stronger contributions to the corresponding component.

\begin{itemize}
\item
  \textbf{Summary of the Model}:

  \begin{itemize}
  \item
    The \textbf{proportion of variance explained} indicates how much of the variability in both the predictors and response is captured by each PLS component.
  \item
    The goal is to retain enough components to explain most of the variance while avoiding overfitting.
  \end{itemize}
\item
  \textbf{Validation Plot}:

  \begin{itemize}
  \item
    The \textbf{Mean Squared Error of Prediction (MSEP)} curve is used to select the optimal number of components.
  \item
    Adding too many components can lead to overfitting, while too few may underfit the data.
  \end{itemize}
\item
  \textbf{Coefficients}:

  \begin{itemize}
  \item
    The extracted coefficients are the weights applied to the predictors in the final PLS model.
  \item
    These coefficients are derived from the PLS components and may differ from OLS regression coefficients due to dimensionality reduction.
  \end{itemize}
\item
  \textbf{Actual vs Predicted Plot}:

  \begin{itemize}
  \item
    This visualization evaluates how well the PLS model predicts the response variable.
  \item
    Points tightly clustered around the diagonal indicate good performance.
  \end{itemize}
\item
  \textbf{VIP Scores}:

  \begin{itemize}
  \item
    VIP scores help identify the most important predictors in the PLS model.
  \item
    Predictors with higher VIP scores contribute more to explaining the response variable.
  \end{itemize}
\end{itemize}

\hypertarget{comparison-with-related-methods}{%
\subsection{Comparison with Related Methods}\label{comparison-with-related-methods}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1348}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2270}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2979}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1560}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1631}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Method}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Handles Multicollinearity}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Supervised Dimensionality Reduction}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Sparse Solution}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Interpretability}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
OLS & No & No & No & High \\
Ridge Regression & Yes & No & No & Moderate \\
Lasso Regression & Yes & No & Yes & High \\
PCR & Yes & No & No & Low \\
PLS & Yes & Yes & No & Moderate \\
\end{longtable}

\hypertarget{non-linear-regression}{%
\chapter{Non-Linear Regression}\label{non-linear-regression}}

Non-linear regression models differ fundamentally from linear regression models in that the derivatives of the mean function with respect to parameters depend on one or more of the parameters. This dependence adds complexity but also provides greater flexibility to model intricate relationships.

\textbf{Linear Regression:}

\begin{itemize}
\item
  \textbf{Model Form Example:} A typical linear regression model looks like \(y = \beta_0 + \beta_1 x\), where \(\beta_0\) and \(\beta_1\) are the parameters.
\item
  \textbf{Parameter Effect:} The influence of each parameter on \(y\) is constant. For example, if \(\beta_1\) increases by 1, the change in \(y\) is always \(x\), regardless of the current value of \(\beta_1\).
\item
  \textbf{Derivatives:} The partial derivatives of \(y\) with respect to each parameter (e.g., \(\frac{\partial y}{\partial \beta_1} = x\)) do not depend on the parameters \(\beta_0\) or \(\beta_1\) themselves---they only depend on the data \(x\). This makes the mathematics of finding the best-fit line straightforward.
\item
  Straightforward estimation via closed-form solutions like \protect\hyperlink{ordinary-least-squares}{Ordinary Least Squares}.
\end{itemize}

\textbf{Non-linear Regression:}

\begin{itemize}
\item
  \textbf{Model Form Example:} Consider \(y = \alpha \cdot e^{\beta x}\). Here, \(\alpha\) and \(\beta\) are parameters, but the relationship is not a straight line.
\item
  \textbf{Parameter Effect:} The effect of changing \(\alpha\) or \(\beta\) on \(y\) is not constant. For instance, if you change \(\beta\), the impact on \(y\) depends on both \(x\) and the current value of \(\beta\). This makes predictions and adjustments more complex.
\item
  \textbf{Derivatives:} Taking the partial derivative with respect to \(\beta\) gives \(\frac{\partial y}{\partial \beta} = \alpha x e^{\beta x}\). Notice this derivative depends on \(\alpha\), \(\beta\), and \(x\). Unlike linear regression, the sensitivity of \(y\) to changes in \(\beta\) changes as \(\beta\) itself changes.
\item
  Estimation requires iterative algorithms like the \protect\hyperlink{gauss-newton-algorithm-1}{Gauss-Newton Algorithm}, as closed-form solutions are not feasible.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2899}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3768}}@{}}
\caption{Summary Table: Linear vs.~Non-Linear Regression}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Feature
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Linear Regression
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Non-Linear Regression
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Feature
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Linear Regression
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Non-Linear Regression
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Relationship & Linear in parameters & Non-linear in parameters \\
Interpretability & High & Often challenging \\
Estimation & Closed-form solutions & Iterative algorithms \\
Computational Cost & Low & Higher \\
\end{longtable}

\textbf{Key Features of Non-linear regression:}

\begin{itemize}
\tightlist
\item
  \textbf{Complex Functional Forms}: Non-linear regression allows for relationships that are not straight lines or planes.
\item
  \textbf{Interpretability Challenges}: Non-linear models can be difficult to interpret, especially if the functional forms are complex.
\item
  \textbf{Practical Use Cases}:

  \begin{itemize}
  \tightlist
  \item
    Growth curves
  \item
    High-order polynomials
  \item
    Linear approximations (e.g., Taylor expansions)
  \item
    Collections of locally linear models or basis functions (e.g., splines)
  \end{itemize}
\end{itemize}

While these approaches can approximate data, they may suffer from interpretability issues or may not generalize well when data is sparse. Hence, intrinsically non-linear models are often preferred.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Intrinsically Non-Linear Models}

The general form of an intrinsically non-linear regression model is:

\[
Y_i = f(\mathbf{x}_i; \mathbf{\theta}) + \epsilon_i
\]

Where:

\begin{itemize}
\tightlist
\item
  \(f(\mathbf{x}_i; \mathbf{\theta})\): A \textbf{non-linear function} that relates \(E(Y_i)\) to the independent variables \(\mathbf{x}_i\).
\item
  \(\mathbf{x}_i\): A \(k \times 1\) vector of independent variables (fixed).
\item
  \(\mathbf{\theta}\): A \(p \times 1\) vector of parameters.
\item
  \(\epsilon_i\): Independent and identically distributed random errors, often assumed to have a mean of 0 and a constant variance \(\sigma^2\). In some cases, \(\epsilon_i \sim \mathcal{N}(0, \sigma^2)\).
\end{itemize}

Example: Exponential Growth Model

A common non-linear model is the exponential growth function:

\[
y = \theta_1 e^{\theta_2 x} + \epsilon
\]

Where:

\begin{itemize}
\tightlist
\item
  \(\theta_1\): Initial value.
\item
  \(\theta_2\): Growth rate.
\item
  \(x\): Independent variable (e.g., time).
\item
  \(\epsilon\): Random error.
\end{itemize}

\hypertarget{inference}{%
\section{Inference}\label{inference}}

Since \(Y_i = f(\mathbf{x}_i, \theta) + \epsilon_i\), where \(\epsilon_i \sim \text{iid}(0, \sigma^2)\), we can estimate parameters (\(\hat{\theta}\)) by minimizing the sum of squared errors:

\[
\sum_{i=1}^{n} \big(Y_i - f(\mathbf{x}_i, \theta)\big)^2
\]

Let \(\hat{\theta}\) be the minimizer, the variance of residuals is estimated as:

\[
s^2 = \hat{\sigma}^2_{\epsilon} = \frac{\sum_{i=1}^{n} \big(Y_i - f(\mathbf{x}_i, \hat{\theta})\big)^2}{n - p}
\]

where \(p\) is the number of parameters in \(\mathbf{\theta}\), and \(n\) is the number of observations.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Asymptotic Distribution} of \(\hat{\theta}\)

Under regularity conditions---most notably that \(\epsilon_i \sim N(0, \sigma^2)\) or that \(n\) is sufficiently large for a central-limit-type argument---the parameter estimates \(\hat{\theta}\) have the following asymptotic normal distribution:

\[
\hat{\theta} \sim AN(\mathbf{\theta}, \sigma^2[\mathbf{F}(\theta)'\mathbf{F}(\theta)]^{-1})
\]

where

\begin{itemize}
\tightlist
\item
  \(AN\) stands for ``asymptotic normality.''
\item
  \(\mathbf{F}(\theta)\) is the \(n \times p\) Jacobian matrix of partial derivatives of \(f(\mathbf{x}_i, \theta)\) with respect to \(\mathbf{\theta}\), evaluated at \(\hat{\theta}\). Specifically,
\end{itemize}

\[
\mathbf{F}(\theta) = \begin{pmatrix}
\frac{\partial f(\mathbf{x}_1, \boldsymbol{\theta})}{\partial \theta_1} & \cdots & \frac{\partial f(\mathbf{x}_1, \boldsymbol{\theta})}{\partial \theta_p} \\
\vdots & \ddots & \vdots \\
\frac{\partial f(\mathbf{x}_n, \boldsymbol{\theta})}{\partial \theta_1} & \cdots & \frac{\partial f(\mathbf{x}_n, \boldsymbol{\theta})}{\partial \theta_p}
\end{pmatrix}
\]

Asymptotic normality means that as the sample size \(n\) becomes large, the sampling distribution of \(\hat{\theta}\) approaches a normal distribution, which enables inference on the parameters.

\hypertarget{linear-functions-of-the-parameters}{%
\subsection{Linear Functions of the Parameters}\label{linear-functions-of-the-parameters}}

A ``linear function of the parameters'' refers to a quantity that can be written as \(\mathbf{a}'\boldsymbol{\theta}\), where \(\mathbf{a}\) is some (constant) contrast vector. Common examples include:

\begin{itemize}
\item
  A single parameter \(\theta_j\) (using a vector \(\mathbf{a}\) with 1 in the \(j\)-th position and 0 elsewhere).
\item
  Differences, sums, or other contrasts, e.g.~\(\theta_1 - \theta_2\).
\end{itemize}

Suppose we are interested in a linear combination of the parameters, such as \(\theta_1 - \theta_2\). Define the contrast vector \(\mathbf{a}\) as:

\[
\mathbf{a} = (0, 1, -1)'
\]

We then consider inference for \(\mathbf{a'\theta}\) (\(\mathbf{a}\) can be \(p\)-dimensional vector). Using rules for the expectation and variance of a linear combination of a random vector \(\mathbf{Z}\):

\[
\begin{aligned}
E(\mathbf{a'Z}) &= \mathbf{a'}E(\mathbf{Z}) \\
\text{Var}(\mathbf{a'Z}) &= \mathbf{a'} \text{Var}(\mathbf{Z}) \mathbf{a}
\end{aligned}
\]

We have

\[
\begin{aligned}
E(\mathbf{a'\hat{\theta}}) &= \mathbf{a'}E(\hat{\theta}) \approx \mathbf{a}' \theta \\
\text{Var}(\mathbf{a'} \hat{\theta}) &= \mathbf{a'} \text{Var}(\hat{\theta}) \mathbf{a} \approx \sigma^2 \mathbf{a'[\mathbf{F}(\theta)'\mathbf{F}(\theta)]^{-1}a}
\end{aligned}
\]

Hence,

\[
\mathbf{a'\hat{\theta}} \sim AN\big(\mathbf{a'\theta}, \sigma^2 \mathbf{a'[\mathbf{F}(\theta)'\mathbf{F}(\theta)]^{-1}a}\big)
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Confidence Intervals for Linear Contrasts

Since \(\mathbf{a'\hat{\theta}}\) is asymptotically independent of \(s^2\) (up to order \(O1/n\)), a two-sided \(100(1-\alpha)\%\) confidence interval for \(\mathbf{a'\theta}\) is given by:

\[
\mathbf{a'\theta} \pm t_{(1-\alpha/2, n-p)} s \sqrt{\mathbf{a'[\mathbf{F}(\hat{\theta})'\mathbf{F}(\hat{\theta})]^{-1}a}}
\]

where

\begin{itemize}
\tightlist
\item
  \(t_{(1-\alpha/2, n-p)}\) is the critical value of the \(t\)-distribution with \(n - p\) degrees of freedom.
\item
  \(s = \sqrt{\hat{\sigma^2}_\epsilon}\) is the estimated standard deviation of residuals.
\end{itemize}

\textbf{Special Case}: A Single Parameter \(\theta_j\)

If we focus on a single parameter \(\theta_j\), let \(\mathbf{a'} = (0, \dots, 1, \dots, 0)\) (with 1 at the \(j\)-th position). Then, the confidence interval for \(\theta_j\) becomes:

\[
\hat{\theta}_j \pm t_{(1-\alpha/2, n-p)} s \sqrt{\hat{c}^j}
\]

where \(\hat{c}^j\) is the \(j\)-th diagonal element of \([\mathbf{F}(\hat{\theta})'\mathbf{F}(\hat{\theta})]^{-1}\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{nonlinear-functions-of-parameters}{%
\subsection{Nonlinear Functions of Parameters}\label{nonlinear-functions-of-parameters}}

In many cases, we are interested in nonlinear functions of \(\boldsymbol{\theta}\). Let \(h(\boldsymbol{\theta})\) be such a function (e.g., a ratio of parameters, a difference of exponentials, etc.).

When \(h(\theta)\) is a nonlinear function of the parameters, we can use a \textbf{Taylor series expansion} about \(\theta\) to approximate \(h(\hat{\theta})\):

\[
h(\hat{\theta}) \approx h(\theta) + \mathbf{h}' [\hat{\theta} - \theta]
\]

where

\begin{itemize}
\tightlist
\item
  \(\mathbf{h} = \left( \frac{\partial h}{\partial \theta_1}, \frac{\partial h}{\partial \theta_2}, \dots, \frac{\partial h}{\partial \theta_p} \right)'\) is the gradient vector of partial derivatives.
\end{itemize}

\textbf{Key Approximations:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Expectation and Variance of} \(\hat{\theta}\) (using the asymptotic normality of \(\hat{\theta}\): \[
  \begin{aligned}
  E(\hat{\theta}) &\approx \theta, \\
  \text{Var}(\hat{\theta}) &\approx \sigma^2 [\mathbf{F}(\theta)' \mathbf{F}(\theta)]^{-1}.
  \end{aligned}
  \]
\item
  \textbf{Expectation and Variance of} \(h(\hat{\theta})\) (properties of expectation and variance of (approximately) linear transformations): \[
  \begin{aligned}
  E(h(\hat{\theta})) &\approx h(\theta), \\
  \text{Var}(h(\hat{\theta})) &\approx \sigma^2 \mathbf{h}'[\mathbf{F}(\theta)' \mathbf{F}(\theta)]^{-1} \mathbf{h}.
  \end{aligned}\]
\end{enumerate}

Combining these results, we find:

\[
h(\hat{\theta}) \sim AN(h(\theta), \sigma^2 \mathbf{h}' [\mathbf{F}(\theta)' \mathbf{F}(\theta)]^{-1} \mathbf{h}),
\]

where \(AN\) represents asymptotic normality.

\textbf{Confidence Interval for} \(h(\theta)\)\textbf{:}

An approximate \(100(1-\alpha)\%\) confidence interval for \(h(\theta)\) is:

\[
h(\hat{\theta}) \pm t_{(1-\alpha/2, n-p)} s \sqrt{\mathbf{h}'[\mathbf{F}(\theta)' \mathbf{F}(\theta)]^{-1} \mathbf{h}},
\]

where \(\mathbf{h}\) and \(\mathbf{F}(\theta)\) are evaluated at \(\hat{\theta}\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

To compute a \textbf{prediction interval} for a new observation \(Y_0\) at \(x = x_0\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Model Definition}: \[
  Y_0 = f(x_0; \theta) + \epsilon_0, \quad \epsilon_0 \sim N(0, \sigma^2),
  \] with the predicted value: \[
  \hat{Y}_0 = f(x_0, \hat{\theta}).
  \]
\item
  \textbf{Approximation for} \(\hat{Y}_0\): As \(n \to \infty\), \(\hat{\theta} \to \theta\), so we have: \[
  f(x_0, \hat{\theta}) \approx f(x_0, \theta) + \mathbf{f}_0(\theta)' [\hat{\theta} - \theta],
  \] where: \[
  \mathbf{f}_0(\theta) = \left( \frac{\partial f(x_0, \theta)}{\partial \theta_1}, \dots, \frac{\partial f(x_0, \theta)}{\partial \theta_p} \right)'.
  \]
\item
  \textbf{Error Approximation}: \[
  \begin{aligned}Y_0 - \hat{Y}_0 &\approx Y_0  - f(x_0,\theta) - f_0(\theta)'[\hat{\theta}-\theta]  \\&= \epsilon_0 - f_0(\theta)'[\hat{\theta}-\theta]\end{aligned}
  \]
\item
  \textbf{Variance of} \(Y_0 - \hat{Y}_0\): \[
  \begin{aligned}
  \text{Var}(Y_0 - \hat{Y}_0) &\approx \text{Var}(\epsilon_0 - \mathbf{f}_0(\theta)' [\hat{\theta} - \theta]) \\
  &= \sigma^2 + \sigma^2 \mathbf{f}_0(\theta)' [\mathbf{F}(\theta)' \mathbf{F}(\theta)]^{-1} \mathbf{f}_0(\theta) \\
  &= \sigma^2 \big(1 + \mathbf{f}_0(\theta)' [\mathbf{F}(\theta)' \mathbf{F}(\theta)]^{-1} \mathbf{f}_0(\theta)\big).
  \end{aligned}
  \]
\end{enumerate}

Hence, the prediction error \(Y_0 - \hat{Y}_0\) follows an asymptotic normal distribution:

\[
Y_0 - \hat{Y}_0 \sim AN\big(0, \sigma^2 \big(1 + \mathbf{f}_0(\theta)' [\mathbf{F}(\theta)' \mathbf{F}(\theta)]^{-1} \mathbf{f}_0(\theta)\big)\big).
\]

A \(100(1-\alpha)\%\) prediction interval for \(Y_0\) is:

\[
\hat{Y}_0 \pm t_{(1-\alpha/2, n-p)} s \sqrt{1 + \mathbf{f}_0(\hat{\theta})' [\mathbf{F}(\hat{\theta})' \mathbf{F}(\hat{\theta})]^{-1} \mathbf{f}_0(\hat{\theta})}.
\]

where we substitute \(\hat{\theta}\) into \(\mathbf{f}_0\) and \(\mathbf{F}\). Recall \(s\) is the estiamte of \(\sigma\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Sometimes we want a confidence interval for \(E(Y_i)\) (i.e., the mean response at \(x_0\)), rather than a prediction interval for an individual future observation. In that case, the variance term for the random error \(\epsilon_0\) is not included. Hence, the formula is the same but without the ``+1'':

\[
E(Y_0) \approx f(x_0; \theta),
\]

and the confidence interval is:

\[
f(x_0, \hat{\theta}) \pm t_{(1-\alpha/2, n-p)} s \sqrt{\mathbf{f}_0(\hat{\theta})' [\mathbf{F}(\hat{\theta})' \mathbf{F}(\hat{\theta})]^{-1} \mathbf{f}_0(\hat{\theta})}.
\]

Summary

\begin{itemize}
\item
  \protect\hyperlink{linear-functions-of-the-parameters}{Linear Functions of the Parameters}: A function \(f(x, \theta)\) is linear in \(\theta\) if it can be written in the form \[f(x, \theta) = \theta_1 g_1(x) + \theta_2 g_2(x) + \dots + \theta_p g_p(x)\] where \(g_j(x)\) do not depend on \(\theta\). In this case, the Jacobian \(\mathbf{F}(\theta)\) does not depend on \(\theta\) itself (only on \(x_i\)), and exact formulas often match what is familiar from linear regression.
\item
  \protect\hyperlink{nonlinear-functions-of-parameters}{Nonlinear Functions of Parameters}: If \(f(x, \theta)\) depends on \(\theta\) in a nonlinear way (e.g., \(\theta_1 e^{\theta_2 x}, \beta_1/\beta_2\) or more complicated expressions), \(\mathbf{F}(\theta)\) depends on \(\theta\). Estimation generally requires iterative numerical methods (e.g., Gauss--Newton, Levenberg--Marquardt), and the asymptotic results rely on evaluating partial derivatives at \(\hat{\theta}\). Nevertheless, the final inference formulas---confidence intervals, prediction intervals---have a similar form, thanks to the asymptotic normality arguments.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{non-linear-least-squares-estimation}{%
\section{Non-linear Least Squares Estimation}\label{non-linear-least-squares-estimation}}

The least squares (LS) estimate of \(\theta\), denoted as \(\hat{\theta}\), minimizes the residual sum of squares:

\[
S(\hat{\theta}) = SSE(\hat{\theta}) = \sum_{i=1}^{n} \{Y_i - f(\mathbf{x}_i; \hat{\theta})\}^2
\]

To solve this, we consider the partial derivatives of \(S(\theta)\) with respect to each \(\theta_j\) and set them to zero, leading to the normal equations:

\[
\frac{\partial S(\theta)}{\partial \theta_j} = -2 \sum_{i=1}^{n} \{Y_i - f(\mathbf{x}_i; \theta)\} \frac{\partial f(\mathbf{x}_i; \theta)}{\partial \theta_j} = 0
\]

However, these equations are inherently non-linear and, in most cases, cannot be solved analytically. As a result, various estimation techniques are employed to approximate solutions efficiently. These approaches include:

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{iterative-optimization-nonlinear-regression}{Iterative Optimization} -- Methods that refine estimates through successive iterations to minimize error.
\item
  \protect\hyperlink{derivative-free}{Derivative-Free} Methods -- Techniques that do not rely on gradient information, useful for complex or non-smooth functions.
\item
  \protect\hyperlink{stochastic-heuristic-nolinear-regression}{Stochastic Heuristic} -- Algorithms that incorporate randomness, such as genetic algorithms or simulated annealing, to explore solution spaces.
\item
  \protect\hyperlink{linearization-nonlinear-regression-optimization}{Linearization}-- Approximating non-linear models with linear ones to enable analytical or numerical solutions.
\item
  \protect\hyperlink{hybrid-nonlinear-regression-optimization}{Hybrid} Approaches -- Combining multiple methods to leverage their respective strengths for improved estimation.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.3014}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1507}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1370}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1370}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1370}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1370}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Category}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Method}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Best For}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Derivative?}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Optimization}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Comp. Cost}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\protect\hyperlink{iterative-optimization-nonlinear-regression}{Iterative Optimization} & \protect\hyperlink{steepest-descent}{Steepest Descent (Gradient Descent)} & Simple problems, slow convergence & Yes & Local & Low \\
\textbf{Iterative Optimization} & \protect\hyperlink{gauss-newton-algorithm}{Gauss-Newton Algorithm} & Faster than GD, ignores exact second-order info & Yes & Local & Medium \\
\textbf{Iterative Optimization} & \protect\hyperlink{levenberg-marquardt}{Levenberg-Marquardt Algorithm} & Balances GN \& GD, robust & Yes & Local & Medium \\
\textbf{Iterative Optimization} & \protect\hyperlink{newton-raphson}{Newton-Raphson Method} & Quadratic convergence, needs Hessian & Yes & Local & High \\
\textbf{Iterative Optimization} & \protect\hyperlink{quasi-newton-method}{Quasi-Newton Method} & Approximates Hessian for large problems & Yes & Local & Medium \\
\textbf{Iterative Optimization} & \protect\hyperlink{trust-region-reflective-algorithm}{Trust-Region Reflective Algorithm} & Handles constraints, robust & Yes & Local & High \\
\href{Techniques\%20that\%20do\%20not\%20rely\%20on\%20gradient\%20information,\%20useful\%20for\%20complex\%20or\%20non-smooth\%20functions.}{Derivative-Free} & \protect\hyperlink{secant-method}{Secant Method} & Approximates derivative from function evaluations & No & Local & Medium \\
\textbf{Derivative-Free} & \protect\hyperlink{nelder-mead}{Nelder-Mead (Simplex)} & No derivatives, heuristic & No & Local & Medium \\
\textbf{Derivative-Free} & \protect\hyperlink{powells-method}{Powell's Method} & Line search, no explicit gradient & No & Local & Medium \\
\textbf{Derivative-Free} & \protect\hyperlink{grid-search}{Grid Search} & Exhaustive search (best in low dims) & No & Global & Very High \\
\textbf{Derivative-Free} & \protect\hyperlink{hooke-jeeves}{Hooke-Jeeves Pattern Search} & Pattern-based, black-box optimization & No & Local & Medium \\
\textbf{Derivative-Free} & \protect\hyperlink{bisection-method}{Bisection Method} & Simple root/interval-based approach & No & Local & Low \\
\protect\hyperlink{stochastic-heuristic-nolinear-regression}{Stochastic Heuristic} & \protect\hyperlink{simulated-annealing}{Simulated Annealing} & Escapes local minima, non-smooth problems & No & Global & High \\
\textbf{Stochastic Heuristic} & \protect\hyperlink{genetic-algorithm}{Genetic Algorithm} & Large search spaces, evolving parameters & No & Global & High \\
\textbf{Stochastic Heuristic} & \protect\hyperlink{particle-swarm-optimization}{Particle Swarm Optimization} & Swarm-based, often fast global convergence & No & Global & Medium \\
\textbf{Stochastic Heuristic} & \protect\hyperlink{evolutionary-strategies}{Evolutionary Strategies} & High-dimensional, adaptive step-size & No & Global & High \\
\textbf{Stochastic Heuristic} & \protect\hyperlink{differential-evolution-algorithm}{Differential Evolution Algorithm} & Robust global optimizer, population-based & No & Global & High \\
\textbf{Stochastic Heuristic} & Ant Colony Optimization (ACO) & Discrete / combinatorial problems & No & Global & High \\
\protect\hyperlink{linearization-nonlinear-regression-optimization}{Linearization} & \protect\hyperlink{taylor-series-approximation-nonlinear-optimization}{Taylor Series Approximation} & Local approximation of non-linearity & Yes & Local & Low \\
\textbf{Linearization} & \protect\hyperlink{log-linearization-nonlinear-optimization}{Log-Linearization} & Transforms non-linear equations & Yes & Local & Low \\
\protect\hyperlink{hybrid-nonlinear-regression-optimization}{Hybrid} & \protect\hyperlink{adaptive-levenberg-marquardt}{Adaptive Levenberg-Marquardt} & Dynamically adjusts damping in LM & Yes & Local & Medium \\
\textbf{Hybrid} & Hybrid Genetic Algorithm \& LM (GA-LM) & GA for coarse search, LM for fine-tuning & No & Hybrid & High \\
\textbf{Hybrid} & Neural Network-Based NLLS & Deep learning for complex non-linear least squares & No & Hybrid & Very High \\
\end{longtable}

\hypertarget{iterative-optimization-nonlinear-regression}{%
\subsection{Iterative Optimization}\label{iterative-optimization-nonlinear-regression}}

\hypertarget{gauss-newton-algorithm}{%
\subsubsection{Gauss-Newton Algorithm}\label{gauss-newton-algorithm}}

The Gauss-Newton Algorithm is an iterative optimization method used to estimate parameters in nonlinear least squares problems. It refines parameter estimates by approximating the Hessian matrix using first-order derivatives, making it computationally efficient for many practical applications (e.g., regression models in finance and marketing analytics). The objective is to minimize the Sum of Squared Errors (SSE):

\[
SSE(\theta) = \sum_{i=1}^{n} [Y_i - f_i(\theta)]^2,
\]

where \(\mathbf{Y} = [Y_1, \dots, Y_n]'\) are the observed responses, and \(f_i(\theta)\) are the model-predicted values.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Iterative Refinement via Taylor Expansion}

The Gauss-Newton algorithm iteratively refines an initial estimate \(\hat{\theta}^{(0)}\) using the \textbf{Taylor series expansion} of \(f(\mathbf{x}_i; \theta)\) about \(\hat{\theta}^{(0)}\). We start with the observation model:

\[
Y_i = f(\mathbf{x}_i; \theta) + \epsilon_i.
\]

By expanding \(f(\mathbf{x}_i; \theta)\) around \(\hat{\theta}^{(0)}\) and ignoring higher-order terms (assuming the remainder is small), we get:

\[
\begin{aligned}
Y_i &\approx f(\mathbf{x}_i; \hat{\theta}^{(0)}) 
+ \sum_{j=1}^{p} \frac{\partial f(\mathbf{x}_i; \theta)}{\partial \theta_j} \bigg|_{\theta = \hat{\theta}^{(0)}} 
\bigl(\theta_j - \hat{\theta}_j^{(0)}\bigr) 
+ \epsilon_i.
\end{aligned}
\]

In \textbf{matrix form}, let

\[
\mathbf{Y} =
\begin{bmatrix}
Y_1 \\ \vdots \\ Y_n
\end{bmatrix}, 
\quad
\mathbf{f}(\hat{\theta}^{(0)}) =
\begin{bmatrix}
f(\mathbf{x}_1, \hat{\theta}^{(0)}) \\ \vdots \\ f(\mathbf{x}_n, \hat{\theta}^{(0)})
\end{bmatrix},
\]

and define the \textbf{Jacobian matrix} of partial derivatives

\[
\mathbf{F}(\hat{\theta}^{(0)}) =
\begin{bmatrix}
\frac{\partial f(\mathbf{x}_1, \theta)}{\partial \theta_1} & \cdots & \frac{\partial f(\mathbf{x}_1, \theta)}{\partial \theta_p} \\
\vdots & \ddots & \vdots \\
\frac{\partial f(\mathbf{x}_n, \theta)}{\partial \theta_1} & \cdots & \frac{\partial f(\mathbf{x}_n, \theta)}{\partial \theta_p}
\end{bmatrix}_{\theta = \hat{\theta}^{(0)}}.
\]

Then,

\[
\mathbf{Y} \approx \mathbf{f}(\hat{\theta}^{(0)}) 
+ \mathbf{F}(\hat{\theta}^{(0)})\,(\theta - \hat{\theta}^{(0)}) + \epsilon,
\]

with \(\epsilon = [\epsilon_1, \dots, \epsilon_n]'\) assumed i.i.d. with mean \(0\) and variance \(\sigma^2\).

From this linear approximation,

\[
\mathbf{Y} - \mathbf{f}(\hat{\theta}^{(0)}) 
\approx \mathbf{F}(\hat{\theta}^{(0)})\,(\theta - \hat{\theta}^{(0)}).
\]

Solving for \(\theta - \hat{\theta}^{(0)}\) in the \textbf{least squares} sense gives the Gauss increment \(\hat{\delta}^{(1)}\), so we can update:

\[
\hat{\theta}^{(1)} = \hat{\theta}^{(0)} + \hat{\delta}^{(1)}.
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Step-by-Step Procedure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Initialize}: Start with an initial estimate \(\hat{\theta}^{(0)}\) and set \(j = 0\).\\
\item
  \textbf{Compute Taylor Expansion}: Calculate \(\mathbf{f}(\hat{\theta}^{(j)})\) and \(\mathbf{F}(\hat{\theta}^{(j)})\).\\
\item
  \textbf{Solve for Increment}: Treating \(\mathbf{Y} - \mathbf{f}(\hat{\theta}^{(j)}) \approx \mathbf{F}(\hat{\theta}^{(j)})\, (\theta - \hat{\theta}^{(j)})\) as a linear model, use Ordinary Least Squares to compute \(\hat{\delta}^{(j+1)}\).\\
\item
  \textbf{Update Parameters}: Set \(\hat{\theta}^{(j+1)} = \hat{\theta}^{(j)} + \hat{\delta}^{(j+1)}\).\\
\item
  \textbf{Check for Convergence}: If the convergence criteria are met (see below), stop; otherwise, return to Step 2.\\
\item
  \textbf{Estimate Variance}: After convergence, we assume \(\epsilon \sim (\mathbf{0}, \sigma^2 \mathbf{I})\). The variance \(\sigma^2\) can be estimated by
\end{enumerate}

\[
\hat{\sigma}^2 = \frac{1}{n-p} \bigl(\mathbf{Y} - \mathbf{f}(\mathbf{x}; \hat{\theta})\bigr)' \bigl(\mathbf{Y} - \mathbf{f}(\mathbf{x}; \hat{\theta})\bigr).
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Convergence Criteria}

Common criteria for deciding when to stop iterating include:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Objective Function Change}: \[
  \frac{\bigl|SSE(\hat{\theta}^{(j+1)}) - SSE(\hat{\theta}^{(j)})\bigr|}{SSE(\hat{\theta}^{(j)})} < \gamma_1.
  \]
\item
  \textbf{Parameter Change}: \[
  \bigl|\hat{\theta}^{(j+1)} - \hat{\theta}^{(j)}\bigr| < \gamma_2.
  \]
\item
  \textbf{Residual Projection Criterion}: The residuals satisfy convergence as defined in \citep{bates1981relative}.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Another way to see the update step is by viewing the \textbf{necessary condition for a minimum}: the gradient of \(SSE(\theta)\) with respect to \(\theta\) should be zero. For

\[
SSE(\theta) = \sum_{i=1}^{n} [Y_i - f_i(\theta)]^2,
\]

the gradient is

\[
\frac{\partial SSE(\theta)}{\partial \theta} 
= 2\,\mathbf{F}(\theta)' \bigl[\mathbf{Y} - \mathbf{f}(\theta)\bigr].
\]

Using the Gauss-Newton update rule from iteration \(j\) to \(j+1\):

\[
\begin{aligned}
\hat{\theta}^{(j+1)} 
&= \hat{\theta}^{(j)} + \hat{\delta}^{(j+1)} \\
&= \hat{\theta}^{(j)} + \bigl[\mathbf{F}(\hat{\theta}^{(j)})' \,\mathbf{F}(\hat{\theta}^{(j)})\bigr]^{-1} 
\,\mathbf{F}(\hat{\theta}^{(j)})' \bigl[\mathbf{Y} - \mathbf{f}(\hat{\theta}^{(j)})\bigr] \\
&= \hat{\theta}^{(j)} 
- \frac{1}{2} \bigl[\mathbf{F}(\hat{\theta}^{(j)})' \,\mathbf{F}(\hat{\theta}^{(j)})\bigr]^{-1} 
\, \frac{\partial SSE(\hat{\theta}^{(j)})}{\partial \theta},
\end{aligned}
\]

where:

\begin{itemize}
\tightlist
\item
  \(\frac{\partial SSE(\hat{\theta}^{(j)})}{\partial \theta}\) is the \textbf{gradient vector}, pointing in the direction of \textbf{steepest ascent} of SSE.\\
\item
  \(\bigl[\mathbf{F}(\hat{\theta}^{(j)})'\mathbf{F}(\hat{\theta}^{(j)})\bigr]^{-1}\) determines the \textbf{step size}, controlling how far to move in the direction of improvement.\\
\item
  The factor \(-\tfrac{1}{2}\) ensures movement in the \textbf{direction of steepest descent}, helping to \textbf{minimize} the SSE.
\end{itemize}

The Gauss-Newton method works well when the nonlinear model can be approximated accurately by a \textbf{first-order Taylor expansion} near the solution. If the assumption of near-linearity in the residual function \(\mathbf{r}(\theta) = \mathbf{Y} - \mathbf{f}(\theta)\) is violated, convergence may be slow or fail altogether. In such cases, more robust methods like \protect\hyperlink{levenberg-marquardt}{Levenberg-Marquardt Algorithm} (which modifies Gauss-Newton with a damping parameter) are often preferred.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load necessary libraries}
\FunctionTok{library}\NormalTok{(minpack.lm)  }\CommentTok{\# Provides nonlinear least squares functions}

\CommentTok{\# Define a nonlinear function (exponential model)}
\NormalTok{nonlinear\_model }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x) \{}
    \CommentTok{\# theta is a vector of parameters: theta[1] = A, theta[2] = B}
    \CommentTok{\# x is the independent variable}
    \CommentTok{\# The model is A * exp(B * x)}
\NormalTok{    theta[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(theta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ x)}
\NormalTok{\}}

\CommentTok{\# Define SSE function for clarity}
\NormalTok{sse }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x, y) \{}
    \CommentTok{\# SSE = sum of squared errors between actual y and model predictions}
    \FunctionTok{sum}\NormalTok{((y }\SpecialCharTok{{-}} \FunctionTok{nonlinear\_model}\NormalTok{(theta, x)) }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{)}
\NormalTok{\}}

\CommentTok{\# Generate synthetic data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)                     }\CommentTok{\# for reproducibility}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{100}\NormalTok{) }\CommentTok{\# 100 points from 0 to 10}
\NormalTok{true\_theta }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\FloatTok{0.3}\NormalTok{)           }\CommentTok{\# true parameter values}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{nonlinear\_model}\NormalTok{(true\_theta, x) }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{length}\NormalTok{(x), }\AttributeTok{sd =} \FloatTok{0.5}\NormalTok{)}

\CommentTok{\# Display the first few data points}
\FunctionTok{head}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(x, y))}
\CommentTok{\#\textgreater{}           x        y}
\CommentTok{\#\textgreater{} 1 0.0000000 1.719762}
\CommentTok{\#\textgreater{} 2 0.1010101 1.946445}
\CommentTok{\#\textgreater{} 3 0.2020202 2.904315}
\CommentTok{\#\textgreater{} 4 0.3030303 2.225593}
\CommentTok{\#\textgreater{} 5 0.4040404 2.322373}
\CommentTok{\#\textgreater{} 6 0.5050505 3.184724}

\CommentTok{\# Gauss{-}Newton optimization using nls.lm (Levenberg{-}Marquardt as extension).}
\CommentTok{\# Initial guess for theta: c(1, 0.1)}
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{nls.lm}\NormalTok{(}
    \AttributeTok{par =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{0.1}\NormalTok{),}
    \AttributeTok{fn =} \ControlFlowTok{function}\NormalTok{(theta)}
\NormalTok{        y }\SpecialCharTok{{-}} \FunctionTok{nonlinear\_model}\NormalTok{(theta, x)}
\NormalTok{)}

\CommentTok{\# Display estimated parameters}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Estimated parameters (A, B):}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Estimated parameters (A, B):}
\FunctionTok{print}\NormalTok{(fit}\SpecialCharTok{$}\NormalTok{par)}
\CommentTok{\#\textgreater{} [1] 1.9934188 0.3008742}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  We defined the model ``nonlinear\_model(theta, x)'' which returns A\emph{exp(B}x).
\item
  We generated synthetic data using the ``true\_theta'' values and added random noise.
\item
  We used \texttt{nls.lm(...)} from the \texttt{minpack.lm} package to fit the data:

  \begin{itemize}
  \tightlist
  \item
    \texttt{par\ =\ c(1,\ 0.1)} is our initial parameter guess.
  \item
    \texttt{fn\ =\ function(theta)\ y\ -\ nonlinear\_model(theta,\ x)} is the residual function, i.e., observed minus predicted.
  \end{itemize}
\item
  The \texttt{fit\$par} provides the estimated parameters after the algorithm converges.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Visualize the data and the fitted model}
\FunctionTok{plot}\NormalTok{(}
\NormalTok{  x,}
\NormalTok{  y,}
  \AttributeTok{main =} \StringTok{"Data and Fitted Curve (Gauss{-}Newton/Levenberg{-}Marquardt)"}\NormalTok{,}
  \AttributeTok{xlab =} \StringTok{"x"}\NormalTok{,}
  \AttributeTok{ylab =} \StringTok{"y"}\NormalTok{,}
  \AttributeTok{pch =} \DecValTok{19}\NormalTok{,}
  \AttributeTok{cex =} \FloatTok{0.5}
\NormalTok{)}
\FunctionTok{curve}\NormalTok{(}
  \FunctionTok{nonlinear\_model}\NormalTok{(fit}\SpecialCharTok{$}\NormalTok{par, x),}
  \AttributeTok{from =} \DecValTok{0}\NormalTok{,}
  \AttributeTok{to =} \DecValTok{10}\NormalTok{,}
  \AttributeTok{add =} \ConstantTok{TRUE}\NormalTok{,}
  \AttributeTok{col =} \StringTok{"red"}\NormalTok{,}
  \AttributeTok{lwd =} \DecValTok{2}
\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}
  \StringTok{"topleft"}\NormalTok{,}
  \AttributeTok{legend =} \FunctionTok{c}\NormalTok{(}\StringTok{"Data"}\NormalTok{, }\StringTok{"Fitted Curve"}\NormalTok{),}
  \AttributeTok{pch =} \FunctionTok{c}\NormalTok{(}\DecValTok{19}\NormalTok{, }\ConstantTok{NA}\NormalTok{),}
  \AttributeTok{lty =} \FunctionTok{c}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\DecValTok{1}\NormalTok{),}
  \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"red"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{06-nonlinear-regession_files/figure-latex/unnamed-chunk-2-1} \end{center}

\hypertarget{modified-gauss-newton-algorithm}{%
\subsubsection{Modified Gauss-Newton Algorithm}\label{modified-gauss-newton-algorithm}}

The Modified Gauss-Newton Algorithm introduces a learning rate \(\alpha_j\) to control step size and prevent overshooting the local minimum. The standard \protect\hyperlink{gauss-newton-algorithm}{Gauss-Newton Algorithm} assumes that the full step direction \(\hat{\delta}^{(j+1)}\) is optimal, but in practice, especially for highly nonlinear problems, it can overstep the minimum or cause numerical instability. The modification introduces a step size reduction, making it more robust.

We redefine the update step as:

\[
\hat{\theta}^{(j+1)} = \hat{\theta}^{(j)} + \alpha_j \hat{\delta}^{(j+1)}, \quad 0 < \alpha_j < 1,
\]

where:

\begin{itemize}
\tightlist
\item
  \(\alpha_j\) is a \textbf{learning rate}, controlling how much of the step \(\hat{\delta}^{(j+1)}\) is taken.
\item
  If \(\alpha_j = 1\), we recover the standard Gauss-Newton method.
\item
  If \(\alpha_j\) is too small, convergence is slow; if too large, the algorithm may diverge.
\end{itemize}

This learning rate \(\alpha_j\) allows for \textbf{adaptive step size adjustments}, helping prevent excessive parameter jumps and ensuring that SSE decreases at each iteration.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

A common approach to determine \(\alpha_j\) is \textbf{step halving}, ensuring that each iteration moves in a direction that reduces SSE. Instead of using a fixed \(\alpha_j\), we iteratively reduce the step size until SSE decreases:

\[
\hat{\theta}^{(j+1)} = \hat{\theta}^{(j)} + \frac{1}{2^k}\hat{\delta}^{(j+1)},
\]

where:

\begin{itemize}
\tightlist
\item
  \(k\) is the smallest non-negative integer such that
\end{itemize}

\[
SSE(\hat{\theta}^{(j)} + \frac{1}{2^k} \hat{\delta}^{(j+1)}) < SSE(\hat{\theta}^{(j)}).
\]

This means we start with the full step \(\hat{\delta}^{(j+1)}\), then try \(\hat{\delta}^{(j+1)}/2\), then \(\hat{\delta}^{(j+1)}/4\), and so on, until SSE is reduced.

\textbf{Algorithm for Step Halving:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute the Gauss-Newton step \(\hat{\delta}^{(j+1)}\).
\item
  Set an initial \(\alpha_j = 1\).
\item
  If the updated parameters \(\hat{\theta}^{(j)} + \alpha_j \hat{\delta}^{(j+1)}\) increase SSE, divide \(\alpha_j\) by 2.
\item
  Repeat until SSE decreases.
\end{enumerate}

This \textbf{ensures monotonic SSE reduction}, preventing divergence due to an overly aggressive step.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Generalized Form of the Modified Algorithm}

A more general form of the update rule, incorporating step size control and a matrix \(\mathbf{A}_j\), is:

\[
\hat{\theta}^{(j+1)} = \hat{\theta}^{(j)} - \alpha_j \mathbf{A}_j \frac{\partial Q(\hat{\theta}^{(j)})}{\partial \theta},
\]

where:

\begin{itemize}
\tightlist
\item
  \(\mathbf{A}_j\) is a \textbf{positive definite matrix} that preconditions the update direction.
\item
  \(\alpha_j\) is the \textbf{learning rate}.
\item
  \(\frac{\partial Q(\hat{\theta}^{(j)})}{\partial \theta}\) is the \textbf{gradient} of the objective function \(Q(\theta)\), typically SSE in nonlinear regression.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Connection to the Modified Gauss-Newton Algorithm}

The \textbf{Modified Gauss-Newton Algorithm} fits this framework:

\[
\hat{\theta}^{(j+1)} = \hat{\theta}^{(j)} - \alpha_j [\mathbf{F}(\hat{\theta}^{(j)})' \mathbf{F}(\hat{\theta}^{(j)})]^{-1} \frac{\partial SSE(\hat{\theta}^{(j)})}{\partial \theta}.
\]

Here, we recognize:

\begin{itemize}
\tightlist
\item
  \textbf{Objective function}: \(Q = SSE\).
\item
  \textbf{Preconditioner matrix}: \([\mathbf{F}(\hat{\theta}^{(j)})' \mathbf{F}(\hat{\theta}^{(j)})]^{-1} = \mathbf{A}\).
\end{itemize}

Thus, the standard Gauss-Newton method can be interpreted as a special case of this broader optimization framework, with a preconditioned gradient descent approach.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load required library}
\FunctionTok{library}\NormalTok{(minpack.lm)}

\CommentTok{\# Define a nonlinear function (exponential model)}
\NormalTok{nonlinear\_model }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x) \{}
\NormalTok{  theta[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(theta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ x)}
\NormalTok{\}}

\CommentTok{\# Define the Sum of Squared Errors function}
\NormalTok{sse }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x, y) \{}
  \FunctionTok{sum}\NormalTok{((y }\SpecialCharTok{{-}} \FunctionTok{nonlinear\_model}\NormalTok{(theta, x)) }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{)}
\NormalTok{\}}

\CommentTok{\# Generate synthetic data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{100}\NormalTok{)}
\NormalTok{true\_theta }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\FloatTok{0.3}\NormalTok{)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{nonlinear\_model}\NormalTok{(true\_theta, x) }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{length}\NormalTok{(x), }\AttributeTok{sd =} \FloatTok{0.5}\NormalTok{)}

\CommentTok{\# Gauss{-}Newton with Step Halving}
\NormalTok{gauss\_newton\_modified }\OtherTok{\textless{}{-}}
  \ControlFlowTok{function}\NormalTok{(theta\_init,}
\NormalTok{           x,}
\NormalTok{           y,}
           \AttributeTok{tol =} \FloatTok{1e{-}6}\NormalTok{,}
           \AttributeTok{max\_iter =} \DecValTok{100}\NormalTok{) \{}
\NormalTok{    theta }\OtherTok{\textless{}{-}}\NormalTok{ theta\_init}
    \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{max\_iter) \{}
      \CommentTok{\# Compute Jacobian matrix numerically}
\NormalTok{      epsilon }\OtherTok{\textless{}{-}} \FloatTok{1e{-}6}
\NormalTok{      F\_matrix }\OtherTok{\textless{}{-}}
        \FunctionTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{, }\AttributeTok{nrow =} \FunctionTok{length}\NormalTok{(y), }\AttributeTok{ncol =} \FunctionTok{length}\NormalTok{(theta))}
      \ControlFlowTok{for}\NormalTok{ (p }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(theta)) \{}
\NormalTok{        theta\_perturb }\OtherTok{\textless{}{-}}\NormalTok{ theta}
\NormalTok{        theta\_perturb[p] }\OtherTok{\textless{}{-}}\NormalTok{ theta[p] }\SpecialCharTok{+}\NormalTok{ epsilon}
\NormalTok{        F\_matrix[, p] }\OtherTok{\textless{}{-}}
\NormalTok{          (}\FunctionTok{nonlinear\_model}\NormalTok{(theta\_perturb, x)}\SpecialCharTok{{-}}\FunctionTok{nonlinear\_model}\NormalTok{(theta, x))}\SpecialCharTok{/}\NormalTok{epsilon}
\NormalTok{      \}}
      
      \CommentTok{\# Compute residuals}
\NormalTok{      residuals }\OtherTok{\textless{}{-}}\NormalTok{ y }\SpecialCharTok{{-}} \FunctionTok{nonlinear\_model}\NormalTok{(theta, x)}
      
      \CommentTok{\# Compute Gauss{-}Newton step}
\NormalTok{      delta }\OtherTok{\textless{}{-}}
        \FunctionTok{solve}\NormalTok{(}\FunctionTok{t}\NormalTok{(F\_matrix) }\SpecialCharTok{\%*\%}\NormalTok{ F\_matrix) }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(F\_matrix) }\SpecialCharTok{\%*\%}\NormalTok{ residuals}
      
      \CommentTok{\# Step Halving Implementation}
\NormalTok{      alpha }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{      k }\OtherTok{\textless{}{-}} \DecValTok{0}
      \ControlFlowTok{while}\NormalTok{ (}\FunctionTok{sse}\NormalTok{(theta }\SpecialCharTok{+}\NormalTok{ alpha }\SpecialCharTok{*}\NormalTok{ delta, x, y) }\SpecialCharTok{\textgreater{}=} \FunctionTok{sse}\NormalTok{(theta, x, y) }\SpecialCharTok{\&\&}
\NormalTok{             k }\SpecialCharTok{\textless{}} \DecValTok{10}\NormalTok{) \{}
\NormalTok{        alpha }\OtherTok{\textless{}{-}}\NormalTok{ alpha }\SpecialCharTok{/} \DecValTok{2}
\NormalTok{        k }\OtherTok{\textless{}{-}}\NormalTok{ k }\SpecialCharTok{+} \DecValTok{1}
\NormalTok{      \}}
      
      \CommentTok{\# Update theta}
\NormalTok{      theta\_new }\OtherTok{\textless{}{-}}\NormalTok{ theta }\SpecialCharTok{+}\NormalTok{ alpha }\SpecialCharTok{*}\NormalTok{ delta}
      
      \CommentTok{\# Check for convergence}
      \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{sum}\NormalTok{(}\FunctionTok{abs}\NormalTok{(theta\_new }\SpecialCharTok{{-}}\NormalTok{ theta)) }\SpecialCharTok{\textless{}}\NormalTok{ tol) \{}
        \ControlFlowTok{break}
\NormalTok{      \}}
      
\NormalTok{      theta }\OtherTok{\textless{}{-}}\NormalTok{ theta\_new}
\NormalTok{    \}}
    \FunctionTok{return}\NormalTok{(theta)}
\NormalTok{  \}}

\CommentTok{\# Run Modified Gauss{-}Newton Algorithm}
\NormalTok{theta\_init }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{0.1}\NormalTok{)  }\CommentTok{\# Initial parameter guess}
\NormalTok{estimated\_theta }\OtherTok{\textless{}{-}} \FunctionTok{gauss\_newton\_modified}\NormalTok{(theta\_init, x, y)}

\CommentTok{\# Display estimated parameters}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Estimated parameters (A, B) with Modified Gauss{-}Newton:}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Estimated parameters (A, B) with Modified Gauss{-}Newton:}
\FunctionTok{print}\NormalTok{(estimated\_theta)}
\CommentTok{\#\textgreater{}           [,1]}
\CommentTok{\#\textgreater{} [1,] 1.9934188}
\CommentTok{\#\textgreater{} [2,] 0.3008742}

\CommentTok{\# Plot data and fitted curve}
\FunctionTok{plot}\NormalTok{(}
\NormalTok{  x,}
\NormalTok{  y,}
  \AttributeTok{main =} \StringTok{"Modified Gauss{-}Newton: Data \& Fitted Curve"}\NormalTok{,}
  \AttributeTok{pch =} \DecValTok{19}\NormalTok{,}
  \AttributeTok{cex =} \FloatTok{0.5}\NormalTok{,}
  \AttributeTok{xlab =} \StringTok{"x"}\NormalTok{,}
  \AttributeTok{ylab =} \StringTok{"y"}
\NormalTok{)}
\FunctionTok{curve}\NormalTok{(}
  \FunctionTok{nonlinear\_model}\NormalTok{(estimated\_theta, x),}
  \AttributeTok{from =} \DecValTok{0}\NormalTok{,}
  \AttributeTok{to =} \DecValTok{10}\NormalTok{,}
  \AttributeTok{add =} \ConstantTok{TRUE}\NormalTok{,}
  \AttributeTok{col =} \StringTok{"red"}\NormalTok{,}
  \AttributeTok{lwd =} \DecValTok{2}
\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}
  \StringTok{"topleft"}\NormalTok{,}
  \AttributeTok{legend =} \FunctionTok{c}\NormalTok{(}\StringTok{"Data"}\NormalTok{, }\StringTok{"Fitted Curve"}\NormalTok{),}
  \AttributeTok{pch =} \FunctionTok{c}\NormalTok{(}\DecValTok{19}\NormalTok{, }\ConstantTok{NA}\NormalTok{),}
  \AttributeTok{lty =} \FunctionTok{c}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\DecValTok{1}\NormalTok{),}
  \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"red"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{06-nonlinear-regession_files/figure-latex/unnamed-chunk-3-1} \end{center}

\hypertarget{steepest-descent}{%
\subsubsection{Steepest Descent (Gradient Descent)}\label{steepest-descent}}

The Steepest Descent Method, commonly known as Gradient Descent, is a fundamental iterative optimization technique used for finding parameter estimates that minimize an objective function \(\mathbf{Q}(\theta)\). It is a special case of the \protect\hyperlink{modified-gauss-newton-algorithm}{Modified Gauss-Newton Algorithm}, where the preconditioning matrix \(\mathbf{A}_j\) is replaced by the identity matrix.

The update rule is given by:

\[
\hat{\theta}^{(j+1)} = \hat{\theta}^{(j)} - \alpha_j \mathbf{I}_{p \times p}\frac{\partial \mathbf{Q}(\hat{\theta}^{(j)})}{\partial \theta},
\]

where:

\begin{itemize}
\tightlist
\item
  \(\alpha_j\) is the \textbf{learning rate}, determining the step size.
\item
  \(\mathbf{I}_{p \times p}\) is the \textbf{identity matrix}, meaning updates occur in the direction of the \textbf{negative gradient}.
\item
  \(\frac{\partial \mathbf{Q}(\hat{\theta}^{(j)})}{\partial \theta}\) is the \textbf{gradient vector}, which provides the direction of \textbf{steepest ascent}; its negation ensures movement toward the minimum.
\end{itemize}

Characteristics of Steepest Descent

\begin{itemize}
\tightlist
\item
  \textbf{Slow to converge:} The algorithm moves in the direction of the gradient but does not take into account curvature, which may result in slow convergence, especially in ill-conditioned problems.
\item
  \textbf{Moves rapidly initially:} The method can exhibit \textbf{fast initial progress}, but as it approaches the minimum, step sizes become small, leading to slow convergence.
\item
  \textbf{Useful for initialization:} Due to its simplicity and ease of implementation, it is often used to obtain \textbf{starting values} for more advanced methods like \textbf{Newton's method} or \protect\hyperlink{gauss-newton-algorithm}{Gauss-Newton Algorithm}.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Comparison with Gauss-Newton

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2055}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3836}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2055}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2055}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Update Rule
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Advantages
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Disadvantages
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Steepest Descent} & \(\hat{\theta}^{(j+1)} = \hat{\theta}^{(j)} - \alpha_j \mathbf{I} \nabla Q(\theta)\) & Simple, requires only first derivatives & Slow convergence, sensitive to \(\alpha_j\) \\
\textbf{Gauss-Newton} & \(\hat{\theta}^{(j+1)} = \hat{\theta}^{(j)} - \mathbf{H}^{-1} \nabla Q(\theta)\) & Uses curvature, faster near solution & Requires Jacobian computation, may diverge \\
\end{longtable}

The \textbf{key difference} is that Steepest Descent \textbf{only} considers the gradient direction, while Gauss-Newton and Newton's method incorporate curvature information.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Choosing the Learning Rate \(\alpha_j\)

A well-chosen \textbf{learning rate} is crucial for the success of gradient descent:

\begin{itemize}
\tightlist
\item
  \textbf{Too large}: The algorithm may overshoot the minimum and diverge.
\item
  \textbf{Too small}: Convergence is \textbf{very slow}.
\item
  \textbf{Adaptive strategies}:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Fixed step size}: \(\alpha_j\) is constant.
  \item
    \textbf{Step size decay}: \(\alpha_j\) decreases over iterations (e.g., \(\alpha_j = \frac{1}{j}\)).
  \item
    \textbf{Line search}: Choose \(\alpha_j\) by minimizing \(\mathbf{Q}(\theta^{(j+1)})\) along the gradient direction.
  \end{itemize}
\end{itemize}

A common approach is \textbf{backtracking line search}, where \(\alpha_j\) is reduced iteratively until a decrease in \(\mathbf{Q}(\theta)\) is observed.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load necessary libraries}
\FunctionTok{library}\NormalTok{(ggplot2)}


\CommentTok{\# Define the nonlinear function (exponential model)}
\NormalTok{nonlinear\_model }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x) \{}
\NormalTok{    theta[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(theta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ x)}
\NormalTok{\}}

\CommentTok{\# Define the Sum of Squared Errors function}
\NormalTok{sse }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x, y) \{}
    \FunctionTok{sum}\NormalTok{((y }\SpecialCharTok{{-}} \FunctionTok{nonlinear\_model}\NormalTok{(theta, x)) }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{)}
\NormalTok{\}}

\CommentTok{\# Define Gradient of SSE w.r.t theta (computed numerically)}
\NormalTok{gradient\_sse }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x, y) \{}
\NormalTok{    n }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(y)}
\NormalTok{    residuals }\OtherTok{\textless{}{-}}\NormalTok{ y }\SpecialCharTok{{-}} \FunctionTok{nonlinear\_model}\NormalTok{(theta, x)}
    
    \CommentTok{\# Partial derivative w.r.t theta\_1}
\NormalTok{    grad\_1 }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\DecValTok{2} \SpecialCharTok{*} \FunctionTok{sum}\NormalTok{(residuals }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(theta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ x))}
    
    \CommentTok{\# Partial derivative w.r.t theta\_2}
\NormalTok{    grad\_2 }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\DecValTok{2} \SpecialCharTok{*} \FunctionTok{sum}\NormalTok{(residuals }\SpecialCharTok{*}\NormalTok{ theta[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ x }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(theta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ x))}
    
    \FunctionTok{return}\NormalTok{(}\FunctionTok{c}\NormalTok{(grad\_1, grad\_2))}
\NormalTok{\}}

\CommentTok{\# Generate synthetic data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{100}\NormalTok{)}
\NormalTok{true\_theta }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\FloatTok{0.3}\NormalTok{)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{nonlinear\_model}\NormalTok{(true\_theta, x) }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{length}\NormalTok{(x), }\AttributeTok{sd =} \FloatTok{0.5}\NormalTok{)}

\CommentTok{\# Safe Gradient Descent Implementation}
\NormalTok{gradient\_descent }\OtherTok{\textless{}{-}}
    \ControlFlowTok{function}\NormalTok{(theta\_init,}
\NormalTok{             x,}
\NormalTok{             y,}
             \AttributeTok{alpha =} \FloatTok{0.01}\NormalTok{,}
             \AttributeTok{tol =} \FloatTok{1e{-}6}\NormalTok{,}
             \AttributeTok{max\_iter =} \DecValTok{500}\NormalTok{) \{}
\NormalTok{        theta }\OtherTok{\textless{}{-}}\NormalTok{ theta\_init}
\NormalTok{        sse\_values }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(max\_iter)}
        
        \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{max\_iter) \{}
\NormalTok{            grad }\OtherTok{\textless{}{-}} \FunctionTok{gradient\_sse}\NormalTok{(theta, x, y)}
            
            \CommentTok{\# Check for NaN or Inf values in the gradient (prevents divergence)}
            \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{any}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(grad)) }\SpecialCharTok{||} \FunctionTok{any}\NormalTok{(}\FunctionTok{is.infinite}\NormalTok{(grad))) \{}
                \FunctionTok{cat}\NormalTok{(}\StringTok{"Numerical instability detected at iteration"}\NormalTok{,}
\NormalTok{                    j,}
                    \StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
                \ControlFlowTok{break}
\NormalTok{            \}}
            
            \CommentTok{\# Update step}
\NormalTok{            theta\_new }\OtherTok{\textless{}{-}}\NormalTok{ theta }\SpecialCharTok{{-}}\NormalTok{ alpha }\SpecialCharTok{*}\NormalTok{ grad}
\NormalTok{            sse\_values[j] }\OtherTok{\textless{}{-}} \FunctionTok{sse}\NormalTok{(theta\_new, x, y)}
            
            \CommentTok{\# Check for convergence}
            \ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\FunctionTok{is.finite}\NormalTok{(sse\_values[j])) \{}
                \FunctionTok{cat}\NormalTok{(}\StringTok{"Divergence detected at iteration"}\NormalTok{, j, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
                \ControlFlowTok{break}
\NormalTok{            \}}
            
            \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{sum}\NormalTok{(}\FunctionTok{abs}\NormalTok{(theta\_new }\SpecialCharTok{{-}}\NormalTok{ theta)) }\SpecialCharTok{\textless{}}\NormalTok{ tol) \{}
                \FunctionTok{cat}\NormalTok{(}\StringTok{"Converged in"}\NormalTok{, j, }\StringTok{"iterations.}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
                \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{theta =}\NormalTok{ theta\_new, }\AttributeTok{sse\_values =}\NormalTok{ sse\_values[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{j]))}
\NormalTok{            \}}
            
\NormalTok{            theta }\OtherTok{\textless{}{-}}\NormalTok{ theta\_new}
\NormalTok{        \}}
        
        \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{theta =}\NormalTok{ theta, }\AttributeTok{sse\_values =}\NormalTok{ sse\_values))}
\NormalTok{    \}}

\CommentTok{\# Run Gradient Descent with a Safe Implementation}
\NormalTok{theta\_init }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{0.1}\NormalTok{)  }\CommentTok{\# Initial guess}
\NormalTok{alpha }\OtherTok{\textless{}{-}} \FloatTok{0.001}           \CommentTok{\# Learning rate}
\NormalTok{result }\OtherTok{\textless{}{-}} \FunctionTok{gradient\_descent}\NormalTok{(theta\_init, x, y, alpha)}
\CommentTok{\#\textgreater{} Divergence detected at iteration 1}

\CommentTok{\# Extract results}
\NormalTok{estimated\_theta }\OtherTok{\textless{}{-}}\NormalTok{ result}\SpecialCharTok{$}\NormalTok{theta}
\NormalTok{sse\_values }\OtherTok{\textless{}{-}}\NormalTok{ result}\SpecialCharTok{$}\NormalTok{sse\_values}

\CommentTok{\# Display estimated parameters}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Estimated parameters (A, B) using Gradient Descent:}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Estimated parameters (A, B) using Gradient Descent:}
\FunctionTok{print}\NormalTok{(estimated\_theta)}
\CommentTok{\#\textgreater{} [1] 1.0 0.1}

\CommentTok{\# Plot convergence of SSE over iterations}
\CommentTok{\# Ensure sse\_values has valid data}
\NormalTok{sse\_df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{Iteration =} \FunctionTok{seq\_along}\NormalTok{(sse\_values),}
  \AttributeTok{SSE =}\NormalTok{ sse\_values}
\NormalTok{)}

\CommentTok{\# Generate improved plot using ggplot()}
\FunctionTok{ggplot}\NormalTok{(sse\_df, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Iteration, }\AttributeTok{y =}\NormalTok{ SSE)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{color =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{linewidth =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{title =} \StringTok{"Gradient Descent Convergence"}\NormalTok{,}
    \AttributeTok{x =} \StringTok{"Iteration"}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"SSE"}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{06-nonlinear-regession_files/figure-latex/unnamed-chunk-4-1} \end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \protect\hyperlink{steepest-descent}{Steepest Descent (Gradient Descent)} moves in the direction of steepest descent, which can lead to zigzagging behavior.
\item
  Slow convergence occurs when the curvature of the function varies significantly across dimensions.
\item
  Learning rate tuning is critical:

  \begin{itemize}
  \item
    If too large, the algorithm diverges.
  \item
    If too small, progress is very slow.
  \end{itemize}
\item
  Useful for initialization: It is often used to get close to the optimal solution before switching to more advanced methods like \protect\hyperlink{gauss-newton-algorithm}{Gauss-Newton Algorithm} or Newton's method.
\end{enumerate}

Several advanced techniques improve the performance of steepest descent:

\begin{itemize}
\item
  \textbf{Momentum Gradient Descent}: Adds a momentum term to smooth updates, reducing oscillations.
\item
  \textbf{Adaptive Learning Rates}:

  \begin{itemize}
  \item
    AdaGrad: Adjusts \(\alpha_j\) per parameter based on historical gradients.
  \item
    RMSprop: Uses a moving average of past gradients to scale updates.
  \item
    Adam (Adaptive Moment Estimation): Combines momentum and adaptive learning rates.
  \end{itemize}
\end{itemize}

In practice, Adam is widely used in machine learning and deep learning, while Newton-based methods (including Gauss-Newton) are preferred for nonlinear regression.

\hypertarget{levenberg-marquardt}{%
\subsubsection{Levenberg-Marquardt Algorithm}\label{levenberg-marquardt}}

The Levenberg-Marquardt Algorithm is a widely used optimization method for solving nonlinear least squares problems. It is an adaptive technique that blends the \protect\hyperlink{gauss-newton-algorithm}{Gauss-Newton Algorithm} and \protect\hyperlink{steepest-descent}{Steepest Descent (Gradient Descent)}, dynamically switching between them based on problem conditions.

The update rule is:

\[
\hat{\theta}^{(j+1)} = \hat{\theta}^{(j)} - \alpha_j [\mathbf{F}(\hat{\theta}^{(j)})' \mathbf{F}(\hat{\theta}^{(j)}) + \tau \mathbf{I}_{p \times p}]\frac{\partial \mathbf{Q}(\hat{\theta}^{(j)})}{\partial \theta}
\]

where:

\begin{itemize}
\tightlist
\item
  \(\tau\) is the \textbf{damping parameter}, controlling whether the step behaves like \protect\hyperlink{gauss-newton-algorithm}{Gauss-Newton Algorithm} or \protect\hyperlink{steepest-descent}{Steepest Descent (Gradient Descent)}.
\item
  \(\mathbf{I}_{p \times p}\) is the \textbf{identity matrix}, ensuring numerical stability.
\item
  \(\mathbf{F}(\hat{\theta}^{(j)})\) is the \textbf{Jacobian matrix} of partial derivatives.
\item
  \(\frac{\partial \mathbf{Q}(\hat{\theta}^{(j)})}{\partial \theta}\) is the \textbf{gradient vector}.
\item
  \(\alpha_j\) is the \textbf{learning rate}, determining step size.
\end{itemize}

The Levenberg-Marquardt algorithm is particularly useful when the Jacobian matrix \(\mathbf{F}(\hat{\theta}^{(j)})\) is nearly singular, meaning that \protect\hyperlink{gauss-newton-algorithm}{Gauss-Newton Algorithm} alone may fail.

\begin{itemize}
\tightlist
\item
  When \(\tau\) is large, the method behaves like Steepest Descent, ensuring stability.
\item
  When \(\tau\) is small, it behaves like Gauss-Newton, accelerating convergence.
\item
  Adaptive control of \(\tau\):

  \begin{itemize}
  \tightlist
  \item
    If \(SSE(\hat{\theta}^{(j+1)}) < SSE(\hat{\theta}^{(j)})\), reduce \(\tau\): \[
    \tau \gets \tau / 10
    \]
  \item
    Otherwise, increase \(\tau\) to stabilize: \[
    \tau \gets 10\tau
    \]
  \end{itemize}
\end{itemize}

This adjustment ensures that the algorithm moves efficiently while avoiding instability.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load required libraries}
\FunctionTok{library}\NormalTok{(minpack.lm)}
\FunctionTok{library}\NormalTok{(ggplot2)}

\CommentTok{\# Define a nonlinear function (exponential model)}
\NormalTok{nonlinear\_model }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x) \{}
\NormalTok{    theta[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(theta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ x)}
\NormalTok{\}}

\CommentTok{\# Define SSE function}
\NormalTok{sse }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x, y) \{}
    \FunctionTok{sum}\NormalTok{((y }\SpecialCharTok{{-}} \FunctionTok{nonlinear\_model}\NormalTok{(theta, x)) }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{)}
\NormalTok{\}}

\CommentTok{\# Generate synthetic data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{100}\NormalTok{)}
\NormalTok{true\_theta }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\FloatTok{0.3}\NormalTok{)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{nonlinear\_model}\NormalTok{(true\_theta, x) }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{length}\NormalTok{(x), }\AttributeTok{sd =} \FloatTok{0.5}\NormalTok{)}

\CommentTok{\# Robust Levenberg{-}Marquardt Optimization Implementation}
\NormalTok{levenberg\_marquardt }\OtherTok{\textless{}{-}}
    \ControlFlowTok{function}\NormalTok{(theta\_init,}
\NormalTok{             x,}
\NormalTok{             y,}
             \AttributeTok{tol =} \FloatTok{1e{-}6}\NormalTok{,}
             \AttributeTok{max\_iter =} \DecValTok{500}\NormalTok{,}
             \AttributeTok{tau\_init =} \DecValTok{1}\NormalTok{) \{}
\NormalTok{        theta }\OtherTok{\textless{}{-}}\NormalTok{ theta\_init}
\NormalTok{        tau }\OtherTok{\textless{}{-}}\NormalTok{ tau\_init}
\NormalTok{        lambda }\OtherTok{\textless{}{-}} \FloatTok{1e{-}8}  \CommentTok{\# Small regularization term}
\NormalTok{        sse\_values }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(max\_iter)}
        
        \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{max\_iter) \{}
            \CommentTok{\# Compute Jacobian matrix numerically}
\NormalTok{            epsilon }\OtherTok{\textless{}{-}} \FloatTok{1e{-}6}
\NormalTok{            F\_matrix }\OtherTok{\textless{}{-}}
                \FunctionTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{, }\AttributeTok{nrow =} \FunctionTok{length}\NormalTok{(y), }\AttributeTok{ncol =} \FunctionTok{length}\NormalTok{(theta))}
            \ControlFlowTok{for}\NormalTok{ (p }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(theta)) \{}
\NormalTok{                theta\_perturb }\OtherTok{\textless{}{-}}\NormalTok{ theta}
\NormalTok{                theta\_perturb[p] }\OtherTok{\textless{}{-}}\NormalTok{ theta[p] }\SpecialCharTok{+}\NormalTok{ epsilon}
\NormalTok{                F\_matrix[, p] }\OtherTok{\textless{}{-}}
\NormalTok{                    (}\FunctionTok{nonlinear\_model}\NormalTok{(theta\_perturb, x) }
                     \SpecialCharTok{{-}} \FunctionTok{nonlinear\_model}\NormalTok{(theta, x)) }\SpecialCharTok{/}\NormalTok{ epsilon}
\NormalTok{            \}}
            
            \CommentTok{\# Compute residuals}
\NormalTok{            residuals }\OtherTok{\textless{}{-}}\NormalTok{ y }\SpecialCharTok{{-}} \FunctionTok{nonlinear\_model}\NormalTok{(theta, x)}
            
            \CommentTok{\# Compute Levenberg{-}Marquardt update}
\NormalTok{            A }\OtherTok{\textless{}{-}}
                \FunctionTok{t}\NormalTok{(F\_matrix) }\SpecialCharTok{\%*\%}\NormalTok{ F\_matrix }\SpecialCharTok{+}\NormalTok{ tau }\SpecialCharTok{*} \FunctionTok{diag}\NormalTok{(}\FunctionTok{length}\NormalTok{(theta)) }
            \SpecialCharTok{+}\NormalTok{ lambda }\SpecialCharTok{*} \FunctionTok{diag}\NormalTok{(}\FunctionTok{length}\NormalTok{(theta))  }\CommentTok{\# Regularized A}
\NormalTok{            delta }\OtherTok{\textless{}{-}} \FunctionTok{tryCatch}\NormalTok{(}
                \FunctionTok{solve}\NormalTok{(A) }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(F\_matrix) }\SpecialCharTok{\%*\%}\NormalTok{ residuals,}
                \AttributeTok{error =} \ControlFlowTok{function}\NormalTok{(e) \{}
                    \FunctionTok{cat}\NormalTok{(}\StringTok{"Singular matrix detected at iteration"}\NormalTok{,}
\NormalTok{                        j,}
                        \StringTok{"{-} Increasing tau}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\NormalTok{                    tau }\OtherTok{\textless{}\textless{}{-}}\NormalTok{ tau }\SpecialCharTok{*} \DecValTok{10}  \CommentTok{\# Increase tau to stabilize}
                    \CommentTok{\# Return zero delta to avoid NaN updates}
                    \FunctionTok{return}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FunctionTok{length}\NormalTok{(theta)))  }
\NormalTok{                \}}
\NormalTok{            )}
            
\NormalTok{            theta\_new }\OtherTok{\textless{}{-}}\NormalTok{ theta }\SpecialCharTok{+}\NormalTok{ delta}
            
            \CommentTok{\# Compute new SSE}
\NormalTok{            sse\_values[j] }\OtherTok{\textless{}{-}} \FunctionTok{sse}\NormalTok{(theta\_new, x, y)}
            
            \CommentTok{\# Adjust tau dynamically}
            \ControlFlowTok{if}\NormalTok{ (sse\_values[j] }\SpecialCharTok{\textless{}} \FunctionTok{sse}\NormalTok{(theta, x, y)) \{}
                \CommentTok{\# Reduce tau but prevent it from going too low}
\NormalTok{                tau }\OtherTok{\textless{}{-}}
                    \FunctionTok{max}\NormalTok{(tau }\SpecialCharTok{/} \DecValTok{10}\NormalTok{, }\FloatTok{1e{-}8}\NormalTok{)  }
\NormalTok{            \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{                tau }\OtherTok{\textless{}{-}}\NormalTok{ tau }\SpecialCharTok{*} \DecValTok{10}  \CommentTok{\# Increase tau if SSE increases}
\NormalTok{            \}}
            
            \CommentTok{\# Check for convergence}
            \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{sum}\NormalTok{(}\FunctionTok{abs}\NormalTok{(delta)) }\SpecialCharTok{\textless{}}\NormalTok{ tol) \{}
                \FunctionTok{cat}\NormalTok{(}\StringTok{"Converged in"}\NormalTok{, j, }\StringTok{"iterations.}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
                \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{theta =}\NormalTok{ theta\_new, }\AttributeTok{sse\_values =}\NormalTok{ sse\_values[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{j]))}
\NormalTok{            \}}
            
\NormalTok{            theta }\OtherTok{\textless{}{-}}\NormalTok{ theta\_new}
\NormalTok{        \}}
        
        \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{theta =}\NormalTok{ theta, }\AttributeTok{sse\_values =}\NormalTok{ sse\_values))}
\NormalTok{    \}}

\CommentTok{\# Run Levenberg{-}Marquardt}
\NormalTok{theta\_init }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{0.1}\NormalTok{)  }\CommentTok{\# Initial guess}
\NormalTok{result }\OtherTok{\textless{}{-}} \FunctionTok{levenberg\_marquardt}\NormalTok{(theta\_init, x, y)}
\CommentTok{\#\textgreater{} Singular matrix detected at iteration 11 {-} Increasing tau}
\CommentTok{\#\textgreater{} Converged in 11 iterations.}

\CommentTok{\# Extract results}
\NormalTok{estimated\_theta }\OtherTok{\textless{}{-}}\NormalTok{ result}\SpecialCharTok{$}\NormalTok{theta}
\NormalTok{sse\_values }\OtherTok{\textless{}{-}}\NormalTok{ result}\SpecialCharTok{$}\NormalTok{sse\_values}

\CommentTok{\# Display estimated parameters}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Estimated parameters (A, B) using Levenberg{-}Marquardt:}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Estimated parameters (A, B) using Levenberg{-}Marquardt:}
\FunctionTok{print}\NormalTok{(estimated\_theta)}
\CommentTok{\#\textgreater{}               [,1]}
\CommentTok{\#\textgreater{} [1,] {-}6.473440e{-}09}
\CommentTok{\#\textgreater{} [2,]  1.120637e+01}

\CommentTok{\# Plot convergence of SSE over iterations}
\NormalTok{sse\_df }\OtherTok{\textless{}{-}}
    \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{Iteration =} \FunctionTok{seq\_along}\NormalTok{(sse\_values), }\AttributeTok{SSE =}\NormalTok{ sse\_values)}

\FunctionTok{ggplot}\NormalTok{(sse\_df, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Iteration, }\AttributeTok{y =}\NormalTok{ SSE)) }\SpecialCharTok{+}
    \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{color =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{linewidth =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Levenberg{-}Marquardt Convergence"}\NormalTok{,}
         \AttributeTok{x =} \StringTok{"Iteration"}\NormalTok{,}
         \AttributeTok{y =} \StringTok{"SSE"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{06-nonlinear-regession_files/figure-latex/unnamed-chunk-5-1} \end{center}

\begin{itemize}
\item
  Early Stability (Flat SSE)

  \begin{itemize}
  \item
    The SSE remains near zero for the first few iterations, which suggests that the algorithm is initially behaving stably.
  \item
    This might indicate that the initial parameter guess is reasonable, or that the updates are too small to significantly affect SSE.
  \end{itemize}
\item
  Sudden Explosion in SSE (Iteration \textasciitilde8-9)

  \begin{itemize}
  \item
    The sharp spike in SSE at iteration 9 indicates a numerical instability or divergence in the optimization process.
  \item
    This could be due to:

    \begin{itemize}
    \item
      An ill-conditioned Jacobian matrix: The step direction is poorly estimated, leading to an unstable jump.
    \item
      A sudden large update (\texttt{delta}): The damping parameter (\texttt{tau}) might have been reduced too aggressively, causing an uncontrolled step.
    \item
      Floating-point issues: If \texttt{A} becomes nearly singular, solving \texttt{A\ \textbackslash{}\ delta\ =\ residuals} may produce excessively large values.
    \end{itemize}
  \end{itemize}
\item
  Return to Stability (After Iteration 9)

  \begin{itemize}
  \item
    The SSE immediately returns to a low value after the spike, which suggests that the damping parameter (\texttt{tau}) might have been increased after detecting instability.
  \item
    This is consistent with the adaptive nature of Levenberg-Marquardt:

    \begin{itemize}
    \item
      If a step leads to a bad SSE increase, the algorithm increases \texttt{tau} to make the next step more conservative.
    \item
      If the next step stabilizes, \texttt{tau} may be reduced again.
    \end{itemize}
  \end{itemize}
\end{itemize}

\hypertarget{newton-raphson}{%
\subsubsection{Newton-Raphson Algorithm}\label{newton-raphson}}

The Newton-Raphson method is a second-order optimization technique used for nonlinear least squares problems. Unlike first-order methods (such as \protect\hyperlink{steepest-descent}{Steepest Descent (Gradient Descent)} and \protect\hyperlink{gauss-newton-algorithm}{Gauss-Newton Algorithm}), Newton-Raphson uses both first and second derivatives of the objective function for faster convergence.

The update rule is:

\[
\hat{\theta}^{(j+1)} = \hat{\theta}^{(j)} - \alpha_j \left[\frac{\partial^2 Q(\hat{\theta}^{(j)})}{\partial \theta \partial \theta'}\right]^{-1} \frac{\partial Q(\hat{\theta}^{(j)})}{\partial \theta}
\]

where:

\begin{itemize}
\tightlist
\item
  \(\frac{\partial Q(\hat{\theta}^{(j)})}{\partial \theta}\) is the \textbf{gradient vector} (first derivative of the objective function).
\item
  \(\frac{\partial^2 Q(\hat{\theta}^{(j)})}{\partial \theta \partial \theta'}\) is the \textbf{Hessian matrix} (second derivative of the objective function).
\item
  \(\alpha_j\) is the \textbf{learning rate}, controlling step size.
\end{itemize}

The \textbf{Hessian matrix} in nonlinear least squares problems is:

\[
\frac{\partial^2 Q(\hat{\theta}^{(j)})}{\partial \theta \partial \theta'} = 2 \mathbf{F}(\hat{\theta}^{(j)})' \mathbf{F}(\hat{\theta}^{(j)}) - 2\sum_{i=1}^{n} [Y_i - f(x_i;\theta)] \frac{\partial^2 f(x_i;\theta)}{\partial \theta \partial \theta'}
\]

where:

\begin{itemize}
\tightlist
\item
  The first term \(2 \mathbf{F}(\hat{\theta}^{(j)})' \mathbf{F}(\hat{\theta}^{(j)})\) is the same as in the \protect\hyperlink{gauss-newton-algorithm}{Gauss-Newton Algorithm}.
\item
  The second term \(-2\sum_{i=1}^{n} [Y_i - f(x_i;\theta)] \frac{\partial^2 f(x_i;\theta)}{\partial \theta \partial \theta'}\) contains second-order derivatives.
\end{itemize}

\textbf{Key Observations}

\begin{itemize}
\tightlist
\item
  Gauss-Newton vs.~Newton-Raphson:

  \begin{itemize}
  \tightlist
  \item
    Gauss-Newton approximates the Hessian by ignoring the second term.
  \item
    Newton-Raphson explicitly incorporates second-order derivatives, making it more precise but computationally expensive.
  \end{itemize}
\item
  Challenges:

  \begin{itemize}
  \tightlist
  \item
    The Hessian matrix may be singular, making it impossible to invert.
  \item
    Computing second derivatives is often difficult for complex functions.
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load required libraries}
\FunctionTok{library}\NormalTok{(ggplot2)}

\CommentTok{\# Define a nonlinear function (exponential model)}
\NormalTok{nonlinear\_model }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x) \{}
\NormalTok{    theta[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(theta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ x)}
\NormalTok{\}}

\CommentTok{\# Define SSE function}
\NormalTok{sse }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x, y) \{}
    \FunctionTok{sum}\NormalTok{((y }\SpecialCharTok{{-}} \FunctionTok{nonlinear\_model}\NormalTok{(theta, x)) }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{)}
\NormalTok{\}}

\CommentTok{\# Compute Gradient (First Derivative) of SSE}
\NormalTok{gradient\_sse }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x, y) \{}
\NormalTok{    residuals }\OtherTok{\textless{}{-}}\NormalTok{ y }\SpecialCharTok{{-}} \FunctionTok{nonlinear\_model}\NormalTok{(theta, x)}
    
    \CommentTok{\# Partial derivative w.r.t theta\_1}
\NormalTok{    grad\_1 }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\DecValTok{2} \SpecialCharTok{*} \FunctionTok{sum}\NormalTok{(residuals }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(theta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ x))}
    
    \CommentTok{\# Partial derivative w.r.t theta\_2}
\NormalTok{    grad\_2 }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\DecValTok{2} \SpecialCharTok{*} \FunctionTok{sum}\NormalTok{(residuals }\SpecialCharTok{*}\NormalTok{ theta[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ x }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(theta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ x))}
    
    \FunctionTok{return}\NormalTok{(}\FunctionTok{c}\NormalTok{(grad\_1, grad\_2))}
\NormalTok{\}}

\CommentTok{\# Compute Hessian (Second Derivative) of SSE}
\NormalTok{hessian\_sse }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x, y) \{}
\NormalTok{    residuals }\OtherTok{\textless{}{-}}\NormalTok{ y }\SpecialCharTok{{-}} \FunctionTok{nonlinear\_model}\NormalTok{(theta, x)}
    
    \CommentTok{\# Compute second derivatives}
\NormalTok{    H\_11 }\OtherTok{\textless{}{-}} \DecValTok{2} \SpecialCharTok{*} \FunctionTok{sum}\NormalTok{(}\FunctionTok{exp}\NormalTok{(}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ theta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ x))}
\NormalTok{    H\_12 }\OtherTok{\textless{}{-}} \DecValTok{2} \SpecialCharTok{*} \FunctionTok{sum}\NormalTok{(x }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ theta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ x) }\SpecialCharTok{*}\NormalTok{ theta[}\DecValTok{1}\NormalTok{])}
\NormalTok{    H\_21 }\OtherTok{\textless{}{-}}\NormalTok{ H\_12}
    
\NormalTok{    term1 }\OtherTok{\textless{}{-}} \DecValTok{2} \SpecialCharTok{*} \FunctionTok{sum}\NormalTok{((x }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{) }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ theta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ x) }\SpecialCharTok{*}\NormalTok{ theta[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{)}
\NormalTok{    term2 }\OtherTok{\textless{}{-}} \DecValTok{2} \SpecialCharTok{*} \FunctionTok{sum}\NormalTok{(residuals }\SpecialCharTok{*}\NormalTok{ (x }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{) }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(theta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ x))}
    
\NormalTok{    H\_22 }\OtherTok{\textless{}{-}}\NormalTok{ term1 }\SpecialCharTok{{-}}\NormalTok{ term2}
    
    \FunctionTok{return}\NormalTok{(}\FunctionTok{matrix}\NormalTok{(}
        \FunctionTok{c}\NormalTok{(H\_11, H\_12, H\_21, H\_22),}
        \AttributeTok{nrow =} \DecValTok{2}\NormalTok{,}
        \AttributeTok{byrow =} \ConstantTok{TRUE}
\NormalTok{    ))}
\NormalTok{\}}

\CommentTok{\# Generate synthetic data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{100}\NormalTok{)}
\NormalTok{true\_theta }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\FloatTok{0.3}\NormalTok{)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{nonlinear\_model}\NormalTok{(true\_theta, x) }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{length}\NormalTok{(x), }\AttributeTok{sd =} \FloatTok{0.5}\NormalTok{)}

\CommentTok{\# Newton{-}Raphson Optimization Implementation}
\NormalTok{newton\_raphson }\OtherTok{\textless{}{-}}
    \ControlFlowTok{function}\NormalTok{(theta\_init,}
\NormalTok{             x,}
\NormalTok{             y,}
             \AttributeTok{tol =} \FloatTok{1e{-}6}\NormalTok{,}
             \AttributeTok{max\_iter =} \DecValTok{500}\NormalTok{) \{}
\NormalTok{        theta }\OtherTok{\textless{}{-}}\NormalTok{ theta\_init}
\NormalTok{        sse\_values }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(max\_iter)}
        
        \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{max\_iter) \{}
\NormalTok{            grad }\OtherTok{\textless{}{-}} \FunctionTok{gradient\_sse}\NormalTok{(theta, x, y)}
\NormalTok{            hessian }\OtherTok{\textless{}{-}} \FunctionTok{hessian\_sse}\NormalTok{(theta, x, y)}
            
            \CommentTok{\# Check if Hessian is invertible}
            \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{det}\NormalTok{(hessian) }\SpecialCharTok{==} \DecValTok{0}\NormalTok{) \{}
                \FunctionTok{cat}\NormalTok{(}\StringTok{"Hessian is singular at iteration"}\NormalTok{,}
\NormalTok{                    j,}
                    \StringTok{"{-} Using identity matrix instead.}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
                \CommentTok{\# Replace with identity matrix if singular}
\NormalTok{                hessian }\OtherTok{\textless{}{-}}
                    \FunctionTok{diag}\NormalTok{(}\FunctionTok{length}\NormalTok{(theta))  }
\NormalTok{            \}}
            
            \CommentTok{\# Compute Newton update}
\NormalTok{            delta }\OtherTok{\textless{}{-}} \FunctionTok{solve}\NormalTok{(hessian) }\SpecialCharTok{\%*\%}\NormalTok{ grad}
\NormalTok{            theta\_new }\OtherTok{\textless{}{-}}\NormalTok{ theta }\SpecialCharTok{{-}}\NormalTok{ delta}
\NormalTok{            sse\_values[j] }\OtherTok{\textless{}{-}} \FunctionTok{sse}\NormalTok{(theta\_new, x, y)}
            
            \CommentTok{\# Check for convergence}
            \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{sum}\NormalTok{(}\FunctionTok{abs}\NormalTok{(delta)) }\SpecialCharTok{\textless{}}\NormalTok{ tol) \{}
                \FunctionTok{cat}\NormalTok{(}\StringTok{"Converged in"}\NormalTok{, j, }\StringTok{"iterations.}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
                \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{theta =}\NormalTok{ theta\_new, }\AttributeTok{sse\_values =}\NormalTok{ sse\_values[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{j]))}
\NormalTok{            \}}
            
\NormalTok{            theta }\OtherTok{\textless{}{-}}\NormalTok{ theta\_new}
\NormalTok{        \}}
        
        \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{theta =}\NormalTok{ theta, }\AttributeTok{sse\_values =}\NormalTok{ sse\_values))}
\NormalTok{    \}}

\CommentTok{\# Run Newton{-}Raphson}
\NormalTok{theta\_init }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{0.1}\NormalTok{)  }\CommentTok{\# Initial guess}
\NormalTok{result }\OtherTok{\textless{}{-}} \FunctionTok{newton\_raphson}\NormalTok{(theta\_init, x, y)}
\CommentTok{\#\textgreater{} Converged in 222 iterations.}

\CommentTok{\# Extract results}
\NormalTok{estimated\_theta }\OtherTok{\textless{}{-}}\NormalTok{ result}\SpecialCharTok{$}\NormalTok{theta}
\NormalTok{sse\_values }\OtherTok{\textless{}{-}}\NormalTok{ result}\SpecialCharTok{$}\NormalTok{sse\_values}

\CommentTok{\# Display estimated parameters}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Estimated parameters (A, B) using Newton{-}Raphson:}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Estimated parameters (A, B) using Newton{-}Raphson:}
\FunctionTok{print}\NormalTok{(estimated\_theta)}
\CommentTok{\#\textgreater{}           [,1]}
\CommentTok{\#\textgreater{} [1,] 1.9934188}
\CommentTok{\#\textgreater{} [2,] 0.3008742}

\CommentTok{\# Plot convergence of SSE over iterations}
\NormalTok{sse\_df }\OtherTok{\textless{}{-}}
    \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{Iteration =} \FunctionTok{seq\_along}\NormalTok{(sse\_values), }\AttributeTok{SSE =}\NormalTok{ sse\_values)}

\FunctionTok{ggplot}\NormalTok{(sse\_df, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Iteration, }\AttributeTok{y =}\NormalTok{ SSE)) }\SpecialCharTok{+}
    \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{color =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{size =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Newton{-}Raphson Convergence"}\NormalTok{,}
         \AttributeTok{x =} \StringTok{"Iteration"}\NormalTok{,}
         \AttributeTok{y =} \StringTok{"SSE"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{06-nonlinear-regession_files/figure-latex/unnamed-chunk-6-1} \end{center}

\hypertarget{quasi-newton-method}{%
\subsubsection{Quasi-Newton Method}\label{quasi-newton-method}}

The Quasi-Newton method is an optimization technique that approximates Newton's method without requiring explicit computation of the Hessian matrix. Instead, it iteratively constructs an approximation \(\mathbf{H}_j\) to the Hessian based on the first derivative information.

The update rule is:

\[
\hat{\theta}^{(j+1)} = \hat{\theta}^{(j)} - \alpha_j \mathbf{H}_j^{-1}\frac{\partial \mathbf{Q}(\hat{\theta}^{(j)})}{\partial \theta}
\]

where:

\begin{itemize}
\tightlist
\item
  \(\mathbf{H}_j\) is a \textbf{symmetric positive definite approximation} to the \textbf{Hessian matrix}.
\item
  As \(j \to \infty\), \(\mathbf{H}_j\) gets closer to the \textbf{true Hessian}.
\item
  \(\frac{\partial Q(\hat{\theta}^{(j)})}{\partial \theta}\) is the \textbf{gradient vector}.
\item
  \(\alpha_j\) is the \textbf{learning rate}, controlling step size.
\end{itemize}

Why Use Quasi-Newton Instead of \protect\hyperlink{newton-raphson}{Newton-Raphson Method}?

\begin{itemize}
\tightlist
\item
  Newton-Raphson requires computing the Hessian matrix explicitly, which is computationally expensive and may be singular.
\item
  Quasi-Newton avoids computing the Hessian directly by approximating it iteratively.
\item
  Among first-order methods (which only require gradients, not Hessians), Quasi-Newton methods perform best.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Hessian Approximation}

Instead of directly computing the Hessian \(\mathbf{H}_j\), Quasi-Newton methods update an approximation \(\mathbf{H}_j\) iteratively.

One of the most widely used formulas is the \textbf{Broyden-Fletcher-Goldfarb-Shanno (BFGS) update}:

\[
\mathbf{H}_{j+1} = \mathbf{H}_j + \frac{(\mathbf{s}_j \mathbf{s}_j')}{\mathbf{s}_j' \mathbf{y}_j} - \frac{\mathbf{H}_j \mathbf{y}_j \mathbf{y}_j' \mathbf{H}_j}{\mathbf{y}_j' \mathbf{H}_j \mathbf{y}_j}
\]

where:

\begin{itemize}
\tightlist
\item
  \(\mathbf{s}_j = \hat{\theta}^{(j+1)} - \hat{\theta}^{(j)}\) (change in parameters).
\item
  \(\mathbf{y}_j = \nabla Q(\hat{\theta}^{(j+1)}) - \nabla Q(\hat{\theta}^{(j)})\) (change in gradient).
\item
  \(\mathbf{H}_j\) is the current \textbf{inverse Hessian approximation}.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load required libraries}
\FunctionTok{library}\NormalTok{(ggplot2)}

\CommentTok{\# Define a nonlinear function (exponential model)}
\NormalTok{nonlinear\_model }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x) \{}
\NormalTok{    theta[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(theta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ x)}
\NormalTok{\}}

\CommentTok{\# Define SSE function}
\NormalTok{sse }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x, y) \{}
    \FunctionTok{sum}\NormalTok{((y }\SpecialCharTok{{-}} \FunctionTok{nonlinear\_model}\NormalTok{(theta, x))}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{\}}

\CommentTok{\# Generate synthetic data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{100}\NormalTok{)}
\NormalTok{true\_theta }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\FloatTok{0.3}\NormalTok{)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{nonlinear\_model}\NormalTok{(true\_theta, x) }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{length}\NormalTok{(x), }\AttributeTok{sd =} \FloatTok{0.5}\NormalTok{)}

\CommentTok{\# Run BFGS Optimization using \textasciigrave{}optim()\textasciigrave{}}
\NormalTok{theta\_init }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{0.1}\NormalTok{)  }\CommentTok{\# Initial guess}
\NormalTok{result }\OtherTok{\textless{}{-}} \FunctionTok{optim}\NormalTok{(}
    \AttributeTok{par =}\NormalTok{ theta\_init,}
    \AttributeTok{fn =} \ControlFlowTok{function}\NormalTok{(theta) }\FunctionTok{sse}\NormalTok{(theta, x, y),  }\CommentTok{\# Minimize SSE}
    \AttributeTok{method =} \StringTok{"BFGS"}\NormalTok{,}
    \AttributeTok{control =} \FunctionTok{list}\NormalTok{(}\AttributeTok{trace =} \DecValTok{0}\NormalTok{)  }\CommentTok{\# Suppress optimization progress}
    \CommentTok{\# control = list(trace = 1, REPORT = 1)  \# Print optimization progress}
\NormalTok{)}

\CommentTok{\# Extract results}
\NormalTok{estimated\_theta }\OtherTok{\textless{}{-}}\NormalTok{ result}\SpecialCharTok{$}\NormalTok{par}
\NormalTok{sse\_final }\OtherTok{\textless{}{-}}\NormalTok{ result}\SpecialCharTok{$}\NormalTok{value}
\NormalTok{convergence\_status }\OtherTok{\textless{}{-}}\NormalTok{ result}\SpecialCharTok{$}\NormalTok{convergence  }\CommentTok{\# 0 means successful convergence}

\CommentTok{\# Display estimated parameters}
\FunctionTok{cat}\NormalTok{(}\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{=== Optimization Results ===}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} === Optimization Results ===}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Estimated parameters (A, B) using Quasi{-}Newton BFGS:}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Estimated parameters (A, B) using Quasi{-}Newton BFGS:}
\FunctionTok{print}\NormalTok{(estimated\_theta)}
\CommentTok{\#\textgreater{} [1] 1.9954216 0.3007569}

\CommentTok{\# Display final SSE}
\FunctionTok{cat}\NormalTok{(}\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{Final SSE:"}\NormalTok{, sse\_final, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Final SSE: 20.3227}
\end{Highlighting}
\end{Shaded}

\hypertarget{trust-region-reflective-algorithm}{%
\subsubsection{Trust-Region Reflective Algorithm}\label{trust-region-reflective-algorithm}}

The Trust-Region Reflective (TRR) algorithm is an optimization technique used for nonlinear least squares problems. Unlike Newton's method and gradient-based approaches, TRR dynamically restricts updates to a trust region, ensuring stability and preventing overshooting.

The goal is to minimize the objective function \(Q(\theta)\) (e.g., Sum of Squared Errors, SSE):

\[
\hat{\theta} = \arg\min_{\theta} Q(\theta)
\]

Instead of taking a full Newton step, TRR solves the following \textbf{quadratic subproblem}:

\[
\min_{\delta} m_j(\delta) = Q(\hat{\theta}^{(j)}) + \nabla Q(\hat{\theta}^{(j)})' \delta + \frac{1}{2} \delta' \mathbf{H}_j \delta
\]

subject to:

\[
\|\delta\| \leq \Delta_j
\]

where:

\begin{itemize}
\item
  \(\mathbf{H}_j\) is an \textbf{approximation of the Hessian matrix}.
\item
  \(\nabla Q(\hat{\theta}^{(j)})\) is the \textbf{gradient vector}.
\item
  \(\Delta_j\) is the \textbf{trust-region radius}, which is adjusted dynamically.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Trust-Region Adjustments}

The algorithm \textbf{modifies the step size dynamically} based on the ratio \(\rho_j\):

\[
\rho_j = \frac{Q(\hat{\theta}^{(j)}) - Q(\hat{\theta}^{(j)} + \delta)}{m_j(0) - m_j(\delta)}
\]

\begin{itemize}
\item
  \textbf{If} \(\rho_j > 0.75\) and \(\|\delta\| = \Delta_j\), then expand the trust region: \[
  \Delta_{j+1} = 2 \Delta_j
  \]
\item
  \textbf{If} \(\rho_j < 0.25\), shrink the trust region: \[
  \Delta_{j+1} = \frac{1}{2} \Delta_j
  \]
\item
  \textbf{If} \(\rho_j > 0\), accept the step; otherwise, reject it.
\end{itemize}

If a step \textbf{violates a constraint}, it is \textbf{reflected back} into the feasible region:

\[
\hat{\theta}^{(j+1)} = \max(\hat{\theta}^{(j)} + \delta, \theta_{\min})
\]

This ensures that the optimization \textbf{respects parameter bounds}.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load required libraries}
\FunctionTok{library}\NormalTok{(ggplot2)}


\CommentTok{\# Define a nonlinear function (exponential model)}
\NormalTok{nonlinear\_model }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x) \{}
\NormalTok{    theta[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(theta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ x)}
\NormalTok{\}}

\CommentTok{\# Define SSE function}
\NormalTok{sse }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x, y) \{}
    \FunctionTok{sum}\NormalTok{((y }\SpecialCharTok{{-}} \FunctionTok{nonlinear\_model}\NormalTok{(theta, x)) }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{)}
\NormalTok{\}}

\CommentTok{\# Compute Gradient (First Derivative) of SSE}
\NormalTok{gradient\_sse }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x, y) \{}
\NormalTok{    residuals }\OtherTok{\textless{}{-}}\NormalTok{ y }\SpecialCharTok{{-}} \FunctionTok{nonlinear\_model}\NormalTok{(theta, x)}
    
    \CommentTok{\# Partial derivative w.r.t theta\_1}
\NormalTok{    grad\_1 }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\DecValTok{2} \SpecialCharTok{*} \FunctionTok{sum}\NormalTok{(residuals }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(theta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ x))}
    
    \CommentTok{\# Partial derivative w.r.t theta\_2}
\NormalTok{    grad\_2 }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\DecValTok{2} \SpecialCharTok{*} \FunctionTok{sum}\NormalTok{(residuals }\SpecialCharTok{*}\NormalTok{ theta[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ x }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(theta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ x))}
    
    \FunctionTok{return}\NormalTok{(}\FunctionTok{c}\NormalTok{(grad\_1, grad\_2))}
\NormalTok{\}}

\CommentTok{\# Compute Hessian Approximation of SSE}
\NormalTok{hessian\_sse }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x, y) \{}
\NormalTok{    residuals }\OtherTok{\textless{}{-}}\NormalTok{ y }\SpecialCharTok{{-}} \FunctionTok{nonlinear\_model}\NormalTok{(theta, x)}
    
    \CommentTok{\# Compute second derivatives}
\NormalTok{    H\_11 }\OtherTok{\textless{}{-}} \DecValTok{2} \SpecialCharTok{*} \FunctionTok{sum}\NormalTok{(}\FunctionTok{exp}\NormalTok{(}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ theta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ x))}
\NormalTok{    H\_12 }\OtherTok{\textless{}{-}} \DecValTok{2} \SpecialCharTok{*} \FunctionTok{sum}\NormalTok{(x }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ theta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ x) }\SpecialCharTok{*}\NormalTok{ theta[}\DecValTok{1}\NormalTok{])}
\NormalTok{    H\_21 }\OtherTok{\textless{}{-}}\NormalTok{ H\_12}
    
\NormalTok{    term1 }\OtherTok{\textless{}{-}} \DecValTok{2} \SpecialCharTok{*} \FunctionTok{sum}\NormalTok{((x }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{) }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ theta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ x) }\SpecialCharTok{*}\NormalTok{ theta[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{)}
\NormalTok{    term2 }\OtherTok{\textless{}{-}} \DecValTok{2} \SpecialCharTok{*} \FunctionTok{sum}\NormalTok{(residuals }\SpecialCharTok{*}\NormalTok{ (x }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{) }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(theta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ x))}
    
\NormalTok{    H\_22 }\OtherTok{\textless{}{-}}\NormalTok{ term1 }\SpecialCharTok{{-}}\NormalTok{ term2}
    
    \FunctionTok{return}\NormalTok{(}\FunctionTok{matrix}\NormalTok{(}
        \FunctionTok{c}\NormalTok{(H\_11, H\_12, H\_21, H\_22),}
        \AttributeTok{nrow =} \DecValTok{2}\NormalTok{,}
        \AttributeTok{byrow =} \ConstantTok{TRUE}
\NormalTok{    ))}
\NormalTok{\}}

\CommentTok{\# Generate synthetic data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{100}\NormalTok{)}
\NormalTok{true\_theta }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\FloatTok{0.3}\NormalTok{)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{nonlinear\_model}\NormalTok{(true\_theta, x) }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{length}\NormalTok{(x), }\AttributeTok{sd =} \FloatTok{0.5}\NormalTok{)}

\CommentTok{\# Manual Trust{-}Region Reflective Optimization Implementation}
\NormalTok{trust\_region\_reflective }\OtherTok{\textless{}{-}}
    \ControlFlowTok{function}\NormalTok{(theta\_init,}
\NormalTok{             x,}
\NormalTok{             y,}
             \AttributeTok{tol =} \FloatTok{1e{-}6}\NormalTok{,}
             \AttributeTok{max\_iter =} \DecValTok{500}\NormalTok{,}
             \AttributeTok{delta\_max =} \FloatTok{1.0}\NormalTok{) \{}
\NormalTok{        theta }\OtherTok{\textless{}{-}}\NormalTok{ theta\_init}
\NormalTok{        delta\_j }\OtherTok{\textless{}{-}} \FloatTok{0.5}  \CommentTok{\# Initial trust{-}region size}
\NormalTok{        n }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(theta)}
\NormalTok{        sse\_values }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(max\_iter)}
        
        \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{max\_iter) \{}
\NormalTok{            grad }\OtherTok{\textless{}{-}} \FunctionTok{gradient\_sse}\NormalTok{(theta, x, y)}
\NormalTok{            hessian }\OtherTok{\textless{}{-}} \FunctionTok{hessian\_sse}\NormalTok{(theta, x, y)}
            
            \CommentTok{\# Check if Hessian is invertible}
            \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{det}\NormalTok{(hessian) }\SpecialCharTok{==} \DecValTok{0}\NormalTok{) \{}
                \FunctionTok{cat}\NormalTok{(}\StringTok{"Hessian is singular at iteration"}\NormalTok{,}
\NormalTok{                    j,}
                    \StringTok{"{-} Using identity matrix instead.}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\NormalTok{                hessian }\OtherTok{\textless{}{-}}
                    \FunctionTok{diag}\NormalTok{(n)  }\CommentTok{\# Replace with identity matrix if singular}
\NormalTok{            \}}
            
            \CommentTok{\# Compute Newton step}
\NormalTok{            delta\_full }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\FunctionTok{solve}\NormalTok{(hessian) }\SpecialCharTok{\%*\%}\NormalTok{ grad}
            
            \CommentTok{\# Apply trust{-}region constraint}
            \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{sum}\NormalTok{(delta\_full }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{)) }\SpecialCharTok{\textgreater{}}\NormalTok{ delta\_j) \{}
                \CommentTok{\# Scale step}
\NormalTok{                delta }\OtherTok{\textless{}{-}}
\NormalTok{                    (delta\_j }\SpecialCharTok{/} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{sum}\NormalTok{(delta\_full }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{))) }\SpecialCharTok{*}\NormalTok{ delta\_full  }
\NormalTok{            \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{                delta }\OtherTok{\textless{}{-}}\NormalTok{ delta\_full}
\NormalTok{            \}}
            
            \CommentTok{\# Compute new theta and ensure it respects constraints}
\NormalTok{            theta\_new }\OtherTok{\textless{}{-}}
                \FunctionTok{pmax}\NormalTok{(theta }\SpecialCharTok{+}\NormalTok{ delta, }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\SpecialCharTok{{-}}\ConstantTok{Inf}\NormalTok{))  }\CommentTok{\# Reflect to lower bound}
\NormalTok{            sse\_new }\OtherTok{\textless{}{-}} \FunctionTok{sse}\NormalTok{(theta\_new, x, y)}
            
            \CommentTok{\# Compute agreement ratio (rho\_j)}
\NormalTok{            predicted\_reduction }\OtherTok{\textless{}{-}}
                \SpecialCharTok{{-}}\FunctionTok{t}\NormalTok{(grad) }\SpecialCharTok{\%*\%}\NormalTok{ delta }\SpecialCharTok{{-}} \FloatTok{0.5} \SpecialCharTok{*} \FunctionTok{t}\NormalTok{(delta) }\SpecialCharTok{\%*\%}\NormalTok{ hessian }\SpecialCharTok{\%*\%}\NormalTok{ delta}
\NormalTok{            actual\_reduction }\OtherTok{\textless{}{-}} \FunctionTok{sse}\NormalTok{(theta, x, y) }\SpecialCharTok{{-}}\NormalTok{ sse\_new}
\NormalTok{            rho\_j }\OtherTok{\textless{}{-}}\NormalTok{ actual\_reduction }\SpecialCharTok{/}\NormalTok{ predicted\_reduction}
            
            \CommentTok{\# Adjust trust region size}
            \ControlFlowTok{if}\NormalTok{ (rho\_j }\SpecialCharTok{\textless{}} \FloatTok{0.25}\NormalTok{) \{}
\NormalTok{                delta\_j }\OtherTok{\textless{}{-}} \FunctionTok{max}\NormalTok{(delta\_j }\SpecialCharTok{/} \DecValTok{2}\NormalTok{, }\FloatTok{1e{-}4}\NormalTok{)  }\CommentTok{\# Shrink}
\NormalTok{            \} }\ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{ (rho\_j }\SpecialCharTok{\textgreater{}} \FloatTok{0.75} \SpecialCharTok{\&\&}
                       \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{sum}\NormalTok{(delta }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{)) }\SpecialCharTok{==}\NormalTok{ delta\_j) \{}
\NormalTok{                delta\_j }\OtherTok{\textless{}{-}} \FunctionTok{min}\NormalTok{(}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ delta\_j, delta\_max)  }\CommentTok{\# Expand}
\NormalTok{            \}}
            
            \CommentTok{\# Accept or reject step}
            \ControlFlowTok{if}\NormalTok{ (rho\_j }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{) \{}
\NormalTok{                theta }\OtherTok{\textless{}{-}}\NormalTok{ theta\_new  }\CommentTok{\# Accept step}
\NormalTok{            \} }\ControlFlowTok{else}\NormalTok{ \{}
                \FunctionTok{cat}\NormalTok{(}\StringTok{"Step rejected at iteration"}\NormalTok{, j, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\NormalTok{            \}}
            
\NormalTok{            sse\_values[j] }\OtherTok{\textless{}{-}} \FunctionTok{sse}\NormalTok{(theta, x, y)}
            
            \CommentTok{\# Check for convergence}
            \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{sum}\NormalTok{(}\FunctionTok{abs}\NormalTok{(delta)) }\SpecialCharTok{\textless{}}\NormalTok{ tol) \{}
                \FunctionTok{cat}\NormalTok{(}\StringTok{"Converged in"}\NormalTok{, j, }\StringTok{"iterations.}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
                \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{theta =}\NormalTok{ theta, }\AttributeTok{sse\_values =}\NormalTok{ sse\_values[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{j]))}
\NormalTok{            \}}
\NormalTok{        \}}
        
        \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{theta =}\NormalTok{ theta, }\AttributeTok{sse\_values =}\NormalTok{ sse\_values))}
\NormalTok{    \}}

\CommentTok{\# Run Manual Trust{-}Region Reflective Algorithm}
\NormalTok{theta\_init }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{0.1}\NormalTok{)  }\CommentTok{\# Initial guess}
\NormalTok{result }\OtherTok{\textless{}{-}} \FunctionTok{trust\_region\_reflective}\NormalTok{(theta\_init, x, y)}

\CommentTok{\# Extract results}
\NormalTok{estimated\_theta }\OtherTok{\textless{}{-}}\NormalTok{ result}\SpecialCharTok{$}\NormalTok{theta}
\NormalTok{sse\_values }\OtherTok{\textless{}{-}}\NormalTok{ result}\SpecialCharTok{$}\NormalTok{sse\_values}

\CommentTok{\# Plot convergence of SSE over iterations}
\NormalTok{sse\_df }\OtherTok{\textless{}{-}}
    \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{Iteration =} \FunctionTok{seq\_along}\NormalTok{(sse\_values), }\AttributeTok{SSE =}\NormalTok{ sse\_values)}

\FunctionTok{ggplot}\NormalTok{(sse\_df, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Iteration, }\AttributeTok{y =}\NormalTok{ SSE)) }\SpecialCharTok{+}
    \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{color =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{size =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Trust{-}Region Reflective Convergence"}\NormalTok{,}
         \AttributeTok{x =} \StringTok{"Iteration"}\NormalTok{,}
         \AttributeTok{y =} \StringTok{"SSE"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{06-nonlinear-regession_files/figure-latex/unnamed-chunk-8-1} \end{center}

\hypertarget{derivative-free}{%
\subsection{Derivative-Free}\label{derivative-free}}

\hypertarget{secant-method}{%
\subsubsection{Secant Method}\label{secant-method}}

The Secant Method is a root-finding algorithm that approximates the derivative using finite differences, making it a derivative-free alternative to Newton's method. It is particularly useful when the exact gradient (or Jacobian in the case of optimization problems) is unavailable or expensive to compute.

In nonlinear optimization, we apply the Secant Method to iteratively refine parameter estimates without explicitly computing second-order derivatives.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

In one dimension, the Secant Method approximates the derivative as:

\[
f'(\theta) \approx \frac{f(\theta_{j}) - f(\theta_{j-1})}{\theta_{j} - \theta_{j-1}}.
\]

Using this approximation, the \textbf{update step} in the Secant Method follows:

\[
\theta_{j+1} = \theta_j - f(\theta_j) \frac{\theta_j - \theta_{j-1}}{f(\theta_j) - f(\theta_{j-1})}.
\]

Instead of computing the exact derivative (as in Newton's method), we use the difference between the last two iterates to approximate it. This makes the Secant Method \textbf{more efficient} in cases where gradient computation is expensive or infeasible.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

In \textbf{higher dimensions}, the Secant Method extends to an approximate \protect\hyperlink{quasi-newton-method}{Quasi-Newton Method}, often referred to as \textbf{Broyden's Method}. We iteratively approximate the inverse Hessian matrix using past updates.

The update formula for a vector-valued function \(F(\theta)\) is:

\[
\theta^{(j+1)} = \theta^{(j)} - \mathbf{B}^{(j)} F(\theta^{(j)}),
\]

where \(\mathbf{B}^{(j)}\) is an approximation of the inverse Jacobian matrix, updated at each step using:

\[
\mathbf{B}^{(j+1)} = \mathbf{B}^{(j)} + \frac{(\Delta \theta^{(j)} - \mathbf{B}^{(j)} \Delta F^{(j)}) (\Delta \theta^{(j)})'}{(\Delta \theta^{(j)})' \Delta F^{(j)}},
\]

where:

\begin{itemize}
\tightlist
\item
  \(\Delta \theta^{(j)} = \theta^{(j+1)} - \theta^{(j)}\),
\item
  \(\Delta F^{(j)} = F(\theta^{(j+1)}) - F(\theta^{(j)})\).
\end{itemize}

This \textbf{secant-based update} approximates the behavior of the \textbf{true Jacobian inverse}, reducing computational cost compared to full Newton's method.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Algorithm: Secant Method for Nonlinear Optimization}

The \textbf{Secant Method} for nonlinear optimization follows these steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Initialize parameters \(\theta^{(0)}\) and \(\theta^{(1)}\) (two starting points).
\item
  Compute the function values \(F(\theta^{(0)})\) and \(F(\theta^{(1)})\).
\item
  Use the Secant update formula to compute the next iterate \(\theta^{(j+1)}\).
\item
  Update the approximate inverse Jacobian \(\mathbf{B}^{(j)}\).
\item
  Repeat until convergence.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load required libraries}
\FunctionTok{library}\NormalTok{(numDeriv)}


\CommentTok{\# Define a nonlinear function (logistic model)}
\NormalTok{nonlinear\_model }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x) \{}
    \FunctionTok{return}\NormalTok{(theta[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{theta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ (x }\SpecialCharTok{{-}}\NormalTok{ theta[}\DecValTok{3}\NormalTok{]))))}
\NormalTok{\}}

\CommentTok{\# Define the Sum of Squared Errors (SSE) function}
\NormalTok{sse }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x, y) \{}
    \FunctionTok{return}\NormalTok{(}\FunctionTok{sum}\NormalTok{((y }\SpecialCharTok{{-}} \FunctionTok{nonlinear\_model}\NormalTok{(theta, x)) }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{))}
\NormalTok{\}}

\CommentTok{\# Generate synthetic data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{100}\NormalTok{)}
\NormalTok{true\_theta }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\FloatTok{1.5}\NormalTok{, }\DecValTok{0}\NormalTok{)  }\CommentTok{\# True parameters (A, B, C)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{nonlinear\_model}\NormalTok{(true\_theta, x) }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{length}\NormalTok{(x), }\AttributeTok{sd =} \FloatTok{0.3}\NormalTok{)}

\CommentTok{\# Improved Secant Method with Line Search}
\NormalTok{secant\_method\_improved }\OtherTok{\textless{}{-}}
    \ControlFlowTok{function}\NormalTok{(theta0,}
\NormalTok{             theta1,}
\NormalTok{             x,}
\NormalTok{             y,}
             \AttributeTok{tol =} \FloatTok{1e{-}6}\NormalTok{,}
             \AttributeTok{max\_iter =} \DecValTok{100}\NormalTok{) \{}
\NormalTok{        theta\_prev }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(theta0)  }\CommentTok{\# Convert to column vector}
\NormalTok{        theta\_curr }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(theta1)}
        
\NormalTok{        alpha }\OtherTok{\textless{}{-}} \DecValTok{1}  \CommentTok{\# Initial step size}
\NormalTok{        step\_reduction\_factor }\OtherTok{\textless{}{-}} \FloatTok{0.5}  \CommentTok{\# Reduce step if SSE increases}
\NormalTok{        B\_inv }\OtherTok{\textless{}{-}} \FunctionTok{diag}\NormalTok{(}\FunctionTok{length}\NormalTok{(theta0))  }\CommentTok{\# Identity matrix initialization}
        
        \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{max\_iter) \{}
            \CommentTok{\# Compute function values using numerical gradient}
\NormalTok{            F\_prev }\OtherTok{\textless{}{-}}
                \FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{grad}\NormalTok{(}\ControlFlowTok{function}\NormalTok{(theta)}
                    \FunctionTok{sse}\NormalTok{(theta, x, y), theta\_prev))}
\NormalTok{            F\_curr }\OtherTok{\textless{}{-}}
                \FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{grad}\NormalTok{(}\ControlFlowTok{function}\NormalTok{(theta)}
                    \FunctionTok{sse}\NormalTok{(theta, x, y), theta\_curr))}
            
            \CommentTok{\# Compute secant step update (convert to column vectors)}
\NormalTok{            delta\_theta }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(theta\_curr }\SpecialCharTok{{-}}\NormalTok{ theta\_prev)}
\NormalTok{            delta\_F }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(F\_curr }\SpecialCharTok{{-}}\NormalTok{ F\_prev)}
            
            \CommentTok{\# Prevent division by zero}
            \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{sum}\NormalTok{(}\FunctionTok{abs}\NormalTok{(delta\_F)) }\SpecialCharTok{\textless{}} \FloatTok{1e{-}8}\NormalTok{) \{}
                \FunctionTok{cat}\NormalTok{(}\StringTok{"Gradient diff is too small, stopping optimization.}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
                \ControlFlowTok{break}
\NormalTok{            \}}
            
            \CommentTok{\# Ensure correct dimensions for Broyden update}
\NormalTok{            numerator }\OtherTok{\textless{}{-}}
\NormalTok{                (delta\_theta }\SpecialCharTok{{-}}\NormalTok{ B\_inv }\SpecialCharTok{\%*\%}\NormalTok{ delta\_F) }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(delta\_theta)}
            \CommentTok{\# Convert scalar to numeric}
\NormalTok{            denominator }\OtherTok{\textless{}{-}}
                \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{t}\NormalTok{(delta\_theta) }\SpecialCharTok{\%*\%}\NormalTok{ delta\_F)  }
            
            \CommentTok{\# Updated inverse Jacobian approximation}
\NormalTok{            B\_inv }\OtherTok{\textless{}{-}}
\NormalTok{                B\_inv }\SpecialCharTok{+}\NormalTok{ numerator }\SpecialCharTok{/}\NormalTok{ denominator  }
            
            \CommentTok{\# Compute next theta using secant update}
\NormalTok{            theta\_next }\OtherTok{\textless{}{-}}\NormalTok{ theta\_curr }\SpecialCharTok{{-}}\NormalTok{ alpha }\SpecialCharTok{*}\NormalTok{ B\_inv }\SpecialCharTok{\%*\%}\NormalTok{ F\_curr}
            
            \CommentTok{\# Line search: Reduce step size if SSE increases}
            \ControlFlowTok{while}\NormalTok{ (}\FunctionTok{sse}\NormalTok{(}\FunctionTok{as.vector}\NormalTok{(theta\_next), x, y) }\SpecialCharTok{\textgreater{}} \FunctionTok{sse}\NormalTok{(}\FunctionTok{as.vector}\NormalTok{(theta\_curr), }
\NormalTok{                                                          x, y)) \{}
\NormalTok{                alpha }\OtherTok{\textless{}{-}}\NormalTok{ alpha }\SpecialCharTok{*}\NormalTok{ step\_reduction\_factor}
\NormalTok{                theta\_next }\OtherTok{\textless{}{-}}\NormalTok{ theta\_curr }\SpecialCharTok{{-}}\NormalTok{ alpha }\SpecialCharTok{*}\NormalTok{ B\_inv }\SpecialCharTok{\%*\%}\NormalTok{ F\_curr}
                
                \CommentTok{\# Print progress}
                \CommentTok{\# cat("Reducing step size to", alpha, "\textbackslash{}n")}
\NormalTok{            \}}
            
            \CommentTok{\# Print intermediate results for debugging}
            \FunctionTok{cat}\NormalTok{(}
                \FunctionTok{sprintf}\NormalTok{(}
                    \StringTok{"Iteration \%d: Theta = (\%.4f, \%.4f, \%.4f), Alpha = \%.4f}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{,}
\NormalTok{                    j,}
\NormalTok{                    theta\_next[}\DecValTok{1}\NormalTok{],}
\NormalTok{                    theta\_next[}\DecValTok{2}\NormalTok{],}
\NormalTok{                    theta\_next[}\DecValTok{3}\NormalTok{],}
\NormalTok{                    alpha}
\NormalTok{                )}
\NormalTok{            )}
            
            \CommentTok{\# Check convergence}
            \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{sum}\NormalTok{(}\FunctionTok{abs}\NormalTok{(theta\_next }\SpecialCharTok{{-}}\NormalTok{ theta\_curr)) }\SpecialCharTok{\textless{}}\NormalTok{ tol) \{}
                \FunctionTok{cat}\NormalTok{(}\StringTok{"Converged successfully.}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
                \ControlFlowTok{break}
\NormalTok{            \}}
            
            \CommentTok{\# Update iterates}
\NormalTok{            theta\_prev }\OtherTok{\textless{}{-}}\NormalTok{ theta\_curr}
\NormalTok{            theta\_curr }\OtherTok{\textless{}{-}}\NormalTok{ theta\_next}
\NormalTok{        \}}
        
        \FunctionTok{return}\NormalTok{(}\FunctionTok{as.vector}\NormalTok{(theta\_curr))  }\CommentTok{\# Convert back to numeric vector}
\NormalTok{    \}}

\CommentTok{\# Adjusted initial parameter guesses}
\NormalTok{theta0 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\FloatTok{0.8}\NormalTok{,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{)   }\CommentTok{\# Closer to true parameters}
\NormalTok{theta1 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\FloatTok{1.2}\NormalTok{,}\SpecialCharTok{{-}}\FloatTok{0.5}\NormalTok{)  }\CommentTok{\# Slightly refined}

\CommentTok{\# Run Improved Secant Method}
\NormalTok{estimated\_theta }\OtherTok{\textless{}{-}} \FunctionTok{secant\_method\_improved}\NormalTok{(theta0, theta1, x, y)}
\CommentTok{\#\textgreater{} Iteration 1: Theta = (3.8521, 1.3054, 0.0057), Alpha = 0.0156}
\CommentTok{\#\textgreater{} Iteration 2: Theta = (3.8521, 1.3054, 0.0057), Alpha = 0.0000}
\CommentTok{\#\textgreater{} Converged successfully.}

\CommentTok{\# Display estimated parameters}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Estimated parameters (A, B, C) using Secant Method:}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Estimated parameters (A, B, C) using Secant Method:}
\FunctionTok{print}\NormalTok{(estimated\_theta)}
\CommentTok{\#\textgreater{} [1] 3.85213912 1.30538435 0.00566417}

\CommentTok{\# Plot data and fitted curve}
\FunctionTok{plot}\NormalTok{(}
\NormalTok{    x,}
\NormalTok{    y,}
    \AttributeTok{main =} \StringTok{"Secant Method: Data \& Fitted Curve"}\NormalTok{,}
    \AttributeTok{pch =} \DecValTok{19}\NormalTok{,}
    \AttributeTok{cex =} \FloatTok{0.5}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"x"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"y"}
\NormalTok{)}
\FunctionTok{curve}\NormalTok{(}
    \FunctionTok{nonlinear\_model}\NormalTok{(estimated\_theta, x),}
    \AttributeTok{from =} \SpecialCharTok{{-}}\DecValTok{5}\NormalTok{,}
    \AttributeTok{to =} \DecValTok{5}\NormalTok{,}
    \AttributeTok{add =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{col =} \StringTok{"red"}\NormalTok{,}
    \AttributeTok{lwd =} \DecValTok{2}
\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}
    \StringTok{"topleft"}\NormalTok{,}
    \AttributeTok{legend =} \FunctionTok{c}\NormalTok{(}\StringTok{"Data"}\NormalTok{, }\StringTok{"Fitted Curve"}\NormalTok{),}
    \AttributeTok{pch =} \FunctionTok{c}\NormalTok{(}\DecValTok{19}\NormalTok{, }\ConstantTok{NA}\NormalTok{),}
    \AttributeTok{lty =} \FunctionTok{c}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\DecValTok{1}\NormalTok{),}
    \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"red"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{06-nonlinear-regession_files/figure-latex/unnamed-chunk-9-1} \end{center}

\hypertarget{grid-search}{%
\subsubsection{Grid Search}\label{grid-search}}

Grid Search (GS) is a brute-force optimization method that systematically explores a grid of possible parameter values to identify the combination that minimizes the residual sum of squares (RSS). Unlike gradient-based optimization, which moves iteratively towards a minimum, grid search evaluates all predefined parameter combinations, making it robust but computationally expensive.

Grid search is particularly useful when:

\begin{itemize}
\item
  \textbf{The function is highly nonlinear}, making gradient methods unreliable.
\item
  \textbf{The parameter space is small}, allowing exhaustive search.
\item
  \textbf{A global minimum is needed}, and prior knowledge about parameter ranges exists.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The goal of Grid Search is to find an optimal parameter set \(\hat{\theta}\) that minimizes the Sum of Squared Errors (SSE):

\[
\hat{\theta} = \arg\min_{\theta \in \Theta} SSE(\theta),
\]

where:

\[
SSE(\theta) = \sum_{i=1}^{n} (y_i - f(x_i; \theta))^2.
\]

Grid search \textbf{discretizes} the search space \(\Theta\) into a finite set of candidate values for each parameter:

\[
\Theta = \theta_1 \times \theta_2 \times \dots \times \theta_p.
\]

The \textbf{accuracy} of the solution depends on the \textbf{grid resolution}---a finer grid leads to \textbf{better accuracy} but \textbf{higher computational cost}.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Grid Search Algorithm}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Define a grid of possible values for each parameter.
\item
  Evaluate SSE for each combination of parameters.
\item
  Select the parameter set that minimizes SSE.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Define a numerically stable logistic function}
\NormalTok{safe\_exp }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{}
    \FunctionTok{return}\NormalTok{(}\FunctionTok{ifelse}\NormalTok{(x }\SpecialCharTok{\textgreater{}} \DecValTok{700}\NormalTok{, }\ConstantTok{Inf}\NormalTok{, }\FunctionTok{exp}\NormalTok{(}\FunctionTok{pmin}\NormalTok{(x, }\DecValTok{700}\NormalTok{))))  }\CommentTok{\# Prevent overflow}
\NormalTok{\}}

\CommentTok{\# Define the logistic growth model}
\NormalTok{nonlinear\_model }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x) \{}
    \FunctionTok{return}\NormalTok{(theta[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{safe\_exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{theta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ (x }\SpecialCharTok{{-}}\NormalTok{ theta[}\DecValTok{3}\NormalTok{]))))}
\NormalTok{\}}

\CommentTok{\# Define the Sum of Squared Errors (SSE) function}
\NormalTok{sse }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x, y) \{}
\NormalTok{    predictions }\OtherTok{\textless{}{-}} \FunctionTok{nonlinear\_model}\NormalTok{(theta, x)}
    \FunctionTok{return}\NormalTok{(}\FunctionTok{sum}\NormalTok{((y }\SpecialCharTok{{-}}\NormalTok{ predictions)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{))}
\NormalTok{\}}

\CommentTok{\# Grid Search Optimization}
\NormalTok{grid\_search\_optimization }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x, y, }\AttributeTok{grid\_size =} \DecValTok{10}\NormalTok{) \{}
    \CommentTok{\# Define grid of parameter values}
\NormalTok{    A\_values }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{6}\NormalTok{, }\AttributeTok{length.out =}\NormalTok{ grid\_size)}
\NormalTok{    B\_values }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\DecValTok{3}\NormalTok{, }\AttributeTok{length.out =}\NormalTok{ grid\_size)}
\NormalTok{    C\_values }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\AttributeTok{length.out =}\NormalTok{ grid\_size)}
    
    \CommentTok{\# Generate all combinations of parameters}
\NormalTok{    param\_grid }\OtherTok{\textless{}{-}} \FunctionTok{expand.grid}\NormalTok{(}\AttributeTok{A =}\NormalTok{ A\_values, }\AttributeTok{B =}\NormalTok{ B\_values, }\AttributeTok{C =}\NormalTok{ C\_values)}
    
    \CommentTok{\# Evaluate SSE for each parameter combination}
\NormalTok{    param\_grid}\SpecialCharTok{$}\NormalTok{SSE }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(param\_grid, }\DecValTok{1}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(theta) \{}
        \FunctionTok{sse}\NormalTok{(}\FunctionTok{as.numeric}\NormalTok{(theta[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{]), x, y)}
\NormalTok{    \})}
    
    \CommentTok{\# Select the best parameter set}
\NormalTok{    best\_params }\OtherTok{\textless{}{-}}\NormalTok{ param\_grid[}\FunctionTok{which.min}\NormalTok{(param\_grid}\SpecialCharTok{$}\NormalTok{SSE), }\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{]}
    \FunctionTok{return}\NormalTok{(}\FunctionTok{as.numeric}\NormalTok{(best\_params))}
\NormalTok{\}}

\CommentTok{\# Generate synthetic data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{100}\NormalTok{)}
\NormalTok{true\_theta }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\FloatTok{1.5}\NormalTok{, }\DecValTok{0}\NormalTok{)  }\CommentTok{\# True parameters (A, B, C)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{nonlinear\_model}\NormalTok{(true\_theta, x) }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{length}\NormalTok{(x), }\AttributeTok{sd =} \FloatTok{0.3}\NormalTok{)}

\CommentTok{\# Run Grid Search}
\NormalTok{estimated\_theta }\OtherTok{\textless{}{-}} \FunctionTok{grid\_search\_optimization}\NormalTok{(x, y, }\AttributeTok{grid\_size =} \DecValTok{20}\NormalTok{)}

\CommentTok{\# Display results}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Estimated parameters (A, B, C) using Grid Search:}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Estimated parameters (A, B, C) using Grid Search:}
\FunctionTok{print}\NormalTok{(estimated\_theta)}
\CommentTok{\#\textgreater{} [1] 4.1052632 1.4210526 0.1052632}

\CommentTok{\# Plot data and fitted curve}
\FunctionTok{plot}\NormalTok{(}
\NormalTok{    x, y,}
    \AttributeTok{main =} \StringTok{"Grid Search: Nonlinear Regression Optimization"}\NormalTok{,}
    \AttributeTok{pch =} \DecValTok{19}\NormalTok{,}
    \AttributeTok{cex =} \FloatTok{0.5}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"x"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"y"}
\NormalTok{)}
\FunctionTok{curve}\NormalTok{(}
    \FunctionTok{nonlinear\_model}\NormalTok{(estimated\_theta, x),}
    \AttributeTok{from =} \FunctionTok{min}\NormalTok{(x),}
    \AttributeTok{to =} \FunctionTok{max}\NormalTok{(x),}
    \AttributeTok{add =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{col =} \StringTok{"red"}\NormalTok{,}
    \AttributeTok{lwd =} \DecValTok{2}
\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}
    \StringTok{"topleft"}\NormalTok{,}
    \AttributeTok{legend =} \FunctionTok{c}\NormalTok{(}\StringTok{"Data"}\NormalTok{, }\StringTok{"Fitted Curve"}\NormalTok{),}
    \AttributeTok{pch =} \FunctionTok{c}\NormalTok{(}\DecValTok{19}\NormalTok{, }\ConstantTok{NA}\NormalTok{),}
    \AttributeTok{lty =} \FunctionTok{c}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\DecValTok{1}\NormalTok{),}
    \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"red"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{06-nonlinear-regession_files/figure-latex/unnamed-chunk-10-1} \end{center}

\hypertarget{nelder-mead}{%
\subsubsection{Nelder-Mead (Simplex)}\label{nelder-mead}}

The Nelder-Mead algorithm, also known as the Simplex method, is a derivative-free optimization algorithm that iteratively adjusts a geometric shape (simplex) to find the minimum of an objective function. It is particularly useful for nonlinear regression when gradient-based methods fail due to non-differentiability or noisy function evaluations.

Nelder-Mead is particularly useful when:

\begin{itemize}
\item
  The function is non-differentiable or noisy.
\item
  Gradient-based methods are unreliable.
\item
  The parameter space is low-dimensional.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The goal of Nelder-Mead is to find an optimal parameter set \(\hat{\theta}\) that minimizes the Sum of Squared Errors (SSE):

\[
\hat{\theta} = \arg\min_{\theta} SSE(\theta),
\]

where:

\[
SSE(\theta) = \sum_{i=1}^{n} (y_i - f(x_i; \theta))^2.
\]

\textbf{1. Simplex Representation}

The algorithm maintains a simplex, a geometric shape with \(p+1\) vertices for a \(p\)-dimensional parameter space.

Each vertex represents a parameter vector:

\[
S = \{ \theta_1, \theta_2, \dots, \theta_{p+1} \}.
\]

\textbf{2. Simplex Operations}

At each iteration, the algorithm \textbf{updates the simplex} based on the objective function values at each vertex:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Reflection}: Reflect the worst point \(\theta_h\) across the centroid.

  \[
  \theta_r = \theta_c + \alpha (\theta_c - \theta_h)
  \]
\item
  \textbf{Expansion}: If reflection improves the objective, expand the step.

  \[
  \theta_e = \theta_c + \gamma (\theta_r - \theta_c)
  \]
\item
  \textbf{Contraction}: If reflection worsens the objective, contract towards the centroid.

  \[
  \theta_c = \theta_c + \rho (\theta_h - \theta_c)
  \]
\item
  \textbf{Shrink}: If contraction fails, shrink the simplex.

  \[
  \theta_i = \theta_1 + \sigma (\theta_i - \theta_1), \quad \forall i > 1
  \]
\end{enumerate}

where:

\begin{itemize}
\item
  \(\alpha = 1\) (reflection coefficient),
\item
  \(\gamma = 2\) (expansion coefficient),
\item
  \(\rho = 0.5\) (contraction coefficient),
\item
  \(\sigma = 0.5\) (shrink coefficient).
\end{itemize}

The process continues until convergence.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Nelder-Mead Algorithm}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Initialize a simplex with \(p+1\) vertices.
\item
  Evaluate SSE at each vertex.
\item
  Perform reflection, expansion, contraction, or shrink operations.
\item
  Repeat until convergence.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load required library}
\FunctionTok{library}\NormalTok{(stats)}


\CommentTok{\# Define a numerically stable logistic function}
\NormalTok{safe\_exp }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{}
    \FunctionTok{return}\NormalTok{(}\FunctionTok{ifelse}\NormalTok{(x }\SpecialCharTok{\textgreater{}} \DecValTok{700}\NormalTok{, }\ConstantTok{Inf}\NormalTok{, }\FunctionTok{exp}\NormalTok{(}\FunctionTok{pmin}\NormalTok{(x, }\DecValTok{700}\NormalTok{))))  }\CommentTok{\# Prevent overflow}
\NormalTok{\}}

\CommentTok{\# Define the logistic growth model}
\NormalTok{nonlinear\_model }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x) \{}
    \FunctionTok{return}\NormalTok{(theta[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{safe\_exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{theta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ (x }\SpecialCharTok{{-}}\NormalTok{ theta[}\DecValTok{3}\NormalTok{]))))}
\NormalTok{\}}

\CommentTok{\# Define the Sum of Squared Errors (SSE) function}
\NormalTok{sse }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x, y) \{}
\NormalTok{    predictions }\OtherTok{\textless{}{-}} \FunctionTok{nonlinear\_model}\NormalTok{(theta, x)}
    \FunctionTok{return}\NormalTok{(}\FunctionTok{sum}\NormalTok{((y }\SpecialCharTok{{-}}\NormalTok{ predictions) }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{))}
\NormalTok{\}}

\CommentTok{\# Nelder{-}Mead Optimization}
\NormalTok{nelder\_mead\_optimization }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x, y) \{}
    \CommentTok{\# Initial guess for parameters}
\NormalTok{    initial\_guess }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
    
    \CommentTok{\# Run Nelder{-}Mead optimization}
\NormalTok{    result }\OtherTok{\textless{}{-}} \FunctionTok{optim}\NormalTok{(}
        \AttributeTok{par =}\NormalTok{ initial\_guess,}
        \AttributeTok{fn =}\NormalTok{ sse,}
        \AttributeTok{x =}\NormalTok{ x,}
        \AttributeTok{y =}\NormalTok{ y,}
        \AttributeTok{method =} \StringTok{"Nelder{-}Mead"}\NormalTok{,}
        \AttributeTok{control =} \FunctionTok{list}\NormalTok{(}\AttributeTok{maxit =} \DecValTok{500}\NormalTok{)}
\NormalTok{    )}
    
    \FunctionTok{return}\NormalTok{(result}\SpecialCharTok{$}\NormalTok{par)  }\CommentTok{\# Return optimized parameters}
\NormalTok{\}}

\CommentTok{\# Generate synthetic data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{100}\NormalTok{)}
\NormalTok{true\_theta }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\FloatTok{1.5}\NormalTok{, }\DecValTok{0}\NormalTok{)  }\CommentTok{\# True parameters (A, B, C)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{nonlinear\_model}\NormalTok{(true\_theta, x) }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{length}\NormalTok{(x), }\AttributeTok{sd =} \FloatTok{0.3}\NormalTok{)}

\CommentTok{\# Run Nelder{-}Mead Optimization}
\NormalTok{estimated\_theta }\OtherTok{\textless{}{-}} \FunctionTok{nelder\_mead\_optimization}\NormalTok{(x, y)}

\CommentTok{\# Display results}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Estimated parameters (A, B, C) using Nelder{-}Mead:}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Estimated parameters (A, B, C) using Nelder{-}Mead:}
\FunctionTok{print}\NormalTok{(estimated\_theta)}
\CommentTok{\#\textgreater{} [1] 4.06873116 1.42759898 0.01119379}

\CommentTok{\# Plot data and fitted curve}
\FunctionTok{plot}\NormalTok{(}
\NormalTok{    x,}
\NormalTok{    y,}
    \AttributeTok{main =} \StringTok{"Nelder{-}Mead: Nonlinear Regression Optimization"}\NormalTok{,}
    \AttributeTok{pch =} \DecValTok{19}\NormalTok{,}
    \AttributeTok{cex =} \FloatTok{0.5}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"x"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"y"}
\NormalTok{)}
\FunctionTok{curve}\NormalTok{(}
    \FunctionTok{nonlinear\_model}\NormalTok{(estimated\_theta, x),}
    \AttributeTok{from =} \FunctionTok{min}\NormalTok{(x),}
    \AttributeTok{to =} \FunctionTok{max}\NormalTok{(x),}
    \AttributeTok{add =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{col =} \StringTok{"red"}\NormalTok{,}
    \AttributeTok{lwd =} \DecValTok{2}
\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}
    \StringTok{"topleft"}\NormalTok{,}
    \AttributeTok{legend =} \FunctionTok{c}\NormalTok{(}\StringTok{"Data"}\NormalTok{, }\StringTok{"Fitted Curve"}\NormalTok{),}
    \AttributeTok{pch =} \FunctionTok{c}\NormalTok{(}\DecValTok{19}\NormalTok{, }\ConstantTok{NA}\NormalTok{),}
    \AttributeTok{lty =} \FunctionTok{c}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\DecValTok{1}\NormalTok{),}
    \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"red"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{06-nonlinear-regession_files/figure-latex/unnamed-chunk-11-1} \end{center}

\hypertarget{powells-method}{%
\subsubsection{Powell's Method}\label{powells-method}}

Powell's Method is a derivative-free optimization algorithm that minimizes a function without using gradients. It works by iteratively refining a set of search directions to efficiently navigate the parameter space. Unlike \protect\hyperlink{nelder-mead}{Nelder-Mead (Simplex)}, which adapts a simplex, Powell's method builds an orthogonal basis of search directions.

Powell's method is particularly useful when:

\begin{itemize}
\item
  The function is non-differentiable or noisy.
\item
  Gradient-based methods are unreliable.
\item
  The parameter space is low-to-moderate dimensional.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The goal of Powell's Method is to find an optimal parameter set \(\hat{\theta}\) that minimizes the Sum of Squared Errors (SSE):

\[
\hat{\theta} = \arg\min_{\theta} SSE(\theta),
\]

where:

\[
SSE(\theta) = \sum_{i=1}^{n} (y_i - f(x_i; \theta))^2.
\]

\textbf{1. Search Directions}

Powell's method \textbf{maintains a set of search directions} \(\mathbf{d}_1, \mathbf{d}_2, \dots, \mathbf{d}_p\):

\[
D = \{ \mathbf{d}_1, \mathbf{d}_2, ..., \mathbf{d}_p \}.
\]

Initially, these directions are chosen as \textbf{unit basis vectors} (each optimizing a single parameter independently).

\textbf{2. Line Minimization}

For each direction \(\mathbf{d}_j\), Powell's method \textbf{performs a 1D optimization}:

\[
\theta' = \theta + \lambda \mathbf{d}_j,
\]

where \(\lambda\) is chosen to minimize \(SSE(\theta')\).

\textbf{3. Updating Search Directions}

After optimizing along all directions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The \textbf{largest improvement} direction \(\mathbf{d}_{\max}\) is replaced with:
\end{enumerate}

\[
\mathbf{d}_{\text{new}} = \theta_{\text{final}} - \theta_{\text{initial}}.
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  The \textbf{new direction set is orthogonalized} using Gram-Schmidt.
\end{enumerate}

This ensures \textbf{efficient exploration} of the parameter space.

\textbf{4. Convergence Criteria}

Powell's method \textbf{stops when function improvement is below a tolerance level}:

\[
|SSE(\theta_{t+1}) - SSE(\theta_t)| < \epsilon.
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Powell's Method Algorithm}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Initialize search directions (standard basis vectors).
\item
  Perform 1D line searches along each direction.
\item
  Update the search directions based on the largest improvement.
\item
  Repeat until convergence.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load required library}
\FunctionTok{library}\NormalTok{(stats)}

\CommentTok{\# Define a numerically stable logistic function}
\NormalTok{safe\_exp }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{}
    \FunctionTok{return}\NormalTok{(}\FunctionTok{ifelse}\NormalTok{(x }\SpecialCharTok{\textgreater{}} \DecValTok{700}\NormalTok{, }\ConstantTok{Inf}\NormalTok{, }\FunctionTok{exp}\NormalTok{(}\FunctionTok{pmin}\NormalTok{(x, }\DecValTok{700}\NormalTok{))))  }\CommentTok{\# Prevent overflow}
\NormalTok{\}}

\CommentTok{\# Define the logistic growth model}
\NormalTok{nonlinear\_model }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x) \{}
    \FunctionTok{return}\NormalTok{(theta[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{safe\_exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{theta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ (x }\SpecialCharTok{{-}}\NormalTok{ theta[}\DecValTok{3}\NormalTok{]))))}
\NormalTok{\}}

\CommentTok{\# Define the Sum of Squared Errors (SSE) function}
\NormalTok{sse }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x, y) \{}
\NormalTok{    predictions }\OtherTok{\textless{}{-}} \FunctionTok{nonlinear\_model}\NormalTok{(theta, x)}
    \FunctionTok{return}\NormalTok{(}\FunctionTok{sum}\NormalTok{((y }\SpecialCharTok{{-}}\NormalTok{ predictions) }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{))}
\NormalTok{\}}

\CommentTok{\# Powell\textquotesingle{}s Method Optimization}
\NormalTok{powell\_optimization }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x, y) \{}
    \CommentTok{\# Initial guess for parameters}
\NormalTok{    initial\_guess }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
    
    \CommentTok{\# Run Powells optimization (via BFGS without gradient)}
\NormalTok{    result }\OtherTok{\textless{}{-}} \FunctionTok{optim}\NormalTok{(}
        \AttributeTok{par =}\NormalTok{ initial\_guess,}
        \AttributeTok{fn =}\NormalTok{ sse,}
        \AttributeTok{x =}\NormalTok{ x,}
        \AttributeTok{y =}\NormalTok{ y,}
        \AttributeTok{method =} \StringTok{"BFGS"}\NormalTok{,}
        \AttributeTok{control =} \FunctionTok{list}\NormalTok{(}\AttributeTok{maxit =} \DecValTok{500}\NormalTok{),}
        \AttributeTok{gr =} \ConstantTok{NULL}  \CommentTok{\# No gradient used}
\NormalTok{    )}
    
    \FunctionTok{return}\NormalTok{(result}\SpecialCharTok{$}\NormalTok{par)  }\CommentTok{\# Return optimized parameters}
\NormalTok{\}}

\CommentTok{\# Generate synthetic data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{100}\NormalTok{)}
\NormalTok{true\_theta }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\FloatTok{1.5}\NormalTok{, }\DecValTok{0}\NormalTok{)  }\CommentTok{\# True parameters (A, B, C)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{nonlinear\_model}\NormalTok{(true\_theta, x) }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{length}\NormalTok{(x), }\AttributeTok{sd =} \FloatTok{0.3}\NormalTok{)}

\CommentTok{\# Run Powell\textquotesingle{}s Method Optimization}
\NormalTok{estimated\_theta }\OtherTok{\textless{}{-}} \FunctionTok{powell\_optimization}\NormalTok{(x, y)}

\CommentTok{\# Display results}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Estimated parameters (A, B, C) using Powells Method:}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Estimated parameters (A, B, C) using Powells Method:}
\FunctionTok{print}\NormalTok{(estimated\_theta)}
\CommentTok{\#\textgreater{} [1] 4.06876538 1.42765687 0.01128753}

\CommentTok{\# Plot data and fitted curve}
\FunctionTok{plot}\NormalTok{(}
\NormalTok{    x,}
\NormalTok{    y,}
    \AttributeTok{main =} \StringTok{"Powell\textquotesingle{}s Method: Nonlinear Regression Optimization"}\NormalTok{,}
    \AttributeTok{pch =} \DecValTok{19}\NormalTok{,}
    \AttributeTok{cex =} \FloatTok{0.5}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"x"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"y"}
\NormalTok{)}
\FunctionTok{curve}\NormalTok{(}
    \FunctionTok{nonlinear\_model}\NormalTok{(estimated\_theta, x),}
    \AttributeTok{from =} \FunctionTok{min}\NormalTok{(x),}
    \AttributeTok{to =} \FunctionTok{max}\NormalTok{(x),}
    \AttributeTok{add =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{col =} \StringTok{"red"}\NormalTok{,}
    \AttributeTok{lwd =} \DecValTok{2}
\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}
    \StringTok{"topleft"}\NormalTok{,}
    \AttributeTok{legend =} \FunctionTok{c}\NormalTok{(}\StringTok{"Data"}\NormalTok{, }\StringTok{"Fitted Curve"}\NormalTok{),}
    \AttributeTok{pch =} \FunctionTok{c}\NormalTok{(}\DecValTok{19}\NormalTok{, }\ConstantTok{NA}\NormalTok{),}
    \AttributeTok{lty =} \FunctionTok{c}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\DecValTok{1}\NormalTok{),}
    \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"red"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{06-nonlinear-regession_files/figure-latex/unnamed-chunk-12-1} \end{center}

\hypertarget{random-search}{%
\subsubsection{Random Search}\label{random-search}}

Random Search (RS) is a simple yet effective optimization algorithm that explores the search space by randomly sampling candidate solutions. Unlike \protect\hyperlink{grid-search}{grid search}, which evaluates all predefined parameter combinations, random search selects a random subset, reducing computational cost.

Random search is particularly useful when:

\begin{itemize}
\item
  The search space is large, making grid search impractical.
\item
  Gradient-based methods fail due to non-differentiability or noisy data.
\item
  The optimal region is unknown, making global exploration essential.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The goal of Random Search is to find an optimal parameter set \(\hat{\theta}\) that minimizes the Sum of Squared Errors (SSE):

\[
\hat{\theta} = \arg\min_{\theta \in \Theta} SSE(\theta),
\]

where:

\[
SSE(\theta) = \sum_{i=1}^{n} (y_i - f(x_i; \theta))^2.
\]

Unlike \protect\hyperlink{grid-search}{grid search}, \textbf{random search does not evaluate all parameter combinations} but instead \textbf{randomly samples a subset}:

\[
\Theta_{\text{sampled}} \subset \Theta.
\]

The \textbf{accuracy} of the solution depends on the \textbf{number of random samples}---a higher number increases \textbf{the likelihood of finding a good solution}.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Random Search Algorithm}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Define a random sampling range for each parameter.
\item
  Randomly sample \(N\) parameter sets.
\item
  Evaluate SSE for each sampled set.
\item
  Select the parameter set that minimizes SSE.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load required library}
\FunctionTok{library}\NormalTok{(stats)}

\CommentTok{\# Define a numerically stable logistic function}
\NormalTok{safe\_exp }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{ifelse}\NormalTok{(x }\SpecialCharTok{\textgreater{}} \DecValTok{700}\NormalTok{, }\ConstantTok{Inf}\NormalTok{, }\FunctionTok{exp}\NormalTok{(}\FunctionTok{pmin}\NormalTok{(x, }\DecValTok{700}\NormalTok{))))  }\CommentTok{\# Prevent overflow}
\NormalTok{\}}

\CommentTok{\# Define the logistic growth model}
\NormalTok{nonlinear\_model }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x) \{}
  \FunctionTok{return}\NormalTok{(theta[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{safe\_exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{theta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ (x }\SpecialCharTok{{-}}\NormalTok{ theta[}\DecValTok{3}\NormalTok{]))))}
\NormalTok{\}}

\CommentTok{\# Define the Sum of Squared Errors (SSE) function}
\NormalTok{sse }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x, y) \{}
\NormalTok{  predictions }\OtherTok{\textless{}{-}} \FunctionTok{nonlinear\_model}\NormalTok{(theta, x)}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{sum}\NormalTok{((y }\SpecialCharTok{{-}}\NormalTok{ predictions) }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{))}
\NormalTok{\}}

\CommentTok{\# Random Search Optimization}
\NormalTok{random\_search\_optimization }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x, y, }\AttributeTok{num\_samples =} \DecValTok{100}\NormalTok{) \{}
  \CommentTok{\# Define parameter ranges}
\NormalTok{  A\_range }\OtherTok{\textless{}{-}}
    \FunctionTok{runif}\NormalTok{(num\_samples, }\DecValTok{2}\NormalTok{, }\DecValTok{6}\NormalTok{)   }\CommentTok{\# Random values between 2 and 6}
\NormalTok{  B\_range }\OtherTok{\textless{}{-}}
    \FunctionTok{runif}\NormalTok{(num\_samples, }\FloatTok{0.5}\NormalTok{, }\DecValTok{3}\NormalTok{) }\CommentTok{\# Random values between 0.5 and 3}
\NormalTok{  C\_range }\OtherTok{\textless{}{-}}
    \FunctionTok{runif}\NormalTok{(num\_samples,}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)  }\CommentTok{\# Random values between {-}2 and 2}
  
  \CommentTok{\# Initialize best parameters}
\NormalTok{  best\_theta }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\NormalTok{  best\_sse }\OtherTok{\textless{}{-}} \ConstantTok{Inf}
  
  \CommentTok{\# Evaluate randomly sampled parameter sets}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{num\_samples) \{}
\NormalTok{    theta }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(A\_range[i], B\_range[i], C\_range[i])}
\NormalTok{    current\_sse }\OtherTok{\textless{}{-}} \FunctionTok{sse}\NormalTok{(theta, x, y)}
    
    \ControlFlowTok{if}\NormalTok{ (current\_sse }\SpecialCharTok{\textless{}}\NormalTok{ best\_sse) \{}
\NormalTok{      best\_sse }\OtherTok{\textless{}{-}}\NormalTok{ current\_sse}
\NormalTok{      best\_theta }\OtherTok{\textless{}{-}}\NormalTok{ theta}
\NormalTok{    \}}
\NormalTok{  \}}
  
  \FunctionTok{return}\NormalTok{(best\_theta)}
\NormalTok{\}}

\CommentTok{\# Generate synthetic data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{100}\NormalTok{)}
\NormalTok{true\_theta }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\FloatTok{1.5}\NormalTok{, }\DecValTok{0}\NormalTok{)  }\CommentTok{\# True parameters (A, B, C)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{nonlinear\_model}\NormalTok{(true\_theta, x) }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{length}\NormalTok{(x), }\AttributeTok{sd =} \FloatTok{0.3}\NormalTok{)}

\CommentTok{\# Run Random Search}
\NormalTok{estimated\_theta }\OtherTok{\textless{}{-}}
  \FunctionTok{random\_search\_optimization}\NormalTok{(x, y, }\AttributeTok{num\_samples =} \DecValTok{500}\NormalTok{)}

\CommentTok{\# Display results}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Estimated parameters (A, B, C) using Random Search:}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Estimated parameters (A, B, C) using Random Search:}
\FunctionTok{print}\NormalTok{(estimated\_theta)}
\CommentTok{\#\textgreater{} [1] 4.0893431 1.4687456 0.1024474}

\CommentTok{\# Plot data and fitted curve}
\FunctionTok{plot}\NormalTok{(}
\NormalTok{  x,}
\NormalTok{  y,}
  \AttributeTok{main =} \StringTok{"Random Search: Nonlinear Regression Optimization"}\NormalTok{,}
  \AttributeTok{pch =} \DecValTok{19}\NormalTok{,}
  \AttributeTok{cex =} \FloatTok{0.5}\NormalTok{,}
  \AttributeTok{xlab =} \StringTok{"x"}\NormalTok{,}
  \AttributeTok{ylab =} \StringTok{"y"}
\NormalTok{)}
\FunctionTok{curve}\NormalTok{(}
  \FunctionTok{nonlinear\_model}\NormalTok{(estimated\_theta, x),}
  \AttributeTok{from =} \FunctionTok{min}\NormalTok{(x),}
  \AttributeTok{to =} \FunctionTok{max}\NormalTok{(x),}
  \AttributeTok{add =} \ConstantTok{TRUE}\NormalTok{,}
  \AttributeTok{col =} \StringTok{"red"}\NormalTok{,}
  \AttributeTok{lwd =} \DecValTok{2}
\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}
  \StringTok{"topleft"}\NormalTok{,}
  \AttributeTok{legend =} \FunctionTok{c}\NormalTok{(}\StringTok{"Data"}\NormalTok{, }\StringTok{"Fitted Curve"}\NormalTok{),}
  \AttributeTok{pch =} \FunctionTok{c}\NormalTok{(}\DecValTok{19}\NormalTok{, }\ConstantTok{NA}\NormalTok{),}
  \AttributeTok{lty =} \FunctionTok{c}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\DecValTok{1}\NormalTok{),}
  \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"red"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{06-nonlinear-regession_files/figure-latex/unnamed-chunk-13-1} \end{center}

\hypertarget{hooke-jeeves}{%
\subsubsection{Hooke-Jeeves Pattern Search}\label{hooke-jeeves}}

Hooke-Jeeves Pattern Search is a derivative-free optimization algorithm that searches for an optimal solution by exploring and adjusting parameter values iteratively. Unlike gradient-based methods, it does not require differentiability, making it effective for non-smooth and noisy functions.

Hooke-Jeeves is particularly useful when:

\begin{itemize}
\item
  The function is non-differentiable or noisy.
\item
  Gradient-based methods are unreliable.
\item
  The parameter space is low-to-moderate dimensional.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The goal of Hooke-Jeeves Pattern Search is to find an optimal parameter set \(\hat{\theta}\) that minimizes the Sum of Squared Errors (SSE):

\[
\hat{\theta} = \arg\min_{\theta} SSE(\theta),
\]

where:

\[
SSE(\theta) = \sum_{i=1}^{n} (y_i - f(x_i; \theta))^2.
\]

\textbf{1. Exploratory Moves}

At each iteration, the algorithm \textbf{perturbs each parameter} to find a lower SSE:

\[
\theta' = \theta \pm \delta.
\]

If a new parameter \(\theta'\) reduces SSE, it becomes the new base point.

\textbf{2. Pattern Moves}

After an improvement, the algorithm \textbf{accelerates movement} in the promising direction:

\[
\theta_{\text{new}} = \theta_{\text{old}} + (\theta_{\text{old}} - \theta_{\text{prev}}).
\]

This \textbf{speeds up convergence} towards an optimum.

\textbf{3. Step Size Adaptation}

If no improvement is found, the step size \(\delta\) is reduced:

\[
\delta_{\text{new}} = \beta \cdot \delta.
\]

where \(\beta < 1\) is a reduction factor.

\textbf{4. Convergence Criteria}

The algorithm \textbf{stops when step size is sufficiently small}:

\[
\delta < \epsilon.
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Hooke-Jeeves Algorithm}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Initialize a starting point \(\theta\) and step size \(\delta\).
\item
  Perform exploratory moves in each parameter direction.
\item
  If improvement is found, perform pattern moves.
\item
  Reduce step size if no improvement is found.
\item
  Repeat until convergence.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load required library}
\FunctionTok{library}\NormalTok{(stats)}

\CommentTok{\# Define a numerically stable logistic function}
\NormalTok{safe\_exp }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{}
    \FunctionTok{return}\NormalTok{(}\FunctionTok{ifelse}\NormalTok{(x }\SpecialCharTok{\textgreater{}} \DecValTok{700}\NormalTok{, }\ConstantTok{Inf}\NormalTok{, }\FunctionTok{exp}\NormalTok{(}\FunctionTok{pmin}\NormalTok{(x, }\DecValTok{700}\NormalTok{))))  }\CommentTok{\# Prevent overflow}
\NormalTok{\}}

\CommentTok{\# Define the logistic growth model}
\NormalTok{nonlinear\_model }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x) \{}
    \FunctionTok{return}\NormalTok{(theta[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{safe\_exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{theta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ (x }\SpecialCharTok{{-}}\NormalTok{ theta[}\DecValTok{3}\NormalTok{]))))}
\NormalTok{\}}

\CommentTok{\# Define the Sum of Squared Errors (SSE) function}
\NormalTok{sse }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x, y) \{}
\NormalTok{    predictions }\OtherTok{\textless{}{-}} \FunctionTok{nonlinear\_model}\NormalTok{(theta, x)}
    \FunctionTok{return}\NormalTok{(}\FunctionTok{sum}\NormalTok{((y }\SpecialCharTok{{-}}\NormalTok{ predictions) }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{))}
\NormalTok{\}}

\CommentTok{\# Hooke{-}Jeeves Pattern Search Optimization}
\NormalTok{hooke\_jeeves\_optimization }\OtherTok{\textless{}{-}}
    \ControlFlowTok{function}\NormalTok{(x,}
\NormalTok{             y,}
             \AttributeTok{step\_size =} \FloatTok{0.5}\NormalTok{,}
             \AttributeTok{tol =} \FloatTok{1e{-}6}\NormalTok{,}
             \AttributeTok{max\_iter =} \DecValTok{500}\NormalTok{) \{}
        \CommentTok{\# Initial guess for parameters}
\NormalTok{        theta }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{        best\_sse }\OtherTok{\textless{}{-}} \FunctionTok{sse}\NormalTok{(theta, x, y)}
\NormalTok{        step }\OtherTok{\textless{}{-}}\NormalTok{ step\_size}
\NormalTok{        iter }\OtherTok{\textless{}{-}} \DecValTok{0}
        
        \ControlFlowTok{while}\NormalTok{ (step }\SpecialCharTok{\textgreater{}}\NormalTok{ tol }\SpecialCharTok{\&}\NormalTok{ iter }\SpecialCharTok{\textless{}}\NormalTok{ max\_iter) \{}
\NormalTok{            iter }\OtherTok{\textless{}{-}}\NormalTok{ iter }\SpecialCharTok{+} \DecValTok{1}
\NormalTok{            improved }\OtherTok{\textless{}{-}} \ConstantTok{FALSE}
\NormalTok{            new\_theta }\OtherTok{\textless{}{-}}\NormalTok{ theta}
            
            \CommentTok{\# Exploratory move in each parameter direction}
            \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(theta)) \{}
                \ControlFlowTok{for}\NormalTok{ (delta }\ControlFlowTok{in} \FunctionTok{c}\NormalTok{(step,}\SpecialCharTok{{-}}\NormalTok{step)) \{}
\NormalTok{                    theta\_test }\OtherTok{\textless{}{-}}\NormalTok{ new\_theta}
\NormalTok{                    theta\_test[i] }\OtherTok{\textless{}{-}}\NormalTok{ theta\_test[i] }\SpecialCharTok{+}\NormalTok{ delta}
\NormalTok{                    sse\_test }\OtherTok{\textless{}{-}} \FunctionTok{sse}\NormalTok{(theta\_test, x, y)}
                    
                    \ControlFlowTok{if}\NormalTok{ (sse\_test }\SpecialCharTok{\textless{}}\NormalTok{ best\_sse) \{}
\NormalTok{                        best\_sse }\OtherTok{\textless{}{-}}\NormalTok{ sse\_test}
\NormalTok{                        new\_theta }\OtherTok{\textless{}{-}}\NormalTok{ theta\_test}
\NormalTok{                        improved }\OtherTok{\textless{}{-}} \ConstantTok{TRUE}
\NormalTok{                    \}}
\NormalTok{                \}}
\NormalTok{            \}}
            
            \CommentTok{\# Pattern move if improvement found}
            \ControlFlowTok{if}\NormalTok{ (improved) \{}
\NormalTok{                theta }\OtherTok{\textless{}{-}} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ new\_theta }\SpecialCharTok{{-}}\NormalTok{ theta}
\NormalTok{                best\_sse }\OtherTok{\textless{}{-}} \FunctionTok{sse}\NormalTok{(theta, x, y)}
\NormalTok{            \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{                step }\OtherTok{\textless{}{-}}\NormalTok{ step }\SpecialCharTok{/} \DecValTok{2}  \CommentTok{\# Reduce step size}
\NormalTok{            \}}
\NormalTok{        \}}
        
        \FunctionTok{return}\NormalTok{(theta)}
\NormalTok{    \}}

\CommentTok{\# Generate synthetic data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{100}\NormalTok{)}
\NormalTok{true\_theta }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\FloatTok{1.5}\NormalTok{, }\DecValTok{0}\NormalTok{)  }\CommentTok{\# True parameters (A, B, C)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{nonlinear\_model}\NormalTok{(true\_theta, x) }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{length}\NormalTok{(x), }\AttributeTok{sd =} \FloatTok{0.3}\NormalTok{)}

\CommentTok{\# Run Hooke{-}Jeeves Optimization}
\NormalTok{estimated\_theta }\OtherTok{\textless{}{-}} \FunctionTok{hooke\_jeeves\_optimization}\NormalTok{(x, y)}

\CommentTok{\# Display results}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Estimated parameters (A, B, C) using Hooke{-}Jeeves:}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Estimated parameters (A, B, C) using Hooke{-}Jeeves:}
\FunctionTok{print}\NormalTok{(estimated\_theta)}
\CommentTok{\#\textgreater{} [1] 4 1 0}

\CommentTok{\# Plot data and fitted curve}
\FunctionTok{plot}\NormalTok{(}
\NormalTok{    x,}
\NormalTok{    y,}
    \AttributeTok{main =} \StringTok{"Hooke{-}Jeeves: Nonlinear Regression Optimization"}\NormalTok{,}
    \AttributeTok{pch =} \DecValTok{19}\NormalTok{,}
    \AttributeTok{cex =} \FloatTok{0.5}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"x"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"y"}
\NormalTok{)}
\FunctionTok{curve}\NormalTok{(}
    \FunctionTok{nonlinear\_model}\NormalTok{(estimated\_theta, x),}
    \AttributeTok{from =} \FunctionTok{min}\NormalTok{(x),}
    \AttributeTok{to =} \FunctionTok{max}\NormalTok{(x),}
    \AttributeTok{add =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{col =} \StringTok{"red"}\NormalTok{,}
    \AttributeTok{lwd =} \DecValTok{2}
\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}
    \StringTok{"topleft"}\NormalTok{,}
    \AttributeTok{legend =} \FunctionTok{c}\NormalTok{(}\StringTok{"Data"}\NormalTok{, }\StringTok{"Fitted Curve"}\NormalTok{),}
    \AttributeTok{pch =} \FunctionTok{c}\NormalTok{(}\DecValTok{19}\NormalTok{, }\ConstantTok{NA}\NormalTok{),}
    \AttributeTok{lty =} \FunctionTok{c}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\DecValTok{1}\NormalTok{),}
    \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"red"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{06-nonlinear-regession_files/figure-latex/unnamed-chunk-14-1} \end{center}

\hypertarget{bisection-method}{%
\subsubsection{Bisection Method}\label{bisection-method}}

The \textbf{Bisection Method} is a fundamental numerical technique primarily used for \textbf{root finding}, but it can also be adapted for \textbf{optimization problems} where the goal is to minimize or maximize a nonlinear function.

In nonlinear regression, optimization often involves \textbf{finding the parameter values that minimize the sum of squared errors (SSE)}:

\[
\hat{\theta} = \arg\min_{\theta} SSE(\theta)
\]

where:

\[
SSE(\theta) = \sum_{i=1}^{n} \left( y_i - f(x_i; \theta) \right)^2.
\]

Since the minimum of a function occurs where the derivative equals zero, we apply the \textbf{Bisection Method} to the \textbf{derivative of the SSE function}:

\[
\frac{d}{d\theta} SSE(\theta) = 0.
\]

This transforms the \textbf{optimization problem} into a \textbf{root-finding problem}.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The \textbf{Bisection Method} is based on the \textbf{Intermediate Value Theorem}, which states:

\begin{quote}
If a continuous function \(f(\theta)\) satisfies \(f(\theta_a) \cdot f(\theta_b) < 0\),\\
then there exists at least one root in the interval \((\theta_a, \theta_b)\).
\end{quote}

For \textbf{optimization}, we apply this principle to the \textbf{first derivative} of the objective function \(Q(\theta)\):

\[
Q'(\theta) = 0.
\]

\textbf{Step-by-Step Algorithm for Optimization}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Choose an interval} \([\theta_a, \theta_b]\) such that: \[ Q'(\theta_a) \cdot Q'(\theta_b) < 0. \]
\item
  \textbf{Compute the midpoint}: \[
  \theta_m = \frac{\theta_a + \theta_b}{2}.
  \]
\item
  \textbf{Evaluate} \(Q'(\theta_m)\):

  \begin{itemize}
  \tightlist
  \item
    If \(Q'(\theta_m) = 0\), then \(\theta_m\) is the optimal point.
  \item
    If \(Q'(\theta_a) \cdot Q'(\theta_m) < 0\), set \(\theta_b = \theta_m\).
  \item
    Otherwise, set \(\theta_a = \theta_m\).
  \end{itemize}
\item
  \textbf{Repeat} until convergence: \[
  |\theta_b - \theta_a| < \epsilon.
  \]
\end{enumerate}

\textbf{Determining the Nature of the Critical Point}

Since the Bisection Method finds a \textbf{stationary point}, we use the \textbf{second derivative test} to classify it:

\begin{itemize}
\tightlist
\item
  If \(Q''(\theta) > 0\), the point is a \textbf{local minimum}.
\item
  If \(Q''(\theta) < 0\), the point is a \textbf{local maximum}.
\end{itemize}

For \textbf{nonlinear regression}, we expect \(Q(\theta) = SSE(\theta)\), so the solution found by Bisection should be a \textbf{minimum}.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load required library}
\FunctionTok{library}\NormalTok{(stats)}

\CommentTok{\# Define a numerically stable logistic function}
\NormalTok{safe\_exp }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{}
    \FunctionTok{return}\NormalTok{(}\FunctionTok{ifelse}\NormalTok{(x }\SpecialCharTok{\textgreater{}} \DecValTok{700}\NormalTok{, }\ConstantTok{Inf}\NormalTok{, }\FunctionTok{exp}\NormalTok{(}\FunctionTok{pmin}\NormalTok{(x, }\DecValTok{700}\NormalTok{))))  }\CommentTok{\# Prevent overflow}
\NormalTok{\}}

\CommentTok{\# Define the logistic growth model}
\NormalTok{nonlinear\_model }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x) \{}
    \FunctionTok{return}\NormalTok{(theta[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{safe\_exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{theta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ (x }\SpecialCharTok{{-}}\NormalTok{ theta[}\DecValTok{3}\NormalTok{]))))}
\NormalTok{\}}

\CommentTok{\# Define the Sum of Squared Errors (SSE) function for optimization}
\NormalTok{sse }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x, y) \{}
\NormalTok{    predictions }\OtherTok{\textless{}{-}} \FunctionTok{nonlinear\_model}\NormalTok{(theta, x)}
    \FunctionTok{return}\NormalTok{(}\FunctionTok{sum}\NormalTok{((y }\SpecialCharTok{{-}}\NormalTok{ predictions)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{))}
\NormalTok{\}}

\CommentTok{\# Optimize all three parameters simultaneously}
\NormalTok{find\_optimal\_parameters }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x, y) \{}
    \CommentTok{\# Initial guess for parameters (based on data)}
\NormalTok{    initial\_guess }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{max}\NormalTok{(y), }\DecValTok{1}\NormalTok{, }\FunctionTok{median}\NormalTok{(x))  }

    \CommentTok{\# Bounds for parameters}
\NormalTok{    lower\_bounds }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.01}\NormalTok{, }\FunctionTok{min}\NormalTok{(x))  }\CommentTok{\# Ensure positive scaling}
\NormalTok{    upper\_bounds }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{max}\NormalTok{(y) }\SpecialCharTok{*} \DecValTok{2}\NormalTok{, }\DecValTok{10}\NormalTok{, }\FunctionTok{max}\NormalTok{(x))}

    \CommentTok{\# Run optim() with L{-}BFGS{-}B (bounded optimization)}
\NormalTok{    result }\OtherTok{\textless{}{-}} \FunctionTok{optim}\NormalTok{(}
        \AttributeTok{par =}\NormalTok{ initial\_guess,}
        \AttributeTok{fn =}\NormalTok{ sse,}
        \AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ y,}
        \AttributeTok{method =} \StringTok{"L{-}BFGS{-}B"}\NormalTok{,}
        \AttributeTok{lower =}\NormalTok{ lower\_bounds,}
        \AttributeTok{upper =}\NormalTok{ upper\_bounds}
\NormalTok{    )}
    
    \FunctionTok{return}\NormalTok{(result}\SpecialCharTok{$}\NormalTok{par)  }\CommentTok{\# Extract optimized parameters}
\NormalTok{\}}

\CommentTok{\# Generate synthetic data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{100}\NormalTok{)}
\NormalTok{true\_theta }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\FloatTok{1.5}\NormalTok{, }\DecValTok{0}\NormalTok{)  }\CommentTok{\# True parameters (A, B, C)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{nonlinear\_model}\NormalTok{(true\_theta, x) }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{length}\NormalTok{(x), }\AttributeTok{sd =} \FloatTok{0.3}\NormalTok{)}

\CommentTok{\# Find optimal parameters using optim()}
\NormalTok{estimated\_theta }\OtherTok{\textless{}{-}} \FunctionTok{find\_optimal\_parameters}\NormalTok{(x, y)}

\CommentTok{\# Display results}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Estimated parameters (A, B, C) using optim():}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Estimated parameters (A, B, C) using optim():}
\FunctionTok{print}\NormalTok{(estimated\_theta)}
\CommentTok{\#\textgreater{} [1] 4.06876536 1.42765688 0.01128756}

\CommentTok{\# Plot data and fitted curve}
\FunctionTok{plot}\NormalTok{(}
\NormalTok{    x, y,}
    \AttributeTok{main =} \StringTok{"Optim(): Nonlinear Regression Optimization"}\NormalTok{,}
    \AttributeTok{pch =} \DecValTok{19}\NormalTok{,}
    \AttributeTok{cex =} \FloatTok{0.5}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"x"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"y"}
\NormalTok{)}
\FunctionTok{curve}\NormalTok{(}
    \FunctionTok{nonlinear\_model}\NormalTok{(estimated\_theta, x),}
    \AttributeTok{from =} \FunctionTok{min}\NormalTok{(x),}
    \AttributeTok{to =} \FunctionTok{max}\NormalTok{(x),}
    \AttributeTok{add =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{col =} \StringTok{"red"}\NormalTok{,}
    \AttributeTok{lwd =} \DecValTok{2}
\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}
    \StringTok{"topleft"}\NormalTok{,}
    \AttributeTok{legend =} \FunctionTok{c}\NormalTok{(}\StringTok{"Data"}\NormalTok{, }\StringTok{"Fitted Curve"}\NormalTok{),}
    \AttributeTok{pch =} \FunctionTok{c}\NormalTok{(}\DecValTok{19}\NormalTok{, }\ConstantTok{NA}\NormalTok{),}
    \AttributeTok{lty =} \FunctionTok{c}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\DecValTok{1}\NormalTok{),}
    \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"red"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{06-nonlinear-regession_files/figure-latex/unnamed-chunk-15-1} \end{center}

\hypertarget{stochastic-heuristic-nolinear-regression}{%
\subsection{Stochastic Heuristic}\label{stochastic-heuristic-nolinear-regression}}

\hypertarget{differential-evolution-algorithm}{%
\subsubsection{Differential Evolution Algorithm}\label{differential-evolution-algorithm}}

The \textbf{Differential Evolution (DE) Algorithm} is a \textbf{stochastic, population-based optimization algorithm} that is widely used for solving complex \textbf{global optimization problems}. Unlike gradient-based methods such as Newton's method or the \protect\hyperlink{secant-method}{Secant method}, DE does not require derivatives and is well-suited for optimizing \textbf{non-differentiable, nonlinear, and multimodal functions}.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Key Features of Differential Evolution}

\begin{itemize}
\tightlist
\item
  \textbf{Population-based approach:} Maintains a population of candidate solutions instead of a single point.
\item
  \textbf{Mutation and crossover:} Introduces variations to explore the search space.
\item
  \textbf{Selection mechanism:} Retains the best candidates for the next generation.
\item
  \textbf{Global optimization:} Avoids local minima by using \textbf{stochastic search strategies}.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Mathematical Formulation of Differential Evolution}

Differential Evolution operates on a \textbf{population} of candidate solutions \(\{\theta_i\}\), where each \(\theta_i\) is a \textbf{vector of parameters}. The algorithm iteratively updates the population using three main operations:

\textbf{1. Mutation}

For each candidate solution \(\theta_i\), a \textbf{mutant vector} \(\mathbf{v}_i^{(j)}\) is generated as:

\[
\mathbf{v}_i^{(j)} = \mathbf{\theta}_{r_1}^{(j)} + F \cdot (\mathbf{\theta}_{r_2}^{(j)} - \mathbf{\theta}_{r_3}^{(j)})
\]

where:

\begin{itemize}
\item
  \(\mathbf{\theta}_{r_1}, \mathbf{\theta}_{r_2}, \mathbf{\theta}_{r_3}\) are \textbf{randomly selected distinct vectors} from the population.
\item
  \(F \in (0,2)\) is the \textbf{mutation factor} controlling the step size.
\end{itemize}

\textbf{2. Crossover}

A trial vector \(\mathbf{u}_i^{(j)}\) is generated by combining the mutant vector \(\mathbf{v}_i^{(j)}\) with the original solution \(\mathbf{\theta}_i^{(j)}\):

\[
u_{i,k}^{(j)} =
\begin{cases}
v_{i,k}^{(j)}  & \text{if } rand_k \leq C_r \text{ or } k = k_{\text{rand}}, \\
\theta_{i,k}^{(j)}  & \text{otherwise}.
\end{cases}
\]

where:

\begin{itemize}
\item
  \(C_r \in (0,1)\) is the \textbf{crossover probability}.
\item
  \(rand_k\) is a \textbf{random value between 0 and 1}.
\item
  \(k_{\text{rand}}\) ensures at least \textbf{one parameter is mutated}.
\end{itemize}

\textbf{3. Selection}

The new candidate solution is accepted only if it improves the objective function:

\[
\mathbf{\theta}_i^{(j+1)} =
\begin{cases}
\mathbf{u}_i^{(j)} & \text{if } Q(\mathbf{u}_i^{(j)}) < Q(\mathbf{\theta}_i^{(j)}), \\
\mathbf{\theta}_i^{(j)} & \text{otherwise}.
\end{cases}
\]

where \(Q(\theta)\) is the \textbf{objective function} (e.g., sum of squared errors in regression problems).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Algorithm: Differential Evolution for Nonlinear Optimization}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Initialize a population of candidate solutions.
\item
  Evaluate the objective function for each candidate.
\item
  Mutate individuals using a difference strategy.
\item
  Apply crossover to create trial solutions.
\item
  Select individuals based on their fitness (objective function value).
\item
  Repeat until convergence (or a stopping criterion is met).
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load required library}
\FunctionTok{library}\NormalTok{(DEoptim)}

\CommentTok{\# Define a nonlinear function (logistic model)}
\NormalTok{nonlinear\_model }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x) \{}
    \FunctionTok{return}\NormalTok{(theta[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{theta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ (x }\SpecialCharTok{{-}}\NormalTok{ theta[}\DecValTok{3}\NormalTok{]))))}
\NormalTok{\}}

\CommentTok{\# Define the Sum of Squared Errors (SSE) function}
\NormalTok{sse }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x, y) \{}
    \FunctionTok{return}\NormalTok{(}\FunctionTok{sum}\NormalTok{((y }\SpecialCharTok{{-}} \FunctionTok{nonlinear\_model}\NormalTok{(theta, x))}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{))}
\NormalTok{\}}

\CommentTok{\# Generate synthetic data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{100}\NormalTok{)}
\NormalTok{true\_theta }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\FloatTok{1.5}\NormalTok{, }\DecValTok{0}\NormalTok{)  }\CommentTok{\# True parameters (A, B, C)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{nonlinear\_model}\NormalTok{(true\_theta, x) }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{length}\NormalTok{(x), }\AttributeTok{sd =} \FloatTok{0.3}\NormalTok{)}

\CommentTok{\# Define the objective function for DEoptim}
\NormalTok{objective\_function }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta) \{}
    \FunctionTok{return}\NormalTok{(}\FunctionTok{sse}\NormalTok{(theta, x, y))}
\NormalTok{\}}

\CommentTok{\# Define parameter bounds}
\NormalTok{lower\_bounds }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{)}
\NormalTok{upper\_bounds }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)}

\CommentTok{\# Run Differential Evolution Algorithm}
\NormalTok{de\_result }\OtherTok{\textless{}{-}}
    \FunctionTok{DEoptim}\NormalTok{(}
\NormalTok{        objective\_function,}
\NormalTok{        lower\_bounds,}
\NormalTok{        upper\_bounds,}
        \FunctionTok{DEoptim.control}\NormalTok{(}
            \AttributeTok{NP =} \DecValTok{50}\NormalTok{,}
            \AttributeTok{itermax =} \DecValTok{100}\NormalTok{,}
            \AttributeTok{F =} \FloatTok{0.8}\NormalTok{,}
            \AttributeTok{CR =} \FloatTok{0.9}\NormalTok{, }
            \AttributeTok{trace =}\NormalTok{ F}
\NormalTok{        )}
\NormalTok{    )}

\CommentTok{\# Extract optimized parameters}
\NormalTok{estimated\_theta }\OtherTok{\textless{}{-}}\NormalTok{ de\_result}\SpecialCharTok{$}\NormalTok{optim}\SpecialCharTok{$}\NormalTok{bestmem}

\CommentTok{\# Display estimated parameters}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Estimated parameters (A, B, C) using Differential Evolution:}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Estimated parameters (A, B, C) using Differential Evolution:}
\FunctionTok{print}\NormalTok{(estimated\_theta)}
\CommentTok{\#\textgreater{}       par1       par2       par3 }
\CommentTok{\#\textgreater{} 4.06876562 1.42765614 0.01128768}

\CommentTok{\# Plot data and fitted curve}
\FunctionTok{plot}\NormalTok{(}
\NormalTok{    x,}
\NormalTok{    y,}
    \AttributeTok{main =} \StringTok{"Differential Evolution: Data \& Fitted Curve"}\NormalTok{,}
    \AttributeTok{pch =} \DecValTok{19}\NormalTok{,}
    \AttributeTok{cex =} \FloatTok{0.5}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"x"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"y"}
\NormalTok{)}
\FunctionTok{curve}\NormalTok{(}
    \FunctionTok{nonlinear\_model}\NormalTok{(estimated\_theta, x),}
    \AttributeTok{from =} \SpecialCharTok{{-}}\DecValTok{5}\NormalTok{,}
    \AttributeTok{to =} \DecValTok{5}\NormalTok{,}
    \AttributeTok{add =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{col =} \StringTok{"red"}\NormalTok{,}
    \AttributeTok{lwd =} \DecValTok{2}
\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}
    \StringTok{"topleft"}\NormalTok{,}
    \AttributeTok{legend =} \FunctionTok{c}\NormalTok{(}\StringTok{"Data"}\NormalTok{, }\StringTok{"Fitted Curve"}\NormalTok{),}
    \AttributeTok{pch =} \FunctionTok{c}\NormalTok{(}\DecValTok{19}\NormalTok{, }\ConstantTok{NA}\NormalTok{),}
    \AttributeTok{lty =} \FunctionTok{c}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\DecValTok{1}\NormalTok{),}
    \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"red"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{06-nonlinear-regession_files/figure-latex/unnamed-chunk-16-1} \end{center}

\hypertarget{simulated-annealing}{%
\subsubsection{Simulated Annealing}\label{simulated-annealing}}

Simulated Annealing (SA) is a probabilistic global optimization algorithm inspired by annealing in metallurgy, where a material is heated and slowly cooled to remove defects. In optimization, SA gradually refines a solution by exploring the search space, allowing occasional jumps to escape local minima, before converging to an optimal solution.

Simulated Annealing is particularly useful when:

\begin{itemize}
\item
  The function is highly nonlinear and multimodal.
\item
  Gradient-based methods struggle due to non-differentiability or poor initialization.
\item
  A global minimum is needed, rather than a local one.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{1. Energy Function (Objective Function)}

The goal of SA is to minimize an objective function \(Q(\theta)\). For nonlinear regression, this is the Sum of Squared Errors (SSE):

\[
Q(\theta) = SSE(\theta) = \sum_{i=1}^{n} (y_i - f(x_i; \theta))^2.
\]

\textbf{2. Probability of Acceptance}

At each step, SA randomly perturbs the parameters \(\theta\) to create a \textbf{new candidate solution} \(\theta'\) and evaluates the change in SSE:

\[
\Delta Q = Q(\theta') - Q(\theta).
\]

The \textbf{Metropolis Criterion} determines whether to accept the new solution:

\[
P(\text{accept}) =
\begin{cases}
1, & \text{if } \Delta Q < 0 \quad \text{(new solution improves fit)} \\
\exp\left( -\frac{\Delta Q}{T} \right), & \text{if } \Delta Q \geq 0 \quad \text{(accept with probability)}.
\end{cases}
\]

where:

\begin{itemize}
\item
  \(T\) is the \textbf{temperature} that \textbf{gradually decreases} over iterations.
\item
  \textbf{Worse solutions} are accepted \textbf{with small probability} to escape local minima.
\end{itemize}

\textbf{3. Cooling Schedule}

The \textbf{temperature} follows a cooling schedule:

\[
T_k = \alpha T_{k-1},
\]

where \(\alpha \in (0,1)\) is a decay factor that controls cooling speed.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Simulated Annealing Algorithm}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Initialize parameters \(\theta\) randomly.
\item
  Set an initial temperature \(T_0\) and cooling rate \(\alpha\).
\item
  Repeat for max iterations:

  \begin{itemize}
  \tightlist
  \item
    Generate perturbed candidate \(\theta'\).
  \item
    Compute \(\Delta Q = Q(\theta') - Q(\theta)\).
  \item
    Accept if \(\Delta Q < 0\) or with probability \(\exp(-\Delta Q / T)\).
  \item
    Reduce temperature: \(T \leftarrow \alpha T\).
  \end{itemize}
\item
  Return the best solution found.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load required library}
\FunctionTok{library}\NormalTok{(stats)}

\CommentTok{\# Define a numerically stable logistic function}
\NormalTok{safe\_exp }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{}
    \FunctionTok{return}\NormalTok{(}\FunctionTok{ifelse}\NormalTok{(x }\SpecialCharTok{\textgreater{}} \DecValTok{700}\NormalTok{, }\ConstantTok{Inf}\NormalTok{, }\FunctionTok{exp}\NormalTok{(}\FunctionTok{pmin}\NormalTok{(x, }\DecValTok{700}\NormalTok{))))  }\CommentTok{\# Prevent overflow}
\NormalTok{\}}

\CommentTok{\# Define the logistic growth model}
\NormalTok{nonlinear\_model }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x) \{}
    \FunctionTok{return}\NormalTok{(theta[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{safe\_exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{theta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ (x }\SpecialCharTok{{-}}\NormalTok{ theta[}\DecValTok{3}\NormalTok{]))))}
\NormalTok{\}}

\CommentTok{\# Define the Sum of Squared Errors (SSE) function}
\NormalTok{sse }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x, y) \{}
\NormalTok{    predictions }\OtherTok{\textless{}{-}} \FunctionTok{nonlinear\_model}\NormalTok{(theta, x)}
    \FunctionTok{return}\NormalTok{(}\FunctionTok{sum}\NormalTok{((y }\SpecialCharTok{{-}}\NormalTok{ predictions) }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{))}
\NormalTok{\}}

\CommentTok{\# Simulated Annealing Algorithm}
\NormalTok{simulated\_annealing }\OtherTok{\textless{}{-}}
    \ControlFlowTok{function}\NormalTok{(x,}
\NormalTok{             y,}
\NormalTok{             initial\_theta,}
             \AttributeTok{T\_init =} \FloatTok{1.0}\NormalTok{,}
             \AttributeTok{alpha =} \FloatTok{0.99}\NormalTok{,}
             \AttributeTok{max\_iter =} \DecValTok{5000}\NormalTok{) \{}
        \CommentTok{\# Initialize parameters}
\NormalTok{        theta }\OtherTok{\textless{}{-}}\NormalTok{ initial\_theta}
\NormalTok{        best\_theta }\OtherTok{\textless{}{-}}\NormalTok{ theta}
\NormalTok{        best\_sse }\OtherTok{\textless{}{-}} \FunctionTok{sse}\NormalTok{(theta, x, y)}
\NormalTok{        T }\OtherTok{\textless{}{-}}\NormalTok{ T\_init  }\CommentTok{\# Initial temperature}
        
        \ControlFlowTok{for}\NormalTok{ (iter }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{max\_iter) \{}
            \CommentTok{\# Generate new candidate solution (small random perturbation)}
\NormalTok{            theta\_new }\OtherTok{\textless{}{-}}\NormalTok{ theta }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{length}\NormalTok{(theta), }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =}\NormalTok{ T)}
            
            \CommentTok{\# Compute new SSE}
\NormalTok{            sse\_new }\OtherTok{\textless{}{-}} \FunctionTok{sse}\NormalTok{(theta\_new, x, y)}
            
            \CommentTok{\# Compute change in SSE}
\NormalTok{            delta\_Q }\OtherTok{\textless{}{-}}\NormalTok{ sse\_new }\SpecialCharTok{{-}}\NormalTok{ best\_sse}
            
            \CommentTok{\# Acceptance criteria}
            \ControlFlowTok{if}\NormalTok{ (delta\_Q }\SpecialCharTok{\textless{}} \DecValTok{0} \SpecialCharTok{||} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{) }\SpecialCharTok{\textless{}} \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{delta\_Q }\SpecialCharTok{/}\NormalTok{ T)) \{}
\NormalTok{                theta }\OtherTok{\textless{}{-}}\NormalTok{ theta\_new}
\NormalTok{                best\_sse }\OtherTok{\textless{}{-}}\NormalTok{ sse\_new}
\NormalTok{                best\_theta }\OtherTok{\textless{}{-}}\NormalTok{ theta\_new}
\NormalTok{            \}}
            
            \CommentTok{\# Reduce temperature}
\NormalTok{            T }\OtherTok{\textless{}{-}}\NormalTok{ alpha }\SpecialCharTok{*}\NormalTok{ T}
            
            \CommentTok{\# Stopping condition (very low temperature)}
            \ControlFlowTok{if}\NormalTok{ (T }\SpecialCharTok{\textless{}} \FloatTok{1e{-}6}\NormalTok{)}
                \ControlFlowTok{break}
\NormalTok{        \}}
        
        \FunctionTok{return}\NormalTok{(best\_theta)}
\NormalTok{    \}}

\CommentTok{\# Generate synthetic data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{100}\NormalTok{)}
\NormalTok{true\_theta }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\FloatTok{1.5}\NormalTok{, }\DecValTok{0}\NormalTok{)  }\CommentTok{\# True parameters (A, B, C)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{nonlinear\_model}\NormalTok{(true\_theta, x) }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{length}\NormalTok{(x), }\AttributeTok{sd =} \FloatTok{0.3}\NormalTok{)}

\CommentTok{\# Initial guess}
\NormalTok{initial\_theta }\OtherTok{\textless{}{-}}
    \FunctionTok{c}\NormalTok{(}\FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{), }\FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\DecValTok{3}\NormalTok{), }\FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{,}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}

\CommentTok{\# Run Simulated Annealing}
\NormalTok{estimated\_theta }\OtherTok{\textless{}{-}} \FunctionTok{simulated\_annealing}\NormalTok{(x, y, initial\_theta)}

\CommentTok{\# Display results}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Estimated parameters (A, B, C) using Simulated Annealing:}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Estimated parameters (A, B, C) using Simulated Annealing:}
\FunctionTok{print}\NormalTok{(estimated\_theta)}
\CommentTok{\#\textgreater{} [1] 4.07180419 1.41457906 0.01422147}

\CommentTok{\# Plot data and fitted curve}
\FunctionTok{plot}\NormalTok{(}
\NormalTok{    x,}
\NormalTok{    y,}
    \AttributeTok{main =} \StringTok{"Simulated Annealing: Nonlinear Regression Optimization"}\NormalTok{,}
    \AttributeTok{pch =} \DecValTok{19}\NormalTok{,}
    \AttributeTok{cex =} \FloatTok{0.5}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"x"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"y"}
\NormalTok{)}
\FunctionTok{curve}\NormalTok{(}
    \FunctionTok{nonlinear\_model}\NormalTok{(estimated\_theta, x),}
    \AttributeTok{from =} \FunctionTok{min}\NormalTok{(x),}
    \AttributeTok{to =} \FunctionTok{max}\NormalTok{(x),}
    \AttributeTok{add =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{col =} \StringTok{"red"}\NormalTok{,}
    \AttributeTok{lwd =} \DecValTok{2}
\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}
    \StringTok{"topleft"}\NormalTok{,}
    \AttributeTok{legend =} \FunctionTok{c}\NormalTok{(}\StringTok{"Data"}\NormalTok{, }\StringTok{"Fitted Curve"}\NormalTok{),}
    \AttributeTok{pch =} \FunctionTok{c}\NormalTok{(}\DecValTok{19}\NormalTok{, }\ConstantTok{NA}\NormalTok{),}
    \AttributeTok{lty =} \FunctionTok{c}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\DecValTok{1}\NormalTok{),}
    \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"red"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{06-nonlinear-regession_files/figure-latex/unnamed-chunk-17-1} \end{center}

\hypertarget{genetic-algorithm}{%
\subsubsection{Genetic Algorithm}\label{genetic-algorithm}}

Genetic Algorithms (GA) are a class of evolutionary algorithms inspired by the principles of natural selection and genetics. Unlike deterministic optimization techniques, GA evolves a population of candidate solutions over multiple generations, using genetic operators such as selection, crossover, and mutation.

GA is particularly useful when:

\begin{itemize}
\item
  The function is nonlinear, non-differentiable, or highly multimodal.
\item
  Gradient-based methods fail due to rugged function landscapes.
\item
  A global minimum is required, rather than a local one.
\end{itemize}

The goal of a Genetic Algorithm is to find an optimal solution \$\textbackslash hat\{\textbackslash theta\}\$ that minimizes an objective function:

\[
\hat{\theta} = \arg\min_{\theta} SSE(\theta),
\]

where:

\[
SSE(\theta) = \sum_{i=1}^{n} (y_i - f(x_i; \theta))^2.
\]

\textbf{1. Population Representation}

Each candidate solution (individual) is represented as a \textbf{chromosome}, which is simply a vector of parameters: \[
\theta = (\theta_1, \theta_2, \theta_3)
\]

An entire \textbf{population} consists of multiple such solutions.

\textbf{2. Selection}

Each individual's fitness is evaluated using:

\[
\text{Fitness}(\theta) = -SSE(\theta)
\]

We use \textbf{Tournament Selection} or \textbf{Roulette Wheel Selection} to choose parents for reproduction.

\textbf{3. Crossover (Recombination)}

A new solution \(\theta'\) is generated by \textbf{combining} two parents:

\[\theta' = \alpha \theta_{\text{parent1}} + (1 - \alpha) \theta_{\text{parent2}}, \quad \alpha \sim U(0,1).\]

\textbf{4. Mutation}

Random small changes are introduced to \textbf{increase diversity}:

\[\theta_i' = \theta_i + \mathcal{N}(0, \sigma),\]

where \(\mathcal{N}(0, \sigma)\) is a \textbf{small Gaussian perturbation}.

\textbf{5. Evolutionary Cycle}

The \textbf{algorithm iterates} through:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Selection
\item
  Crossover
\item
  Mutation
\item
  Survival of the fittest
\item
  Termination when convergence is reached.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load required library}
\FunctionTok{library}\NormalTok{(GA)}

\CommentTok{\# Define a numerically stable logistic function}
\NormalTok{safe\_exp }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{}
    \FunctionTok{return}\NormalTok{(}\FunctionTok{ifelse}\NormalTok{(x }\SpecialCharTok{\textgreater{}} \DecValTok{700}\NormalTok{, }\ConstantTok{Inf}\NormalTok{, }\FunctionTok{exp}\NormalTok{(}\FunctionTok{pmin}\NormalTok{(x, }\DecValTok{700}\NormalTok{))))  }\CommentTok{\# Prevent overflow}
\NormalTok{\}}

\CommentTok{\# Define the logistic growth model}
\NormalTok{nonlinear\_model }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x) \{}
    \FunctionTok{return}\NormalTok{(theta[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{safe\_exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{theta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ (x }\SpecialCharTok{{-}}\NormalTok{ theta[}\DecValTok{3}\NormalTok{]))))}
\NormalTok{\}}

\CommentTok{\# Define the Sum of Squared Errors (SSE) function for optimization}
\NormalTok{sse }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x, y) \{}
\NormalTok{    predictions }\OtherTok{\textless{}{-}} \FunctionTok{nonlinear\_model}\NormalTok{(theta, x)}
    \FunctionTok{return}\NormalTok{(}\FunctionTok{sum}\NormalTok{((y }\SpecialCharTok{{-}}\NormalTok{ predictions) }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{))}
\NormalTok{\}}

\CommentTok{\# Genetic Algorithm for Optimization}
\NormalTok{ga\_optimization }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x, y) \{}
    \CommentTok{\# Define fitness function (negative SSE for maximization)}
\NormalTok{    fitness\_function }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta) \{}
        \CommentTok{\# GA maximizes fitness, so we use negative SSE}
        \FunctionTok{return}\NormalTok{(}\SpecialCharTok{{-}}\FunctionTok{sse}\NormalTok{(theta, x, y))  }
\NormalTok{    \}}
    
    \CommentTok{\# Set parameter bounds}
\NormalTok{    lower\_bounds }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.01}\NormalTok{, }\FunctionTok{min}\NormalTok{(x))  }\CommentTok{\# Ensure positive scaling}
\NormalTok{    upper\_bounds }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{max}\NormalTok{(y) }\SpecialCharTok{*} \DecValTok{2}\NormalTok{, }\DecValTok{10}\NormalTok{, }\FunctionTok{max}\NormalTok{(x))}
    
    \CommentTok{\# Run GA optimization}
\NormalTok{    ga\_result }\OtherTok{\textless{}{-}} \FunctionTok{ga}\NormalTok{(}
        \AttributeTok{type =} \StringTok{"real{-}valued"}\NormalTok{,}
        \AttributeTok{fitness =}\NormalTok{ fitness\_function,}
        \AttributeTok{lower =}\NormalTok{ lower\_bounds,}
        \AttributeTok{upper =}\NormalTok{ upper\_bounds,}
        \AttributeTok{popSize =} \DecValTok{50}\NormalTok{,}
        \CommentTok{\# Population size}
        \AttributeTok{maxiter =} \DecValTok{200}\NormalTok{,}
        \CommentTok{\# Max generations}
        \AttributeTok{pmutation =} \FloatTok{0.1}\NormalTok{,}
        \CommentTok{\# Mutation probability}
        \AttributeTok{monitor =} \ConstantTok{FALSE}
\NormalTok{    )}
    
    \FunctionTok{return}\NormalTok{(ga\_result}\SpecialCharTok{@}\NormalTok{solution)  }\CommentTok{\# Return optimized parameters}
\NormalTok{\}}

\CommentTok{\# Generate synthetic data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{100}\NormalTok{)}
\NormalTok{true\_theta }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\FloatTok{1.5}\NormalTok{, }\DecValTok{0}\NormalTok{)  }\CommentTok{\# True parameters (A, B, C)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{nonlinear\_model}\NormalTok{(true\_theta, x) }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{length}\NormalTok{(x), }\AttributeTok{sd =} \FloatTok{0.3}\NormalTok{)}

\CommentTok{\# Run Genetic Algorithm}
\NormalTok{estimated\_theta }\OtherTok{\textless{}{-}} \FunctionTok{ga\_optimization}\NormalTok{(x, y)}

\CommentTok{\# Display results}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Estimated parameters (A, B, C) using Genetic Algorithm:}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Estimated parameters (A, B, C) using Genetic Algorithm:}
\FunctionTok{print}\NormalTok{(estimated\_theta)}
\CommentTok{\#\textgreater{}            x1       x2         x3}
\CommentTok{\#\textgreater{} [1,] 4.066144 1.433886 0.00824126}

\CommentTok{\# Plot data and fitted curve}
\FunctionTok{plot}\NormalTok{(}
\NormalTok{    x,}
\NormalTok{    y,}
    \AttributeTok{main =} \StringTok{"Genetic Algorithm: Nonlinear Regression Optimization"}\NormalTok{,}
    \AttributeTok{pch =} \DecValTok{19}\NormalTok{,}
    \AttributeTok{cex =} \FloatTok{0.5}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"x"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"y"}
\NormalTok{)}
\FunctionTok{curve}\NormalTok{(}
    \FunctionTok{nonlinear\_model}\NormalTok{(estimated\_theta, x),}
    \AttributeTok{from =} \FunctionTok{min}\NormalTok{(x),}
    \AttributeTok{to =} \FunctionTok{max}\NormalTok{(x),}
    \AttributeTok{add =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{col =} \StringTok{"red"}\NormalTok{,}
    \AttributeTok{lwd =} \DecValTok{2}
\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}
    \StringTok{"topleft"}\NormalTok{,}
    \AttributeTok{legend =} \FunctionTok{c}\NormalTok{(}\StringTok{"Data"}\NormalTok{, }\StringTok{"Fitted Curve"}\NormalTok{),}
    \AttributeTok{pch =} \FunctionTok{c}\NormalTok{(}\DecValTok{19}\NormalTok{, }\ConstantTok{NA}\NormalTok{),}
    \AttributeTok{lty =} \FunctionTok{c}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\DecValTok{1}\NormalTok{),}
    \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"red"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{06-nonlinear-regession_files/figure-latex/unnamed-chunk-18-1} \end{center}

\hypertarget{particle-swarm-optimization}{%
\subsubsection{Particle Swarm Optimization}\label{particle-swarm-optimization}}

Particle Swarm Optimization (PSO) is a population-based global optimization algorithm inspired by the social behavior of birds and fish schools. Instead of using genetic operators (like in Genetic Algorithms), PSO models particles (solutions) flying through the search space, adjusting their position based on their own experience and the experience of their neighbors.

PSO is particularly useful when:

\begin{itemize}
\item
  The function is nonlinear, noisy, or lacks smooth gradients.
\item
  Gradient-based methods struggle due to non-differentiability.
\item
  A global minimum is needed, rather than a local one.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The goal of PSO is to find an optimal solution \(\hat{\theta}\) that minimizes an objective function:

\[
\hat{\theta} = \arg\min_{\theta} SSE(\theta),
\]

where:

\[
SSE(\theta) = \sum_{i=1}^{n} (y_i - f(x_i; \theta))^2.
\]

\textbf{1. Particle Representation}

Each \textbf{particle} represents a candidate solution:

\[
\theta_i = (\theta_{i1}, \theta_{i2}, \theta_{i3})
\]

where \(\theta_{ij}\) is the \(j^{th}\) parameter of particle \(i\).

\textbf{2. Particle Velocity and Position Updates}

Each particle moves in the search space with \textbf{velocity} \(v_i\), which is updated as:

\[
v_i^{(t+1)} = \omega v_i^{(t)} + c_1 r_1 (p_i - \theta_i^{(t)}) + c_2 r_2 (g - \theta_i^{(t)})
\]

where:

\begin{itemize}
\item
  \(\omega\) is the \textbf{inertia weight} (controls exploration vs.~exploitation),
\item
  \(c_1, c_2\) are \textbf{acceleration coefficients},
\item
  \(r_1, r_2 \sim U(0,1)\) are random numbers,
\item
  \(p_i\) is the \textbf{particle's personal best position},
\item
  \(g\) is the \textbf{global best position}.
\end{itemize}

Then, the \textbf{position update} is:

\[
\theta_i^{(t+1)} = \theta_i^{(t)} + v_i^{(t+1)}
\]

This process continues until \textbf{convergence criteria} (like a max number of iterations or minimum error) is met.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Particle Swarm Optimization Algorithm}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Initialize particles randomly within search bounds.
\item
  Set random initial velocities.
\item
  Evaluate SSE for each particle.
\item
  Update the personal and global best solutions.
\item
  Update velocities and positions using the update equations.
\item
  Repeat until convergence.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load required library}
\FunctionTok{library}\NormalTok{(pso)}

\CommentTok{\# Define a numerically stable logistic function}
\NormalTok{safe\_exp }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{}
    \FunctionTok{return}\NormalTok{(}\FunctionTok{ifelse}\NormalTok{(x }\SpecialCharTok{\textgreater{}} \DecValTok{700}\NormalTok{, }\ConstantTok{Inf}\NormalTok{, }\FunctionTok{exp}\NormalTok{(}\FunctionTok{pmin}\NormalTok{(x, }\DecValTok{700}\NormalTok{))))  }\CommentTok{\# Prevent overflow}
\NormalTok{\}}

\CommentTok{\# Define the logistic growth model}
\NormalTok{nonlinear\_model }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x) \{}
    \FunctionTok{return}\NormalTok{(theta[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{safe\_exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{theta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ (x }\SpecialCharTok{{-}}\NormalTok{ theta[}\DecValTok{3}\NormalTok{]))))}
\NormalTok{\}}

\CommentTok{\# Define the Sum of Squared Errors (SSE) function for optimization}
\NormalTok{sse }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x, y) \{}
\NormalTok{    predictions }\OtherTok{\textless{}{-}} \FunctionTok{nonlinear\_model}\NormalTok{(theta, x)}
    \FunctionTok{return}\NormalTok{(}\FunctionTok{sum}\NormalTok{((y }\SpecialCharTok{{-}}\NormalTok{ predictions) }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{))}
\NormalTok{\}}

\CommentTok{\# Particle Swarm Optimization (PSO) for Nonlinear Regression}
\NormalTok{pso\_optimization }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x, y) \{}
    \CommentTok{\# Define fitness function (minimize SSE)}
\NormalTok{    fitness\_function }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta) \{}
        \FunctionTok{return}\NormalTok{(}\FunctionTok{sse}\NormalTok{(theta, x, y))}
\NormalTok{    \}}
    
    \CommentTok{\# Set parameter bounds}
\NormalTok{    lower\_bounds }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.01}\NormalTok{, }\FunctionTok{min}\NormalTok{(x))  }\CommentTok{\# Ensure positive scaling}
\NormalTok{    upper\_bounds }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{max}\NormalTok{(y) }\SpecialCharTok{*} \DecValTok{2}\NormalTok{, }\DecValTok{10}\NormalTok{, }\FunctionTok{max}\NormalTok{(x))}
    
    \CommentTok{\# Run PSO optimization}
\NormalTok{    pso\_result }\OtherTok{\textless{}{-}} \FunctionTok{psoptim}\NormalTok{(}
        \AttributeTok{par =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{),}
        \CommentTok{\# Initial guess}
        \AttributeTok{fn =}\NormalTok{ fitness\_function,}
        \AttributeTok{lower =}\NormalTok{ lower\_bounds,}
        \AttributeTok{upper =}\NormalTok{ upper\_bounds,}
        \AttributeTok{control =} \FunctionTok{list}\NormalTok{(}\AttributeTok{maxit =} \DecValTok{200}\NormalTok{, }\AttributeTok{s =} \DecValTok{50}\NormalTok{)  }\CommentTok{\# 200 iterations, 50 particles}
\NormalTok{    )}
    
    \FunctionTok{return}\NormalTok{(pso\_result}\SpecialCharTok{$}\NormalTok{par)  }\CommentTok{\# Return optimized parameters}
\NormalTok{\}}

\CommentTok{\# Generate synthetic data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{100}\NormalTok{)}
\NormalTok{true\_theta }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\FloatTok{1.5}\NormalTok{, }\DecValTok{0}\NormalTok{)  }\CommentTok{\# True parameters (A, B, C)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{nonlinear\_model}\NormalTok{(true\_theta, x) }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{length}\NormalTok{(x), }\AttributeTok{sd =} \FloatTok{0.3}\NormalTok{)}

\CommentTok{\# Run Particle Swarm Optimization}
\NormalTok{estimated\_theta }\OtherTok{\textless{}{-}} \FunctionTok{pso\_optimization}\NormalTok{(x, y)}

\CommentTok{\# Display results}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Estimated parameters (A, B, C) using Particle Swarm Optimization:}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Estimated parameters (A, B, C) using Particle Swarm Optimization:}
\FunctionTok{print}\NormalTok{(estimated\_theta)}
\CommentTok{\#\textgreater{} [1] 4.06876562 1.42765613 0.01128767}

\CommentTok{\# Plot data and fitted curve}
\FunctionTok{plot}\NormalTok{(}
\NormalTok{    x,}
\NormalTok{    y,}
    \AttributeTok{main =} \StringTok{"Particle Swarm Optimization: Nonlinear Regression Optimization"}\NormalTok{,}
    \AttributeTok{pch =} \DecValTok{19}\NormalTok{,}
    \AttributeTok{cex =} \FloatTok{0.5}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"x"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"y"}
\NormalTok{)}
\FunctionTok{curve}\NormalTok{(}
    \FunctionTok{nonlinear\_model}\NormalTok{(estimated\_theta, x),}
    \AttributeTok{from =} \FunctionTok{min}\NormalTok{(x),}
    \AttributeTok{to =} \FunctionTok{max}\NormalTok{(x),}
    \AttributeTok{add =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{col =} \StringTok{"red"}\NormalTok{,}
    \AttributeTok{lwd =} \DecValTok{2}
\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}
    \StringTok{"topleft"}\NormalTok{,}
    \AttributeTok{legend =} \FunctionTok{c}\NormalTok{(}\StringTok{"Data"}\NormalTok{, }\StringTok{"Fitted Curve"}\NormalTok{),}
    \AttributeTok{pch =} \FunctionTok{c}\NormalTok{(}\DecValTok{19}\NormalTok{, }\ConstantTok{NA}\NormalTok{),}
    \AttributeTok{lty =} \FunctionTok{c}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\DecValTok{1}\NormalTok{),}
    \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"red"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{06-nonlinear-regession_files/figure-latex/unnamed-chunk-19-1} \end{center}

\hypertarget{evolutionary-strategies}{%
\subsubsection{Evolutionary Strategies}\label{evolutionary-strategies}}

Evolutionary Strategies (ES) are a class of evolutionary optimization algorithms that improve solutions by mutating and selecting individuals based on fitness. Unlike \protect\hyperlink{genetic-algorithm}{Genetic Algorithm}, ES focuses on self-adaptive mutation rates and selection pressure rather than crossover. This makes ES particularly robust for continuous optimization problems like nonlinear regression.

ES is particularly useful when:

\begin{itemize}
\item
  The function is complex, noisy, or lacks smooth gradients.
\item
  Gradient-based methods fail due to non-differentiability.
\item
  An adaptive approach to exploration and exploitation is needed.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The goal of ES is to find an optimal solution \(\hat{\theta}\) that minimizes an objective function:

\[
\hat{\theta} = \arg\min_{\theta} SSE(\theta),
\]

where:

\[
SSE(\theta) = \sum_{i=1}^{n} (y_i - f(x_i; \theta))^2.
\]

\textbf{1. Population Representation}

Each \textbf{individual} is a solution \(\theta_i\) in the parameter space:

\[
\theta_i = (\theta_{i1}, \theta_{i2}, \theta_{i3}).
\]

The \textbf{population} consists of multiple individuals, each representing different candidate parameters.

\textbf{2. Mutation}

New candidate solutions are generated by \textbf{adding random noise}:

\[
\theta'_i = \theta_i + \sigma \mathcal{N}(0, I),
\]

where:

\begin{itemize}
\item
  \(\sigma\) is the \textbf{mutation step size}, which adapts over time.
\item
  \(\mathcal{N}(0, I)\) is a standard normal distribution.
\end{itemize}

\textbf{3. Selection}

ES employs \((\mu, \lambda)\)-selection:

\begin{itemize}
\item
  \((\mu, \lambda)\)-ES: Select the best \(\mu\) solutions from \(\lambda\) offspring.
\item
  \((\mu + \lambda)\)-ES: Combine parents and offspring, selecting the top \(\mu\).
\end{itemize}

\textbf{4. Step-Size Adaptation}

Mutation strength \(\sigma\) \textbf{self-adapts} using the \textbf{1/5 success rule}:

\[
\sigma_{t+1} =
\begin{cases}
\sigma_t / c, & \text{if success rate } > 1/5 \\
\sigma_t \cdot c, & \text{if success rate } < 1/5
\end{cases}
\]

where \(c > 1\) is a scaling factor.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Evolutionary Strategies Algorithm}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Initialize a population of \(\lambda\) solutions with random parameters.
\item
  Set mutation step size \(\sigma\).
\item
  Repeat for max iterations:

  \begin{itemize}
  \tightlist
  \item
    Generate \(\lambda\) offspring by mutating parent solutions.
  \item
    Evaluate fitness (SSE) of each offspring.
  \item
    Select the best \(\mu\) solutions for the next generation.
  \item
    Adapt mutation step size based on success rate.
  \end{itemize}
\item
  Return the best solution found.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load required library}
\FunctionTok{library}\NormalTok{(DEoptim)}

\CommentTok{\# Define a numerically stable logistic function}
\NormalTok{safe\_exp }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{}
    \FunctionTok{return}\NormalTok{(}\FunctionTok{ifelse}\NormalTok{(x }\SpecialCharTok{\textgreater{}} \DecValTok{700}\NormalTok{, }\ConstantTok{Inf}\NormalTok{, }\FunctionTok{exp}\NormalTok{(}\FunctionTok{pmin}\NormalTok{(x, }\DecValTok{700}\NormalTok{))))  }\CommentTok{\# Prevent overflow}
\NormalTok{\}}

\CommentTok{\# Define the logistic growth model}
\NormalTok{nonlinear\_model }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x) \{}
    \FunctionTok{return}\NormalTok{(theta[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{safe\_exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{theta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ (x }\SpecialCharTok{{-}}\NormalTok{ theta[}\DecValTok{3}\NormalTok{]))))}
\NormalTok{\}}

\CommentTok{\# Define the Sum of Squared Errors (SSE) function for optimization}
\NormalTok{sse }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x, y) \{}
\NormalTok{    predictions }\OtherTok{\textless{}{-}} \FunctionTok{nonlinear\_model}\NormalTok{(theta, x)}
    \FunctionTok{return}\NormalTok{(}\FunctionTok{sum}\NormalTok{((y }\SpecialCharTok{{-}}\NormalTok{ predictions)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{))}
\NormalTok{\}}

\CommentTok{\# Evolutionary Strategies Optimization (Using Differential Evolution)}
\NormalTok{es\_optimization }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x, y) \{}
    \CommentTok{\# Define fitness function (minimize SSE)}
\NormalTok{    fitness\_function }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta) \{}
        \FunctionTok{return}\NormalTok{(}\FunctionTok{sse}\NormalTok{(theta, x, y))}
\NormalTok{    \}}

    \CommentTok{\# Set parameter bounds}
\NormalTok{    lower\_bounds }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.01}\NormalTok{, }\FunctionTok{min}\NormalTok{(x))  }\CommentTok{\# Ensure positive scaling}
\NormalTok{    upper\_bounds }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{max}\NormalTok{(y) }\SpecialCharTok{*} \DecValTok{2}\NormalTok{, }\DecValTok{10}\NormalTok{, }\FunctionTok{max}\NormalTok{(x))}

    \CommentTok{\# Run Differential Evolution (mimicking ES)}
\NormalTok{    es\_result }\OtherTok{\textless{}{-}} \FunctionTok{DEoptim}\NormalTok{(}
        \AttributeTok{fn =}\NormalTok{ fitness\_function,}
        \AttributeTok{lower =}\NormalTok{ lower\_bounds,}
        \AttributeTok{upper =}\NormalTok{ upper\_bounds,}
        \CommentTok{\# 50 individuals, 200 generations, suppress iteration output}
        \FunctionTok{DEoptim.control}\NormalTok{(}\AttributeTok{NP =} \DecValTok{50}\NormalTok{, }\AttributeTok{itermax =} \DecValTok{200}\NormalTok{, }\AttributeTok{trace =}\NormalTok{ F)  }
\NormalTok{    )}

    \FunctionTok{return}\NormalTok{(es\_result}\SpecialCharTok{$}\NormalTok{optim}\SpecialCharTok{$}\NormalTok{bestmem)  }\CommentTok{\# Return optimized parameters}
\NormalTok{\}}

\CommentTok{\# Generate synthetic data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{100}\NormalTok{)}
\NormalTok{true\_theta }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\FloatTok{1.5}\NormalTok{, }\DecValTok{0}\NormalTok{)  }\CommentTok{\# True parameters (A, B, C)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{nonlinear\_model}\NormalTok{(true\_theta, x) }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{length}\NormalTok{(x), }\AttributeTok{sd =} \FloatTok{0.3}\NormalTok{)}

\CommentTok{\# Run Evolutionary Strategies Optimization}
\NormalTok{estimated\_theta }\OtherTok{\textless{}{-}} \FunctionTok{es\_optimization}\NormalTok{(x, y)}

\CommentTok{\# Display results}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Estimated parameters (A, B, C) using Evolutionary Strategies:}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Estimated parameters (A, B, C) using Evolutionary Strategies:}
\FunctionTok{print}\NormalTok{(estimated\_theta)}
\CommentTok{\#\textgreater{}       par1       par2       par3 }
\CommentTok{\#\textgreater{} 4.06876561 1.42765613 0.01128767}

\CommentTok{\# Plot data and fitted curve}
\FunctionTok{plot}\NormalTok{(}
\NormalTok{    x, y,}
    \AttributeTok{main =} \StringTok{"Evolutionary Strategies: Nonlinear Regression Optimization"}\NormalTok{,}
    \AttributeTok{pch =} \DecValTok{19}\NormalTok{,}
    \AttributeTok{cex =} \FloatTok{0.5}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"x"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"y"}
\NormalTok{)}
\FunctionTok{curve}\NormalTok{(}
    \FunctionTok{nonlinear\_model}\NormalTok{(estimated\_theta, x),}
    \AttributeTok{from =} \FunctionTok{min}\NormalTok{(x),}
    \AttributeTok{to =} \FunctionTok{max}\NormalTok{(x),}
    \AttributeTok{add =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{col =} \StringTok{"red"}\NormalTok{,}
    \AttributeTok{lwd =} \DecValTok{2}
\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}
    \StringTok{"topleft"}\NormalTok{,}
    \AttributeTok{legend =} \FunctionTok{c}\NormalTok{(}\StringTok{"Data"}\NormalTok{, }\StringTok{"Fitted Curve"}\NormalTok{),}
    \AttributeTok{pch =} \FunctionTok{c}\NormalTok{(}\DecValTok{19}\NormalTok{, }\ConstantTok{NA}\NormalTok{),}
    \AttributeTok{lty =} \FunctionTok{c}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\DecValTok{1}\NormalTok{),}
    \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"red"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{06-nonlinear-regession_files/figure-latex/unnamed-chunk-20-1} \end{center}

\hypertarget{linearization-nonlinear-regression-optimization}{%
\subsection{Linearization}\label{linearization-nonlinear-regression-optimization}}

\hypertarget{taylor-series-approximation-nonlinear-optimization}{%
\subsubsection{Taylor Series Approximation}\label{taylor-series-approximation-nonlinear-optimization}}

Taylor Series Approximation is a fundamental tool in nonlinear optimization, enabling local approximation of complex functions using polynomial expansions. It is widely used to linearize nonlinear models, facilitate derivative-based optimization, and derive Newton-type methods.

Taylor series approximation is particularly useful when:

\begin{itemize}
\item
  A nonlinear function is difficult to compute directly.
\item
  Optimization requires local gradient and curvature information.
\item
  A simpler, polynomial-based approximation improves computational efficiency.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Given a differentiable function \(f(\theta)\), its \textbf{Taylor series expansion} around a point \(\theta_0\) is:

\[
f(\theta) = f(\theta_0) + f'(\theta_0)(\theta - \theta_0) + \frac{1}{2} f''(\theta_0)(\theta - \theta_0)^2 + \mathcal{O}((\theta - \theta_0)^3).
\]

For optimization, we often use:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{First-order approximation (Linear Approximation):} \[
     f(\theta) \approx f(\theta_0) + f'(\theta_0)(\theta - \theta_0).
     \]
\item
  \textbf{Second-order approximation (Quadratic Approximation):} \[
  f(\theta) \approx f(\theta_0) + f'(\theta_0)(\theta - \theta_0) + \frac{1}{2} f''(\theta_0)(\theta - \theta_0)^2.
  \]
\end{enumerate}

For \textbf{gradient-based optimization}, we use the \textbf{Newton-Raphson update}:

\[
\theta^{(k+1)} = \theta^{(k)} - [H_f(\theta^{(k)})]^{-1} \nabla f(\theta^{(k)}),
\]

where:

\begin{itemize}
\item
  \(\nabla f(\theta)\) is the \textbf{gradient} (first derivative),
\item
  \(H_f(\theta)\) is the \textbf{Hessian matrix} (second derivative).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

For nonlinear regression, we approximate the \textbf{Sum of Squared Errors (SSE)}:

\[
SSE(\theta) = \sum_{i=1}^{n} (y_i - f(x_i; \theta))^2.
\]

\textbf{1. First-Order Approximation (Gradient Descent)}

The \textbf{gradient} of SSE w.r.t. parameters \(\theta\) is:

\[
\nabla SSE(\theta) = -2 \sum_{i=1}^{n} (y_i - f(x_i; \theta)) \nabla f(x_i; \theta).
\]

Using \textbf{first-order Taylor approximation}, we update parameters via \textbf{gradient descent}:

\[
\theta^{(k+1)} = \theta^{(k)} - \alpha \nabla SSE(\theta^{(k)}),
\]

where \(\alpha\) is the \textbf{learning rate}.

\textbf{2. Second-Order Approximation (Newton's Method)}

The \textbf{Hessian matrix} of SSE is:

\[
H_{SSE}(\theta) = 2 \sum_{i=1}^{n} \nabla f(x_i; \theta) \nabla f(x_i; \theta)^T - 2 \sum_{i=1}^{n} (y_i - f(x_i; \theta)) H_f(x_i; \theta).
\]

The \textbf{Newton-Raphson update} becomes:

\[
\theta^{(k+1)} = \theta^{(k)} - H_{SSE}(\theta)^{-1} \nabla SSE(\theta).
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load required libraries}
\FunctionTok{library}\NormalTok{(numDeriv)}

\CommentTok{\# Define a numerically stable logistic function}
\NormalTok{safe\_exp }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{}
    \FunctionTok{return}\NormalTok{(}\FunctionTok{ifelse}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(x) }\SpecialCharTok{|}
\NormalTok{                      x }\SpecialCharTok{\textgreater{}} \DecValTok{700}\NormalTok{, }\ConstantTok{Inf}\NormalTok{, }\FunctionTok{exp}\NormalTok{(}\FunctionTok{pmin}\NormalTok{(x, }\DecValTok{700}\NormalTok{))))  }\CommentTok{\# Prevent overflow}
\NormalTok{\}}

\CommentTok{\# Define the logistic growth model}
\NormalTok{nonlinear\_model }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x) \{}
    \FunctionTok{return}\NormalTok{(theta[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{safe\_exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{theta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ (x }\SpecialCharTok{{-}}\NormalTok{ theta[}\DecValTok{3}\NormalTok{]))))}
\NormalTok{\}}

\CommentTok{\# Define the Sum of Squared Errors (SSE) function}
\NormalTok{sse }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x, y) \{}
\NormalTok{    predictions }\OtherTok{\textless{}{-}} \FunctionTok{nonlinear\_model}\NormalTok{(theta, x)}
    \FunctionTok{return}\NormalTok{(}\FunctionTok{sum}\NormalTok{((y }\SpecialCharTok{{-}}\NormalTok{ predictions) }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{))  }\CommentTok{\# Avoid NA errors}
\NormalTok{\}}

\CommentTok{\# First{-}Order Approximation: Gradient Descent Optimization}
\NormalTok{gradient\_descent }\OtherTok{\textless{}{-}}
    \ControlFlowTok{function}\NormalTok{(x,}
\NormalTok{             y,}
             \AttributeTok{alpha =} \FloatTok{0.005}\NormalTok{,}
             \AttributeTok{tol =} \FloatTok{1e{-}6}\NormalTok{,}
             \AttributeTok{max\_iter =} \DecValTok{5000}\NormalTok{) \{}
\NormalTok{        theta }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)  }\CommentTok{\# Initial guess}
        \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{max\_iter) \{}
\NormalTok{            grad\_sse }\OtherTok{\textless{}{-}}
                \FunctionTok{grad}\NormalTok{(}\ControlFlowTok{function}\NormalTok{(t)}
                    \FunctionTok{sse}\NormalTok{(t, x, y), theta)  }\CommentTok{\# Compute gradient}
\NormalTok{            theta\_new }\OtherTok{\textless{}{-}}
\NormalTok{                theta }\SpecialCharTok{{-}}\NormalTok{ alpha }\SpecialCharTok{*}\NormalTok{ grad\_sse  }\CommentTok{\# Update parameters}
            
            \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{sum}\NormalTok{(}\FunctionTok{abs}\NormalTok{(theta\_new }\SpecialCharTok{{-}}\NormalTok{ theta)) }\SpecialCharTok{\textless{}}\NormalTok{ tol)}
                \ControlFlowTok{break}  \CommentTok{\# Check convergence}
\NormalTok{            theta }\OtherTok{\textless{}{-}}\NormalTok{ theta\_new}
\NormalTok{        \}}
        \FunctionTok{return}\NormalTok{(theta)}
\NormalTok{    \}}

\CommentTok{\# Second{-}Order Approximation: Newton\textquotesingle{}s Method with Regularization}
\NormalTok{newton\_method }\OtherTok{\textless{}{-}}
    \ControlFlowTok{function}\NormalTok{(x,}
\NormalTok{             y,}
             \AttributeTok{tol =} \FloatTok{1e{-}6}\NormalTok{,}
             \AttributeTok{max\_iter =} \DecValTok{100}\NormalTok{,}
             \AttributeTok{lambda =} \FloatTok{1e{-}4}\NormalTok{) \{}
\NormalTok{        theta }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)  }\CommentTok{\# Initial guess}
        \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{max\_iter) \{}
\NormalTok{            grad\_sse }\OtherTok{\textless{}{-}}
                \FunctionTok{grad}\NormalTok{(}\ControlFlowTok{function}\NormalTok{(t)}
                    \FunctionTok{sse}\NormalTok{(t, x, y), theta)  }\CommentTok{\# Compute gradient}
\NormalTok{            hessian\_sse }\OtherTok{\textless{}{-}}
                \FunctionTok{hessian}\NormalTok{(}\ControlFlowTok{function}\NormalTok{(t)}
                    \FunctionTok{sse}\NormalTok{(t, x, y), theta)  }\CommentTok{\# Compute Hessian}
            
            \CommentTok{\# Regularize Hessian to avoid singularity}
\NormalTok{            hessian\_reg }\OtherTok{\textless{}{-}}
\NormalTok{                hessian\_sse }\SpecialCharTok{+}\NormalTok{ lambda }\SpecialCharTok{*} \FunctionTok{diag}\NormalTok{(}\FunctionTok{length}\NormalTok{(theta))}
            
            \CommentTok{\# Ensure Hessian is invertible}
            \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{is.na}\NormalTok{(}\FunctionTok{det}\NormalTok{(hessian\_reg)) }\SpecialCharTok{||}
                \FunctionTok{det}\NormalTok{(hessian\_reg) }\SpecialCharTok{\textless{}} \FloatTok{1e{-}10}\NormalTok{) \{}
                \FunctionTok{message}\NormalTok{(}\StringTok{"Singular Hessian found; increasing regularization."}\NormalTok{)}
\NormalTok{                lambda }\OtherTok{\textless{}{-}}\NormalTok{ lambda }\SpecialCharTok{*} \DecValTok{10}  \CommentTok{\# Increase regularization}
                \ControlFlowTok{next}
\NormalTok{            \}}
            
            \CommentTok{\# Newton update}
\NormalTok{            theta\_new }\OtherTok{\textless{}{-}}\NormalTok{ theta }\SpecialCharTok{{-}} \FunctionTok{solve}\NormalTok{(hessian\_reg) }\SpecialCharTok{\%*\%}\NormalTok{ grad\_sse}
            
            \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{sum}\NormalTok{(}\FunctionTok{abs}\NormalTok{(theta\_new }\SpecialCharTok{{-}}\NormalTok{ theta)) }\SpecialCharTok{\textless{}}\NormalTok{ tol)}
                \ControlFlowTok{break}  \CommentTok{\# Check convergence}
\NormalTok{            theta }\OtherTok{\textless{}{-}}\NormalTok{ theta\_new}
\NormalTok{        \}}
        \FunctionTok{return}\NormalTok{(theta)}
\NormalTok{    \}}

\CommentTok{\# Generate synthetic data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{100}\NormalTok{)}
\NormalTok{true\_theta }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\FloatTok{1.5}\NormalTok{, }\DecValTok{0}\NormalTok{)  }\CommentTok{\# True parameters (A, B, C)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{nonlinear\_model}\NormalTok{(true\_theta, x) }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{length}\NormalTok{(x), }\AttributeTok{sd =} \FloatTok{0.3}\NormalTok{)}

\CommentTok{\# Run Gradient Descent}
\NormalTok{estimated\_theta\_gd }\OtherTok{\textless{}{-}} \FunctionTok{gradient\_descent}\NormalTok{(x, y)}

\CommentTok{\# Run Newton\textquotesingle{}s Method with Regularization}
\NormalTok{estimated\_theta\_newton }\OtherTok{\textless{}{-}} \FunctionTok{newton\_method}\NormalTok{(x, y)}

\CommentTok{\# Display results}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Estimated parameters (A, B, C) using Gradient Descent:}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Estimated parameters (A, B, C) using Gradient Descent:}
\FunctionTok{print}\NormalTok{(estimated\_theta\_gd)}
\CommentTok{\#\textgreater{} [1] 4.06876224 1.42766371 0.01128539}

\FunctionTok{cat}\NormalTok{(}\StringTok{"Estimated parameters (A, B, C) using Newton\textquotesingle{}s Method:}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Estimated parameters (A, B, C) using Newton\textquotesingle{}s Method:}
\FunctionTok{print}\NormalTok{(estimated\_theta\_newton)}
\CommentTok{\#\textgreater{}            [,1]}
\CommentTok{\#\textgreater{} [1,] 4.06876368}
\CommentTok{\#\textgreater{} [2,] 1.42766047}
\CommentTok{\#\textgreater{} [3,] 0.01128636}

\CommentTok{\# Plot data and fitted curve}
\FunctionTok{plot}\NormalTok{(}
\NormalTok{    x,}
\NormalTok{    y,}
    \AttributeTok{main =} \StringTok{"Taylor Series Approximation: Nonlinear Regression Optimization"}\NormalTok{,}
    \AttributeTok{pch =} \DecValTok{19}\NormalTok{,}
    \AttributeTok{cex =} \FloatTok{0.5}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"x"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"y"}
\NormalTok{)}
\FunctionTok{curve}\NormalTok{(}
    \FunctionTok{nonlinear\_model}\NormalTok{(estimated\_theta\_gd, x),}
    \AttributeTok{from =} \FunctionTok{min}\NormalTok{(x),}
    \AttributeTok{to =} \FunctionTok{max}\NormalTok{(x),}
    \AttributeTok{add =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{col =} \StringTok{"blue"}\NormalTok{,}
    \AttributeTok{lwd =} \DecValTok{2}\NormalTok{,}
    \AttributeTok{lty =} \DecValTok{2}  \CommentTok{\# Dashed line to differentiate Gradient Descent}
\NormalTok{)}
\FunctionTok{curve}\NormalTok{(}
    \FunctionTok{nonlinear\_model}\NormalTok{(estimated\_theta\_newton, x),}
    \AttributeTok{from =} \FunctionTok{min}\NormalTok{(x),}
    \AttributeTok{to =} \FunctionTok{max}\NormalTok{(x),}
    \AttributeTok{add =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{col =} \StringTok{"red"}\NormalTok{,}
    \AttributeTok{lwd =} \DecValTok{2}
\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}
    \StringTok{"topleft"}\NormalTok{,}
    \AttributeTok{legend =} \FunctionTok{c}\NormalTok{(}\StringTok{"Data"}\NormalTok{, }\StringTok{"Gradient Descent"}\NormalTok{, }\StringTok{"Newton\textquotesingle{}s Method (Regularized)"}\NormalTok{),}
    \AttributeTok{pch =} \FunctionTok{c}\NormalTok{(}\DecValTok{19}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\ConstantTok{NA}\NormalTok{),}
    \AttributeTok{lty =} \FunctionTok{c}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{),}
    \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"blue"}\NormalTok{, }\StringTok{"red"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{06-nonlinear-regession_files/figure-latex/unnamed-chunk-21-1} \end{center}

\hypertarget{log-linearization-nonlinear-optimization}{%
\subsubsection{Log-Linearization}\label{log-linearization-nonlinear-optimization}}

Log-Linearization is a mathematical technique used to transform nonlinear models into linear models by taking the logarithm of both sides. This transformation simplifies parameter estimation and enables the use of linear regression techniques on originally nonlinear functions.

Log-linearization is particularly useful when:

\begin{itemize}
\item
  The model exhibits exponential, power-law, or logistic growth behavior.
\item
  Linear regression methods are preferred over nonlinear optimization.
\item
  A linearized version provides better interpretability and computational efficiency.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

A \textbf{nonlinear model} can often be expressed in the form:

\[
y = f(x; \theta).
\]

Applying a \textbf{log transformation}, we obtain:

\[
\log y = g(x; \theta),
\]

where \(g(x; \theta)\) is now \textbf{linear in parameters}. We then estimate \(\theta\) using \protect\hyperlink{ordinary-least-squares}{Ordinary Least Squares}.

\textbf{Example 1: Exponential Model}

Consider an \textbf{exponential growth model}:

\[
y = A e^{Bx}.
\]

Taking the natural logarithm:

\[
\log y = \log A + Bx.
\]

This is now \textbf{linear in} \(\log y\), allowing estimation via \textbf{linear regression}.

\textbf{Example 2: Power Law Model}

For a \textbf{power law function}:

\[
y = A x^B.
\]

Taking logs:

\[
\log y = \log A + B \log x.
\]

Again, this is \textbf{linearized}, making it solvable via \textbf{OLS regression}.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Log-Linearization Algorithm}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Apply the logarithm transformation to the dependent variable.
\item
  Transform the equation into a linear form.
\item
  Use linear regression (OLS) to estimate parameters.
\item
  Convert parameters back to original scale if necessary.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load required library}
\FunctionTok{library}\NormalTok{(stats)}

\CommentTok{\# Generate synthetic data for an exponential model}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{100}\NormalTok{)}
\NormalTok{true\_A }\OtherTok{\textless{}{-}} \DecValTok{2}
\NormalTok{true\_B }\OtherTok{\textless{}{-}} \FloatTok{0.3}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ true\_A }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(true\_B }\SpecialCharTok{*}\NormalTok{ x) }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{length}\NormalTok{(x), }\AttributeTok{sd =} \FloatTok{0.5}\NormalTok{)}

\CommentTok{\# Apply logarithmic transformation}
\NormalTok{log\_y }\OtherTok{\textless{}{-}} \FunctionTok{log}\NormalTok{(y)}

\CommentTok{\# Fit linear regression model}
\NormalTok{log\_linear\_model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(log\_y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x)}

\CommentTok{\# Extract estimated parameters}
\NormalTok{estimated\_B }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(log\_linear\_model)[}\DecValTok{2}\NormalTok{]  }\CommentTok{\# Slope in log{-}space}
\NormalTok{estimated\_A }\OtherTok{\textless{}{-}}
    \FunctionTok{exp}\NormalTok{(}\FunctionTok{coef}\NormalTok{(log\_linear\_model)[}\DecValTok{1}\NormalTok{])  }\CommentTok{\# Intercept (back{-}transformed)}

\CommentTok{\# Display results}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Estimated parameters (A, B) using Log{-}Linearization:}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Estimated parameters (A, B) using Log{-}Linearization:}
\FunctionTok{print}\NormalTok{(}\FunctionTok{c}\NormalTok{(estimated\_A, estimated\_B))}
\CommentTok{\#\textgreater{} (Intercept)           x }
\CommentTok{\#\textgreater{}   2.0012577   0.3001223}

\CommentTok{\# Plot data and fitted curve}
\FunctionTok{plot}\NormalTok{(}
\NormalTok{    x,}
\NormalTok{    y,}
    \AttributeTok{main =} \StringTok{"Log{-}Linearization: Nonlinear Regression Optimization"}\NormalTok{,}
    \AttributeTok{pch =} \DecValTok{19}\NormalTok{,}
    \AttributeTok{cex =} \FloatTok{0.5}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"x"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"y"}
\NormalTok{)}
\FunctionTok{curve}\NormalTok{(}
\NormalTok{    estimated\_A }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(estimated\_B }\SpecialCharTok{*}\NormalTok{ x),}
    \AttributeTok{from =} \FunctionTok{min}\NormalTok{(x),}
    \AttributeTok{to =} \FunctionTok{max}\NormalTok{(x),}
    \AttributeTok{add =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{col =} \StringTok{"red"}\NormalTok{,}
    \AttributeTok{lwd =} \DecValTok{2}
\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}
    \StringTok{"topleft"}\NormalTok{,}
    \AttributeTok{legend =} \FunctionTok{c}\NormalTok{(}\StringTok{"Data"}\NormalTok{, }\StringTok{"Fitted Log{-}Linear Model"}\NormalTok{),}
    \AttributeTok{pch =} \FunctionTok{c}\NormalTok{(}\DecValTok{19}\NormalTok{, }\ConstantTok{NA}\NormalTok{),}
    \AttributeTok{lty =} \FunctionTok{c}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\DecValTok{1}\NormalTok{),}
    \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"red"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{06-nonlinear-regession_files/figure-latex/unnamed-chunk-22-1} \end{center}

\hypertarget{hybrid-nonlinear-regression-optimization}{%
\subsection{Hybrid}\label{hybrid-nonlinear-regression-optimization}}

\hypertarget{adaptive-levenberg-marquardt}{%
\subsubsection{Adaptive Levenberg-Marquardt}\label{adaptive-levenberg-marquardt}}

The \textbf{Levenberg-Marquardt Algorithm (LMA)} is a powerful \textbf{nonlinear least squares optimization method} that adaptively combines:

\begin{itemize}
\item
  \protect\hyperlink{gauss-newton-algorithm}{Gauss-Newton Algorithm} for fast convergence near the solution.
\item
  \protect\hyperlink{steepest-descent}{Steepest Descent (Gradient Descent)} for stability when far from the solution.
\end{itemize}

The Adaptive Levenberg-Marquardt Algorithm further adjusts the damping parameter \(\tau\) dynamically, making it more efficient in practice.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Given an objective function \textbf{Sum of Squared Errors (SSE)}:

\[
SSE(\theta) = \sum_{i=1}^{n} (y_i - f(x_i; \theta))^2.
\]

The \textbf{update rule for LMA} is:

\[
\hat{\theta}^{(j+1)} = \hat{\theta}^{(j)} - \alpha_j [\mathbf{F}(\hat{\theta}^{(j)})' \mathbf{F}(\hat{\theta}^{(j)}) + \tau \mathbf{I}_{p \times p}]\frac{\partial \mathbf{Q}(\hat{\theta}^{(j)})}{\partial \theta}.
\]

where:

\begin{itemize}
\item
  \(\tau\) is the \textbf{adaptive damping parameter}.
\item
  \(\mathbf{I}_{p \times p}\) is the \textbf{identity matrix}.
\item
  \(\mathbf{F}(\hat{\theta}^{(j)})\) is the \textbf{Jacobian matrix} of partial derivatives.
\item
  \(\frac{\partial \mathbf{Q}(\hat{\theta}^{(j)})}{\partial \theta}\) is the \textbf{gradient vector}.
\item
  \(\alpha_j\) is the \textbf{learning rate}.
\end{itemize}

The key \textbf{adaptation rule for} \(\tau\):

\begin{itemize}
\item
  \textbf{If the new step decreases SSE}, \textbf{reduce} \(\tau\): \[
    \tau \gets \tau / 10.
    \]
\item
  \textbf{Otherwise, increase} \(\tau\) to ensure stability: \[
    \tau \gets 10\tau.
    \]
\end{itemize}

This adjustment \textbf{ensures a balance between stability and efficiency}.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Adaptive Levenberg-Marquardt Algorithm}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Initialize parameters \(\theta_0\), damping factor \(\tau\).
\item
  Compute Jacobian \(\mathbf{F}(\hat{\theta}^{(j)})\).
\item
  Compute step direction using modified Gauss-Newton update.
\item
  Adjust \(\tau\) dynamically:

  \begin{itemize}
  \tightlist
  \item
    Decrease \(\tau\) if SSE improves.
  \item
    Increase \(\tau\) if SSE worsens.
  \end{itemize}
\item
  Repeat until convergence.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load required libraries}
\FunctionTok{library}\NormalTok{(numDeriv)}

\CommentTok{\# Define a numerically stable logistic function}
\NormalTok{safe\_exp }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{}
    \FunctionTok{return}\NormalTok{(}\FunctionTok{ifelse}\NormalTok{(x }\SpecialCharTok{\textgreater{}} \DecValTok{700}\NormalTok{, }\ConstantTok{Inf}\NormalTok{, }\FunctionTok{exp}\NormalTok{(}\FunctionTok{pmin}\NormalTok{(x, }\DecValTok{700}\NormalTok{))))  }\CommentTok{\# Prevent overflow}
\NormalTok{\}}

\CommentTok{\# Define the logistic growth model}
\NormalTok{nonlinear\_model }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x) \{}
    \FunctionTok{return}\NormalTok{(theta[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{safe\_exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{theta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ (x }\SpecialCharTok{{-}}\NormalTok{ theta[}\DecValTok{3}\NormalTok{]))))}
\NormalTok{\}}

\CommentTok{\# Define the Sum of Squared Errors (SSE) function}
\NormalTok{sse }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x, y) \{}
\NormalTok{    predictions }\OtherTok{\textless{}{-}} \FunctionTok{nonlinear\_model}\NormalTok{(theta, x)}
    \FunctionTok{return}\NormalTok{(}\FunctionTok{sum}\NormalTok{((y }\SpecialCharTok{{-}}\NormalTok{ predictions) }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{))}
\NormalTok{\}}

\CommentTok{\# Adaptive Levenberg{-}Marquardt Optimization}
\NormalTok{adaptive\_lm\_optimization }\OtherTok{\textless{}{-}}
    \ControlFlowTok{function}\NormalTok{(x, y, }\AttributeTok{tol =} \FloatTok{1e{-}6}\NormalTok{, }\AttributeTok{max\_iter =} \DecValTok{100}\NormalTok{) \{}
\NormalTok{        theta }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)  }\CommentTok{\# Initial parameter guess}
\NormalTok{        tau }\OtherTok{\textless{}{-}} \FloatTok{1e{-}3}  \CommentTok{\# Initial damping parameter}
\NormalTok{        alpha }\OtherTok{\textless{}{-}} \DecValTok{1}  \CommentTok{\# Step size scaling}
\NormalTok{        iter }\OtherTok{\textless{}{-}} \DecValTok{0}
        
        \ControlFlowTok{while}\NormalTok{ (iter }\SpecialCharTok{\textless{}}\NormalTok{ max\_iter) \{}
\NormalTok{            iter }\OtherTok{\textless{}{-}}\NormalTok{ iter }\SpecialCharTok{+} \DecValTok{1}
            
            \CommentTok{\# Compute Jacobian numerically}
\NormalTok{            J }\OtherTok{\textless{}{-}} \FunctionTok{jacobian}\NormalTok{(}\ControlFlowTok{function}\NormalTok{(t)}
                \FunctionTok{nonlinear\_model}\NormalTok{(t, x), theta)}
            
            \CommentTok{\# Compute gradient of SSE}
\NormalTok{            residuals }\OtherTok{\textless{}{-}}\NormalTok{ y }\SpecialCharTok{{-}} \FunctionTok{nonlinear\_model}\NormalTok{(theta, x)}
\NormalTok{            grad\_sse }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\DecValTok{2} \SpecialCharTok{*} \FunctionTok{t}\NormalTok{(J) }\SpecialCharTok{\%*\%}\NormalTok{ residuals}
            
            \CommentTok{\# Compute Hessian approximation}
\NormalTok{            H }\OtherTok{\textless{}{-}} \DecValTok{2} \SpecialCharTok{*} \FunctionTok{t}\NormalTok{(J) }\SpecialCharTok{\%*\%}\NormalTok{ J }\SpecialCharTok{+}\NormalTok{ tau }\SpecialCharTok{*} \FunctionTok{diag}\NormalTok{(}\FunctionTok{length}\NormalTok{(theta))}
            
            \CommentTok{\# Compute parameter update step}
\NormalTok{            delta\_theta }\OtherTok{\textless{}{-}} \FunctionTok{solve}\NormalTok{(H, grad\_sse)}
            
            \CommentTok{\# Trial step}
\NormalTok{            theta\_new }\OtherTok{\textless{}{-}}\NormalTok{ theta }\SpecialCharTok{{-}}\NormalTok{ alpha }\SpecialCharTok{*}\NormalTok{ delta\_theta}
            
            \CommentTok{\# Compute SSE for new parameters}
            \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{sse}\NormalTok{(theta\_new, x, y) }\SpecialCharTok{\textless{}} \FunctionTok{sse}\NormalTok{(theta, x, y)) \{}
                \CommentTok{\# Accept step, decrease tau}
\NormalTok{                theta }\OtherTok{\textless{}{-}}\NormalTok{ theta\_new}
\NormalTok{                tau }\OtherTok{\textless{}{-}}\NormalTok{ tau }\SpecialCharTok{/} \DecValTok{10}
\NormalTok{            \} }\ControlFlowTok{else}\NormalTok{ \{}
                \CommentTok{\# Reject step, increase tau}
\NormalTok{                tau }\OtherTok{\textless{}{-}}\NormalTok{ tau }\SpecialCharTok{*} \DecValTok{10}
\NormalTok{            \}}
            
            \CommentTok{\# Check convergence}
            \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{sum}\NormalTok{(}\FunctionTok{abs}\NormalTok{(delta\_theta)) }\SpecialCharTok{\textless{}}\NormalTok{ tol)}
                \ControlFlowTok{break}
\NormalTok{        \}}
        
        \FunctionTok{return}\NormalTok{(theta)}
\NormalTok{    \}}

\CommentTok{\# Generate synthetic data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{100}\NormalTok{)}
\NormalTok{true\_theta }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\FloatTok{1.5}\NormalTok{, }\DecValTok{0}\NormalTok{)  }\CommentTok{\# True parameters (A, B, C)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{nonlinear\_model}\NormalTok{(true\_theta, x) }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{length}\NormalTok{(x), }\AttributeTok{sd =} \FloatTok{0.3}\NormalTok{)}

\CommentTok{\# Run Adaptive Levenberg{-}Marquardt Optimization}
\NormalTok{estimated\_theta }\OtherTok{\textless{}{-}} \FunctionTok{adaptive\_lm\_optimization}\NormalTok{(x, y)}

\CommentTok{\# Display results}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Estimated parameters (A, B, C) using Adaptive Levenberg{-}Marquardt:}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Estimated parameters (A, B, C) using Adaptive Levenberg{-}Marquardt:}
\FunctionTok{print}\NormalTok{(estimated\_theta)}
\CommentTok{\#\textgreater{}            [,1]}
\CommentTok{\#\textgreater{} [1,] 4.06876562}
\CommentTok{\#\textgreater{} [2,] 1.42765612}
\CommentTok{\#\textgreater{} [3,] 0.01128767}

\CommentTok{\# Plot data and fitted curve}
\FunctionTok{plot}\NormalTok{(}
\NormalTok{    x,}
\NormalTok{    y,}
    \AttributeTok{main =} \StringTok{"Adaptive Levenberg{-}Marquardt: Nonlinear Regression Optimization"}\NormalTok{,}
    \AttributeTok{pch =} \DecValTok{19}\NormalTok{,}
    \AttributeTok{cex =} \FloatTok{0.5}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"x"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"y"}
\NormalTok{)}
\FunctionTok{curve}\NormalTok{(}
    \FunctionTok{nonlinear\_model}\NormalTok{(estimated\_theta, x),}
    \AttributeTok{from =} \FunctionTok{min}\NormalTok{(x),}
    \AttributeTok{to =} \FunctionTok{max}\NormalTok{(x),}
    \AttributeTok{add =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{col =} \StringTok{"red"}\NormalTok{,}
    \AttributeTok{lwd =} \DecValTok{2}
\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}
    \StringTok{"topleft"}\NormalTok{,}
    \AttributeTok{legend =} \FunctionTok{c}\NormalTok{(}\StringTok{"Data"}\NormalTok{, }\StringTok{"Fitted Curve"}\NormalTok{),}
    \AttributeTok{pch =} \FunctionTok{c}\NormalTok{(}\DecValTok{19}\NormalTok{, }\ConstantTok{NA}\NormalTok{),}
    \AttributeTok{lty =} \FunctionTok{c}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\DecValTok{1}\NormalTok{),}
    \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"red"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{06-nonlinear-regession_files/figure-latex/unnamed-chunk-23-1} \end{center}

\hypertarget{comparison-of-nonlinear-optimizers}{%
\subsection{Comparison of Nonlinear Optimizers}\label{comparison-of-nonlinear-optimizers}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# ALL{-}IN{-}ONE R SCRIPT COMPARING MULTIPLE NONLINEAR{-}REGRESSION OPTIMIZERS}

\FunctionTok{library}\NormalTok{(minpack.lm) }\CommentTok{\# nlsLM (Levenberg{-}Marquardt)}
\FunctionTok{library}\NormalTok{(dfoptim)    }\CommentTok{\# Powell (nmk), Hooke{-}Jeeves}
\FunctionTok{library}\NormalTok{(nloptr)     }\CommentTok{\# trust{-}region reflective}
\FunctionTok{library}\NormalTok{(GA)         }\CommentTok{\# genetic algorithm}
\FunctionTok{library}\NormalTok{(DEoptim)    }\CommentTok{\# differential evolution}
\FunctionTok{library}\NormalTok{(GenSA)      }\CommentTok{\# simulated annealing}
\FunctionTok{library}\NormalTok{(pso)        }\CommentTok{\# particle swarm}
\FunctionTok{library}\NormalTok{(MASS)       }\CommentTok{\# for ginv fallback}
\FunctionTok{library}\NormalTok{(microbenchmark)}
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(dplyr)}

\CommentTok{\# {-}{-} 1) DEFINE MODELS (SIMPLE VS COMPLEX) {-}{-}{-}}

\CommentTok{\# 3{-}parameter logistic}
\NormalTok{f\_logistic }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x) \{}
\NormalTok{  A }\OtherTok{\textless{}{-}}\NormalTok{ theta[}\DecValTok{1}\NormalTok{]}
\NormalTok{  B }\OtherTok{\textless{}{-}}\NormalTok{ theta[}\DecValTok{2}\NormalTok{]}
\NormalTok{  C }\OtherTok{\textless{}{-}}\NormalTok{ theta[}\DecValTok{3}\NormalTok{]}
\NormalTok{  A }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{B }\SpecialCharTok{*}\NormalTok{ (x }\SpecialCharTok{{-}}\NormalTok{ C)))}
\NormalTok{\}}
\NormalTok{sse\_logistic }\OtherTok{\textless{}{-}}
  \ControlFlowTok{function}\NormalTok{(theta, x, y)}
    \FunctionTok{sum}\NormalTok{((y }\SpecialCharTok{{-}} \FunctionTok{f\_logistic}\NormalTok{(theta, x)) }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{)}

\CommentTok{\# 4{-}parameter "extended" model}
\NormalTok{f\_complex }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, x) \{}
\NormalTok{  A }\OtherTok{\textless{}{-}}\NormalTok{ theta[}\DecValTok{1}\NormalTok{]}
\NormalTok{  B }\OtherTok{\textless{}{-}}\NormalTok{ theta[}\DecValTok{2}\NormalTok{]}
\NormalTok{  C }\OtherTok{\textless{}{-}}\NormalTok{ theta[}\DecValTok{3}\NormalTok{]}
\NormalTok{  D }\OtherTok{\textless{}{-}}\NormalTok{ theta[}\DecValTok{4}\NormalTok{]}
\NormalTok{  A }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{B }\SpecialCharTok{*}\NormalTok{ (x }\SpecialCharTok{{-}}\NormalTok{ C))) }\SpecialCharTok{+}\NormalTok{ D }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.5} \SpecialCharTok{*}\NormalTok{ x)}
\NormalTok{\}}
\NormalTok{sse\_complex }\OtherTok{\textless{}{-}}
  \ControlFlowTok{function}\NormalTok{(theta, x, y)}
    \FunctionTok{sum}\NormalTok{((y }\SpecialCharTok{{-}} \FunctionTok{f\_complex}\NormalTok{(theta, x)) }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{)}

\CommentTok{\# Generate synthetic data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{100}
\NormalTok{x\_data }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\AttributeTok{length.out =}\NormalTok{ n)}
\CommentTok{\# "simple" scenario}
\NormalTok{true\_theta\_simple }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\FloatTok{1.5}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{y\_data\_simple }\OtherTok{\textless{}{-}}
  \FunctionTok{f\_logistic}\NormalTok{(true\_theta\_simple, x\_data) }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{sd =} \FloatTok{0.3}\NormalTok{)}
\CommentTok{\# "complex" scenario}
\NormalTok{true\_theta\_complex }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\FloatTok{1.2}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{)}
\NormalTok{y\_data\_complex }\OtherTok{\textless{}{-}}
  \FunctionTok{f\_complex}\NormalTok{(true\_theta\_complex, x\_data) }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{sd =} \FloatTok{0.3}\NormalTok{)}

\CommentTok{\# {-}{-} 2) OPTIMIZERS (EXCEPT BISECTION) {-}{-}{-}{-}}
\CommentTok{\#}
\CommentTok{\# All methods share signature:}
\CommentTok{\#   FUN(par, x, y, sse\_fn, model\_fn, lower=NULL, upper=NULL, ...)}
\CommentTok{\# Some do not strictly use lower/upper if unconstrained.}

\CommentTok{\# 2.1 GaussNewton}
\NormalTok{gauss\_newton\_fit }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(par,}
\NormalTok{                             x,}
\NormalTok{                             y,}
\NormalTok{                             sse\_fn,}
\NormalTok{                             model\_fn,}
                             \AttributeTok{lower =} \ConstantTok{NULL}\NormalTok{,}
                             \AttributeTok{upper =} \ConstantTok{NULL}\NormalTok{,}
                             \AttributeTok{max\_iter =} \DecValTok{100}\NormalTok{,}
                             \AttributeTok{tol =} \FloatTok{1e{-}6}\NormalTok{)}
\NormalTok{\{}
\NormalTok{  theta }\OtherTok{\textless{}{-}}\NormalTok{ par}
  \ControlFlowTok{for}\NormalTok{ (iter }\ControlFlowTok{in} \FunctionTok{seq\_len}\NormalTok{(max\_iter)) \{}
\NormalTok{    eps }\OtherTok{\textless{}{-}} \FloatTok{1e{-}6}
\NormalTok{    nP  }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(theta)}
\NormalTok{    Fmat }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{, }\AttributeTok{nrow =} \FunctionTok{length}\NormalTok{(x), }\AttributeTok{ncol =}\NormalTok{ nP)}
    \ControlFlowTok{for}\NormalTok{ (p }\ControlFlowTok{in} \FunctionTok{seq\_len}\NormalTok{(nP)) \{}
\NormalTok{      pert }\OtherTok{\textless{}{-}}\NormalTok{ theta}
\NormalTok{      pert[p] }\OtherTok{\textless{}{-}}\NormalTok{ pert[p] }\SpecialCharTok{+}\NormalTok{ eps}
\NormalTok{      Fmat[, p] }\OtherTok{\textless{}{-}}
\NormalTok{        (}\FunctionTok{model\_fn}\NormalTok{(pert, x) }\SpecialCharTok{{-}} \FunctionTok{model\_fn}\NormalTok{(theta, x)) }\SpecialCharTok{/}\NormalTok{ eps}
\NormalTok{    \}}
\NormalTok{    r }\OtherTok{\textless{}{-}}\NormalTok{ y }\SpecialCharTok{{-}} \FunctionTok{model\_fn}\NormalTok{(theta, x)}
\NormalTok{    delta }\OtherTok{\textless{}{-}} \FunctionTok{tryCatch}\NormalTok{(}
      \FunctionTok{solve}\NormalTok{(}\FunctionTok{t}\NormalTok{(Fmat) }\SpecialCharTok{\%*\%}\NormalTok{ Fmat, }\FunctionTok{t}\NormalTok{(Fmat) }\SpecialCharTok{\%*\%}\NormalTok{ r),}
      \AttributeTok{error =} \ControlFlowTok{function}\NormalTok{(e) \{}
        \CommentTok{\# fallback to pseudoinverse}
\NormalTok{        MASS}\SpecialCharTok{::}\FunctionTok{ginv}\NormalTok{(}\FunctionTok{t}\NormalTok{(Fmat) }\SpecialCharTok{\%*\%}\NormalTok{ Fmat) }\SpecialCharTok{\%*\%}\NormalTok{ (}\FunctionTok{t}\NormalTok{(Fmat) }\SpecialCharTok{\%*\%}\NormalTok{ r)}
\NormalTok{      \}}
\NormalTok{    )}
\NormalTok{    theta\_new }\OtherTok{\textless{}{-}}\NormalTok{ theta }\SpecialCharTok{+}\NormalTok{ delta}
    \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{sum}\NormalTok{(}\FunctionTok{abs}\NormalTok{(theta\_new }\SpecialCharTok{{-}}\NormalTok{ theta)) }\SpecialCharTok{\textless{}}\NormalTok{ tol)}
      \ControlFlowTok{break}
\NormalTok{    theta }\OtherTok{\textless{}{-}}\NormalTok{ theta\_new}
\NormalTok{  \}}
\NormalTok{  theta}
\NormalTok{\}}

\CommentTok{\# 2.2 Modified Gauss{-}Newton (with step halving)}
\NormalTok{modified\_gauss\_newton\_fit }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(par,}
\NormalTok{                                      x,}
\NormalTok{                                      y,}
\NormalTok{                                      sse\_fn,}
\NormalTok{                                      model\_fn,}
                                      \AttributeTok{lower =} \ConstantTok{NULL}\NormalTok{,}
                                      \AttributeTok{upper =} \ConstantTok{NULL}\NormalTok{,}
                                      \AttributeTok{max\_iter =} \DecValTok{100}\NormalTok{,}
                                      \AttributeTok{tol =} \FloatTok{1e{-}6}\NormalTok{)}
\NormalTok{\{}
\NormalTok{  theta }\OtherTok{\textless{}{-}}\NormalTok{ par}
  \ControlFlowTok{for}\NormalTok{ (iter }\ControlFlowTok{in} \FunctionTok{seq\_len}\NormalTok{(max\_iter)) \{}
\NormalTok{    eps }\OtherTok{\textless{}{-}} \FloatTok{1e{-}6}
\NormalTok{    nP  }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(theta)}
\NormalTok{    Fmat }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{, }\AttributeTok{nrow =} \FunctionTok{length}\NormalTok{(x), }\AttributeTok{ncol =}\NormalTok{ nP)}
    \ControlFlowTok{for}\NormalTok{ (p }\ControlFlowTok{in} \FunctionTok{seq\_len}\NormalTok{(nP)) \{}
\NormalTok{      pert }\OtherTok{\textless{}{-}}\NormalTok{ theta}
\NormalTok{      pert[p] }\OtherTok{\textless{}{-}}\NormalTok{ pert[p] }\SpecialCharTok{+}\NormalTok{ eps}
\NormalTok{      Fmat[, p] }\OtherTok{\textless{}{-}}
\NormalTok{        (}\FunctionTok{model\_fn}\NormalTok{(pert, x) }\SpecialCharTok{{-}} \FunctionTok{model\_fn}\NormalTok{(theta, x)) }\SpecialCharTok{/}\NormalTok{ eps}
\NormalTok{    \}}
\NormalTok{    r }\OtherTok{\textless{}{-}}\NormalTok{ y }\SpecialCharTok{{-}} \FunctionTok{model\_fn}\NormalTok{(theta, x)}
\NormalTok{    lhs }\OtherTok{\textless{}{-}} \FunctionTok{t}\NormalTok{(Fmat) }\SpecialCharTok{\%*\%}\NormalTok{ Fmat}
\NormalTok{    rhs }\OtherTok{\textless{}{-}} \FunctionTok{t}\NormalTok{(Fmat) }\SpecialCharTok{\%*\%}\NormalTok{ r}
\NormalTok{    delta }\OtherTok{\textless{}{-}} \FunctionTok{tryCatch}\NormalTok{(}
      \FunctionTok{solve}\NormalTok{(lhs, rhs),}
      \AttributeTok{error =} \ControlFlowTok{function}\NormalTok{(e)}
\NormalTok{        MASS}\SpecialCharTok{::}\FunctionTok{ginv}\NormalTok{(lhs) }\SpecialCharTok{\%*\%}\NormalTok{ rhs}
\NormalTok{    )}
\NormalTok{    sse\_old }\OtherTok{\textless{}{-}} \FunctionTok{sse\_fn}\NormalTok{(theta, x, y)}
\NormalTok{    alpha }\OtherTok{\textless{}{-}} \DecValTok{1}
    \ControlFlowTok{for}\NormalTok{ (k }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{) \{}
\NormalTok{      new\_sse }\OtherTok{\textless{}{-}} \FunctionTok{sse\_fn}\NormalTok{(theta }\SpecialCharTok{+}\NormalTok{ alpha }\SpecialCharTok{*}\NormalTok{ delta, x, y)}
      \ControlFlowTok{if}\NormalTok{ (new\_sse }\SpecialCharTok{\textless{}}\NormalTok{ sse\_old)}
        \ControlFlowTok{break}
\NormalTok{      alpha }\OtherTok{\textless{}{-}}\NormalTok{ alpha }\SpecialCharTok{/} \DecValTok{2}
\NormalTok{    \}}
\NormalTok{    theta\_new }\OtherTok{\textless{}{-}}\NormalTok{ theta }\SpecialCharTok{+}\NormalTok{ alpha }\SpecialCharTok{*}\NormalTok{ delta}
    \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{sum}\NormalTok{(}\FunctionTok{abs}\NormalTok{(theta\_new }\SpecialCharTok{{-}}\NormalTok{ theta)) }\SpecialCharTok{\textless{}}\NormalTok{ tol)}
      \ControlFlowTok{break}
\NormalTok{    theta }\OtherTok{\textless{}{-}}\NormalTok{ theta\_new}
\NormalTok{  \}}
\NormalTok{  theta}
\NormalTok{\}}

\CommentTok{\# 2.3 Steepest Descent (Gradient Descent)}
\NormalTok{steepest\_descent\_fit }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(par,}
\NormalTok{                                 x,}
\NormalTok{                                 y,}
\NormalTok{                                 sse\_fn,}
\NormalTok{                                 model\_fn,}
                                 \AttributeTok{lower =} \ConstantTok{NULL}\NormalTok{,}
                                 \AttributeTok{upper =} \ConstantTok{NULL}\NormalTok{,}
                                 \AttributeTok{lr =} \FloatTok{0.001}\NormalTok{,}
                                 \AttributeTok{max\_iter =} \DecValTok{5000}\NormalTok{,}
                                 \AttributeTok{tol =} \FloatTok{1e{-}6}\NormalTok{)}
\NormalTok{\{}
\NormalTok{  theta }\OtherTok{\textless{}{-}}\NormalTok{ par}
  \ControlFlowTok{for}\NormalTok{ (iter }\ControlFlowTok{in} \FunctionTok{seq\_len}\NormalTok{(max\_iter)) \{}
\NormalTok{    eps }\OtherTok{\textless{}{-}} \FloatTok{1e{-}6}
\NormalTok{    f0  }\OtherTok{\textless{}{-}} \FunctionTok{sse\_fn}\NormalTok{(theta, x, y)}
\NormalTok{    grad }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(}\FunctionTok{length}\NormalTok{(theta))}
    \ControlFlowTok{for}\NormalTok{ (p }\ControlFlowTok{in} \FunctionTok{seq\_along}\NormalTok{(theta)) \{}
\NormalTok{      pert }\OtherTok{\textless{}{-}}\NormalTok{ theta}
\NormalTok{      pert[p] }\OtherTok{\textless{}{-}}\NormalTok{ pert[p] }\SpecialCharTok{+}\NormalTok{ eps}
\NormalTok{      grad[p] }\OtherTok{\textless{}{-}}\NormalTok{ (}\FunctionTok{sse\_fn}\NormalTok{(pert, x, y) }\SpecialCharTok{{-}}\NormalTok{ f0) }\SpecialCharTok{/}\NormalTok{ eps}
\NormalTok{    \}}
\NormalTok{    theta\_new }\OtherTok{\textless{}{-}}\NormalTok{ theta }\SpecialCharTok{{-}}\NormalTok{ lr }\SpecialCharTok{*}\NormalTok{ grad}
    \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{sum}\NormalTok{(}\FunctionTok{abs}\NormalTok{(theta\_new }\SpecialCharTok{{-}}\NormalTok{ theta)) }\SpecialCharTok{\textless{}}\NormalTok{ tol)}
      \ControlFlowTok{break}
\NormalTok{    theta }\OtherTok{\textless{}{-}}\NormalTok{ theta\_new}
\NormalTok{  \}}
\NormalTok{  theta}
\NormalTok{\}}

\CommentTok{\# 2.4 LevenbergMarquardt (nlsLM)}
\NormalTok{lm\_fit }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(par,}
\NormalTok{                   x,}
\NormalTok{                   y,}
\NormalTok{                   sse\_fn,}
\NormalTok{                   model\_fn,}
                   \AttributeTok{lower =} \ConstantTok{NULL}\NormalTok{,}
                   \AttributeTok{upper =} \ConstantTok{NULL}\NormalTok{,}
                   \AttributeTok{form =} \FunctionTok{c}\NormalTok{(}\StringTok{"simple"}\NormalTok{, }\StringTok{"complex"}\NormalTok{))}
\NormalTok{\{}
\NormalTok{  form }\OtherTok{\textless{}{-}} \FunctionTok{match.arg}\NormalTok{(form)}
  \ControlFlowTok{if}\NormalTok{ (form }\SpecialCharTok{==} \StringTok{"simple"}\NormalTok{) \{}
\NormalTok{    fit }\OtherTok{\textless{}{-}} \FunctionTok{nlsLM}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ A }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{B }\SpecialCharTok{*}\NormalTok{ (x }\SpecialCharTok{{-}}\NormalTok{ C))),}
                 \AttributeTok{start =} \FunctionTok{list}\NormalTok{(}\AttributeTok{A =}\NormalTok{ par[}\DecValTok{1}\NormalTok{],}
                              \AttributeTok{B =}\NormalTok{ par[}\DecValTok{2}\NormalTok{],}
                              \AttributeTok{C =}\NormalTok{ par[}\DecValTok{3}\NormalTok{]))}
\NormalTok{  \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{    fit }\OtherTok{\textless{}{-}} \FunctionTok{nlsLM}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ A }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{B }\SpecialCharTok{*}\NormalTok{ (x }\SpecialCharTok{{-}}\NormalTok{ C))) }\SpecialCharTok{+}\NormalTok{ D }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.5} \SpecialCharTok{*}\NormalTok{ x),}
                 \AttributeTok{start =} \FunctionTok{list}\NormalTok{(}
                   \AttributeTok{A =}\NormalTok{ par[}\DecValTok{1}\NormalTok{],}
                   \AttributeTok{B =}\NormalTok{ par[}\DecValTok{2}\NormalTok{],}
                   \AttributeTok{C =}\NormalTok{ par[}\DecValTok{3}\NormalTok{],}
                   \AttributeTok{D =}\NormalTok{ par[}\DecValTok{4}\NormalTok{]}
\NormalTok{                 ))}
\NormalTok{  \}}
  \FunctionTok{coef}\NormalTok{(fit)}
\NormalTok{\}}

\CommentTok{\# 2.5 NewtonRaphson (with numeric Hessian, fallback if singular)}
\NormalTok{newton\_raphson\_fit }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(par,}
\NormalTok{                               x,}
\NormalTok{                               y,}
\NormalTok{                               sse\_fn,}
\NormalTok{                               model\_fn,}
                               \AttributeTok{lower =} \ConstantTok{NULL}\NormalTok{,}
                               \AttributeTok{upper =} \ConstantTok{NULL}\NormalTok{,}
                               \AttributeTok{max\_iter =} \DecValTok{50}\NormalTok{,}
                               \AttributeTok{tol =} \FloatTok{1e{-}6}\NormalTok{)}
\NormalTok{\{}
\NormalTok{  theta }\OtherTok{\textless{}{-}}\NormalTok{ par}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \FunctionTok{seq\_len}\NormalTok{(max\_iter)) \{}
\NormalTok{    eps }\OtherTok{\textless{}{-}} \FloatTok{1e{-}6}
\NormalTok{    f0  }\OtherTok{\textless{}{-}} \FunctionTok{sse\_fn}\NormalTok{(theta, x, y)}
\NormalTok{    grad }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(}\FunctionTok{length}\NormalTok{(theta))}
    \ControlFlowTok{for}\NormalTok{ (p }\ControlFlowTok{in} \FunctionTok{seq\_along}\NormalTok{(theta)) \{}
\NormalTok{      pert }\OtherTok{\textless{}{-}}\NormalTok{ theta}
\NormalTok{      pert[p] }\OtherTok{\textless{}{-}}\NormalTok{ pert[p] }\SpecialCharTok{+}\NormalTok{ eps}
\NormalTok{      grad[p] }\OtherTok{\textless{}{-}}\NormalTok{ (}\FunctionTok{sse\_fn}\NormalTok{(pert, x, y) }\SpecialCharTok{{-}}\NormalTok{ f0) }\SpecialCharTok{/}\NormalTok{ eps}
\NormalTok{    \}}
\NormalTok{    Hess }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FunctionTok{length}\NormalTok{(theta), }\FunctionTok{length}\NormalTok{(theta))}
    \ControlFlowTok{for}\NormalTok{ (p }\ControlFlowTok{in} \FunctionTok{seq\_along}\NormalTok{(theta)) \{}
\NormalTok{      pert\_p }\OtherTok{\textless{}{-}}\NormalTok{ theta}
\NormalTok{      pert\_p[p] }\OtherTok{\textless{}{-}}\NormalTok{ pert\_p[p] }\SpecialCharTok{+}\NormalTok{ eps}
\NormalTok{      f\_p }\OtherTok{\textless{}{-}} \FunctionTok{sse\_fn}\NormalTok{(pert\_p, x, y)}
      \ControlFlowTok{for}\NormalTok{ (q }\ControlFlowTok{in} \FunctionTok{seq\_along}\NormalTok{(theta)) \{}
\NormalTok{        pert\_q }\OtherTok{\textless{}{-}}\NormalTok{ pert\_p}
\NormalTok{        pert\_q[q] }\OtherTok{\textless{}{-}}\NormalTok{ pert\_q[q] }\SpecialCharTok{+}\NormalTok{ eps}
\NormalTok{        Hess[p, q] }\OtherTok{\textless{}{-}}\NormalTok{ (}\FunctionTok{sse\_fn}\NormalTok{(pert\_q, x, y) }\SpecialCharTok{{-}}
\NormalTok{                         f\_p }\SpecialCharTok{{-}}\NormalTok{ (f0 }\SpecialCharTok{{-}} \FunctionTok{sse\_fn}\NormalTok{(theta, x, y))) }\SpecialCharTok{/}\NormalTok{ (eps }\SpecialCharTok{\^{}}
                                                                \DecValTok{2}\NormalTok{)}
\NormalTok{      \}}
\NormalTok{    \}}
\NormalTok{    delta }\OtherTok{\textless{}{-}} \FunctionTok{tryCatch}\NormalTok{(}
      \FunctionTok{solve}\NormalTok{(Hess, grad),}
      \AttributeTok{error =} \ControlFlowTok{function}\NormalTok{(e)}
\NormalTok{        MASS}\SpecialCharTok{::}\FunctionTok{ginv}\NormalTok{(Hess) }\SpecialCharTok{\%*\%}\NormalTok{ grad}
\NormalTok{    )}
\NormalTok{    theta\_new }\OtherTok{\textless{}{-}}\NormalTok{ theta }\SpecialCharTok{{-}}\NormalTok{ delta}
    \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{sum}\NormalTok{(}\FunctionTok{abs}\NormalTok{(theta\_new }\SpecialCharTok{{-}}\NormalTok{ theta)) }\SpecialCharTok{\textless{}}\NormalTok{ tol)}
      \ControlFlowTok{break}
\NormalTok{    theta }\OtherTok{\textless{}{-}}\NormalTok{ theta\_new}
\NormalTok{  \}}
\NormalTok{  theta}
\NormalTok{\}}

\CommentTok{\# 2.6 QuasiNewton (BFGS via optim)}
\NormalTok{quasi\_newton\_fit }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(par,}
\NormalTok{                             x,}
\NormalTok{                             y,}
\NormalTok{                             sse\_fn,}
\NormalTok{                             model\_fn,}
                             \AttributeTok{lower =} \ConstantTok{NULL}\NormalTok{,}
                             \AttributeTok{upper =} \ConstantTok{NULL}\NormalTok{)}
\NormalTok{\{}
\NormalTok{  fn }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(pp)}
    \FunctionTok{sse\_fn}\NormalTok{(pp, x, y)}
\NormalTok{  res }\OtherTok{\textless{}{-}} \FunctionTok{optim}\NormalTok{(par, fn, }\AttributeTok{method =} \StringTok{"BFGS"}\NormalTok{)}
\NormalTok{  res}\SpecialCharTok{$}\NormalTok{par}
\NormalTok{\}}

\CommentTok{\# 2.7 Trust{-}region reflective (nloptr)}
\NormalTok{trust\_region\_fit }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(par,}
\NormalTok{                             x,}
\NormalTok{                             y,}
\NormalTok{                             sse\_fn,}
\NormalTok{                             model\_fn,}
                             \AttributeTok{lower =} \ConstantTok{NULL}\NormalTok{,}
                             \AttributeTok{upper =} \ConstantTok{NULL}\NormalTok{)}
\NormalTok{\{}
  \CommentTok{\# numeric gradient}
\NormalTok{  grad\_numeric }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(pp, }\AttributeTok{eps =} \FloatTok{1e{-}6}\NormalTok{) \{}
\NormalTok{    g  }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(}\FunctionTok{length}\NormalTok{(pp))}
\NormalTok{    f0 }\OtherTok{\textless{}{-}} \FunctionTok{sse\_fn}\NormalTok{(pp, x, y)}
    \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \FunctionTok{seq\_along}\NormalTok{(pp)) \{}
\NormalTok{      p2 }\OtherTok{\textless{}{-}}\NormalTok{ pp}
\NormalTok{      p2[i] }\OtherTok{\textless{}{-}}\NormalTok{ p2[i] }\SpecialCharTok{+}\NormalTok{ eps}
\NormalTok{      g[i] }\OtherTok{\textless{}{-}}\NormalTok{ (}\FunctionTok{sse\_fn}\NormalTok{(p2, x, y) }\SpecialCharTok{{-}}\NormalTok{ f0) }\SpecialCharTok{/}\NormalTok{ eps}
\NormalTok{    \}}
\NormalTok{    g}
\NormalTok{  \}}
\NormalTok{  eval\_f }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(pp) \{}
\NormalTok{    val }\OtherTok{\textless{}{-}} \FunctionTok{sse\_fn}\NormalTok{(pp, x, y)}
\NormalTok{    gr  }\OtherTok{\textless{}{-}} \FunctionTok{grad\_numeric}\NormalTok{(pp)}
    \FunctionTok{list}\NormalTok{(}\AttributeTok{objective =}\NormalTok{ val, }\AttributeTok{gradient =}\NormalTok{ gr)}
\NormalTok{  \}}
\NormalTok{  lb }\OtherTok{\textless{}{-}} \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{is.null}\NormalTok{(lower))}
    \FunctionTok{rep}\NormalTok{(}\SpecialCharTok{{-}}\ConstantTok{Inf}\NormalTok{, }\FunctionTok{length}\NormalTok{(par))}
  \ControlFlowTok{else}
\NormalTok{    lower}
\NormalTok{  ub }\OtherTok{\textless{}{-}} \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{is.null}\NormalTok{(upper))}
    \FunctionTok{rep}\NormalTok{(}\ConstantTok{Inf}\NormalTok{, }\FunctionTok{length}\NormalTok{(par))}
  \ControlFlowTok{else}
\NormalTok{    upper}
\NormalTok{  res }\OtherTok{\textless{}{-}} \FunctionTok{nloptr}\NormalTok{(}
    \AttributeTok{x0 =}\NormalTok{ par,}
    \AttributeTok{eval\_f =}\NormalTok{ eval\_f,}
    \AttributeTok{lb =}\NormalTok{ lb,}
    \AttributeTok{ub =}\NormalTok{ ub,}
    \AttributeTok{opts =} \FunctionTok{list}\NormalTok{(}
      \AttributeTok{algorithm =} \StringTok{"NLOPT\_LD\_TNEWTON"}\NormalTok{,}
      \AttributeTok{maxeval =} \DecValTok{500}\NormalTok{,}
      \AttributeTok{xtol\_rel =} \FloatTok{1e{-}6}
\NormalTok{    )}
\NormalTok{  )}
\NormalTok{  res}\SpecialCharTok{$}\NormalTok{solution}
\NormalTok{\}}

\CommentTok{\# 2.8 Grid search}
\NormalTok{grid\_search\_fit }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(par,}
\NormalTok{                            x,}
\NormalTok{                            y,}
\NormalTok{                            sse\_fn,}
\NormalTok{                            model\_fn,}
                            \AttributeTok{lower =} \ConstantTok{NULL}\NormalTok{,}
                            \AttributeTok{upper =} \ConstantTok{NULL}\NormalTok{,}
                            \AttributeTok{grid\_defs =} \ConstantTok{NULL}\NormalTok{)}
\NormalTok{\{}
  \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{is.null}\NormalTok{(grid\_defs))}
    \FunctionTok{stop}\NormalTok{(}\StringTok{"Must provide grid\_defs for multi{-}parameter grid search."}\NormalTok{)}
\NormalTok{  g }\OtherTok{\textless{}{-}} \FunctionTok{expand.grid}\NormalTok{(grid\_defs)}
\NormalTok{  g}\SpecialCharTok{$}\NormalTok{SSE }\OtherTok{\textless{}{-}}
    \FunctionTok{apply}\NormalTok{(g, }\DecValTok{1}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(rowp)}
      \FunctionTok{sse\_fn}\NormalTok{(}\FunctionTok{as.numeric}\NormalTok{(rowp), x, y))}
\NormalTok{  best\_idx }\OtherTok{\textless{}{-}} \FunctionTok{which.min}\NormalTok{(g}\SpecialCharTok{$}\NormalTok{SSE)}
  \FunctionTok{as.numeric}\NormalTok{(g[best\_idx, }\FunctionTok{seq\_along}\NormalTok{(grid\_defs)])}
\NormalTok{\}}

\CommentTok{\# 2.9 Nelder{-}Mead}
\NormalTok{nelder\_mead\_fit }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(par,}
\NormalTok{                            x,}
\NormalTok{                            y,}
\NormalTok{                            sse\_fn,}
\NormalTok{                            model\_fn,}
                            \AttributeTok{lower =} \ConstantTok{NULL}\NormalTok{,}
                            \AttributeTok{upper =} \ConstantTok{NULL}\NormalTok{)}
\NormalTok{\{}
\NormalTok{  fn }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(pp)}
    \FunctionTok{sse\_fn}\NormalTok{(pp, x, y)}
\NormalTok{  res }\OtherTok{\textless{}{-}} \FunctionTok{optim}\NormalTok{(par, fn, }\AttributeTok{method =} \StringTok{"Nelder{-}Mead"}\NormalTok{)}
\NormalTok{  res}\SpecialCharTok{$}\NormalTok{par}
\NormalTok{\}}

\CommentTok{\# 2.10 Powells method (dfoptim::nmk for unconstrained)}
\NormalTok{powell\_fit }\OtherTok{\textless{}{-}}
  \ControlFlowTok{function}\NormalTok{(par,}
\NormalTok{           x,}
\NormalTok{           y,}
\NormalTok{           sse\_fn,}
\NormalTok{           model\_fn,}
           \AttributeTok{lower =} \ConstantTok{NULL}\NormalTok{,}
           \AttributeTok{upper =} \ConstantTok{NULL}\NormalTok{) \{}
\NormalTok{    fn }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(pp)}
      \FunctionTok{sse\_fn}\NormalTok{(pp, x, y)}
\NormalTok{    dfoptim}\SpecialCharTok{::}\FunctionTok{nmk}\NormalTok{(par, fn)}\SpecialCharTok{$}\NormalTok{par}
\NormalTok{  \}}

\CommentTok{\# 2.11 Hooke{-}Jeeves (dfoptim::hjkb)}
\NormalTok{hooke\_jeeves\_fit }\OtherTok{\textless{}{-}}
  \ControlFlowTok{function}\NormalTok{(par,}
\NormalTok{           x,}
\NormalTok{           y,}
\NormalTok{           sse\_fn,}
\NormalTok{           model\_fn,}
           \AttributeTok{lower =} \ConstantTok{NULL}\NormalTok{,}
           \AttributeTok{upper =} \ConstantTok{NULL}\NormalTok{) \{}
\NormalTok{    fn }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(pp)}
      \FunctionTok{sse\_fn}\NormalTok{(pp, x, y)}
\NormalTok{    dfoptim}\SpecialCharTok{::}\FunctionTok{hjkb}\NormalTok{(par, fn)}\SpecialCharTok{$}\NormalTok{par}
\NormalTok{  \}}

\CommentTok{\# 2.12 Random Search}
\NormalTok{random\_search\_fit }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(par,}
\NormalTok{                              x,}
\NormalTok{                              y,}
\NormalTok{                              sse\_fn,}
\NormalTok{                              model\_fn,}
\NormalTok{                              lower,}
\NormalTok{                              upper,}
                              \AttributeTok{max\_iter =} \DecValTok{2000}\NormalTok{,}
\NormalTok{                              ...)}
\NormalTok{\{}
\NormalTok{  best\_par }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\NormalTok{  best\_sse }\OtherTok{\textless{}{-}} \ConstantTok{Inf}
\NormalTok{  dimp }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(lower)}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \FunctionTok{seq\_len}\NormalTok{(max\_iter)) \{}
\NormalTok{    candidate }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(dimp, }\AttributeTok{min =}\NormalTok{ lower, }\AttributeTok{max =}\NormalTok{ upper)}
\NormalTok{    val }\OtherTok{\textless{}{-}} \FunctionTok{sse\_fn}\NormalTok{(candidate, x, y)}
    \ControlFlowTok{if}\NormalTok{ (val }\SpecialCharTok{\textless{}}\NormalTok{ best\_sse) \{}
\NormalTok{      best\_sse }\OtherTok{\textless{}{-}}\NormalTok{ val}
\NormalTok{      best\_par }\OtherTok{\textless{}{-}}\NormalTok{ candidate}
\NormalTok{    \}}
\NormalTok{  \}}
\NormalTok{  best\_par}
\NormalTok{\}}

\CommentTok{\# 2.13 Differential Evolution (DEoptim)}
\NormalTok{diff\_evo\_fit }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(par,}
\NormalTok{                         x,}
\NormalTok{                         y,}
\NormalTok{                         sse\_fn,}
\NormalTok{                         model\_fn,}
\NormalTok{                         lower,}
\NormalTok{                         upper,}
                         \AttributeTok{max\_iter =} \DecValTok{100}\NormalTok{,}
\NormalTok{                         ...)}
\NormalTok{\{}
\NormalTok{  fn }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(v)}
    \FunctionTok{sse\_fn}\NormalTok{(v, x, y)}
\NormalTok{  out }\OtherTok{\textless{}{-}} \FunctionTok{DEoptim}\NormalTok{(fn,}
                 \AttributeTok{lower =}\NormalTok{ lower,}
                 \AttributeTok{upper =}\NormalTok{ upper,}
                 \FunctionTok{DEoptim.control}\NormalTok{(}\AttributeTok{NP =} \DecValTok{50}\NormalTok{, }\AttributeTok{itermax =}\NormalTok{ max\_iter, }\AttributeTok{trace =}\NormalTok{ F))}
\NormalTok{  out}\SpecialCharTok{$}\NormalTok{optim}\SpecialCharTok{$}\NormalTok{bestmem}
\NormalTok{\}}

\CommentTok{\# 2.14 Simulated Annealing (GenSA)}
\NormalTok{sim\_anneal\_fit }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(par,}
\NormalTok{                           x,}
\NormalTok{                           y,}
\NormalTok{                           sse\_fn,}
\NormalTok{                           model\_fn,}
                           \AttributeTok{lower =} \ConstantTok{NULL}\NormalTok{,}
                           \AttributeTok{upper =} \ConstantTok{NULL}\NormalTok{,}
\NormalTok{                           ...)}
\NormalTok{\{}
\NormalTok{  fn }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(pp)}
    \FunctionTok{sse\_fn}\NormalTok{(pp, x, y)}
\NormalTok{  lb }\OtherTok{\textless{}{-}} \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{is.null}\NormalTok{(lower))}
    \FunctionTok{rep}\NormalTok{(}\SpecialCharTok{{-}}\ConstantTok{Inf}\NormalTok{, }\FunctionTok{length}\NormalTok{(par))}
  \ControlFlowTok{else}
\NormalTok{    lower}
\NormalTok{  ub }\OtherTok{\textless{}{-}} \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{is.null}\NormalTok{(upper))}
    \FunctionTok{rep}\NormalTok{(}\ConstantTok{Inf}\NormalTok{, }\FunctionTok{length}\NormalTok{(par))}
  \ControlFlowTok{else}
\NormalTok{    upper}
  \CommentTok{\# GenSA requires: GenSA(par, fn, lower, upper, control=list(...))}
\NormalTok{  out }\OtherTok{\textless{}{-}}
    \FunctionTok{GenSA}\NormalTok{(}
\NormalTok{      par,}
\NormalTok{      fn,}
      \AttributeTok{lower =}\NormalTok{ lb,}
      \AttributeTok{upper =}\NormalTok{ ub,}
      \AttributeTok{control =} \FunctionTok{list}\NormalTok{(}\AttributeTok{max.call =} \DecValTok{10000}\NormalTok{)}
\NormalTok{    )}
\NormalTok{  out}\SpecialCharTok{$}\NormalTok{par}
\NormalTok{\}}

\CommentTok{\# 2.15 Genetic Algorithm (GA)}
\NormalTok{genetic\_fit }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(par,}
\NormalTok{                        x,}
\NormalTok{                        y,}
\NormalTok{                        sse\_fn,}
\NormalTok{                        model\_fn,}
\NormalTok{                        lower,}
\NormalTok{                        upper,}
                        \AttributeTok{max\_iter =} \DecValTok{100}\NormalTok{,}
\NormalTok{                        ...)}
\NormalTok{\{}
\NormalTok{  fitness\_fun }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(pp)}
    \SpecialCharTok{{-}} \FunctionTok{sse\_fn}\NormalTok{(pp, x, y)}
\NormalTok{  gares }\OtherTok{\textless{}{-}} \FunctionTok{ga}\NormalTok{(}
    \AttributeTok{type =} \StringTok{"real{-}valued"}\NormalTok{,}
    \AttributeTok{fitness =}\NormalTok{ fitness\_fun,}
    \AttributeTok{lower =}\NormalTok{ lower,}
    \AttributeTok{upper =}\NormalTok{ upper,}
    \AttributeTok{popSize =} \DecValTok{50}\NormalTok{,}
    \AttributeTok{maxiter =}\NormalTok{ max\_iter,}
    \AttributeTok{run =} \DecValTok{50}
\NormalTok{  )}
\NormalTok{  gares}\SpecialCharTok{@}\NormalTok{solution[}\DecValTok{1}\NormalTok{,]}
\NormalTok{\}}

\CommentTok{\# 2.16 Particle Swarm (pso)}
\NormalTok{particle\_swarm\_fit }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(par,}
\NormalTok{                               x,}
\NormalTok{                               y,}
\NormalTok{                               sse\_fn,}
\NormalTok{                               model\_fn,}
\NormalTok{                               lower,}
\NormalTok{                               upper,}
                               \AttributeTok{max\_iter =} \DecValTok{100}\NormalTok{,}
\NormalTok{                               ...)}
\NormalTok{\{}
\NormalTok{  fn }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(pp)}
    \FunctionTok{sse\_fn}\NormalTok{(pp, x, y)}
\NormalTok{  res }\OtherTok{\textless{}{-}} \FunctionTok{psoptim}\NormalTok{(}
    \AttributeTok{par =}\NormalTok{ (lower }\SpecialCharTok{+}\NormalTok{ upper) }\SpecialCharTok{/} \DecValTok{2}\NormalTok{,}
    \AttributeTok{fn =}\NormalTok{ fn,}
    \AttributeTok{lower =}\NormalTok{ lower,}
    \AttributeTok{upper =}\NormalTok{ upper,}
    \AttributeTok{control =} \FunctionTok{list}\NormalTok{(}\AttributeTok{maxit =}\NormalTok{ max\_iter)}
\NormalTok{  )}
\NormalTok{  res}\SpecialCharTok{$}\NormalTok{par}
\NormalTok{\}}


\CommentTok{\# {-}{-} 3) RUN METHOD WRAPPER {-}{-}{-}}
\NormalTok{run\_method }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(method\_name,}
\NormalTok{                       FUN,}
\NormalTok{                       par\_init,}
\NormalTok{                       x,}
\NormalTok{                       y,}
\NormalTok{                       sse\_fn,}
\NormalTok{                       model\_fn,}
                       \AttributeTok{lower =} \ConstantTok{NULL}\NormalTok{,}
                       \AttributeTok{upper =} \ConstantTok{NULL}\NormalTok{,}
\NormalTok{                       ...)}
\NormalTok{\{}
\NormalTok{  mb }\OtherTok{\textless{}{-}} \FunctionTok{microbenchmark}\NormalTok{(}\AttributeTok{result =}\NormalTok{ \{}
\NormalTok{    out }\OtherTok{\textless{}{-}} \FunctionTok{FUN}\NormalTok{(par\_init, x, y, sse\_fn, model\_fn, lower, upper, ...)}
\NormalTok{    out}
\NormalTok{  \}, }\AttributeTok{times =} \DecValTok{1}\NormalTok{)}
\NormalTok{  final\_par }\OtherTok{\textless{}{-}}
    \FunctionTok{FUN}\NormalTok{(par\_init, x, y, sse\_fn, model\_fn, lower, upper, ...)}
  \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{is.null}\NormalTok{(final\_par)) \{}
    \CommentTok{\# e.g. placeholders that return NULL}
    \FunctionTok{return}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(}
      \AttributeTok{Method =}\NormalTok{ method\_name,}
      \AttributeTok{Parameters =} \StringTok{"N/A"}\NormalTok{,}
      \AttributeTok{SSE =} \ConstantTok{NA}\NormalTok{,}
      \AttributeTok{Time\_ms =} \ConstantTok{NA}
\NormalTok{    ))}
\NormalTok{  \}}
  \FunctionTok{data.frame}\NormalTok{(}
    \AttributeTok{Method     =}\NormalTok{ method\_name,}
    \AttributeTok{Parameters =} \FunctionTok{paste}\NormalTok{(}\FunctionTok{round}\NormalTok{(final\_par, }\DecValTok{4}\NormalTok{), }\AttributeTok{collapse =} \StringTok{", "}\NormalTok{),}
    \AttributeTok{SSE        =} \FunctionTok{round}\NormalTok{(}\FunctionTok{sse\_fn}\NormalTok{(final\_par, x, y), }\DecValTok{6}\NormalTok{),}
    \AttributeTok{Time\_ms    =} \FunctionTok{median}\NormalTok{(mb}\SpecialCharTok{$}\NormalTok{time) }\SpecialCharTok{/} \FloatTok{1e6}
\NormalTok{  )}
\NormalTok{\}}

\CommentTok{\# {-}{-} 4) MASTER FUNCTION TO COMPARE ALL METHODS (SIMPLE / COMPLEX) {-}{-}{-}}

\NormalTok{compare\_all\_methods }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(}\AttributeTok{is\_complex =} \ConstantTok{FALSE}\NormalTok{) \{}
  \ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\NormalTok{is\_complex) \{}
    \CommentTok{\# SIMPLE (3{-}param logistic)}
\NormalTok{    x }\OtherTok{\textless{}{-}}\NormalTok{ x\_data}
\NormalTok{    y }\OtherTok{\textless{}{-}}\NormalTok{ y\_data\_simple}
\NormalTok{    sse\_fn   }\OtherTok{\textless{}{-}}\NormalTok{ sse\_logistic}
\NormalTok{    model\_fn }\OtherTok{\textless{}{-}}\NormalTok{ f\_logistic}
\NormalTok{    init\_par }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{)}
\NormalTok{    grid\_defs }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
      \AttributeTok{A =} \FunctionTok{seq}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{6}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{10}\NormalTok{),}
      \AttributeTok{B =} \FunctionTok{seq}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\DecValTok{2}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{10}\NormalTok{),}
      \AttributeTok{C =} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{10}\NormalTok{)}
\NormalTok{    )}
\NormalTok{    lower }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{0.1}\NormalTok{,}\SpecialCharTok{{-}}\DecValTok{3}\NormalTok{)}
\NormalTok{    upper }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{3}\NormalTok{,  }\DecValTok{3}\NormalTok{)}
\NormalTok{    lm\_form }\OtherTok{\textless{}{-}} \StringTok{"simple"}
\NormalTok{  \} }\ControlFlowTok{else}\NormalTok{ \{}
    \CommentTok{\# COMPLEX (4{-}param model)}
\NormalTok{    x }\OtherTok{\textless{}{-}}\NormalTok{ x\_data}
\NormalTok{    y }\OtherTok{\textless{}{-}}\NormalTok{ y\_data\_complex}
\NormalTok{    sse\_fn   }\OtherTok{\textless{}{-}}\NormalTok{ sse\_complex}
\NormalTok{    model\_fn }\OtherTok{\textless{}{-}}\NormalTok{ f\_complex}
\NormalTok{    init\_par }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{,}\SpecialCharTok{{-}}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.2}\NormalTok{)}
\NormalTok{    grid\_defs }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
      \AttributeTok{A =} \FunctionTok{seq}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{6}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{8}\NormalTok{),}
      \AttributeTok{B =} \FunctionTok{seq}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\DecValTok{2}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{8}\NormalTok{),}
      \AttributeTok{C =} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{8}\NormalTok{),}
      \AttributeTok{D =} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{8}\NormalTok{)}
\NormalTok{    )}
\NormalTok{    lower }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{0.1}\NormalTok{,}\SpecialCharTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{    upper }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{3}\NormalTok{,  }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\NormalTok{    lm\_form }\OtherTok{\textless{}{-}} \StringTok{"complex"}
\NormalTok{  \}}
  
  \CommentTok{\# RUN each method}
\NormalTok{  out }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(}
    \FunctionTok{run\_method}\NormalTok{(}
      \StringTok{"Gauss{-}Newton"}\NormalTok{,}
\NormalTok{      gauss\_newton\_fit,}
\NormalTok{      init\_par,}
\NormalTok{      x,}
\NormalTok{      y,}
\NormalTok{      sse\_fn,}
\NormalTok{      model\_fn}
\NormalTok{    ),}
    \FunctionTok{run\_method}\NormalTok{(}
      \StringTok{"Modified Gauss{-}Newton"}\NormalTok{,}
\NormalTok{      modified\_gauss\_newton\_fit,}
\NormalTok{      init\_par,}
\NormalTok{      x,}
\NormalTok{      y,}
\NormalTok{      sse\_fn,}
\NormalTok{      model\_fn}
\NormalTok{    ),}
    \FunctionTok{run\_method}\NormalTok{(}
      \StringTok{"Steepest Descent"}\NormalTok{,}
\NormalTok{      steepest\_descent\_fit,}
\NormalTok{      init\_par,}
\NormalTok{      x,}
\NormalTok{      y,}
\NormalTok{      sse\_fn,}
\NormalTok{      model\_fn}
\NormalTok{    ),}
    \FunctionTok{run\_method}\NormalTok{(}
      \StringTok{"Levenberg{-}Marquardt (nlsLM)"}\NormalTok{,}
\NormalTok{      lm\_fit,}
\NormalTok{      init\_par,}
\NormalTok{      x,}
\NormalTok{      y,}
\NormalTok{      sse\_fn,}
\NormalTok{      model\_fn,}
      \AttributeTok{form =}\NormalTok{ lm\_form}
\NormalTok{    ),}
    \FunctionTok{run\_method}\NormalTok{(}
      \StringTok{"Newton{-}Raphson"}\NormalTok{,}
\NormalTok{      newton\_raphson\_fit,}
\NormalTok{      init\_par,}
\NormalTok{      x,}
\NormalTok{      y,}
\NormalTok{      sse\_fn,}
\NormalTok{      model\_fn}
\NormalTok{    ),}
    \FunctionTok{run\_method}\NormalTok{(}
      \StringTok{"Quasi{-}Newton (BFGS)"}\NormalTok{,}
\NormalTok{      quasi\_newton\_fit,}
\NormalTok{      init\_par,}
\NormalTok{      x,}
\NormalTok{      y,}
\NormalTok{      sse\_fn,}
\NormalTok{      model\_fn}
\NormalTok{    ),}
    \FunctionTok{run\_method}\NormalTok{(}
      \StringTok{"Trust{-}region Reflective"}\NormalTok{,}
\NormalTok{      trust\_region\_fit,}
\NormalTok{      init\_par,}
\NormalTok{      x,}
\NormalTok{      y,}
\NormalTok{      sse\_fn,}
\NormalTok{      model\_fn,}
\NormalTok{      lower,}
\NormalTok{      upper}
\NormalTok{    ),}
    \FunctionTok{run\_method}\NormalTok{(}
      \StringTok{"Grid Search"}\NormalTok{,}
\NormalTok{      grid\_search\_fit,}
      \ConstantTok{NULL}\NormalTok{,}
\NormalTok{      x,}
\NormalTok{      y,}
\NormalTok{      sse\_fn,}
\NormalTok{      model\_fn,}
      \AttributeTok{grid\_defs =}\NormalTok{ grid\_defs}
\NormalTok{    ),}
    \FunctionTok{run\_method}\NormalTok{(}
      \StringTok{"Nelder{-}Mead"}\NormalTok{,}
\NormalTok{      nelder\_mead\_fit,}
\NormalTok{      init\_par,}
\NormalTok{      x,}
\NormalTok{      y,}
\NormalTok{      sse\_fn,}
\NormalTok{      model\_fn}
\NormalTok{    ),}
    \FunctionTok{run\_method}\NormalTok{(}\StringTok{"Powell\textquotesingle{}s method"}\NormalTok{,}
\NormalTok{               powell\_fit,}
\NormalTok{               init\_par,}
\NormalTok{               x,}
\NormalTok{               y,}
\NormalTok{               sse\_fn,}
\NormalTok{               model\_fn),}
    \FunctionTok{run\_method}\NormalTok{(}
      \StringTok{"Hooke{-}Jeeves"}\NormalTok{,}
\NormalTok{      hooke\_jeeves\_fit,}
\NormalTok{      init\_par,}
\NormalTok{      x,}
\NormalTok{      y,}
\NormalTok{      sse\_fn,}
\NormalTok{      model\_fn}
\NormalTok{    ),}
    \FunctionTok{run\_method}\NormalTok{(}
      \StringTok{"Random Search"}\NormalTok{,}
\NormalTok{      random\_search\_fit,}
      \ConstantTok{NULL}\NormalTok{,}
\NormalTok{      x,}
\NormalTok{      y,}
\NormalTok{      sse\_fn,}
\NormalTok{      model\_fn,}
\NormalTok{      lower,}
\NormalTok{      upper,}
      \AttributeTok{max\_iter =} \DecValTok{1000}
\NormalTok{    ),}
    \FunctionTok{run\_method}\NormalTok{(}
      \StringTok{"Differential Evolution"}\NormalTok{,}
\NormalTok{      diff\_evo\_fit,}
      \ConstantTok{NULL}\NormalTok{,}
\NormalTok{      x,}
\NormalTok{      y,}
\NormalTok{      sse\_fn,}
\NormalTok{      model\_fn,}
\NormalTok{      lower,}
\NormalTok{      upper,}
      \AttributeTok{max\_iter =} \DecValTok{50}
\NormalTok{    ),}
    \FunctionTok{run\_method}\NormalTok{(}
      \StringTok{"Simulated Annealing"}\NormalTok{,}
\NormalTok{      sim\_anneal\_fit,}
\NormalTok{      init\_par,}
\NormalTok{      x,}
\NormalTok{      y,}
\NormalTok{      sse\_fn,}
\NormalTok{      model\_fn,}
\NormalTok{      lower,}
\NormalTok{      upper}
\NormalTok{    ),}
    \FunctionTok{run\_method}\NormalTok{(}
      \StringTok{"Genetic Algorithm"}\NormalTok{,}
\NormalTok{      genetic\_fit,}
      \ConstantTok{NULL}\NormalTok{,}
\NormalTok{      x,}
\NormalTok{      y,}
\NormalTok{      sse\_fn,}
\NormalTok{      model\_fn,}
\NormalTok{      lower,}
\NormalTok{      upper,}
      \AttributeTok{max\_iter =} \DecValTok{50}
\NormalTok{    ),}
    \FunctionTok{run\_method}\NormalTok{(}
      \StringTok{"Particle Swarm"}\NormalTok{,}
\NormalTok{      particle\_swarm\_fit,}
      \ConstantTok{NULL}\NormalTok{,}
\NormalTok{      x,}
\NormalTok{      y,}
\NormalTok{      sse\_fn,}
\NormalTok{      model\_fn,}
\NormalTok{      lower,}
\NormalTok{      upper,}
      \AttributeTok{max\_iter =} \DecValTok{50}
\NormalTok{    )}
\NormalTok{  )}
\NormalTok{  out}
\NormalTok{\}}

\CommentTok{\# {-}{-} 5) RUN \& VISUALIZE {-}{-}{-}{-}}

\CommentTok{\# Compare "simple" logistic (3 params)}
\NormalTok{results\_simple  }\OtherTok{\textless{}{-}} \FunctionTok{compare\_all\_methods}\NormalTok{(}\AttributeTok{is\_complex =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{results\_simple}\SpecialCharTok{$}\NormalTok{Problem }\OtherTok{\textless{}{-}} \StringTok{"Simple"}

\CommentTok{\# Compare "complex" (4 params)}
\NormalTok{results\_complex }\OtherTok{\textless{}{-}} \FunctionTok{compare\_all\_methods}\NormalTok{(}\AttributeTok{is\_complex =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{results\_complex}\SpecialCharTok{$}\NormalTok{Problem }\OtherTok{\textless{}{-}} \StringTok{"Complex"}

\CommentTok{\# Combine}
\NormalTok{all\_results }\OtherTok{\textless{}{-}} \FunctionTok{rbind}\NormalTok{(results\_simple, results\_complex)}
\CommentTok{\# print(all\_results)}
\CommentTok{\# DT::datatable(all\_results)}

\CommentTok{\# Example: SSE by method \& problem}
\FunctionTok{ggplot}\NormalTok{(all\_results, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Method, }\AttributeTok{y =} \FunctionTok{log}\NormalTok{(SSE), }\AttributeTok{fill =}\NormalTok{ Problem)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat =} \StringTok{"identity"}\NormalTok{, }\AttributeTok{position =} \StringTok{"dodge"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{(}\AttributeTok{base\_size =} \DecValTok{11}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Comparison of SSE by Method \& Problem Complexity"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{""}\NormalTok{, }\AttributeTok{y =} \StringTok{"Log(Sum of Squared Errors)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.text.x =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{angle =} \DecValTok{45}\NormalTok{, }\AttributeTok{hjust =} \DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{06-nonlinear-regession_files/figure-latex/unnamed-chunk-24-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Example: Time (ms) by method \& problem}
\FunctionTok{ggplot}\NormalTok{(all\_results, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Method, }\AttributeTok{y =}\NormalTok{ Time\_ms, }\AttributeTok{fill =}\NormalTok{ Problem)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat =} \StringTok{"identity"}\NormalTok{, }\AttributeTok{position =} \StringTok{"dodge"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{(}\AttributeTok{base\_size =} \DecValTok{11}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Comparison of Computation Time by Method \& Problem Complexity"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{""}\NormalTok{, }\AttributeTok{y =} \StringTok{"Time (ms)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.text.x =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{angle =} \DecValTok{45}\NormalTok{, }\AttributeTok{hjust =} \DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{06-nonlinear-regession_files/figure-latex/unnamed-chunk-24-2} \end{center}

\hypertarget{practical-considerations-2}{%
\section{Practical Considerations}\label{practical-considerations-2}}

For optimization algorithms to converge, they require good initial estimates of the parameters. The choice of starting values, constraints, and the complexity of the model all play a role in whether an optimization algorithm successfully finds a suitable solution.

\hypertarget{selecting-starting-values}{%
\subsection{Selecting Starting Values}\label{selecting-starting-values}}

Choosing good starting values can significantly impact the efficiency and success of optimization algorithms. Several approaches can be used:

\begin{itemize}
\tightlist
\item
  \textbf{Prior or theoretical information}: If prior knowledge about the parameters is available, it should be incorporated into the choice of initial values.
\item
  \textbf{Grid search or graphical inspection of} \(SSE(\theta)\): Evaluating the sum of squared errors (SSE) across a grid of possible values can help identify promising starting points.
\item
  \protect\hyperlink{ordinary-least-squares}{Ordinary Least Squares} \textbf{estimates}: If a linear approximation of the model exists, using OLS to obtain initial estimates can be effective.
\item
  \textbf{Model interpretation}: Understanding the structure and behavior of the model can provide intuition for reasonable starting values.
\item
  \textbf{Expected Value Parameterization}: Reformulating the model based on expected values may improve the interpretability and numerical stability of the estimation.
\end{itemize}

\hypertarget{grid-search-for-optimal-starting-values}{%
\subsubsection{Grid Search for Optimal Starting Values}\label{grid-search-for-optimal-starting-values}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Set seed for reproducibility}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\# Generate x as 100 integers using seq function}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{)}

\CommentTok{\# Generate coefficients for exponential function}
\NormalTok{a }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{20}\NormalTok{)  }\CommentTok{\# Random coefficient a}
\NormalTok{b }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{0.005}\NormalTok{, }\FloatTok{0.075}\NormalTok{)  }\CommentTok{\# Random coefficient b}
\NormalTok{c }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\DecValTok{101}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{)  }\CommentTok{\# Random noise}

\CommentTok{\# Generate y as a * e\^{}(b*x) + c}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ a }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(b }\SpecialCharTok{*}\NormalTok{ x) }\SpecialCharTok{+}\NormalTok{ c}

\CommentTok{\# Print the generated parameters}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Generated coefficients:}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} Generated coefficients:}
\FunctionTok{cat}\NormalTok{(}\StringTok{"a ="}\NormalTok{, a, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} a = 5.75155}
\FunctionTok{cat}\NormalTok{(}\StringTok{"b ="}\NormalTok{, b, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\CommentTok{\#\textgreater{} b = 0.06018136}

\CommentTok{\# Define our data frame}
\NormalTok{datf }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(x, y)}

\CommentTok{\# Define our model function}
\NormalTok{mod }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(a, b, x) \{}
\NormalTok{  a }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(b }\SpecialCharTok{*}\NormalTok{ x)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Ensure all y values are positive (avoid log issues)}
\NormalTok{y\_adj }\OtherTok{\textless{}{-}}
  \FunctionTok{ifelse}\NormalTok{(y }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{, y, }\FunctionTok{min}\NormalTok{(y[y }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{]) }\SpecialCharTok{+} \FloatTok{1e{-}3}\NormalTok{)  }\CommentTok{\# Shift small values slightly}

\CommentTok{\# Create adjusted dataframe}
\NormalTok{datf\_adj }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(x, y\_adj)}

\CommentTok{\# Linearize by taking log(y)}
\NormalTok{lin\_mod }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(y\_adj) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{data =}\NormalTok{ datf\_adj)}

\CommentTok{\# Extract starting values}
\NormalTok{astrt }\OtherTok{\textless{}{-}}
  \FunctionTok{exp}\NormalTok{(}\FunctionTok{coef}\NormalTok{(lin\_mod)[}\DecValTok{1}\NormalTok{])  }\CommentTok{\# Convert intercept back from log scale}
\NormalTok{bstrt }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(lin\_mod)[}\DecValTok{2}\NormalTok{]  }\CommentTok{\# Slope remains the same}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Starting values for non{-}linear fit:}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\FunctionTok{print}\NormalTok{(}\FunctionTok{c}\NormalTok{(astrt, bstrt))}

\CommentTok{\# Fit nonlinear model with these starting values}
\NormalTok{nlin\_mod }\OtherTok{\textless{}{-}} \FunctionTok{nls}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}} \FunctionTok{mod}\NormalTok{(a, b, x),}
                \AttributeTok{start =} \FunctionTok{list}\NormalTok{(}\AttributeTok{a =}\NormalTok{ astrt, }\AttributeTok{b =}\NormalTok{ bstrt),}
                \AttributeTok{data =}\NormalTok{ datf)}

\CommentTok{\# Model summary}
\FunctionTok{summary}\NormalTok{(nlin\_mod)}

\CommentTok{\# Plot original data}
\FunctionTok{plot}\NormalTok{(}
\NormalTok{  x,}
\NormalTok{  y,}
  \AttributeTok{main =} \StringTok{"Exponential Growth Fit"}\NormalTok{,}
  \AttributeTok{col =} \StringTok{"blue"}\NormalTok{,}
  \AttributeTok{pch =} \DecValTok{16}\NormalTok{,}
  \AttributeTok{xlab =} \StringTok{"x"}\NormalTok{,}
  \AttributeTok{ylab =} \StringTok{"y"}
\NormalTok{)}

\CommentTok{\# Add fitted curve in red}
\FunctionTok{lines}\NormalTok{(x, }\FunctionTok{predict}\NormalTok{(nlin\_mod), }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}

\CommentTok{\# Add legend}
\FunctionTok{legend}\NormalTok{(}
  \StringTok{"topleft"}\NormalTok{,}
  \AttributeTok{legend =} \FunctionTok{c}\NormalTok{(}\StringTok{"Original Data"}\NormalTok{, }\StringTok{"Fitted Model"}\NormalTok{),}
  \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\StringTok{"red"}\NormalTok{),}
  \AttributeTok{pch =} \FunctionTok{c}\NormalTok{(}\DecValTok{16}\NormalTok{, }\ConstantTok{NA}\NormalTok{),}
  \AttributeTok{lwd =} \FunctionTok{c}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Define grid of possible parameter values}
\NormalTok{aseq }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{18}\NormalTok{, }\FloatTok{0.2}\NormalTok{)}
\NormalTok{bseq }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\FloatTok{0.001}\NormalTok{, }\FloatTok{0.075}\NormalTok{, }\FloatTok{0.001}\NormalTok{)}

\NormalTok{na }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(aseq)}
\NormalTok{nb }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(bseq)}
\NormalTok{SSout }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{, na }\SpecialCharTok{*}\NormalTok{ nb, }\DecValTok{3}\NormalTok{)  }\CommentTok{\# Matrix to store SSE values}
\NormalTok{cnt }\OtherTok{\textless{}{-}} \DecValTok{0}

\CommentTok{\# Evaluate SSE across grid}
\ControlFlowTok{for}\NormalTok{ (k }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{na) \{}
  \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{nb) \{}
\NormalTok{    cnt }\OtherTok{\textless{}{-}}\NormalTok{ cnt }\SpecialCharTok{+} \DecValTok{1}
\NormalTok{    ypred }\OtherTok{\textless{}{-}}
      \CommentTok{\# Evaluate model at these parameter values}
      \FunctionTok{mod}\NormalTok{(aseq[k], bseq[j], x)  }
    
    \CommentTok{\# Compute SSE}
\NormalTok{    ss }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{((y }\SpecialCharTok{{-}}\NormalTok{ ypred) }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{)  }
\NormalTok{    SSout[cnt, }\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ aseq[k]}
\NormalTok{    SSout[cnt, }\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ bseq[j]}
\NormalTok{    SSout[cnt, }\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ ss}
\NormalTok{  \}}
\NormalTok{\}}

\CommentTok{\# Identify optimal starting values}
\NormalTok{mn\_indx }\OtherTok{\textless{}{-}} \FunctionTok{which.min}\NormalTok{(SSout[, }\DecValTok{3}\NormalTok{])}
\NormalTok{astrt }\OtherTok{\textless{}{-}}\NormalTok{ SSout[mn\_indx, }\DecValTok{1}\NormalTok{]}
\NormalTok{bstrt }\OtherTok{\textless{}{-}}\NormalTok{ SSout[mn\_indx, }\DecValTok{2}\NormalTok{]}

\CommentTok{\# Fit nonlinear model using optimal starting values}
\NormalTok{nlin\_modG }\OtherTok{\textless{}{-}}
  \FunctionTok{nls}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}} \FunctionTok{mod}\NormalTok{(a, b, x), }\AttributeTok{start =} \FunctionTok{list}\NormalTok{(}\AttributeTok{a =}\NormalTok{ astrt, }\AttributeTok{b =}\NormalTok{ bstrt))}

\CommentTok{\# Display model results}
\FunctionTok{summary}\NormalTok{(nlin\_modG)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Formula: y \textasciitilde{} mod(a, b, x)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Parameters:}
\CommentTok{\#\textgreater{}    Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} a 5.889e+00  1.986e{-}02   296.6   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} b 5.995e{-}02  3.644e{-}05  1645.0   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 2.135 on 99 degrees of freedom}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of iterations to convergence: 4 }
\CommentTok{\#\textgreater{} Achieved convergence tolerance: 7.204e{-}06}
\end{Highlighting}
\end{Shaded}

Note: The \texttt{nls\_multstart} package can perform a grid search more efficiently without requiring manual looping.

Visualizing Prediction Intervals

Once the model is fitted, it is useful to visualize prediction intervals to assess model uncertainty.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load necessary package}
\FunctionTok{library}\NormalTok{(nlstools)}

\CommentTok{\# Plot fitted model with confidence and prediction intervals}
\FunctionTok{plotFit}\NormalTok{(}
\NormalTok{  nlin\_modG,}
  \AttributeTok{interval =} \StringTok{"both"}\NormalTok{,}
  \AttributeTok{pch =} \DecValTok{19}\NormalTok{,}
  \AttributeTok{shade =} \ConstantTok{TRUE}\NormalTok{,}
  \AttributeTok{col.conf =} \StringTok{"skyblue4"}\NormalTok{,}
  \AttributeTok{col.pred =} \StringTok{"lightskyblue2"}\NormalTok{,}
  \AttributeTok{data =}\NormalTok{ datf}
\NormalTok{)  }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{06-nonlinear-regession_files/figure-latex/unnamed-chunk-27-1} \end{center}

\hypertarget{using-programmed-starting-values-in-nls}{%
\subsubsection{\texorpdfstring{Using Programmed Starting Values in \texttt{nls}}{Using Programmed Starting Values in nls}}\label{using-programmed-starting-values-in-nls}}

Many nonlinear models have well-established functional forms, allowing for programmed starting values in the \texttt{nls} function. For example, models such as \textbf{logistic growth} and \textbf{asymptotic regression} have built-in self-starting functions.

To explore available self-starting models in R, use:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{apropos}\NormalTok{(}\StringTok{"\^{}SS"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This command lists functions with names starting with \texttt{SS}, which typically denote self-starting functions for nonlinear regression.

\hypertarget{custom-self-starting-functions}{%
\subsubsection{Custom Self-Starting Functions}\label{custom-self-starting-functions}}

If your model does not match any built-in \texttt{nls} functions, you can define your own \textbf{self-starting function}. Self-starting functions in \texttt{R} automate the process of estimating initial values, which helps in fitting nonlinear models more efficiently.

If needed, a self-starting function should:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Define the nonlinear equation.
\item
  Implement a method for computing starting values.
\item
  Return the function structure in an appropriate format.
\end{enumerate}

\hypertarget{handling-constrained-parameters}{%
\subsection{Handling Constrained Parameters}\label{handling-constrained-parameters}}

In some cases, parameters must satisfy constraints (e.g., \(\theta_i > a\) or \(a < \theta_i < b\)). The following strategies help address constrained parameter estimation:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Fit the model without constraints first}: If the unconstrained parameter estimates satisfy the desired constraints, no further action is needed.
\item
  \textbf{Re-parameterization}: If the estimated parameters violate constraints, consider re-parameterizing the model to naturally enforce the required bounds.
\end{enumerate}

\hypertarget{failure-to-converge}{%
\subsection{Failure to Converge}\label{failure-to-converge}}

Several factors can cause an algorithm to fail to converge:

\begin{itemize}
\tightlist
\item
  \textbf{A ``flat'' SSE function}: If the sum of squared errors \(SSE(\theta)\) is relatively constant in the neighborhood of the minimum, the algorithm may struggle to locate an optimal solution.
\item
  \textbf{Poor starting values}: Trying different or better initial values can help.
\item
  \textbf{Overly complex models}: If the model is too complex relative to the data, consider simplifying it.
\end{itemize}

\hypertarget{convergence-to-a-local-minimum}{%
\subsection{Convergence to a Local Minimum}\label{convergence-to-a-local-minimum}}

\begin{itemize}
\tightlist
\item
  \textbf{Linear least squares models} have a well-defined, unique minimum because the SSE function is quadratic:\\
  \[ SSE(\theta) = (Y - X\beta)'(Y - X\beta) \]
\item
  \textbf{Nonlinear least squares models} may have multiple local minima.
\item
  \textbf{Testing different starting values} can help identify a global minimum.
\item
  \textbf{Graphing} \(SSE(\theta)\) as a function of individual parameters (if feasible) can provide insights.
\item
  \textbf{Alternative optimization algorithms} such as \protect\hyperlink{genetic-algorithm}{Genetic Algorithm} or \protect\hyperlink{particle-swarm-optimization}{particle swarm optimization} may be useful in non-convex problems.
\end{itemize}

\hypertarget{model-adequacy-and-estimation-considerations}{%
\subsection{Model Adequacy and Estimation Considerations}\label{model-adequacy-and-estimation-considerations}}

Assessing the adequacy of a \textbf{nonlinear model} involves checking its \textbf{nonlinearity}, \textbf{goodness of fit}, and \textbf{residual behavior}. Unlike linear models, nonlinear models do not always have a direct equivalent of \(R^2\), and issues such as collinearity, leverage, and residual heteroscedasticity must be carefully evaluated.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{components-of-nonlinearity}{%
\subsubsection{Components of Nonlinearity}\label{components-of-nonlinearity}}

\citet{bates1980relative} defines two key aspects of \textbf{nonlinearity} in statistical modeling:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Intrinsic Nonlinearity}
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Measures the \textbf{bending and twisting} in the function \(f(\theta)\).
\item
  Assumes that the function is relatively \textbf{flat (planar)} in the neighborhood of \(\hat{\theta}\).
\item
  If severe, the \textbf{distribution of residuals} will be \textbf{distorted}.
\item
  Leads to:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Slow convergence} of optimization algorithms.
  \item
    \textbf{Difficulties in identifying} parameter estimates.
  \end{itemize}
\item
  Solution approaches:

  \begin{itemize}
  \tightlist
  \item
    Higher-order \textbf{Taylor expansions} for estimation.
  \item
    \textbf{Bayesian methods} for parameter estimation.
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Check intrinsic curvature}
\NormalTok{modD }\OtherTok{\textless{}{-}} \FunctionTok{deriv3}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ a }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(b }\SpecialCharTok{*}\NormalTok{ x), }\FunctionTok{c}\NormalTok{(}\StringTok{"a"}\NormalTok{, }\StringTok{"b"}\NormalTok{), }\ControlFlowTok{function}\NormalTok{(a, b, x) }\ConstantTok{NULL}\NormalTok{)}

\NormalTok{nlin\_modD }\OtherTok{\textless{}{-}} \FunctionTok{nls}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}} \FunctionTok{modD}\NormalTok{(a, b, x),}
                 \AttributeTok{start =} \FunctionTok{list}\NormalTok{(}\AttributeTok{a =}\NormalTok{ astrt, }\AttributeTok{b =}\NormalTok{ bstrt),}
                 \AttributeTok{data =}\NormalTok{ datf)}

\FunctionTok{rms.curv}\NormalTok{(nlin\_modD)  }\CommentTok{\# Function from the MASS package to assess curvature}
\CommentTok{\#\textgreater{} Parameter effects: c\^{}theta x sqrt(F) = 0.0564 }
\CommentTok{\#\textgreater{}         Intrinsic: c\^{}iota  x sqrt(F) = 9e{-}04}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Parameter-Effects Nonlinearity}
\end{enumerate}

\begin{itemize}
\item
  Measures \textbf{how the curvature} (nonlinearity) depends on the parameterization.
\item
  Strong parameter effects nonlinearity can cause \textbf{problems with inference on} \(\hat{\theta}\).
\item
  Can be assessed using:

  \begin{itemize}
  \item
    \texttt{rms.curv} function from \texttt{MASS}.
  \item
    Bootstrap-based inference.
  \end{itemize}
\item
  \textbf{Solution:} Try \textbf{reparameterization} to stabilize the function.
\end{itemize}

\hypertarget{goodness-of-fit-in-nonlinear-models}{%
\subsubsection{Goodness of Fit in Nonlinear Models}\label{goodness-of-fit-in-nonlinear-models}}

In \textbf{linear regression}, we use the standard \textbf{coefficient of determination} (\$R\^{}2\$):

\[
R^2 = \frac{SSR}{SSTO} = 1 - \frac{SSE}{SSTO}
\]\hspace{0pt}where:

\begin{itemize}
\item
  \(SSR\) = Regression Sum of Squares
\item
  \(SSE\) = Error Sum of Squares
\item
  \(SSTO\) = Total Sum of Squares
\end{itemize}

However, in \textbf{nonlinear models}, the error and model sum of squares do not necessarily add up to the total corrected sum of squares:

\[
SSR + SSE \neq SST
\]

Thus, \(R^2\) is not directly valid in the nonlinear case. Instead, we use a pseudo-\(R^2\):

\[
R^2_{pseudo} = 1 - \frac{\sum_{i=1}^n ({Y}_i- \hat{Y})^2}{\sum_{i=1}^n (Y_i- \bar{Y})^2}
\]

\begin{itemize}
\item
  Unlike true \(R^2\), this cannot be interpreted as the proportion of variability explained by the model.
\item
  Should be used only for relative model comparison (e.g., comparing different nonlinear models).
\end{itemize}

\hypertarget{residual-analysis-in-nonlinear-models}{%
\subsubsection{Residual Analysis in Nonlinear Models}\label{residual-analysis-in-nonlinear-models}}

Residual plots help assess model adequacy, particularly when \textbf{intrinsic curvature is small}.

In nonlinear models, the \textbf{studentized residuals} are:

\[
r_i = \frac{e_i}{s \sqrt{1-\hat{c}_i}}
\]

where:

\begin{itemize}
\item
  \(e_i\) = residual for observation \(i\)
\item
  \(\hat{c}_i\) = \(i\)th diagonal element of the \textbf{tangent-plane hat matrix}:
\end{itemize}

\[
\mathbf{\hat{H} = F(\hat{\theta})[F(\hat{\theta})'F(\hat{\theta})]^{-1}F(\hat{\theta})'}
\]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Residual diagnostics for nonlinear models}
\FunctionTok{library}\NormalTok{(nlstools)}
\NormalTok{resid\_nls }\OtherTok{\textless{}{-}} \FunctionTok{nlsResiduals}\NormalTok{(nlin\_modD)}

\CommentTok{\# Generate residual plots}
\FunctionTok{plot}\NormalTok{(resid\_nls)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{06-nonlinear-regession_files/figure-latex/unnamed-chunk-30-1} \end{center}

\hypertarget{potential-issues-in-nonlinear-regression-models}{%
\subsubsection{Potential Issues in Nonlinear Regression Models}\label{potential-issues-in-nonlinear-regression-models}}

\hypertarget{collinearity-1}{%
\paragraph{Collinearity}\label{collinearity-1}}

\begin{itemize}
\item
  Measures how correlated the model's predictors are.
\item
  In nonlinear models, collinearity is assessed using the condition number of:
\end{itemize}

\[
\mathbf{[F(\hat{\theta})'F(\hat{\theta})]^{-1}}
\]

\begin{itemize}
\item
  If condition number \textgreater{} 30, collinearity is a concern.
\item
  Solution: Consider reparameterization \citep{Magel_1987}.
\end{itemize}

\hypertarget{leverage}{%
\paragraph{Leverage}\label{leverage}}

\begin{itemize}
\item
  Similar to leverage in \protect\hyperlink{ordinary-least-squares}{Ordinary Least Squares}.
\item
  In nonlinear models, leverage is assessed using the \textbf{tangent-plane hat matrix}:
\end{itemize}

\[
\mathbf{\hat{H} = F(\hat{\theta})[F(\hat{\theta})'F(\hat{\theta})]^{-1}F(\hat{\theta})'}
\]

\begin{itemize}
\tightlist
\item
  \textbf{Solution:} Identify influential points and assess their impact on parameter estimates \citep{Laurent_1992}.
\end{itemize}

\hypertarget{heterogeneous-errors}{%
\paragraph{Heterogeneous Errors}\label{heterogeneous-errors}}

\begin{itemize}
\item
  Non-constant variance across observations.
\item
  \textbf{Solution:} Use \textbf{Weighted Nonlinear Least Squares (WNLS)}.
\end{itemize}

\hypertarget{correlated-errors}{%
\paragraph{Correlated Errors}\label{correlated-errors}}

\begin{itemize}
\item
  Residuals may be autocorrelated.
\item
  \textbf{Solution approaches:}

  \begin{itemize}
  \item
    \textbf{Generalized Nonlinear Least Squares (GNLS)}
  \item
    \textbf{Nonlinear Mixed Models (NLMEM)}
  \item
    \textbf{Bayesian Methods}
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2676}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3803}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3521}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Issue
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Solution
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Intrinsic Nonlinearity} & Function curvature independent of parameterization & Bayesian estimation, Taylor expansion \\
\textbf{Parameter-Effects Nonlinearity} & Curvature influenced by parameterization & Reparameterization, bootstrap \\
\textbf{Collinearity} & High correlation among predictors & Reparameterization, condition number check \\
\textbf{Leverage} & Influential points affecting model fit & Assess tangent-plane hat matrix \\
\textbf{Heterogeneous Errors} & Unequal variance in residuals & Weighted Nonlinear Least Squares \\
\textbf{Correlated Errors} & Autocorrelated residuals & GNLS, Nonlinear Mixed Models, Bayesian Methods \\
\end{longtable}

\hypertarget{application}{%
\section{Application}\label{application}}

\hypertarget{nonlinear-estimation-using-gauss-newton-algorithm}{%
\subsection{Nonlinear Estimation Using Gauss-Newton Algorithm}\label{nonlinear-estimation-using-gauss-newton-algorithm}}

This section demonstrates nonlinear parameter estimation using the Gauss-Newton algorithm and compares results with \texttt{nls()}. The model is given by:

\[
y_i = \frac{\theta_0 + \theta_1 x_i}{1 + \theta_2 \exp(0.4 x_i)} + \epsilon_i
\]

where

\begin{itemize}
\tightlist
\item
  \(i = 1, \dots ,n\)
\item
  \(\theta_0\), \(\theta_1\), \(\theta_2\) are the unknown parameters.
\item
  \(\epsilon_i\) represents errors.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Loading and Visualizing the Data
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr)}
\FunctionTok{library}\NormalTok{(ggplot2)}

\CommentTok{\# Load the dataset}
\NormalTok{my\_data }\OtherTok{\textless{}{-}} \FunctionTok{read.delim}\NormalTok{(}\StringTok{"images/S21hw1pr4.txt"}\NormalTok{, }\AttributeTok{header =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{sep =} \StringTok{""}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{rename}\NormalTok{(}\AttributeTok{x =}\NormalTok{ V1, }\AttributeTok{y =}\NormalTok{ V2)}

\CommentTok{\# Plot data}
\FunctionTok{ggplot}\NormalTok{(my\_data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ y)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{color =} \StringTok{"blue"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Observed Data"}\NormalTok{, }\AttributeTok{x =} \StringTok{"X"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Y"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{06-nonlinear-regession_files/figure-latex/unnamed-chunk-31-1} \end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Deriving Starting Values for Parameters
\end{enumerate}

Since nonlinear optimization is sensitive to starting values, we estimate reasonable initial values based on model interpretation.

Finding the Maximum \(Y\) Value

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{max}\NormalTok{(my\_data}\SpecialCharTok{$}\NormalTok{y)}
\CommentTok{\#\textgreater{} [1] 2.6722}
\NormalTok{my\_data}\SpecialCharTok{$}\NormalTok{x[}\FunctionTok{which.max}\NormalTok{(my\_data}\SpecialCharTok{$}\NormalTok{y)]}
\CommentTok{\#\textgreater{} [1] 0.0094}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  When \(y = 2.6722\), the corresponding \(x = 0.0094\).
\item
  From the model equation: \(\theta_0 + 0.0094 \theta_1 = 2.6722\)
\end{itemize}

Estimating \(\theta_2\) from the Median \(y\) Value

\begin{itemize}
\tightlist
\item
  The equation simplifies to: \(1 + \theta_2 \exp(0.4 x) = 2\)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# find mean y}
\FunctionTok{mean}\NormalTok{(my\_data}\SpecialCharTok{$}\NormalTok{y) }
\CommentTok{\#\textgreater{} [1] {-}0.0747864}

\CommentTok{\# find y closest to its mean}
\NormalTok{my\_data}\SpecialCharTok{$}\NormalTok{y[}\FunctionTok{which.min}\NormalTok{(}\FunctionTok{abs}\NormalTok{(my\_data}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{{-}}\NormalTok{ (}\FunctionTok{mean}\NormalTok{(my\_data}\SpecialCharTok{$}\NormalTok{y))))] }
\CommentTok{\#\textgreater{} [1] {-}0.0773}


\CommentTok{\# find x closest to the mean y}
\NormalTok{my\_data}\SpecialCharTok{$}\NormalTok{x[}\FunctionTok{which.min}\NormalTok{(}\FunctionTok{abs}\NormalTok{(my\_data}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{{-}}\NormalTok{ (}\FunctionTok{mean}\NormalTok{(my\_data}\SpecialCharTok{$}\NormalTok{y))))] }
\CommentTok{\#\textgreater{} [1] 11.0648}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  This yields the equation: \(83.58967 \theta_2 = 1\)
\end{itemize}

Finding the Value of \(\theta_0\) and \(\theta_1\)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# find value of x closet to 1}
\NormalTok{my\_data}\SpecialCharTok{$}\NormalTok{x[}\FunctionTok{which.min}\NormalTok{(}\FunctionTok{abs}\NormalTok{(my\_data}\SpecialCharTok{$}\NormalTok{x }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{))] }
\CommentTok{\#\textgreater{} [1] 0.9895}

\CommentTok{\# find index of x closest to 1}
\FunctionTok{match}\NormalTok{(my\_data}\SpecialCharTok{$}\NormalTok{x[}\FunctionTok{which.min}\NormalTok{(}\FunctionTok{abs}\NormalTok{(my\_data}\SpecialCharTok{$}\NormalTok{x }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{))], my\_data}\SpecialCharTok{$}\NormalTok{x) }
\CommentTok{\#\textgreater{} [1] 14}

\CommentTok{\# find y value}
\NormalTok{my\_data}\SpecialCharTok{$}\NormalTok{y[}\FunctionTok{match}\NormalTok{(my\_data}\SpecialCharTok{$}\NormalTok{x[}\FunctionTok{which.min}\NormalTok{(}\FunctionTok{abs}\NormalTok{(my\_data}\SpecialCharTok{$}\NormalTok{x }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{))], my\_data}\SpecialCharTok{$}\NormalTok{x)]}
\CommentTok{\#\textgreater{} [1] 1.4577}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  This provides another equation: \(\theta_0 + \theta_1 \times 0.9895 - 2.164479 \theta_2 = 1.457\)
\end{itemize}

Solving for \(\theta_0, \theta_1, \theta_2\)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(matlib)}

\CommentTok{\# Define coefficient matrix}
\NormalTok{A }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}
  \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.0094}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\FloatTok{83.58967}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.9895}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{2.164479}\NormalTok{),}
  \AttributeTok{nrow =} \DecValTok{3}\NormalTok{,}
  \AttributeTok{ncol =} \DecValTok{3}\NormalTok{,}
  \AttributeTok{byrow =}\NormalTok{ T}
\NormalTok{)}

\CommentTok{\# Define constant vector}
\NormalTok{b }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{2.6722}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{1.457}\NormalTok{)}

\CommentTok{\# Display system of equations}
\FunctionTok{showEqn}\NormalTok{(A, b)}
\CommentTok{\#\textgreater{} 0*x1 + 0.0094*x2        + 0*x3  =  2.6722 }
\CommentTok{\#\textgreater{} 0*x1      + 0*x2 + 83.58967*x3  =       1 }
\CommentTok{\#\textgreater{} 1*x1 + 0.9895*x2 {-} 2.164479*x3  =   1.457}

\CommentTok{\# Solve for parameters}
\NormalTok{theta\_start }\OtherTok{\textless{}{-}} \FunctionTok{Solve}\NormalTok{(A, b, }\AttributeTok{fractions =} \ConstantTok{FALSE}\NormalTok{)}
\CommentTok{\#\textgreater{} x1      =  {-}279.80879739 }
\CommentTok{\#\textgreater{}   x2    =   284.27659574 }
\CommentTok{\#\textgreater{}     x3  =      0.0119632}
\NormalTok{theta\_start}
\CommentTok{\#\textgreater{} [1] "x1      =  {-}279.80879739" "  x2    =   284.27659574"}
\CommentTok{\#\textgreater{} [3] "    x3  =      0.0119632"}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Implementing the Gauss-Newton Algorithm
\end{enumerate}

Using these estimates, we manually implement the \textbf{Gauss-Newton optimization}.

\textbf{Defining the Model and Its Derivatives}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Starting values}
\NormalTok{theta\_0\_strt }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{gsub}\NormalTok{(}\StringTok{".*=}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{s*"}\NormalTok{, }\StringTok{""}\NormalTok{, theta\_start[}\DecValTok{1}\NormalTok{]))}
\NormalTok{theta\_1\_strt }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{gsub}\NormalTok{(}\StringTok{".*=}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{s*"}\NormalTok{, }\StringTok{""}\NormalTok{, theta\_start[}\DecValTok{2}\NormalTok{]))}
\NormalTok{theta\_2\_strt }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{gsub}\NormalTok{(}\StringTok{".*=}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{s*"}\NormalTok{, }\StringTok{""}\NormalTok{, theta\_start[}\DecValTok{3}\NormalTok{]))}

\CommentTok{\# Model function}
\NormalTok{mod\_4 }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta\_0, theta\_1, theta\_2, x) \{}
\NormalTok{  (theta\_0 }\SpecialCharTok{+}\NormalTok{ theta\_1 }\SpecialCharTok{*}\NormalTok{ x) }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+}\NormalTok{ theta\_2 }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(}\FloatTok{0.4} \SpecialCharTok{*}\NormalTok{ x))}
\NormalTok{\}}

\CommentTok{\# Define function expression}
\NormalTok{f\_4 }\OtherTok{=} \FunctionTok{expression}\NormalTok{((theta\_0 }\SpecialCharTok{+}\NormalTok{ theta\_1 }\SpecialCharTok{*}\NormalTok{ x) }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+}\NormalTok{ theta\_2 }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(}\FloatTok{0.4} \SpecialCharTok{*}\NormalTok{ x)))}

\CommentTok{\# First derivatives}
\NormalTok{df\_4.d\_theta\_0 }\OtherTok{\textless{}{-}} \FunctionTok{D}\NormalTok{(f\_4, }\StringTok{\textquotesingle{}theta\_0\textquotesingle{}}\NormalTok{)}
\NormalTok{df\_4.d\_theta\_1 }\OtherTok{\textless{}{-}} \FunctionTok{D}\NormalTok{(f\_4, }\StringTok{\textquotesingle{}theta\_1\textquotesingle{}}\NormalTok{)}
\NormalTok{df\_4.d\_theta\_2 }\OtherTok{\textless{}{-}} \FunctionTok{D}\NormalTok{(f\_4, }\StringTok{\textquotesingle{}theta\_2\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Iterative Gauss-Newton Optimization

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Initialize}
\NormalTok{theta\_vec }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(theta\_0\_strt, theta\_1\_strt, theta\_2\_strt))}
\NormalTok{delta }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\AttributeTok{nrow =} \DecValTok{3}\NormalTok{, }\AttributeTok{ncol =} \DecValTok{1}\NormalTok{)}
\NormalTok{i }\OtherTok{\textless{}{-}} \DecValTok{1}

\CommentTok{\# Evaluate function at initial estimates}
\NormalTok{f\_theta }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{eval}\NormalTok{(f\_4, }\FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ my\_data}\SpecialCharTok{$}\NormalTok{x,}
  \AttributeTok{theta\_0 =}\NormalTok{ theta\_vec[}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{],}
  \AttributeTok{theta\_1 =}\NormalTok{ theta\_vec[}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{],}
  \AttributeTok{theta\_2 =}\NormalTok{ theta\_vec[}\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{]}
\NormalTok{)))}

\ControlFlowTok{repeat}\NormalTok{ \{}
  \CommentTok{\# Compute Jacobian matrix}
\NormalTok{  F\_theta\_0 }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(}
    \FunctionTok{eval}\NormalTok{(df\_4.d\_theta\_0, }\FunctionTok{list}\NormalTok{(}
      \AttributeTok{x =}\NormalTok{ my\_data}\SpecialCharTok{$}\NormalTok{x,}
      \AttributeTok{theta\_0 =}\NormalTok{ theta\_vec[}\DecValTok{1}\NormalTok{, i],}
      \AttributeTok{theta\_1 =}\NormalTok{ theta\_vec[}\DecValTok{2}\NormalTok{, i],}
      \AttributeTok{theta\_2 =}\NormalTok{ theta\_vec[}\DecValTok{3}\NormalTok{, i]}
\NormalTok{    )),}
    \FunctionTok{eval}\NormalTok{(df\_4.d\_theta\_1, }\FunctionTok{list}\NormalTok{(}
      \AttributeTok{x =}\NormalTok{ my\_data}\SpecialCharTok{$}\NormalTok{x,}
      \AttributeTok{theta\_0 =}\NormalTok{ theta\_vec[}\DecValTok{1}\NormalTok{, i],}
      \AttributeTok{theta\_1 =}\NormalTok{ theta\_vec[}\DecValTok{2}\NormalTok{, i],}
      \AttributeTok{theta\_2 =}\NormalTok{ theta\_vec[}\DecValTok{3}\NormalTok{, i]}
\NormalTok{    )),}
    \FunctionTok{eval}\NormalTok{(df\_4.d\_theta\_2, }\FunctionTok{list}\NormalTok{(}
      \AttributeTok{x =}\NormalTok{ my\_data}\SpecialCharTok{$}\NormalTok{x,}
      \AttributeTok{theta\_0 =}\NormalTok{ theta\_vec[}\DecValTok{1}\NormalTok{, i],}
      \AttributeTok{theta\_1 =}\NormalTok{ theta\_vec[}\DecValTok{2}\NormalTok{, i],}
      \AttributeTok{theta\_2 =}\NormalTok{ theta\_vec[}\DecValTok{3}\NormalTok{, i]}
\NormalTok{    ))}
\NormalTok{  ))}
  
  \CommentTok{\# Compute parameter updates}
\NormalTok{  delta[, i] }\OtherTok{=}\NormalTok{ (}\FunctionTok{solve}\NormalTok{(}\FunctionTok{t}\NormalTok{(F\_theta\_0)}\SpecialCharTok{\%*\%}\NormalTok{F\_theta\_0))}\SpecialCharTok{\%*\%}\FunctionTok{t}\NormalTok{(F\_theta\_0)}\SpecialCharTok{\%*\%}\NormalTok{(my\_data}\SpecialCharTok{$}\NormalTok{y}\SpecialCharTok{{-}}\NormalTok{f\_theta[,i])}
    
  
  \CommentTok{\# Update parameter estimates}
\NormalTok{  theta\_vec }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(theta\_vec, theta\_vec[, i] }\SpecialCharTok{+}\NormalTok{ delta[, i])}
\NormalTok{  theta\_vec[, i }\SpecialCharTok{+} \DecValTok{1}\NormalTok{] }\OtherTok{=}\NormalTok{ theta\_vec[, i] }\SpecialCharTok{+}\NormalTok{ delta[, i]}
  
  \CommentTok{\# Increment iteration counter}
\NormalTok{  i }\OtherTok{\textless{}{-}}\NormalTok{ i }\SpecialCharTok{+} \DecValTok{1}
  
  \CommentTok{\# Compute new function values}
\NormalTok{  f\_theta }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(f\_theta, }\FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{eval}\NormalTok{(f\_4, }\FunctionTok{list}\NormalTok{(}
    \AttributeTok{x =}\NormalTok{ my\_data}\SpecialCharTok{$}\NormalTok{x,}
    \AttributeTok{theta\_0 =}\NormalTok{ theta\_vec[}\DecValTok{1}\NormalTok{, i],}
    \AttributeTok{theta\_1 =}\NormalTok{ theta\_vec[}\DecValTok{2}\NormalTok{, i],}
    \AttributeTok{theta\_2 =}\NormalTok{ theta\_vec[}\DecValTok{3}\NormalTok{, i]}
\NormalTok{  ))))}
  
\NormalTok{  delta }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(delta, }\FunctionTok{matrix}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\AttributeTok{nrow =} \DecValTok{3}\NormalTok{, }\AttributeTok{ncol =} \DecValTok{1}\NormalTok{))}
  
  \CommentTok{\# Convergence criteria based on SSE}
  \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{abs}\NormalTok{(}\FunctionTok{sum}\NormalTok{((my\_data}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{{-}}\NormalTok{ f\_theta[, i])}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{{-}} \FunctionTok{sum}\NormalTok{((my\_data}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{{-}}\NormalTok{ f\_theta[, i }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{])}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)) }\SpecialCharTok{/} 
      \FunctionTok{sum}\NormalTok{((my\_data}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{{-}}\NormalTok{ f\_theta[, i }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{])}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{\textless{}} \FloatTok{0.001}\NormalTok{) \{}
    \ControlFlowTok{break}
\NormalTok{  \}}
\NormalTok{\}}

\CommentTok{\# Final parameter estimates}
\NormalTok{theta\_vec[, }\FunctionTok{ncol}\NormalTok{(theta\_vec)]}
\CommentTok{\#\textgreater{} [1]  3.6335135 {-}1.3055166  0.5043502}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Checking Convergence and Variance
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Final objective function value (SSE)}
\FunctionTok{sum}\NormalTok{((my\_data}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{{-}}\NormalTok{ f\_theta[, i])}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 19.80165}

\NormalTok{sigma2 }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{/}\NormalTok{ (}\FunctionTok{nrow}\NormalTok{(my\_data) }\SpecialCharTok{{-}} \DecValTok{3}\NormalTok{) }\SpecialCharTok{*} 
\NormalTok{  (}\FunctionTok{t}\NormalTok{(my\_data}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{{-}}\NormalTok{ f\_theta[, }\FunctionTok{ncol}\NormalTok{(f\_theta)]) }\SpecialCharTok{\%*\%} 
\NormalTok{   (my\_data}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{{-}}\NormalTok{ f\_theta[, }\FunctionTok{ncol}\NormalTok{(f\_theta)]))  }\CommentTok{\# p = 3}

\CommentTok{\# Asymptotic variance{-}covariance matrix}
\FunctionTok{as.numeric}\NormalTok{(sigma2)}\SpecialCharTok{*}\FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{solve}\NormalTok{(}\FunctionTok{crossprod}\NormalTok{(F\_theta\_0)))}
\CommentTok{\#\textgreater{}             [,1]        [,2]        [,3]}
\CommentTok{\#\textgreater{} [1,]  0.11552571 {-}0.04817428  0.02685848}
\CommentTok{\#\textgreater{} [2,] {-}0.04817428  0.02100861 {-}0.01158212}
\CommentTok{\#\textgreater{} [3,]  0.02685848 {-}0.01158212  0.00703916}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Validating with \texttt{nls()}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nlin\_4 }\OtherTok{\textless{}{-}} \FunctionTok{nls}\NormalTok{(}
\NormalTok{  y }\SpecialCharTok{\textasciitilde{}} \FunctionTok{mod\_4}\NormalTok{(theta\_0, theta\_1, theta\_2, x),}
  \AttributeTok{start =} \FunctionTok{list}\NormalTok{(}
    \AttributeTok{theta\_0 =} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{gsub}\NormalTok{(}\StringTok{".*=}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{s*"}\NormalTok{, }\StringTok{""}\NormalTok{, theta\_start[}\DecValTok{1}\NormalTok{])),}
    \AttributeTok{theta\_1 =} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{gsub}\NormalTok{(}\StringTok{".*=}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{s*"}\NormalTok{, }\StringTok{""}\NormalTok{, theta\_start[}\DecValTok{2}\NormalTok{])),}
    \AttributeTok{theta\_2 =} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{gsub}\NormalTok{(}\StringTok{".*=}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{s*"}\NormalTok{, }\StringTok{""}\NormalTok{, theta\_start[}\DecValTok{3}\NormalTok{]))}
\NormalTok{  ),}
  \AttributeTok{data =}\NormalTok{ my\_data}
\NormalTok{)}
\FunctionTok{summary}\NormalTok{(nlin\_4)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Formula: y \textasciitilde{} mod\_4(theta\_0, theta\_1, theta\_2, x)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Parameters:}
\CommentTok{\#\textgreater{}         Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} theta\_0  3.63591    0.36528   9.954  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} theta\_1 {-}1.30639    0.15561  {-}8.395 3.65e{-}15 ***}
\CommentTok{\#\textgreater{} theta\_2  0.50528    0.09215   5.483 1.03e{-}07 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 0.2831 on 247 degrees of freedom}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of iterations to convergence: 9 }
\CommentTok{\#\textgreater{} Achieved convergence tolerance: 2.294e{-}07}
\end{Highlighting}
\end{Shaded}

\hypertarget{logistic-growth-model}{%
\subsection{Logistic Growth Model}\label{logistic-growth-model}}

A classic logistic growth model follows the equation:

\[
P = \frac{K}{1 + \exp(P_0 + r t)} + \epsilon
\]

where:

\begin{itemize}
\item
  \(P\) = population at time \(t\)
\item
  \(K\) = carrying capacity (maximum population)
\item
  \(r\) = population growth rate
\item
  \(P_0\) = initial population log-ratio
\end{itemize}

However, R's built-in \texttt{SSlogis} function uses a slightly different parameterization:

\[
P = \frac{asym}{1 + \exp\left(\frac{xmid - t}{scal}\right)}
\]

where:

\begin{itemize}
\item
  \(asym\) = carrying capacity (\(K\))
\item
  \(xmid\) = the \(x\)-value at the inflection point of the curve
\item
  \(scal\) = scaling parameter
\end{itemize}

This gives the parameter relationships:

\begin{itemize}
\item
  \(K = asym\)
\item
  \(r = -1 / scal\)
\item
  \(P_0 = -r \cdot xmid\)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Simulated time{-}series data}
\NormalTok{time }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{35}\NormalTok{)}
\NormalTok{population }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{2.8}\NormalTok{, }\FloatTok{4.2}\NormalTok{, }\FloatTok{3.5}\NormalTok{, }\FloatTok{6.3}\NormalTok{, }\FloatTok{15.7}\NormalTok{, }\FloatTok{21.3}\NormalTok{, }\FloatTok{23.7}\NormalTok{, }\FloatTok{25.1}\NormalTok{, }\FloatTok{25.8}\NormalTok{, }\FloatTok{25.9}\NormalTok{)}

\CommentTok{\# Plot data points}
\FunctionTok{plot}\NormalTok{(time, population, }\AttributeTok{las =} \DecValTok{1}\NormalTok{, }\AttributeTok{pch =} \DecValTok{16}\NormalTok{, }\AttributeTok{main =} \StringTok{"Logistic Growth Model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{06-nonlinear-regession_files/figure-latex/unnamed-chunk-40-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Fit the logistic growth model using programmed starting values}
\NormalTok{logisticModelSS }\OtherTok{\textless{}{-}} \FunctionTok{nls}\NormalTok{(population }\SpecialCharTok{\textasciitilde{}} \FunctionTok{SSlogis}\NormalTok{(time, Asym, xmid, scal))}

\CommentTok{\# Model summary}
\FunctionTok{summary}\NormalTok{(logisticModelSS)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Formula: population \textasciitilde{} SSlogis(time, Asym, xmid, scal)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Parameters:}
\CommentTok{\#\textgreater{}      Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} Asym  25.5029     0.3666   69.56 3.34e{-}11 ***}
\CommentTok{\#\textgreater{} xmid   8.7347     0.3007   29.05 1.48e{-}08 ***}
\CommentTok{\#\textgreater{} scal   3.6353     0.2186   16.63 6.96e{-}07 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 0.6528 on 7 degrees of freedom}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of iterations to convergence: 1 }
\CommentTok{\#\textgreater{} Achieved convergence tolerance: 1.908e{-}06}

\CommentTok{\# Extract parameter estimates}
\FunctionTok{coef}\NormalTok{(logisticModelSS)}
\CommentTok{\#\textgreater{}      Asym      xmid      scal }
\CommentTok{\#\textgreater{} 25.502890  8.734698  3.635333}
\end{Highlighting}
\end{Shaded}

To fit the model using an alternative parameterization (\(K, r, P_0\)), we convert the estimated coefficients:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Convert parameter estimates to alternative logistic model parameters}
\NormalTok{Ks }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{coef}\NormalTok{(logisticModelSS)[}\DecValTok{1}\NormalTok{])  }\CommentTok{\# Carrying capacity (K)}
\NormalTok{rs }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{/} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{coef}\NormalTok{(logisticModelSS)[}\DecValTok{3}\NormalTok{])  }\CommentTok{\# Growth rate (r)}
\NormalTok{Pos }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\NormalTok{rs }\SpecialCharTok{*} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{coef}\NormalTok{(logisticModelSS)[}\DecValTok{2}\NormalTok{])  }\CommentTok{\# P\_0}

\CommentTok{\# Fit the logistic model with the alternative parameterization}
\NormalTok{logisticModel }\OtherTok{\textless{}{-}} \FunctionTok{nls}\NormalTok{(}
\NormalTok{    population }\SpecialCharTok{\textasciitilde{}}\NormalTok{ K }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(Po }\SpecialCharTok{+}\NormalTok{ r }\SpecialCharTok{*}\NormalTok{ time)),}
    \AttributeTok{start =} \FunctionTok{list}\NormalTok{(}\AttributeTok{Po =}\NormalTok{ Pos, }\AttributeTok{r =}\NormalTok{ rs, }\AttributeTok{K =}\NormalTok{ Ks)}
\NormalTok{)}

\CommentTok{\# Model summary}
\FunctionTok{summary}\NormalTok{(logisticModel)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Formula: population \textasciitilde{} K/(1 + exp(Po + r * time))}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Parameters:}
\CommentTok{\#\textgreater{}    Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} Po  2.40272    0.12702   18.92 2.87e{-}07 ***}
\CommentTok{\#\textgreater{} r  {-}0.27508    0.01654  {-}16.63 6.96e{-}07 ***}
\CommentTok{\#\textgreater{} K  25.50289    0.36665   69.56 3.34e{-}11 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 0.6528 on 7 degrees of freedom}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of iterations to convergence: 0 }
\CommentTok{\#\textgreater{} Achieved convergence tolerance: 1.924e{-}06}
\end{Highlighting}
\end{Shaded}

Visualizing the Logistic Model Fit

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Plot original data}
\FunctionTok{plot}\NormalTok{(time,}
\NormalTok{     population,}
     \AttributeTok{las =} \DecValTok{1}\NormalTok{,}
     \AttributeTok{pch =} \DecValTok{16}\NormalTok{,}
     \AttributeTok{main =} \StringTok{"Logistic Growth Model Fit"}\NormalTok{)}

\CommentTok{\# Overlay the fitted logistic curve}
\FunctionTok{lines}\NormalTok{(time,}
      \FunctionTok{predict}\NormalTok{(logisticModel),}
      \AttributeTok{col =} \StringTok{"red"}\NormalTok{,}
      \AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{06-nonlinear-regession_files/figure-latex/unnamed-chunk-42-1} \end{center}

\hypertarget{nonlinear-plateau-model}{%
\subsection{Nonlinear Plateau Model}\label{nonlinear-plateau-model}}

This example is based on \citep{Schabenberger_2001} and demonstrates the use of a \textbf{plateau model} to estimate the relationship between soil nitrate (\(NO_3\)) concentration and relative yield percent (RYP) at two different depths (30 cm and 60 cm).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load data}
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{read.table}\NormalTok{(}\StringTok{"images/dat.txt"}\NormalTok{, }\AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{)}

\CommentTok{\# Plot NO3 concentration vs. relative yield percent, colored by depth}
\FunctionTok{library}\NormalTok{(ggplot2)}
\NormalTok{dat.plot }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(dat) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ no3, }\AttributeTok{y =}\NormalTok{ ryp, }\AttributeTok{color =} \FunctionTok{as.factor}\NormalTok{(depth))) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{color =} \StringTok{\textquotesingle{}Depth (cm)\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{xlab}\NormalTok{(}\StringTok{\textquotesingle{}Soil NO3 Concentration\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{ylab}\NormalTok{(}\StringTok{\textquotesingle{}Relative Yield Percent\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}

\CommentTok{\# Display plot}
\NormalTok{dat.plot}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{06-nonlinear-regession_files/figure-latex/unnamed-chunk-43-1} \end{center}

The \textbf{suggested nonlinear plateau model} is given by:

\[
E(Y_{ij}) = (\beta_{0j} + \beta_{1j}N_{ij})I_{N_{ij}\le \alpha_j} + (\beta_{0j} + \beta_{1j}\alpha_j)I_{N_{ij} > \alpha_j}
\]

where:

\begin{itemize}
\item
  \(N_{ij}\) represents the soil nitrate (\(NO_3\)) concentration for observation \(i\) at depth \(j\).
\item
  \(i\) indexes individual observations.
\item
  \(j = 1, 2\) corresponds to depths \textbf{30 cm} and \textbf{60 cm}.
\end{itemize}

This model assumes a \textbf{linear increase} up to a threshold (\(\alpha_j\)), beyond which the response \textbf{levels off (plateaus).}

Defining the Plateau Model as a Function

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Define the nonlinear plateau model function}
\NormalTok{nonlinModel }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(predictor, b0, b1, alpha) \{}
  \FunctionTok{ifelse}\NormalTok{(predictor }\SpecialCharTok{\textless{}=}\NormalTok{ alpha, }
\NormalTok{         b0 }\SpecialCharTok{+}\NormalTok{ b1 }\SpecialCharTok{*}\NormalTok{ predictor,  }\CommentTok{\# Linear growth below threshold}
\NormalTok{         b0 }\SpecialCharTok{+}\NormalTok{ b1 }\SpecialCharTok{*}\NormalTok{ alpha)      }\CommentTok{\# Plateau beyond threshold}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Creating a Self-Starting Function for \texttt{nls}

Since the model is \textbf{piecewise linear}, we can estimate starting values using:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  A \textbf{linear regression} on the \textbf{first half} of sorted predictor values to estimate \(b_0\) and \(b_1\).
\item
  The \textbf{last predictor value} used in the regression as the plateau threshold (\(\alpha\))
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Define initialization function for self{-}starting plateau model}
\NormalTok{nonlinModelInit }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(mCall, LHS, data) \{}
  \CommentTok{\# Sort data by increasing predictor value}
\NormalTok{  xy }\OtherTok{\textless{}{-}} \FunctionTok{sortedXyData}\NormalTok{(mCall[[}\StringTok{\textquotesingle{}predictor\textquotesingle{}}\NormalTok{]], LHS, data)}
\NormalTok{  n }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(xy)}
  
  \CommentTok{\# Fit a simple linear model using the first half of the sorted data}
\NormalTok{  lmFit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(xy[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(n }\SpecialCharTok{/} \DecValTok{2}\NormalTok{), }\StringTok{\textquotesingle{}y\textquotesingle{}}\NormalTok{] }\SpecialCharTok{\textasciitilde{}}\NormalTok{ xy[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(n }\SpecialCharTok{/} \DecValTok{2}\NormalTok{), }\StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{])}
  
  \CommentTok{\# Extract initial estimates}
\NormalTok{  b0 }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(lmFit)[}\DecValTok{1}\NormalTok{]  }\CommentTok{\# Intercept}
\NormalTok{  b1 }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(lmFit)[}\DecValTok{2}\NormalTok{]  }\CommentTok{\# Slope}
\NormalTok{  alpha }\OtherTok{\textless{}{-}}\NormalTok{ xy[(n }\SpecialCharTok{/} \DecValTok{2}\NormalTok{), }\StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{]  }\CommentTok{\# Last x{-}value in the fitted linear range}
  
  \CommentTok{\# Return initial parameter estimates}
\NormalTok{  value }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(b0, b1, alpha)}
  \FunctionTok{names}\NormalTok{(value) }\OtherTok{\textless{}{-}}\NormalTok{ mCall[}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}b0\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}b1\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}alpha\textquotesingle{}}\NormalTok{)]}
\NormalTok{  value}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Combining Model and Self-Start Function

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Define a self{-}starting nonlinear model for nls}
\NormalTok{SS\_nonlinModel }\OtherTok{\textless{}{-}} \FunctionTok{selfStart}\NormalTok{(nonlinModel,}
\NormalTok{                            nonlinModelInit,}
                            \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}b0\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}b1\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}alpha\textquotesingle{}}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

The \texttt{nls} function is used to estimate parameters separately for \textbf{each soil depth (30 cm and 60 cm).}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fit the model for depth = 30 cm}
\NormalTok{sep30\_nls }\OtherTok{\textless{}{-}} \FunctionTok{nls}\NormalTok{(ryp }\SpecialCharTok{\textasciitilde{}} \FunctionTok{SS\_nonlinModel}\NormalTok{(}\AttributeTok{predictor =}\NormalTok{ no3, b0, b1, alpha),}
                  \AttributeTok{data =}\NormalTok{ dat[dat}\SpecialCharTok{$}\NormalTok{depth }\SpecialCharTok{==} \DecValTok{30}\NormalTok{,])}

\CommentTok{\# Fit the model for depth = 60 cm}
\NormalTok{sep60\_nls }\OtherTok{\textless{}{-}} \FunctionTok{nls}\NormalTok{(ryp }\SpecialCharTok{\textasciitilde{}} \FunctionTok{SS\_nonlinModel}\NormalTok{(}\AttributeTok{predictor =}\NormalTok{ no3, b0, b1, alpha),}
                  \AttributeTok{data =}\NormalTok{ dat[dat}\SpecialCharTok{$}\NormalTok{depth }\SpecialCharTok{==} \DecValTok{60}\NormalTok{,])}
\end{Highlighting}
\end{Shaded}

We generate separate plots for \textbf{30 cm} and \textbf{60 cm} depths, showing both \textbf{confidence} and \textbf{prediction intervals.}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Set plotting layout}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}

\CommentTok{\# Plot model fit for 30 cm depth}
\FunctionTok{plotFit}\NormalTok{(}
\NormalTok{  sep30\_nls,}
  \AttributeTok{interval =} \StringTok{"both"}\NormalTok{,}
  \AttributeTok{pch =} \DecValTok{19}\NormalTok{,}
  \AttributeTok{shade =} \ConstantTok{TRUE}\NormalTok{,}
  \AttributeTok{col.conf =} \StringTok{"skyblue4"}\NormalTok{,}
  \AttributeTok{col.pred =} \StringTok{"lightskyblue2"}\NormalTok{,}
  \AttributeTok{data =}\NormalTok{ dat[dat}\SpecialCharTok{$}\NormalTok{depth }\SpecialCharTok{==} \DecValTok{30}\NormalTok{,],}
  \AttributeTok{main =} \StringTok{"Results at 30 cm Depth"}\NormalTok{,}
  \AttributeTok{ylab =} \StringTok{"Relative Yield Percent"}\NormalTok{,}
  \AttributeTok{xlab =} \StringTok{"Soil NO3 Concentration"}\NormalTok{,}
  \AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{120}\NormalTok{)}
\NormalTok{)}

\CommentTok{\# Plot model fit for 60 cm depth}
\FunctionTok{plotFit}\NormalTok{(}
\NormalTok{  sep60\_nls,}
  \AttributeTok{interval =} \StringTok{"both"}\NormalTok{,}
  \AttributeTok{pch =} \DecValTok{19}\NormalTok{,}
  \AttributeTok{shade =} \ConstantTok{TRUE}\NormalTok{,}
  \AttributeTok{col.conf =} \StringTok{"lightpink4"}\NormalTok{,}
  \AttributeTok{col.pred =} \StringTok{"lightpink2"}\NormalTok{,}
  \AttributeTok{data =}\NormalTok{ dat[dat}\SpecialCharTok{$}\NormalTok{depth }\SpecialCharTok{==} \DecValTok{60}\NormalTok{,],}
  \AttributeTok{main =} \StringTok{"Results at 60 cm Depth"}\NormalTok{,}
  \AttributeTok{ylab =} \StringTok{"Relative Yield Percent"}\NormalTok{,}
  \AttributeTok{xlab =} \StringTok{"Soil NO3 Concentration"}\NormalTok{,}
  \AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{120}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{06-nonlinear-regession_files/figure-latex/unnamed-chunk-48-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(sep30\_nls)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Formula: ryp \textasciitilde{} SS\_nonlinModel(predictor = no3, b0, b1, alpha)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Parameters:}
\CommentTok{\#\textgreater{}       Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} b0     15.1943     2.9781   5.102 6.89e{-}07 ***}
\CommentTok{\#\textgreater{} b1      3.5760     0.1853  19.297  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} alpha  23.1324     0.5098  45.373  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 8.258 on 237 degrees of freedom}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of iterations to convergence: 6 }
\CommentTok{\#\textgreater{} Achieved convergence tolerance: 3.608e{-}09}
\FunctionTok{summary}\NormalTok{(sep60\_nls)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Formula: ryp \textasciitilde{} SS\_nonlinModel(predictor = no3, b0, b1, alpha)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Parameters:}
\CommentTok{\#\textgreater{}       Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} b0      5.4519     2.9785    1.83   0.0684 .  }
\CommentTok{\#\textgreater{} b1      5.6820     0.2529   22.46   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} alpha  16.2863     0.2818   57.80   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 7.427 on 237 degrees of freedom}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of iterations to convergence: 5 }
\CommentTok{\#\textgreater{} Achieved convergence tolerance: 8.571e{-}09}
\end{Highlighting}
\end{Shaded}

\textbf{Modeling Soil Depths Together and Comparing Models}

Instead of fitting separate models for different soil depths, we first fit a \textbf{combined model} where all observations share a \textbf{common slope, intercept, and plateau}. We then test whether modeling the two depths separately provides a significantly better fit.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Fitting a Reduced (Combined) Model}
\end{enumerate}

The reduced model assumes that \textbf{all soil depths follow the same nonlinear relationship}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fit the combined model (common parameters across all depths)}
\NormalTok{red\_nls }\OtherTok{\textless{}{-}} \FunctionTok{nls}\NormalTok{(}
\NormalTok{  ryp }\SpecialCharTok{\textasciitilde{}} \FunctionTok{SS\_nonlinModel}\NormalTok{(}\AttributeTok{predictor =}\NormalTok{ no3, b0, b1, alpha), }
  \AttributeTok{data =}\NormalTok{ dat}
\NormalTok{)}

\CommentTok{\# Display model summary}
\FunctionTok{summary}\NormalTok{(red\_nls)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Formula: ryp \textasciitilde{} SS\_nonlinModel(predictor = no3, b0, b1, alpha)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Parameters:}
\CommentTok{\#\textgreater{}       Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} b0      8.7901     2.7688   3.175   0.0016 ** }
\CommentTok{\#\textgreater{} b1      4.8995     0.2207  22.203   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} alpha  18.0333     0.3242  55.630   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 9.13 on 477 degrees of freedom}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of iterations to convergence: 7 }
\CommentTok{\#\textgreater{} Achieved convergence tolerance: 7.126e{-}09}

\CommentTok{\# Visualizing the combined model fit}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\FunctionTok{plotFit}\NormalTok{(}
\NormalTok{  red\_nls,}
  \AttributeTok{interval =} \StringTok{"both"}\NormalTok{,}
  \AttributeTok{pch =} \DecValTok{19}\NormalTok{,}
  \AttributeTok{shade =} \ConstantTok{TRUE}\NormalTok{,}
  \AttributeTok{col.conf =} \StringTok{"lightblue4"}\NormalTok{,}
  \AttributeTok{col.pred =} \StringTok{"lightblue2"}\NormalTok{,}
  \AttributeTok{data =}\NormalTok{ dat,}
  \AttributeTok{main =} \StringTok{\textquotesingle{}Results for Combined Model\textquotesingle{}}\NormalTok{,}
  \AttributeTok{ylab =} \StringTok{\textquotesingle{}Relative Yield Percent\textquotesingle{}}\NormalTok{,}
  \AttributeTok{xlab =} \StringTok{\textquotesingle{}Soil NO3 Concentration\textquotesingle{}}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{06-nonlinear-regession_files/figure-latex/reduce-model-1} \end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Examining Residuals for the Combined Model}
\end{enumerate}

Checking residuals helps diagnose potential \textbf{lack of fit}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(nlstools)}

\CommentTok{\# Residual diagnostics using nlstools}
\NormalTok{resid }\OtherTok{\textless{}{-}} \FunctionTok{nlsResiduals}\NormalTok{(red\_nls)}

\CommentTok{\# Plot residuals}
\FunctionTok{plot}\NormalTok{(resid)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{06-nonlinear-regession_files/figure-latex/unnamed-chunk-50-1} \end{center}

If there is a \textbf{pattern} in the residuals (e.g., systematic deviations based on soil depth), this suggests that a \textbf{separate model for each depth} may be necessary.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Testing Whether Depths Require Separate Models}
\end{enumerate}

To formally test whether soil depth significantly affects the model parameters, we introduce a \textbf{parameterization where depth-specific parameters are increments from a baseline model} (30 cm depth):

\[
\begin{aligned}
\beta_{02} &= \beta_{01} + d_0 \\
\beta_{12} &= \beta_{11} + d_1 \\
\alpha_{2} &= \alpha_{1} + d_a
\end{aligned}
\]

where:

\begin{itemize}
\item
  \(\beta_{01}, \beta_{11}, \alpha_1\) are parameters for \textbf{30 cm depth}.
\item
  \(d_0, d_1, d_a\) represent \textbf{depth-specific differences} for \textbf{60 cm depth}.
\item
  If \(d_0, d_1, d_a\) are significantly \textbf{different from 0}, the \textbf{two depths should be modeled separately}.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  \textbf{Defining the Full (Depth-Specific) Model}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nonlinModelF }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(predictor, soildep, b01, b11, a1, d0, d1, da) \{}
  
  \CommentTok{\# Define parameters for 60 cm depth as increments from 30 cm parameters}
\NormalTok{  b02 }\OtherTok{\textless{}{-}}\NormalTok{ b01 }\SpecialCharTok{+}\NormalTok{ d0}
\NormalTok{  b12 }\OtherTok{\textless{}{-}}\NormalTok{ b11 }\SpecialCharTok{+}\NormalTok{ d1}
\NormalTok{  a2 }\OtherTok{\textless{}{-}}\NormalTok{ a1 }\SpecialCharTok{+}\NormalTok{ da}
  
  \CommentTok{\# Compute model output for 30 cm depth}
\NormalTok{  y1 }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(}
\NormalTok{    predictor }\SpecialCharTok{\textless{}=}\NormalTok{ a1, }
\NormalTok{    b01 }\SpecialCharTok{+}\NormalTok{ b11 }\SpecialCharTok{*}\NormalTok{ predictor, }
\NormalTok{    b01 }\SpecialCharTok{+}\NormalTok{ b11 }\SpecialCharTok{*}\NormalTok{ a1}
\NormalTok{  )}
  
  \CommentTok{\# Compute model output for 60 cm depth}
\NormalTok{  y2 }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(}
\NormalTok{    predictor }\SpecialCharTok{\textless{}=}\NormalTok{ a2, }
\NormalTok{    b02 }\SpecialCharTok{+}\NormalTok{ b12 }\SpecialCharTok{*}\NormalTok{ predictor, }
\NormalTok{    b02 }\SpecialCharTok{+}\NormalTok{ b12 }\SpecialCharTok{*}\NormalTok{ a2}
\NormalTok{  )}
  
  \CommentTok{\# Assign correct model output based on depth}
\NormalTok{  y }\OtherTok{\textless{}{-}}\NormalTok{ y1 }\SpecialCharTok{*}\NormalTok{ (soildep }\SpecialCharTok{==} \DecValTok{30}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ y2 }\SpecialCharTok{*}\NormalTok{ (soildep }\SpecialCharTok{==} \DecValTok{60}\NormalTok{)}
  
  \FunctionTok{return}\NormalTok{(y)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  \textbf{Fitting the Full (Depth-Specific) Model}
\end{enumerate}

The starting values are taken from the \textbf{separately fitted models} for each depth.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Soil\_full }\OtherTok{\textless{}{-}} \FunctionTok{nls}\NormalTok{(}
\NormalTok{  ryp }\SpecialCharTok{\textasciitilde{}} \FunctionTok{nonlinModelF}\NormalTok{(}
    \AttributeTok{predictor =}\NormalTok{ no3,}
    \AttributeTok{soildep =}\NormalTok{ depth,}
\NormalTok{    b01,}
\NormalTok{    b11,}
\NormalTok{    a1,}
\NormalTok{    d0,}
\NormalTok{    d1,}
\NormalTok{    da}
\NormalTok{  ),}
  \AttributeTok{data =}\NormalTok{ dat,}
  \AttributeTok{start =} \FunctionTok{list}\NormalTok{(}
    \AttributeTok{b01 =} \FloatTok{15.2}\NormalTok{,   }\CommentTok{\# Intercept for 30 cm depth}
    \AttributeTok{b11 =} \FloatTok{3.58}\NormalTok{,   }\CommentTok{\# Slope for 30 cm depth}
    \AttributeTok{a1 =} \FloatTok{23.13}\NormalTok{,   }\CommentTok{\# Plateau cutoff for 30 cm depth}
    \AttributeTok{d0 =} \SpecialCharTok{{-}}\FloatTok{9.74}\NormalTok{,   }\CommentTok{\# Intercept difference (60 cm {-} 30 cm)}
    \AttributeTok{d1 =} \FloatTok{2.11}\NormalTok{,    }\CommentTok{\# Slope difference (60 cm {-} 30 cm)}
    \AttributeTok{da =} \SpecialCharTok{{-}}\FloatTok{6.85}    \CommentTok{\# Plateau cutoff difference (60 cm {-} 30 cm)}
\NormalTok{  )}
\NormalTok{)}

\CommentTok{\# Display model summary}
\FunctionTok{summary}\NormalTok{(Soil\_full)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Formula: ryp \textasciitilde{} nonlinModelF(predictor = no3, soildep = depth, b01, b11, }
\CommentTok{\#\textgreater{}     a1, d0, d1, da)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Parameters:}
\CommentTok{\#\textgreater{}     Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} b01  15.1943     2.8322   5.365 1.27e{-}07 ***}
\CommentTok{\#\textgreater{} b11   3.5760     0.1762  20.291  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} a1   23.1324     0.4848  47.711  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} d0   {-}9.7424     4.2357  {-}2.300   0.0219 *  }
\CommentTok{\#\textgreater{} d1    2.1060     0.3203   6.575 1.29e{-}10 ***}
\CommentTok{\#\textgreater{} da   {-}6.8461     0.5691 {-}12.030  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 7.854 on 474 degrees of freedom}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of iterations to convergence: 1 }
\CommentTok{\#\textgreater{} Achieved convergence tolerance: 3.742e{-}06}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  \textbf{Model Comparison: Does Depth Matter?}
\end{enumerate}

\begin{itemize}
\item
  If \(d_0, d_1, d_a\) \textbf{are significantly different from 0}, the depths should be modeled separately.
\item
  The \textbf{p-values} for these parameters indicate whether depth-specific modeling is necessary.
\end{itemize}

\hypertarget{generalized-linear-models}{%
\chapter{Generalized Linear Models}\label{generalized-linear-models}}

Generalized Linear Models (GLMs) extend the traditional linear regression framework to accommodate response variables that do not necessarily follow a normal distribution. They provide a \textbf{flexible approach} to modeling relationships between a set of predictors and various types of dependent variables.

While \protect\hyperlink{ordinary-least-squares}{Ordinary Least Squares} regression assumes that the response variable is continuous and normally distributed, GLMs allow for response variables that follow distributions from the \textbf{exponential family}, such as \textbf{binomial, Poisson, and gamma distributions}. This flexibility makes them particularly useful in a wide range of business and research applications.

A \textbf{Generalized Linear Model} (GLM) consists of three key components:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{A random component}: The response variable \(Y_i\) follows a distribution from the exponential family (e.g., binomial, Poisson, gamma).
\item
  \textbf{A systematic component}: A linear predictor \(\eta_i = \mathbf{x'_i} \beta\), where \(\mathbf{x'_i}\) is a vector of observed covariates (predictor variables) and \(\beta\) is a vector of parameters to be estimated.
\item
  \textbf{A link function}: A function \(g(\cdot)\) that relates the expected value of the response variable, \(\mu_i = E(Y_i)\), to the linear predictor (i.e., \(\eta_i = g(\mu_i)\)).
\end{enumerate}

Although the relationship between the predictors and the outcome \emph{may appear nonlinear} on the original outcome scale (due to the link function), a GLM is still considered ``linear'' in the statistical sense because it remains linear in the parameters \(\beta\). Consequently, GLMs are \textbf{not} generally classified as nonlinear regression models. They ``generalize'' the traditional linear model by allowing for a broader range of response variable distributions and link functions, but retain linearity in their parameters.

The choice of \textbf{distribution} and \textbf{link function} depends on the nature of the response variable. In the following sections, we will explore several important GLM variants:

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{sec-logistic-regression}{Logistic Regression}: Used for binary response variables (e.g., customer churn, loan defaults).
\item
  \protect\hyperlink{sec-probit-regression}{Probit Regression}: Similar to logistic regression but assumes a normal distribution for the underlying probability.
\item
  \protect\hyperlink{sec-poisson-regression}{Poisson Regression}: Used for modeling count data (e.g., number of purchases, call center inquiries).
\item
  \protect\hyperlink{sec-negative-binomial-regression}{Negative Binomial Regression}: An extension of Poisson regression that accounts for overdispersion in count data.
\item
  \protect\hyperlink{sec-quasi-poisson-regression}{Quasi-Poisson Regression}: A variation of Poisson regression that adjusts for overdispersion by allowing the variance to be a linear function of the mean.
\item
  \protect\hyperlink{sec-multinomial-logistic-regression}{Multinomial Logistic Regression}: A generalization of logistic regression for categorical response variables with more than two outcomes.
\item
  \protect\hyperlink{sec-generalization-of-generalized-linear-models}{Generalization of Generalized Linear Model}: A flexible generalization of ordinary linear regression that allows for response variables with different distributions (e.g., normal, binomial, Poisson).
\end{itemize}

\hypertarget{sec-logistic-regression}{%
\section{Logistic Regression}\label{sec-logistic-regression}}

Logistic regression is a widely used \protect\hyperlink{generalized-linear-models}{Generalized Linear Model} designed for modeling binary response variables. It is particularly useful in applications such as credit scoring, medical diagnosis, and customer churn prediction.

\hypertarget{logistic-model}{%
\subsection{Logistic Model}\label{logistic-model}}

Given a set of predictor variables \(\mathbf{x}_i\), the probability of a positive outcome (e.g., success, event occurring) is modeled as:

\[
p_i = f(\mathbf{x}_i ; \beta) = \frac{\exp(\mathbf{x_i'\beta})}{1 + \exp(\mathbf{x_i'\beta})}
\]

where:

\begin{itemize}
\tightlist
\item
  \(p_i = \mathbb{E}[Y_i]\) is the probability of success for observation \(i\).
\item
  \(\mathbf{x_i}\) is the vector of predictor variables.
\item
  \(\beta\) is the vector of model coefficients.
\end{itemize}

\hypertarget{sec-logit-transformation}{%
\subsubsection{Logit Transformation}\label{sec-logit-transformation}}

The logistic function can be rewritten in terms of the \textbf{log-odds}, also known as the \textbf{logit function}:

\[
\text{logit}(p_i) = \log \left(\frac{p_i}{1 - p_i} \right) = \mathbf{x_i'\beta}
\]

where:

\begin{itemize}
\tightlist
\item
  \(\frac{p_i}{1 - p_i}\) represents the \textbf{odds} of success (the ratio of the probability of success to the probability of failure).
\item
  The logit function \textbf{ensures linearity} in the parameters, which aligns with the GLM framework.
\end{itemize}

Thus, logistic regression belongs to the family of \textbf{Generalized Linear Models} because \textbf{a function of the mean response (logit) is linear in the predictors}.

\hypertarget{sec-likelihood-function-logistic}{%
\subsection{Likelihood Function}\label{sec-likelihood-function-logistic}}

Since \(Y_i\) follows a \textbf{Bernoulli distribution} with probability \(p_i\), the likelihood function for \(n\) independent observations is:

\[
L(p_i) = \prod_{i=1}^{n} p_i^{Y_i} (1 - p_i)^{1 - Y_i}
\]

By substituting the logistic function for \(p_i\):

\[
p_i = \frac{\exp(\mathbf{x'_i \beta})}{1+\exp(\mathbf{x'_i \beta})}, \quad 1 - p_i = \frac{1}{1+\exp(\mathbf{x'_i \beta})}
\]

we obtain:

\[
L(\beta) = \prod_{i=1}^{n} \left( \frac{\exp(\mathbf{x'_i \beta})}{1+\exp(\mathbf{x'_i \beta})} \right)^{Y_i} \left( \frac{1}{1+\exp(\mathbf{x'_i \beta})} \right)^{1 - Y_i}
\]

Taking the natural logarithm of the likelihood function gives the \textbf{log-likelihood function}:

\[
Q(\beta) = \log L(\beta) = \sum_{i=1}^n Y_i \mathbf{x'_i \beta} - \sum_{i=1}^n \log(1 + \exp(\mathbf{x'_i \beta}))
\]

Since this function is \textbf{concave}, we can maximize it numerically using \textbf{iterative optimization techniques}, such as:

\begin{itemize}
\tightlist
\item
  \textbf{Newton-Raphson Method}
\item
  \textbf{Fisher Scoring Algorithm}
\end{itemize}

These methods allow us to obtain the \protect\hyperlink{maximum-likelihood-estimator}{Maximum Likelihood} Estimates of the parameters, \(\hat{\beta}\).

Under standard regularity conditions, the \textbf{MLEs of logistic regression parameters are asymptotically normal}:

\[
\hat{\beta} \dot{\sim} AN(\beta, [\mathbf{I}(\beta)]^{-1})
\]

where:

\begin{itemize}
\tightlist
\item
  \(\mathbf{I}(\beta)\) is the \protect\hyperlink{fisher-information-matrix}{Fisher Information Matrix}, which determines the variance-covariance structure of \(\hat{\beta}\).
\end{itemize}

\hypertarget{fisher-information-matrix}{%
\subsection{Fisher Information Matrix}\label{fisher-information-matrix}}

The \textbf{Fisher Information Matrix} quantifies the amount of information that an observable random variable carries about the \textbf{unknown parameter} \(\beta\). It is crucial in estimating the \textbf{variance-covariance matrix} of the estimated coefficients in logistic regression.

Mathematically, the Fisher Information Matrix is defined as:

\[
\mathbf{I}(\beta) = E\left[ \frac{\partial \log L(\beta)}{\partial \beta} \frac{\partial \log L(\beta)}{\partial \beta'} \right]
\]

which expands to:

\[
\mathbf{I}(\beta) = E\left[ \left(\frac{\partial \log L(\beta)}{\partial \beta_i} \frac{\partial \log L(\beta)}{\partial \beta_j} \right)_{ij} \right]
\]

Under \textbf{regularity conditions}, the Fisher Information Matrix is equivalent to the \textbf{negative expected Hessian matrix}:

\[
\mathbf{I}(\beta) = -E\left[ \frac{\partial^2 \log L(\beta)}{\partial \beta \partial \beta'} \right]
\]

which further expands to:

\[
\mathbf{I}(\beta) = -E \left[ \left( \frac{\partial^2 \log L(\beta)}{\partial \beta_i \partial \beta_j} \right)_{ij} \right]
\]

This representation is particularly useful because it allows us to compute the Fisher Information Matrix directly from the Hessian of the log-likelihood function.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Example: Fisher Information Matrix in Logistic Regression

Consider a \textbf{simple logistic regression model} with one predictor:

\[
x_i' \beta = \beta_0 + \beta_1 x_i
\]

From the log-\protect\hyperlink{sec-likelihood-function-logistic}{likelihood function}, the second-order partial derivatives are:

\[
\begin{aligned} 
- \frac{\partial^2 \ln(L(\beta))}{\partial \beta^2_0} &= \sum_{i=1}^n \frac{\exp(x'_i \beta)}{1 + \exp(x'_i \beta)} - \left[\frac{\exp(x_i' \beta)}{1+ \exp(x'_i \beta)}\right]^2 & \text{Intercept} \\
&= \sum_{i=1}^n p_i (1-p_i)  \\
- \frac{\partial^2 \ln(L(\beta))}{\partial \beta^2_1} &= \sum_{i=1}^n \frac{x_i^2\exp(x'_i \beta)}{1 + \exp(x'_i \beta)} - \left[\frac{x_i\exp(x_i' \beta)}{1+ \exp(x'_i \beta)}\right]^2 & \text{Slope}\\
&= \sum_{i=1}^n x_i^2p_i (1-p_i)  \\
- \frac{\partial^2 \ln(L(\beta))}{\partial \beta_0 \partial \beta_1} &= \sum_{i=1}^n \frac{x_i\exp(x'_i \beta)}{1 + \exp(x'_i \beta)} - x_i\left[\frac{\exp(x_i' \beta)}{1+ \exp(x'_i \beta)}\right]^2 & \text{Cross-derivative}\\
&= \sum_{i=1}^n x_ip_i (1-p_i) 
\end{aligned}
\]

Combining these elements, the \textbf{Fisher Information Matrix} for the logistic regression model is:

\[
\mathbf{I} (\beta) = 
\begin{bmatrix}
\sum_{i=1}^{n} p_i(1 - p_i) & \sum_{i=1}^{n} x_i p_i(1 - p_i) \\
\sum_{i=1}^{n} x_i p_i(1 - p_i) & \sum_{i=1}^{n} x_i^2 p_i(1 - p_i)
\end{bmatrix}
\]

where:

\begin{itemize}
\tightlist
\item
  \(p_i = \frac{\exp(x_i' \beta)}{1+\exp(x_i' \beta)}\) represents the predicted probability.
\item
  \(p_i (1 - p_i)\) is the \textbf{variance of the Bernoulli response variable}.
\item
  The diagonal elements represent the variances of the estimated coefficients.
\item
  The off-diagonal elements represent the covariances between \(\beta_0\) and \(\beta_1\).
\end{itemize}

The inverse of the Fisher Information Matrix provides the \textbf{variance-covariance matrix} of the estimated coefficients:

\[
\mathbf{Var}(\hat{\beta}) = \mathbf{I}(\hat{\beta})^{-1}
\]

This matrix is essential for:

\begin{itemize}
\tightlist
\item
  \textbf{Estimating standard errors} of the logistic regression coefficients.
\item
  \textbf{Constructing confidence intervals} for \(\beta\).
\item
  \textbf{Performing hypothesis tests} (e.g., Wald Test).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load necessary library}
\FunctionTok{library}\NormalTok{(stats)}

\CommentTok{\# Simulated dataset}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{100}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(n, }\DecValTok{1}\NormalTok{, }\AttributeTok{prob =} \FunctionTok{plogis}\NormalTok{(}\FloatTok{0.5} \SpecialCharTok{+} \FloatTok{1.2} \SpecialCharTok{*}\NormalTok{ x))}

\CommentTok{\# Fit logistic regression model}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{family =}\NormalTok{ binomial)}

\CommentTok{\# Extract the Fisher Information Matrix (Negative Hessian)}
\NormalTok{fisher\_info }\OtherTok{\textless{}{-}} \FunctionTok{summary}\NormalTok{(model)}\SpecialCharTok{$}\NormalTok{cov.unscaled}

\CommentTok{\# Display the Fisher Information Matrix}
\FunctionTok{print}\NormalTok{(fisher\_info)}
\CommentTok{\#\textgreater{}             (Intercept)          x}
\CommentTok{\#\textgreater{} (Intercept)  0.05718171 0.01564322}
\CommentTok{\#\textgreater{} x            0.01564322 0.10302992}
\end{Highlighting}
\end{Shaded}

\hypertarget{inference-in-logistic-regression}{%
\subsection{Inference in Logistic Regression}\label{inference-in-logistic-regression}}

Once we estimate the model parameters \(\hat{\beta}\) using \protect\hyperlink{maximum-likelihood-estimator}{Maximum Likelihood} Estimation, we can conduct inference to assess the significance of predictors, construct confidence intervals, and perform hypothesis testing. The two most common inference approaches in logistic regression are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \protect\hyperlink{sec-likelihood-ratio-test-logistic}{Likelihood Ratio Test}
\item
  \protect\hyperlink{sec-wald-test-logistic}{Wald Statistics}
\end{enumerate}

These tests rely on the \textbf{asymptotic normality} of MLEs and the properties of the \protect\hyperlink{fisher-information-matrix}{Fisher Information Matrix}.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{sec-likelihood-ratio-test-logistic}{%
\subsubsection{Likelihood Ratio Test}\label{sec-likelihood-ratio-test-logistic}}

The \textbf{Likelihood Ratio Test} compares two models:

\begin{itemize}
\tightlist
\item
  \textbf{Restricted Model}: A simpler model where some parameters are constrained to specific values.
\item
  \textbf{Unrestricted Model}: The full model without constraints.
\end{itemize}

To test a hypothesis about a subset of parameters \(\beta_1\), we leave \(\beta_2\) (nuisance parameters) unspecified.

Hypothesis Setup:

\[
H_0: \beta_1 = \beta_{1,0}
\]

where \(\beta_{1,0}\) is a specified value (often zero). Let:

\begin{itemize}
\tightlist
\item
  \(\hat{\beta}_{2,0}\) be the \textbf{MLE of} \(\beta_2\) under the constraint \(\beta_1 = \beta_{1,0}\).
\item
  \(\hat{\beta}_1, \hat{\beta}_2\) be the \textbf{MLEs under the full model}.
\end{itemize}

The \textbf{likelihood ratio test statistic} is:

\[
-2\log\Lambda = -2[\log L(\beta_{1,0}, \hat{\beta}_{2,0}) - \log L(\hat{\beta}_1, \hat{\beta}_2)]
\]

where:

\begin{itemize}
\tightlist
\item
  The first term is the \textbf{log-likelihood of the restricted model}.
\item
  The second term is the \textbf{log-likelihood of the unrestricted model}.
\end{itemize}

Under the null hypothesis:

\[
-2 \log \Lambda \sim \chi^2_{\upsilon}
\]

where \(\upsilon\) is the number of restricted parameters. We \textbf{reject} \(H_0\) if:

\[
-2\log \Lambda > \chi^2_{\upsilon,1-\alpha}
\]

\textbf{Interpretation}: If the likelihood ratio test statistic is large, this suggests that the restricted model (under \(H_0\)) fits significantly worse than the full model, leading us to reject the null hypothesis.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{sec-wald-test-logistic}{%
\subsubsection{Wald Test}\label{sec-wald-test-logistic}}

The \textbf{Wald test} is based on the asymptotic normality of MLEs:

\[
\hat{\beta} \sim AN (\beta, [\mathbf{I}(\beta)]^{-1})
\]

We test:

\[
H_0: \mathbf{L} \hat{\beta} = 0
\]

where \(\mathbf{L}\) is a \(q \times p\) matrix with \(q\) linearly independent rows (often used to test multiple coefficients simultaneously). The \textbf{Wald test statistic} is:

\[
W = (\mathbf{L\hat{\beta}})'(\mathbf{L[I(\hat{\beta})]^{-1}L'})^{-1}(\mathbf{L\hat{\beta}})
\]

Under \(H_0\):

\[
W \sim \chi^2_q
\]

\textbf{Interpretation}: If \(W\) is large, the null hypothesis is rejected, suggesting that at least one of the tested coefficients is significantly different from zero.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Comparing Likelihood Ratio and Wald Tests}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.3521}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.6479}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Test
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Best Used When\ldots{}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\protect\hyperlink{sec-likelihood-ratio-test-logistic}{Likelihood Ratio Test} & More accurate in small samples, providing better control of error rates. Recommended when sample sizes are small. \\
\protect\hyperlink{sec-wald-test-logistic}{Wald Test} & Easier to compute but may be inaccurate in small samples. Recommended when computational efficiency is a priority. \\
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load necessary library}
\FunctionTok{library}\NormalTok{(stats)}

\CommentTok{\# Simulate some binary outcome data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{100}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(n, }\DecValTok{1}\NormalTok{, }\AttributeTok{prob =} \FunctionTok{plogis}\NormalTok{(}\FloatTok{0.5} \SpecialCharTok{+} \FloatTok{1.2} \SpecialCharTok{*}\NormalTok{ x))}

\CommentTok{\# Fit logistic regression model}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{family =}\NormalTok{ binomial)}

\CommentTok{\# Display model summary (includes Wald tests)}
\FunctionTok{summary}\NormalTok{(model)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} glm(formula = y \textasciitilde{} x, family = binomial)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Deviance Residuals: }
\CommentTok{\#\textgreater{}     Min       1Q   Median       3Q      Max  }
\CommentTok{\#\textgreater{} {-}2.6226  {-}0.9385   0.5287   0.8333   1.4656  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}             Estimate Std. Error z value Pr(\textgreater{}|z|)    }
\CommentTok{\#\textgreater{} (Intercept)   0.7223     0.2391   3.020 0.002524 ** }
\CommentTok{\#\textgreater{} x             1.2271     0.3210   3.823 0.000132 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} (Dispersion parameter for binomial family taken to be 1)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Null deviance: 128.21  on 99  degrees of freedom}
\CommentTok{\#\textgreater{} Residual deviance: 108.29  on 98  degrees of freedom}
\CommentTok{\#\textgreater{} AIC: 112.29}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Fisher Scoring iterations: 4}

\CommentTok{\# Perform likelihood ratio test using anova()}
\FunctionTok{anova}\NormalTok{(model, }\AttributeTok{test=}\StringTok{"Chisq"}\NormalTok{)}
\CommentTok{\#\textgreater{} Analysis of Deviance Table}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Model: binomial, link: logit}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Response: y}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Terms added sequentially (first to last)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}      Df Deviance Resid. Df Resid. Dev  Pr(\textgreater{}Chi)    }
\CommentTok{\#\textgreater{} NULL                    99     128.21              }
\CommentTok{\#\textgreater{} x     1   19.913        98     108.29 8.105e{-}06 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

\hypertarget{confidence-intervals-for-coefficients}{%
\subsubsection{Confidence Intervals for Coefficients}\label{confidence-intervals-for-coefficients}}

A \textbf{95\% confidence interval} for a logistic regression coefficient \(\beta_i\) is given by:

\[
\hat{\beta}_i \pm 1.96 \hat{s}_{ii}
\]

where:

\begin{itemize}
\tightlist
\item
  \(\hat{\beta}_i\) is the estimated coefficient.
\item
  \(\hat{s}_{ii}\) is the standard error (square root of the diagonal element of \(\mathbf{[I(\hat{\beta})]}^{-1}\)).
\end{itemize}

This confidence interval provides a \textbf{range of plausible values} for \(\beta_i\). If the interval does not include \textbf{zero}, we conclude that \(\beta_i\) is statistically significant.

\begin{itemize}
\tightlist
\item
  \textbf{For large sample sizes}, the \protect\hyperlink{sec-likelihood-ratio-test-logistic}{Likelihood Ratio Test} and \protect\hyperlink{sec-wald-test-logistic}{Wald Test} yield similar results.
\item
  \textbf{For small sample sizes}, the \protect\hyperlink{sec-likelihood-ratio-test-logistic}{Likelihood Ratio Test} is preferred because the \protect\hyperlink{sec-wald-test-logistic}{Wald test} can be less reliable.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{interpretation-of-logistic-regression-coefficients}{%
\subsubsection{Interpretation of Logistic Regression Coefficients}\label{interpretation-of-logistic-regression-coefficients}}

For a \textbf{single predictor variable}, the logistic regression model is:

\[
\text{logit}(\hat{p}_i) = \log\left(\frac{\hat{p}_i}{1 - \hat{p}_i} \right) = \hat{\beta}_0 + \hat{\beta}_1 x_i
\]

where:

\begin{itemize}
\tightlist
\item
  \(\hat{p}_i\) is the predicted probability of success at \(x_i\).
\item
  \(\hat{\beta}_1\) represents the \textbf{log odds change} for a one-unit increase in \(x\).
\end{itemize}

\textbf{Interpreting} \(\beta_1\) \textbf{in Terms of Odds}

When the predictor variable increases by \textbf{one unit}, the logit of the probability changes by \(\hat{\beta}_1\):

\[
\text{logit}(\hat{p}_{x_i +1}) = \hat{\beta}_0 + \hat{\beta}_1 (x_i + 1) = \text{logit}(\hat{p}_{x_i}) + \hat{\beta}_1
\]

Thus, the difference in log odds is:

\[
\begin{aligned}
\text{logit}(\hat{p}_{x_i +1}) - \text{logit}(\hat{p}_{x_i}) 
&= \log ( \text{odds}(\hat{p}_{x_i + 1})) - \log (\text{odds}(\hat{p}_{x_i}) )\\
&= \log\left( \frac{\text{odds}(\hat{p}_{x_i + 1})}{\text{odds}(\hat{p}_{x_i})} \right) \\
&= \hat{\beta}_1
\end{aligned}
\]

Exponentiating both sides:

\[
\exp(\hat{\beta}_1) = \frac{\text{odds}(\hat{p}_{x_i + 1})}{\text{odds}(\hat{p}_{x_i})}
\]

This quantity, \(\exp(\hat{\beta}_1)\), is the \textbf{odds ratio}, which quantifies the effect of a one-unit increase in \(x\) on the odds of success.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Generalization: Odds Ratio for Any Change in} \(x\)

For a difference of \(c\) units in the predictor \(x\), the estimated odds ratio is:

\[
\exp(c\hat{\beta}_1)
\]

For multiple predictors, \(\exp(\hat{\beta}_k)\) represents the odds ratio for \(x_k\), holding all other variables constant.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{inference-on-the-mean-response}{%
\subsubsection{Inference on the Mean Response}\label{inference-on-the-mean-response}}

For a given set of predictor values \(x_h = (1, x_{h1}, ..., x_{h,p-1})'\), the estimated \textbf{mean response} (probability of success) is:

\[
\hat{p}_h = \frac{\exp(\mathbf{x'_h \hat{\beta}})}{1 + \exp(\mathbf{x'_h \hat{\beta}})}
\]

The \textbf{variance of the estimated probability} is:

\[
s^2(\hat{p}_h) = \mathbf{x'_h[I(\hat{\beta})]^{-1}x_h}
\]

where:

\begin{itemize}
\tightlist
\item
  \(\mathbf{I}(\hat{\beta})^{-1}\) is the \textbf{variance-covariance matrix} of \(\hat{\beta}\).
\item
  \(s^2(\hat{p}_h)\) provides an estimate of \textbf{uncertainty} in \(\hat{p}_h\).
\end{itemize}

In many applications, logistic regression is used for \textbf{classification}, where we predict whether an observation belongs to \textbf{category 0 or 1}. A commonly used decision rule is:

\begin{itemize}
\tightlist
\item
  Assign \(y = 1\) if \(\hat{p}_h \geq \tau\)
\item
  Assign \(y = 0\) if \(\hat{p}_h < \tau\)
\end{itemize}

where \(\tau\) is a chosen cutoff threshold (typically \(\tau = 0.5\)).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load necessary library}
\FunctionTok{library}\NormalTok{(stats)}

\CommentTok{\# Simulated dataset}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{100}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(n, }\DecValTok{1}\NormalTok{, }\AttributeTok{prob =} \FunctionTok{plogis}\NormalTok{(}\FloatTok{0.5} \SpecialCharTok{+} \FloatTok{1.2} \SpecialCharTok{*}\NormalTok{ x))}

\CommentTok{\# Fit logistic regression model}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{family =}\NormalTok{ binomial)}

\CommentTok{\# Display model summary}
\FunctionTok{summary}\NormalTok{(model)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} glm(formula = y \textasciitilde{} x, family = binomial)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Deviance Residuals: }
\CommentTok{\#\textgreater{}     Min       1Q   Median       3Q      Max  }
\CommentTok{\#\textgreater{} {-}2.6226  {-}0.9385   0.5287   0.8333   1.4656  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}             Estimate Std. Error z value Pr(\textgreater{}|z|)    }
\CommentTok{\#\textgreater{} (Intercept)   0.7223     0.2391   3.020 0.002524 ** }
\CommentTok{\#\textgreater{} x             1.2271     0.3210   3.823 0.000132 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} (Dispersion parameter for binomial family taken to be 1)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Null deviance: 128.21  on 99  degrees of freedom}
\CommentTok{\#\textgreater{} Residual deviance: 108.29  on 98  degrees of freedom}
\CommentTok{\#\textgreater{} AIC: 112.29}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Fisher Scoring iterations: 4}

\CommentTok{\# Extract coefficients and standard errors}
\NormalTok{coef\_estimates }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(}\FunctionTok{summary}\NormalTok{(model))}
\NormalTok{beta\_hat }\OtherTok{\textless{}{-}}\NormalTok{ coef\_estimates[, }\DecValTok{1}\NormalTok{]   }\CommentTok{\# Estimated coefficients}
\NormalTok{se\_beta  }\OtherTok{\textless{}{-}}\NormalTok{ coef\_estimates[, }\DecValTok{2}\NormalTok{]   }\CommentTok{\# Standard errors}

\CommentTok{\# Compute 95\% confidence intervals for coefficients}
\NormalTok{conf\_intervals }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(}
\NormalTok{  beta\_hat }\SpecialCharTok{{-}} \FloatTok{1.96} \SpecialCharTok{*}\NormalTok{ se\_beta, }
\NormalTok{  beta\_hat }\SpecialCharTok{+} \FloatTok{1.96} \SpecialCharTok{*}\NormalTok{ se\_beta}
\NormalTok{)}

\CommentTok{\# Compute Odds Ratios}
\NormalTok{odds\_ratios }\OtherTok{\textless{}{-}} \FunctionTok{exp}\NormalTok{(beta\_hat)}

\CommentTok{\# Display results}
\FunctionTok{print}\NormalTok{(}\StringTok{"Confidence Intervals for Coefficients:"}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] "Confidence Intervals for Coefficients:"}
\FunctionTok{print}\NormalTok{(conf\_intervals)}
\CommentTok{\#\textgreater{}                  [,1]     [,2]}
\CommentTok{\#\textgreater{} (Intercept) 0.2535704 1.190948}
\CommentTok{\#\textgreater{} x           0.5979658 1.856218}

\FunctionTok{print}\NormalTok{(}\StringTok{"Odds Ratios:"}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] "Odds Ratios:"}
\FunctionTok{print}\NormalTok{(odds\_ratios)}
\CommentTok{\#\textgreater{} (Intercept)           x }
\CommentTok{\#\textgreater{}    2.059080    3.411295}

\CommentTok{\# Predict probability for a new observation (e.g., x = 1)}
\NormalTok{new\_x }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x =} \DecValTok{1}\NormalTok{)}
\NormalTok{predicted\_prob }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(model, }\AttributeTok{newdata =}\NormalTok{ new\_x, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}

\FunctionTok{print}\NormalTok{(}\StringTok{"Predicted Probability for x = 1:"}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] "Predicted Probability for x = 1:"}
\FunctionTok{print}\NormalTok{(predicted\_prob)}
\CommentTok{\#\textgreater{}         1 }
\CommentTok{\#\textgreater{} 0.8753759}
\end{Highlighting}
\end{Shaded}

\hypertarget{application-logistic-regression}{%
\subsection{Application: Logistic Regression}\label{application-logistic-regression}}

In this section, we demonstrate the application of \textbf{logistic regression} using simulated data. We explore model fitting, inference, residual analysis, and goodness-of-fit testing.

\textbf{1. Load Required Libraries}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(kableExtra)}
\FunctionTok{library}\NormalTok{(dplyr)}
\FunctionTok{library}\NormalTok{(pscl)}
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(faraway)}
\FunctionTok{library}\NormalTok{(nnet)}
\FunctionTok{library}\NormalTok{(agridat)}
\FunctionTok{library}\NormalTok{(nlstools)}
\end{Highlighting}
\end{Shaded}

\textbf{2. Data Generation}

We generate a dataset where the predictor variable \(X\) follows a \textbf{uniform distribution}:

\[
x \sim Unif(-0.5,2.5)
\]

The \textbf{linear predictor} is given by:

\[
\eta = 0.5 + 0.75 x
\]

Passing \(\eta\) into the inverse-logit function, we obtain:

\[
p = \frac{\exp(\eta)}{1+ \exp(\eta)}
\]

which ensures that \(p \in [0,1]\). We then generate the binary response variable:

\[
y \sim Bernoulli(p)
\]

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{23}\NormalTok{) }\CommentTok{\# Set seed for reproducibility}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\AttributeTok{min =} \SpecialCharTok{{-}}\FloatTok{0.5}\NormalTok{, }\AttributeTok{max =} \FloatTok{2.5}\NormalTok{)  }\CommentTok{\# Generate X values}
\NormalTok{eta1 }\OtherTok{\textless{}{-}} \FloatTok{0.5} \SpecialCharTok{+} \FloatTok{0.75} \SpecialCharTok{*}\NormalTok{ x                   }\CommentTok{\# Compute linear predictor}
\NormalTok{p }\OtherTok{\textless{}{-}} \FunctionTok{exp}\NormalTok{(eta1) }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(eta1))          }\CommentTok{\# Compute probabilities}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\DecValTok{1}\NormalTok{, p)                   }\CommentTok{\# Generate binary response}
\NormalTok{BinData }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{X =}\NormalTok{ x, }\AttributeTok{Y =}\NormalTok{ y)       }\CommentTok{\# Create data frame}
\end{Highlighting}
\end{Shaded}

\textbf{3. Model Fitting}

We fit a \textbf{logistic regression model} to the simulated data:

\[
\text{logit}(p) = \beta_0 + \beta_1 X
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Logistic\_Model }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X,}
                      \CommentTok{\# Specifies the response distribution}
                      \AttributeTok{family =}\NormalTok{ binomial,}
                      \AttributeTok{data =}\NormalTok{ BinData)}

\FunctionTok{summary}\NormalTok{(Logistic\_Model) }\CommentTok{\# Model summary}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} glm(formula = Y \textasciitilde{} X, family = binomial, data = BinData)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Deviance Residuals: }
\CommentTok{\#\textgreater{}     Min       1Q   Median       3Q      Max  }
\CommentTok{\#\textgreater{} {-}2.2317   0.4153   0.5574   0.7922   1.1469  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}             Estimate Std. Error z value Pr(\textgreater{}|z|)    }
\CommentTok{\#\textgreater{} (Intercept)  0.46205    0.10201   4.530 5.91e{-}06 ***}
\CommentTok{\#\textgreater{} X            0.78527    0.09296   8.447  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} (Dispersion parameter for binomial family taken to be 1)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Null deviance: 1106.7  on 999  degrees of freedom}
\CommentTok{\#\textgreater{} Residual deviance: 1027.4  on 998  degrees of freedom}
\CommentTok{\#\textgreater{} AIC: 1031.4}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Fisher Scoring iterations: 4}
\NormalTok{nlstools}\SpecialCharTok{::}\FunctionTok{confint2}\NormalTok{(Logistic\_Model) }\CommentTok{\# Confidence intervals}
\CommentTok{\#\textgreater{}                 2.5 \%    97.5 \%}
\CommentTok{\#\textgreater{} (Intercept) 0.2618709 0.6622204}
\CommentTok{\#\textgreater{} X           0.6028433 0.9676934}

\CommentTok{\# Compute odds ratios}
\NormalTok{OddsRatio }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(Logistic\_Model) }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ exp}
\NormalTok{OddsRatio}
\CommentTok{\#\textgreater{} (Intercept)           X }
\CommentTok{\#\textgreater{}    1.587318    2.192995}
\end{Highlighting}
\end{Shaded}

Interpretation of the Odds Ratio

\begin{itemize}
\item
  When \(x = 0\), the \textbf{odds of success} are \textbf{1.59}.
\item
  When \(x = 1\), the \textbf{odds of success increase by a factor of 2.19}, indicating a \textbf{119.29\% increase}.
\end{itemize}

\textbf{4. Deviance Test}

We assess the model's significance using the \textbf{deviance test}, which compares:

\begin{itemize}
\item
  \(H_0\): No predictors are related to the response (intercept-only model).
\item
  \(H_1\): At least one predictor is related to the response.
\end{itemize}

The \textbf{test statistic} is:

\[
D = \text{Null Deviance} - \text{Residual Deviance}
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Test\_Dev }\OtherTok{\textless{}{-}}\NormalTok{ Logistic\_Model}\SpecialCharTok{$}\NormalTok{null.deviance }\SpecialCharTok{{-}}\NormalTok{ Logistic\_Model}\SpecialCharTok{$}\NormalTok{deviance}
\NormalTok{p\_val\_dev }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{pchisq}\NormalTok{(}\AttributeTok{q =}\NormalTok{ Test\_Dev, }\AttributeTok{df =} \DecValTok{1}\NormalTok{)}
\NormalTok{p\_val\_dev}
\CommentTok{\#\textgreater{} [1] 0}
\end{Highlighting}
\end{Shaded}

\textbf{Conclusion:}

Since the \textbf{p-value is approximately 0}, we \textbf{reject} \(H_0\), confirming that \(X\) is significantly related to \(Y\).

\textbf{5. Residual Analysis}

We compute \textbf{deviance residuals} and plot them against \(X\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Logistic\_Resids }\OtherTok{\textless{}{-}} \FunctionTok{residuals}\NormalTok{(Logistic\_Model, }\AttributeTok{type =} \StringTok{"deviance"}\NormalTok{)}

\FunctionTok{plot}\NormalTok{(}
    \AttributeTok{y =}\NormalTok{ Logistic\_Resids,}
    \AttributeTok{x =}\NormalTok{ BinData}\SpecialCharTok{$}\NormalTok{X,}
    \AttributeTok{xlab =} \StringTok{\textquotesingle{}X\textquotesingle{}}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{\textquotesingle{}Deviance Residuals\textquotesingle{}}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{07-generalized-linear-models_files/figure-latex/unnamed-chunk-7-1} \end{center}

This plot is not very informative. A more insightful approach is \textbf{binned residual plots}.

\textbf{6. Binned Residual Plot}

We group residuals into \textbf{bins} based on predicted values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot\_bin }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(Y,}
\NormalTok{                     X,}
                     \AttributeTok{bins =} \DecValTok{100}\NormalTok{,}
                     \AttributeTok{return.DF =} \ConstantTok{FALSE}\NormalTok{) \{}
\NormalTok{  Y\_Name }\OtherTok{\textless{}{-}} \FunctionTok{deparse}\NormalTok{(}\FunctionTok{substitute}\NormalTok{(Y))}
\NormalTok{  X\_Name }\OtherTok{\textless{}{-}} \FunctionTok{deparse}\NormalTok{(}\FunctionTok{substitute}\NormalTok{(X))}
  
\NormalTok{  Binned\_Plot }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{Plot\_Y =}\NormalTok{ Y, }\AttributeTok{Plot\_X =}\NormalTok{ X)}
\NormalTok{  Binned\_Plot}\SpecialCharTok{$}\NormalTok{bin }\OtherTok{\textless{}{-}}
    \FunctionTok{cut}\NormalTok{(Binned\_Plot}\SpecialCharTok{$}\NormalTok{Plot\_X, }\AttributeTok{breaks =}\NormalTok{ bins) }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ as.numeric}
  
\NormalTok{  Binned\_Plot\_summary }\OtherTok{\textless{}{-}}\NormalTok{ Binned\_Plot }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{group\_by}\NormalTok{(bin) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{summarise}\NormalTok{(}
      \AttributeTok{Y\_ave =} \FunctionTok{mean}\NormalTok{(Plot\_Y),}
      \AttributeTok{X\_ave =} \FunctionTok{mean}\NormalTok{(Plot\_X),}
      \AttributeTok{Count =} \FunctionTok{n}\NormalTok{()}
\NormalTok{    ) }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ as.data.frame}
  
  \FunctionTok{plot}\NormalTok{(}
    \AttributeTok{y =}\NormalTok{ Binned\_Plot\_summary}\SpecialCharTok{$}\NormalTok{Y\_ave,}
    \AttributeTok{x =}\NormalTok{ Binned\_Plot\_summary}\SpecialCharTok{$}\NormalTok{X\_ave,}
    \AttributeTok{ylab =}\NormalTok{ Y\_Name,}
    \AttributeTok{xlab =}\NormalTok{ X\_Name}
\NormalTok{  )}
  
  \ControlFlowTok{if}\NormalTok{ (return.DF)}
    \FunctionTok{return}\NormalTok{(Binned\_Plot\_summary)}
\NormalTok{\}}

\FunctionTok{plot\_bin}\NormalTok{(}\AttributeTok{Y =}\NormalTok{ Logistic\_Resids, }\AttributeTok{X =}\NormalTok{ BinData}\SpecialCharTok{$}\NormalTok{X, }\AttributeTok{bins =} \DecValTok{100}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{07-generalized-linear-models_files/figure-latex/unnamed-chunk-8-1} \end{center}

We also examine \textbf{predicted values vs residuals}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Logistic\_Predictions }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(Logistic\_Model, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\FunctionTok{plot\_bin}\NormalTok{(}\AttributeTok{Y =}\NormalTok{ Logistic\_Resids, }\AttributeTok{X =}\NormalTok{ Logistic\_Predictions, }\AttributeTok{bins =} \DecValTok{100}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{07-generalized-linear-models_files/figure-latex/unnamed-chunk-9-1} \end{center}

Finally, we compare \textbf{predicted probabilities} to actual outcomes:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{NumBins }\OtherTok{\textless{}{-}} \DecValTok{10}
\NormalTok{Binned\_Data }\OtherTok{\textless{}{-}} \FunctionTok{plot\_bin}\NormalTok{(}
    \AttributeTok{Y =}\NormalTok{ BinData}\SpecialCharTok{$}\NormalTok{Y,}
    \AttributeTok{X =}\NormalTok{ Logistic\_Predictions,}
    \AttributeTok{bins =}\NormalTok{ NumBins,}
    \AttributeTok{return.DF =} \ConstantTok{TRUE}
\NormalTok{)}
\NormalTok{Binned\_Data}
\CommentTok{\#\textgreater{}    bin     Y\_ave     X\_ave Count}
\CommentTok{\#\textgreater{} 1    1 0.5833333 0.5382095    72}
\CommentTok{\#\textgreater{} 2    2 0.5200000 0.5795887    75}
\CommentTok{\#\textgreater{} 3    3 0.6567164 0.6156540    67}
\CommentTok{\#\textgreater{} 4    4 0.7014925 0.6579674    67}
\CommentTok{\#\textgreater{} 5    5 0.6373626 0.6984765    91}
\CommentTok{\#\textgreater{} 6    6 0.7500000 0.7373341    72}
\CommentTok{\#\textgreater{} 7    7 0.7096774 0.7786747    93}
\CommentTok{\#\textgreater{} 8    8 0.8503937 0.8203819   127}
\CommentTok{\#\textgreater{} 9    9 0.8947368 0.8601232   133}
\CommentTok{\#\textgreater{} 10  10 0.8916256 0.9004734   203}
\FunctionTok{abline}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{, }\AttributeTok{col =} \StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{07-generalized-linear-models_files/figure-latex/unnamed-chunk-10-1} \end{center}

\textbf{7. Model Goodness-of-Fit: Hosmer-Lemeshow Test}

The \textbf{Hosmer-Lemeshow test} evaluates whether the model fits the data well. The test statistic is: \[
X^2_{HL} = \sum_{j=1}^{J} \frac{(y_j - m_j \hat{p}_j)^2}{m_j \hat{p}_j(1-\hat{p}_j)}
\] where:

\begin{itemize}
\item
  \(y_j\) is the observed number of successes in bin \(j\).
\item
  \(m_j\) is the number of observations in bin \(j\).
\item
  \(\hat{p}_j\) is the predicted probability in bin \(j\).
\end{itemize}

Under \(H_0\), we assume:

\[
X^2_{HL} \sim \chi^2_{J-1}
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{HL\_BinVals }\OtherTok{\textless{}{-}}\NormalTok{ (Binned\_Data}\SpecialCharTok{$}\NormalTok{Count }\SpecialCharTok{*}\NormalTok{ Binned\_Data}\SpecialCharTok{$}\NormalTok{Y\_ave }\SpecialCharTok{{-}} 
\NormalTok{               Binned\_Data}\SpecialCharTok{$}\NormalTok{Count }\SpecialCharTok{*}\NormalTok{ Binned\_Data}\SpecialCharTok{$}\NormalTok{X\_ave) }\SpecialCharTok{\^{}} \DecValTok{2} \SpecialCharTok{/}   
\NormalTok{               (Binned\_Data}\SpecialCharTok{$}\NormalTok{Count }\SpecialCharTok{*}\NormalTok{ Binned\_Data}\SpecialCharTok{$}\NormalTok{X\_ave }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ Binned\_Data}\SpecialCharTok{$}\NormalTok{X\_ave))}

\NormalTok{HLpval }\OtherTok{\textless{}{-}} \FunctionTok{pchisq}\NormalTok{(}\AttributeTok{q =} \FunctionTok{sum}\NormalTok{(HL\_BinVals),}
                 \AttributeTok{df =}\NormalTok{ NumBins }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{,}
                 \AttributeTok{lower.tail =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{HLpval}
\CommentTok{\#\textgreater{} [1] 0.4150004}
\end{Highlighting}
\end{Shaded}

\textbf{Conclusion:}

\begin{itemize}
\item
  Since \(p\)-value = 0.99, we \textbf{fail to reject} \(H_0\).
\item
  This indicates that \textbf{the model fits the data well}.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{sec-probit-regression}{%
\section{Probit Regression}\label{sec-probit-regression}}

Probit regression is a type of \protect\hyperlink{generalized-linear-models}{Generalized Linear Models} used for binary outcome variables. Unlike \protect\hyperlink{sec-logistic-regression}{logistic regression}, which uses the \protect\hyperlink{sec-logit-transformation}{logit function}, probit regression assumes that the probability of success is determined by an \textbf{underlying normally distributed latent variable}.

\hypertarget{probit-model}{%
\subsection{Probit Model}\label{probit-model}}

Let \(Y_i\) be a binary response variable:

\[
Y_i =
\begin{cases} 
1, & \text{if success occurs} \\ 
0, & \text{otherwise}
\end{cases}
\]

We assume that \(Y_i\) follows a \textbf{Bernoulli distribution}:

\[
Y_i \sim \text{Bernoulli}(p_i), \quad \text{where } p_i = P(Y_i = 1 | \mathbf{x_i})
\]

Instead of the \protect\hyperlink{sec-logit-transformation}{logit function} in \protect\hyperlink{sec-logistic-regression}{logistic regression}:

\[
\text{logit}(p_i) = \log\left( \frac{p_i}{1 - p_i} \right) = \mathbf{x_i'\beta}
\]

Probit regression uses the \textbf{inverse standard normal CDF}:

\[
\Phi^{-1}(p_i) = \mathbf{x_i'\theta}
\]

where:

\begin{itemize}
\item
  \(\Phi(\cdot)\) is the \textbf{CDF of the standard normal distribution}.
\item
  \(\mathbf{x_i}\) is the vector of predictors.
\item
  \(\theta\) is the vector of regression coefficients.
\end{itemize}

Thus, the probability of success is:

\[
p_i = P(Y_i = 1 | \mathbf{x_i}) = \Phi(\mathbf{x_i'\theta})
\]

where:

\[
\Phi(z) = \int_{-\infty}^{z} \frac{1}{\sqrt{2\pi}} e^{-t^2/2} dt
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{application-probit-regression}{%
\subsection{Application: Probit Regression}\label{application-probit-regression}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load necessary library}
\FunctionTok{library}\NormalTok{(ggplot2)}

\CommentTok{\# Set seed for reproducibility}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\# Simulate data}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{x1 }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{x2 }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{latent }\OtherTok{\textless{}{-}} \FloatTok{0.5} \SpecialCharTok{*}\NormalTok{ x1 }\SpecialCharTok{+} \FloatTok{0.7} \SpecialCharTok{*}\NormalTok{ x2 }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)  }\CommentTok{\# Linear combination}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(latent }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)  }\CommentTok{\# Binary outcome}

\CommentTok{\# Create dataframe}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(y, x1, x2)}

\CommentTok{\# Fit Probit model}
\NormalTok{probit\_model }\OtherTok{\textless{}{-}}
    \FunctionTok{glm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x1 }\SpecialCharTok{+}\NormalTok{ x2, }\AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"probit"}\NormalTok{), }\AttributeTok{data =}\NormalTok{ data)}
\FunctionTok{summary}\NormalTok{(probit\_model)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} glm(formula = y \textasciitilde{} x1 + x2, family = binomial(link = "probit"), }
\CommentTok{\#\textgreater{}     data = data)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Deviance Residuals: }
\CommentTok{\#\textgreater{}     Min       1Q   Median       3Q      Max  }
\CommentTok{\#\textgreater{} {-}2.3740  {-}0.8663  {-}0.2318   0.8684   2.6666  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}             Estimate Std. Error z value Pr(\textgreater{}|z|)    }
\CommentTok{\#\textgreater{} (Intercept) {-}0.09781    0.04499  {-}2.174   0.0297 *  }
\CommentTok{\#\textgreater{} x1           0.43838    0.04891   8.963   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} x2           0.75538    0.05306  14.235   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} (Dispersion parameter for binomial family taken to be 1)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Null deviance: 1385.1  on 999  degrees of freedom}
\CommentTok{\#\textgreater{} Residual deviance: 1045.3  on 997  degrees of freedom}
\CommentTok{\#\textgreater{} AIC: 1051.3}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Fisher Scoring iterations: 5}

\CommentTok{\# Fit Logit model}
\NormalTok{logit\_model }\OtherTok{\textless{}{-}}
    \FunctionTok{glm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x1 }\SpecialCharTok{+}\NormalTok{ x2, }\AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"logit"}\NormalTok{), }\AttributeTok{data =}\NormalTok{ data)}
\FunctionTok{summary}\NormalTok{(logit\_model)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} glm(formula = y \textasciitilde{} x1 + x2, family = binomial(link = "logit"), }
\CommentTok{\#\textgreater{}     data = data)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Deviance Residuals: }
\CommentTok{\#\textgreater{}     Min       1Q   Median       3Q      Max  }
\CommentTok{\#\textgreater{} {-}2.3048  {-}0.8571  {-}0.2805   0.8632   2.5335  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}             Estimate Std. Error z value Pr(\textgreater{}|z|)    }
\CommentTok{\#\textgreater{} (Intercept) {-}0.16562    0.07600  {-}2.179   0.0293 *  }
\CommentTok{\#\textgreater{} x1           0.73234    0.08507   8.608   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} x2           1.25220    0.09486  13.201   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} (Dispersion parameter for binomial family taken to be 1)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Null deviance: 1385.1  on 999  degrees of freedom}
\CommentTok{\#\textgreater{} Residual deviance: 1048.4  on 997  degrees of freedom}
\CommentTok{\#\textgreater{} AIC: 1054.4}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Fisher Scoring iterations: 4}

\CommentTok{\# Compare Coefficients}
\NormalTok{coef\_comparison }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
    \AttributeTok{Variable =} \FunctionTok{names}\NormalTok{(}\FunctionTok{coef}\NormalTok{(probit\_model)),}
    \AttributeTok{Probit\_Coef =} \FunctionTok{coef}\NormalTok{(probit\_model),}
    \AttributeTok{Logit\_Coef =} \FunctionTok{coef}\NormalTok{(logit\_model),}
    \AttributeTok{Logit\_Probit\_Ratio =} \FunctionTok{coef}\NormalTok{(logit\_model) }\SpecialCharTok{/} \FunctionTok{coef}\NormalTok{(probit\_model)}
\NormalTok{)}

\FunctionTok{print}\NormalTok{(coef\_comparison)}
\CommentTok{\#\textgreater{}                Variable Probit\_Coef Logit\_Coef Logit\_Probit\_Ratio}
\CommentTok{\#\textgreater{} (Intercept) (Intercept) {-}0.09780689 {-}0.1656216           1.693353}
\CommentTok{\#\textgreater{} x1                   x1  0.43837627  0.7323392           1.670572}
\CommentTok{\#\textgreater{} x2                   x2  0.75538259  1.2522008           1.657704}

\CommentTok{\# Compute predicted probabilities}
\NormalTok{data}\SpecialCharTok{$}\NormalTok{probit\_pred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(probit\_model, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\NormalTok{data}\SpecialCharTok{$}\NormalTok{logit\_pred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(logit\_model, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}

\CommentTok{\# Plot Probit vs Logit predictions}
\FunctionTok{ggplot}\NormalTok{(data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ probit\_pred, }\AttributeTok{y =}\NormalTok{ logit\_pred)) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_abline}\NormalTok{(}\AttributeTok{slope =} \DecValTok{1}\NormalTok{,}
                \AttributeTok{intercept =} \DecValTok{0}\NormalTok{,}
                \AttributeTok{col =} \StringTok{"red"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Comparison of Predicted Probabilities"}\NormalTok{,}
         \AttributeTok{x =} \StringTok{"Probit Predictions"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Logit Predictions"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{07-generalized-linear-models_files/figure-latex/unnamed-chunk-12-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Classification Accuracy}
\NormalTok{threshold }\OtherTok{\textless{}{-}} \FloatTok{0.5}
\NormalTok{data}\SpecialCharTok{$}\NormalTok{probit\_class }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{probit\_pred }\SpecialCharTok{\textgreater{}}\NormalTok{ threshold, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{data}\SpecialCharTok{$}\NormalTok{logit\_class }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{logit\_pred }\SpecialCharTok{\textgreater{}}\NormalTok{ threshold, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}

\NormalTok{probit\_acc }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{probit\_class }\SpecialCharTok{==}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{y)}
\NormalTok{logit\_acc }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{logit\_class }\SpecialCharTok{==}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{y)}

\FunctionTok{print}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"Probit Accuracy:"}\NormalTok{, }\FunctionTok{round}\NormalTok{(probit\_acc, }\DecValTok{4}\NormalTok{)))}
\CommentTok{\#\textgreater{} [1] "Probit Accuracy: 0.71"}
\FunctionTok{print}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"Logit Accuracy:"}\NormalTok{, }\FunctionTok{round}\NormalTok{(logit\_acc, }\DecValTok{4}\NormalTok{)))}
\CommentTok{\#\textgreater{} [1] "Logit Accuracy: 0.71"}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-binomial-regression}{%
\section{Binomial Regression}\label{sec-binomial-regression}}

In previous sections, we introduced \textbf{binomial regression models}, including both \protect\hyperlink{sec-logistic-regression}{Logistic Regression} and \href{An\%20alternative\%20to\%20logistic\%20regression,\%20commonly\%20used\%20in\%20decision\%20modeling.}{probit regression}, and discussed their theoretical foundations. Now, we apply these methods to real-world data using the \texttt{esoph} dataset, which examines the relationship between esophageal cancer and potential risk factors such as \textbf{alcohol consumption} and \textbf{age group}.

\hypertarget{dataset-overview}{%
\subsection{Dataset Overview}\label{dataset-overview}}

The \texttt{esoph} dataset consists of:

\begin{itemize}
\item
  \textbf{Successes (\texttt{ncases})}: The number of individuals diagnosed with esophageal cancer.
\item
  \textbf{Failures (\texttt{ncontrols})}: The number of individuals in the control group (without cancer).
\item
  \textbf{Predictors}:

  \begin{itemize}
  \item
    \texttt{agegp}: Age group of individuals.
  \item
    \texttt{alcgp}: Alcohol consumption category.
  \item
    \texttt{tobgp}: Tobacco consumption category.
  \end{itemize}
\end{itemize}

Before fitting our models, let's inspect the dataset and visualize some key relationships.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load and inspect the dataset}
\FunctionTok{data}\NormalTok{(}\StringTok{"esoph"}\NormalTok{)}
\FunctionTok{head}\NormalTok{(esoph, }\AttributeTok{n =} \DecValTok{3}\NormalTok{)}
\CommentTok{\#\textgreater{}   agegp     alcgp    tobgp ncases ncontrols}
\CommentTok{\#\textgreater{} 1 25{-}34 0{-}39g/day 0{-}9g/day      0        40}
\CommentTok{\#\textgreater{} 2 25{-}34 0{-}39g/day    10{-}19      0        10}
\CommentTok{\#\textgreater{} 3 25{-}34 0{-}39g/day    20{-}29      0         6}

\CommentTok{\# Visualizing the proportion of cancer cases by alcohol consumption}
\FunctionTok{plot}\NormalTok{(}
\NormalTok{  esoph}\SpecialCharTok{$}\NormalTok{ncases }\SpecialCharTok{/}\NormalTok{ (esoph}\SpecialCharTok{$}\NormalTok{ncases }\SpecialCharTok{+}\NormalTok{ esoph}\SpecialCharTok{$}\NormalTok{ncontrols) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ esoph}\SpecialCharTok{$}\NormalTok{alcgp,}
  \AttributeTok{ylab =} \StringTok{"Proportion of Cancer Cases"}\NormalTok{,}
  \AttributeTok{xlab =} \StringTok{"Alcohol Consumption Group"}\NormalTok{,}
  \AttributeTok{main =} \StringTok{"Esophageal Cancer Data"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{07-generalized-linear-models_files/figure-latex/unnamed-chunk-13-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Ensure categorical variables are treated as factors}
\FunctionTok{class}\NormalTok{(esoph}\SpecialCharTok{$}\NormalTok{agegp) }\OtherTok{\textless{}{-}} \StringTok{"factor"}
\FunctionTok{class}\NormalTok{(esoph}\SpecialCharTok{$}\NormalTok{alcgp) }\OtherTok{\textless{}{-}} \StringTok{"factor"}
\FunctionTok{class}\NormalTok{(esoph}\SpecialCharTok{$}\NormalTok{tobgp) }\OtherTok{\textless{}{-}} \StringTok{"factor"}
\end{Highlighting}
\end{Shaded}

\hypertarget{apply-logistic-model}{%
\subsection{Apply Logistic Model}\label{apply-logistic-model}}

We first fit a \textbf{logistic regression model}, where the response variable is the proportion of cancer cases (\texttt{ncases}) relative to total observations (\texttt{ncases\ +\ ncontrols}).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Logistic regression using alcohol consumption as a predictor}
\NormalTok{model }\OtherTok{\textless{}{-}}
    \FunctionTok{glm}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(ncases, ncontrols) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ alcgp,}
        \AttributeTok{data =}\NormalTok{ esoph,}
        \AttributeTok{family =}\NormalTok{ binomial)}

\CommentTok{\# Summary of the model}
\FunctionTok{summary}\NormalTok{(model)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} glm(formula = cbind(ncases, ncontrols) \textasciitilde{} alcgp, family = binomial, }
\CommentTok{\#\textgreater{}     data = esoph)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Deviance Residuals: }
\CommentTok{\#\textgreater{}     Min       1Q   Median       3Q      Max  }
\CommentTok{\#\textgreater{} {-}4.0759  {-}1.2037  {-}0.0183   1.0928   3.7336  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}             Estimate Std. Error z value Pr(\textgreater{}|z|)    }
\CommentTok{\#\textgreater{} (Intercept)  {-}2.5885     0.1925 {-}13.444  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} alcgp40{-}79    1.2712     0.2323   5.472 4.46e{-}08 ***}
\CommentTok{\#\textgreater{} alcgp80{-}119   2.0545     0.2611   7.868 3.59e{-}15 ***}
\CommentTok{\#\textgreater{} alcgp120+     3.3042     0.3237  10.209  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} (Dispersion parameter for binomial family taken to be 1)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Null deviance: 367.95  on 87  degrees of freedom}
\CommentTok{\#\textgreater{} Residual deviance: 221.46  on 84  degrees of freedom}
\CommentTok{\#\textgreater{} AIC: 344.51}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Fisher Scoring iterations: 5}
\end{Highlighting}
\end{Shaded}

Interpretation

\begin{itemize}
\item
  The coefficients represent the \textbf{log-odds} of having esophageal cancer relative to the baseline alcohol consumption group.
\item
  \textbf{P-values} indicate whether alcohol consumption levels significantly influence cancer risk.
\end{itemize}

Model Diagnostics

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Convert coefficients to odds ratios}
\FunctionTok{exp}\NormalTok{(}\FunctionTok{coefficients}\NormalTok{(model))}
\CommentTok{\#\textgreater{} (Intercept)  alcgp40{-}79 alcgp80{-}119   alcgp120+ }
\CommentTok{\#\textgreater{}  0.07512953  3.56527094  7.80261593 27.22570533}

\CommentTok{\# Model goodness{-}of{-}fit measures}
\FunctionTok{deviance}\NormalTok{(model) }\SpecialCharTok{/} \FunctionTok{df.residual}\NormalTok{(model)  }\CommentTok{\# Closer to 1 suggests a better fit}
\CommentTok{\#\textgreater{} [1] 2.63638}
\NormalTok{model}\SpecialCharTok{$}\NormalTok{aic  }\CommentTok{\# Lower AIC is preferable for model comparison}
\CommentTok{\#\textgreater{} [1] 344.5109}
\end{Highlighting}
\end{Shaded}

To improve our model, we include \textbf{age group (\texttt{agegp})} as an additional predictor.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Logistic regression with alcohol consumption and age}
\NormalTok{better\_model }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(}
    \FunctionTok{cbind}\NormalTok{(ncases, ncontrols) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ agegp }\SpecialCharTok{+}\NormalTok{ alcgp,}
    \AttributeTok{data =}\NormalTok{ esoph,}
    \AttributeTok{family =}\NormalTok{ binomial}
\NormalTok{)}

\CommentTok{\# Summary of the improved model}
\FunctionTok{summary}\NormalTok{(better\_model)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} glm(formula = cbind(ncases, ncontrols) \textasciitilde{} agegp + alcgp, family = binomial, }
\CommentTok{\#\textgreater{}     data = esoph)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Deviance Residuals: }
\CommentTok{\#\textgreater{}     Min       1Q   Median       3Q      Max  }
\CommentTok{\#\textgreater{} {-}2.2395  {-}0.7186  {-}0.2324   0.7930   3.3538  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}             Estimate Std. Error z value Pr(\textgreater{}|z|)    }
\CommentTok{\#\textgreater{} (Intercept)  {-}6.1472     1.0419  {-}5.900 3.63e{-}09 ***}
\CommentTok{\#\textgreater{} agegp35{-}44    1.6311     1.0800   1.510 0.130973    }
\CommentTok{\#\textgreater{} agegp45{-}54    3.4258     1.0389   3.297 0.000976 ***}
\CommentTok{\#\textgreater{} agegp55{-}64    3.9435     1.0346   3.811 0.000138 ***}
\CommentTok{\#\textgreater{} agegp65{-}74    4.3568     1.0413   4.184 2.87e{-}05 ***}
\CommentTok{\#\textgreater{} agegp75+      4.4242     1.0914   4.054 5.04e{-}05 ***}
\CommentTok{\#\textgreater{} alcgp40{-}79    1.4343     0.2448   5.859 4.64e{-}09 ***}
\CommentTok{\#\textgreater{} alcgp80{-}119   2.0071     0.2776   7.230 4.84e{-}13 ***}
\CommentTok{\#\textgreater{} alcgp120+     3.6800     0.3763   9.778  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} (Dispersion parameter for binomial family taken to be 1)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Null deviance: 367.95  on 87  degrees of freedom}
\CommentTok{\#\textgreater{} Residual deviance: 105.88  on 79  degrees of freedom}
\CommentTok{\#\textgreater{} AIC: 238.94}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Fisher Scoring iterations: 6}

\CommentTok{\# Model evaluation}
\NormalTok{better\_model}\SpecialCharTok{$}\NormalTok{aic }\CommentTok{\# Lower AIC is better}
\CommentTok{\#\textgreater{} [1] 238.9361}

\CommentTok{\# Convert coefficients to odds ratios}
\CommentTok{\# exp(coefficients(better\_model))}
\FunctionTok{data.frame}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Odds Ratios}\StringTok{\textasciigrave{}} \OtherTok{=} \FunctionTok{exp}\NormalTok{(}\FunctionTok{coefficients}\NormalTok{(better\_model)))}
\CommentTok{\#\textgreater{}              Odds.Ratios}
\CommentTok{\#\textgreater{} (Intercept)  0.002139482}
\CommentTok{\#\textgreater{} agegp35{-}44   5.109601844}
\CommentTok{\#\textgreater{} agegp45{-}54  30.748594216}
\CommentTok{\#\textgreater{} agegp55{-}64  51.596634690}
\CommentTok{\#\textgreater{} agegp65{-}74  78.005283850}
\CommentTok{\#\textgreater{} agegp75+    83.448437749}
\CommentTok{\#\textgreater{} alcgp40{-}79   4.196747169}
\CommentTok{\#\textgreater{} alcgp80{-}119  7.441782227}
\CommentTok{\#\textgreater{} alcgp120+   39.646885126}

\CommentTok{\# Compare models using likelihood ratio test (Chi{-}square test)}
\FunctionTok{pchisq}\NormalTok{(}
    \AttributeTok{q =}\NormalTok{ model}\SpecialCharTok{$}\NormalTok{deviance }\SpecialCharTok{{-}}\NormalTok{ better\_model}\SpecialCharTok{$}\NormalTok{deviance,}
    \AttributeTok{df =}\NormalTok{ model}\SpecialCharTok{$}\NormalTok{df.residual }\SpecialCharTok{{-}}\NormalTok{ better\_model}\SpecialCharTok{$}\NormalTok{df.residual,}
    \AttributeTok{lower.tail =} \ConstantTok{FALSE}
\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 2.713923e{-}23}
\end{Highlighting}
\end{Shaded}

Key Takeaways

\begin{itemize}
\item
  \textbf{AIC Reduction}: A lower AIC suggests that adding age as a predictor improves the model.
\item
  \textbf{Likelihood Ratio Test}: This test compares the two models and determines whether the improvement is statistically significant.
\end{itemize}

\hypertarget{apply-probit-model}{%
\subsection{Apply Probit Model}\label{apply-probit-model}}

As discussed earlier, the \textbf{probit} model is an alternative to logistic regression, using a cumulative normal distribution instead of the logistic function.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Probit regression model}
\NormalTok{Prob\_better\_model }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(}
    \FunctionTok{cbind}\NormalTok{(ncases, ncontrols) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ agegp }\SpecialCharTok{+}\NormalTok{ alcgp,}
    \AttributeTok{data =}\NormalTok{ esoph,}
    \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =}\NormalTok{ probit)}
\NormalTok{)}

\CommentTok{\# Summary of the probit model}
\FunctionTok{summary}\NormalTok{(Prob\_better\_model)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} glm(formula = cbind(ncases, ncontrols) \textasciitilde{} agegp + alcgp, family = binomial(link = probit), }
\CommentTok{\#\textgreater{}     data = esoph)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Deviance Residuals: }
\CommentTok{\#\textgreater{}     Min       1Q   Median       3Q      Max  }
\CommentTok{\#\textgreater{} {-}2.1325  {-}0.6877  {-}0.1661   0.7654   3.3258  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}             Estimate Std. Error z value Pr(\textgreater{}|z|)    }
\CommentTok{\#\textgreater{} (Intercept)  {-}3.3741     0.4922  {-}6.855 7.13e{-}12 ***}
\CommentTok{\#\textgreater{} agegp35{-}44    0.8562     0.5081   1.685 0.092003 .  }
\CommentTok{\#\textgreater{} agegp45{-}54    1.7829     0.4904   3.636 0.000277 ***}
\CommentTok{\#\textgreater{} agegp55{-}64    2.1034     0.4876   4.314 1.61e{-}05 ***}
\CommentTok{\#\textgreater{} agegp65{-}74    2.3374     0.4930   4.741 2.13e{-}06 ***}
\CommentTok{\#\textgreater{} agegp75+      2.3694     0.5275   4.491 7.08e{-}06 ***}
\CommentTok{\#\textgreater{} alcgp40{-}79    0.8080     0.1330   6.076 1.23e{-}09 ***}
\CommentTok{\#\textgreater{} alcgp80{-}119   1.1399     0.1558   7.318 2.52e{-}13 ***}
\CommentTok{\#\textgreater{} alcgp120+     2.1204     0.2060  10.295  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} (Dispersion parameter for binomial family taken to be 1)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Null deviance: 367.95  on 87  degrees of freedom}
\CommentTok{\#\textgreater{} Residual deviance: 104.48  on 79  degrees of freedom}
\CommentTok{\#\textgreater{} AIC: 237.53}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Fisher Scoring iterations: 6}
\end{Highlighting}
\end{Shaded}

Why Consider a Probit Model?

\begin{itemize}
\item
  Like logistic regression, probit regression estimates probabilities, but it assumes a \textbf{normal distribution of the latent variable}.
\item
  While the \textbf{interpretation of coefficients differs}, model comparisons can still be made using \textbf{AIC}.
\end{itemize}

\hypertarget{sec-poisson-regression}{%
\section{Poisson Regression}\label{sec-poisson-regression}}

\hypertarget{the-poisson-distribution}{%
\subsection{The Poisson Distribution}\label{the-poisson-distribution}}

Poisson regression is used for modeling \textbf{count data}, where the response variable represents the number of occurrences of an event within a fixed period, space, or other unit. The Poisson distribution is defined as:

\[
\begin{aligned} f(Y_i) &= \frac{\mu_i^{Y_i} \exp(-\mu_i)}{Y_i!}, \quad Y_i = 0,1,2, \dots \\ E(Y_i) &= \mu_i \\ \text{Var}(Y_i) &= \mu_i \end{aligned}
\] where:

\begin{itemize}
\item
  \(Y_i\) is the count variable.
\item
  \(\mu_i\) is the expected count for the \(i\)-th observation.
\item
  The \textbf{mean and variance are equal} \(E(Y_i) = \text{Var}(Y_i)\), making Poisson regression suitable when variance follows this property.
\end{itemize}

However, real-world count data often exhibit \textbf{overdispersion}, where the variance exceeds the mean. We will discuss remedies such as \protect\hyperlink{sec-quasi-poisson-regression}{Quasi-Poisson} and \protect\hyperlink{sec-negative-binomial-regression}{Negative Binomial Regression} later.

\hypertarget{poisson-model}{%
\subsection{Poisson Model}\label{poisson-model}}

We model the expected count \(\mu_i\) as a function of predictors \(\mathbf{x_i}\) and parameters \(\boldsymbol{\theta}\):

\[
\mu_i = f(\mathbf{x_i; \theta})
\]

\hypertarget{link-function-choices}{%
\subsection{Link Function Choices}\label{link-function-choices}}

Since \(\mu_i\) must be positive, we often use a \textbf{log-link function}:

\[
\log(\mu_i) = \mathbf{x_i' \theta}
\]

This ensures that the predicted counts are always non-negative. This is analogous to \protect\hyperlink{sec-logistic-regression}{logistic regression}, where we use the logit link for binary outcomes.

Rewriting:

\[
\mu_i = \exp(\mathbf{x_i' \theta})
\] which ensures \(\mu_i > 0\) for all parameter values.

\hypertarget{application-poisson-regression}{%
\subsection{Application: Poisson Regression}\label{application-poisson-regression}}

We apply Poisson regression to the \textbf{bioChemists} dataset (from the \texttt{pscl} package), which contains information on academic productivity in terms of published articles.

\hypertarget{dataset-overview-1}{%
\subsubsection{Dataset Overview}\label{dataset-overview-1}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\CommentTok{\# Load dataset}
\FunctionTok{data}\NormalTok{(bioChemists, }\AttributeTok{package =} \StringTok{"pscl"}\NormalTok{)}

\CommentTok{\# Rename columns for clarity}
\NormalTok{bioChemists }\OtherTok{\textless{}{-}}\NormalTok{ bioChemists }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{(}
    \AttributeTok{Num\_Article =}\NormalTok{ art,}
    \CommentTok{\# Number of articles in last 3 years}
    \AttributeTok{Sex =}\NormalTok{ fem,}
    \CommentTok{\# 1 if female, 0 if male}
    \AttributeTok{Married =}\NormalTok{ mar,}
    \CommentTok{\# 1 if married, 0 otherwise}
    \AttributeTok{Num\_Kid5 =}\NormalTok{ kid5,}
    \CommentTok{\# Number of children under age 6}
    \AttributeTok{PhD\_Quality =}\NormalTok{ phd,}
    \CommentTok{\# Prestige of PhD program}
    \AttributeTok{Num\_MentArticle =}\NormalTok{ ment   }\CommentTok{\# Number of articles by mentor in last 3 years}
\NormalTok{  )}

\CommentTok{\# Visualize response variable distribution}
\FunctionTok{hist}\NormalTok{(bioChemists}\SpecialCharTok{$}\NormalTok{Num\_Article, }
     \AttributeTok{breaks =} \DecValTok{25}\NormalTok{, }
     \AttributeTok{main =} \StringTok{"Number of Articles Published"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{07-generalized-linear-models_files/figure-latex/unnamed-chunk-18-1} \end{center}

The \textbf{distribution of the number of articles} is right-skewed, which suggests a Poisson model may be appropriate.

\hypertarget{fitting-a-poisson-regression-model}{%
\subsubsection{Fitting a Poisson Regression Model}\label{fitting-a-poisson-regression-model}}

We model the number of articles published (\texttt{Num\_Article}) as a function of various predictors.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Poisson regression model}
\NormalTok{Poisson\_Mod }\OtherTok{\textless{}{-}}
  \FunctionTok{glm}\NormalTok{(Num\_Article }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .,}
      \AttributeTok{family =}\NormalTok{ poisson,}
      \AttributeTok{data =}\NormalTok{ bioChemists)}

\CommentTok{\# Summary of the model}
\FunctionTok{summary}\NormalTok{(Poisson\_Mod)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} glm(formula = Num\_Article \textasciitilde{} ., family = poisson, data = bioChemists)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Deviance Residuals: }
\CommentTok{\#\textgreater{}     Min       1Q   Median       3Q      Max  }
\CommentTok{\#\textgreater{} {-}3.5672  {-}1.5398  {-}0.3660   0.5722   5.4467  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}                  Estimate Std. Error z value Pr(\textgreater{}|z|)    }
\CommentTok{\#\textgreater{} (Intercept)      0.304617   0.102981   2.958   0.0031 ** }
\CommentTok{\#\textgreater{} SexWomen        {-}0.224594   0.054613  {-}4.112 3.92e{-}05 ***}
\CommentTok{\#\textgreater{} MarriedMarried   0.155243   0.061374   2.529   0.0114 *  }
\CommentTok{\#\textgreater{} Num\_Kid5        {-}0.184883   0.040127  {-}4.607 4.08e{-}06 ***}
\CommentTok{\#\textgreater{} PhD\_Quality      0.012823   0.026397   0.486   0.6271    }
\CommentTok{\#\textgreater{} Num\_MentArticle  0.025543   0.002006  12.733  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} (Dispersion parameter for poisson family taken to be 1)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Null deviance: 1817.4  on 914  degrees of freedom}
\CommentTok{\#\textgreater{} Residual deviance: 1634.4  on 909  degrees of freedom}
\CommentTok{\#\textgreater{} AIC: 3314.1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Fisher Scoring iterations: 5}
\end{Highlighting}
\end{Shaded}

Interpretation:

\begin{itemize}
\item
  \textbf{Coefficients} are on the \textbf{log scale}, meaning they represent \textbf{log-rate ratios}.
\item
  \textbf{Exponentiating} the coefficients gives the \textbf{rate ratios}.
\item
  \textbf{Statistical significance} tells us whether each variable has a meaningful impact on publication count.
\end{itemize}

\hypertarget{model-diagnostics-goodness-of-fit}{%
\subsubsection{Model Diagnostics: Goodness of Fit}\label{model-diagnostics-goodness-of-fit}}

\hypertarget{pearsons-chi-square-test-for-overdispersion}{%
\paragraph{Pearson's Chi-Square Test for Overdispersion}\label{pearsons-chi-square-test-for-overdispersion}}

We compute the \textbf{Pearson chi-square statistic} to check whether the variance significantly exceeds the mean. \[
X^2 = \sum \frac{(Y_i - \hat{\mu}_i)^2}{\hat{\mu}_i}
\]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Compute predicted means}
\NormalTok{Predicted\_Means }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(Poisson\_Mod, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}

\CommentTok{\# Pearson chi{-}square test}
\NormalTok{X2 }\OtherTok{\textless{}{-}}
  \FunctionTok{sum}\NormalTok{((bioChemists}\SpecialCharTok{$}\NormalTok{Num\_Article }\SpecialCharTok{{-}}\NormalTok{ Predicted\_Means) }\SpecialCharTok{\^{}} \DecValTok{2} \SpecialCharTok{/}\NormalTok{ Predicted\_Means)}
\NormalTok{X2}
\CommentTok{\#\textgreater{} [1] 1662.547}
\FunctionTok{pchisq}\NormalTok{(X2, Poisson\_Mod}\SpecialCharTok{$}\NormalTok{df.residual, }\AttributeTok{lower.tail =} \ConstantTok{FALSE}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 7.849882e{-}47}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  If \textbf{p-value is small}, overdispersion is present.
\item
  Large \textbf{X statistic} suggests the model may not adequately capture variability.
\end{itemize}

\hypertarget{overdispersion-check-ratio-of-deviance-to-degrees-of-freedom}{%
\paragraph{Overdispersion Check: Ratio of Deviance to Degrees of Freedom}\label{overdispersion-check-ratio-of-deviance-to-degrees-of-freedom}}

We compute: \[
\hat{\phi} = \frac{\text{deviance}}{\text{degrees of freedom}}
\]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Overdispersion check}
\NormalTok{Poisson\_Mod}\SpecialCharTok{$}\NormalTok{deviance }\SpecialCharTok{/}\NormalTok{ Poisson\_Mod}\SpecialCharTok{$}\NormalTok{df.residual}
\CommentTok{\#\textgreater{} [1] 1.797988}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  If \(\hat{\phi} > 1\), overdispersion is likely present.
\item
  A value significantly above 1 suggests the need for an alternative model.
\end{itemize}

\hypertarget{addressing-overdispersion}{%
\subsubsection{Addressing Overdispersion}\label{addressing-overdispersion}}

\hypertarget{including-interaction-terms}{%
\paragraph{Including Interaction Terms}\label{including-interaction-terms}}

One possible remedy is to incorporate \textbf{interaction terms}, capturing complex relationships between predictors.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Adding two{-}way and three{-}way interaction terms}
\NormalTok{Poisson\_Mod\_All2way }\OtherTok{\textless{}{-}}
  \FunctionTok{glm}\NormalTok{(Num\_Article }\SpecialCharTok{\textasciitilde{}}\NormalTok{ . }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{, }\AttributeTok{family =}\NormalTok{ poisson, }\AttributeTok{data =}\NormalTok{ bioChemists)}
\NormalTok{Poisson\_Mod\_All3way }\OtherTok{\textless{}{-}}
  \FunctionTok{glm}\NormalTok{(Num\_Article }\SpecialCharTok{\textasciitilde{}}\NormalTok{ . }\SpecialCharTok{\^{}} \DecValTok{3}\NormalTok{, }\AttributeTok{family =}\NormalTok{ poisson, }\AttributeTok{data =}\NormalTok{ bioChemists)}
\end{Highlighting}
\end{Shaded}

This may improve model fit, but can lead to overfitting.

\hypertarget{quasi-poisson-model-adjusting-for-overdispersion}{%
\paragraph{Quasi-Poisson Model (Adjusting for Overdispersion)}\label{quasi-poisson-model-adjusting-for-overdispersion}}

A quick fix is to \textbf{allow the variance to scale} by introducing \(\hat{\phi}\):

\[
\text{Var}(Y_i) = \hat{\phi} \mu_i
\]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Estimate dispersion parameter}
\NormalTok{phi\_hat }\OtherTok{=}\NormalTok{ Poisson\_Mod}\SpecialCharTok{$}\NormalTok{deviance }\SpecialCharTok{/}\NormalTok{ Poisson\_Mod}\SpecialCharTok{$}\NormalTok{df.residual}

\CommentTok{\# Adjusting Poisson model to account for overdispersion}
\FunctionTok{summary}\NormalTok{(Poisson\_Mod, }\AttributeTok{dispersion =}\NormalTok{ phi\_hat)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} glm(formula = Num\_Article \textasciitilde{} ., family = poisson, data = bioChemists)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Deviance Residuals: }
\CommentTok{\#\textgreater{}     Min       1Q   Median       3Q      Max  }
\CommentTok{\#\textgreater{} {-}3.5672  {-}1.5398  {-}0.3660   0.5722   5.4467  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}                 Estimate Std. Error z value Pr(\textgreater{}|z|)    }
\CommentTok{\#\textgreater{} (Intercept)      0.30462    0.13809   2.206  0.02739 *  }
\CommentTok{\#\textgreater{} SexWomen        {-}0.22459    0.07323  {-}3.067  0.00216 ** }
\CommentTok{\#\textgreater{} MarriedMarried   0.15524    0.08230   1.886  0.05924 .  }
\CommentTok{\#\textgreater{} Num\_Kid5        {-}0.18488    0.05381  {-}3.436  0.00059 ***}
\CommentTok{\#\textgreater{} PhD\_Quality      0.01282    0.03540   0.362  0.71715    }
\CommentTok{\#\textgreater{} Num\_MentArticle  0.02554    0.00269   9.496  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} (Dispersion parameter for poisson family taken to be 1.797988)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Null deviance: 1817.4  on 914  degrees of freedom}
\CommentTok{\#\textgreater{} Residual deviance: 1634.4  on 909  degrees of freedom}
\CommentTok{\#\textgreater{} AIC: 3314.1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Fisher Scoring iterations: 5}
\end{Highlighting}
\end{Shaded}

Alternatively, we refit using a \textbf{Quasi-Poisson model}, which adjusts standard errors:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Quasi{-}Poisson model}
\NormalTok{quasiPoisson\_Mod }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(Num\_Article }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{family =}\NormalTok{ quasipoisson, }\AttributeTok{data =}\NormalTok{ bioChemists)}
\FunctionTok{summary}\NormalTok{(quasiPoisson\_Mod)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} glm(formula = Num\_Article \textasciitilde{} ., family = quasipoisson, data = bioChemists)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Deviance Residuals: }
\CommentTok{\#\textgreater{}     Min       1Q   Median       3Q      Max  }
\CommentTok{\#\textgreater{} {-}3.5672  {-}1.5398  {-}0.3660   0.5722   5.4467  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}                  Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)      0.304617   0.139273   2.187 0.028983 *  }
\CommentTok{\#\textgreater{} SexWomen        {-}0.224594   0.073860  {-}3.041 0.002427 ** }
\CommentTok{\#\textgreater{} MarriedMarried   0.155243   0.083003   1.870 0.061759 .  }
\CommentTok{\#\textgreater{} Num\_Kid5        {-}0.184883   0.054268  {-}3.407 0.000686 ***}
\CommentTok{\#\textgreater{} PhD\_Quality      0.012823   0.035700   0.359 0.719544    }
\CommentTok{\#\textgreater{} Num\_MentArticle  0.025543   0.002713   9.415  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} (Dispersion parameter for quasipoisson family taken to be 1.829006)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Null deviance: 1817.4  on 914  degrees of freedom}
\CommentTok{\#\textgreater{} Residual deviance: 1634.4  on 909  degrees of freedom}
\CommentTok{\#\textgreater{} AIC: NA}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Fisher Scoring iterations: 5}
\end{Highlighting}
\end{Shaded}

While Quasi-Poisson corrects standard errors, \textbf{it does not introduce an extra parameter} for overdispersion.

\hypertarget{negative-binomial-regression-preferred-approach}{%
\paragraph{Negative Binomial Regression (Preferred Approach)}\label{negative-binomial-regression-preferred-approach}}

A \protect\hyperlink{sec-negative-binomial-regression}{Negative Binomial Regression} explicitly models overdispersion by introducing a \textbf{dispersion parameter} \(\theta\):

\[
\text{Var}(Y_i) = \mu_i + \theta \mu_i^2
\]

This extends Poisson regression by allowing the variance to \textbf{grow quadratically} rather than linearly.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load MASS package}
\FunctionTok{library}\NormalTok{(MASS)}

\CommentTok{\# Fit Negative Binomial regression}
\NormalTok{NegBin\_Mod }\OtherTok{\textless{}{-}} \FunctionTok{glm.nb}\NormalTok{(Num\_Article }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ bioChemists)}

\CommentTok{\# Model summary}
\FunctionTok{summary}\NormalTok{(NegBin\_Mod)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} glm.nb(formula = Num\_Article \textasciitilde{} ., data = bioChemists, init.theta = 2.264387695, }
\CommentTok{\#\textgreater{}     link = log)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Deviance Residuals: }
\CommentTok{\#\textgreater{}     Min       1Q   Median       3Q      Max  }
\CommentTok{\#\textgreater{} {-}2.1678  {-}1.3617  {-}0.2806   0.4476   3.4524  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}                  Estimate Std. Error z value Pr(\textgreater{}|z|)    }
\CommentTok{\#\textgreater{} (Intercept)      0.256144   0.137348   1.865 0.062191 .  }
\CommentTok{\#\textgreater{} SexWomen        {-}0.216418   0.072636  {-}2.979 0.002887 ** }
\CommentTok{\#\textgreater{} MarriedMarried   0.150489   0.082097   1.833 0.066791 .  }
\CommentTok{\#\textgreater{} Num\_Kid5        {-}0.176415   0.052813  {-}3.340 0.000837 ***}
\CommentTok{\#\textgreater{} PhD\_Quality      0.015271   0.035873   0.426 0.670326    }
\CommentTok{\#\textgreater{} Num\_MentArticle  0.029082   0.003214   9.048  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} (Dispersion parameter for Negative Binomial(2.2644) family taken to be 1)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Null deviance: 1109.0  on 914  degrees of freedom}
\CommentTok{\#\textgreater{} Residual deviance: 1004.3  on 909  degrees of freedom}
\CommentTok{\#\textgreater{} AIC: 3135.9}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Fisher Scoring iterations: 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}               Theta:  2.264 }
\CommentTok{\#\textgreater{}           Std. Err.:  0.271 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  2 x log{-}likelihood:  {-}3121.917}
\end{Highlighting}
\end{Shaded}

This model is generally \textbf{preferred over Quasi-Poisson}, as it explicitly accounts for \textbf{heterogeneity} in the data.

\hypertarget{sec-negative-binomial-regression}{%
\section{Negative Binomial Regression}\label{sec-negative-binomial-regression}}

When modeling \textbf{count data}, \protect\hyperlink{sec-poisson-regression}{Poisson regression} assumes that the \textbf{mean and variance are equal}:

\[
\text{Var}(Y_i) = E(Y_i) = \mu_i
\] However, in many real-world datasets, the variance exceeds the mean---a phenomenon known as \textbf{overdispersion}. When overdispersion is present, the \textbf{Poisson model underestimates the variance}, leading to:

\begin{itemize}
\item
  \textbf{Inflated test statistics} (small p-values).
\item
  \textbf{Overconfident predictions}.
\item
  \textbf{Poor model fit}.
\end{itemize}

\hypertarget{negative-binomial-distribution}{%
\subsection{Negative Binomial Distribution}\label{negative-binomial-distribution}}

To address overdispersion, \textbf{Negative Binomial (NB) regression} introduces an extra \textbf{dispersion parameter} \(\theta\) to allow variance to be greater than the mean: \[
\text{Var}(Y_i) = \mu_i + \theta \mu_i^2
\] where:

\begin{itemize}
\item
  \(\mu_i = \exp(\mathbf{x_i' \theta})\) is the expected count.
\item
  \(\theta\) is the dispersion parameter.
\item
  When \(\theta \to 0\), the NB model reduces to the \protect\hyperlink{poisson-model}{Poisson model}.
\end{itemize}

Thus, Negative Binomial regression is a \textbf{generalization of Poisson regression} that accounts for overdispersion.

\hypertarget{application-negative-binomial-regression}{%
\subsection{Application: Negative Binomial Regression}\label{application-negative-binomial-regression}}

We apply \textbf{Negative Binomial regression} to the \texttt{bioChemists} dataset to model the number of research articles (\texttt{Num\_Article}) as a function of several predictors.

\hypertarget{fitting-the-negative-binomial-model}{%
\subsubsection{Fitting the Negative Binomial Model}\label{fitting-the-negative-binomial-model}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load necessary package}
\FunctionTok{library}\NormalTok{(MASS)}

\CommentTok{\# Fit Negative Binomial model}
\NormalTok{NegBinom\_Mod }\OtherTok{\textless{}{-}}\NormalTok{ MASS}\SpecialCharTok{::}\FunctionTok{glm.nb}\NormalTok{(Num\_Article }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ bioChemists)}

\CommentTok{\# Model summary}
\FunctionTok{summary}\NormalTok{(NegBinom\_Mod)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} MASS::glm.nb(formula = Num\_Article \textasciitilde{} ., data = bioChemists, init.theta = 2.264387695, }
\CommentTok{\#\textgreater{}     link = log)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Deviance Residuals: }
\CommentTok{\#\textgreater{}     Min       1Q   Median       3Q      Max  }
\CommentTok{\#\textgreater{} {-}2.1678  {-}1.3617  {-}0.2806   0.4476   3.4524  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}                  Estimate Std. Error z value Pr(\textgreater{}|z|)    }
\CommentTok{\#\textgreater{} (Intercept)      0.256144   0.137348   1.865 0.062191 .  }
\CommentTok{\#\textgreater{} SexWomen        {-}0.216418   0.072636  {-}2.979 0.002887 ** }
\CommentTok{\#\textgreater{} MarriedMarried   0.150489   0.082097   1.833 0.066791 .  }
\CommentTok{\#\textgreater{} Num\_Kid5        {-}0.176415   0.052813  {-}3.340 0.000837 ***}
\CommentTok{\#\textgreater{} PhD\_Quality      0.015271   0.035873   0.426 0.670326    }
\CommentTok{\#\textgreater{} Num\_MentArticle  0.029082   0.003214   9.048  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} (Dispersion parameter for Negative Binomial(2.2644) family taken to be 1)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Null deviance: 1109.0  on 914  degrees of freedom}
\CommentTok{\#\textgreater{} Residual deviance: 1004.3  on 909  degrees of freedom}
\CommentTok{\#\textgreater{} AIC: 3135.9}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Fisher Scoring iterations: 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}               Theta:  2.264 }
\CommentTok{\#\textgreater{}           Std. Err.:  0.271 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  2 x log{-}likelihood:  {-}3121.917}
\end{Highlighting}
\end{Shaded}

Interpretation:

\begin{itemize}
\item
  The coefficients are on the log scale.
\item
  The dispersion parameter \(\theta\) (also called size parameter in some contexts) is estimated as 2.264 with a standard error of 0.271. Check \protect\hyperlink{over-dispersion}{Over-Dispersion} for more detail.
\item
  Since \(\theta\) is significantly different from 1, this confirms overdispersion, validating the choice of the Negative Binomial model over Poisson regression.
\end{itemize}

\hypertarget{model-comparison-poisson-vs.-negative-binomial}{%
\subsubsection{Model Comparison: Poisson vs.~Negative Binomial}\label{model-comparison-poisson-vs.-negative-binomial}}

\hypertarget{checking-overdispersion-in-poisson-model}{%
\paragraph{Checking Overdispersion in Poisson Model}\label{checking-overdispersion-in-poisson-model}}

Before using NB regression, we confirm \textbf{overdispersion} by computing:

\[
\hat{\phi} = \frac{\text{deviance}}{\text{degrees of freedom}}
\]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Overdispersion check for Poisson model}
\NormalTok{Poisson\_Mod}\SpecialCharTok{$}\NormalTok{deviance }\SpecialCharTok{/}\NormalTok{ Poisson\_Mod}\SpecialCharTok{$}\NormalTok{df.residual}
\CommentTok{\#\textgreater{} [1] 1.797988}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  If \(\hat{\phi} > 1\), overdispersion is present.
\item
  A large value suggests that \textbf{Poisson regression underestimates variance}.
\end{itemize}

\hypertarget{likelihood-ratio-test-poisson-vs.-negative-binomial}{%
\paragraph{Likelihood Ratio Test: Poisson vs.~Negative Binomial}\label{likelihood-ratio-test-poisson-vs.-negative-binomial}}

We compare the Poisson and Negative Binomial models using a \textbf{likelihood ratio test}, where: \[
G^2 = 2 \times ( \log L_{NB} - \log L_{Poisson})
\] with \(\text{df} = 1\)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Likelihood ratio test between Poisson and Negative Binomial}
\FunctionTok{pchisq}\NormalTok{(}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ (}\FunctionTok{logLik}\NormalTok{(NegBinom\_Mod) }\SpecialCharTok{{-}} \FunctionTok{logLik}\NormalTok{(Poisson\_Mod)),}
       \AttributeTok{df =} \DecValTok{1}\NormalTok{,}
       \AttributeTok{lower.tail =} \ConstantTok{FALSE}\NormalTok{)}
\CommentTok{\#\textgreater{} \textquotesingle{}log Lik.\textquotesingle{} 4.391728e{-}41 (df=7)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  \textbf{Small p-value (\textless{} 0.05)}  Negative Binomial model is \textbf{significantly better}.
\item
  \textbf{Large p-value (\textgreater{} 0.05)}  Poisson model \textbf{is adequate}.
\end{itemize}

Since \textbf{overdispersion is confirmed}, the Negative Binomial model is preferred.

\hypertarget{model-diagnostics-and-evaluation}{%
\subsubsection{Model Diagnostics and Evaluation}\label{model-diagnostics-and-evaluation}}

\hypertarget{checking-dispersion-parameter-theta}{%
\paragraph{\texorpdfstring{Checking Dispersion Parameter \(\theta\)}{Checking Dispersion Parameter \textbackslash theta}}\label{checking-dispersion-parameter-theta}}

The Negative Binomial dispersion parameter \(\theta\) can be retrieved:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Extract dispersion parameter estimate}
\NormalTok{NegBinom\_Mod}\SpecialCharTok{$}\NormalTok{theta}
\CommentTok{\#\textgreater{} [1] 2.264388}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  A large \(\theta\) suggests that overdispersion is \textbf{not extreme}, while a small \(\theta\) (close to 0) would indicate the Poisson model is reasonable.
\end{itemize}

\hypertarget{predictions-and-rate-ratios}{%
\subsubsection{Predictions and Rate Ratios}\label{predictions-and-rate-ratios}}

In Negative Binomial regression, exponentiating the coefficients gives \textbf{rate ratios}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Convert coefficients to rate ratios}
\FunctionTok{data.frame}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Odds Ratios}\StringTok{\textasciigrave{}} \OtherTok{=} \FunctionTok{exp}\NormalTok{(}\FunctionTok{coef}\NormalTok{(NegBinom\_Mod)))}
\CommentTok{\#\textgreater{}                 Odds.Ratios}
\CommentTok{\#\textgreater{} (Intercept)       1.2919388}
\CommentTok{\#\textgreater{} SexWomen          0.8053982}
\CommentTok{\#\textgreater{} MarriedMarried    1.1624030}
\CommentTok{\#\textgreater{} Num\_Kid5          0.8382698}
\CommentTok{\#\textgreater{} PhD\_Quality       1.0153884}
\CommentTok{\#\textgreater{} Num\_MentArticle   1.0295094}
\end{Highlighting}
\end{Shaded}

A rate ratio of:

\begin{itemize}
\item
  \textbf{\textgreater{} 1} \(\to\) Increases expected article count.
\item
  \textbf{\textless{} 1} \(\to\) Decreases expected article count.
\item
  \textbf{= 1} \(\to\) No effect.
\end{itemize}

For example:

\begin{itemize}
\item
  If \texttt{PhD\_Quality} has an exponentiated coefficient of \textbf{1.5}, individuals from higher-quality PhD programs are expected to publish \textbf{50\% more articles}.
\item
  If \texttt{Sex} has an exponentiated coefficient of \textbf{0.8}, females publish \textbf{20\% fewer articles} than males, all else equal.
\end{itemize}

\hypertarget{alternative-approach-zero-inflated-models}{%
\subsubsection{Alternative Approach: Zero-Inflated Models}\label{alternative-approach-zero-inflated-models}}

If a dataset has \textbf{excess zeros} (many individuals publish \textbf{no articles}), \textbf{Zero-Inflated Negative Binomial (ZINB) models} may be required.

\[
\text{P}(Y_i = 0) = p + (1 - p) f(Y_i = 0 | \mu, \theta)
\]

where:

\begin{itemize}
\item
  \(p\) is the probability of \textbf{always being a zero} (e.g., inactive researchers).
\item
  \(f(Y_i)\) follows the \textbf{Negative Binomial distribution}.
\end{itemize}

\hypertarget{fitting-a-zero-inflated-negative-binomial-model}{%
\subsection{Fitting a Zero-Inflated Negative Binomial Model}\label{fitting-a-zero-inflated-negative-binomial-model}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load package for zero{-}inflated models}
\FunctionTok{library}\NormalTok{(pscl)}

\CommentTok{\# Fit ZINB model}
\NormalTok{ZINB\_Mod }\OtherTok{\textless{}{-}} \FunctionTok{zeroinfl}\NormalTok{(Num\_Article }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ bioChemists, }\AttributeTok{dist =} \StringTok{"negbin"}\NormalTok{)}

\CommentTok{\# Model summary}
\FunctionTok{summary}\NormalTok{(ZINB\_Mod)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} zeroinfl(formula = Num\_Article \textasciitilde{} ., data = bioChemists, dist = "negbin")}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Pearson residuals:}
\CommentTok{\#\textgreater{}     Min      1Q  Median      3Q     Max }
\CommentTok{\#\textgreater{} {-}1.2942 {-}0.7601 {-}0.2909  0.4448  6.4155 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Count model coefficients (negbin with log link):}
\CommentTok{\#\textgreater{}                   Estimate Std. Error z value Pr(\textgreater{}|z|)    }
\CommentTok{\#\textgreater{} (Intercept)      0.4167466  0.1435964   2.902  0.00371 ** }
\CommentTok{\#\textgreater{} SexWomen        {-}0.1955076  0.0755926  {-}2.586  0.00970 ** }
\CommentTok{\#\textgreater{} MarriedMarried   0.0975826  0.0844520   1.155  0.24789    }
\CommentTok{\#\textgreater{} Num\_Kid5        {-}0.1517321  0.0542061  {-}2.799  0.00512 ** }
\CommentTok{\#\textgreater{} PhD\_Quality     {-}0.0006998  0.0362697  {-}0.019  0.98461    }
\CommentTok{\#\textgreater{} Num\_MentArticle  0.0247862  0.0034927   7.097 1.28e{-}12 ***}
\CommentTok{\#\textgreater{} Log(theta)       0.9763577  0.1354696   7.207 5.71e{-}13 ***}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Zero{-}inflation model coefficients (binomial with logit link):}
\CommentTok{\#\textgreater{}                 Estimate Std. Error z value Pr(\textgreater{}|z|)   }
\CommentTok{\#\textgreater{} (Intercept)     {-}0.19161    1.32280  {-}0.145  0.88483   }
\CommentTok{\#\textgreater{} SexWomen         0.63587    0.84890   0.749  0.45382   }
\CommentTok{\#\textgreater{} MarriedMarried  {-}1.49944    0.93866  {-}1.597  0.11017   }
\CommentTok{\#\textgreater{} Num\_Kid5         0.62841    0.44277   1.419  0.15583   }
\CommentTok{\#\textgreater{} PhD\_Quality     {-}0.03773    0.30801  {-}0.123  0.90250   }
\CommentTok{\#\textgreater{} Num\_MentArticle {-}0.88227    0.31622  {-}2.790  0.00527 **}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Theta = 2.6548 }
\CommentTok{\#\textgreater{} Number of iterations in BFGS optimization: 27 }
\CommentTok{\#\textgreater{} Log{-}likelihood: {-}1550 on 13 Df}
\end{Highlighting}
\end{Shaded}

This model accounts for:

\begin{itemize}
\item
  Structural \textbf{zero inflation}.
\item
  Overdispersion.
\end{itemize}

ZINB is often preferred when many observations are zero. However, since ZINB does not fall under the GLM framework, we will discuss it further in \protect\hyperlink{sec-nonlinear-and-generalized-linear-mixed-models}{Nonlinear and Generalized Linear Mixed Models}.

\textbf{Why ZINB is Not a GLM?}

\begin{itemize}
\item
  Unlike GLMs, which assume a single response distribution from the exponential family, ZINB is a mixture model with two components:

  \begin{itemize}
  \item
    Count model -- A negative binomial regression for the main count process.
  \item
    Inflation model -- A logistic regression for excess zeros.
  \end{itemize}
\end{itemize}

Because ZINB combines two distinct processes rather than using a single exponential family distribution, it does not fit within the standard GLM framework.

\textbf{What ZINB Belongs To}

ZINB is part of finite mixture models and is sometimes considered within generalized linear mixed models (GLMMs) or semi-parametric models.

\hypertarget{sec-quasi-poisson-regression}{%
\section{Quasi-Poisson Regression}\label{sec-quasi-poisson-regression}}

Poisson regression assumes that the \textbf{mean and variance are equal}:

\[
\text{Var}(Y_i) = E(Y_i) = \mu_i
\]

However, many real-world datasets exhibit \textbf{overdispersion}, where the variance exceeds the mean:

\[
\text{Var}(Y_i) = \phi \mu_i
\]

where \(\phi\) (the \textbf{dispersion parameter}) allows the variance to scale beyond the Poisson assumption.

To correct for this, we use Quasi-Poisson regression, which:

\begin{itemize}
\item
  Follows the \protect\hyperlink{generalized-linear-models}{Generalized Linear Models} structure but is not a strict GLM.
\item
  Uses a variance function proportional to the mean: \(\text{Var}(Y_i) = \phi \mu_i\).
\item
  Does not assume a specific probability distribution, unlike Poisson or Negative Binomial models.
\end{itemize}

\hypertarget{is-quasi-poisson-regression-a-generalized-linear-model}{%
\subsection{\texorpdfstring{Is Quasi-Poisson Regression a \protect\hyperlink{generalized-linear-models}{Generalized Linear Model}?}{Is Quasi-Poisson Regression a Generalized Linear Model?}}\label{is-quasi-poisson-regression-a-generalized-linear-model}}

 Yes, Quasi-Poisson is GLM-like:

\begin{itemize}
\item
  \textbf{Linear Predictor:} Like Poisson regression, it models the log of the expected count as a function of predictors: \[
  \log(E(Y)) = X\beta
  \]
\item
  \textbf{Canonical Link Function:} It typically uses a log link function, just like standard Poisson regression.
\item
  \textbf{Variance Structure:} Unlike standard Poisson, which assumes \(\text{Var}(Y) = E(Y)\), Quasi-Poisson allows for \textbf{overdispersion}: \[
  \text{Var}(Y) = \phi E(Y)
  \] where \(\phi\) is estimated rather than assumed to be 1.
\end{itemize}

 No, Quasi-Poisson is not a strict GLM because:

\begin{itemize}
\item
  GLMs require a full probability distribution from the exponential family.

  \begin{itemize}
  \item
    Standard Poisson regression assumes a Poisson distribution (which belongs to the exponential family).
  \item
    Quasi-Poisson does not assume a full probability distribution, only a mean-variance relationship.
  \end{itemize}
\item
  It does not use \protect\hyperlink{maximum-likelihood-estimator}{Maximum Likelihood} Estimation.

  \begin{itemize}
  \item
    Standard GLMs use MLE to estimate parameters.
  \item
    Quasi-Poisson uses quasi-likelihood methods, which require specifying only the mean and variance, but not a full likelihood function.
  \end{itemize}
\item
  Likelihood-based inference is not valid.

  \begin{itemize}
  \tightlist
  \item
    AIC, BIC, and Likelihood Ratio Tests cannot be used with Quasi-Poisson regression.
  \end{itemize}
\end{itemize}

\textbf{When to Use Quasi-Poisson:}

\begin{itemize}
\item
  When \textbf{data exhibit overdispersion} (variance \textgreater{} mean), making standard Poisson regression inappropriate.
\item
  When \protect\hyperlink{sec-negative-binomial-regression}{Negative Binomial Regression} is not preferred, but an alternative is needed to handle overdispersion.
\item
  If overdispersion is present, \protect\hyperlink{sec-negative-binomial-regression}{Negative Binomial Regression} is often a better alternative because it is a true GLM with a full likelihood function, whereas Quasi-Poisson is only a quasi-likelihood approach.
\end{itemize}

\hypertarget{application-quasi-poisson-regression}{%
\subsection{Application: Quasi-Poisson Regression}\label{application-quasi-poisson-regression}}

We analyze the \texttt{bioChemists} dataset, modeling the number of published articles (\texttt{Num\_Article}) as a function of various predictors.

\hypertarget{checking-overdispersion-in-the-poisson-model}{%
\subsubsection{Checking Overdispersion in the Poisson Model}\label{checking-overdispersion-in-the-poisson-model}}

We first fit a \protect\hyperlink{sec-poisson-regression}{Poisson regression model} and check for overdispersion using the deviance-to-degrees-of-freedom ratio:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fit Poisson regression model}
\NormalTok{Poisson\_Mod }\OtherTok{\textless{}{-}}
    \FunctionTok{glm}\NormalTok{(Num\_Article }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{family =}\NormalTok{ poisson, }\AttributeTok{data =}\NormalTok{ bioChemists)}

\CommentTok{\# Compute dispersion parameter}
\NormalTok{dispersion\_estimate }\OtherTok{\textless{}{-}}
\NormalTok{    Poisson\_Mod}\SpecialCharTok{$}\NormalTok{deviance }\SpecialCharTok{/}\NormalTok{ Poisson\_Mod}\SpecialCharTok{$}\NormalTok{df.residual}
\NormalTok{dispersion\_estimate}
\CommentTok{\#\textgreater{} [1] 1.797988}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  If \(\hat{\phi} > 1\), the Poisson model underestimates variance.
\item
  A \textbf{large value (\textgreater\textgreater{} 1)} suggests that Poisson regression \textbf{is not appropriate}.
\end{itemize}

\hypertarget{fitting-the-quasi-poisson-model}{%
\subsubsection{Fitting the Quasi-Poisson Model}\label{fitting-the-quasi-poisson-model}}

Since overdispersion is present, we refit the model using \protect\hyperlink{sec-quasi-poisson-regression}{Quasi-Poisson regression}, which scales standard errors by \(\phi\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fit Quasi{-}Poisson regression model}
\NormalTok{quasiPoisson\_Mod }\OtherTok{\textless{}{-}}
    \FunctionTok{glm}\NormalTok{(Num\_Article }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{family =}\NormalTok{ quasipoisson, }\AttributeTok{data =}\NormalTok{ bioChemists)}

\CommentTok{\# Summary of the model}
\FunctionTok{summary}\NormalTok{(quasiPoisson\_Mod)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} glm(formula = Num\_Article \textasciitilde{} ., family = quasipoisson, data = bioChemists)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Deviance Residuals: }
\CommentTok{\#\textgreater{}     Min       1Q   Median       3Q      Max  }
\CommentTok{\#\textgreater{} {-}3.5672  {-}1.5398  {-}0.3660   0.5722   5.4467  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}                  Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)      0.304617   0.139273   2.187 0.028983 *  }
\CommentTok{\#\textgreater{} SexWomen        {-}0.224594   0.073860  {-}3.041 0.002427 ** }
\CommentTok{\#\textgreater{} MarriedMarried   0.155243   0.083003   1.870 0.061759 .  }
\CommentTok{\#\textgreater{} Num\_Kid5        {-}0.184883   0.054268  {-}3.407 0.000686 ***}
\CommentTok{\#\textgreater{} PhD\_Quality      0.012823   0.035700   0.359 0.719544    }
\CommentTok{\#\textgreater{} Num\_MentArticle  0.025543   0.002713   9.415  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} (Dispersion parameter for quasipoisson family taken to be 1.829006)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Null deviance: 1817.4  on 914  degrees of freedom}
\CommentTok{\#\textgreater{} Residual deviance: 1634.4  on 909  degrees of freedom}
\CommentTok{\#\textgreater{} AIC: NA}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Fisher Scoring iterations: 5}
\end{Highlighting}
\end{Shaded}

Interpretation:

\begin{itemize}
\item
  The \textbf{coefficients remain the same} as in Poisson regression.
\item
  \textbf{Standard errors are inflated} to account for overdispersion.
\item
  \textbf{P-values increase}, leading to more conservative inference.
\end{itemize}

\hypertarget{comparing-poisson-and-quasi-poisson}{%
\subsubsection{Comparing Poisson and Quasi-Poisson}\label{comparing-poisson-and-quasi-poisson}}

To see the effect of using \textbf{Quasi-Poisson}, we compare \textbf{standard errors}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Extract coefficients and standard errors}
\NormalTok{poisson\_se }\OtherTok{\textless{}{-}} \FunctionTok{summary}\NormalTok{(Poisson\_Mod)}\SpecialCharTok{$}\NormalTok{coefficients[, }\DecValTok{2}\NormalTok{]}
\NormalTok{quasi\_se }\OtherTok{\textless{}{-}} \FunctionTok{summary}\NormalTok{(quasiPoisson\_Mod)}\SpecialCharTok{$}\NormalTok{coefficients[, }\DecValTok{2}\NormalTok{]}

\CommentTok{\# Compare standard errors}
\NormalTok{se\_comparison }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{Poisson =}\NormalTok{ poisson\_se,}
                            \AttributeTok{Quasi\_Poisson =}\NormalTok{ quasi\_se)}
\NormalTok{se\_comparison}
\CommentTok{\#\textgreater{}                     Poisson Quasi\_Poisson}
\CommentTok{\#\textgreater{} (Intercept)     0.102981443   0.139272885}
\CommentTok{\#\textgreater{} SexWomen        0.054613488   0.073859696}
\CommentTok{\#\textgreater{} MarriedMarried  0.061374395   0.083003199}
\CommentTok{\#\textgreater{} Num\_Kid5        0.040126898   0.054267922}
\CommentTok{\#\textgreater{} PhD\_Quality     0.026397045   0.035699564}
\CommentTok{\#\textgreater{} Num\_MentArticle 0.002006073   0.002713028}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  \textbf{Quasi-Poisson has larger standard errors} than Poisson.
\item
  This leads to \textbf{wider confidence intervals}, reducing the likelihood of false positives.
\end{itemize}

\hypertarget{model-diagnostics-checking-residuals}{%
\subsubsection{Model Diagnostics: Checking Residuals}\label{model-diagnostics-checking-residuals}}

We examine \textbf{residuals} to assess model fit:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Residual plot}
\FunctionTok{plot}\NormalTok{(}
\NormalTok{    quasiPoisson\_Mod}\SpecialCharTok{$}\NormalTok{fitted.values,}
    \FunctionTok{residuals}\NormalTok{(quasiPoisson\_Mod, }\AttributeTok{type =} \StringTok{"pearson"}\NormalTok{),}
    \AttributeTok{xlab =} \StringTok{"Fitted Values"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"Pearson Residuals"}\NormalTok{,}
    \AttributeTok{main =} \StringTok{"Residuals vs. Fitted Values (Quasi{-}Poisson)"}
\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{h =} \DecValTok{0}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{07-generalized-linear-models_files/figure-latex/unnamed-chunk-35-1} \end{center}

\begin{itemize}
\item
  If residuals show a \textbf{pattern}, additional predictors or transformations may be needed.
\item
  \textbf{Random scatter around zero} suggests a well-fitting model.
\end{itemize}

\hypertarget{alternative-negative-binomial-vs.-quasi-poisson}{%
\subsubsection{Alternative: Negative Binomial vs.~Quasi-Poisson}\label{alternative-negative-binomial-vs.-quasi-poisson}}

If overdispersion is \textbf{severe}, \protect\hyperlink{sec-negative-binomial-regression}{Negative Binomial regression} may be preferable because it explicitly models dispersion:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fit Negative Binomial model}
\FunctionTok{library}\NormalTok{(MASS)}
\NormalTok{NegBinom\_Mod }\OtherTok{\textless{}{-}} \FunctionTok{glm.nb}\NormalTok{(Num\_Article }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ bioChemists)}

\CommentTok{\# Model summaries}
\FunctionTok{summary}\NormalTok{(quasiPoisson\_Mod)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} glm(formula = Num\_Article \textasciitilde{} ., family = quasipoisson, data = bioChemists)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Deviance Residuals: }
\CommentTok{\#\textgreater{}     Min       1Q   Median       3Q      Max  }
\CommentTok{\#\textgreater{} {-}3.5672  {-}1.5398  {-}0.3660   0.5722   5.4467  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}                  Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)      0.304617   0.139273   2.187 0.028983 *  }
\CommentTok{\#\textgreater{} SexWomen        {-}0.224594   0.073860  {-}3.041 0.002427 ** }
\CommentTok{\#\textgreater{} MarriedMarried   0.155243   0.083003   1.870 0.061759 .  }
\CommentTok{\#\textgreater{} Num\_Kid5        {-}0.184883   0.054268  {-}3.407 0.000686 ***}
\CommentTok{\#\textgreater{} PhD\_Quality      0.012823   0.035700   0.359 0.719544    }
\CommentTok{\#\textgreater{} Num\_MentArticle  0.025543   0.002713   9.415  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} (Dispersion parameter for quasipoisson family taken to be 1.829006)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Null deviance: 1817.4  on 914  degrees of freedom}
\CommentTok{\#\textgreater{} Residual deviance: 1634.4  on 909  degrees of freedom}
\CommentTok{\#\textgreater{} AIC: NA}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Fisher Scoring iterations: 5}
\FunctionTok{summary}\NormalTok{(NegBinom\_Mod)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} glm.nb(formula = Num\_Article \textasciitilde{} ., data = bioChemists, init.theta = 2.264387695, }
\CommentTok{\#\textgreater{}     link = log)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Deviance Residuals: }
\CommentTok{\#\textgreater{}     Min       1Q   Median       3Q      Max  }
\CommentTok{\#\textgreater{} {-}2.1678  {-}1.3617  {-}0.2806   0.4476   3.4524  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}                  Estimate Std. Error z value Pr(\textgreater{}|z|)    }
\CommentTok{\#\textgreater{} (Intercept)      0.256144   0.137348   1.865 0.062191 .  }
\CommentTok{\#\textgreater{} SexWomen        {-}0.216418   0.072636  {-}2.979 0.002887 ** }
\CommentTok{\#\textgreater{} MarriedMarried   0.150489   0.082097   1.833 0.066791 .  }
\CommentTok{\#\textgreater{} Num\_Kid5        {-}0.176415   0.052813  {-}3.340 0.000837 ***}
\CommentTok{\#\textgreater{} PhD\_Quality      0.015271   0.035873   0.426 0.670326    }
\CommentTok{\#\textgreater{} Num\_MentArticle  0.029082   0.003214   9.048  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} (Dispersion parameter for Negative Binomial(2.2644) family taken to be 1)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Null deviance: 1109.0  on 914  degrees of freedom}
\CommentTok{\#\textgreater{} Residual deviance: 1004.3  on 909  degrees of freedom}
\CommentTok{\#\textgreater{} AIC: 3135.9}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Fisher Scoring iterations: 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}               Theta:  2.264 }
\CommentTok{\#\textgreater{}           Std. Err.:  0.271 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  2 x log{-}likelihood:  {-}3121.917}
\end{Highlighting}
\end{Shaded}

\hypertarget{key-differences-quasi-poisson-vs.-negative-binomial}{%
\subsubsection{Key Differences: Quasi-Poisson vs.~Negative Binomial}\label{key-differences-quasi-poisson-vs.-negative-binomial}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4658}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2877}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2466}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Feature
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Quasi-Poisson
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Negative Binomial
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Handles Overdispersion?} &  Yes &  Yes \\
\textbf{Uses a Full Probability Distribution?} &  No &  Yes \\
\textbf{MLE-Based?} &  No (quasi-likelihood) &  Yes \\
\textbf{Can Use AIC/BIC for Model Selection?} &  No &  Yes \\
\textbf{Better for Model Interpretation?} &  Yes &  Yes \\
\textbf{Best for Severe Overdispersion?} &  No &  Yes \\
\end{longtable}

\textbf{When to Choose:}

\begin{itemize}
\item
  Use \protect\hyperlink{sec-quasi-poisson-regression}{Quasi-Poisson} when you \textbf{only need robust standard errors} and do not require model selection via AIC/BIC.
\item
  Use \protect\hyperlink{sec-negative-binomial-regression}{Negative Binomial} when overdispersion is \textbf{large} and you want a \textbf{true likelihood-based model}.
\end{itemize}

While Quasi-Poisson is a quick fix, Negative Binomial is generally the better choice for modeling count data with overdispersion.

\hypertarget{sec-multinomial-logistic-regression}{%
\section{Multinomial Logistic Regression}\label{sec-multinomial-logistic-regression}}

When dealing with categorical response variables with more than two possible outcomes, the \textbf{multinomial logistic regression} is a natural extension of the binary logistic model.

\hypertarget{the-multinomial-distribution}{%
\subsection{The Multinomial Distribution}\label{the-multinomial-distribution}}

Suppose we have a categorical response variable \(Y_i\) that can take values in \(\{1, 2, \dots, J\}\). For each observation \(i\), the probability that it falls into category \(j\) is given by:

\[
p_{ij} = P(Y_i = j), \quad \text{where} \quad \sum_{j=1}^{J} p_{ij} = 1.
\]

The response follows a \textbf{multinomial distribution}:

\[
Y_i \sim \text{Multinomial}(1; p_{i1}, p_{i2}, ..., p_{iJ}).
\]

This means that each observation belongs to exactly one of the \(J\) categories.

\hypertarget{modeling-probabilities-using-log-odds}{%
\subsection{Modeling Probabilities Using Log-Odds}\label{modeling-probabilities-using-log-odds}}

We cannot model the probabilities \(p_{ij}\) directly because they must sum to 1. Instead, we use a \textbf{logit transformation}, comparing each category \(j\) to a \textbf{baseline category} (typically the first category, \(j=1\)):

\[
\eta_{ij} = \log \frac{p_{ij}}{p_{i1}}, \quad j = 2, \dots, J.
\]

Using a \textbf{linear function of covariates} \(\mathbf{x}_i\), we define:

\[
\eta_{ij} = \mathbf{x}_i' \beta_j = \beta_{j0} + \sum_{p=1}^{P} \beta_{jp} x_{ip}.
\]

Rearranging to express probabilities explicitly:

\[
p_{ij} = p_{i1} \exp(\mathbf{x}_i' \beta_j).
\]

Since all probabilities must sum to 1:

\[
p_{i1} + \sum_{j=2}^{J} p_{ij} = 1.
\]

Substituting for \(p_{ij}\):

\[
p_{i1} + \sum_{j=2}^{J} p_{i1} \exp(\mathbf{x}_i' \beta_j) = 1.
\]

Solving for \(p_{i1}\):

\[
p_{i1} = \frac{1}{1 + \sum_{j=2}^{J} \exp(\mathbf{x}_i' \beta_j)}.
\]

Thus, the probability for category \(j\) is:

\[
p_{ij} = \frac{\exp(\mathbf{x}_i' \beta_j)}{1 + \sum_{l=2}^{J} \exp(\mathbf{x}_i' \beta_l)}, \quad j = 2, \dots, J.
\]

This formulation is known as the \textbf{multinomial logit model}.

\hypertarget{softmax-representation}{%
\subsection{Softmax Representation}\label{softmax-representation}}

An alternative formulation avoids choosing a baseline category and instead treats all \(J\) categories \textbf{symmetrically} using the \textbf{softmax function}:

\[
P(Y_i = j | X_i = x) = \frac{\exp(\beta_{j0} + \sum_{p=1}^{P} \beta_{jp} x_p)}{\sum_{l=1}^{J} \exp(\beta_{l0} + \sum_{p=1}^{P} \beta_{lp} x_p)}.
\]

This representation is often used in \textbf{neural networks} and general machine learning models.

\hypertarget{log-odds-ratio-between-two-categories}{%
\subsection{Log-Odds Ratio Between Two Categories}\label{log-odds-ratio-between-two-categories}}

The \textbf{log-odds ratio} between two categories \(k\) and \(k'\) is:

\[
\log \frac{P(Y = k | X = x)}{P(Y = k' | X = x)}
= (\beta_{k0} - \beta_{k'0}) + \sum_{p=1}^{P} (\beta_{kp} - \beta_{k'p}) x_p.
\]

This equation tells us that:

\begin{itemize}
\tightlist
\item
  If \(\beta_{kp} > \beta_{k'p}\), then increasing \(x_p\) increases the odds of choosing category \(k\) over \(k'\).
\item
  If \(\beta_{kp} < \beta_{k'p}\), then increasing \(x_p\) decreases the odds of choosing \(k\) over \(k'\).
\end{itemize}

\hypertarget{estimation}{%
\subsection{Estimation}\label{estimation}}

To estimate the parameters \(\beta_j\), we use \protect\hyperlink{maximum-likelihood-estimator}{Maximum Likelihood} estimation.

Given \(n\) independent observations \((Y_i, X_i)\), the likelihood function is:

\[
L(\beta) = \prod_{i=1}^{n} \prod_{j=1}^{J} p_{ij}^{Y_{ij}}.
\]

Taking the log-likelihood:

\[
\log L(\beta) = \sum_{i=1}^{n} \sum_{j=1}^{J} Y_{ij} \log p_{ij}.
\]

Since there is no closed-form solution, numerical methods (see \protect\hyperlink{non-linear-least-squares-estimation}{Non-linear Least Squares Estimation}) are used for estimation.

\hypertarget{interpretation-of-coefficients}{%
\subsection{Interpretation of Coefficients}\label{interpretation-of-coefficients}}

\begin{itemize}
\tightlist
\item
  Each \(\beta_{jp}\) represents the effect of \(x_p\) on the \textbf{log-odds} of category \(j\) relative to the baseline.
\item
  \textbf{Positive coefficients} mean increasing \(x_p\) makes category \(j\) more likely relative to the baseline.
\item
  \textbf{Negative coefficients} mean increasing \(x_p\) makes category \(j\) less likely relative to the baseline.
\end{itemize}

\hypertarget{application-multinomial-logistic-regression}{%
\subsection{Application: Multinomial Logistic Regression}\label{application-multinomial-logistic-regression}}

\textbf{1. Load Necessary Libraries and Data}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(faraway)  }\CommentTok{\# For the dataset}
\FunctionTok{library}\NormalTok{(dplyr)    }\CommentTok{\# For data manipulation}
\FunctionTok{library}\NormalTok{(ggplot2)  }\CommentTok{\# For visualization}
\FunctionTok{library}\NormalTok{(nnet)     }\CommentTok{\# For multinomial logistic regression}

\CommentTok{\# Load and inspect data}
\FunctionTok{data}\NormalTok{(nes96, }\AttributeTok{package=}\StringTok{"faraway"}\NormalTok{)}
\FunctionTok{head}\NormalTok{(nes96, }\DecValTok{3}\NormalTok{)}
\CommentTok{\#\textgreater{}   popul TVnews selfLR ClinLR DoleLR     PID age  educ   income    vote}
\CommentTok{\#\textgreater{} 1     0      7 extCon extLib    Con  strRep  36    HS $3Kminus    Dole}
\CommentTok{\#\textgreater{} 2   190      1 sliLib sliLib sliCon weakDem  20  Coll $3Kminus Clinton}
\CommentTok{\#\textgreater{} 3    31      7    Lib    Lib    Con weakDem  24 BAdeg $3Kminus Clinton}
\end{Highlighting}
\end{Shaded}

The dataset \texttt{nes96} contains survey responses, including political party identification (\texttt{PID}), age (\texttt{age}), and education level (\texttt{educ}).

\textbf{2. Define Political Strength Categories}

We classify political strength into three categories:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Strong}: Strong Democrat or Strong Republican
\item
  \textbf{Weak}: Weak Democrat or Weak Republican
\item
  \textbf{Neutral}: Independents and other affiliations
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Check distribution of political identity}
\FunctionTok{table}\NormalTok{(nes96}\SpecialCharTok{$}\NormalTok{PID)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  strDem weakDem  indDem  indind  indRep weakRep  strRep }
\CommentTok{\#\textgreater{}     200     180     108      37      94     150     175}

\CommentTok{\# Define Political Strength variable}
\NormalTok{nes96 }\OtherTok{\textless{}{-}}\NormalTok{ nes96 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Political\_Strength =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{    PID }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"strDem"}\NormalTok{, }\StringTok{"strRep"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"Strong"}\NormalTok{,}
\NormalTok{    PID }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"weakDem"}\NormalTok{, }\StringTok{"weakRep"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"Weak"}\NormalTok{,}
\NormalTok{    PID }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"indDem"}\NormalTok{, }\StringTok{"indind"}\NormalTok{, }\StringTok{"indRep"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"Neutral"}\NormalTok{,}
    \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \ConstantTok{NA\_character\_}
\NormalTok{  ))}

\CommentTok{\# Summarize}
\NormalTok{nes96 }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{group\_by}\NormalTok{(Political\_Strength) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{summarise}\NormalTok{(}\AttributeTok{Count =} \FunctionTok{n}\NormalTok{())}
\CommentTok{\#\textgreater{} \# A tibble: 3 x 2}
\CommentTok{\#\textgreater{}   Political\_Strength Count}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}              \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 Neutral              239}
\CommentTok{\#\textgreater{} 2 Strong               375}
\CommentTok{\#\textgreater{} 3 Weak                 330}
\end{Highlighting}
\end{Shaded}

\textbf{3. Visualizing Political Strength by Age}

We visualize the proportion of each political strength category across age groups.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Prepare data for visualization}
\NormalTok{Plot\_DF }\OtherTok{\textless{}{-}}\NormalTok{ nes96 }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Age\_Grp =} \FunctionTok{cut\_number}\NormalTok{(age, }\DecValTok{4}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{group\_by}\NormalTok{(Age\_Grp, Political\_Strength) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{summarise}\NormalTok{(}\AttributeTok{count =} \FunctionTok{n}\NormalTok{(), }\AttributeTok{.groups =} \StringTok{\textquotesingle{}drop\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{group\_by}\NormalTok{(Age\_Grp) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{etotal =} \FunctionTok{sum}\NormalTok{(count), }\AttributeTok{proportion =}\NormalTok{ count }\SpecialCharTok{/}\NormalTok{ etotal)}

\CommentTok{\# Plot age vs political strength}
\NormalTok{Age\_Plot }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(}
\NormalTok{    Plot\_DF,}
    \FunctionTok{aes}\NormalTok{(}
        \AttributeTok{x        =}\NormalTok{ Age\_Grp,}
        \AttributeTok{y        =}\NormalTok{ proportion,}
        \AttributeTok{group    =}\NormalTok{ Political\_Strength,}
        \AttributeTok{linetype =}\NormalTok{ Political\_Strength,}
        \AttributeTok{color    =}\NormalTok{ Political\_Strength}
\NormalTok{    )}
\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{size =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Political Strength by Age Group"}\NormalTok{,}
         \AttributeTok{x =} \StringTok{"Age Group"}\NormalTok{,}
         \AttributeTok{y =} \StringTok{"Proportion"}\NormalTok{)}

\CommentTok{\# Display plot}
\NormalTok{Age\_Plot}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{07-generalized-linear-models_files/figure-latex/unnamed-chunk-39-1} \end{center}

\textbf{4. Fit a Multinomial Logistic Model}

We model \textbf{political strength} as a function of \textbf{age} and \textbf{education}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fit multinomial logistic regression}
\NormalTok{Multinomial\_Model }\OtherTok{\textless{}{-}}
    \FunctionTok{multinom}\NormalTok{(Political\_Strength }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ educ,}
             \AttributeTok{data =}\NormalTok{ nes96,}
             \AttributeTok{trace =} \ConstantTok{FALSE}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(Multinomial\_Model)}
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} multinom(formula = Political\_Strength \textasciitilde{} age + educ, data = nes96, }
\CommentTok{\#\textgreater{}     trace = FALSE)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}        (Intercept)          age     educ.L     educ.Q     educ.C      educ\^{}4}
\CommentTok{\#\textgreater{} Strong {-}0.08788729  0.010700364 {-}0.1098951 {-}0.2016197 {-}0.1757739 {-}0.02116307}
\CommentTok{\#\textgreater{} Weak    0.51976285 {-}0.004868771 {-}0.1431104 {-}0.2405395 {-}0.2411795  0.18353634}
\CommentTok{\#\textgreater{}            educ\^{}5     educ\^{}6}
\CommentTok{\#\textgreater{} Strong {-}0.1664377 {-}0.1359449}
\CommentTok{\#\textgreater{} Weak   {-}0.1489030 {-}0.2173144}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Std. Errors:}
\CommentTok{\#\textgreater{}        (Intercept)         age    educ.L    educ.Q    educ.C    educ\^{}4}
\CommentTok{\#\textgreater{} Strong   0.3017034 0.005280743 0.4586041 0.4318830 0.3628837 0.2964776}
\CommentTok{\#\textgreater{} Weak     0.3097923 0.005537561 0.4920736 0.4616446 0.3881003 0.3169149}
\CommentTok{\#\textgreater{}           educ\^{}5    educ\^{}6}
\CommentTok{\#\textgreater{} Strong 0.2515012 0.2166774}
\CommentTok{\#\textgreater{} Weak   0.2643747 0.2199186}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual Deviance: 2024.596 }
\CommentTok{\#\textgreater{} AIC: 2056.596}
\end{Highlighting}
\end{Shaded}

\textbf{5. Stepwise Model Selection Based on AIC}

We perform stepwise selection to find the best model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Multinomial\_Step }\OtherTok{\textless{}{-}} \FunctionTok{step}\NormalTok{(Multinomial\_Model, }\AttributeTok{trace =} \DecValTok{0}\NormalTok{)}
\CommentTok{\#\textgreater{} trying {-} age }
\CommentTok{\#\textgreater{} trying {-} educ }
\CommentTok{\#\textgreater{} trying {-} age}
\NormalTok{Multinomial\_Step}
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} multinom(formula = Political\_Strength \textasciitilde{} age, data = nes96, trace = FALSE)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}        (Intercept)          age}
\CommentTok{\#\textgreater{} Strong {-}0.01988977  0.009832916}
\CommentTok{\#\textgreater{} Weak    0.59497046 {-}0.005954348}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual Deviance: 2030.756 }
\CommentTok{\#\textgreater{} AIC: 2038.756}
\end{Highlighting}
\end{Shaded}

Compare the best model to the full model based on \textbf{deviance}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pchisq}\NormalTok{(}
    \AttributeTok{q =} \FunctionTok{deviance}\NormalTok{(Multinomial\_Step) }\SpecialCharTok{{-}} \FunctionTok{deviance}\NormalTok{(Multinomial\_Model),}
    \AttributeTok{df =}\NormalTok{ Multinomial\_Model}\SpecialCharTok{$}\NormalTok{edf }\SpecialCharTok{{-}}\NormalTok{ Multinomial\_Step}\SpecialCharTok{$}\NormalTok{edf,}
    \AttributeTok{lower.tail =} \ConstantTok{FALSE}
\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 0.9078172}
\end{Highlighting}
\end{Shaded}

A non-significant p-value suggests \textbf{no major difference} between the full and stepwise models.

\textbf{6. Predictions \& Visualization}

Predicting Political Strength Probabilities by Age

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create data for prediction}
\NormalTok{PlotData }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{age =} \FunctionTok{seq}\NormalTok{(}\AttributeTok{from =} \DecValTok{19}\NormalTok{, }\AttributeTok{to =} \DecValTok{91}\NormalTok{))}

\CommentTok{\# Get predicted probabilities}
\NormalTok{Preds }\OtherTok{\textless{}{-}}\NormalTok{ PlotData }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{bind\_cols}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(}\FunctionTok{predict}\NormalTok{(Multinomial\_Step, }
\NormalTok{                                 PlotData, }
                                 \AttributeTok{type =} \StringTok{"probs"}\NormalTok{)))}

\CommentTok{\# Plot predicted probabilities across age}
\FunctionTok{plot}\NormalTok{(}
    \AttributeTok{x =}\NormalTok{ Preds}\SpecialCharTok{$}\NormalTok{age,}
    \AttributeTok{y =}\NormalTok{ Preds}\SpecialCharTok{$}\NormalTok{Neutral,}
    \AttributeTok{type =} \StringTok{"l"}\NormalTok{,}
    \AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\FloatTok{0.6}\NormalTok{),}
    \AttributeTok{col =} \StringTok{"black"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"Proportion"}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"Age"}
\NormalTok{)}

\FunctionTok{lines}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Preds}\SpecialCharTok{$}\NormalTok{age,}
      \AttributeTok{y =}\NormalTok{ Preds}\SpecialCharTok{$}\NormalTok{Weak,}
      \AttributeTok{col =} \StringTok{"blue"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Preds}\SpecialCharTok{$}\NormalTok{age,}
      \AttributeTok{y =}\NormalTok{ Preds}\SpecialCharTok{$}\NormalTok{Strong,}
      \AttributeTok{col =} \StringTok{"red"}\NormalTok{)}

\FunctionTok{legend}\NormalTok{(}
    \StringTok{"topleft"}\NormalTok{,}
    \AttributeTok{legend =} \FunctionTok{c}\NormalTok{(}\StringTok{"Neutral"}\NormalTok{, }\StringTok{"Weak"}\NormalTok{, }\StringTok{"Strong"}\NormalTok{),}
    \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"blue"}\NormalTok{, }\StringTok{"red"}\NormalTok{),}
    \AttributeTok{lty =} \DecValTok{1}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{07-generalized-linear-models_files/figure-latex/unnamed-chunk-43-1} \end{center}

Predict for Specific Ages

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Predict class for a 34{-}year{-}old}
\FunctionTok{predict}\NormalTok{(Multinomial\_Step, }\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{age =} \DecValTok{34}\NormalTok{))}
\CommentTok{\#\textgreater{} [1] Weak}
\CommentTok{\#\textgreater{} Levels: Neutral Strong Weak}

\CommentTok{\# Predict probabilities for 34 and 35{-}year{-}olds}
\FunctionTok{predict}\NormalTok{(Multinomial\_Step, }\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{age =} \FunctionTok{c}\NormalTok{(}\DecValTok{34}\NormalTok{, }\DecValTok{35}\NormalTok{)), }\AttributeTok{type =} \StringTok{"probs"}\NormalTok{)}
\CommentTok{\#\textgreater{}     Neutral    Strong      Weak}
\CommentTok{\#\textgreater{} 1 0.2597275 0.3556910 0.3845815}
\CommentTok{\#\textgreater{} 2 0.2594080 0.3587639 0.3818281}
\end{Highlighting}
\end{Shaded}

\hypertarget{application-gamma-regression}{%
\subsection{Application: Gamma Regression}\label{application-gamma-regression}}

When response variables are \textbf{strictly positive}, we use \textbf{Gamma regression}.

\textbf{1. Load and Prepare Data}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(agridat)  }\CommentTok{\# Agricultural dataset}

\CommentTok{\# Load and filter data}
\NormalTok{dat }\OtherTok{\textless{}{-}}\NormalTok{ agridat}\SpecialCharTok{::}\NormalTok{streibig.competition}
\NormalTok{gammaDat }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(dat, sseeds }\SpecialCharTok{\textless{}} \DecValTok{1}\NormalTok{)  }\CommentTok{\# Keep only barley}
\NormalTok{gammaDat }\OtherTok{\textless{}{-}}
    \FunctionTok{transform}\NormalTok{(gammaDat,}
              \AttributeTok{x =}\NormalTok{ bseeds,}
              \AttributeTok{y =}\NormalTok{ bdwt,}
              \AttributeTok{block =} \FunctionTok{factor}\NormalTok{(block))}
\end{Highlighting}
\end{Shaded}

\textbf{2. Visualization of Inverse Yield}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(gammaDat, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =} \DecValTok{1} \SpecialCharTok{/}\NormalTok{ y)) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color =}\NormalTok{ block, }\AttributeTok{shape =}\NormalTok{ block)) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Inverse Yield vs Seeding Rate"}\NormalTok{,}
         \AttributeTok{x =} \StringTok{"Seeding Rate"}\NormalTok{,}
         \AttributeTok{y =} \StringTok{"Inverse Yield"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{07-generalized-linear-models_files/figure-latex/unnamed-chunk-46-1} \end{center}

\textbf{3. Fit Gamma Regression Model}

Gamma regression models \textbf{yield as a function of seeding rate} using an inverse link: \[
\eta_{ij} = \beta_{0j} + \beta_{1j} x_{ij} + \beta_2 x_{ij}^2, \quad Y_{ij} = \eta_{ij}^{-1}
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1 }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ block }\SpecialCharTok{+}\NormalTok{ block }\SpecialCharTok{*}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ block }\SpecialCharTok{*} \FunctionTok{I}\NormalTok{(x}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{),}
          \AttributeTok{data =}\NormalTok{ gammaDat, }\AttributeTok{family =} \FunctionTok{Gamma}\NormalTok{(}\AttributeTok{link =} \StringTok{"inverse"}\NormalTok{))}

\FunctionTok{summary}\NormalTok{(m1)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} glm(formula = y \textasciitilde{} block + block * x + block * I(x\^{}2), family = Gamma(link = "inverse"), }
\CommentTok{\#\textgreater{}     data = gammaDat)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Deviance Residuals: }
\CommentTok{\#\textgreater{}      Min        1Q    Median        3Q       Max  }
\CommentTok{\#\textgreater{} {-}1.21708  {-}0.44148   0.02479   0.17999   0.80745  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}                  Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)     1.115e{-}01  2.870e{-}02   3.886 0.000854 ***}
\CommentTok{\#\textgreater{} blockB2        {-}1.208e{-}02  3.880e{-}02  {-}0.311 0.758630    }
\CommentTok{\#\textgreater{} blockB3        {-}2.386e{-}02  3.683e{-}02  {-}0.648 0.524029    }
\CommentTok{\#\textgreater{} x              {-}2.075e{-}03  1.099e{-}03  {-}1.888 0.072884 .  }
\CommentTok{\#\textgreater{} I(x\^{}2)          1.372e{-}05  9.109e{-}06   1.506 0.146849    }
\CommentTok{\#\textgreater{} blockB2:x       5.198e{-}04  1.468e{-}03   0.354 0.726814    }
\CommentTok{\#\textgreater{} blockB3:x       7.475e{-}04  1.393e{-}03   0.537 0.597103    }
\CommentTok{\#\textgreater{} blockB2:I(x\^{}2) {-}5.076e{-}06  1.184e{-}05  {-}0.429 0.672475    }
\CommentTok{\#\textgreater{} blockB3:I(x\^{}2) {-}6.651e{-}06  1.123e{-}05  {-}0.592 0.560012    }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} (Dispersion parameter for Gamma family taken to be 0.3232083)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Null deviance: 13.1677  on 29  degrees of freedom}
\CommentTok{\#\textgreater{} Residual deviance:  7.8605  on 21  degrees of freedom}
\CommentTok{\#\textgreater{} AIC: 225.32}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Fisher Scoring iterations: 5}
\end{Highlighting}
\end{Shaded}

\textbf{4. Predictions and Visualization}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Generate new data for prediction}
\NormalTok{newdf }\OtherTok{\textless{}{-}}
    \FunctionTok{expand.grid}\NormalTok{(}\AttributeTok{x =} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{120}\NormalTok{, }\AttributeTok{length =} \DecValTok{50}\NormalTok{), }
                \AttributeTok{block =} \FunctionTok{factor}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"B1"}\NormalTok{, }\StringTok{"B2"}\NormalTok{, }\StringTok{"B3"}\NormalTok{)))}

\CommentTok{\# Predict responses}
\NormalTok{newdf}\SpecialCharTok{$}\NormalTok{pred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(m1, }\AttributeTok{newdata =}\NormalTok{ newdf, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}

\CommentTok{\# Plot predictions}
\FunctionTok{ggplot}\NormalTok{(gammaDat, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ y)) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color =}\NormalTok{ block, }\AttributeTok{shape =}\NormalTok{ block)) }\SpecialCharTok{+}
    \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{data =}\NormalTok{ newdf, }\FunctionTok{aes}\NormalTok{(}
        \AttributeTok{x =}\NormalTok{ x,}
        \AttributeTok{y =}\NormalTok{ pred,}
        \AttributeTok{color =}\NormalTok{ block,}
        \AttributeTok{linetype =}\NormalTok{ block}
\NormalTok{    )) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Predicted Yield by Seeding Rate"}\NormalTok{,}
         \AttributeTok{x =} \StringTok{"Seeding Rate"}\NormalTok{,}
         \AttributeTok{y =} \StringTok{"Yield"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{07-generalized-linear-models_files/figure-latex/unnamed-chunk-48-1} \end{center}

\hypertarget{sec-generalization-of-generalized-linear-models}{%
\section{Generalization of Generalized Linear Models}\label{sec-generalization-of-generalized-linear-models}}

We have seen that \protect\hyperlink{sec-poisson-regression}{Poisson regression} bears similarities to logistic regression. This insight leads us to a broader class of models known as \protect\hyperlink{generalized-linear-models}{Generalized Linear Models}, introduced by \citet{nelder1972generalized}. These models provide a unified framework for handling different types of response variables while maintaining the fundamental principles of linear modeling.

\hypertarget{exponential-family}{%
\subsection{Exponential Family}\label{exponential-family}}

The foundation of GLMs is built on the \textbf{exponential family of distributions}, which provides a flexible class of probability distributions that share a common form:

\[
f(y;\theta, \phi) = \exp\left(\frac{\theta y - b(\theta)}{a(\phi)} + c(y, \phi)\right)
\]

where:

\begin{itemize}
\tightlist
\item
  \(\theta\) is the \textbf{natural parameter} (canonical parameter),
\item
  \(\phi\) is the \textbf{dispersion parameter},
\item
  \(a(\phi)\), \(b(\theta)\), and \(c(y, \phi)\) are functions ensuring the proper distributional form.
\end{itemize}

\textbf{Distributions in the Exponential Family that Can Be Used in GLMs:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \protect\hyperlink{normal-distribution}{Normal Distribution}
\item
  \protect\hyperlink{binomial-distribution}{Binomial Distribution}
\item
  \protect\hyperlink{poisson-distribution}{Poisson Distribution}
\item
  \protect\hyperlink{gamma-distribution}{Gamma Distribution}
\item
  Inverse Gaussian Distribution
\item
  Negative Binomial Distribution (used in GLMs but requires overdispersion adjustments)
\item
  Multinomial Distribution (for categorical response)
\end{enumerate}

\textbf{Exponential Family Distributions Not Commonly Used in GLMs:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Beta Distribution
\item
  Dirichlet Distribution
\item
  Wishart Distribution
\item
  \protect\hyperlink{geometric-distribution}{Geometric Distribution}
\item
  Exponential Distribution (can be used indirectly through survival models)
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Example: Normal Distribution}

Consider a normally distributed response variable \(Y \sim N(\mu, \sigma^2)\). The probability density function (PDF) is:

\[
\begin{aligned}
f(y; \mu, \sigma^2) &= \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left(-\frac{(y - \mu)^2}{2\sigma^2}\right) \\
&= \exp\left(-\frac{y^2 - 2y\mu + \mu^2}{2\sigma^2} - \frac{1}{2} \log(2\pi\sigma^2)\right)
\end{aligned}
\]

Rewriting in exponential family form:

\[
\begin{aligned}
f(y; \mu, \sigma^2) &= \exp\left(\frac{y \mu - \frac{\mu^2}{2}}{\sigma^2} - \frac{y^2}{2\sigma^2} - \frac{1}{2} \log(2\pi\sigma^2)\right) \\
&= \exp\left(\frac{\theta y - b(\theta)}{a(\phi)} + c(y, \phi)\right)
\end{aligned}
\]

where:

\begin{itemize}
\tightlist
\item
  \textbf{Natural parameter}: \(\theta = \mu\)
\item
  \textbf{Function} \(b(\theta)\): \(b(\theta) = \frac{\mu^2}{2}\)
\item
  \textbf{Dispersion function}: \(a(\phi) = \sigma^2 = \phi\)
\item
  \textbf{Function} \(c(y, \phi)\): \(c(y, \phi) = -\frac{1}{2} \left(\frac{y^2}{\phi} + \log(2\pi \sigma^2)\right)\)
\end{itemize}

\hypertarget{properties-of-glm-exponential-families}{%
\subsection{Properties of GLM Exponential Families}\label{properties-of-glm-exponential-families}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Expected Value (Mean)} \[
  E(Y) = b'(\theta)
  \] where \(b'(\theta) = \frac{\partial b(\theta)}{\partial \theta}\). (Note: \texttt{\textquotesingle{}} is ``prime,'' not transpose).
\item
  \textbf{Variance} \[
  \text{Var}(Y) = a(\phi) b''(\theta) = a(\phi) V(\mu)
  \] where:

  \begin{itemize}
  \tightlist
  \item
    \(V(\mu) = b''(\theta)\) is the \textbf{variance function}, though it only represents the variance when \(a(\phi) = 1\).
  \end{itemize}
\item
  If \(a(\phi)\), \(b(\theta)\), and \(c(y, \phi)\) are identifiable, we can derive the expected value and variance of \(Y\).
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Examples of Exponential Family Distributions}

\textbf{1. Normal Distribution}

For a normal distribution \(Y \sim N(\mu, \sigma^2)\), the exponential family representation is:

\[
f(y; \mu, \sigma^2) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp \left( -\frac{(y - \mu)^2}{2\sigma^2} \right)
\]

which can be rewritten in exponential form:

\[
\exp \left( \frac{y\mu - \frac{1}{2} \mu^2}{\sigma^2} - \frac{y^2}{2\sigma^2} - \frac{1}{2} \log(2\pi\sigma^2) \right)
\]

From this, we identify:

\begin{itemize}
\tightlist
\item
  \(\theta = \mu\)
\item
  \(b(\theta) = \frac{\theta^2}{2}\)
\item
  \(a(\phi) = \sigma^2\)
\end{itemize}

Computing derivatives:

\[
b'(\theta) = \frac{\partial b(\theta)}{\partial \theta} = \mu, \quad V(\mu) = b''(\theta) = 1
\]

Thus,

\[
E(Y) = \mu, \quad \text{Var}(Y) = a(\phi) V(\mu) = \sigma^2
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{2. Poisson Distribution}

For a Poisson-distributed response \(Y \sim \text{Poisson}(\mu)\), the probability mass function is:

\[
f(y; \mu) = \frac{\mu^y e^{-\mu}}{y!}
\]

Rewriting in exponential form:

\[
\exp(y \log \mu - \mu - \log y!)
\]

Thus, we identify:

\begin{itemize}
\tightlist
\item
  \(\theta = \log \mu\)
\item
  \(b(\theta) = e^\theta\)
\item
  \(a(\phi) = 1\)
\item
  \(c(y, \phi) = \log y!\)
\end{itemize}

Computing derivatives:

\[
E(Y) = b'(\theta) = e^\theta = \mu, \quad \text{Var}(Y) = b''(\theta) = \mu
\]

Since \(\mu = E(Y)\), we confirm the variance function:

\[
\text{Var}(Y) = V(\mu) = \mu
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{structure-of-a-generalized-linear-model}{%
\subsection{Structure of a Generalized Linear Model}\label{structure-of-a-generalized-linear-model}}

In a GLM, we model the mean \(\mu\) through a \textbf{link function} that connects it to a linear predictor:

\[
g(\mu) = g(b'(\theta)) = \mathbf{x' \beta}
\]

Equivalently,

\[
\mu = g^{-1}(\mathbf{x' \beta})
\]

where:

\begin{itemize}
\tightlist
\item
  \(g(\cdot)\) is the \textbf{link function}, which ensures a transformation between the expected response \(E(Y) = \mu\) and the linear predictor.
\item
  \(\eta = \mathbf{x' \beta}\) is called the \textbf{linear predictor}.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{components-of-a-glm}{%
\subsection{Components of a GLM}\label{components-of-a-glm}}

A GLM consists of two main components:

\hypertarget{random-component}{%
\subsubsection{Random Component}\label{random-component}}

This describes the \textbf{distribution of the response variable} \(Y_1, \dots, Y_n\). The response variables are assumed to follow a distribution from the \textbf{exponential family}, which can be written as:

\[
f(y_i ; \theta_i, \phi) = \exp \left( \frac{\theta_i y_i - b(\theta_i)}{a(\phi)} + c(y_i, \phi) \right)
\]

where:

\begin{itemize}
\item
  \(Y_i\) are \textbf{independent} random variables.
\item
  The \textbf{canonical parameter} \(\theta_i\) may differ for each observation.
\item
  The \textbf{dispersion parameter} \(\phi\) is assumed to be constant across all \(i\).
\item
  The mean response is given by:

  \[
  \mu_i = E(Y_i)
  \]
\end{itemize}

\hypertarget{systematic-component}{%
\subsubsection{Systematic Component}\label{systematic-component}}

This specifies how the mean response \(\mu\) is related to the explanatory variables \(\mathbf{x}\) through a \textbf{linear predictor} \(\eta\):

\begin{itemize}
\item
  The systematic component consists of:

  \begin{itemize}
  \tightlist
  \item
    A \textbf{link function} \(g(\mu)\).
  \item
    A \textbf{linear predictor} \(\eta = \mathbf{x' \beta}\).
  \end{itemize}
\item
  \textbf{Notation}:

  \begin{itemize}
  \item
    We assume:

    \[
    g(\mu_i) = \mathbf{x' \beta} = \eta_i
    \]
  \item
    The parameter vector \(\mathbf{\beta} = (\beta_1, \dots, \beta_p)'\) needs to be estimated.
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{canonical-link}{%
\subsection{Canonical Link}\label{canonical-link}}

In a GLM, a link function \(g(\cdot)\) relates the mean \(\mu_i\) of the response \(Y_i\) to the linear predictor \(\eta_i\) via

\[
\eta_i = g(\mu_i).
\]

A \textbf{canonical link} is a special case of \(g(\cdot)\) where

\[
g(\mu_i) = \eta_i = \theta_i,
\]

and \(\theta_i\) is the \textbf{natural parameter} of the exponential family. In other words, the link function directly equates the linear predictor \(\eta_i\) with the distribution's natural parameter \(\theta_i\). Hence, \(g(\mu)\) is \textbf{canonical} if \(g(\mu) = \theta\).

\textbf{Exponential Family Components}

\begin{itemize}
\tightlist
\item
  \(b(\theta)\): the \textbf{cumulant (moment generating) function}, which defines the variance function.
\item
  \(g(\mu)\): the \textbf{link function}, which must be

  \begin{itemize}
  \tightlist
  \item
    Monotonically increasing
  \item
    Continuously differentiable
  \item
    Invertible
  \end{itemize}
\end{itemize}

\begin{figure}
\centering
\includegraphics[width=0.9\textwidth,height=\textheight]{images/GLM.PNG}
\caption{GLM Structure}
\end{figure}

For an exponential-family distribution, the function \(b(\theta)\) is called the \emph{cumulant moment generating function}, and it relates \(\theta\) to the mean via its derivative:

\[
\mu = b'(\theta)
\quad\Longleftrightarrow\quad
\theta = b'^{-1}(\mu).
\]

By defining the link so that \(g(\mu) = \theta\), we impose \(\eta_i = \theta_i\), which is why \(g(\cdot)\) is termed \textbf{canonical} in this setting.

When the link is canonical, an equivalent way to express this is

\[
\gamma^{-1} \circ g^{-1} = I,
\]

indicating that the inverse link \(g^{-1}(\cdot)\) directly maps the linear predictor (now the natural parameter \(\theta\)) back to \(\mu\) in a way that respects the structure of the exponential family.

Choosing \(g(\cdot)\) to be canonical often simplifies mathematical derivations and computations---especially for parameter estimation---because the linear predictor \(\eta\) and the natural parameter \(\theta\) coincide. Common examples of canonical links include:

\begin{itemize}
\tightlist
\item
  \textbf{Identity link} for the normal (Gaussian) distribution
\item
  \textbf{Log link} for the Poisson distribution
\item
  \textbf{Logit link} for the Bernoulli (binomial) distribution
\end{itemize}

In each case, setting \(\eta = \theta\) streamlines the relationship between the mean and the linear predictor, making the model both elegant and practically convenient.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{inverse-link-functions}{%
\subsection{Inverse Link Functions}\label{inverse-link-functions}}

The \textbf{inverse link function} \(g^{-1}(\eta)\) (also called the \textbf{mean function}) transforms the linear predictor \(\eta\) (which can take any real value) into a \textbf{valid mean response} \(\mu\).

Example 1: Normal Distribution (Identity Link)

\begin{itemize}
\item
  Random Component: \(Y_i \sim N(\mu_i, \sigma^2)\).
\item
  Mean Response: \(\mu_i = \theta_i\).
\item
  Canonical Link Function:

  \[
  g(\mu_i) = \mu_i
  \]

  (i.e., the identity function).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Example 2: Binomial Distribution (Logit Link)

\begin{itemize}
\item
  Random Component: \(Y_i \sim \text{Binomial}(n_i, p_i)\).
\item
  Mean Response:

  \[
  \mu_i = \frac{n_i e^{\theta_i}}{1+e^{\theta_i}}
  \]
\item
  Canonical Link Function:

  \[
  g(\mu_i) = \log \left( \frac{\mu_i}{n_i - \mu_i} \right)
  \]

  (Logit link function).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Example 3: Poisson Distribution (Log Link)

\begin{itemize}
\item
  Random Component: \(Y_i \sim \text{Poisson}(\mu_i)\).
\item
  Mean Response:

  \[
  \mu_i = e^{\theta_i}
  \]
\item
  Canonical Link Function:

  \[
  g(\mu_i) = \log(\mu_i)
  \]

  (Log link function).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Example 4: Gamma Distribution (Inverse Link)

\begin{itemize}
\item
  Random Component: \(Y_i \sim \text{Gamma}(\alpha, \mu_i)\).
\item
  Mean Response:

  \[
  \mu_i = -\frac{1}{\theta_i}
  \]
\item
  Canonical Link Function:

  \[
  g(\mu_i) = -\frac{1}{\mu_i}
  \]

  (Inverse link function).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The following table presents common \textbf{GLM link functions} and their corresponding \textbf{inverse functions}.

\[ \begin{array}{|l|c|c|} \hline \textbf{Link} & \eta_i = g(\mu_i) & \mu_i = g^{-1}(\eta_i) \\ \hline \text{Identity} & \mu_i & \eta_i \\ \text{Log} & \log_e \mu_i & e^{\eta_i} \\ \text{Inverse} & \mu_i^{-1} & \eta_i^{-1} \\ \text{Inverse-square} & \mu_i^{-2} & \eta_i^{-1/2} \\ \text{Square-root} & \sqrt{\mu_i} & \eta_i^2 \\ \text{Logit} & \log_e \left( \frac{\mu_i}{1 - \mu_i} \right) & \frac{1}{1 + e^{-\eta_i}} \\ \text{Probit} & \Phi^{-1}(\mu_i) & \Phi(\eta_i) \\ \hline \end{array} \]

where

\begin{itemize}
\item
  \(\mu_i\) is the expected value of the response.
\item
  \(\eta_i\) is the linear predictor.
\item
  \(\Phi(\cdot)\) represents the CDF of the standard normal distribution.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{estimation-of-parameters-in-glms}{%
\subsection{Estimation of Parameters in GLMs}\label{estimation-of-parameters-in-glms}}

The GLM framework extends \protect\hyperlink{linear-regression}{Linear Regression} by allowing for response variables that follow \textbf{exponential family distributions}.

\begin{itemize}
\item
  \protect\hyperlink{maximum-likelihood-estimator}{Maximum Likelihood} Estimation is used to estimate the parameters of the systematic component (\(\beta\)), providing a consistent and efficient approach. The derivation and computation processes are unified, thanks to the exponential form of the model, which simplifies mathematical treatment and implementation.
\item
  However, this unification does not extend to the estimation of the dispersion parameter (\(\phi\)), which requires separate treatment, often involving alternative estimation methods such as moment-based approaches or quasi-likelihood estimation.
\end{itemize}

In GLMs, the response variable \(Y_i\) follows an \textbf{exponential family distribution} characterized by the density function:

\[
f(y_i ; \theta_i, \phi) = \exp\left(\frac{\theta_i y_i - b(\theta_i)}{a(\phi)} + c(y_i, \phi) \right)
\]

where:

\begin{itemize}
\tightlist
\item
  \(\theta_i\) is the \textbf{canonical parameter}.
\item
  \(\phi\) is the \textbf{dispersion parameter} (which may be known or estimated separately).
\item
  \(b(\theta_i)\) determines the mean and variance of \(Y_i\).
\item
  \(a(\phi)\) scales the variance.
\item
  \(c(y_i, \phi)\) ensures proper normalization.
\end{itemize}

For this family, we obtain:

\begin{itemize}
\tightlist
\item
  \textbf{Mean of} \(Y_i\): \[
  E(Y_i) = \mu_i = b'(\theta_i)
  \]
\item
  \textbf{Variance of} \(Y_i\): \[
  \text{Var}(Y_i) = b''(\theta_i) a(\phi) = V(\mu_i)a(\phi)
  \] where \(V(\mu_i)\) is the \textbf{variance function}.
\item
  \textbf{Systematic component (link function):} \[
  g(\mu_i) = \eta_i = \mathbf{x}_i' \beta
  \]
\end{itemize}

The function \(g(\cdot)\) is the \textbf{link function}, which connects the expected response \(\mu_i\) to the linear predictor \(\mathbf{x}_i' \beta\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

For a single observation \(Y_i\), the log-likelihood function is:

\[
l_i(\beta, \phi) = \frac{\theta_i y_i - b(\theta_i)}{a(\phi)} + c(y_i, \phi)
\]

For \(n\) independent observations, the \textbf{total log-likelihood} is:

\[
l(\beta, \phi) = \sum_{i=1}^n l_i(\beta, \phi)
\]

Expanding this,

\[
l(\beta, \phi) = \sum_{i=1}^n \left( \frac{\theta_i y_i - b(\theta_i)}{a(\phi)} + c(y_i, \phi) \right).
\]

To estimate \(\beta\), we maximize this log-likelihood function.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{estimation-of-systematic-component-beta}{%
\subsubsection{\texorpdfstring{Estimation of Systematic Component (\(\beta\))}{Estimation of Systematic Component (\textbackslash beta)}}\label{estimation-of-systematic-component-beta}}

To differentiate \(l(\beta,\phi)\) with respect to \(\beta_j\), we apply the \textbf{chain rule}:

\[
\frac{\partial l_i(\beta,\phi)}{\partial \beta_j} =
\underbrace{\frac{\partial l_i(\beta,\phi)}{\partial \theta_i}}_{\text{depends on }(y_i - \mu_i)}
\times
\underbrace{\frac{\partial \theta_i}{\partial \mu_i}}_{= 1/V(\mu_i)\text{ if canonical link}}
\times
\underbrace{\frac{\partial \mu_i}{\partial \eta_i}}_{\text{depends on the link}}
\times
\underbrace{\frac{\partial \eta_i}{\partial \beta_j}}_{= x_{ij}}.
\]

Let us see why these four pieces appear:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \(l_i(\beta,\phi)\) depends on \(\theta_i\). So we start by computing \(\frac{\partial l_i}{\partial \theta_i}\).
\item
  \(\theta_i\) (the ``natural parameter'' in the exponential family) may in turn be a function of \(\mu_i\).

  \begin{itemize}
  \item
    In \textbf{canonical}link GLMs, we often have \(\theta_i = \eta_i\).
  \item
    In more \textbf{general}link GLMs,\(\theta_i\) is still some function of \(\mu_i\).\\
    Hence we need \(\frac{\partial \theta_i}{\partial \mu_i}\).
  \end{itemize}
\item
  \(\mu_i\) (the mean) is a function of the linear predictor \(\eta_i\). Typically, \(\eta_i = g(\mu_i)\) implies \(\mu_i = g^{-1}(\eta_i)\). So we need \(\frac{\partial \mu_i}{\partial \eta_i}\).
\item
  Finally, \(\eta_i = \mathbf{x}_i^\prime \beta\). So the derivative \(\frac{\partial \eta_i}{\partial \beta_j}\) is simply \(x_{ij}\), the \(j\)th component of the covariate vector \(\mathbf{x}_i\).
\end{enumerate}

Let us look at each factor in turn.

\textbf{First term:}

\[
\displaystyle \frac{\partial l_i(\beta,\phi)}{\partial \theta_i}
\]

Recall

\[
l_i(\theta_i,\phi) = \frac{\theta_i \,y_i - b(\theta_i)}{a(\phi)} + c(y_i,\phi).
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Differentiate \(\theta_i y_i - b(\theta_i)\) with respect to \(\theta_i\): \[
  \frac{\partial}{\partial \theta_i} \bigl[\theta_i\,y_i - b(\theta_i)\bigr] = y_i - b'(\theta_i).
  \] But by exponentialfamily definitions,\(b'(\theta_i) = \mu_i\).\\
  So that is \(y_i - \mu_i\).
\item
  Since everything is divided by \(a(\phi)\), we get \[
  \frac{\partial l_i}{\partial \theta_i} = \frac{1}{a(\phi)}\,[\,y_i - \mu_i\,].
  \] Hence, \[
  \boxed{ \frac{\partial l_i(\beta,\phi)}{\partial \theta_i} = \frac{y_i - \mu_i}{a(\phi)}. }
  \]
\end{enumerate}

\textbf{Second term:} \[
\displaystyle \frac{\partial \theta_i}{\partial \mu_i}
\] 1. In an exponential family with \textbf{canonical link}, we have \[
    \theta_i = \eta_i = \mathbf{x}_i^\prime \beta.
    \] Then \(\theta_i\) is actually the same as \(\eta_i\), which is the same as \(g(\mu_i)\). Recall also that if \(\mu_i = b'(\theta_i)\), then \(d\mu_i/d\theta_i = b''(\theta_i)\). But \(b''(\theta_i) = V(\mu_i)\). Hence \[
    \frac{d \mu_i}{d \theta_i} = V(\mu_i) \quad\Longrightarrow\quad \frac{d \theta_i}{d \mu_i} = \frac{1}{V(\mu_i)}.
    \] This identity is a wellknown property of \textbf{canonical} links in the exponential family.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  In more general (noncanonical) links, \(\theta_i\) may be some other smooth function of \(\mu_i\). The key idea is: if \(\mu_i = b'(\theta_i)\) but \(\eta_i \neq \theta_i\), you would have to carefully derive \(\partial \theta_i / \partial \mu_i\). Often, a canonical link is assumed to keep expressions simpler.
\end{enumerate}

If we \textbf{assume} a canonical link throughout, then

\[
\boxed{ \frac{\partial \theta_i}{\partial \mu_i} = \frac{1}{V(\mu_i)}. }
\]

\textbf{Third term:}

\[
\displaystyle \frac{\partial \mu_i}{\partial \eta_i}
\]

Here we consider the link function \(g(\cdot)\), defined by

\[
\eta_i = g(\mu_i) \quad\Longrightarrow\quad \mu_i = g^{-1}(\eta_i).
\]

For example,

\begin{itemize}
\tightlist
\item
  In a Bernoulli (logisticregression) model, \(g(\mu) = \log\frac{\mu}{1-\mu}\). So \(\mu = g^{-1}(\eta) = \frac{1}{1+e^{-\eta}}\). Then \(\frac{d\mu}{d\eta} = \mu\,(1-\mu)\).
\item
  For a Poisson (log) link, \(g(\mu) = \log(\mu)\). So \(\mu = e^\eta\). Then \(\frac{d\mu}{d\eta} = e^\eta = \mu\).
\item
  For an identity link, \(g(\mu) = \mu\). Then \(\eta = \mu\) and \(\frac{d\mu}{d\eta} = 1\).
\end{itemize}

In general,

\[
\boxed{ \frac{\partial \mu_i}{\partial \eta_i} = \left.\frac{d}{d\eta}\bigl[g^{-1}(\eta)\bigr]\right|_{\eta=\eta_i} = \left(g^{-1}\right)'(\eta_i). }
\]

If the link is \emph{also} canonical, then typically \(\frac{\partial \mu_i}{\partial \eta_i} = V(\mu_i)\). Indeed, that is consistent with the second term result.

\textbf{Fourth term:}

\[
\displaystyle \frac{\partial \eta_i}{\partial \beta_j}
\]

Finally, the linear predictor is

\[
\eta_i = \mathbf{x}_i^\prime \beta = \sum_{k=1}^p x_{ik}\,\beta_k.
\]

Hence, the derivative of \(\eta_i\) with respect to \(\beta_j\) is simply \[
\boxed{ \frac{\partial \eta_i}{\partial \beta_j} = x_{ij}. }
\]

Therefore, for the \textbf{entire} loglikelihood \(l(\beta, \phi) = \sum_{i=1}^n l_i(\beta,\phi)\), we sum over \(i\): \[
\boxed{ \frac{\partial l(\beta,\phi)}{\partial \beta_j} = \sum_{i=1}^n \Bigl[ \frac{y_i - \mu_i}{a(\phi)} \times \frac{1}{V(\mu_i)} \times \frac{\partial \mu_i}{\partial \eta_i} \times x_{ij} \Bigr]. }
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

To simplify expressions, we define the \textbf{weight}:

\[
w_i = \left(\left(\frac{\partial \eta_i}{\partial \mu_i}\right)^2 V(\mu_i)\right)^{-1}.
\]

For \textbf{canonical links}, this often simplifies to \(w_i = V(\mu_i)\), such as:

\begin{itemize}
\tightlist
\item
  \textbf{Bernoulli (logit link)}: \(w_i = p_i(1-p_i)\).
\item
  \textbf{Poisson (log link)}: \(w_i = \mu_i\).
\end{itemize}

Rewriting the score function in terms of \(w_i\):

\[
\frac{\partial l(\beta,\phi)}{\partial \beta_j} =
\sum_{i=1}^n
\left[
\frac{y_i - \mu_i}{a(\phi)}
\times
w_i
\times
\frac{\partial \eta_i}{\partial \mu_i}
\times
x_{ij}
\right].
\]

To use the \protect\hyperlink{newton-raphson}{Newton-Raphson Algorithm} for estimating \(\beta\), we require the expected second derivative:

\[
- E\left(\frac{\partial^2 l(\beta,\phi)}{\partial \beta_j \partial \beta_k}\right),
\]

which corresponds to the \((j,k)\)-th element of the \textbf{Fisher information matrix} \(\mathbf{I}(\beta)\):

\[
\mathbf{I}_{jk}(\beta) = - E\left(\frac{\partial^2 l(\beta,\phi)}{\partial \beta_j \partial \beta_k}\right) = \sum_{i=1}^n \frac{w_i}{a(\phi)}x_{ij}x_{ik}.
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Example: Bernoulli Model with Logit Link}

For a \textbf{Bernoulli} response with a \textbf{logit link function} (canonical link), we have:

\[
\begin{aligned}
b(\theta) &= \log(1 + \exp(\theta)) = \log(1 + \exp(\mathbf{x'}\beta)), \\
a(\phi) &= 1, \\
c(y_i, \phi) &= 0.
\end{aligned}
\]

From the mean and link function:

\[
\begin{aligned}
E(Y) = b'(\theta) &= \frac{\exp(\theta)}{1 + \exp(\theta)} = \mu = p, \\
\eta = g(\mu) &= \log\left(\frac{\mu}{1-\mu}\right) = \theta = \log\left(\frac{p}{1-p}\right) = \mathbf{x'}\beta.
\end{aligned}
\]

The log-likelihood for \(Y_i\) is:

\[
l_i (\beta, \phi) = \frac{y_i \theta_i - b(\theta_i)}{a(\phi)} + c(y_i, \phi) = y_i \mathbf{x'}_i \beta - \log(1+ \exp(\mathbf{x'}\beta)).
\]

We also obtain:

\[
\begin{aligned}
V(\mu_i) &= \mu_i(1-\mu_i) = p_i (1-p_i), \\
\frac{\partial \mu_i}{\partial \eta_i} &= p_i(1-p_i).
\end{aligned}
\]

Thus, the first derivative simplifies as:

\[
\begin{aligned}
\frac{\partial l(\beta, \phi)}{\partial \beta_j} &= \sum_{i=1}^n \left[\frac{y_i - \mu_i}{a(\phi)} \times \frac{1}{V(\mu_i)}\times \frac{\partial \mu_i}{\partial \eta_i} \times x_{ij} \right] \\
&= \sum_{i=1}^n (y_i - p_i) \times \frac{1}{p_i(1-p_i)} \times p_i(1-p_i) \times x_{ij} \\
&= \sum_{i=1}^n (y_i - p_i) x_{ij} \\
&= \sum_{i=1}^n \left(y_i - \frac{\exp(\mathbf{x'}_i\beta)}{1+ \exp(\mathbf{x'}_i\beta)}\right)x_{ij}.
\end{aligned}
\]

The weight term in this case is:

\[
w_i = \left(\left(\frac{\partial \eta_i}{\partial \mu_i} \right)^2 V(\mu_i)\right)^{-1} = p_i (1-p_i).
\]

Thus, the Fisher information matrix has elements:

\[
\mathbf{I}_{jk}(\beta) = \sum_{i=1}^n \frac{w_i}{a(\phi)} x_{ij}x_{ik} = \sum_{i=1}^n p_i (1-p_i)x_{ij}x_{ik}.
\]`

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The \textbf{Fisher-Scoring} algorithm for the \protect\hyperlink{maximum-likelihood-estimator}{Maximum Likelihood} estimate of \(\mathbf{\beta}\) is given by:

\[
\left(
\begin{array}
{c}
\beta_1 \\
\beta_2 \\
\vdots \\
\beta_p \\
\end{array}
\right)^{(m+1)}
=
\left(
\begin{array}
{c}
\beta_1 \\
\beta_2 \\
\vdots \\
\beta_p \\
\end{array}
\right)^{(m)} +
\mathbf{I}^{-1}(\mathbf{\beta})
\left(
\begin{array}
{c}
\frac{\partial l (\beta, \phi)}{\partial \beta_1} \\
\frac{\partial l (\beta, \phi)}{\partial \beta_2} \\
\vdots \\
\frac{\partial l (\beta, \phi)}{\partial \beta_p} \\
\end{array}
\right)\Bigg|_{\beta = \beta^{(m)}}
\]

This is similar to the \protect\hyperlink{newton-raphson}{Newton-Raphson Algorithm}, except that we replace the observed matrix of second derivatives (Hessian) with its expected value.

In matrix representation, the score function (gradient of the log-likelihood) is:

\[
\begin{aligned}
\frac{\partial l }{\partial \beta} &= \frac{1}{a(\phi)}\mathbf{X'W\Delta(y - \mu)} \\
&= \frac{1}{a(\phi)}\mathbf{F'V^{-1}(y - \mu)}
\end{aligned}
\]

The expected Fisher information matrix is:

\[
\mathbf{I}(\beta) = \frac{1}{a(\phi)}\mathbf{X'WX} = \frac{1}{a(\phi)}\mathbf{F'V^{-1}F}
\]

where:

\begin{itemize}
\tightlist
\item
  \(\mathbf{X}\) is an \(n \times p\) matrix of covariates.
\item
  \(\mathbf{W}\) is an \(n \times n\) diagonal matrix with \((i,i)\)-th element given by \(w_i\).
\item
  \(\mathbf{\Delta}\) is an \(n \times n\) diagonal matrix with \((i,i)\)-th element given by \(\frac{\partial \eta_i}{\partial \mu_i}\).
\item
  \(\mathbf{F} = \frac{\partial \mu}{\partial \beta}\) is an \(n \times p\) matrix, where the \(i\)-th row is given by \(\frac{\partial \mu_i}{\partial \beta} = (\frac{\partial \mu_i}{\partial \eta_i})\mathbf{x}'_i\).
\item
  \(\mathbf{V}\) is an \(n \times n\) diagonal matrix with \((i,i)\)-th element given by \(V(\mu_i)\).
\end{itemize}

Setting the derivative of the log-likelihood equal to zero gives the MLE equations:

\[
\mathbf{F'V^{-1}y} = \mathbf{F'V^{-1}\mu}
\]

Since all components of this equation (except for \(\mathbf{y}\)) depend on \(\beta\), we solve iteratively.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Special Cases}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Canonical Link Function
\end{enumerate}

If a canonical link is used, the estimating equations simplify to:

\[
\mathbf{X'y} = \mathbf{X'\mu}
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Identity Link Function
\end{enumerate}

If the identity link is used, the estimating equation reduces to:

\[
\mathbf{X'V^{-1}y} = \mathbf{X'V^{-1}X\hat{\beta}}
\]

which leads to the \protect\hyperlink{generalized-least-squares}{Generalized Least Squares} estimator:

\[
\hat{\beta} = (\mathbf{X'V^{-1}X})^{-1} \mathbf{X'V^{-1}y}
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Fisher-Scoring Algorithm in General Form}

The iterative update formula for Fisher-scoring can be rewritten as:

\[
\beta^{(m+1)} = \beta^{(m)} + \mathbf{(\hat{F}'\hat{V}^{-1}\hat{F})^{-1}\hat{F}'\hat{V}^{-1}(y- \hat{\mu})}
\]

Since \(\hat{F}, \hat{V}, \hat{\mu}\) depend on \(\beta\), we evaluate them at \(\beta^{(m)}\).

From an initial guess \(\beta^{(0)}\), we iterate until convergence.

\textbf{Notes}:

\begin{itemize}
\tightlist
\item
  If \(a(\phi)\) is a constant or takes the form \(m_i \phi\) with known \(m_i\), then \(\phi\) cancels from the equations, simplifying the estimation.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{estimation-of-dispersion-parameter-phi}{%
\subsubsection{\texorpdfstring{Estimation of Dispersion Parameter (\(\phi\))}{Estimation of Dispersion Parameter (\textbackslash phi)}}\label{estimation-of-dispersion-parameter-phi}}

There are two common approaches to estimating \(\phi\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \protect\hyperlink{maximum-likelihood-estimator}{Maximum Likelihood} Estimation
\end{enumerate}

The derivative of the log-likelihood with respect to \(\phi\) is:

\[
\frac{\partial l_i}{\partial \phi} = \frac{(\theta_i y_i - b(\theta_i)a'(\phi))}{a^2(\phi)} + \frac{\partial c(y_i,\phi)}{\partial \phi}
\]

The MLE of \(\phi\) satisfies the equation:

\[
\frac{a^2(\phi)}{a'(\phi)}\sum_{i=1}^n \frac{\partial c(y_i, \phi)}{\partial \phi} = \sum_{i=1}^n(\theta_i y_i - b(\theta_i))
\]

\textbf{Challenges:}

\begin{itemize}
\item
  For distributions other than the normal case, the expression for \(\frac{\partial c(y,\phi)}{\partial \phi}\) is often complicated.
\item
  Even with a \textbf{canonical link function} and constant \(a(\phi)\), there is no simple general expression for the expected Fisher information:
\end{itemize}

\[
  -E\left(\frac{\partial^2 l}{\partial \phi^2} \right)
  \]

This means that the unification GLMs provide for estimating \(\beta\) does not extend as neatly to \(\phi\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Moment Estimation (Bias-Corrected \(\chi^2\) Method)
\end{enumerate}

\begin{itemize}
\item
  The MLE is \textbf{not} the conventional approach for estimating \(\phi\) in \protect\hyperlink{generalized-linear-models}{Generalized Linear Models}.
\item
  For an exponential family distribution, the variance function is:

  \[
  \text{Var}(Y) = V(\mu)a(\phi)
  \]

  This implies the following moment-based estimator:

  \[
  \begin{aligned}
  a(\phi) &= \frac{\text{Var}(Y)}{V(\mu)} = \frac{E(Y- \mu)^2}{V(\mu)} \\
  a(\hat{\phi})  &= \frac{1}{n-p} \sum_{i=1}^n \frac{(y_i -\hat{\mu}_i)^2}{V(\hat{\mu}_i)}
  \end{aligned}
  \]

  where \(p\) is the number of parameters in \(\beta\).
\item
  For a \textbf{GLM with a canonical link function} \(g(.)= (b'(.))^{-1}\):

  \[
  \begin{aligned}
  g(\mu) &= \theta = \eta = \mathbf{x'\beta} \\
  \mu &= g^{-1}(\eta)= b'(\eta)
  \end{aligned}
  \]

  Using this, the moment estimator for \(a(\phi) = \phi\) becomes:

  \[
  \hat{\phi} = \frac{1}{n-p} \sum_{i=1}^n \frac{(y_i - g^{-1}(\hat{\eta}_i))^2}{V(g^{-1}(\hat{\eta}_i))}
  \]
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2055}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3014}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2055}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2877}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Approach
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Pros
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Cons
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{MLE} & Estimates \(\phi\) by maximizing the likelihood function & Theoretically optimal & Computationally complex, lacks a general closed-form solution \\
\textbf{Moment Estimation} & Uses a bias-corrected \(\chi^2\) method based on residual variance & Simpler, widely used in GLMs & Not as efficient as MLE \\
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{inference-1}{%
\subsection{Inference}\label{inference-1}}

The estimated variance of \(\hat{\beta}\) is given by:

\[
\hat{\text{var}}(\beta) = a(\phi)(\mathbf{\hat{F}'\hat{V}^{-1}\hat{F}})^{-1}
\]

where:

\begin{itemize}
\item
  \(\mathbf{V}\) is an \(n \times n\) diagonal matrix with diagonal elements given by \(V(\mu_i)\).
\item
  \(\mathbf{F}\) is an \(n \times p\) matrix given by:

  \[
  \mathbf{F} = \frac{\partial \mu}{\partial \beta}
  \]
\item
  Since \(\mathbf{V}\) and \(\mathbf{F}\) depend on the mean \(\mu\) (and thus on \(\beta\)), their estimates \(\mathbf{\hat{V}}\) and \(\mathbf{\hat{F}}\) depend on \(\hat{\beta}\).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

To test a general hypothesis:

\[
H_0: \mathbf{L\beta = d}
\]

where \(\mathbf{L}\) is a \(q \times p\) matrix, we use the \textbf{Wald test}:

\[
W = \mathbf{(L \hat{\beta}-d)'(a(\phi)L(\hat{F}'\hat{V}^{-1}\hat{F})L')^{-1}(L \hat{\beta}-d)}
\]

Under \(H_0\), the Wald statistic follows a \textbf{chi-square} distribution:

\[
W \sim \chi^2_q
\]

where \(q\) is the rank of \(\mathbf{L}\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Special Case: Testing a Single Coefficient}

For a hypothesis of the form:

\[
H_0: \beta_j = 0
\]

the Wald test simplifies to:

\[
W = \frac{\hat{\beta}_j^2}{\hat{\text{var}}(\hat{\beta}_j)} \sim \chi^2_1
\]

asymptotically.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Another common test is the likelihood ratio test, which compares the likelihoods of a \textbf{full model} and a \textbf{reduced model}:

\[
\Lambda = 2 \big(l(\hat{\beta}_f) - l(\hat{\beta}_r)\big) \sim \chi^2_q
\]

where:

\begin{itemize}
\tightlist
\item
  \(l(\hat{\beta}_f)\) is the log-likelihood of the \textbf{full} model.
\item
  \(l(\hat{\beta}_r)\) is the log-likelihood of the \textbf{reduced} model.
\item
  \(q\) is the number of constraints used in fitting the reduced model.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4028}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3472}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Test
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Pros
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Cons
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Wald Test} & Easy to compute, does not require fitting two models & May perform poorly in small samples \\
\textbf{Likelihood Ratio Test} & More accurate, especially for small samples & Requires fitting both full and reduced models \\
\end{longtable}

While the Wald test is more convenient, the likelihood ratio test is often preferred when sample sizes are small, as it tends to have better statistical properties.

\hypertarget{deviance}{%
\subsection{Deviance}\label{deviance}}

\protect\hyperlink{deviance}{Deviance} plays a crucial role in:

\begin{itemize}
\tightlist
\item
  \textbf{Goodness-of-fit assessment}: Checking how well a model explains the observed data.
\item
  \textbf{Statistical inference}: Used in hypothesis testing, particularly likelihood ratio tests.
\item
  \textbf{Estimating dispersion parameters}: Helps in refining variance estimates.
\item
  \textbf{Model comparison}: Facilitates selection between competing models.
\end{itemize}

Assuming the dispersion parameter \(\phi\) is known, let:

\begin{itemize}
\item
  \(\tilde{\theta}\) be the \textbf{maximum likelihood estimate (MLE)} under the \textbf{full model}.
\item
  \(\hat{\theta}\) be the \textbf{MLE} under the \textbf{reduced model}.
\end{itemize}

The \textbf{likelihood ratio statistic} (twice the difference in log-likelihoods) is:

\[
2\sum_{i=1}^{n} \frac{y_i (\tilde{\theta}_i- \hat{\theta}_i)-b(\tilde{\theta}_i) + b(\hat{\theta}_i)}{a_i(\phi)}
\]

For \textbf{exponential family} distributions, the mean parameter is:

\[
\mu = E(y) = b'(\theta)
\]

Thus, the \textbf{natural parameter} is a function of \(\mu\):

\[
\theta = \theta(\mu) = b'^{-1}(\mu)
\]

Rewriting the likelihood ratio statistic in terms of \(\mu\):

\[
2 \sum_{i=1}^n \frac{y_i\{\theta(\tilde{\mu}_i) - \theta(\hat{\mu}_i)\} - b(\theta(\tilde{\mu}_i)) + b(\theta(\hat{\mu}_i))}{a_i(\phi)}
\]

A \textbf{saturated model} provides the fullest possible fit, where each observation is perfectly predicted:

\[
\tilde{\mu}_i = y_i, \quad i = 1, \dots, n
\]

Setting \(\tilde{\theta}_i^* = \theta(y_i)\) and \(\hat{\theta}_i^* = \theta (\hat{\mu}_i)\), the likelihood ratio simplifies to:

\[
2 \sum_{i=1}^{n} \frac{y_i (\tilde{\theta}_i^* - \hat{\theta}_i^*) - b(\tilde{\theta}_i^*) + b(\hat{\theta}_i^*)}{a_i(\phi)}
\]

Following \citet{McCullagh_2019}, for \(a_i(\phi) = \phi\), we define the \textbf{scaled deviance} as:

\[
D^*(\mathbf{y, \hat{\mu}}) = \frac{2}{\phi} \sum_{i=1}^n \{y_i (\tilde{\theta}_i^*- \hat{\theta}_i^*) - b(\tilde{\theta}_i^*) + b(\hat{\theta}_i^*)\}
\]

and the \textbf{deviance} as:

\[
D(\mathbf{y, \hat{\mu}}) = \phi D^*(\mathbf{y, \hat{\mu}})
\]

where:

\begin{itemize}
\tightlist
\item
  \(D^*(\mathbf{y, \hat{\mu}})\) is \textbf{scaled deviance}.
\item
  \(D(\mathbf{y, \hat{\mu}})\) is \textbf{deviance}.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

In some models, \(a_i(\phi) = \phi m_i\), where \(m_i\) is a known scalar that varies across observations. Then, the \textbf{scaled deviance} is:

\[
D^*(\mathbf{y, \hat{\mu}}) = \sum_{i=1}^n \frac{2\{y_i (\tilde{\theta}_i^*- \hat{\theta}_i^*) - b(\tilde{\theta}_i^*) + b(\hat{\theta}_i^*)\}}{\phi m_i}
\]

The deviance is often decomposed into \textbf{deviance contributions}:

\[
D^*(\mathbf{y, \hat{\mu}}) = \sum_{i=1}^n d_i
\]

where \(d_i\) is the \textbf{deviance contribution} from the \(i\)-th observation.

\textbf{Uses of Deviance:}

\begin{itemize}
\item
  \(D\) is used for model selection.
\item
  \(D^*\) is used for goodness-of-fit tests, as it is a likelihood ratio statistic:
\end{itemize}

\[
D^*(\mathbf{y, \hat{\mu}}) = 2\{l(\mathbf{y,\tilde{\mu}})-l(\mathbf{y,\hat{\mu}})\}
\]

\begin{itemize}
\tightlist
\item
  The individual deviance contributions \(d_i\) are used to form deviance residuals.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Deviance for Normal Distribution}

For the normal distribution:

\[
\begin{aligned}
\theta &= \mu, \\
\phi &= \sigma^2, \\
b(\theta) &= \frac{1}{2} \theta^2, \\
a(\phi) &= \phi.
\end{aligned}
\]

The MLEs are:

\[
\begin{aligned}
\tilde{\theta}_i &= y_i, \\
\hat{\theta}_i &= \hat{\mu}_i = g^{-1}(\hat{\eta}_i).
\end{aligned}
\]

The \textbf{deviance} simplifies to:

\[
\begin{aligned}
D &= 2 \sum_{i=1}^n \left(y_i^2 - y_i \hat{\mu}_i - \frac{1}{2} y_i^2 + \frac{1}{2} \hat{\mu}_i^2 \right) \\
&= \sum_{i=1}^n (y_i^2 - 2y_i \hat{\mu}_i + \hat{\mu}_i^2) \\
&= \sum_{i=1}^n (y_i - \hat{\mu}_i)^2.
\end{aligned}
\]

Thus, for the normal model, the deviance corresponds to the residual sum of squares.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Deviance for Poisson Distribution}

For the Poisson distribution:

\[
\begin{aligned}
f(y) &= \exp\{y\log(\mu) - \mu - \log(y!)\}, \\
\theta &= \log(\mu), \\
b(\theta) &= \exp(\theta), \\
a(\phi) &= 1.
\end{aligned}
\]

MLEs:

\[
\begin{aligned}
\tilde{\theta}_i &= \log(y_i), \\
\hat{\theta}_i &= \log(\hat{\mu}_i), \\
\hat{\mu}_i &= g^{-1}(\hat{\eta}_i).
\end{aligned}
\]

The \textbf{deviance} simplifies to:

\[
\begin{aligned}
D &= 2 \sum_{i = 1}^n \left(y_i \log(y_i) - y_i \log(\hat{\mu}_i) - y_i + \hat{\mu}_i \right) \\
&= 2 \sum_{i = 1}^n \left[y_i \log\left(\frac{y_i}{\hat{\mu}_i}\right) - (y_i - \hat{\mu}_i) \right].
\end{aligned}
\]

The \textbf{deviance contribution} for each observation:

\[
d_i = 2 \left[y_i \log\left(\frac{y_i}{\hat{\mu}_i}\right) - (y_i - \hat{\mu}_i)\right].
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{analysis-of-deviance}{%
\subsubsection{Analysis of Deviance}\label{analysis-of-deviance}}

The \textbf{Analysis of Deviance} is a likelihood-based approach for comparing nested models in \protect\hyperlink{generalized-linear-models}{GLM}. It is analogous to the \protect\hyperlink{sec-analysis-of-variance-anova}{Analysis of Variance (ANOVA)} in linear models.

When comparing a \textbf{reduced model} (denoted by \(\hat{\mu}_r\)) and a \textbf{full model} (denoted by \(\hat{\mu}_f\)), the difference in \textbf{scaled deviance} follows an asymptotic chi-square distribution:

\[ D^*(\mathbf{y;\hat{\mu}_r}) - D^*(\mathbf{y;\hat{\mu}_f}) = 2\{l(\mathbf{y;\hat{\mu}_f})-l(\mathbf{y;\hat{\mu}_r})\} \sim \chi^2_q \]

where \(q\) is the difference in the number of free parameters between the two models.

This test provides a means to assess whether the additional parameters in the full model significantly improve model fit.

The dispersion parameter \(\phi\) is estimated as:

\[ \hat{\phi} = \frac{D(\mathbf{y, \hat{\mu}})}{n - p} \]

where:

\begin{itemize}
\tightlist
\item
  \(D(\mathbf{y, \hat{\mu}})\) is the deviance of the fitted model.
\item
  \(n\) is the total number of observations.
\item
  \(p\) is the number of estimated parameters.
\end{itemize}

\textbf{Caution:} The frequent use of \(\chi^2\) tests can be problematic due to their reliance on asymptotic approximations, especially for small samples or overdispersed data \citep{McCullagh_2019}.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{deviance-residuals}{%
\subsubsection{Deviance Residuals}\label{deviance-residuals}}

Since deviance plays a role in model evaluation, we define \textbf{deviance residuals} to examine individual data points. Given that the total deviance is:

\[ D = \sum_{i=1}^{n}d_i \]

the \textbf{deviance residual} for observation \(i\) is:

\[ r_{D_i} = \text{sign}(y_i -\hat{\mu}_i)\sqrt{d_i} \]

where:

\begin{itemize}
\tightlist
\item
  \(d_i\) is the deviance contribution from observation \(i\).
\item
  The \textbf{sign function} preserves the direction of the residual.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

To account for varying leverage, we define the \textbf{standardized deviance residual}:

\[ r_{s,i} = \frac{y_i -\hat{\mu}_i}{\hat{\sigma}(1-h_{ii})^{1/2}} \]

where:

\begin{itemize}
\tightlist
\item
  \(h_{ii}\) is the \textbf{leverage} of observation \(i\).
\item
  \(\hat{\sigma}\) is an estimate of the standard deviation.
\end{itemize}

Alternatively, using the \textbf{GLM hat matrix}:

\[ \mathbf{H}^{GLM} = \mathbf{W}^{1/2}X(X'WX)^{-1}X'\mathbf{W}^{-1/2} \]

where \(\mathbf{W}\) is an \(n \times n\) diagonal matrix with \((i,i)\)-th element given by \(w_i\) (see \protect\hyperlink{estimation-of-systematic-component-beta}{Estimation of Systematic Component (\(\beta\))}), we express standardized deviance residuals as:

\[ r_{s, D_i} = \frac{r_{D_i}}{\{\hat{\phi}(1-h_{ii}^{glm})\}^{1/2}} \]

where \(h_{ii}^{glm}\) is the \(i\)-th diagonal element of \(\mathbf{H}^{GLM}\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{pearson-chi-square-residuals}{%
\subsubsection{Pearson Chi-Square Residuals}\label{pearson-chi-square-residuals}}

Another goodness-of-fit statistic is the \textbf{Pearson Chi-Square} statistic, defined as:

\[ X^2 = \sum_{i=1}^{n} \frac{(y_i - \hat{\mu}_i)^2}{V(\hat{\mu}_i)} \]

where:

\begin{itemize}
\tightlist
\item
  \(\hat{\mu}_i\) is the fitted mean response.
\item
  \(V(\hat{\mu}_i)\) is the variance function of the response.
\end{itemize}

The \textbf{Scaled Pearson} \(\chi^2\) statistic is:

\[ \frac{X^2}{\phi} \sim \chi^2_{n-p} \]

where \(p\) is the number of estimated parameters.

The \textbf{Pearson residuals} are:

\[ X^2_i = \frac{(y_i - \hat{\mu}_i)^2}{V(\hat{\mu}_i)} \]

These residuals assess the difference between observed and fitted values, standardized by the variance.

If the assumptions hold:

\begin{itemize}
\item
  \textbf{Independent samples}
\item
  \textbf{No overdispersion}: If \(\phi = 1\), we expect:

  \[ \frac{D(\mathbf{y;\hat{\mu}})}{n-p} \approx 1, \quad \frac{X^2}{n-p} \approx 1 \]

  A value \textbf{substantially greater than 1} suggests \textbf{overdispersion} or \textbf{model misspecification}.
\item
  \textbf{Multiple groups}: If the model includes categorical predictors, separate calculations may be needed for each group.
\end{itemize}

Under these assumptions, both:

\[ \frac{X^2}{\phi} \quad \text{and} \quad D^*(\mathbf{y; \hat{\mu}}) \]

follow a \(\chi^2_{n-p}\) distribution.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{) }
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{100} 
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n) }
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{rpois}\NormalTok{(n, }\AttributeTok{lambda =} \FunctionTok{exp}\NormalTok{(}\FloatTok{0.5} \SpecialCharTok{+} \FloatTok{0.3} \SpecialCharTok{*}\NormalTok{ x))  }

\CommentTok{\# Poisson response  }
\CommentTok{\# Fit Poisson GLM }
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{family =} \FunctionTok{poisson}\NormalTok{(}\AttributeTok{link =} \StringTok{"log"}\NormalTok{))  }

\CommentTok{\# Extract deviance and Pearson residuals }
\NormalTok{deviance\_residuals }\OtherTok{\textless{}{-}} \FunctionTok{residuals}\NormalTok{(fit, }\AttributeTok{type =} \StringTok{"deviance"}\NormalTok{) }
\NormalTok{pearson\_residuals  }\OtherTok{\textless{}{-}} \FunctionTok{residuals}\NormalTok{(fit, }\AttributeTok{type =} \StringTok{"pearson"}\NormalTok{)  }
\CommentTok{\# Display residual summaries }
\FunctionTok{summary}\NormalTok{(deviance\_residuals) }
\CommentTok{\#\textgreater{}    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. }
\CommentTok{\#\textgreater{} {-}2.1286 {-}1.0189 {-}0.1441 {-}0.1806  0.6171  1.8942}
\FunctionTok{summary}\NormalTok{(pearson\_residuals)}
\CommentTok{\#\textgreater{}      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. }
\CommentTok{\#\textgreater{} {-}1.505136 {-}0.837798 {-}0.140873  0.002003  0.658076  2.366618}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  The residuals suggest a reasonably \textbf{well-fitted model} with a \textbf{slight underprediction} tendency.
\item
  No extreme deviations or severe skewness.
\item
  The largest residuals indicate \textbf{potential outliers}, but they are not extreme enough to immediately suggest model inadequacy.
\end{itemize}

\hypertarget{diagnostic-plots}{%
\subsection{Diagnostic Plots}\label{diagnostic-plots}}

Standardized residual plots help diagnose potential issues with model specification, such as an incorrect link function or variance structure. Common residual plots include:

\begin{itemize}
\item
  \textbf{Plot of Standardized Deviance Residuals vs.~Fitted Values}\\
  \[ \text{plot}(r_{s, D_i}, \hat{\mu}_i) \] or\\
  \[ \text{plot}(r_{s, D_i}, T(\hat{\mu}_i)) \] where \(T(\hat{\mu}_i)\) is a \textbf{constant information scale transformation}, which adjusts \(\hat{\mu}_i\) to maintain a stable variance across observations.
\item
  \textbf{Plot of Standardized Deviance Residuals vs.~Linear Predictor}\\
  \[ \text{plot}(r_{s, D_i}, \hat{\eta}_i) \] where \(\hat{\eta}_i\) represents the linear predictor before applying the link function.
\end{itemize}

The following table summarizes the commonly used transformations \(T(\hat{\mu}_i)\) for different random components:

\begin{longtable}[]{@{}cc@{}}
\toprule\noalign{}
Random Component & \(T(\hat{\mu}_i)\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Normal & \(\hat{\mu}\) \\
Poisson & \(2\sqrt{\hat{\mu}}\) \\
Binomial & \(2 \sin^{-1}(\sqrt{\hat{\mu}})\) \\
Gamma & \(2 \log(\hat{\mu})\) \\
Inverse Gaussian & \(-2\hat{\mu}^{-1/2}\) \\
\end{longtable}

Interpretation of Residual Plots

\begin{itemize}
\tightlist
\item
  Trend in residuals: Indicates an incorrect link function or an inappropriate scale transformation.
\item
  Systematic change in residual range with \(T(\hat{\mu})\): Suggests an incorrect choice of random component---i.e., the assumed variance function does not match the data.
\item
  Plot of absolute residuals vs.~fitted values:\\
  \[ \text{plot}(|r_{D_i}|, \hat{\mu}_i) \] This checks whether the variance function is correctly specified.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{goodness-of-fit}{%
\subsection{Goodness of Fit}\label{goodness-of-fit}}

To assess how well the model fits the data, we use:

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{deviance}{Deviance}
\item
  \protect\hyperlink{pearson-chi-square-residuals}{Pearson Chi-square Residuals}
\end{itemize}

For \textbf{nested models}, likelihood-based information criteria provide a comparison metric:

\[
\begin{aligned}
AIC &= -2l(\mathbf{\hat{\mu}}) + 2p \\
AICC &= -2l(\mathbf{\hat{\mu}}) + 2p \left(\frac{n}{n-p-1} \right) \\
BIC &= -2l(\mathbf{\hat{\mu}}) + p \log(n)
\end{aligned}
\]

where:

\begin{itemize}
\tightlist
\item
  \(l(\hat{\mu})\) is the log-likelihood at the estimated parameters.
\item
  \(p\) is the number of model parameters.
\item
  \(n\) is the number of observations.
\end{itemize}

\textbf{Important Considerations:}

\begin{itemize}
\item
  The \textbf{same data and model structure} (i.e., same link function and assumed random component distribution) must be used for comparisons.
\item
  Models can differ in \textbf{number of parameters} but must remain consistent in other respects.
\end{itemize}

While traditional \(R^2\) is not directly applicable to GLMs, an analogous measure is:

\[
R^2_p = 1 - \frac{l(\hat{\mu})}{l(\hat{\mu}_0)}
\]

where \(l(\hat{\mu}_0)\) is the log-likelihood of a model with only an intercept.

For binary response models, a \textbf{rescaled generalized} \(R^2\) is often used:

\[
\bar{R}^2 = \frac{R^2_*}{\max(R^2_*)} = \frac{1-\exp\left(-\frac{2}{n}(l(\hat{\mu}) - l(\hat{\mu}_0))\right)}{1 - \exp\left(\frac{2}{n} l(\hat{\mu}_0) \right)}
\]

where the denominator ensures the maximum possible \(R^2\) scaling.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{over-dispersion}{%
\subsection{Over-Dispersion}\label{over-dispersion}}

Over-dispersion occurs when the observed variance exceeds what the assumed model allows. This is common in \protect\hyperlink{sec-poisson-regression}{Poisson} and \protect\hyperlink{sec-binomial-regression}{Binomial} models.

\textbf{Variance Assumptions for Common Random Components}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2222}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3194}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4583}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Random Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Standard Assumption (\(\text{var}(Y)\))
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Alternative Model Allowing Over-Dispersion (\(V(\mu)\))
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Binomial} & \(n \mu (1- \mu)\) & \(\phi n \mu (1- \mu)\), where \(m_i = n\) \\
\textbf{Poisson} & \(\mu\) & \(\phi \mu\) \\
\end{longtable}

\begin{itemize}
\tightlist
\item
  \textbf{By default,} \(\phi = 1\), meaning the variance follows the assumed model.
\item
  If \(\phi \neq 1\), the variance differs from the expectation:

  \begin{itemize}
  \tightlist
  \item
    \(\phi > 1\): \textbf{Over-dispersion} (greater variance than expected).
  \item
    \(\phi < 1\): \textbf{Under-dispersion} (less variance than expected).
  \end{itemize}
\end{itemize}

This discrepancy suggests the assumed \textbf{random component distribution} may be inappropriate.

To account for dispersion issues, we can:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Choose a more flexible random component distribution}

  \begin{itemize}
  \tightlist
  \item
    \protect\hyperlink{sec-negative-binomial-regression}{Negative Binomial} for over-dispersed \protect\hyperlink{sec-poisson-regression}{Poisson} data.
  \item
    Conway-Maxwell Poisson for both over- and under-dispersed count data.
  \end{itemize}
\item
  \textbf{Use Mixed Models to account for random effects}

  \begin{itemize}
  \tightlist
  \item
    \protect\hyperlink{sec-nonlinear-and-generalized-linear-mixed-models}{Nonlinear and Generalized Linear Mixed Models} introduce random effects, which can help capture additional variance.
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load necessary libraries}
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(MASS)   }\CommentTok{\# For negative binomial models}
\FunctionTok{library}\NormalTok{(MuMIn)  }\CommentTok{\# For R\^{}2 in GLMs}
\FunctionTok{library}\NormalTok{(glmmTMB) }\CommentTok{\# For handling overdispersion}

\CommentTok{\# Generate Example Data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{100}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{mu }\OtherTok{\textless{}{-}} \FunctionTok{exp}\NormalTok{(}\FloatTok{0.5} \SpecialCharTok{+} \FloatTok{0.3} \SpecialCharTok{*}\NormalTok{ x)  }\CommentTok{\# True mean function}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{rpois}\NormalTok{(n, }\AttributeTok{lambda =}\NormalTok{ mu)  }\CommentTok{\# Poisson outcome}

\CommentTok{\# Fit a Poisson GLM}
\NormalTok{model\_pois }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{family =} \FunctionTok{poisson}\NormalTok{(}\AttributeTok{link =} \StringTok{"log"}\NormalTok{))}

\CommentTok{\# Compute residuals}
\NormalTok{resid\_dev }\OtherTok{\textless{}{-}} \FunctionTok{residuals}\NormalTok{(model\_pois, }\AttributeTok{type =} \StringTok{"deviance"}\NormalTok{)}
\NormalTok{fitted\_vals }\OtherTok{\textless{}{-}} \FunctionTok{fitted}\NormalTok{(model\_pois)}

\CommentTok{\# Standardized Residual Plot: Residuals vs Fitted Values}
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =} \FunctionTok{data.frame}\NormalTok{(fitted\_vals, resid\_dev),}
       \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ fitted\_vals, }\AttributeTok{y =}\NormalTok{ resid\_dev)) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.6}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"loess"}\NormalTok{,}
                \AttributeTok{color =} \StringTok{"red"}\NormalTok{,}
                \AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Standardized Deviance Residuals vs Fitted Values"}\NormalTok{,}
         \AttributeTok{x =} \StringTok{"Fitted Values"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Standardized Deviance Residuals"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{07-generalized-linear-models_files/figure-latex/unnamed-chunk-50-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Absolute Residuals vs Fitted Values (Variance Function Check)}
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =} \FunctionTok{data.frame}\NormalTok{(fitted\_vals, }\AttributeTok{abs\_resid =} \FunctionTok{abs}\NormalTok{(resid\_dev)),}
       \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ fitted\_vals, }\AttributeTok{y =}\NormalTok{ abs\_resid)) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.6}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"loess"}\NormalTok{,}
                \AttributeTok{color =} \StringTok{"blue"}\NormalTok{,}
                \AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Absolute Deviance Residuals vs Fitted Values"}\NormalTok{,}
         \AttributeTok{x =} \StringTok{"Fitted Values"}\NormalTok{, }\AttributeTok{y =} \StringTok{"|Residuals|"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{07-generalized-linear-models_files/figure-latex/unnamed-chunk-50-2} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Goodness{-}of{-}Fit Metrics}
\FunctionTok{AIC}\NormalTok{(model\_pois)    }\CommentTok{\# Akaike Information Criterion}
\CommentTok{\#\textgreater{} [1] 322.9552}
\FunctionTok{BIC}\NormalTok{(model\_pois)    }\CommentTok{\# Bayesian Information Criterion}
\CommentTok{\#\textgreater{} [1] 328.1655}
\FunctionTok{logLik}\NormalTok{(model\_pois) }\CommentTok{\# Log{-}likelihood}
\CommentTok{\#\textgreater{} \textquotesingle{}log Lik.\textquotesingle{} {-}159.4776 (df=2)}

\CommentTok{\# R{-}squared for GLM}
\NormalTok{r\_squared }\OtherTok{\textless{}{-}}
    \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ (}\FunctionTok{logLik}\NormalTok{(model\_pois) }\SpecialCharTok{/} \FunctionTok{logLik}\NormalTok{(}\FunctionTok{glm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }\AttributeTok{family =}\NormalTok{ poisson)))}
\NormalTok{r\_squared}
\CommentTok{\#\textgreater{} \textquotesingle{}log Lik.\textquotesingle{} 0.05192856 (df=2)}

\CommentTok{\# Overdispersion Check}
\NormalTok{resid\_pearson }\OtherTok{\textless{}{-}} \FunctionTok{residuals}\NormalTok{(model\_pois, }\AttributeTok{type =} \StringTok{"pearson"}\NormalTok{)}
\CommentTok{\# Estimated dispersion parameter}
\NormalTok{phi\_hat }\OtherTok{\textless{}{-}}
    \FunctionTok{sum}\NormalTok{(resid\_pearson }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{) }\SpecialCharTok{/}\NormalTok{ (n }\SpecialCharTok{{-}} \FunctionTok{length}\NormalTok{(}\FunctionTok{coef}\NormalTok{(model\_pois)))  }
\NormalTok{phi\_hat}
\CommentTok{\#\textgreater{} [1] 1.055881}

\CommentTok{\# If phi \textgreater{} 1, }
\CommentTok{\# fit a Negative Binomial Model to correct for overdispersion}
\ControlFlowTok{if}\NormalTok{ (phi\_hat }\SpecialCharTok{\textgreater{}} \DecValTok{1}\NormalTok{) \{}
\NormalTok{    model\_nb }\OtherTok{\textless{}{-}} \FunctionTok{glm.nb}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x)}
    \FunctionTok{summary}\NormalTok{(model\_nb)}
\NormalTok{\}}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} glm.nb(formula = y \textasciitilde{} x, init.theta = 50.70707605, link = log)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Deviance Residuals: }
\CommentTok{\#\textgreater{}     Min       1Q   Median       3Q      Max  }
\CommentTok{\#\textgreater{} {-}2.1034  {-}0.9978  {-}0.1393   0.6047   1.8553  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}             Estimate Std. Error z value Pr(\textgreater{}|z|)    }
\CommentTok{\#\textgreater{} (Intercept)  0.42913    0.08514   5.040 4.65e{-}07 ***}
\CommentTok{\#\textgreater{} x            0.35262    0.08654   4.075 4.61e{-}05 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} (Dispersion parameter for Negative Binomial(50.7071) family taken to be 1)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Null deviance: 135.71  on 99  degrees of freedom}
\CommentTok{\#\textgreater{} Residual deviance: 118.92  on 98  degrees of freedom}
\CommentTok{\#\textgreater{} AIC: 324.91}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Fisher Scoring iterations: 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}               Theta:  51 }
\CommentTok{\#\textgreater{}           Std. Err.:  233 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  2 x log{-}likelihood:  {-}318.906}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Standardized Residuals vs.~Fitted Values
\end{enumerate}

\begin{itemize}
\item
  If there is a clear trend in the residual plot (e.g., a curve), this suggests an incorrect link function.
\item
  If the residual spread increases with fitted values, the variance function may be misspecified.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Absolute Residuals vs.~Fitted Values
\end{enumerate}

\begin{itemize}
\item
  A systematic pattern (e.g., funnel shape) suggests heteroscedasticity (i.e., changing variance).
\item
  If the residuals increase with fitted values, a different variance function (e.g., Negative Binomial instead of Poisson) may be needed.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Goodness-of-Fit Metrics
\end{enumerate}

\begin{itemize}
\item
  AIC/BIC: Lower values indicate a better model, but must be compared across nested models.
\item
  Log-likelihood: Higher values suggest a better-fitting model.
\item
  \(R^2\) for GLMs: Since traditional \(R^2\) is not available, the likelihood-based \(R^2\) is used to measure improvement over a null model.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Overdispersion Check (\(\phi\))
\end{enumerate}

\begin{itemize}
\item
  If \(\phi \approx 1\), the Poisson assumption is valid.
\item
  If \(\phi > 1\), there is overdispersion, meaning a Negative Binomial model may be more appropriate.
\item
  If \(\phi < 1\), underdispersion is present, requiring alternative distributions like the Conway-Maxwell Poisson.
\end{itemize}

\hypertarget{sec-linear-mixed-models}{%
\chapter{Linear Mixed Models}\label{sec-linear-mixed-models}}

\hypertarget{dependent-data}{%
\section{Dependent Data}\label{dependent-data}}

In many real-world applications, observations are not independent but exhibit \textbf{correlations} due to shared characteristics. Below are common forms of dependent data:

\begin{itemize}
\tightlist
\item
  \textbf{Multivariate measurements on individuals}: Multiple attributes measured on the same person may be correlated (e.g., blood pressure, cholesterol, and BMI).
\item
  \textbf{Clustered measurements}: Individuals within a shared environment (e.g., families, schools, hospitals) often exhibit correlated responses.
\item
  \textbf{Repeated measurements}: When the same individual is measured multiple times, correlations arise naturally.

  \begin{itemize}
  \tightlist
  \item
    \emph{Example}: Tracking cholesterol levels of a person over time.
  \item
    If these repeated measurements follow an experimental design where treatments were applied initially, they are referred to as repeated measures \citep{Schabenberger_2001}.
  \end{itemize}
\item
  \textbf{Longitudinal data}: Repeated measurements taken over time in an observational study are known as longitudinal data \citep{Schabenberger_2001}.
\item
  \textbf{Spatial data}: Measurements taken from individuals in nearby locations (e.g., residents of the same neighborhood) often exhibit spatial correlation.
\end{itemize}

Since standard linear regression assumes independent observations, these correlations violate its assumptions. Thus, \protect\hyperlink{sec-linear-mixed-models}{Linear Mixed Models} (LMMs) provide a framework to account for these dependencies.

A \protect\hyperlink{sec-linear-mixed-models}{Linear Mixed Model} (also called a \textbf{Mixed Linear Model}) consists of two components:

\begin{itemize}
\tightlist
\item
  \textbf{Fixed effects}: Parameters associated with variables that have the same effect across all observations (e.g., gender, age, diet, time).
\item
  \textbf{Random effects}: Individual-specific variations or correlation structures (e.g., subject-specific effects, spatial correlations), leading to dependent (correlated) errors.
\end{itemize}

A key advantage of LMMs is that they model random subject-specific effects, rather than including individual dummy variables. This provides:

\begin{itemize}
\tightlist
\item
  A reduction in the number of parameters to estimate, avoiding overfitting.
\item
  A framework for inference on a population level, rather than restricting conclusions to observed individuals.
\end{itemize}

\hypertarget{motivation-a-repeated-measurements-example}{%
\subsection{Motivation: A Repeated Measurements Example}\label{motivation-a-repeated-measurements-example}}

Consider a scenario where we analyze \textbf{repeated measurements} of a response variable \(Y_{ij}\) for the \(i\)-th subject at time \(j\):

\begin{itemize}
\tightlist
\item
  \(i = 1, \dots, N\) (subjects)
\item
  \(j = 1, \dots, n_i\) (measurements per subject)
\end{itemize}

We define the response vector for subject \(i\) as:

\[
\mathbf{Y}_i = 
\begin{bmatrix}
Y_{i1} \\
Y_{i2} \\
\vdots \\
Y_{in_i}
\end{bmatrix}
\]

To model this data, we use a two-stage hierarchical approach:

\hypertarget{stage-1-regression-model-within-subject-variation-sec-stage-1-regression-model-within-subject-variation}{%
\subsubsection{Stage 1: Regression Model (Within-Subject Variation) \{\#sec-stage-1-regression-model-(within-subject-variation\}}\label{stage-1-regression-model-within-subject-variation-sec-stage-1-regression-model-within-subject-variation}}

We first model how the response changes over time for each subject:

\[
\mathbf{Y}_i = Z_i \beta_i + \epsilon_i
\]

where:

\begin{itemize}
\tightlist
\item
  \(Z_i\) is an \(n_i \times q\) matrix of known covariates (e.g., time, treatment).
\item
  \(\beta_i\) is a \(q \times 1\) vector of subject-specific regression coefficients.
\item
  \(\epsilon_i\) represents random errors, often assumed to follow \(\epsilon_i \sim N(0, \sigma^2 I)\).
\end{itemize}

At this stage, each individual has their own unique regression coefficients \(\beta_i\). However, estimating a separate \(\beta_i\) for each subject is impractical when \(N\) is large. Thus, we introduce a {[}second stage{]}(\#sec-stage-2-parameter-model-(between-subject-variation) to impose structure on \(\beta_i\).

\hypertarget{stage-2-parameter-model-between-subject-variation-sec-stage-2-parameter-model-between-subject-variation}{%
\subsubsection{Stage 2: Parameter Model (Between-Subject Variation) \{\#sec-stage-2-parameter-model-(between-subject-variation\}}\label{stage-2-parameter-model-between-subject-variation-sec-stage-2-parameter-model-between-subject-variation}}

We assume that the subject-specific coefficients \(\beta_i\) arise from a common population distribution:

\[
\beta_i = K_i \beta + b_i
\]

where:

\begin{itemize}
\tightlist
\item
  \(K_i\) is a \(q \times p\) matrix of known covariates.
\item
  \(\beta\) is a \(p \times 1\) vector of global parameters (fixed effects).
\item
  \(\mathbf{b}_i\) are random effects, assumed to follow \(\mathbf{b}_i \sim N(0, D)\).
\end{itemize}

Thus, each individual's regression coefficients \(\beta_i\) are modeled as deviations from a population-level mean \(\beta\), with subject-specific variations \(b_i\). This hierarchical structure enables:

\begin{itemize}
\tightlist
\item
  Borrowing of strength: Individual estimates \(\beta_i\) are informed by the overall population distribution.
\item
  Improved generalization: The model captures both fixed and random variability efficiently.
\end{itemize}

The full \protect\hyperlink{sec-linear-mixed-models}{Linear Mixed Model} can then be written as:

\[
\mathbf{Y}_i = Z_i K_i \beta + Z_i b_i + \epsilon_i
\]

where:

\begin{itemize}
\tightlist
\item
  \(Z_i K_i \beta\) represents the \textbf{fixed effects} component.
\item
  \(Z_i b_i\) represents the \textbf{random effects} component.
\item
  \(\epsilon_i\) represents the residual errors.
\end{itemize}

This formulation accounts for both \textbf{within-subject correlations} (via random effects) and \textbf{between-subject variability} (via fixed effects), making it a powerful tool for analyzing dependent data.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-linear-mixed-model-for-repeated-measurements}{%
\subsection{Example: Linear Mixed Model for Repeated Measurements}\label{example-linear-mixed-model-for-repeated-measurements}}

\hypertarget{sec-stage-1-subject-specific-model-lmm-example}{%
\subsubsection{Stage 1: Subject-Specific Model}\label{sec-stage-1-subject-specific-model-lmm-example}}

The first stage models the response variable \(Y_{ij}\) for subject \(i\) at time \(t_{ij}\):

\[
Y_{ij} = \beta_{1i} + \beta_{2i} t_{ij} + \epsilon_{ij}
\]

where:

\begin{itemize}
\tightlist
\item
  \(j = 1, \dots, n_i\) represents different time points for subject \(i\).
\item
  \(\beta_{1i}\) is the subject-specific intercept (baseline response for subject \(i\)).
\item
  \(\beta_{2i}\) is the subject-specific slope (rate of change over time).
\item
  \(\epsilon_{ij} \sim N(0, \sigma^2)\) are independent errors.
\end{itemize}

In matrix notation, the model is written as:

\[
\mathbf{Y}_i = 
\begin{bmatrix}
Y_{i1} \\
Y_{i2} \\
\vdots \\
Y_{in_i}
\end{bmatrix}, 
\quad
\mathbf{Z}_i = 
\begin{bmatrix}
1 & t_{i1} \\
1 & t_{i2} \\
\vdots & \vdots \\
1 & t_{in_i} 
\end{bmatrix},
\]

\[
\beta_i =
\begin{bmatrix}
\beta_{1i} \\
\beta_{2i}
\end{bmatrix}, 
\quad
\epsilon_i = 
\begin{bmatrix}
\epsilon_{i1} \\
\epsilon_{i2} \\
\vdots \\
\epsilon_{in_i}
\end{bmatrix}.
\]

Thus, the model can be rewritten as:

\[
\mathbf{Y_i = Z_i \beta_i + \epsilon_i}.
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{sec-stage-2-population-level-model-lmm-example}{%
\subsubsection{Stage 2: Population-Level Model}\label{sec-stage-2-population-level-model-lmm-example}}

Since estimating a separate \(\beta_{1i}\) and \(\beta_{2i}\) for each subject is impractical, we assume they follow a population distribution:

\[
\begin{aligned}
\beta_{1i} &= \beta_0 + b_{1i}, \\
\beta_{2i} &= \beta_1 L_i + \beta_2 H_i + \beta_3 C_i + b_{2i}.
\end{aligned}
\]

where:

\begin{itemize}
\tightlist
\item
  \(L_i, H_i, C_i\) are indicator variables for treatment groups:

  \begin{itemize}
  \tightlist
  \item
    \(L_i = 1\) if the subject belongs to treatment group 1, else 0.
  \item
    \(H_i = 1\) if the subject belongs to treatment group 2, else 0.
  \item
    \(C_i = 1\) if the subject belongs to treatment group 3, else 0.
  \end{itemize}
\item
  \(\beta_0\) represents the average baseline response across all subjects.
\item
  \(\beta_1, \beta_2, \beta_3\) are the average time effects for the respective treatment groups.
\item
  \(b_{1i}, b_{2i}\) are random effects representing subject-specific deviations.
\end{itemize}

This structure implies that while the intercept \(\beta_{1i}\) varies randomly across subjects, the slope \(\beta_{2i}\) depends both on treatment and random subject-specific deviations.

In matrix form, this is:

\[
\begin{aligned}
\mathbf{K}_i &= 
\begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & L_i & H_i & C_i 
\end{bmatrix}, \\ 
\beta &= 
\begin{bmatrix}
\beta_0 \\
\beta_1 \\
\beta_2 \\
\beta_3
\end{bmatrix}, \\ 
\mathbf{b}_i &= 
\begin{bmatrix}
b_{1i} \\
b_{2i}
\end{bmatrix}, \\ 
\beta_i &= \mathbf{K_i \beta + b_i}.
\end{aligned}
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{final-mixed-model-formulation}{%
\subsubsection{Final Mixed Model Formulation}\label{final-mixed-model-formulation}}

Substituting \protect\hyperlink{sec-stage-2-population-level-model-lmm-example}{Stage 2} into \protect\hyperlink{sec-stage-1-subject-specific-model-lmm-example}{Stage 1}, we obtain the full mixed model:

\[
\mathbf{Y}_i = \mathbf{Z}_i (\mathbf{K}_i \beta + \mathbf{b}_i) + \epsilon_i.
\]

Expanding:

\[
\mathbf{Y}_i = \mathbf{Z}_i \mathbf{K}_i \beta + \mathbf{Z}_i \mathbf{b}_i + \epsilon_i.
\]

Interpretation:

\begin{itemize}
\tightlist
\item
  \(\mathbf{Z}_i \mathbf{K}_i \beta\) represents the \textbf{fixed effects} (population-level trends).
\item
  \(\mathbf{Z}_i \mathbf{b}_i\) represents the \textbf{random effects} (subject-specific variations).
\item
  \(\epsilon_i\) represents the \textbf{residual errors}.
\end{itemize}

\textbf{Assumptions}:

\begin{itemize}
\tightlist
\item
  Random effects: \(\mathbf{b}_i \sim N(0, D)\), where \(D\) is the variance-covariance matrix.
\item
  Residual errors: \(\epsilon_i \sim N(0, \sigma^2 I)\).
\item
  Independence: \(\mathbf{b}_i\) and \(\epsilon_i\) are independent.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

To estimate \(\hat{\beta}\), one might consider a sequential approach:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Estimate \(\hat{\beta}_i\) for each subject in the first stage.
\item
  Estimate \(\hat{\beta}\) in the second stage by replacing \(\beta_i\) with \(\hat{\beta}_i\).
\end{enumerate}

However, this method introduces several problems:

\begin{itemize}
\tightlist
\item
  Loss of information: Summarizing \(\mathbf{Y}_i\) solely by \(\hat{\beta}_i\) discards valuable within-subject variability.
\item
  Ignoring uncertainty: Treating estimated values \(\hat{\beta}_i\) as known values leads to underestimated variability.
\item
  Unequal sample sizes: Subjects may have different numbers of observations (\(n_i\)), which affects variance estimation.
\end{itemize}

To address these issues, we adopt the \protect\hyperlink{sec-linear-mixed-models}{Linear Mixed Model} (LMM) framework \citep{laird1982random}.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{reformulating-the-linear-mixed-model}{%
\subsubsection{Reformulating the Linear Mixed Model}\label{reformulating-the-linear-mixed-model}}

Substituting {[}Stage 2{]}(\#sec-stage-2-parameter-model-(between-subject-variation) into {[}Stage 1{]}(\#sec-stage-2-parameter-model-(between-subject-variation), we obtain:

\[
\mathbf{Y}_i = \mathbf{Z}_i \mathbf{K}_i \beta + \mathbf{Z}_i \mathbf{b}_i + \mathbf{\epsilon}_i.
\]

Defining \(\mathbf{X}_i = \mathbf{Z}_i \mathbf{K}_i\) as an \(n_i \times p\) matrix of covariates, we rewrite the model as:

\[
\mathbf{Y}_i = \mathbf{X}_i \beta + \mathbf{Z}_i \mathbf{b}_i + \mathbf{\epsilon}_i.
\]

where:

\begin{itemize}
\item
  \(i = 1, \dots, N\) (subjects).
\item
  \(\beta\) are \textbf{fixed effects}, common to all subjects.
\item
  \(\mathbf{b}_i\) are \textbf{subject-specific random effects}, assumed to follow:

  \[
  \mathbf{b}_i \sim N_q(\mathbf{0,D}).
  \]
\item
  \(\mathbf{\epsilon}_i\) represents residual errors:

  \[
  \mathbf{\epsilon}_i \sim N_{n_i}(\mathbf{0,\Sigma_i}).
  \]
\item
  Independence assumption: \(\mathbf{b}_i\) and \(\mathbf{\epsilon}_i\) are independent.
\item
  Dimension notation:

  \begin{itemize}
  \tightlist
  \item
    \(\mathbf{Z}_i\) is an \(n_i \times q\) matrix of known covariates for random effects.
  \item
    \(\mathbf{X}_i\) is an \(n_i \times p\) matrix of known covariates for fixed effects.
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{hierarchical-conditional-formulation}{%
\subsubsection{Hierarchical (Conditional) Formulation}\label{hierarchical-conditional-formulation}}

Rewriting the LMM in \textbf{hierarchical form}:

\[
\begin{aligned}
\mathbf{Y}_i | \mathbf{b}_i &\sim N(\mathbf{X}_i \beta+ \mathbf{Z}_i \mathbf{b}_i, \mathbf{\Sigma}_i), \\
\mathbf{b}_i &\sim N(\mathbf{0,D}).
\end{aligned}
\]

where:

\begin{itemize}
\tightlist
\item
  The first equation states that, given the subject-specific random effects \(\mathbf{b}_i\), the response follows a normal distribution with mean \(\mathbf{X}_i \beta+ \mathbf{Z}_i \mathbf{b}_i\) and covariance \(\mathbf{\Sigma}_i\).
\item
  The second equation states that the random effects \(\mathbf{b}_i\) follow a \textbf{multivariate normal} distribution with mean zero and covariance \(D\).
\end{itemize}

We denote the respective probability density functions as:

\[
f(\mathbf{Y}_i |\mathbf{b}_i) \quad \text{and} \quad f(\mathbf{b}_i).
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Using the general \textbf{marginalization formula}:

\[
\begin{aligned}
f(A,B) &= f(A|B)f(B) \\
f(A) &= \int f(A,B)dB = \int f(A|B) f(B) dB
\end{aligned}
\]

we obtain the \textbf{marginal density} of \(\mathbf{Y}_i\):

\[
f(\mathbf{Y}_i) = \int f(\mathbf{Y}_i | \mathbf{b}_i) f(\mathbf{b}_i) d\mathbf{b}_i.
\]

Solving this integral, we obtain:

\[
\mathbf{Y}_i \sim N(\mathbf{X_i \beta}, \mathbf{Z_i D Z_i'} + \mathbf{\Sigma_i}).
\]

Interpretation:

\begin{itemize}
\item
  \textbf{Mean structure:} The expectation remains \(\mathbf{X}_i \beta\), the fixed effects.
\item
  \textbf{Variance structure:} The covariance now incorporates random effect variability:

  \[
  \mathbf{Z_i D Z_i'} + \mathbf{\Sigma_i}.
  \]

  This shows that the random effects contribute additional correlation between observations.
\end{itemize}

\begin{quote}
 \textbf{Key Takeaway:}\\
The marginal formulation of LMM no longer includes \(Z_i b_i\) in the mean, but instead incorporates it into the covariance structure, introducing marginal dependence in \(\mathbf{Y}_i\).
\end{quote}

Technically, rather than ``averaging out'' the random effect \(b_i\), we add its contribution to the variance-covariance matrix.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Continue with the example:

\[
Y_{ij} = (\beta_0 + b_{1i}) + (\beta_1L_i + \beta_2 H_i + \beta_3 C_i + b_{2i})t_{ij} + \epsilon_{ij}.
\]

For each treatment group:

\[
Y_{ij} =
\begin{cases}
\beta_0 + b_{1i} + (\beta_1 + b_{2i}) t_{ij} + \epsilon_{ij}, & L_i = 1 \\
\beta_0 + b_{1i} + (\beta_2 + b_{2i}) t_{ij} + \epsilon_{ij}, & H_i = 1 \\
\beta_0 + b_{1i} + (\beta_3 + b_{2i}) t_{ij} + \epsilon_{ij}, & C_i = 1.
\end{cases}
\]

Interpretation:

\begin{itemize}
\tightlist
\item
  \textbf{Intercepts and slopes are subject-specific}: Each subject has their own baseline response and rate of change.
\item
  \textbf{Treatment groups affect slopes, but not intercepts}:

  \begin{itemize}
  \tightlist
  \item
    \(\beta_0\) is the common intercept across groups.
  \item
    Slopes differ by treatment: \(\beta_1\) for \(L\), \(\beta_2\) for \(H\), \(\beta_3\) for \(C\).
  \end{itemize}
\item
  \textbf{Random effects} introduce within-subject correlation:

  \begin{itemize}
  \tightlist
  \item
    \(b_{1i}\) allows individual variation in baseline response.
  \item
    \(b_{2i}\) allows subject-specific deviations in slopes.
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

In the \textbf{hierarchical model form}, we again express the LMM as:

\[
\begin{aligned}
\mathbf{Y}_i | \mathbf{b}_i &\sim N(\mathbf{X}_i \beta + \mathbf{Z}_i \mathbf{b}_i, \mathbf{\Sigma}_i)\\
\mathbf{b}_i &\sim N(\mathbf{0,D})
\end{aligned}
\]

where:

\begin{itemize}
\tightlist
\item
  \(\mathbf{X}_i\) and \(\mathbf{Z}_i\) are design matrices for \textbf{fixed} and \textbf{random} effects, respectively.
\item
  \(\mathbf{b}_i\) represents \textbf{subject-specific random effects}.
\item
  \(\mathbf{\Sigma}_i\) is the \textbf{residual error covariance matrix}.
\item
  \(\mathbf{D}\) is the \textbf{random effects covariance matrix}.
\end{itemize}

The fixed-effects parameter vector is:

\[
\beta = (\beta_0, \beta_1, \beta_2, \beta_3)'.
\]

From the model structure, we define:

\[
\mathbf{X}_i = \mathbf{Z}_i \mathbf{K}_i.
\]

Expanding,

\[
\mathbf{Z}_i =
\begin{bmatrix}
1 & t_{i1} \\
1 & t_{i2} \\
\vdots & \vdots \\
1 & t_{in_i}
\end{bmatrix},
\quad
\mathbf{K}_i =
\begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & L_i & H_i & C_i
\end{bmatrix}.
\]

Multiplying:

\[
\mathbf{X}_i =
\begin{bmatrix}
1 & t_{i1}L_i & t_{i1}H_i & t_{i1}C_i \\
1 & t_{i2}L_i & t_{i2}H_i & t_{i2}C_i \\
\vdots & \vdots & \vdots & \vdots \\
1 & t_{in_i}L_i & t_{in_i}H_i & t_{in_i}C_i
\end{bmatrix}.
\]

The random effects vector is:

\[
\mathbf{b}_i =
\begin{bmatrix}
b_{1i} \\
b_{2i}
\end{bmatrix}.
\]

The covariance matrix \(\mathbf{D}\) for random effects is:

\[
\mathbf{D} =
\begin{bmatrix}
d_{11} & d_{12} \\
d_{12} & d_{22}
\end{bmatrix}.
\]

We assume \textbf{conditional independence}:

\[
\mathbf{\Sigma}_i = \sigma^2 \mathbf{I}_{n_i}.
\]

This means that, \textbf{given the random effects} \(\mathbf{b}_i\) and \(\beta\), the responses for subject \(i\) are independent.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

To derive the \textbf{marginal model}, we integrate out the random effects \(\mathbf{b}_i\). This leads to:

\[
Y_{ij} = \beta_0 + \beta_1 L_i t_{ij} + \beta_2 H_i t_{ij} + \beta_3 C_i t_{ij} + \eta_{ij},
\]

where:

\[
\eta_i \sim N(\mathbf{0}, \mathbf{Z}_i \mathbf{D} \mathbf{Z}_i' + \mathbf{\Sigma}_i).
\]

Thus, the full marginal model is:

\[
\mathbf{Y_i} \sim N(\mathbf{X}_i \beta, \mathbf{Z}_i \mathbf{D} \mathbf{Z}_i' + \mathbf{\Sigma}_i).
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Example: Case Where \(n_i = 2\)

For a subject with \textbf{two observations}, we compute:

\[
\mathbf{Z}_i =
\begin{bmatrix}
1 & t_{i1} \\
1 & t_{i2}
\end{bmatrix}.
\]

Then:

\[
\mathbf{Z}_i \mathbf{D} \mathbf{Z}_i' =
\begin{bmatrix}
1 & t_{i1} \\
1 & t_{i2}
\end{bmatrix}
\begin{bmatrix}
d_{11} & d_{12} \\
d_{12} & d_{22}
\end{bmatrix}
\begin{bmatrix}
1 & 1 \\
t_{i1} & t_{i2}
\end{bmatrix}.
\]

Expanding the multiplication:

\[
\mathbf{Z}_i \mathbf{D} \mathbf{Z}_i' =
\begin{bmatrix}
d_{11} + 2d_{12}t_{i1} + d_{22} t_{i1}^2 & d_{11} + d_{12}(t_{i1} + t_{i2}) + d_{22}t_{i1}t_{i2} \\
d_{11} + d_{12}(t_{i1} + t_{i2}) + d_{22}t_{i1}t_{i2} & d_{11} + 2d_{12}t_{i2} + d_{22} t_{i2}^2
\end{bmatrix}.
\]

Finally, incorporating the residual variance:

\[
\text{Var}(Y_{i1}) = d_{11} + 2d_{12}t_{i1} + d_{22} t_{i1}^2 + \sigma^2.
\]

Interpretation of the Marginal Model

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Correlation in the Errors}:

  \begin{itemize}
  \tightlist
  \item
    The off-diagonal elements of \(\mathbf{Z}_i \mathbf{D} \mathbf{Z}_i'\) introduce \textbf{correlation} between observations within the same subject.
  \item
    This accounts for the fact that repeated measurements on the same individual are not independent.
  \end{itemize}
\item
  \textbf{Variance Structure}:

  \begin{itemize}
  \item
    The variance function of \(Y_{ij}\) is quadratic in time:

    \[
    \text{Var}(Y_{ij}) = d_{11} + 2d_{12}t_{ij} + d_{22}t_{ij}^2 + \sigma^2.
    \]
  \item
    The curvature of this variance function is determined by \(d_{22}\).
  \item
    If \(d_{22} > 0\), variance increases over time.
  \end{itemize}
\end{enumerate}

\begin{quote}
 \textbf{Key Takeaway}:\\
In the hierarchical model, random effects contribute to the mean structure.\\
In the marginal model, they affect the covariance structure, leading to heteroskedasticity (changing variance over time) and correlation between repeated measures.
\end{quote}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{sec-random-intercepts-model-lmm}{%
\subsection{Random-Intercepts Model}\label{sec-random-intercepts-model-lmm}}

The \href{}{Random-Intercepts Model} is obtained by \textbf{removing random slopes}, meaning:

\begin{itemize}
\tightlist
\item
  All subject-specific variability in slopes is attributed only to treatment differences.
\item
  The model allows each subject to have their own intercept, but within each treatment group, all subjects share the same slope.
\end{itemize}

The hierarchical (conditional) model is:

\[
\begin{aligned}
\mathbf{Y}_i | b_i &\sim N(\mathbf{X}_i \beta + 1 b_i, \mathbf{\Sigma}_i), \\
b_i &\sim N(0, d_{11}).
\end{aligned}
\]

where:

\begin{itemize}
\tightlist
\item
  \(b_i\) is the \textbf{random intercept} for subject \(i\), assumed to follow \(N(0, d_{11})\).
\item
  \(\mathbf{X}_i\) contains the \textbf{fixed effects}, which include treatment and time.
\item
  \(\mathbf{\Sigma}_i\) represents \textbf{residual variance}, typically assumed to be \(\sigma^2 \mathbf{I}\) (independent errors).
\end{itemize}

Since there are no \textbf{random slopes}, the only source of \textbf{subject-specific variability} is the intercept.

Integrating out \(b_i\) and assuming conditional independence \(\mathbf{\Sigma}_i = \sigma^2 \mathbf{I}_{n_i}\), the \textbf{marginal distribution} of \(\mathbf{Y}_i\) is:

\[
\mathbf{Y}_i \sim N(\mathbf{X}_i \beta, 11'd_{11} + \sigma^2 \mathbf{I}).
\]

The \textbf{marginal covariance matrix} is:

\[
\begin{aligned}
\text{Cov}(\mathbf{Y}_i) &= 11'd_{11} + \sigma^2 I \\
&=
\begin{bmatrix}
d_{11} + \sigma^2 & d_{11} & d_{11} & \dots & d_{11} \\
d_{11} & d_{11} + \sigma^2 & d_{11} & \dots & d_{11} \\
d_{11} & d_{11} & d_{11} + \sigma^2 & \dots & d_{11} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
d_{11} & d_{11} & d_{11} & \dots & d_{11} + \sigma^2
\end{bmatrix}.
\end{aligned}
\]

The \textbf{correlation matrix} is obtained by standardizing the covariance matrix:

\[
\text{Corr}(\mathbf{Y}_i) =
\begin{bmatrix}
1 & \rho & \rho & \dots & \rho \\
\rho & 1 & \rho & \dots & \rho \\
\rho & \rho & 1 & \dots & \rho \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\rho & \rho & \rho & \dots & 1
\end{bmatrix},
\]

where:

\[
\rho = \frac{d_{11}}{d_{11} + \sigma^2}.
\]

This \textbf{correlation structure} is known as \textbf{compound symmetry}, meaning:

\begin{itemize}
\tightlist
\item
  \textbf{Constant variance}: \(\text{Var}(Y_{ij}) = d_{11} + \sigma^2\) for all \(j\).
\item
  \textbf{Equal correlation}: \(\text{Corr}(Y_{ij}, Y_{ik}) = \rho\) for all \(j \neq k\).
\item
  \textbf{Positive correlation}: Any two observations from the same subject are equally correlated.
\end{itemize}

Interpretation of \(\rho\) (Intra-Class Correlation)

\begin{itemize}
\item
  \(\rho\) is called the intra-class correlation coefficient (ICC).
\item
  It measures the proportion of total variability that is due to between-subject variability:

  \[
  \rho = \frac{\text{Between-Subject Variance}}{\text{Total Variance}} = \frac{d_{11}}{d_{11} + \sigma^2}.
  \]
\item
  If \(\rho\) is large:

  \begin{itemize}
  \tightlist
  \item
    The inter-subject variability (\(d_{11}\)) is large relative to the intra-subject variability (\(\sigma^2\)).
  \item
    This means subjects differ substantially in their intercepts.
  \item
    Responses from the same subject are highly correlated.
  \end{itemize}
\item
  If \(\rho\) is small:

  \begin{itemize}
  \tightlist
  \item
    The residual error variance dominates, meaning individual differences in intercepts are small.
  \item
    Measurements from the same subject are weakly correlated.
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Summary of the Random-Intercepts Model

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.3482}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.6518}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Feature
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Explanation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Intercepts} & Random, subject-specific (\(b_i\)) \\
\textbf{Slopes} & Fixed within each treatment group \\
\textbf{Covariance Structure} & \textbf{Compound symmetry} (constant variance, equal correlation) \\
\textbf{Intra-Class Correlation (}\(\rho\)) & Measures \textbf{between-subject variability} relative to total variability \\
\textbf{Interpretation} & Large \(\rho\)  Strong subject-level differences,

Small \(\rho\)  Mostly residual noise \\
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{covariance-models-in-linear-mixed-models}{%
\subsection{Covariance Models in Linear Mixed Models}\label{covariance-models-in-linear-mixed-models}}

Previously, we assumed that the \textbf{within-subject errors} are \textbf{conditionally independent}, meaning:

\[
\mathbf{\Sigma}_i = \sigma^2 \mathbf{I}_{n_i}.
\]

However, real-world data often exhibit \textbf{correlated errors}, particularly in \textbf{longitudinal studies} where observations over time are influenced by underlying stochastic processes.

To model this dependence, we decompose the error term into two components:

\[
\epsilon_i = \epsilon_{(1)i} + \epsilon_{(2)i},
\]

where:

\begin{itemize}
\tightlist
\item
  \(\epsilon_{(1)i}\) (Serial Correlation Component):

  \begin{itemize}
  \tightlist
  \item
    Represents subject-specific response to time-varying stochastic processes.
  \item
    Captures dependency across observations for the same subject.
  \end{itemize}
\item
  \(\epsilon_{(2)i}\) (Measurement Error Component):

  \begin{itemize}
  \tightlist
  \item
    Represents pure measurement error, assumed independent of \(\epsilon_{(1)i}\).
  \end{itemize}
\end{itemize}

Thus, the full LMM formulation becomes:

\[
\mathbf{Y_i} = \mathbf{X_i \beta} + \mathbf{Z_i b_i} + \mathbf{\epsilon_{(1)i}} + \mathbf{\epsilon_{(2)i}}.
\]

where:

\begin{itemize}
\tightlist
\item
  Random effects: \(\mathbf{b_i} \sim N(\mathbf{0, D})\).
\item
  Measurement errors: \(\epsilon_{(2)i} \sim N(\mathbf{0, \sigma^2 I_{n_i}})\).
\item
  Serial correlation errors: \(\epsilon_{(1)i} \sim N(\mathbf{0, \tau^2 H_i})\).
\item
  Independence assumption: \(\mathbf{b}_i\) and \(\epsilon_i\) are mutually independent.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

To model the correlation structure of the \textbf{serial correlation component} \(\epsilon_{(1)i}\), we define the \(n_i \times n_i\) correlation (or covariance) matrix \(\mathbf{H}_i\).

The \((j,k)\)th element of \(\mathbf{H}_i\) is denoted as:

\[
h_{ijk} = g(t_{ij}, t_{ik}),
\]

where:

\begin{itemize}
\item
  \(h_{ijk}\) represents the covariance (or correlation) between time points \(t_{ij}\) and \(t_{ik}\).
\item
  \(g(t_{ij}, t_{ik})\) is a decreasing function that defines the correlation between time points \(t_{ij}\) and \(t_{ik}\).
\item
  Typically, we assume that this function depends only on the time difference (stationarity assumption):

  \[
  h_{ijk} = g(|t_{ij} - t_{ik}|)
  \]

  meaning that the correlation only depends on the absolute time lag.
\item
  To be a valid correlation matrix, we require:

  \[
  g(0) = 1.
  \]

  This ensures that each observation has a perfect correlation with itself.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Common Choices} for \(g(|t_{ij} - t_{ik}|)\)

Several functions can be used to define the \textbf{decay in correlation} as time differences increase.

\textbf{1. Exponential Correlation (Continuous-Time AR(1))}

\[
g(|t_{ij} - t_{ik}|) = \exp(-\phi |t_{ij} - t_{ik}|)
\]

\begin{itemize}
\tightlist
\item
  \textbf{Decay rate:} Controlled by \(\phi > 0\).
\item
  \textbf{Interpretation:}

  \begin{itemize}
  \tightlist
  \item
    Observations closer in time are more correlated.
  \item
    The correlation decreases exponentially as time separation increases.
  \end{itemize}
\item
  \textbf{Use case:} Often used in biological and economic time-series models.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{2. Gaussian Correlation (Squared Exponential Kernel)}

\[
g(|t_{ij} - t_{ik}|) = \exp(-\phi (t_{ij} - t_{ik})^2)
\]

\begin{itemize}
\tightlist
\item
  \textbf{Decay rate:} Faster than exponential.
\item
  \textbf{Interpretation:}

  \begin{itemize}
  \tightlist
  \item
    If \(\phi\) is large, correlation drops sharply as time separation increases.
  \item
    Produces smooth correlation structures.
  \end{itemize}
\item
  \textbf{Use case:} Used in spatial statistics and machine learning (Gaussian processes).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{3. Autoregressive (AR(1)) Correlation}

A \textbf{First-Order Autoregressive Model (AR(1))} assumes that the value at time \(t\) depends on its previous value:

\[
\alpha_t = \phi \alpha_{t-1} + \eta_t,
\]

where

\begin{itemize}
\item
  \(\eta_t \sim \text{i.i.d. } N(0, \sigma^2_\eta)\) (white noise process).
\item
  \(\phi\) is the \textbf{autocorrelation coefficient}.
\end{itemize}

Then, the covariance between two observations at different times is:

\[ \text{Cov}(\alpha_t, \alpha_{t+h}) = \frac{\sigma^2_\eta \phi^{|h|}}{1 - \phi^2}. \]

Thus, the \textbf{correlation} between time points \(t\) and \(t+h\) is:

\[ \text{Corr}(\alpha_t, \alpha_{t+h}) = \phi^{|h|}. \]

For a sequence of \(T\) time points, the resulting \textbf{Toeplitz correlation matrix} is:

\[ \text{Corr}(\alpha_T) = \begin{bmatrix} 1 & \phi^1 & \phi^2 & \dots & \phi^{T-1} \\ \phi^1 & 1 & \phi^1 & \dots & \phi^{T-2} \\ \phi^2 & \phi^1 & 1 & \dots & \phi^{T-3} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ \phi^{T-1} & \phi^{T-2} & \phi^{T-3} & \dots & 1 \end{bmatrix}. \]

\begin{itemize}
\tightlist
\item
  \textbf{Decay rate:} \(\phi\) controls how fast correlation decreases.
\item
  \textbf{Use case:}

  \begin{itemize}
  \tightlist
  \item
    Common in \textbf{time series models}.
  \item
    Appropriate when \textbf{observations are equally spaced in time}.
  \end{itemize}
\end{itemize}

Properties of the AR(1) Structure:

\begin{itemize}
\tightlist
\item
  Correlation decreases with time lag:

  \begin{itemize}
  \tightlist
  \item
    Observations closer in time are more correlated.
  \item
    Decay rate is controlled by \(\phi\).
  \end{itemize}
\item
  Toeplitz structure:

  \begin{itemize}
  \tightlist
  \item
    The covariance matrix exhibits a banded diagonal pattern.
  \item
    Useful in longitudinal and time-series data.
  \end{itemize}
\item
  Applicability:

  \begin{itemize}
  \tightlist
  \item
    Small \(\phi\) (\(\approx 0\))  Weak autocorrelation (errors are mostly independent).
  \item
    Large \(\phi\) (\(\approx 1\))  Strong autocorrelation (highly dependent observations).
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{covariance-structures-in-mixed-models}{%
\subsection{Covariance Structures in Mixed Models}\label{covariance-structures-in-mixed-models}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1481}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1852}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.0782}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1152}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.2757}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1975}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Structure
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Covariance Function \(g(|t_{ij} - t_{ik}|)\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Decay Behavior
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Correlation Matrix Pattern
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Key Properties
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Common Use Cases
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Compound Symmetry} & Constant: \(g(|t_{ij} - t_{ik}|) = \rho\) & No decay & Constant off-diagonal & Equal correlation across all time points (homogeneous) & Random-intercepts model, repeated measures \\
\textbf{AR(1) (Autoregressive)} & \(\phi^{|t_{ij} - t_{ik}|}\) & Exponential decay & Toeplitz (banded diagonal) & Strong correlation for nearby observations, weak for distant ones & Time series, longitudinal data \\
\textbf{Exponential} & \(\exp(-\phi |t_{ij} - t_{ik}|)\) & Smooth decay & Spatially continuous & Models continuous correlation decline over time/space & Growth curves, ecological models \\
\textbf{Gaussian (Squared Exponential)} & \(\exp(-\phi (t_{ij} - t_{ik})^2)\) & Rapid decay & Spatially continuous & Smooth decay but stronger locality effect & Spatial processes, geostatistics \\
\textbf{Toeplitz} & Varies by lag, \(g(|t_{ij} - t_{ik}|) = c_h\) & General pattern & General symmetric matrix & Arbitrary structure, allows irregular spacing & Irregular time points, flexible spatial models \\
\textbf{Unstructured} & Fully parameterized, no constraints & No fixed pattern & General symmetric matrix & Allows any correlation pattern, but many parameters & Small datasets with unknown correlation \\
\textbf{Banded} & Zero correlation beyond a certain lag & Abrupt cutoff & Banded diagonal & Assumes only close observations are correlated & Large datasets, high-dimensional time series \\
\textbf{Spatial Power} & \(\phi^{|s_{ij} - s_{ik}|}\) & Exponential decay & Distance-based structure & Used when correlation depends on spatial distance & Spatial statistics, environmental data \\
\textbf{Matrn Covariance} & Function of \(|t_{ij} - t_{ik}|^\nu\) & Flexible decay & Spatially continuous & Generalization of Gaussian and exponential structures & Geostatistics, spatiotemporal models \\
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Choosing the Right Covariance Structure}

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Scenario & Recommended Structure \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Repeated measures with equal correlation & Compound Symmetry \\
Longitudinal data with time dependence & AR(1), Toeplitz \\
Continuous-time process & Exponential, Gaussian \\
Spatial correlation & Spatial Power, Matrn \\
Irregularly spaced time points & Toeplitz, Unstructured \\
High-dimensional data & Banded, AR(1) \\
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{estimation-in-linear-mixed-models}{%
\section{Estimation in Linear Mixed Models}\label{estimation-in-linear-mixed-models}}

The general \protect\hyperlink{sec-linear-mixed-models}{Linear Mixed Model} is:

\[ \mathbf{Y}_i = \mathbf{X}_i \beta + \mathbf{Z}_i \mathbf{b}_i + \epsilon_i, \]

where:

\begin{itemize}
\tightlist
\item
  \(\beta\): Fixed effects (parameters shared across subjects).
\item
  \(\mathbf{b}_i\): Random effects (subject-specific deviations).
\item
  \(\mathbf{X}_i\): Design matrix for fixed effects.
\item
  \(\mathbf{Z}_i\): Design matrix for random effects.
\item
  \(\mathbf{D}\): Covariance matrix of the random effects.
\item
  \(\mathbf{\Sigma}_i\): Covariance matrix of the residual errors.
\end{itemize}

Since \(\beta\), \(\mathbf{b}_i\), \(\mathbf{D}\), and \(\mathbf{\Sigma}_i\) are \textbf{unknown}, they must be \textbf{estimated} from data.

\begin{itemize}
\tightlist
\item
  \(\beta, \mathbf{D}, \mathbf{\Sigma}_i\) are \textbf{fixed parameters}  must be \textbf{estimated}.
\item
  \(\mathbf{b}_i\) is a \textbf{random variable}  must be \textbf{predicted} (not estimated). In other words, random thing/variable can't be estimated.
\end{itemize}

Thus, we define:

\begin{itemize}
\tightlist
\item
  \textbf{Estimator of} \(\beta\): \(\hat{\beta}\) (fixed effect estimation).
\item
  \textbf{Predictor of} \(\mathbf{b}_i\): \(\hat{\mathbf{b}}_i\) (random effect prediction).
\end{itemize}

Then:

\begin{itemize}
\item
  The \textbf{population-level estimate} of \(\mathbf{Y}_i\) is:

  \[
  \hat{\mathbf{Y}}_i = \mathbf{X}_i \hat{\beta}.
  \]
\item
  The \textbf{subject-specific prediction} is:

  \[
  \hat{\mathbf{Y}}_i = \mathbf{X}_i \hat{\beta} + \mathbf{Z}_i \hat{\mathbf{b}}_i.
  \]
\end{itemize}

For all \(N\) subjects, we stack the equations into the Mixed Model Equations \citep{henderson1975best}:

\[
\mathbf{Y} = \mathbf{X} \beta + \mathbf{Z} \mathbf{b} + \epsilon.
\]

and

\[
\mathbf{Y} \sim N(\mathbf{X \beta, ZBZ' + \Sigma})
\]

where:

\[
\mathbf{Y} =
\begin{bmatrix}
\mathbf{y}_1 \\
\vdots \\
\mathbf{y}_N
\end{bmatrix},
\quad
\mathbf{X} =
\begin{bmatrix}
\mathbf{X}_1 \\
\vdots \\
\mathbf{X}_N
\end{bmatrix},
\quad
\mathbf{b} =
\begin{bmatrix}
\mathbf{b}_1 \\
\vdots \\
\mathbf{b}_N
\end{bmatrix},
\quad\mathbf{\epsilon} =    \begin{bmatrix}\mathbf{\epsilon}_1 \\\vdots \\\mathbf{\epsilon}_N\end{bmatrix}.
\]

The covariance structure is:

\[
\text{Cov}(\mathbf{b}) = \mathbf{B}, \quad \text{Cov}(\epsilon) = \mathbf{\Sigma}, \quad \text{Cov}(\mathbf{b}, \epsilon) = 0.
\]

Expanding \(\mathbf{Z}\) and \(\mathbf{B}\) as block diagonal matrices:

\[
\mathbf{Z} =
\begin{bmatrix}
\mathbf{Z}_1 & 0 & \dots & 0 \\
0 & \mathbf{Z}_2 & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & \mathbf{Z}_N
\end{bmatrix},
\quad
\mathbf{B} =
\begin{bmatrix}
\mathbf{D} & 0 & \dots & 0 \\
0 & \mathbf{D} & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & \mathbf{D}
\end{bmatrix}.
\]

The best linear unbiased estimator (BLUE) for \(\beta\) and the best linear unbiased predictor (BLUP) for \(\mathbf{b}\) are obtained by solving \citep{henderson1975best}:

\[
\left[
\begin{array}{c}
\hat{\beta} \\
\hat{\mathbf{b}}
\end{array}
\right]
=
\left[
\begin{array}{cc}
\mathbf{X' \Sigma^{-1} X} & \mathbf{X' \Sigma^{-1} Z} \\
\mathbf{Z' \Sigma^{-1} X} & \mathbf{Z' \Sigma^{-1} Z + B^{-1}}
\end{array}
\right]^{-1}
\left[
\begin{array}{c}
\mathbf{X' \Sigma^{-1} Y} \\
\mathbf{Z' \Sigma^{-1} Y}
\end{array}
\right].
\]

where:

\begin{itemize}
\tightlist
\item
  \(\hat{\beta}\) is the \protect\hyperlink{generalized-least-squares}{Generalized Least Squares} estimator for \(\beta\).
\item
  \(\hat{\mathbf{b}}\) is the BLUP for \(\mathbf{b}\).
\end{itemize}

If we define:

\[
\mathbf{V} = \mathbf{Z B Z'} + \mathbf{\Sigma}.
\]

Then, the solutions to the \textbf{Mixed Model Equations} are:

\[
\begin{aligned}
\hat{\beta} &= (\mathbf{X'V^{-1}X})^{-1} \mathbf{X'V^{-1}Y}, \\
\hat{\mathbf{b}} &= \mathbf{BZ'V^{-1}(Y - X\hat{\beta})}.
\end{aligned}
\]

where:

\begin{itemize}
\tightlist
\item
  \(\hat{\beta}\) is obtained using \protect\hyperlink{generalized-least-squares}{Generalized Least Squares}.
\item
  \(\hat{\mathbf{b}}\) is a \protect\hyperlink{weighted-least-squares}{Weighted Least Squares} predictor, where weights come from \(\mathbf{B}\) and \(\mathbf{V}\).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Properties of the Estimators}

For \(\hat{\beta}\):

\[
E(\hat{\beta}) = \beta, \quad \text{Var}(\hat{\beta}) = (\mathbf{X'V^{-1}X})^{-1}.
\]

For \(\hat{\mathbf{b}}\):

\[
E(\hat{\mathbf{b}}) = 0.
\]

The \textbf{variance of the prediction error} (Mean Squared Prediction Error, MSPE) is:

\[
\text{Var}(\hat{\mathbf{b}} - \mathbf{b}) =
\mathbf{B - BZ'V^{-1}ZB + BZ'V^{-1}X (X'V^{-1}X)^{-1} X'V^{-1}B}.
\]

\begin{quote}
 \textbf{Key Insight:}\\
Mean Squared Prediction Error is more meaningful than \(\text{Var}(\hat{\mathbf{b}})\) alone, since it accounts for both variance and bias in prediction.
\end{quote}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{interpretation-of-the-mixed-model-equations}{%
\subsection{Interpretation of the Mixed Model Equations}\label{interpretation-of-the-mixed-model-equations}}

The system:

\[
\left[
\begin{array}{cc}
\mathbf{X'\Sigma^{-1}X} & \mathbf{X'\Sigma^{-1}Z} \\
\mathbf{Z'\Sigma^{-1}X} & \mathbf{Z'\Sigma^{-1}Z + B^{-1}}
\end{array}
\right]
\left[
\begin{array}{c}
\beta \\
\mathbf{b}
\end{array}
\right]
= 
\left[
\begin{array}{c}
\mathbf{X'\Sigma^{-1}Y} \\
\mathbf{Z'\Sigma^{-1}Y}
\end{array}
\right].
\]

can be understood as:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Fixed Effects Estimation (\(\hat{\beta}\))

  \begin{itemize}
  \tightlist
  \item
    Uses \protect\hyperlink{generalized-least-squares}{Generalized Least Squares}.
  \item
    Adjusted for both random effects and correlated errors.
  \end{itemize}
\item
  Random Effects Prediction (\(\hat{\mathbf{b}}\))

  \begin{itemize}
  \tightlist
  \item
    Computed using the BLUP formula.
  \item
    Shrinks subject-specific estimates toward the population mean.
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2547}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4224}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3230}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Component}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Equation}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Interpretation}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Fixed effects (}\(\hat{\beta}\)) & \((\mathbf{X'V^{-1}X})^{-1} \mathbf{X'V^{-1}Y}\) & \protect\hyperlink{generalized-least-squares}{Generalized Least Squares} estimator \\
\textbf{Random effects (}\(\hat{\mathbf{b}}\)) & \(\mathbf{BZ'V^{-1}(Y - X\hat{\beta})}\) & Best Linear Unbiased Predictor (BLUP) \\
\textbf{Variance of} \(\hat{\beta}\) & \((\mathbf{X'V^{-1}X})^{-1}\) & Accounts for uncertainty in fixed effect estimates \\
\textbf{Variance of prediction error} & \(\mathbf{B - BZ'V^{-1}ZB + BZ'V^{-1}X (X'V^{-1}X)^{-1} X'V^{-1}B}\) & Includes both variance and bias in prediction \\
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{derivation-of-the-mixed-model-equations}{%
\subsection{Derivation of the Mixed Model Equations}\label{derivation-of-the-mixed-model-equations}}

To derive the \textbf{Mixed Model Equations}, consider:

\[
\mathbf{\epsilon} = \mathbf{Y} - \mathbf{X \beta} - \mathbf{Z b}.
\]

Define:

\begin{itemize}
\tightlist
\item
  \(T = \sum_{i=1}^N n_i\)  \textbf{Total number of observations}.
\item
  \(Nq\)  \textbf{Total number of random effects}.
\end{itemize}

The \textbf{joint distribution} of \((\mathbf{b}, \mathbf{\epsilon})\) is:

\[
\begin{aligned}
f(\mathbf{b}, \epsilon) &= \frac{1}{(2\pi)^{(T+Nq)/2}}
\left|
\begin{array}{cc}
\mathbf{B} & 0 \\
0 & \mathbf{\Sigma}
\end{array} 
\right|^{-1/2} \\
&
\exp
\left(
-\frac{1}{2}
\begin{bmatrix}
\mathbf{b} \\
\mathbf{Y - X \beta - Zb}
\end{bmatrix}'
\begin{bmatrix}
\mathbf{B} & 0 \\
0 & \mathbf{\Sigma}
\end{bmatrix}^{-1}
\begin{bmatrix}
\mathbf{b} \\
\mathbf{Y - X \beta - Zb}
\end{bmatrix}
\right).
\end{aligned}
\]

Maximizing \(f(\mathbf{b},\epsilon)\) with respect to \(\mathbf{b}\) and \(\beta\) requires \textbf{minimization} of:

\[
\begin{aligned}
Q &= \left[
\begin{array}
{c}
\mathbf{b} \\
\mathbf{Y - X \beta - Zb}
\end{array}
\right]'
\left[
\begin{array}
{cc}
\mathbf{B} & 0 \\
0 & \mathbf{\Sigma}
\end{array}
\right]^{-1}
\left[
\begin{array}
{c}
\mathbf{b} \\
\mathbf{Y - X \beta - Zb}
\end{array}
\right] \\
&=\mathbf{b' B^{-1} b} + (\mathbf{Y - X \beta - Z b})' \mathbf{\Sigma^{-1}} (\mathbf{Y - X \beta - Z b}).
\end{aligned}
\]

Setting the derivatives of \(Q\) with respect to \(\mathbf{b}\) and \(\mathbf{\beta}\) to zero leads to the system of equations:

\[
\begin{aligned}
\mathbf{X'\Sigma^{-1}X\beta + X'\Sigma^{-1}Zb} &= \mathbf{X'\Sigma^{-1}Y}\\
\mathbf{(Z'\Sigma^{-1}Z + B^{-1})b + Z'\Sigma^{-1}X\beta} &= \mathbf{Z'\Sigma^{-1}Y}
\end{aligned}
\]

Rearranging

\[
\left[
\begin{array}
{cc}
\mathbf{X'\Sigma^{-1}X} & \mathbf{X'\Sigma^{-1}Z} \\
\mathbf{Z'\Sigma^{-1}X} & \mathbf{Z'\Sigma^{-1}Z + B^{-1}}
\end{array}
\right]
\left[
\begin{array}
{c}
\beta \\
\mathbf{b}
\end{array}
\right]
= 
\left[
\begin{array}
{c}
\mathbf{X'\Sigma^{-1}Y} \\
\mathbf{Z'\Sigma^{-1}Y}
\end{array}
\right]
\]

Thus, the solution to the mixed model equations give:

\[
\left[
\begin{array}
{c}
\hat{\beta} \\
\hat{\mathbf{b}}
\end{array}
\right]
= 
\left[
\begin{array}
{cc}
\mathbf{X'\Sigma^{-1}X} & \mathbf{X'\Sigma^{-1}Z} \\
\mathbf{Z'\Sigma^{-1}X} & \mathbf{Z'\Sigma^{-1}Z + B^{-1}}
\end{array}
\right] ^{-1}
\left[
\begin{array}
{c}
\mathbf{X'\Sigma^{-1}Y} \\
\mathbf{Z'\Sigma^{-1}Y}
\end{array}
\right]
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{bayesian-interpretation-of-linear-mixed-models}{%
\subsection{Bayesian Interpretation of Linear Mixed Models}\label{bayesian-interpretation-of-linear-mixed-models}}

In a \textbf{Bayesian framework}, the posterior distribution of the \textbf{random effects} \(\mathbf{b}\) given the observed data \(\mathbf{Y}\) is derived using \textbf{Bayes' theorem}:

\[
f(\mathbf{b}| \mathbf{Y}) = \frac{f(\mathbf{Y}|\mathbf{b})f(\mathbf{b})}{\int f(\mathbf{Y}|\mathbf{b})f(\mathbf{b}) d\mathbf{b}}.
\]

where:

\begin{itemize}
\tightlist
\item
  \(f(\mathbf{Y}|\mathbf{b})\) is the likelihood function, describing how the data are generated given the random effects.
\item
  \(f(\mathbf{b})\) is the prior distribution of the random effects.
\item
  The denominator \(\int f(\mathbf{Y}|\mathbf{b}) f(\mathbf{b}) d\mathbf{b}\) is the normalizing constant that ensures the posterior integrates to 1.
\item
  \(f(\mathbf{b}|\mathbf{Y})\) is the posterior distribution, which updates our belief about \(\mathbf{b}\) given the observed data \(\mathbf{Y}\).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

In the \protect\hyperlink{sec-linear-mixed-models}{Linear Mixed Model}, we assume:

\[
\begin{aligned}
\mathbf{Y} | \mathbf{b} &\sim N(\mathbf{X\beta+Zb, \Sigma}), \\
\mathbf{b} &\sim N(\mathbf{0, B}).
\end{aligned}
\]

This means:

\begin{itemize}
\tightlist
\item
  Likelihood: Given \(\mathbf{b}\), the data \(\mathbf{Y}\) follows a multivariate normal distribution with mean \(\mathbf{X\beta+Zb}\) and covariance \(\mathbf{\Sigma}\).
\item
  Prior for \(\mathbf{b}\): The random effects are assumed to follow a multivariate normal distribution with mean 0 and covariance \(\mathbf{B}\).
\end{itemize}

By applying \textbf{Bayes' theorem}, the posterior distribution of \(\mathbf{b}\) given \(\mathbf{Y}\) is:

\[
\mathbf{b} | \mathbf{Y} \sim N(\mathbf{BZ'V^{-1}(Y - X\beta)}, (\mathbf{Z'\Sigma^{-1}Z} + \mathbf{B^{-1}})^{-1}).
\]

where:

\begin{itemize}
\tightlist
\item
  Mean: \(\mathbf{BZ'V^{-1}(Y - X\beta)}\)

  \begin{itemize}
  \tightlist
  \item
    This is the BLUP.
  \item
    It represents the expected value of \(\mathbf{b}\) given \(\mathbf{Y}\) under squared-error loss.
  \end{itemize}
\item
  Covariance: \((\mathbf{Z'\Sigma^{-1}Z} + \mathbf{B^{-1}})^{-1}\)

  \begin{itemize}
  \tightlist
  \item
    This posterior variance accounts for both prior uncertainty (\(\mathbf{B}\)) and data uncertainty (\(\mathbf{\Sigma}\)).
  \end{itemize}
\end{itemize}

Thus, the Bayesian posterior mean of \(\mathbf{b}\) coincides with the BLUP predictor:

\[
E(\mathbf{b}|\mathbf{Y}) = \mathbf{BZ'V^{-1}(Y-X\beta)}.
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Interpretation of the Posterior Distribution}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Posterior Mean as a Shrinkage Estimator (BLUP)

  \begin{itemize}
  \tightlist
  \item
    The expectation \(E(\mathbf{b}|\mathbf{Y})\) shrinks individual estimates toward the population mean.
  \item
    Subjects with less data or more variability will have estimates closer to zero.
  \item
    This is similar to \protect\hyperlink{ridge-regression}{Ridge Regression} in penalized estimation.
  \end{itemize}
\item
  Posterior Variance Quantifies Uncertainty

  \begin{itemize}
  \tightlist
  \item
    The matrix \((\mathbf{Z'\Sigma^{-1}Z} + \mathbf{B^{-1}})^{-1}\) captures the remaining uncertainty in \(\mathbf{b}\) after seeing \(\mathbf{Y}\).
  \item
    If \(\mathbf{Z'\Sigma^{-1}Z}\) is large, the data provide strong information about \(\mathbf{b}\), reducing posterior variance.
  \item
    If \(\mathbf{B^{-1}}\) dominates, prior information heavily influences estimates.
  \end{itemize}
\item
  Connection to Bayesian Inference

  \begin{itemize}
  \tightlist
  \item
    The random effects \(\mathbf{b}\) follow a Gaussian posterior due to conjugacy.
  \item
    This is analogous to Bayesian hierarchical models, where random effects are latent variables estimated from data.
  \end{itemize}
\end{enumerate}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1534}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.6364}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2102}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Step}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Equation}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Interpretation}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Likelihood} & \(\mathbf{Y} | \mathbf{b} \sim N(\mathbf{X\beta+Zb, \Sigma})\) & Data given random effects \\
\textbf{Prior} & \(\mathbf{b} \sim N(\mathbf{0, B})\) & Random effects distribution \\
\textbf{Posterior} & \(\mathbf{b}|\mathbf{Y} \sim N(\mathbf{BZ'V^{-1}(Y-X\beta)}, (\mathbf{Z'\Sigma^{-1}Z} + \mathbf{B^{-1}})^{-1})\) & Updated belief about \(\mathbf{b}\) \\
\textbf{Posterior Mean (BLUP)} & \(E(\mathbf{b}|\mathbf{Y}) = \mathbf{BZ'V^{-1}(Y-X\beta)}\) & Best predictor (squared error loss) \\
\textbf{Posterior Variance} & \((\mathbf{Z'\Sigma^{-1}Z} + \mathbf{B^{-1}})^{-1}\) & Uncertainty in predictions \\
\end{longtable}

\hypertarget{estimating-the-variance-covariance-matrix}{%
\subsection{Estimating the Variance-Covariance Matrix}\label{estimating-the-variance-covariance-matrix}}

If we have an estimate \(\tilde{\mathbf{V}}\) for \(\mathbf{V}\), we can estimate the fixed and random effects as:

\[
\begin{aligned}
\hat{\beta} &= \mathbf{(X'\tilde{V}^{-1}X)^{-1}X'\tilde{V}^{-1}Y}, \\
\hat{\mathbf{b}} &= \mathbf{B Z' \tilde{V}^{-1} (Y - X \hat{\beta})}.
\end{aligned}
\]

where:

\begin{itemize}
\tightlist
\item
  \(\hat{\beta}\) is the estimated fixed effects.
\item
  \(\hat{\mathbf{b}}\) is the Empirical Best Linear Unbiased Predictor (EBLUP), also called the Empirical Bayes estimate of \(\mathbf{b}\).
\end{itemize}

Properties of \(\hat{\beta}\) and Variance Estimation

\begin{itemize}
\tightlist
\item
  \textbf{Consistency}: \(\hat{\text{Var}}(\hat{\beta})\) is a consistent estimator of \(\text{Var}(\hat{\beta})\) if \(\tilde{\mathbf{V}}\) is a consistent estimator of \(\mathbf{V}\).
\item
  \textbf{Bias} \textbf{Issue}: \(\hat{\text{Var}}(\hat{\beta})\) is biased because it does not account for the uncertainty in estimating \(\mathbf{V}\).
\item
  \textbf{Implication}: This means that \(\hat{\text{Var}}(\hat{\beta})\) underestimates the true variability.
\end{itemize}

To estimate \(\mathbf{V}\), several approaches can be used:

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{sec-maximum-likelihood-estimation-lmm}{Maximum Likelihood Estimation} (MLE)
\item
  \protect\hyperlink{restricted-maximum-likelihood-lmm}{Restricted Maximum Likelihood} (REML)
\item
  \protect\hyperlink{sec-estimated-generalized-least-squares-lmm}{Estimated Generalized Least Squares} (EGLS)
\item
  \protect\hyperlink{sec-bayesian-hierarchical-models-lmm}{Bayesian Hierarchical Models} (BHM)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{sec-maximum-likelihood-estimation-lmm}{%
\subsubsection{Maximum Likelihood Estimation}\label{sec-maximum-likelihood-estimation-lmm}}

MLE finds parameter estimates by maximizing the likelihood function.

Define a parameter vector \(\theta\) that includes all unknown variance components in \(\mathbf{\Sigma}\) and \(\mathbf{B}\). Then, we assume:

\[
\mathbf{Y} \sim N(\mathbf{X\beta}, \mathbf{V}(\theta)).
\]

The log-likelihood function (ignoring constant terms) is:

\[
-2\log L(\mathbf{y}; \theta, \beta) =
\log |\mathbf{V}(\theta)| + (\mathbf{Y - X\beta})' \mathbf{V}(\theta)^{-1} (\mathbf{Y - X\beta}).
\]

\textbf{Steps for MLE Estimation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Estimate \(\hat{\beta}\), assuming \(\theta\) is known:

  \[
  \hat{\beta}_{MLE} = (\mathbf{X'V(\theta)^{-1}X})^{-1} \mathbf{X'V(\theta)^{-1}Y}.
  \]
\item
  Obtain \(\hat{\theta}_{MLE}\) by maximizing the log-likelihood:

  \[
  \hat{\theta}_{MLE} = \arg\max_{\theta} -2\log L(\mathbf{y}; \theta, \beta).
  \]
\item
  Substitute \(\hat{\theta}_{MLE}\) to get updated estimates:

  \[
  \hat{\beta}_{MLE} = (\mathbf{X'V(\hat{\theta}_{MLE})^{-1}X})^{-1} \mathbf{X'V(\hat{\theta}_{MLE})^{-1}Y}.
  \]
\item
  Predict random effects:

  \[
  \hat{\mathbf{b}}_{MLE} = \mathbf{B}(\hat{\theta}_{MLE}) \mathbf{Z'V}(\hat{\theta}_{MLE})^{-1} (\mathbf{Y - X \hat{\beta}_{MLE}}).
  \]
\end{enumerate}

Key Observations about MLE

\begin{itemize}
\tightlist
\item
  MLE tends to underestimate \(\theta\) because it does not account for the estimation of fixed effects.
\item
  Bias in variance estimates can be corrected using REML.
\end{itemize}

\hypertarget{restricted-maximum-likelihood-lmm}{%
\subsubsection{Restricted Maximum Likelihood}\label{restricted-maximum-likelihood-lmm}}

Restricted Maximum Likelihood (REML) is an estimation method that improves upon \protect\hyperlink{sec-maximum-likelihood-estimation-lmm}{Maximum Likelihood Estimation} by accounting for the loss of degrees of freedom due to the estimation of fixed effects.

Unlike MLE, which estimates both fixed effects (\(\beta\)) and variance components (\(\theta\)) simultaneously, REML focuses on estimating variance components by considering linear combinations of the data that are independent of the fixed effects.

Consider the \protect\hyperlink{sec-linear-mixed-models}{Linear Mixed Model}:

\[
\mathbf{y} = \mathbf{X} \beta + \mathbf{Z} \mathbf{b} + \epsilon,
\]

where:

\begin{itemize}
\tightlist
\item
  \(\mathbf{y}\): Response vector of length \(N\)
\item
  \(\mathbf{X}\): Design matrix for fixed effects (\(N \times p\))
\item
  \(\beta\): Fixed effects parameter vector (\(p \times 1\))
\item
  \(\mathbf{Z}\): Design matrix for random effects
\item
  \(\mathbf{b} \sim N(\mathbf{0, D})\): Random effects
\item
  \(\epsilon \sim N(\mathbf{0, \Sigma})\): Residual errors
\end{itemize}

The marginal distribution of \(\mathbf{y}\) is:

\[
\mathbf{y} \sim N(\mathbf{X} \beta, \mathbf{V}(\theta)),
\]

where:

\[
\mathbf{V}(\theta) = \mathbf{Z D Z'} + \mathbf{\Sigma}.
\]

To eliminate dependence on \(\beta\), consider linear transformations of \(\mathbf{y}\) that are orthogonal to the fixed effects.

Let \(\mathbf{K}\) be a full-rank contrast matrix of size \(N \times (N - p)\) such that:

\[
\mathbf{K}' \mathbf{X} = 0.
\]

Then, we consider the transformed data:

\[
\mathbf{K}' \mathbf{y} \sim N(\mathbf{0}, \mathbf{K}' \mathbf{V}(\theta) \mathbf{K}).
\]

\begin{itemize}
\tightlist
\item
  This transformation removes \(\beta\) from the likelihood, focusing solely on the variance components \(\theta\).
\item
  Importantly, the choice of \(\mathbf{K}\) does not affect the final REML estimates.
\end{itemize}

The REML log-likelihood is:

\[
-2 \log L_{REML}(\theta) = \log |\mathbf{K}' \mathbf{V}(\theta) \mathbf{K}| + \mathbf{y}' \mathbf{K} (\mathbf{K}' \mathbf{V}(\theta) \mathbf{K})^{-1} \mathbf{K}' \mathbf{y}.
\]

An equivalent form of the REML log-likelihood, avoiding explicit use of \(\mathbf{K}\), is:

\[
-2 \log L_{REML}(\theta) = \log |\mathbf{V}(\theta)| + \log |\mathbf{X}' \mathbf{V}(\theta)^{-1} \mathbf{X}| + (\mathbf{y} - \mathbf{X} \hat{\beta})' \mathbf{V}(\theta)^{-1} (\mathbf{y} - \mathbf{X} \hat{\beta}),
\]

where:

\[
\hat{\beta} = (\mathbf{X}' \mathbf{V}(\theta)^{-1} \mathbf{X})^{-1} \mathbf{X}' \mathbf{V}(\theta)^{-1} \mathbf{y}.
\]

This form highlights how REML adjusts for the estimation of fixed effects via the second term \(\log |\mathbf{X}' \mathbf{V}^{-1} \mathbf{X}|\).

\textbf{Steps for REML Estimation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Transform the data using \(\mathbf{K}' \mathbf{y}\) to remove \(\beta\) from the likelihood.
\item
  Maximize the restricted likelihood to estimate \(\hat{\theta}_{REML}\).
\item
  Estimate fixed effects using:

  \[
  \hat{\beta}_{REML} = (\mathbf{X}' \mathbf{V}(\hat{\theta}_{REML})^{-1} \mathbf{X})^{-1} \mathbf{X}' \mathbf{V}(\hat{\theta}_{REML})^{-1} \mathbf{y}.
  \]
\end{enumerate}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2357}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3857}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3786}}@{}}
\caption{Comparison of REML and MLE}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Criterion
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MLE
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
REML
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Criterion
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MLE
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
REML
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Approach & Maximizes full likelihood & Maximizes likelihood of contrasts (removes \(\beta\)) \\
Estimates Fixed Effects? & Yes & No (focuses on variance components) \\
Bias in Variance Estimates & Biased (underestimates variance components) & Unbiased (corrects for loss of degrees of freedom) \\
Effect of Changing \(\mathbf{X}\) & Affects variance estimates & No effect on variance estimates \\
Consistency & Yes & Yes \\
Asymptotic Normality & Yes & Yes \\
Efficiency & Efficient under normality & More efficient for variance components \\
Model Comparison (AIC/BIC) & Suitable for comparing different fixed-effect models & Not suitable (penalizes fixed effects differently) \\
Performance in Small Samples & Sensitive to small sample bias & More robust to small sample bias \\
Handling Outliers & More sensitive & Less sensitive \\
Equivalent to ANOVA? & No & Yes, in balanced designs \\
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{sec-estimated-generalized-least-squares-lmm}{%
\subsubsection{Estimated Generalized Least Squares}\label{sec-estimated-generalized-least-squares-lmm}}

MLE and REML rely on the Gaussian assumption, which may not always hold.\\
EGLS provides an alternative by relying only on the first two moments (mean and variance).

The LMM framework is:

\[
\mathbf{Y}_i = \mathbf{X}_i \beta + \mathbf{Z}_i \mathbf{b}_i + \epsilon_i.
\]

where:

\begin{itemize}
\tightlist
\item
  Random effects: \(\mathbf{b}_i \sim N(\mathbf{0, D})\).
\item
  Residual errors: \(\epsilon_i \sim N(\mathbf{0, \Sigma_i})\).
\item
  Independence assumption: \(\text{Cov}(\epsilon_i, \mathbf{b}_i) = 0\).
\end{itemize}

Thus, the first two moments are:

\[
E(\mathbf{Y}_i) = \mathbf{X}_i \beta, \quad \text{Var}(\mathbf{Y}_i) = \mathbf{V}_i.
\]

The EGLS estimator is:

\[
\hat{\beta}_{GLS} = \left\{ \sum_{i=1}^n \mathbf{X'_iV_i(\theta)^{-1}X_i}  \right\}^{-1} 
\sum_{i=1}^n \mathbf{X'_iV_i(\theta)^{-1}Y_i}.
\]

Writing in matrix form:

\[
\hat{\beta}_{GLS} = \left\{ \mathbf{X'V(\theta)^{-1}X} \right\}^{-1} \mathbf{X'V(\theta)^{-1}Y}.
\]

Since \(\mathbf{V}(\theta)\) is unknown, we estimate it as \(\hat{\mathbf{V}}\), leading to the EGLS estimator:

\[
\hat{\beta}_{EGLS} = \left\{ \mathbf{X'\hat{V}^{-1}X} \right\}^{-1} \mathbf{X'\hat{V}^{-1}Y}.
\]

Key Insights about EGLS

\begin{itemize}
\tightlist
\item
  Computational Simplicity:

  \begin{itemize}
  \tightlist
  \item
    EGLS does not require iterative maximization of a likelihood function, making it computationally attractive.
  \end{itemize}
\item
  Same Form as MLE/REML:

  \begin{itemize}
  \tightlist
  \item
    The fixed effects estimators for MLE, REML, and EGLS have the same form, differing only in how \(\mathbf{V}\) is estimated.
  \end{itemize}
\item
  Robust to Non-Gaussian Data:

  \begin{itemize}
  \tightlist
  \item
    Since it only depends on first and second moments, it can handle cases where MLE and REML struggle with non-normality.
  \end{itemize}
\end{itemize}

When to Use EGLS?

\begin{itemize}
\tightlist
\item
  When the normality assumption for MLE/REML is questionable.
\item
  When \(\mathbf{V}\) can be estimated efficiently without requiring complex optimization.
\item
  In non-iterative approaches, where computational simplicity is a priority.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{sec-bayesian-hierarchical-models-lmm}{%
\subsubsection{Bayesian Hierarchical Models}\label{sec-bayesian-hierarchical-models-lmm}}

Bayesian methods offer a fully probabilistic framework to estimate \(\mathbf{V}\) by incorporating prior distributions.

The joint distribution can be decomposed hierarchically:

\[
f(A, B, C) = f(A | B, C) f(B | C) f(C).
\]

Applying this to LMMs:

\[
\begin{aligned}
f(\mathbf{Y, \beta, b, \theta}) &= f(\mathbf{Y | \beta, b, \theta}) f(\mathbf{b | \theta, \beta}) f(\mathbf{\beta | \theta}) f(\mathbf{\theta}) \\
&= f(\mathbf{Y | \beta, b, \theta}) f(\mathbf{b | \theta}) f(\mathbf{\beta}) f(\mathbf{\theta}).
\end{aligned}
\]

where:

\begin{itemize}
\tightlist
\item
  The first equality follows from probability decomposition.
\item
  The second equality assumes conditional independence, meaning:

  \begin{itemize}
  \tightlist
  \item
    Given \(\theta\), no additional information about \(\mathbf{b}\) is obtained from knowing \(\beta\).
  \end{itemize}
\end{itemize}

Using Bayes' theorem, the posterior distribution is:

\[
f(\mathbf{\beta, b, \theta | Y}) \propto f(\mathbf{Y | \beta, b, \theta}) f(\mathbf{b | \theta}) f(\mathbf{\beta}) f(\mathbf{\theta}).
\]

where:

\[
\begin{aligned}
\mathbf{Y | \beta, b, \theta} &\sim N(\mathbf{X\beta + Zb}, \mathbf{\Sigma(\theta)}), \\
\mathbf{b | \theta} &\sim N(\mathbf{0, B(\theta)}).
\end{aligned}
\]

To complete the Bayesian model, we specify prior distributions:

\begin{itemize}
\tightlist
\item
  \(f(\beta)\): Prior on fixed effects.
\item
  \(f(\theta)\): Prior on variance components.
\end{itemize}

Since analytical solutions are generally unavailable, we use Markov Chain Monte Carlo (MCMC) to sample from the posterior:

\begin{itemize}
\tightlist
\item
  Gibbs sampling (if conjugate priors are used).
\item
  Hamiltonian Monte Carlo (HMC) (for complex models).
\end{itemize}

\textbf{Advantages}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Accounts for Parameter Uncertainty

  \begin{itemize}
  \tightlist
  \item
    Unlike MLE/REML, Bayesian methods propagate uncertainty in variance component estimation.
  \end{itemize}
\item
  Flexible Model Specification

  \begin{itemize}
  \tightlist
  \item
    Can incorporate prior knowledge via informative priors.
  \item
    Extends naturally beyond Gaussian assumptions (e.g., Student-\(t\) distributions for heavy-tailed errors).
  \end{itemize}
\item
  Robustness in Small Samples

  \begin{itemize}
  \tightlist
  \item
    Bayesian methods can stabilize variance estimation in small datasets where MLE/REML are unreliable.
  \end{itemize}
\end{enumerate}

\textbf{Challenges}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Computational Complexity

  \begin{itemize}
  \tightlist
  \item
    Requires MCMC algorithms, which can be computationally expensive.
  \end{itemize}
\item
  Convergence Issues

  \begin{itemize}
  \tightlist
  \item
    MCMC chains must be checked for convergence (e.g., using R-hat diagnostic).
  \end{itemize}
\item
  Choice of Priors

  \begin{itemize}
  \tightlist
  \item
    Poorly chosen priors can bias estimates or slow down convergence.
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Comparison of Estimation Methods for \(\mathbf{V}\)

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.3455}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1152}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1273}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1455}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2667}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Assumptions
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Computational Cost
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Handles Non-Normality?
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Best Use Case
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\protect\hyperlink{sec-maximum-likelihood-estimation-lmm}{MLE} & Gaussian errors & High (iterative) &  No & Model selection (AIC/BIC) \\
\protect\hyperlink{sec-estimated-generalized-least-squares-lmm}{REML} & Gaussian errors & High (iterative) &  No & Variance estimation \\
\protect\hyperlink{sec-estimated-generalized-least-squares-lmm}{EGLS} & First two moments & Low (non-iterative) &  Yes & Large-scale models with correlated errors \\
\protect\hyperlink{sec-bayesian-hierarchical-models-lmm}{Bayesian} (BHM) & Probabilistic & Very High (MCMC) &  Yes & Small samples, prior information available \\
\end{longtable}

\hypertarget{inference-in-linear-mixed-models}{%
\section{Inference in Linear Mixed Models}\label{inference-in-linear-mixed-models}}

\hypertarget{inference-for-fixed-effects-beta}{%
\subsection{\texorpdfstring{Inference for Fixed Effects (\(\beta\))}{Inference for Fixed Effects (\textbackslash beta)}}\label{inference-for-fixed-effects-beta}}

The goal is to test hypotheses about the fixed effects parameters \(\beta\) using various statistical tests:

\begin{itemize}
\item
  \protect\hyperlink{wald-test-lmm}{Wald Test}
\item
  \protect\hyperlink{sec-f-test-lmm}{F-Test}
\item
  \protect\hyperlink{sec-likelihood-ratio-test-lmm}{Likelihood Ratio Test}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{wald-test-lmm}{%
\subsubsection{Wald Test}\label{wald-test-lmm}}

The Wald test assesses whether certain linear combinations of fixed effects are equal to specified values.

Given:

\[
\hat{\beta}(\theta) = \left( \mathbf{X}' \mathbf{V}^{-1}(\theta) \mathbf{X} \right)^{-1} \mathbf{X}' \mathbf{V}^{-1}(\theta) \mathbf{Y},
\]

and its variance:

\[
\text{Var}(\hat{\beta}(\theta)) = \left( \mathbf{X}' \mathbf{V}^{-1}(\theta) \mathbf{X} \right)^{-1}.
\]

In practice, we substitute \(\hat{\theta}\) (the estimate of \(\theta\)) to obtain:

\begin{itemize}
\item
  Hypotheses:

  \[
  H_0: \mathbf{A \beta} = \mathbf{d}
  \]

  where:

  \begin{itemize}
  \tightlist
  \item
    \(\mathbf{A}\) is a contrast matrix specifying linear combinations of \(\beta\).
  \item
    \(\mathbf{d}\) is a constant vector representing the null hypothesis values.
  \end{itemize}
\item
  Wald Test Statistic:

  \[
  W = (\mathbf{A} \hat{\beta} - \mathbf{d})' \left[ \mathbf{A} \left( \mathbf{X}' \hat{\mathbf{V}}^{-1} \mathbf{X} \right)^{-1} \mathbf{A}' \right]^{-1} (\mathbf{A} \hat{\beta} - \mathbf{d}).
  \]
\item
  Distribution under \(H_0\):

  \[
  W \sim \chi^2_{\text{rank}(\mathbf{A})}.
  \]
\end{itemize}

\textbf{Caution with Wald Test:}

\begin{itemize}
\tightlist
\item
  Underestimation of Variance:\\
  The Wald test ignores the variability from estimating \(\hat{\theta}\), leading to underestimated standard errors and potentially inflated Type I error rates.
\item
  Small Sample Issues:\\
  Less reliable in small samples or when variance components are near boundary values (e.g., variances close to zero).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{sec-f-test-lmm}{%
\subsubsection{F-Test}\label{sec-f-test-lmm}}

An alternative to the Wald test, the F-test adjusts for the estimation of \(\sigma^2\) and provides better performance in small samples.

Assume:

\[
\text{Var}(\mathbf{Y}) = \sigma^2 \mathbf{V}(\theta).
\]

The F-statistic is:

\[
F^* = \frac{(\mathbf{A} \hat{\beta} - \mathbf{d})' \left[ \mathbf{A} \left( \mathbf{X}' \hat{\mathbf{V}}^{-1} \mathbf{X} \right)^{-1} \mathbf{A}' \right]^{-1} (\mathbf{A} \hat{\beta} - \mathbf{d})}{\hat{\sigma}^2 \, \text{rank}(\mathbf{A})}.
\]

\begin{itemize}
\item
  Distribution under \(H_0\):

  \[
  F^* \sim F_{\text{rank}(\mathbf{A}), \, \text{df}_{\text{denominator}}}.
  \]
\item
  Approximating Denominator Degrees of Freedom:

  \begin{itemize}
  \tightlist
  \item
    Satterthwaite approximation
  \item
    Kenward-Roger approximation (provides bias-corrected standard errors)
  \end{itemize}
\end{itemize}

F-Test Advantages:

\begin{itemize}
\tightlist
\item
  More accurate in small samples compared to the Wald test.
\item
  Adjusts for variance estimation, reducing bias in hypothesis testing.
\end{itemize}

Wald Test vs.~F-Test:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2600}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3900}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3500}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Criterion
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Wald Test
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
F-Test
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Small Sample Performance & Poor (can inflate Type I error) & Better control of Type I error \\
Variance Estimation & Ignores variability in \(\hat{\theta}\) & Adjusts using \(\hat{\sigma}^2\) \\
Reduction to t-test & Yes (for single \(\beta\)) & Yes (when rank(\(\mathbf{A}\)) = 1) \\
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{sec-likelihood-ratio-test-lmm}{%
\subsubsection{Likelihood Ratio Test}\label{sec-likelihood-ratio-test-lmm}}

The Likelihood Ratio Test (LRT) compares the fit of nested models:

\begin{itemize}
\item
  Null Hypothesis:

  \[
  H_0: \beta \in \Theta_{\beta,0}
  \]

  where \(\Theta_{\beta,0}\) is a subset of the full parameter space \(\Theta_{\beta}\).
\item
  Test Statistic:

  \[
  -2 \log \lambda = -2 \log \left( \frac{\hat{L}_{ML,0}}{\hat{L}_{ML}} \right),
  \]

  where:

  \begin{itemize}
  \tightlist
  \item
    \(\hat{L}_{ML,0}\) = Maximized likelihood under \(H_0\) (restricted model)
  \item
    \(\hat{L}_{ML}\) = Maximized likelihood under the alternative (full model)
  \end{itemize}
\item
  Distribution under \(H_0\):

  \[
  -2 \log \lambda \sim \chi^2_{df}
  \]

  where \(df = \dim(\Theta_{\beta}) - \dim(\Theta_{\beta,0})\) (the difference in the number of parameters).
\end{itemize}

\textbf{Important Notes:}

\begin{itemize}
\tightlist
\item
  LRT is applicable only for \protect\hyperlink{sec-maximum-likelihood-estimation-lmm}{ML} estimates (not \protect\hyperlink{restricted-maximum-likelihood-lmm}{REML}) when comparing models with different fixed effects.
\item
  REML-based LRT can be used for comparing models that differ in random effects (variance components), but not fixed effects.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{inference-for-variance-components-theta}{%
\subsection{\texorpdfstring{Inference for Variance Components (\(\theta\))}{Inference for Variance Components (\textbackslash theta)}}\label{inference-for-variance-components-theta}}

For ML and REML estimators:

\[
\hat{\theta} \sim N(\theta, I(\theta)^{-1}),
\]

where \(I(\theta)\) is the \protect\hyperlink{fisher-information-matrix}{Fisher Information Matrix}.

This normal approximation holds well for large samples, enabling Wald-type tests and confidence intervals.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{wald-test-for-variance-components}{%
\subsubsection{Wald Test for Variance Components}\label{wald-test-for-variance-components}}

The Wald test for variance components follows the same structure as for \protect\hyperlink{wald-test-lmm}{fixed effects}:

\begin{itemize}
\item
  Test Statistic:

  \[
  W = \frac{(\hat{\theta} - \theta_0)^2}{\widehat{\text{Var}}(\hat{\theta})}.
  \]
\item
  Distribution under \(H_0\):

  \[
  W \sim \chi^2_1.
  \]
\end{itemize}

Limitations of Wald Test for Variance Components:

\begin{itemize}
\tightlist
\item
  Boundary Issues: The normal approximation fails when the true variance component is near zero (boundary of the parameter space).
\item
  Less reliable for variance parameters than for covariance parameters.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{likelihood-ratio-test-for-variance-components}{%
\subsubsection{Likelihood Ratio Test for Variance Components}\label{likelihood-ratio-test-for-variance-components}}

LRT can also be applied to variance components:

\begin{itemize}
\item
  Test Statistic:

  \[
  -2 \log \lambda = -2 \log \left( \frac{\hat{L}_{REML,0}}{\hat{L}_{REML}} \right).
  \]
\item
  Distribution under \(H_0\):

  \begin{itemize}
  \tightlist
  \item
    Not always \(\chi^2\)-distributed when variance components are on the boundary (e.g., testing if \(\sigma^2 = 0\)).
  \item
    May require mixture distributions or adjusted critical values.
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2353}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2721}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3676}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Test
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Best For
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strengths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitations
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Wald Test & Fixed effects (\(\beta\)) & Simple, widely used & Underestimates variance, biased in small samples \\
F-Test & Fixed effects (\(\beta\)) & Better in small samples, adjusts df & Requires approximation for degrees of freedom \\
LRT (ML) & Fixed effects, nested models & Powerful, widely used & Not valid for REML with fixed effects \\
LRT (REML) & Variance components & Robust for random effects & Boundary issues when variances are near zero \\
Wald (Variance) & Variance components (\(\theta\)) & Simple extension of Wald test & Fails near parameter space boundaries \\
\end{longtable}

\hypertarget{information-criteria-for-model-selection}{%
\section{Information Criteria for Model Selection}\label{information-criteria-for-model-selection}}

Information Criteria are statistical tools used to compare competing models by balancing model fit (likelihood) with model complexity (number of parameters).\\
They help in identifying the most parsimonious model that adequately explains the data without overfitting.

The three most commonly used criteria are:

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{sec-akaike-information-criterion-lmm}{Akaike Information Criterion} (AIC)
\item
  \protect\hyperlink{sec-corrected-aic-lmm}{Corrected Akaike Information Criterion} (AICc)
\item
  \protect\hyperlink{sec-bayesian-information-criterion-lmm}{Bayesian Information Criterion} (BIC)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{sec-akaike-information-criterion-lmm}{%
\subsection{Akaike Information Criterion}\label{sec-akaike-information-criterion-lmm}}

The Akaike Information Criterion is derived from the Kullback-Leibler divergence, which measures the difference between the true data-generating process and the fitted model.

\textbf{AIC Formula}:

\[
AIC = -2 \, l(\hat{\theta}, \hat{\beta}) + 2q
\]

where:

\begin{itemize}
\tightlist
\item
  \(l(\hat{\theta}, \hat{\beta})\): The maximized log-likelihood of the model, evaluated at the estimates \(\hat{\theta}\) (variance components) and \(\hat{\beta}\) (fixed effects).
\item
  \(q\): The effective number of parameters, including:

  \begin{itemize}
  \tightlist
  \item
    The number of fixed effects.
  \item
    The number of variance-covariance parameters (random effects).
  \item
    Excludes parameters constrained to boundary values (e.g., variances estimated as zero).
  \end{itemize}
\end{itemize}

\textbf{Key Points About AIC}

\begin{itemize}
\tightlist
\item
  Model Selection Rule:

  \begin{itemize}
  \tightlist
  \item
    Lower AIC indicates a better model.
  \item
    Occasionally, software may report AIC as \(l - q\), in which case higher AIC is better (rare).
  \end{itemize}
\item
  Comparing Random Effects Models:

  \begin{itemize}
  \tightlist
  \item
    Not recommended when comparing models with different random effects because it's difficult to accurately count the effective number of parameters.
  \end{itemize}
\item
  Sample Size Considerations:

  \begin{itemize}
  \tightlist
  \item
    Requires large sample sizes for reliable comparisons.
  \item
    In small samples, AIC tends to favor more complex models due to insufficient penalty for model complexity.
  \end{itemize}
\item
  Potential Bias:

  \begin{itemize}
  \tightlist
  \item
    Can be negatively biased (i.e., favoring overly complex models) when the sample size is small relative to the number of parameters.
  \end{itemize}
\end{itemize}

When to Use AIC:

\begin{itemize}
\tightlist
\item
  Comparing models with the same random effects structure but different fixed effects.
\item
  Selecting covariance structures in mixed models when the sample size is large.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{sec-corrected-aic-lmm}{%
\subsection{Corrected AIC}\label{sec-corrected-aic-lmm}}

The Corrected AIC (AICc) addresses the bias in AIC for small sample sizes. It was developed by \citep{hurvich1989regression}.

\textbf{AICc Formula}:

\[
AICc = AIC + \frac{2q(q + 1)}{n - q - 1}
\]

where:

\begin{itemize}
\tightlist
\item
  \(n\): The sample size.
\item
  \(q\): The number of estimated parameters.
\end{itemize}

\textbf{Key Points About AICc}

\begin{itemize}
\tightlist
\item
  Small Sample Correction:

  \begin{itemize}
  \tightlist
  \item
    Provides a stronger penalty for model complexity when the sample size is small.
  \end{itemize}
\item
  Applicability:

  \begin{itemize}
  \tightlist
  \item
    Valid when comparing models with fixed covariance structures.
  \item
    Not recommended for models with general covariance structures due to difficulties in bias correction.
  \end{itemize}
\item
  Model Selection Rule:

  \begin{itemize}
  \tightlist
  \item
    Lower AICc indicates a better model.
  \end{itemize}
\end{itemize}

When to Use AICc:

\begin{itemize}
\tightlist
\item
  Small sample sizes (\(n/q\) ratio is low).
\item
  Models with fixed random effects or simple covariance structures.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{sec-bayesian-information-criterion-lmm}{%
\subsection{Bayesian Information Criterion}\label{sec-bayesian-information-criterion-lmm}}

The Bayesian Information Criterion is derived from a Bayesian framework and incorporates a stronger penalty for model complexity compared to AIC.

\textbf{BIC Formula}

\[
BIC = -2 \, l(\hat{\theta}, \hat{\beta}) + q \log(n)
\]

where:

\begin{itemize}
\tightlist
\item
  \(n\): The number of observations.
\item
  \(q\): The number of effective parameters.
\item
  \(l(\hat{\theta}, \hat{\beta})\): The maximized log-likelihood.
\end{itemize}

\textbf{Key Points About BIC}

\begin{itemize}
\tightlist
\item
  Model Selection Rule:

  \begin{itemize}
  \tightlist
  \item
    Lower BIC indicates a better model.
  \end{itemize}
\item
  Stronger Penalty:

  \begin{itemize}
  \tightlist
  \item
    The penalty term \(q \log(n)\) grows with sample size, leading BIC to favor simpler models more than AIC.
  \end{itemize}
\item
  Applicability to MLE and REML:

  \begin{itemize}
  \tightlist
  \item
    BIC can be used with both MLE and REML, but:

    \begin{itemize}
    \tightlist
    \item
      Use MLE when comparing models with different fixed effects.
    \item
      Use REML when comparing models with different random effects (same fixed effects).
    \end{itemize}
  \end{itemize}
\item
  Consistency:

  \begin{itemize}
  \tightlist
  \item
    BIC is consistent, meaning that as the sample size increases, it will select the true model with probability 1 (if the true model is among the candidates).
  \end{itemize}
\end{itemize}

When to Use BIC:

\begin{itemize}
\tightlist
\item
  Large sample sizes where model simplicity is prioritized.
\item
  Model selection for hypothesis testing (due to its connection to Bayesian inference).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Comparison of AIC, AICc, and BIC}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.0949}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2215}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1772}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.3418}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1646}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Criterion}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Formula}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Penalty Term}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Best For}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Model Selection Rule}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{AIC} & \(-2l + 2q\) & \(2q\) & General model comparison (large \(n\)) & Lower is better \\
\textbf{AICc} & \(AIC + \frac{2q(q+1)}{n - q - 1}\) & Adjusted for small samples & Small sample sizes, simple random effects & Lower is better \\
\textbf{BIC} & \(-2l + q \log(n)\) & \(q \log(n)\) & Large samples, model selection in hypothesis testing & Lower is better \\
\end{longtable}

\textbf{Key Takeaways}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  AIC is suitable for large datasets and general model comparisons but may favor overly complex models in small samples.
\item
  AICc corrects AIC's bias in small sample sizes.
\item
  BIC favors simpler models, especially as the sample size increases, making it suitable for hypothesis testing and situations where parsimony is essential.
\item
  Use MLE for comparing models with different fixed effects, and REML when comparing models with different random effects (same fixed effects).
\item
  When comparing random effects structures, AIC and BIC may not be reliable due to difficulty in counting effective parameters accurately.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{practical-example-with-linear-mixed-models}{%
\subsection{Practical Example with Linear Mixed Models}\label{practical-example-with-linear-mixed-models}}

Consider the \protect\hyperlink{sec-linear-mixed-models}{Linear Mixed Model}:

\[
Y_{ik} = 
\begin{cases}
\beta_0 + b_{1i} + (\beta_1 + b_{2i}) t_{ij} + \epsilon_{ij} & L \\
\beta_0 + b_{1i} + (\beta_2 + b_{2i}) t_{ij} + \epsilon_{ij} & H \\
\beta_0 + b_{1i} + (\beta_3 + b_{2i}) t_{ij} + \epsilon_{ij} & C
\end{cases}
\]

where:

\begin{itemize}
\tightlist
\item
  \(i = 1, \dots, N\) (subjects)
\item
  \(j = 1, \dots, n_i\) (repeated measures at time \(t_{ij}\))
\end{itemize}

\[
\begin{aligned}
\mathbf{Y}_i | b_i &\sim N(\mathbf{X}_i \beta + \mathbf{1} b_i, \sigma^2 \mathbf{I}) \\
b_i &\sim N(0, d_{11})
\end{aligned}
\]

We aim to \textbf{estimate}:

\begin{itemize}
\tightlist
\item
  \textbf{Fixed effects}: \(\beta\)
\item
  \textbf{Variance components}: \(\sigma^2\), \(d_{11}\)
\item
  \textbf{Random effects}: Predict \(b_i\)
\end{itemize}

When comparing models (e.g., different random slopes or covariance structures), we can compute:

\begin{itemize}
\tightlist
\item
  \textbf{AIC}: Penalizes model complexity with \(2q\).
\item
  \textbf{BIC}: Stronger penalty via \(q \log(n)\), favoring simpler models.
\item
  \textbf{AICc}: Adjusted AIC for small sample sizes.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Practical Example with Linear Mixed Models in R}


\CommentTok{\# Load required libraries}
\FunctionTok{library}\NormalTok{(lme4)     }\CommentTok{\# For fitting linear mixed{-}effects models}
\FunctionTok{library}\NormalTok{(MuMIn)    }\CommentTok{\# For calculating AICc}
\FunctionTok{library}\NormalTok{(dplyr)    }\CommentTok{\# For data manipulation}

\CommentTok{\# Set seed for reproducibility}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\# Simulate Data}
\NormalTok{N }\OtherTok{\textless{}{-}} \DecValTok{50}             \CommentTok{\# Number of subjects}
\NormalTok{n\_i }\OtherTok{\textless{}{-}} \DecValTok{5}            \CommentTok{\# Number of repeated measures per subject}
\NormalTok{t\_ij }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_i, N)  }\CommentTok{\# Time points}

\CommentTok{\# Treatment groups (L, H, C)}
\NormalTok{treatment }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"L"}\NormalTok{, }\StringTok{"H"}\NormalTok{, }\StringTok{"C"}\NormalTok{), }\AttributeTok{length.out =}\NormalTok{ N)}
\NormalTok{group }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(}\FunctionTok{rep}\NormalTok{(treatment, }\AttributeTok{each =}\NormalTok{ n\_i))}

\CommentTok{\# Simulate random effects}
\NormalTok{b1\_i }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(N, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{2}\NormalTok{)    }\CommentTok{\# Random intercepts}
\NormalTok{b2\_i }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(N, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)    }\CommentTok{\# Random slopes}

\CommentTok{\# Fixed effects}
\NormalTok{beta\_0 }\OtherTok{\textless{}{-}} \DecValTok{5}
\NormalTok{beta\_1 }\OtherTok{\textless{}{-}} \FloatTok{0.5}
\NormalTok{beta\_2 }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{beta\_3 }\OtherTok{\textless{}{-}} \FloatTok{1.5}

\CommentTok{\# Generate response variable Y based on the specified model}
\NormalTok{Y }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(N }\SpecialCharTok{*}\NormalTok{ n\_i)}
\NormalTok{subject\_id }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{N, }\AttributeTok{each =}\NormalTok{ n\_i)}

\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N) \{}
    \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_i) \{}
\NormalTok{        idx }\OtherTok{\textless{}{-}}\NormalTok{ (i }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{) }\SpecialCharTok{*}\NormalTok{ n\_i }\SpecialCharTok{+}\NormalTok{ j}
\NormalTok{        time }\OtherTok{\textless{}{-}}\NormalTok{ t\_ij[idx]}
        
        \CommentTok{\# Treatment{-}specific model}
        \ControlFlowTok{if}\NormalTok{ (group[idx] }\SpecialCharTok{==} \StringTok{"L"}\NormalTok{) \{}
\NormalTok{            Y[idx] }\OtherTok{\textless{}{-}}
\NormalTok{                beta\_0 }\SpecialCharTok{+}\NormalTok{ b1\_i[i] }\SpecialCharTok{+}\NormalTok{ (beta\_1 }\SpecialCharTok{+}\NormalTok{ b2\_i[i]) }\SpecialCharTok{*}\NormalTok{ time }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{        \} }\ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{ (group[idx] }\SpecialCharTok{==} \StringTok{"H"}\NormalTok{) \{}
\NormalTok{            Y[idx] }\OtherTok{\textless{}{-}}
\NormalTok{                beta\_0 }\SpecialCharTok{+}\NormalTok{ b1\_i[i] }\SpecialCharTok{+}\NormalTok{ (beta\_2 }\SpecialCharTok{+}\NormalTok{ b2\_i[i]) }\SpecialCharTok{*}\NormalTok{ time }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{        \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{            Y[idx] }\OtherTok{\textless{}{-}}
\NormalTok{                beta\_0 }\SpecialCharTok{+}\NormalTok{ b1\_i[i] }\SpecialCharTok{+}\NormalTok{ (beta\_3 }\SpecialCharTok{+}\NormalTok{ b2\_i[i]) }\SpecialCharTok{*}\NormalTok{ time }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{        \}}
\NormalTok{    \}}
\NormalTok{\}}

\CommentTok{\# Combine into a data frame}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
    \AttributeTok{Y =}\NormalTok{ Y,}
    \AttributeTok{time =}\NormalTok{ t\_ij,}
    \AttributeTok{group =}\NormalTok{ group,}
    \AttributeTok{subject =} \FunctionTok{factor}\NormalTok{(subject\_id)}
\NormalTok{)}

\CommentTok{\# Fit Linear Mixed Models}
\CommentTok{\# Model 1: Random Intercepts Only}
\NormalTok{model1 }\OtherTok{\textless{}{-}}
    \FunctionTok{lmer}\NormalTok{(Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ time }\SpecialCharTok{*}\NormalTok{ group }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ subject), }\AttributeTok{data =}\NormalTok{ data, }\AttributeTok{REML =} \ConstantTok{FALSE}\NormalTok{)}

\CommentTok{\# Model 2: Random Intercepts and Random Slopes}
\NormalTok{model2 }\OtherTok{\textless{}{-}}
    \FunctionTok{lmer}\NormalTok{(Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ time }\SpecialCharTok{*}\NormalTok{ group }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+}\NormalTok{ time }\SpecialCharTok{|}
\NormalTok{                                 subject),}
         \AttributeTok{data =}\NormalTok{ data,}
         \AttributeTok{REML =} \ConstantTok{FALSE}\NormalTok{)}

\CommentTok{\# Model 3: Simpler Model (No Interaction)}
\NormalTok{model3 }\OtherTok{\textless{}{-}}
    \FunctionTok{lmer}\NormalTok{(Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ time }\SpecialCharTok{+}\NormalTok{ group }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ subject), }\AttributeTok{data =}\NormalTok{ data, }\AttributeTok{REML =} \ConstantTok{FALSE}\NormalTok{)}

\CommentTok{\# Extract Information Criteria}
\NormalTok{results }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
    \AttributeTok{Model =} \FunctionTok{c}\NormalTok{(}
        \StringTok{"Random Intercepts"}\NormalTok{,}
        \StringTok{"Random Intercepts + Slopes"}\NormalTok{,}
        \StringTok{"No Interaction"}
\NormalTok{    ),}
    \AttributeTok{AIC =} \FunctionTok{c}\NormalTok{(}\FunctionTok{AIC}\NormalTok{(model1), }\FunctionTok{AIC}\NormalTok{(model2), }\FunctionTok{AIC}\NormalTok{(model3)),}
    \AttributeTok{BIC =} \FunctionTok{c}\NormalTok{(}\FunctionTok{BIC}\NormalTok{(model1), }\FunctionTok{BIC}\NormalTok{(model2), }\FunctionTok{BIC}\NormalTok{(model3)),}
    \AttributeTok{AICc =} \FunctionTok{c}\NormalTok{(}\FunctionTok{AICc}\NormalTok{(model1), }\FunctionTok{AICc}\NormalTok{(model2), }\FunctionTok{AICc}\NormalTok{(model3))}
\NormalTok{)}

\CommentTok{\# Display the results}
\FunctionTok{print}\NormalTok{(results)}
\CommentTok{\#\textgreater{}                        Model       AIC      BIC      AICc}
\CommentTok{\#\textgreater{} 1          Random Intercepts 1129.2064 1157.378 1129.8039}
\CommentTok{\#\textgreater{} 2 Random Intercepts + Slopes  974.5514 1009.766  975.4719}
\CommentTok{\#\textgreater{} 3             No Interaction 1164.2797 1185.408 1164.6253}
\end{Highlighting}
\end{Shaded}

\textbf{Interpretation of Results:}

\begin{itemize}
\item
  Model 2 (with random intercepts and slopes) has the lowest AIC, BIC, and AICc, indicating the best fit among the models.
\item
  Model 1 (random intercepts only) performs worse, suggesting that allowing random slopes improves model fit.
\item
  Model 3 (simpler fixed effects without interaction) has the highest AIC/BIC/AICc, indicating poor fit compared to Models 1 and 2.
\end{itemize}

\textbf{Model Selection Criteria:}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1899}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2025}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.6076}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Criterion}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Best Model}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Reason}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{AIC} & Model 2 & Best trade-off between fit and complexity \\
\textbf{BIC} & Model 2 & Stronger penalty for complexity, still favored \\
\textbf{AICc} & Model 2 & Adjusted for small samples, Model 2 still best \\
\end{longtable}

\hypertarget{split-plot-designs}{%
\section{Split-Plot Designs}\label{split-plot-designs}}

Split-plot designs are commonly used in experimental settings where there are \textbf{two or more factors}, and at least one of them requires \textbf{larger experimental units} compared to the other(s). This situation often arises in agricultural, industrial, and business experiments where certain treatments are harder or more expensive to apply.

\textbf{Key Characteristics}

\begin{itemize}
\tightlist
\item
  Two factors with different experimental unit requirements:

  \begin{itemize}
  \tightlist
  \item
    Factor A (Whole-plot factor): Requires large experimental units (e.g., different fields, production batches).
  \item
    Factor B (Sub-plot factor): Can be applied to smaller units within the larger experimental units (e.g., plots within fields, products within batches).
  \end{itemize}
\item
  Blocking: The experiment is typically divided into blocks (or replicates) to account for variability. However, unlike \protect\hyperlink{randomized-block-designs}{Randomized Block Designs}, the randomization process in split-plot designs occurs at two levels:

  \begin{itemize}
  \tightlist
  \item
    Whole-plot randomization: Factor A is randomized across large units within each block.
  \item
    Sub-plot randomization: Factor B is randomized within each whole plot.
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-setup}{%
\subsection{Example Setup}\label{example-setup}}

\begin{itemize}
\tightlist
\item
  \textbf{Factor A:} 3 levels (applied to large units).
\item
  \textbf{Factor B:} 2 levels (applied within the large units).
\item
  \textbf{4 Blocks (replicates):} Each containing all combinations of A and B.
\end{itemize}

Unlike in \protect\hyperlink{randomized-block-designs}{Randomized Block Designs}, the randomization of Factor A is \textbf{restricted} due to the larger unit size. Factor A is applied \textbf{once per block}, while Factor B can be applied \textbf{multiple times within each block}.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{statistical-model-for-split-plot-designs}{%
\subsection{Statistical Model for Split-Plot Designs}\label{statistical-model-for-split-plot-designs}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  When Factor A is the Primary Focus (Whole-Plot Analysis)
\end{enumerate}

\[
Y_{ij} = \mu + \rho_i + \alpha_j + e_{ij}
\]

Where:

\begin{itemize}
\item
  \(Y_{ij}\) = Response for the \(j\)-th level of factor A in the \(i\)-th block.
\item
  \(\mu\) = Overall mean.
\item
  \(\rho_i\) = Random effect of the \(i\)-th block (\(\rho_i \sim N(0, \sigma^2_{\rho})\)).
\item
  \(\alpha_j\) = Fixed effect of factor A (main effect).
\item
  \(e_{ij} \sim N(0, \sigma^2_{e})\) = Whole-plot error (random), representing the variability within blocks due to factor A.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  When Factor B is the Primary Focus (Sub-Plot Analysis)
\end{enumerate}

\[
Y_{ijk} = \mu + \phi_{ij} + \beta_k + \epsilon_{ijk}
\]

Where:

\begin{itemize}
\item
  \(Y_{ijk}\) = Response for the \(k\)-th level of factor B within the \(j\)-th level of factor A and \(i\)-th block.
\item
  \(\phi_{ij}\) = Combined effect of block and factor A: \[
    \phi_{ij} = \rho_i + \alpha_j + e_{ij}
    \]
\item
  \(\beta_k\) = Fixed effect of factor B (main effect).
\item
  \(\epsilon_{ijk} \sim N(0, \sigma^2_{\epsilon})\) = Sub-plot error (random), capturing variability within whole plots.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Full Split-Plot Model (Including Interaction)
\end{enumerate}

\[
Y_{ijk} = \mu + \rho_i + \alpha_j + e_{ij} + \beta_k + (\alpha \beta)_{jk} + \epsilon_{ijk}
\]

Where:

\begin{itemize}
\item
  \(i\) = Block (replication). - \(j\) = Level of factor A (whole-plot factor).
\item
  \(k\) = Level of factor B (sub-plot factor).
\item
  \(\mu\) = Overall mean.
\item
  \(\rho_i\) = Random effect of the \(i\)-th block.
\item
  \(\alpha_j\) = Fixed main effect of factor A.
\item
  \(e_{ij} \sim N(0, \sigma^2_{e})\) = Whole-plot error (random).
\item
  \(\beta_k\) = Fixed main effect of factor B.
\item
  \((\alpha \beta)_{jk}\) = Fixed interaction effect between factors A and B.
\item
  \(\epsilon_{ijk} \sim N(0, \sigma^2_{\epsilon})\) = Sub-plot error (random).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{approaches-to-analyzing-split-plot-designs}{%
\subsection{Approaches to Analyzing Split-Plot Designs}\label{approaches-to-analyzing-split-plot-designs}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \protect\hyperlink{sec-analysis-of-variance-anova}{ANOVA} Perspective
\end{enumerate}

Whole-Plot Comparisons:

\begin{itemize}
\tightlist
\item
  Factor A vs.~Whole-Plot Error:\\
  Compare the variation due to factor A (\(\alpha_j\)) against the whole-plot error (\(e_{ij}\)).
\item
  Blocks vs.~Whole-Plot Error:\\
  Compare the variation due to blocks (\(\rho_i\)) against the whole-plot error (\(e_{ij}\)).
\end{itemize}

Sub-Plot Comparisons:

\begin{itemize}
\tightlist
\item
  Factor B vs.~Sub-Plot Error:\\
  Compare the variation due to factor B (\(\beta_k\)) against the sub-plot error (\(\epsilon_{ijk}\)).
\item
  Interaction (A  B) vs.~Sub-Plot Error:\\
  Compare the interaction effect (\((\alpha \beta)_{jk}\)) against the sub-plot error (\(\epsilon_{ijk}\)).
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Mixed Model Perspective}
\end{enumerate}

A more flexible approach is to treat split-plot designs using mixed-effects models, which can handle both fixed and random effects explicitly:

\[
\mathbf{Y = X \beta + Zb + \epsilon}
\]

Where:

\begin{itemize}
\item
  \(\mathbf{Y}\) = Vector of observed responses.
\item
  \(\mathbf{X}\) = Design matrix for fixed effects (e.g., factors A, B, and their interaction).
\item
  \(\boldsymbol{\beta}\) = Vector of fixed-effect coefficients (e.g., \(\mu\), \(\alpha_j\), \(\beta_k\), \((\alpha \beta)_{jk}\)).
\item
  \(\mathbf{Z}\) = Design matrix for random effects (e.g., blocks and whole-plot errors).
\item
  \(\mathbf{b}\) = Vector of random-effect coefficients (e.g., \(\rho_i\), \(e_{ij}\)).
\item
  \(\boldsymbol{\epsilon}\) = Vector of residuals (sub-plot error).
\end{itemize}

Mixed models are particularly useful when:

\begin{itemize}
\item
  There are \textbf{unbalanced designs} (missing data).
\item
  You need to account for \textbf{complex correlation structures} within the data.
\end{itemize}

\hypertarget{application-split-plot-design}{%
\subsection{Application: Split-Plot Design}\label{application-split-plot-design}}

Consider an agricultural experiment designed to study the effects of irrigation and crop variety on yield. This scenario is well-suited for a split-plot design because irrigation treatments are applied to large plots (fields), while different crop varieties are planted within these plots.

\hypertarget{model-specification}{%
\subsubsection{Model Specification}\label{model-specification}}

The linear mixed-effects model is defined as:

\[
y_{ijk} = \mu + i_i + v_j + (iv)_{ij} + f_k + \epsilon_{ijk}
\]

Where:

\begin{itemize}
\item
  \(y_{ijk}\) = Observed yield for the \(i\)-th irrigation, \(j\)-th variety, in the \(k\)-th field.
\item
  \(\mu\) = Overall mean yield.
\item
  \(i_i\) = Fixed effect of the \(i\)-th irrigation level.
\item
  \(v_j\) = Fixed effect of the \(j\)-th crop variety.
\item
  \((iv)_{ij}\) = Interaction effect between irrigation and variety (fixed).
\item
  \(f_k \sim N(0, \sigma^2_f)\) = Random effect of field (captures variability between fields).
\item
  \(\epsilon_{ijk} \sim N(0, \sigma^2_\epsilon)\) = Residual error.
\end{itemize}

\textbf{Note:}\\
Since each variety-field combination is observed only once, we cannot model a random interaction between variety and field.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{data-exploration}{%
\subsubsection{Data Exploration}\label{data-exploration}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{data}\NormalTok{(irrigation, }\AttributeTok{package =} \StringTok{"faraway"}\NormalTok{)  }\CommentTok{\# Load the dataset}

\CommentTok{\# Summary statistics and preview}
\FunctionTok{summary}\NormalTok{(irrigation)}
\CommentTok{\#\textgreater{}      field   irrigation variety     yield      }
\CommentTok{\#\textgreater{}  f1     :2   i1:4       v1:8    Min.   :34.80  }
\CommentTok{\#\textgreater{}  f2     :2   i2:4       v2:8    1st Qu.:37.60  }
\CommentTok{\#\textgreater{}  f3     :2   i3:4               Median :40.15  }
\CommentTok{\#\textgreater{}  f4     :2   i4:4               Mean   :40.23  }
\CommentTok{\#\textgreater{}  f5     :2                      3rd Qu.:42.73  }
\CommentTok{\#\textgreater{}  f6     :2                      Max.   :47.60  }
\CommentTok{\#\textgreater{}  (Other):4}
\FunctionTok{head}\NormalTok{(irrigation, }\DecValTok{4}\NormalTok{)}
\CommentTok{\#\textgreater{}   field irrigation variety yield}
\CommentTok{\#\textgreater{} 1    f1         i1      v1  35.4}
\CommentTok{\#\textgreater{} 2    f1         i1      v2  37.9}
\CommentTok{\#\textgreater{} 3    f2         i2      v1  36.7}
\CommentTok{\#\textgreater{} 4    f2         i2      v2  38.2}

\CommentTok{\# Exploratory plot: Yield by field, colored by variety and shaped by irrigation}
\FunctionTok{ggplot}\NormalTok{(irrigation,}
       \FunctionTok{aes}\NormalTok{(}
           \AttributeTok{x     =}\NormalTok{ field,}
           \AttributeTok{y     =}\NormalTok{ yield,}
           \AttributeTok{shape =}\NormalTok{ irrigation,}
           \AttributeTok{color =}\NormalTok{ variety}
\NormalTok{       )) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{size =} \DecValTok{3}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Yield by Field, Irrigation, and Variety"}\NormalTok{,}
         \AttributeTok{x =} \StringTok{"Field"}\NormalTok{,}
         \AttributeTok{y =} \StringTok{"Yield"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{08-linear-mixed-models_files/figure-latex/unnamed-chunk-2-1} \end{center}

This plot helps visualize how yield varies across fields, under different irrigation treatments, and for different varieties.

\hypertarget{fitting-the-initial-mixed-effects-model}{%
\subsubsection{Fitting the Initial Mixed-Effects Model}\label{fitting-the-initial-mixed-effects-model}}

We fit a mixed-effects model where:

\begin{itemize}
\item
  Irrigation and variety (and their interaction) are fixed effects.
\item
  Field is modeled as a random effect to account for variability between fields.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(lmerTest)  }\CommentTok{\# Provides p{-}values for lmer models}

\CommentTok{\# Full model with interaction term}
\NormalTok{sp\_model }\OtherTok{\textless{}{-}}
    \FunctionTok{lmer}\NormalTok{(yield }\SpecialCharTok{\textasciitilde{}}\NormalTok{ irrigation }\SpecialCharTok{*}\NormalTok{ variety }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ field), }\AttributeTok{data =}\NormalTok{ irrigation)}
\FunctionTok{summary}\NormalTok{(sp\_model)}
\CommentTok{\#\textgreater{} Linear mixed model fit by REML. t{-}tests use Satterthwaite\textquotesingle{}s method [}
\CommentTok{\#\textgreater{} lmerModLmerTest]}
\CommentTok{\#\textgreater{} Formula: yield \textasciitilde{} irrigation * variety + (1 | field)}
\CommentTok{\#\textgreater{}    Data: irrigation}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} REML criterion at convergence: 45.4}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Scaled residuals: }
\CommentTok{\#\textgreater{}     Min      1Q  Median      3Q     Max }
\CommentTok{\#\textgreater{} {-}0.7448 {-}0.5509  0.0000  0.5509  0.7448 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Random effects:}
\CommentTok{\#\textgreater{}  Groups   Name        Variance Std.Dev.}
\CommentTok{\#\textgreater{}  field    (Intercept) 16.200   4.025   }
\CommentTok{\#\textgreater{}  Residual              2.107   1.452   }
\CommentTok{\#\textgreater{} Number of obs: 16, groups:  field, 8}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Fixed effects:}
\CommentTok{\#\textgreater{}                        Estimate Std. Error     df t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)              38.500      3.026  4.487  12.725 0.000109 ***}
\CommentTok{\#\textgreater{} irrigationi2              1.200      4.279  4.487   0.280 0.791591    }
\CommentTok{\#\textgreater{} irrigationi3              0.700      4.279  4.487   0.164 0.877156    }
\CommentTok{\#\textgreater{} irrigationi4              3.500      4.279  4.487   0.818 0.454584    }
\CommentTok{\#\textgreater{} varietyv2                 0.600      1.452  4.000   0.413 0.700582    }
\CommentTok{\#\textgreater{} irrigationi2:varietyv2   {-}0.400      2.053  4.000  {-}0.195 0.855020    }
\CommentTok{\#\textgreater{} irrigationi3:varietyv2   {-}0.200      2.053  4.000  {-}0.097 0.927082    }
\CommentTok{\#\textgreater{} irrigationi4:varietyv2    1.200      2.053  4.000   0.584 0.590265    }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Correlation of Fixed Effects:}
\CommentTok{\#\textgreater{}             (Intr) irrgt2 irrgt3 irrgt4 vrtyv2 irr2:2 irr3:2}
\CommentTok{\#\textgreater{} irrigation2 {-}0.707                                          }
\CommentTok{\#\textgreater{} irrigation3 {-}0.707  0.500                                   }
\CommentTok{\#\textgreater{} irrigation4 {-}0.707  0.500  0.500                            }
\CommentTok{\#\textgreater{} varietyv2   {-}0.240  0.170  0.170  0.170                     }
\CommentTok{\#\textgreater{} irrgtn2:vr2  0.170 {-}0.240 {-}0.120 {-}0.120 {-}0.707              }
\CommentTok{\#\textgreater{} irrgtn3:vr2  0.170 {-}0.120 {-}0.240 {-}0.120 {-}0.707  0.500       }
\CommentTok{\#\textgreater{} irrgtn4:vr2  0.170 {-}0.120 {-}0.120 {-}0.240 {-}0.707  0.500  0.500}

\CommentTok{\# ANOVA table using Kenward{-}Roger approximation for accurate p{-}values}
\FunctionTok{anova}\NormalTok{(sp\_model, }\AttributeTok{ddf =} \StringTok{"Kenward{-}Roger"}\NormalTok{)}
\CommentTok{\#\textgreater{} Type III Analysis of Variance Table with Kenward{-}Roger\textquotesingle{}s method}
\CommentTok{\#\textgreater{}                    Sum Sq Mean Sq NumDF DenDF F value Pr(\textgreater{}F)}
\CommentTok{\#\textgreater{} irrigation         2.4545 0.81818     3     4  0.3882 0.7685}
\CommentTok{\#\textgreater{} variety            2.2500 2.25000     1     4  1.0676 0.3599}
\CommentTok{\#\textgreater{} irrigation:variety 1.5500 0.51667     3     4  0.2452 0.8612}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  Check the p-value of the interaction term (\texttt{irrigation:variety}).

  \begin{itemize}
  \tightlist
  \item
    If insignificant, this suggests no strong evidence of an interaction effect, and we may simplify the model by removing it.
  \end{itemize}
\end{itemize}

\hypertarget{model-simplification-testing-for-additivity}{%
\subsubsection{Model Simplification: Testing for Additivity}\label{model-simplification-testing-for-additivity}}

We compare the \textbf{full model} (with interaction) to an \textbf{additive model} (without interaction):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(lme4)}

\CommentTok{\# Additive model (no interaction)}
\NormalTok{sp\_model\_additive }\OtherTok{\textless{}{-}} \FunctionTok{lmer}\NormalTok{(yield }\SpecialCharTok{\textasciitilde{}}\NormalTok{ irrigation }\SpecialCharTok{+}\NormalTok{ variety }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ field), }\AttributeTok{data =}\NormalTok{ irrigation)}

\CommentTok{\# Likelihood ratio test comparing the two models}
\FunctionTok{anova}\NormalTok{(sp\_model\_additive, sp\_model, }\AttributeTok{ddf =} \StringTok{"Kenward{-}Roger"}\NormalTok{)}
\CommentTok{\#\textgreater{} Data: irrigation}
\CommentTok{\#\textgreater{} Models:}
\CommentTok{\#\textgreater{} sp\_model\_additive: yield \textasciitilde{} irrigation + variety + (1 | field)}
\CommentTok{\#\textgreater{} sp\_model: yield \textasciitilde{} irrigation * variety + (1 | field)}
\CommentTok{\#\textgreater{}                   npar    AIC    BIC  logLik deviance  Chisq Df Pr(\textgreater{}Chisq)}
\CommentTok{\#\textgreater{} sp\_model\_additive    7 83.959 89.368 {-}34.980   69.959                     }
\CommentTok{\#\textgreater{} sp\_model            10 88.609 96.335 {-}34.305   68.609 1.3503  3     0.7172}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  \textbf{Hypotheses:}

  \begin{itemize}
  \item
    \(H_0\): The \textbf{additive model} (without interaction) fits the data adequately.
  \item
    \(H_a\): The \textbf{interaction model} provides a significantly better fit.
  \end{itemize}
\item
  \textbf{Interpretation:}

  \begin{itemize}
  \item
    If the p-value is insignificant, we fail to reject \(H_0\), meaning the simpler additive model is sufficient.
  \item
    Check AIC and BIC: Lower values indicate a better-fitting model, supporting the use of the additive model if consistent with the hypothesis test.
  \end{itemize}
\end{itemize}

\hypertarget{assessing-the-random-effect-exact-restricted-likelihood-ratio-test}{%
\subsubsection{Assessing the Random Effect: Exact Restricted Likelihood Ratio Test}\label{assessing-the-random-effect-exact-restricted-likelihood-ratio-test}}

To verify whether the random field effect is necessary, we conduct an exact RLRT:

\begin{itemize}
\item
  Hypotheses:

  \begin{itemize}
  \item
    \(H_0\): \(\sigma^2_f = 0\) (no variability between fields; random effect is unnecessary).
  \item
    \(H_a\): \(\sigma^2_f > 0\) (random field effect is significant).
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(RLRsim)}

\CommentTok{\# RLRT for the random effect of field}
\FunctionTok{exactRLRT}\NormalTok{(sp\_model)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  simulated finite sample distribution of RLRT.}
\CommentTok{\#\textgreater{}  }
\CommentTok{\#\textgreater{}  (p{-}value based on 10000 simulated values)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  }
\CommentTok{\#\textgreater{} RLRT = 6.1118, p{-}value = 0.01}
\end{Highlighting}
\end{Shaded}

\textbf{Interpretation:}

\begin{itemize}
\item
  If the p-value is significant, we reject \(H_0\), confirming that the random field effect is essential.
\item
  A significant random effect implies substantial variability between fields that must be accounted for in the model.
\end{itemize}

\hypertarget{repeated-measures-in-mixed-models}{%
\section{Repeated Measures in Mixed Models}\label{repeated-measures-in-mixed-models}}

Repeated measures data arise when \textbf{multiple observations} are collected from the \textbf{same subjects} over time or under different conditions. This introduces \textbf{correlation} between observations from the same subject, which must be accounted for in the statistical model.

Mixed-effects models are particularly effective for repeated measures because they allow us to model both \textbf{fixed effects} (e.g., treatment, time) and \textbf{random effects} (e.g., subject-specific variability).

The general form of a mixed-effects model for repeated measures is:

\[
Y_{ijk} = \mu + \alpha_i + \beta_j + (\alpha \beta)_{ij} + \delta_{i(k)} + \epsilon_{ijk}
\]

Where:

\begin{itemize}
\item
  \(Y_{ijk}\) = Response for the \(i\)-th group, \(j\)-th time point, and \(k\)-th subject.
\item
  \(\mu\) = Overall mean.
\item
  \(\alpha_i\) = Fixed effect of the \(i\)-th group (e.g., treatment group).
\item
  \(\beta_j\) = Fixed effect of the \(j\)-th time point (repeated measure effect).
\item
  \((\alpha \beta)_{ij}\) = Interaction effect between group and time (fixed).
\item
  \(\delta_{i(k)} \sim N(0, \sigma^2_\delta)\) = Random effect of the \(k\)-th subject within the \(i\)-th group (captures subject-specific deviations).
\item
  \(\epsilon_{ijk} \sim N(0, \sigma^2)\) = Residual error (independent across observations).
\end{itemize}

Here, \(i = 1, \dots, n_A\) (number of groups), \(j = 1, \dots, n_B\) (number of repeated measures), and \(k = 1, \dots, n_i\) (number of subjects in group \(i\)).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The \textbf{variance-covariance matrix} of the repeated observations for the \(k\)-th subject in the \(i\)-th group is given by:

\[
\mathbf{Y}_{ik} = 
\begin{pmatrix}
Y_{i1k} \\
Y_{i2k} \\
\vdots \\
Y_{in_Bk}
\end{pmatrix}
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compound Symmetry (CS) Structure
\end{enumerate}

Under the \textbf{compound symmetry} assumption (common in \protect\hyperlink{sec-random-intercepts-model-lmm}{random-intercepts models}), the covariance matrix is:

\[
\mathbf{\Sigma}_{\text{subject}} =
\begin{pmatrix}
\sigma^2_\delta + \sigma^2 & \sigma^2_\delta & \cdots & \sigma^2_\delta \\
\sigma^2_\delta & \sigma^2_\delta + \sigma^2 & \cdots & \sigma^2_\delta \\
\vdots & \vdots & \ddots & \vdots \\
\sigma^2_\delta & \sigma^2_\delta & \cdots & \sigma^2_\delta + \sigma^2
\end{pmatrix}
\]

This matrix can be rewritten as:

\[
\mathbf{\Sigma}_{\text{subject}} = (\sigma^2_\delta + \sigma^2)
\begin{pmatrix}
1 & \rho & \cdots & \rho \\
\rho & 1 & \cdots & \rho \\
\vdots & \vdots & \ddots & \vdots \\
\rho & \rho & \cdots & 1
\end{pmatrix}
\]

Where:

\begin{itemize}
\item
  \(\sigma^2_\delta\) = Variance due to subject-specific random effects.
\item
  \(\sigma^2\) = Residual variance.
\item
  \(\rho = \frac{\sigma^2_\delta}{\sigma^2_\delta + \sigma^2}\) = Intra-class correlation coefficient (ICC).
\end{itemize}

Key Points:

\begin{itemize}
\item
  Compound Symmetry Structure is the product of a scalar and a correlation matrix.
\item
  The correlation between any two repeated measures from the same subject is constant (\(\rho\)).
\item
  This structure assumes equal correlation across time points, which may not hold if measurements are collected over time.
\end{itemize}

\emph{Refer to \protect\hyperlink{sec-random-intercepts-model-lmm}{Random-Intercepts Model} for a detailed discussion of compound symmetry.}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Autoregressive (AR(1)) Structure
\end{enumerate}

If repeated measures are collected over time, it may be more appropriate to assume an autoregressive correlation structure, where correlations decay as the time gap increases.

The AR(1) variance-covariance matrix is:

\[
\mathbf{\Sigma}_{\text{subject}} =
\sigma^2
\begin{pmatrix}
1 & \rho & \rho^2 & \cdots & \rho^{n_B-1} \\
\rho & 1 & \rho & \cdots & \rho^{n_B-2} \\
\rho^2 & \rho & 1 & \cdots & \rho^{n_B-3} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\rho^{n_B-1} & \rho^{n_B-2} & \rho^{n_B-3} & \cdots & 1
\end{pmatrix}
\]

Where:

\begin{itemize}
\item
  \(\sigma^2\) = Residual variance.
\item
  \(\rho\) = Autoregressive parameter (\(|\rho| < 1\)), representing the correlation between consecutive time points.
\end{itemize}

\textbf{Key Characteristics}:

\begin{itemize}
\item
  Correlations decrease exponentially as the time lag increases.
\item
  Appropriate for longitudinal data where temporal proximity influences correlation.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

In matrix notation, the mixed model can be written as:

\[
\mathbf{Y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}
\]

Where:

\begin{itemize}
\item
  \(\mathbf{Y}\) = Vector of observed responses.
\item
  \(\mathbf{X}\) = Design matrix for fixed effects (e.g., group, time, interaction).
\item
  \(\boldsymbol{\beta}\) = Vector of fixed-effect coefficients.
\item
  \(\boldsymbol{\epsilon} \sim N(0, \sigma^2 \mathbf{\Sigma})\) = Random error vector.
\item
  \(\mathbf{\Sigma}\) = Variance-covariance matrix of residuals:

  \begin{itemize}
  \tightlist
  \item
    Block diagonal structure if the covariance structure is identical for each subject.
  \item
    Within each block (subject), the structure can be compound symmetry, AR(1), or another suitable structure depending on the data.
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Choosing the Right Covariance Structure}

\begin{itemize}
\tightlist
\item
  Compound Symmetry:

  \begin{itemize}
  \tightlist
  \item
    Suitable when correlations are constant across repeated measures (e.g., in randomized controlled trials).
  \item
    Simple and interpretable but may be too restrictive for longitudinal data.
  \end{itemize}
\item
  Autoregressive (AR(1)):

  \begin{itemize}
  \tightlist
  \item
    Best when measurements are taken over equally spaced time intervals and correlations decay over time.
  \item
    Assumes stronger correlation for adjacent time points.
  \end{itemize}
\item
  Unstructured (UN):

  \begin{itemize}
  \tightlist
  \item
    Allows different variances and covariances for each time point.
  \item
    Provides maximum flexibility but requires more parameters and larger sample sizes.
  \end{itemize}
\end{itemize}

Model selection criteria (AIC, BIC, likelihood ratio tests) can help determine the most appropriate covariance structure.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{unbalanced-or-unequally-spaced-data}{%
\section{Unbalanced or Unequally Spaced Data}\label{unbalanced-or-unequally-spaced-data}}

In many real-world applications, data are \textbf{unbalanced} (i.e., different numbers of observations per subject) or \textbf{unequally spaced} over time. This is common in longitudinal studies, clinical trials, and business analytics where subjects may be observed at irregular intervals or miss certain time points.

Mixed-effects models are flexible enough to handle such data structures, especially when we carefully model the \textbf{variance-covariance structure} of repeated measurements.

Consider the following mixed-effects model:

\[
Y_{ikt} = \beta_0 + \beta_{0i} + \beta_{1} t + \beta_{1i} t + \beta_{2} t^2 + \beta_{2i} t^2 + \epsilon_{ikt}
\]

Where:

\begin{itemize}
\item
  \(Y_{ikt}\) = Response for the \(k\)-th subject in the \(i\)-th group at time \(t\).
\item
  \(i = 1, 2\) = Groups (e.g., treatment vs.~control).
\item
  \(k = 1, \dots, n_i\) = Individuals within group \(i\).
\item
  \(t = (t_1, t_2, t_3, t_4)\) = Time points (which may be unequally spaced).
\end{itemize}

Model Components:

\begin{itemize}
\tightlist
\item
  \textbf{Fixed Effects:}

  \begin{itemize}
  \tightlist
  \item
    \(\beta_0\) = Overall intercept (baseline).
  \item
    \(\beta_1\) = Common linear time trend.
  \item
    \(\beta_2\) = Common quadratic time trend.
  \end{itemize}
\item
  \textbf{Random Effects:}

  \begin{itemize}
  \tightlist
  \item
    \(\beta_{0i}\) = Random intercept for group \(i\) (captures group-specific baseline variation).
  \item
    \(\beta_{1i}\) = Random slope for time in group \(i\) (captures group-specific linear trends).
  \item
    \(\beta_{2i}\) = Random quadratic effect for group \(i\) (captures group-specific curvature over time).
  \end{itemize}
\item
  \textbf{Residual Error:}

  \begin{itemize}
  \tightlist
  \item
    \(\epsilon_{ikt} \sim N(0, \sigma^2)\) = Measurement error, assumed independent of the random effects.
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{variance-covariance-structure-power-model}{%
\subsection{Variance-Covariance Structure: Power Model}\label{variance-covariance-structure-power-model}}

Since observations are taken at unequally spaced time points, we cannot rely on simple structures like compound symmetry or AR(1). Instead, we use a power covariance model, which allows the correlation to depend on the distance between time points.

The variance-covariance matrix of the repeated measurements for subject \(k\) in group \(i\) is:

\[
\mathbf{\Sigma}_{ik} = \sigma^2
\begin{pmatrix}
1 & \rho^{|t_2 - t_1|} & \rho^{|t_3 - t_1|} & \rho^{|t_4 - t_1|} \\
\rho^{|t_2 - t_1|} & 1 & \rho^{|t_3 - t_2|} & \rho^{|t_4 - t_2|} \\
\rho^{|t_3 - t_1|} & \rho^{|t_3 - t_2|} & 1 & \rho^{|t_4 - t_3|} \\
\rho^{|t_4 - t_1|} & \rho^{|t_4 - t_2|} & \rho^{|t_4 - t_3|} & 1
\end{pmatrix}
\]

Where:

\begin{itemize}
\item
  \(\sigma^2\) = Residual variance.
\item
  \(\rho\) = Correlation parameter (\(0 < |\rho| < 1\)), controlling how correlation decays with increasing time gaps.
\item
  \(|t_j - t_i|\) = Absolute time difference between measurements at times \(t_i\) and \(t_j\).
\end{itemize}

\textbf{Key Characteristics}:

\begin{itemize}
\tightlist
\item
  The correlation between observations decreases as the time difference increases, similar to AR(1), but flexible enough to handle unequal time intervals.
\item
  This structure is sometimes referred to as a continuous-time autoregressive model or power covariance model.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

After fitting the full model, we can evaluate whether all terms are necessary, focusing on the random effects:

\begin{itemize}
\item
  \(\beta_{0i}\) (Random Intercepts):\\
  Is there significant baseline variability between groups?
\item
  \(\beta_{1i}\) (Random Slopes for Time):\\
  Do groups exhibit different linear trends over time?
\item
  \(\beta_{2i}\) (Random Quadratic Terms):\\
  Is there group-specific curvature in the response over time?
\end{itemize}

Model Comparison Approach:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Fit the Full Model:}\\
  Includes all random effects.
\item
  \textbf{Fit Reduced Models:}\\
  Systematically remove random effects (e.g., quadratic terms) to create simpler models.
\item
  \textbf{Compare Models Using:}

  \begin{itemize}
  \item
    \textbf{Likelihood Ratio Tests (LRT):}\\
    Test whether the more complex model significantly improves fit.
  \item
    \textbf{Information Criteria (AIC, BIC):}\\
    Lower values indicate a better trade-off between fit and complexity.
  \end{itemize}
\item
  \textbf{Assess Random Effects:}

  \begin{itemize}
  \tightlist
  \item
    Use the \textbf{exactRLRT} test to determine if random effects are significant.
  \item
    Check variance estimates: if the variance of a random effect is near zero, it may not be necessary.
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

In matrix notation, the model can be written as:

\[
\mathbf{Y} = \mathbf{X} \boldsymbol{\beta} + \mathbf{Z} \mathbf{b} + \boldsymbol{\epsilon}
\]

Where:

\begin{itemize}
\item
  \(\mathbf{Y}\) = Vector of observed responses.
\item
  \(\mathbf{X}\) = Design matrix for fixed effects (intercept, time, time, and group interactions).
\item
  \(\boldsymbol{\beta}\) = Vector of fixed-effect coefficients.
\item
  \(\mathbf{Z}\) = Design matrix for random effects (random intercepts, slopes, etc.).
\item
  \(\mathbf{b} \sim N(0, \mathbf{G})\) = Vector of random effects with covariance matrix \(\mathbf{G}\).
\item
  \(\boldsymbol{\epsilon} \sim N(0, \mathbf{R})\) = Vector of residual errors with covariance matrix \(\mathbf{R}\), where \(\mathbf{R}\) follows the power covariance structure.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{application-mixed-models-in-practice}{%
\section{Application: Mixed Models in Practice}\label{application-mixed-models-in-practice}}

Several R packages are available for fitting mixed-effects models, each with unique strengths:

\begin{itemize}
\tightlist
\item
  \texttt{nlme}

  \begin{itemize}
  \tightlist
  \item
    Supports nested and crossed random effects.
  \item
    Flexible for complex covariance structures.
  \item
    Less intuitive syntax compared to \texttt{lme4}.
  \end{itemize}
\item
  \texttt{lme4}

  \begin{itemize}
  \tightlist
  \item
    Computationally efficient and widely used.
  \item
    User-friendly formula syntax.
  \item
    Can handle non-normal responses (e.g., GLMMs).
  \item
    For detailed documentation, refer to \citet{bates2014fittinglinearmixedeffectsmodels}.
  \end{itemize}
\item
  Others:

  \begin{itemize}
  \tightlist
  \item
    Bayesian Mixed Models: \texttt{MCMCglmm}, \texttt{brms}.
  \item
    Genetics/Plant Breeding: \texttt{ASReml}.
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-1-pulp-brightness-analysis}{%
\subsection{Example 1: Pulp Brightness Analysis}\label{example-1-pulp-brightness-analysis}}

\hypertarget{model-specification-1}{%
\subsubsection{Model Specification}\label{model-specification-1}}

We start with a \protect\hyperlink{sec-random-intercepts-model-lmm}{random-intercepts model} for pulp brightness:

\[
y_{ij} = \mu + \alpha_i + \epsilon_{ij}
\]

Where:

\begin{itemize}
\item
  \(i = 1, \dots, a\) = Groups for random effect \(\alpha_i\).
\item
  \(j = 1, \dots, n\) = Observations per group.
\item
  \(\mu\) = Overall mean brightness (fixed effect).
\item
  \(\alpha_i \sim N(0, \sigma^2_\alpha)\) = Group-specific random effect.
\item
  \(\epsilon_{ij} \sim N(0, \sigma^2_\epsilon)\) = Residual error.
\end{itemize}

This implies a compound symmetry structure, where the intraclass correlation coefficient is:

\[
\rho = \frac{\sigma^2_\alpha}{\sigma^2_\alpha + \sigma^2_\epsilon}
\]

\begin{itemize}
\tightlist
\item
  If \(\sigma^2_\alpha \to 0\): Low correlation within groups (\(\rho \to 0\)) (i.e., when factor \(a\) doesn't explain much variation).
\item
  If \(\sigma^2_\alpha \to \infty\): High correlation within groups (\(\rho \to 1\)).
\end{itemize}

\hypertarget{data-exploration-1}{%
\subsubsection{Data Exploration}\label{data-exploration-1}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(pulp, }\AttributeTok{package =} \StringTok{"faraway"}\NormalTok{)}
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(dplyr)}

\CommentTok{\# Visualize brightness by operator}
\FunctionTok{ggplot}\NormalTok{(pulp, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ operator, }\AttributeTok{y =}\NormalTok{ bright)) }\SpecialCharTok{+}
    \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{fill =} \StringTok{"lightblue"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Pulp Brightness by Operator"}\NormalTok{,}
         \AttributeTok{x =} \StringTok{"Operator"}\NormalTok{,}
         \AttributeTok{y =} \StringTok{"Brightness"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{08-linear-mixed-models_files/figure-latex/unnamed-chunk-6-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Group{-}wise summary}
\NormalTok{pulp }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{group\_by}\NormalTok{(operator) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{summarise}\NormalTok{(}\AttributeTok{average\_brightness =} \FunctionTok{mean}\NormalTok{(bright), }\AttributeTok{.groups =} \StringTok{\textquotesingle{}drop\textquotesingle{}}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 4 x 2}
\CommentTok{\#\textgreater{}   operator average\_brightness}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{}                 \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 a                      60.2}
\CommentTok{\#\textgreater{} 2 b                      60.1}
\CommentTok{\#\textgreater{} 3 c                      60.6}
\CommentTok{\#\textgreater{} 4 d                      60.7}
\end{Highlighting}
\end{Shaded}

\hypertarget{fitting-the-mixed-model-with-lme4}{%
\subsubsection{\texorpdfstring{Fitting the Mixed Model with \texttt{lme4}}{Fitting the Mixed Model with lme4}}\label{fitting-the-mixed-model-with-lme4}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(lme4)}

\CommentTok{\# Random intercepts model: operator as a random effect}
\NormalTok{mixed\_model }\OtherTok{\textless{}{-}} \FunctionTok{lmer}\NormalTok{(bright }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ operator), }\AttributeTok{data =}\NormalTok{ pulp)}

\CommentTok{\# Model summary}
\FunctionTok{summary}\NormalTok{(mixed\_model)}
\CommentTok{\#\textgreater{} Linear mixed model fit by REML. t{-}tests use Satterthwaite\textquotesingle{}s method [}
\CommentTok{\#\textgreater{} lmerModLmerTest]}
\CommentTok{\#\textgreater{} Formula: bright \textasciitilde{} 1 + (1 | operator)}
\CommentTok{\#\textgreater{}    Data: pulp}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} REML criterion at convergence: 18.6}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Scaled residuals: }
\CommentTok{\#\textgreater{}     Min      1Q  Median      3Q     Max }
\CommentTok{\#\textgreater{} {-}1.4666 {-}0.7595 {-}0.1244  0.6281  1.6012 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Random effects:}
\CommentTok{\#\textgreater{}  Groups   Name        Variance Std.Dev.}
\CommentTok{\#\textgreater{}  operator (Intercept) 0.06808  0.2609  }
\CommentTok{\#\textgreater{}  Residual             0.10625  0.3260  }
\CommentTok{\#\textgreater{} Number of obs: 20, groups:  operator, 4}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Fixed effects:}
\CommentTok{\#\textgreater{}             Estimate Std. Error      df t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)  60.4000     0.1494  3.0000   404.2 3.34e{-}08 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}

\CommentTok{\# Fixed effects (overall mean)}
\FunctionTok{fixef}\NormalTok{(mixed\_model)}
\CommentTok{\#\textgreater{} (Intercept) }
\CommentTok{\#\textgreater{}        60.4}

\CommentTok{\# Random effects (BLUPs)}
\FunctionTok{ranef}\NormalTok{(mixed\_model)}
\CommentTok{\#\textgreater{} $operator}
\CommentTok{\#\textgreater{}   (Intercept)}
\CommentTok{\#\textgreater{} a  {-}0.1219403}
\CommentTok{\#\textgreater{} b  {-}0.2591231}
\CommentTok{\#\textgreater{} c   0.1676679}
\CommentTok{\#\textgreater{} d   0.2133955}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} with conditional variances for "operator"}

\CommentTok{\# Variance components}
\FunctionTok{VarCorr}\NormalTok{(mixed\_model)}
\CommentTok{\#\textgreater{}  Groups   Name        Std.Dev.}
\CommentTok{\#\textgreater{}  operator (Intercept) 0.26093 }
\CommentTok{\#\textgreater{}  Residual             0.32596}
\NormalTok{re\_dat }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{VarCorr}\NormalTok{(mixed\_model))}

\CommentTok{\# Intraclass Correlation Coefficient}
\NormalTok{rho }\OtherTok{\textless{}{-}}\NormalTok{ re\_dat[}\DecValTok{1}\NormalTok{, }\StringTok{\textquotesingle{}vcov\textquotesingle{}}\NormalTok{] }\SpecialCharTok{/}\NormalTok{ (re\_dat[}\DecValTok{1}\NormalTok{, }\StringTok{\textquotesingle{}vcov\textquotesingle{}}\NormalTok{] }\SpecialCharTok{+}\NormalTok{ re\_dat[}\DecValTok{2}\NormalTok{, }\StringTok{\textquotesingle{}vcov\textquotesingle{}}\NormalTok{])}
\NormalTok{rho}
\CommentTok{\#\textgreater{} [1] 0.3905354}
\end{Highlighting}
\end{Shaded}

\hypertarget{inference-with-lmertest}{%
\subsubsection{\texorpdfstring{Inference with \texttt{lmerTest}}{Inference with lmerTest}}\label{inference-with-lmertest}}

To obtain p-values for fixed effects using Satterthwaite's approximation:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(lmerTest)}

\CommentTok{\# Model with p{-}values}
\FunctionTok{summary}\NormalTok{(}\FunctionTok{lmer}\NormalTok{(bright }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ operator), }\AttributeTok{data =}\NormalTok{ pulp))}\SpecialCharTok{$}\NormalTok{coefficients}
\CommentTok{\#\textgreater{}             Estimate Std. Error df  t value     Pr(\textgreater{}|t|)}
\CommentTok{\#\textgreater{} (Intercept)     60.4  0.1494434  3 404.1664 3.340265e{-}08}

\CommentTok{\# Confidence interval for the fixed effect}
\FunctionTok{confint}\NormalTok{(mixed\_model)[}\DecValTok{3}\NormalTok{,]}
\CommentTok{\#\textgreater{}   2.5 \%  97.5 \% }
\CommentTok{\#\textgreater{} 60.0713 60.7287}
\end{Highlighting}
\end{Shaded}

In this example, we can see that the confidence interval computed by \texttt{confint} from the \texttt{lme4} package is very close to the one computed by \texttt{confint} from the \texttt{lmerTest} package.

\hypertarget{bayesian-mixed-model-with-mcmcglmm}{%
\subsubsection{\texorpdfstring{Bayesian Mixed Model with \texttt{MCMCglmm}}{Bayesian Mixed Model with MCMCglmm}}\label{bayesian-mixed-model-with-mcmcglmm}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(MCMCglmm)}

\CommentTok{\# Bayesian mixed model}
\NormalTok{mixed\_model\_bayes }\OtherTok{\textless{}{-}} \FunctionTok{MCMCglmm}\NormalTok{(}
\NormalTok{  bright }\SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{,}
  \AttributeTok{random =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ operator,}
  \AttributeTok{data =}\NormalTok{ pulp,}
  \AttributeTok{verbose =} \ConstantTok{FALSE}
\NormalTok{)}

\CommentTok{\# Posterior summaries}
\FunctionTok{summary}\NormalTok{(mixed\_model\_bayes)}\SpecialCharTok{$}\NormalTok{solutions}
\CommentTok{\#\textgreater{}             post.mean l{-}95\% CI u{-}95\% CI eff.samp pMCMC}
\CommentTok{\#\textgreater{} (Intercept)  60.40789  60.1488 60.69595     1000 0.001}
\end{Highlighting}
\end{Shaded}

Bayesian credible intervals may differ slightly from frequentist confidence intervals due to prior assumptions.

\hypertarget{predictions}{%
\subsubsection{Predictions}\label{predictions}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Random effects predictions (BLUPs)}
\FunctionTok{ranef}\NormalTok{(mixed\_model)}\SpecialCharTok{$}\NormalTok{operator}
\CommentTok{\#\textgreater{}   (Intercept)}
\CommentTok{\#\textgreater{} a  {-}0.1219403}
\CommentTok{\#\textgreater{} b  {-}0.2591231}
\CommentTok{\#\textgreater{} c   0.1676679}
\CommentTok{\#\textgreater{} d   0.2133955}

\CommentTok{\# Predictions per operator}
\FunctionTok{fixef}\NormalTok{(mixed\_model) }\SpecialCharTok{+} \FunctionTok{ranef}\NormalTok{(mixed\_model)}\SpecialCharTok{$}\NormalTok{operator}
\CommentTok{\#\textgreater{}   (Intercept)}
\CommentTok{\#\textgreater{} a    60.27806}
\CommentTok{\#\textgreater{} b    60.14088}
\CommentTok{\#\textgreater{} c    60.56767}
\CommentTok{\#\textgreater{} d    60.61340}

\CommentTok{\# Equivalent using predict()}
\FunctionTok{predict}\NormalTok{(mixed\_model, }\AttributeTok{newdata =} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{operator =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}a\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}b\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}c\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}d\textquotesingle{}}\NormalTok{)))}
\CommentTok{\#\textgreater{}        1        2        3        4 }
\CommentTok{\#\textgreater{} 60.27806 60.14088 60.56767 60.61340}
\end{Highlighting}
\end{Shaded}

For \textbf{bootstrap confidence intervals}, use:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{bootMer}\NormalTok{(mixed\_model, }\AttributeTok{FUN =}\NormalTok{ fixef, }\AttributeTok{nsim =} \DecValTok{100}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} PARAMETRIC BOOTSTRAP}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} bootMer(x = mixed\_model, FUN = fixef, nsim = 100)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Bootstrap Statistics :}
\CommentTok{\#\textgreater{}     original        bias    std. error}
\CommentTok{\#\textgreater{} t1*     60.4 {-}0.0005452538    0.156374}
\end{Highlighting}
\end{Shaded}

\hypertarget{example-2-penicillin-yield-glmm-with-blocking}{%
\subsection{Example 2: Penicillin Yield (GLMM with Blocking)}\label{example-2-penicillin-yield-glmm-with-blocking}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(penicillin, }\AttributeTok{package =} \StringTok{"faraway"}\NormalTok{)}
\FunctionTok{library}\NormalTok{(ggplot2)}

\CommentTok{\# Visualize yield by treatment and blend}
\FunctionTok{ggplot}\NormalTok{(penicillin, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ yield, }\AttributeTok{x =}\NormalTok{ treat, }\AttributeTok{shape =}\NormalTok{ blend, }\AttributeTok{color =}\NormalTok{ blend)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{size =} \DecValTok{3}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Penicillin Yield by Treatment and Blend"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{08-linear-mixed-models_files/figure-latex/unnamed-chunk-12-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Mixed model: blend as random effect, treatment as fixed}
\NormalTok{mixed\_model }\OtherTok{\textless{}{-}} \FunctionTok{lmer}\NormalTok{(yield }\SpecialCharTok{\textasciitilde{}}\NormalTok{ treat }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ blend), }\AttributeTok{data =}\NormalTok{ penicillin)}
\FunctionTok{summary}\NormalTok{(mixed\_model)}
\CommentTok{\#\textgreater{} Linear mixed model fit by REML. t{-}tests use Satterthwaite\textquotesingle{}s method [}
\CommentTok{\#\textgreater{} lmerModLmerTest]}
\CommentTok{\#\textgreater{} Formula: yield \textasciitilde{} treat + (1 | blend)}
\CommentTok{\#\textgreater{}    Data: penicillin}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} REML criterion at convergence: 103.8}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Scaled residuals: }
\CommentTok{\#\textgreater{}     Min      1Q  Median      3Q     Max }
\CommentTok{\#\textgreater{} {-}1.4152 {-}0.5017 {-}0.1644  0.6830  1.2836 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Random effects:}
\CommentTok{\#\textgreater{}  Groups   Name        Variance Std.Dev.}
\CommentTok{\#\textgreater{}  blend    (Intercept) 11.79    3.434   }
\CommentTok{\#\textgreater{}  Residual             18.83    4.340   }
\CommentTok{\#\textgreater{} Number of obs: 20, groups:  blend, 5}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Fixed effects:}
\CommentTok{\#\textgreater{}             Estimate Std. Error     df t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)   84.000      2.475 11.075  33.941 1.51e{-}12 ***}
\CommentTok{\#\textgreater{} treatB         1.000      2.745 12.000   0.364   0.7219    }
\CommentTok{\#\textgreater{} treatC         5.000      2.745 12.000   1.822   0.0935 .  }
\CommentTok{\#\textgreater{} treatD         2.000      2.745 12.000   0.729   0.4802    }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Correlation of Fixed Effects:}
\CommentTok{\#\textgreater{}        (Intr) treatB treatC}
\CommentTok{\#\textgreater{} treatB {-}0.555              }
\CommentTok{\#\textgreater{} treatC {-}0.555  0.500       }
\CommentTok{\#\textgreater{} treatD {-}0.555  0.500  0.500}

\CommentTok{\# BLUPs for each blend}
\FunctionTok{ranef}\NormalTok{(mixed\_model)}\SpecialCharTok{$}\NormalTok{blend}
\CommentTok{\#\textgreater{}        (Intercept)}
\CommentTok{\#\textgreater{} Blend1   4.2878788}
\CommentTok{\#\textgreater{} Blend2  {-}2.1439394}
\CommentTok{\#\textgreater{} Blend3  {-}0.7146465}
\CommentTok{\#\textgreater{} Blend4   1.4292929}
\CommentTok{\#\textgreater{} Blend5  {-}2.8585859}
\end{Highlighting}
\end{Shaded}

Testing for Treatment Effect

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# ANOVA for fixed effects}
\FunctionTok{anova}\NormalTok{(mixed\_model)}
\CommentTok{\#\textgreater{} Type III Analysis of Variance Table with Satterthwaite\textquotesingle{}s method}
\CommentTok{\#\textgreater{}       Sum Sq Mean Sq NumDF DenDF F value Pr(\textgreater{}F)}
\CommentTok{\#\textgreater{} treat     70  23.333     3    12  1.2389 0.3387}
\end{Highlighting}
\end{Shaded}

Since the \(p\)-value \(> .05\), we fail to reject the null hypothesis (no treatment effect).

Model Comparison with Kenward-Roger Approximation

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(pbkrtest)}

\CommentTok{\# Full model vs. null model}
\NormalTok{full\_model }\OtherTok{\textless{}{-}}
    \FunctionTok{lmer}\NormalTok{(yield }\SpecialCharTok{\textasciitilde{}}\NormalTok{ treat }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ blend), penicillin, }\AttributeTok{REML =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{null\_model }\OtherTok{\textless{}{-}}
    \FunctionTok{lmer}\NormalTok{(yield }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ blend), penicillin, }\AttributeTok{REML =} \ConstantTok{FALSE}\NormalTok{)}

\CommentTok{\# Kenward{-}Roger approximation}
\FunctionTok{KRmodcomp}\NormalTok{(full\_model, null\_model)}
\CommentTok{\#\textgreater{} large : yield \textasciitilde{} treat + (1 | blend)}
\CommentTok{\#\textgreater{} small : yield \textasciitilde{} 1 + (1 | blend)}
\CommentTok{\#\textgreater{}          stat     ndf     ddf F.scaling p.value}
\CommentTok{\#\textgreater{} Ftest  1.2389  3.0000 12.0000         1  0.3387}
\end{Highlighting}
\end{Shaded}

The results are consistent with the earlier ANOVA: \textbf{no significant treatment effect}.

\hypertarget{example-3-growth-in-rats-over-time}{%
\subsection{Example 3: Growth in Rats Over Time}\label{example-3-growth-in-rats-over-time}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rats }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}
    \StringTok{"images/rats.dat"}\NormalTok{,}
    \AttributeTok{header =} \ConstantTok{FALSE}\NormalTok{,}
    \AttributeTok{sep =} \StringTok{\textquotesingle{} \textquotesingle{}}\NormalTok{,}
    \AttributeTok{col.names =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Treatment\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}rat\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}y\textquotesingle{}}\NormalTok{)}
\NormalTok{)}

\CommentTok{\# Log{-}transformed time variable}
\NormalTok{rats}\SpecialCharTok{$}\NormalTok{t }\OtherTok{\textless{}{-}} \FunctionTok{log}\NormalTok{(}\DecValTok{1} \SpecialCharTok{+}\NormalTok{ (rats}\SpecialCharTok{$}\NormalTok{age }\SpecialCharTok{{-}} \DecValTok{45}\NormalTok{) }\SpecialCharTok{/} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Model Fitting

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Treatment as fixed effect, random intercepts for rats}
\NormalTok{rat\_model }\OtherTok{\textless{}{-}} \FunctionTok{lmer}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ t}\SpecialCharTok{:}\NormalTok{Treatment }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ rat), }\AttributeTok{data =}\NormalTok{ rats)}
\FunctionTok{summary}\NormalTok{(rat\_model)}
\CommentTok{\#\textgreater{} Linear mixed model fit by REML. t{-}tests use Satterthwaite\textquotesingle{}s method [}
\CommentTok{\#\textgreater{} lmerModLmerTest]}
\CommentTok{\#\textgreater{} Formula: y \textasciitilde{} t:Treatment + (1 | rat)}
\CommentTok{\#\textgreater{}    Data: rats}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} REML criterion at convergence: 932.4}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Scaled residuals: }
\CommentTok{\#\textgreater{}      Min       1Q   Median       3Q      Max }
\CommentTok{\#\textgreater{} {-}2.25574 {-}0.65898 {-}0.01163  0.58356  2.88309 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Random effects:}
\CommentTok{\#\textgreater{}  Groups   Name        Variance Std.Dev.}
\CommentTok{\#\textgreater{}  rat      (Intercept) 3.565    1.888   }
\CommentTok{\#\textgreater{}  Residual             1.445    1.202   }
\CommentTok{\#\textgreater{} Number of obs: 252, groups:  rat, 50}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Fixed effects:}
\CommentTok{\#\textgreater{}                Estimate Std. Error       df t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)     68.6074     0.3312  89.0275  207.13   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} t:Treatmentcon   7.3138     0.2808 247.2762   26.05   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} t:Treatmenthig   6.8711     0.2276 247.7097   30.19   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} t:Treatmentlow   7.5069     0.2252 247.5196   33.34   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Correlation of Fixed Effects:}
\CommentTok{\#\textgreater{}             (Intr) t:Trtmntc t:Trtmnth}
\CommentTok{\#\textgreater{} t:Tretmntcn {-}0.327                    }
\CommentTok{\#\textgreater{} t:Tretmnthg {-}0.340  0.111             }
\CommentTok{\#\textgreater{} t:Tretmntlw {-}0.351  0.115     0.119}
\FunctionTok{anova}\NormalTok{(rat\_model)}
\CommentTok{\#\textgreater{} Type III Analysis of Variance Table with Satterthwaite\textquotesingle{}s method}
\CommentTok{\#\textgreater{}             Sum Sq Mean Sq NumDF  DenDF F value    Pr(\textgreater{}F)    }
\CommentTok{\#\textgreater{} t:Treatment 3181.9  1060.6     3 223.21  734.11 \textless{} 2.2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

Since the \textbf{p-value is significant}, we conclude that the \textbf{treatment effect varies over time}.

\hypertarget{example-4-tree-water-use-agridat}{%
\subsection{Example 4: Tree Water Use (Agridat)}\label{example-4-tree-water-use-agridat}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(agridat)}
\NormalTok{dat }\OtherTok{\textless{}{-}}\NormalTok{ harris.wateruse}

\CommentTok{\# Visualizing water use by species and age}
\FunctionTok{library}\NormalTok{(latticeExtra)}
\FunctionTok{useOuterStrips}\NormalTok{(}
  \FunctionTok{xyplot}\NormalTok{(water }\SpecialCharTok{\textasciitilde{}}\NormalTok{ day }\SpecialCharTok{|}\NormalTok{ species }\SpecialCharTok{*}\NormalTok{ age, }
\NormalTok{         dat, }\AttributeTok{group =}\NormalTok{ tree,}
         \AttributeTok{type =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}p\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}smooth\textquotesingle{}}\NormalTok{), }
         \AttributeTok{main =} \StringTok{"harris.wateruse 2 species, 2 ages (10 trees each)"}\NormalTok{,}
         \AttributeTok{as.table =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{08-linear-mixed-models_files/figure-latex/unnamed-chunk-17-1} \end{center}

Remove outlier

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(dat, day}\SpecialCharTok{!=}\DecValTok{268}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Plot between water use and day for one age and species group

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{xyplot}\NormalTok{(}
\NormalTok{    water }\SpecialCharTok{\textasciitilde{}}\NormalTok{ day }\SpecialCharTok{|}\NormalTok{ tree,}
\NormalTok{    dat,}
    \AttributeTok{subset   =}\NormalTok{ age }\SpecialCharTok{==} \StringTok{"A2"} \SpecialCharTok{\&}\NormalTok{ species }\SpecialCharTok{==} \StringTok{"S2"}\NormalTok{,}
    \AttributeTok{as.table =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{type     =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}p\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}smooth\textquotesingle{}}\NormalTok{),}
    \AttributeTok{ylab     =} \StringTok{"Water use profiles of individual trees"}\NormalTok{,}
    \AttributeTok{main     =} \StringTok{"harris.wateruse (Age 2, Species 2)"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{08-linear-mixed-models_files/figure-latex/unnamed-chunk-19-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Rescale day for nicer output, and convergence issues}
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{transform}\NormalTok{(dat, }\AttributeTok{ti =}\NormalTok{ day }\SpecialCharTok{/} \DecValTok{100}\NormalTok{)}
\CommentTok{\# add quadratic term}
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{transform}\NormalTok{(dat, }\AttributeTok{ti2 =}\NormalTok{ ti }\SpecialCharTok{*}\NormalTok{ ti)}

\CommentTok{\# Start with a subgroup: age 2, species 2}
\NormalTok{d22 }\OtherTok{\textless{}{-}} \FunctionTok{droplevels}\NormalTok{(}\FunctionTok{subset}\NormalTok{(dat, age }\SpecialCharTok{==} \StringTok{"A2"} \SpecialCharTok{\&}\NormalTok{ species }\SpecialCharTok{==} \StringTok{"S2"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Fitting with \texttt{nlme} using \texttt{lme}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(nlme)}

\DocumentationTok{\#\# We use pdDiag() to get uncorrelated random effects}
\NormalTok{m1n }\OtherTok{\textless{}{-}} \FunctionTok{lme}\NormalTok{(}
\NormalTok{    water }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ ti }\SpecialCharTok{+}\NormalTok{ ti2,}
    \CommentTok{\#intercept, time and time{-}squared = fixed effects}
    \AttributeTok{data =}\NormalTok{ d22,}
    \AttributeTok{na.action =}\NormalTok{ na.omit,}
    \AttributeTok{random =} \FunctionTok{list}\NormalTok{(}\AttributeTok{tree =} \FunctionTok{pdDiag}\NormalTok{(}\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ ti }\SpecialCharTok{+}\NormalTok{ ti2)) }
    \CommentTok{\# random intercept, time }
    \CommentTok{\# and time squared per tree = random effects}
\NormalTok{)}

\CommentTok{\# for all trees}
\CommentTok{\# m1n \textless{}{-} lme(}
\CommentTok{\#   water \textasciitilde{} 1 + ti + ti2,}
\CommentTok{\#   random = list(tree = pdDiag(\textasciitilde{} 1 + ti + ti2)),}
\CommentTok{\#   data = dat,}
\CommentTok{\#   na.action = na.omit}
\CommentTok{\# )}

\FunctionTok{summary}\NormalTok{(m1n)}
\CommentTok{\#\textgreater{} Linear mixed{-}effects model fit by REML}
\CommentTok{\#\textgreater{}   Data: d22 }
\CommentTok{\#\textgreater{}        AIC     BIC    logLik}
\CommentTok{\#\textgreater{}   276.5142 300.761 {-}131.2571}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Random effects:}
\CommentTok{\#\textgreater{}  Formula: \textasciitilde{}1 + ti + ti2 | tree}
\CommentTok{\#\textgreater{}  Structure: Diagonal}
\CommentTok{\#\textgreater{}         (Intercept)           ti          ti2  Residual}
\CommentTok{\#\textgreater{} StdDev:   0.5187869 1.631223e{-}05 4.374982e{-}06 0.3836614}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Fixed effects:  water \textasciitilde{} 1 + ti + ti2 }
\CommentTok{\#\textgreater{}                  Value Std.Error  DF   t{-}value p{-}value}
\CommentTok{\#\textgreater{} (Intercept) {-}10.798799 0.8814666 227 {-}12.25094       0}
\CommentTok{\#\textgreater{} ti           12.346704 0.7827112 227  15.77428       0}
\CommentTok{\#\textgreater{} ti2          {-}2.838503 0.1720614 227 {-}16.49704       0}
\CommentTok{\#\textgreater{}  Correlation: }
\CommentTok{\#\textgreater{}     (Intr) ti    }
\CommentTok{\#\textgreater{} ti  {-}0.979       }
\CommentTok{\#\textgreater{} ti2  0.970 {-}0.997}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Standardized Within{-}Group Residuals:}
\CommentTok{\#\textgreater{}         Min          Q1         Med          Q3         Max }
\CommentTok{\#\textgreater{} {-}3.07588246 {-}0.58531056  0.01210209  0.65402695  3.88777402 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Observations: 239}
\CommentTok{\#\textgreater{} Number of Groups: 10}

\FunctionTok{ranef}\NormalTok{(m1n)}
\CommentTok{\#\textgreater{}     (Intercept)            ti           ti2}
\CommentTok{\#\textgreater{} T04   0.1985796  2.070606e{-}09  6.397103e{-}10}
\CommentTok{\#\textgreater{} T05   0.3492827  3.199664e{-}10 {-}6.211457e{-}11}
\CommentTok{\#\textgreater{} T19  {-}0.1978989 {-}9.879555e{-}10 {-}2.514502e{-}10}
\CommentTok{\#\textgreater{} T23   0.4519003 {-}4.206418e{-}10 {-}3.094113e{-}10}
\CommentTok{\#\textgreater{} T38  {-}0.6457494 {-}2.069198e{-}09 {-}4.227912e{-}10}
\CommentTok{\#\textgreater{} T40   0.3739432  4.199061e{-}10 {-}3.260161e{-}11}
\CommentTok{\#\textgreater{} T49   0.8620648  1.160387e{-}09 {-}6.925457e{-}12}
\CommentTok{\#\textgreater{} T53  {-}0.5655049 {-}1.064849e{-}09 {-}5.870462e{-}11}
\CommentTok{\#\textgreater{} T67  {-}0.4394623 {-}4.482549e{-}10  2.752922e{-}11}
\CommentTok{\#\textgreater{} T71  {-}0.3871552  1.020034e{-}09  4.767595e{-}10}

\FunctionTok{fixef}\NormalTok{(m1n)}
\CommentTok{\#\textgreater{} (Intercept)          ti         ti2 }
\CommentTok{\#\textgreater{}  {-}10.798799   12.346704   {-}2.838503}
\end{Highlighting}
\end{Shaded}

Fitting with \texttt{lme4} using \texttt{lmer}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(lme4)}

\NormalTok{m1lmer }\OtherTok{\textless{}{-}}
    \FunctionTok{lmer}\NormalTok{(water }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ ti }\SpecialCharTok{+}\NormalTok{ ti2 }\SpecialCharTok{+}\NormalTok{ (ti }\SpecialCharTok{+}\NormalTok{ ti2 }\SpecialCharTok{||}
\NormalTok{                                     tree),}
         \AttributeTok{data =}\NormalTok{ d22,}
         \AttributeTok{na.action =}\NormalTok{ na.omit)}

\CommentTok{\# for all trees}
\CommentTok{\# m1lmer \textless{}{-} lmer(water \textasciitilde{} 1 + ti + ti2 + (ti + ti2 || tree),}
\CommentTok{\#                data = dat, na.action = na.omit)}

\CommentTok{\# summary(m1lmer)}
\FunctionTok{ranef}\NormalTok{(m1lmer)}
\CommentTok{\#\textgreater{} $tree}
\CommentTok{\#\textgreater{}     (Intercept) ti ti2}
\CommentTok{\#\textgreater{} T04   0.1985796  0   0}
\CommentTok{\#\textgreater{} T05   0.3492827  0   0}
\CommentTok{\#\textgreater{} T19  {-}0.1978989  0   0}
\CommentTok{\#\textgreater{} T23   0.4519003  0   0}
\CommentTok{\#\textgreater{} T38  {-}0.6457494  0   0}
\CommentTok{\#\textgreater{} T40   0.3739432  0   0}
\CommentTok{\#\textgreater{} T49   0.8620648  0   0}
\CommentTok{\#\textgreater{} T53  {-}0.5655049  0   0}
\CommentTok{\#\textgreater{} T67  {-}0.4394623  0   0}
\CommentTok{\#\textgreater{} T71  {-}0.3871552  0   0}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} with conditional variances for "tree"}

\FunctionTok{fixef}\NormalTok{(m1lmer)}
\CommentTok{\#\textgreater{} (Intercept)          ti         ti2 }
\CommentTok{\#\textgreater{}  {-}10.798799   12.346704   {-}2.838503}
\end{Highlighting}
\end{Shaded}

Notes:

\begin{itemize}
\tightlist
\item
  \texttt{\textbar{}\textbar{}} double pipes = uncorrelated random effects
\item
  To remove the intercept term:

  \begin{itemize}
  \tightlist
  \item
    \texttt{(0+ti\textbar{}tree)}
  \item
    \texttt{(ti-1\textbar{}tree)}
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1l }\OtherTok{\textless{}{-}}
    \FunctionTok{lmer}\NormalTok{(water }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ ti }\SpecialCharTok{+}\NormalTok{ ti2 }
         \SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ tree) }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{0} \SpecialCharTok{+}\NormalTok{ ti }\SpecialCharTok{|}\NormalTok{ tree) }
         \SpecialCharTok{+}\NormalTok{ (}\DecValTok{0} \SpecialCharTok{+}\NormalTok{ ti2 }\SpecialCharTok{|}\NormalTok{ tree), }\AttributeTok{data =}\NormalTok{ d22)}
\FunctionTok{ranef}\NormalTok{(m1l)}
\CommentTok{\#\textgreater{} $tree}
\CommentTok{\#\textgreater{}     (Intercept) ti ti2}
\CommentTok{\#\textgreater{} T04   0.1985796  0   0}
\CommentTok{\#\textgreater{} T05   0.3492827  0   0}
\CommentTok{\#\textgreater{} T19  {-}0.1978989  0   0}
\CommentTok{\#\textgreater{} T23   0.4519003  0   0}
\CommentTok{\#\textgreater{} T38  {-}0.6457494  0   0}
\CommentTok{\#\textgreater{} T40   0.3739432  0   0}
\CommentTok{\#\textgreater{} T49   0.8620648  0   0}
\CommentTok{\#\textgreater{} T53  {-}0.5655049  0   0}
\CommentTok{\#\textgreater{} T67  {-}0.4394623  0   0}
\CommentTok{\#\textgreater{} T71  {-}0.3871552  0   0}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} with conditional variances for "tree"}
\FunctionTok{fixef}\NormalTok{(m1l)}
\CommentTok{\#\textgreater{} (Intercept)          ti         ti2 }
\CommentTok{\#\textgreater{}  {-}10.798799   12.346704   {-}2.838503}
\end{Highlighting}
\end{Shaded}

Adding Correlation Structure

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m2n }\OtherTok{\textless{}{-}} \FunctionTok{lme}\NormalTok{(}
\NormalTok{    water }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ ti }\SpecialCharTok{+}\NormalTok{ ti2,}
    \AttributeTok{data =}\NormalTok{ d22,}
    \AttributeTok{random =} \SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{|}\NormalTok{ tree,}
    \AttributeTok{cor =} \FunctionTok{corExp}\NormalTok{(}\AttributeTok{form =}  \SpecialCharTok{\textasciitilde{}}\NormalTok{ day }\SpecialCharTok{|}\NormalTok{ tree),}
    \AttributeTok{na.action =}\NormalTok{ na.omit}
\NormalTok{)}

\CommentTok{\# for all trees}
\CommentTok{\# m2n \textless{}{-} lme(}
\CommentTok{\#   water \textasciitilde{} 1 + ti + ti2,}
\CommentTok{\#   random = \textasciitilde{} 1 | tree,}
\CommentTok{\#   cor = corExp(form = \textasciitilde{} day | tree),}
\CommentTok{\#   data = dat,}
\CommentTok{\#   na.action = na.omit}
\CommentTok{\# )}

\FunctionTok{summary}\NormalTok{(m2n)}
\CommentTok{\#\textgreater{} Linear mixed{-}effects model fit by REML}
\CommentTok{\#\textgreater{}   Data: d22 }
\CommentTok{\#\textgreater{}        AIC      BIC   logLik}
\CommentTok{\#\textgreater{}   263.3081 284.0911 {-}125.654}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Random effects:}
\CommentTok{\#\textgreater{}  Formula: \textasciitilde{}1 | tree}
\CommentTok{\#\textgreater{}         (Intercept)  Residual}
\CommentTok{\#\textgreater{} StdDev:   0.5154042 0.3925777}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Correlation Structure: Exponential spatial correlation}
\CommentTok{\#\textgreater{}  Formula: \textasciitilde{}day | tree }
\CommentTok{\#\textgreater{}  Parameter estimate(s):}
\CommentTok{\#\textgreater{}    range }
\CommentTok{\#\textgreater{} 3.794624 }
\CommentTok{\#\textgreater{} Fixed effects:  water \textasciitilde{} 1 + ti + ti2 }
\CommentTok{\#\textgreater{}                  Value Std.Error  DF   t{-}value p{-}value}
\CommentTok{\#\textgreater{} (Intercept) {-}11.223310 1.0988725 227 {-}10.21348       0}
\CommentTok{\#\textgreater{} ti           12.712094 0.9794235 227  12.97916       0}
\CommentTok{\#\textgreater{} ti2          {-}2.913682 0.2148551 227 {-}13.56115       0}
\CommentTok{\#\textgreater{}  Correlation: }
\CommentTok{\#\textgreater{}     (Intr) ti    }
\CommentTok{\#\textgreater{} ti  {-}0.985       }
\CommentTok{\#\textgreater{} ti2  0.976 {-}0.997}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Standardized Within{-}Group Residuals:}
\CommentTok{\#\textgreater{}         Min          Q1         Med          Q3         Max }
\CommentTok{\#\textgreater{} {-}3.04861039 {-}0.55703950  0.00278101  0.62558762  3.80676991 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Observations: 239}
\CommentTok{\#\textgreater{} Number of Groups: 10}
\FunctionTok{ranef}\NormalTok{(m2n)}
\CommentTok{\#\textgreater{}     (Intercept)}
\CommentTok{\#\textgreater{} T04   0.1929971}
\CommentTok{\#\textgreater{} T05   0.3424631}
\CommentTok{\#\textgreater{} T19  {-}0.1988495}
\CommentTok{\#\textgreater{} T23   0.4538660}
\CommentTok{\#\textgreater{} T38  {-}0.6413664}
\CommentTok{\#\textgreater{} T40   0.3769378}
\CommentTok{\#\textgreater{} T49   0.8410043}
\CommentTok{\#\textgreater{} T53  {-}0.5528236}
\CommentTok{\#\textgreater{} T67  {-}0.4452930}
\CommentTok{\#\textgreater{} T71  {-}0.3689358}
\FunctionTok{fixef}\NormalTok{(m2n)}
\CommentTok{\#\textgreater{} (Intercept)          ti         ti2 }
\CommentTok{\#\textgreater{}  {-}11.223310   12.712094   {-}2.913682}
\end{Highlighting}
\end{Shaded}

\textbf{Key Takeaways}

\begin{itemize}
\item
  \texttt{lme4} is preferred for general mixed models due to efficiency and simplicity.
\item
  \texttt{nlme} is powerful for complex correlation structures and nested designs.
\item
  Bayesian models (e.g., \texttt{MCMCglmm}) offer flexible inference under uncertainty.
\item
  Always consider model diagnostics and random effects structure carefully.
\end{itemize}

\hypertarget{sec-nonlinear-and-generalized-linear-mixed-models}{%
\chapter{Nonlinear and Generalized Linear Mixed Models}\label{sec-nonlinear-and-generalized-linear-mixed-models}}

Nonlinear Mixed Models (NLMMs) and Generalized Linear Mixed Models (GLMMs) extend traditional models by incorporating both fixed effects and random effects, allowing for greater flexibility in modeling complex data structures.

\begin{itemize}
\tightlist
\item
  NLMMs extend nonlinear models to include both fixed and random effects, accommodating nonlinear relationships in the data.
\item
  GLMMs extend generalized linear models to include random effects, allowing for correlated data and non-constant variance structures.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{sec-nonlinear-mixed-models}{%
\section{Nonlinear Mixed Models}\label{sec-nonlinear-mixed-models}}

A general form of a nonlinear mixed model is:

\[
Y_{ij} = f(\mathbf{x}_{ij}, \boldsymbol{\theta}, \boldsymbol{\alpha}_i) + \epsilon_{ij}
\]

for the \(j\)-th response from the \(i\)-th cluster (or subject), where:

\begin{itemize}
\tightlist
\item
  \(i = 1, \ldots, n\) (number of clusters/subjects),
\item
  \(j = 1, \ldots, n_i\) (number of observations per cluster),
\item
  \(\boldsymbol{\theta}\) represents the fixed effects,
\item
  \(\boldsymbol{\alpha}_i\) are the random effects for cluster \(i\),
\item
  \(\mathbf{x}_{ij}\) are the regressors or design variables,
\item
  \(f(\cdot)\) is a nonlinear mean response function,
\item
  \(\epsilon_{ij}\) represents the residual error, often assumed to be normally distributed with mean 0.
\end{itemize}

NLMMs are particularly useful when the relationship between predictors and the response cannot be adequately captured by a linear model.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{sec-generalized-linear-mixed-models}{%
\section{Generalized Linear Mixed Models}\label{sec-generalized-linear-mixed-models}}

GLMMs extend GLMs by incorporating random effects, which allows for modeling data with hierarchical or clustered structures.

The conditional distribution of \(y_i\) given the random effects \(\boldsymbol{\alpha}_i\) is:

\[
y_i \mid \boldsymbol{\alpha}_i \sim \text{independent } f(y_i \mid \boldsymbol{\alpha})
\]

where \(f(y_i \mid \boldsymbol{\alpha})\) belongs to the exponential family of distributions:

\[
f(y_i \mid \boldsymbol{\alpha}) = \exp \left( \frac{y_i \theta_i - b(\theta_i)}{a(\phi)} - c(y_i, \phi) \right)
\]

\begin{itemize}
\tightlist
\item
  \(\theta_i\) is the canonical parameter,
\item
  \(a(\phi)\) is a dispersion parameter,
\item
  \(b(\theta_i)\) and \(c(y_i, \phi)\) are specific functions defining the exponential family.
\end{itemize}

The conditional mean of \(y_i\) is related to \(\theta_i\) by:

\[
\mu_i = \frac{\partial b(\theta_i)}{\partial \theta_i}
\]

Applying a link function \(g(\cdot)\), we relate the mean response to both fixed and random effects:

\[
\begin{aligned}
E(y_i \mid \boldsymbol{\alpha}) &= \mu_i \\
g(\mu_i) &= \mathbf{x}_i' \boldsymbol{\beta} + \mathbf{z}_i' \boldsymbol{\alpha}
\end{aligned}
\]

\begin{itemize}
\tightlist
\item
  \(g(\cdot)\) is a known link function,
\item
  \(\mathbf{x}_i\) and \(\mathbf{z}_i\) are design matrices for fixed and random effects, respectively,
\item
  \(\boldsymbol{\beta}\) represents fixed effects, and \(\boldsymbol{\alpha}\) represents random effects.
\end{itemize}

We also specify the distribution of the random effects:

\[
\boldsymbol{\alpha} \sim f(\boldsymbol{\alpha})
\]

This distribution is often assumed to be multivariate normal (Law of large Number applies to fixed effects) but can be chosen (subjectively) based on the context.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{relationship-between-nlmms-and-glmms}{%
\section{Relationship Between NLMMs and GLMMs}\label{relationship-between-nlmms-and-glmms}}

NLMMs can be viewed as a special case of GLMMs when the inverse link function corresponds to a nonlinear transformation of the linear predictor:

\[
\begin{aligned}
\mathbf{Y}_i &= \mathbf{f}(\mathbf{x}_i, \boldsymbol{\theta}, \boldsymbol{\alpha}_i) + \boldsymbol{\epsilon}_i \\
\mathbf{Y}_i &= g^{-1}(\mathbf{x}_i' \boldsymbol{\beta} + \mathbf{z}_i' \boldsymbol{\alpha}_i) + \boldsymbol{\epsilon}_i
\end{aligned}
\]

Here, \(g^{-1}(\cdot)\) represents the inverse link function, corresponding to a nonlinear transformation of the fixed and random effects.

Note:\\
We can't derive the analytical formulation of the marginal distribution because nonlinear combination of normal variables is not normally distributed, even in the case of additive error (\(\epsilon_i\)) and random effects (\(\alpha_i\)) are both normal.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{marginal-properties-of-glmms}{%
\section{Marginal Properties of GLMMs}\label{marginal-properties-of-glmms}}

\hypertarget{marginal-mean-of-y_i}{%
\subsection{\texorpdfstring{Marginal Mean of \(y_i\)}{Marginal Mean of y\_i}}\label{marginal-mean-of-y_i}}

The marginal mean is obtained by integrating over the distribution of the random effects:

\[
E(y_i) = E_{\boldsymbol{\alpha}}(E(y_i \mid \boldsymbol{\alpha})) = E_{\boldsymbol{\alpha}}(\mu_i) = E\left(g^{-1}(\mathbf{x}_i' \boldsymbol{\beta} + \mathbf{z}_i' \boldsymbol{\alpha})\right)
\]

Since \(g^{-1}(\cdot)\) is nonlinear, this expectation cannot be simplified further without specific distributional assumptions.

\hypertarget{special-case-log-link-function}{%
\subsubsection{Special Case: Log Link Function}\label{special-case-log-link-function}}

For a log-link function, \(g(\mu) = \log(\mu)\), the inverse link is the exponential function:

\[
E(y_i) = E\left(\exp(\mathbf{x}_i' \boldsymbol{\beta} + \mathbf{z}_i' \boldsymbol{\alpha})\right)
\]

Using properties of the moment-generating function (MGF):

\[
E(y_i) = \exp(\mathbf{x}_i' \boldsymbol{\beta}) \cdot E\left(\exp(\mathbf{z}_i' \boldsymbol{\alpha})\right)
\]

Here, \(E(\exp(\mathbf{z}_i' \boldsymbol{\alpha}))\) is the MGF of \(\boldsymbol{\alpha}\) evaluated at \(\mathbf{z}_i\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{marginal-variance-of-y_i}{%
\subsection{\texorpdfstring{Marginal Variance of \(y_i\)}{Marginal Variance of y\_i}}\label{marginal-variance-of-y_i}}

The variance decomposition formula applies:

\[
\begin{aligned}
\operatorname{Var}(y_i) &= \operatorname{Var}_{\boldsymbol{\alpha}}\left(E(y_i \mid \boldsymbol{\alpha})\right) + E_{\boldsymbol{\alpha}}\left(\operatorname{Var}(y_i \mid \boldsymbol{\alpha})\right) \\
&= \operatorname{Var}(\mu_i) + E\left(a(\phi) V(\mu_i)\right)
\end{aligned}
\]

Expressed explicitly:

\[
\operatorname{Var}(y_i) = \operatorname{Var}\left(g^{-1}(\mathbf{x}_i' \boldsymbol{\beta} + \mathbf{z}_i' \boldsymbol{\alpha})\right) + E\left(a(\phi) V\left(g^{-1}(\mathbf{x}_i' \boldsymbol{\beta} + \mathbf{z}_i' \boldsymbol{\alpha})\right)\right)
\]

Without specific assumptions about \(g(\cdot)\) and the distribution of \(\boldsymbol{\alpha}\), this is the most general form.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{marginal-covariance-of-mathbfy}{%
\subsection{\texorpdfstring{Marginal Covariance of \(\mathbf{y}\)}{Marginal Covariance of \textbackslash mathbf\{y\}}}\label{marginal-covariance-of-mathbfy}}

Random effects induce correlation between observations within the same cluster. The covariance between \(y_i\) and \(y_j\) is:

\[
\begin{aligned}
\operatorname{Cov}(y_i, y_j) &= \operatorname{Cov}_{\boldsymbol{\alpha}}\left(E(y_i \mid \boldsymbol{\alpha}), E(y_j \mid \boldsymbol{\alpha})\right) + E_{\boldsymbol{\alpha}}\left(\operatorname{Cov}(y_i, y_j \mid \boldsymbol{\alpha})\right) \\
&= \operatorname{Cov}(\mu_i, \mu_j) + E(0) \\
&= \operatorname{Cov}\left(g^{-1}(\mathbf{x}_i' \boldsymbol{\beta} + \mathbf{z}_i' \boldsymbol{\alpha}), g^{-1}(\mathbf{x}_j' \boldsymbol{\beta} + \mathbf{z}_j' \boldsymbol{\alpha})\right)
\end{aligned}
\]

The second term vanishes when \(y_i\) and \(y_j\) are conditionally independent given \(\boldsymbol{\alpha}\). This dependency structure is a hallmark of mixed models.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Example: Repeated Measurements with a Poisson GLMM

Consider repeated count measurements for subjects:

\begin{itemize}
\tightlist
\item
  Let \(y_{ij}\) be the \(j\)-th count for subject \(i\).
\item
  Assume \(y_{ij} \mid \alpha_i \sim \text{independent } \text{Poisson}(\mu_{ij})\).
\end{itemize}

The model is specified as:

\[
\log(\mu_{ij}) = \mathbf{x}_{ij}' \boldsymbol{\beta} + \alpha_i
\]

where:

\begin{itemize}
\tightlist
\item
  \(\alpha_i \sim \text{i.i.d. } N(0, \sigma^2_{\alpha})\) represents subject-specific random effects,
\item
  This is a log-link GLMM with random intercepts for subjects.
\end{itemize}

The inclusion of \(\alpha_i\) accounts for subject-level heterogeneity, capturing unobserved variability across individuals.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{estimation-in-nonlinear-and-generalized-linear-mixed-models}{%
\section{Estimation in Nonlinear and Generalized Linear Mixed Models}\label{estimation-in-nonlinear-and-generalized-linear-mixed-models}}

In \protect\hyperlink{sec-linear-mixed-models}{Linear Mixed Models}, the marginal likelihood of the observed data \(\mathbf{y}\) is derived by integrating out the random effects from the hierarchical formulation:

\[
f(\mathbf{y}) = \int f(\mathbf{y} \mid \boldsymbol{\alpha}) \, f(\boldsymbol{\alpha}) \, d\boldsymbol{\alpha}
\]

For LMMs, both component distributions---

\begin{itemize}
\item
  the conditional distribution \(f(\mathbf{y} \mid \boldsymbol{\alpha})\), and
\item
  the random effects distribution \(f(\boldsymbol{\alpha})\)---
\end{itemize}

are typically assumed to be Gaussian with linear relationships. These assumptions imply that the marginal distribution of \(\mathbf{y}\) is also Gaussian, allowing the integral to be solved analytically using properties of the multivariate normal distribution.

In contrast:

\begin{itemize}
\item
  For GLMMs, the conditional distribution \(f(\mathbf{y} \mid \boldsymbol{\alpha})\) belongs to the exponential family but is not Gaussian in general.
\item
  For NLMMs, the relationship between the mean response and the random (and fixed) effects is nonlinear, complicating the integral.
\end{itemize}

In both cases, the marginal likelihood integral:

\[
L(\boldsymbol{\beta}; \mathbf{y}) = \int f(\mathbf{y} \mid \boldsymbol{\alpha}) \, f(\boldsymbol{\alpha}) \, d\boldsymbol{\alpha}
\]

cannot be solved analytically. Consequently, estimation requires:

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{estimation-by-numerical-integration}{Numerical Integration}
\item
  Linearization of the Model
\item
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{estimation-by-numerical-integration}{%
\subsection{Estimation by Numerical Integration}\label{estimation-by-numerical-integration}}

The marginal likelihood for parameter estimation is given by:

\[
L(\boldsymbol{\beta}; \mathbf{y}) = \int f(\mathbf{y} \mid \boldsymbol{\alpha}) \, f(\boldsymbol{\alpha}) \, d\boldsymbol{\alpha}
\]

To estimate the fixed effects \(\boldsymbol{\beta}\), we often maximize the log-likelihood:

\[
\ell(\boldsymbol{\beta}) = \log L(\boldsymbol{\beta}; \mathbf{y})
\]

Optimization requires the score function (gradient):

\[
\frac{\partial \ell(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}}
\]

Since the integral in \(L(\boldsymbol{\beta}; \mathbf{y})\) is generally intractable, we rely on numerical techniques to approximate it.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{methods-for-numerical-integration}{%
\subsubsection{Methods for Numerical Integration}\label{methods-for-numerical-integration}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Gaussian Quadrature

  \begin{itemize}
  \tightlist
  \item
    Suitable for low-dimensional random effects (\(\dim(\boldsymbol{\alpha})\) is small).
  \item
    Approximates the integral using weighted sums of function evaluations at specific points (nodes).
  \item
    Gauss-Hermite quadrature is commonly used when random effects are normally distributed.
  \end{itemize}

  Limitation: Computational cost grows exponentially with the dimension of \(\boldsymbol{\alpha}\) (curse of dimensionality).
\item
  Laplace Approximation

  \begin{itemize}
  \tightlist
  \item
    Approximates the integral by expanding the log-likelihood around the mode of the integrand (i.e., the most likely value of \(\boldsymbol{\alpha}\)).
  \item
    Provides accurate results for moderate-sized random effects and large sample sizes.
  \item
    First-order Laplace approximation is commonly used; higher-order versions improve accuracy but increase complexity.
  \end{itemize}

  Key Idea: Approximate the integral as:

  \[
  \int e^{h(\boldsymbol{\alpha})} d\boldsymbol{\alpha} \approx e^{h(\hat{\boldsymbol{\alpha}})} \sqrt{\frac{(2\pi)^q}{|\mathbf{H}|}}
  \]

  where:

  \begin{itemize}
  \tightlist
  \item
    \(\hat{\boldsymbol{\alpha}}\) is the mode of \(h(\boldsymbol{\alpha})\),
  \item
    \(\mathbf{H}\) is the Hessian matrix of second derivatives at \(\hat{\boldsymbol{\alpha}}\),
  \item
    \(q\) is the dimension of \(\boldsymbol{\alpha}\).
  \end{itemize}
\item
  Monte Carlo Integration

  \begin{itemize}
  \tightlist
  \item
    Uses random sampling to approximate the integral.
  \item
    Importance Sampling improves efficiency by sampling from a distribution that better matches the integrand.
  \item
    Markov Chain Monte Carlo methods, such as Gibbs sampling or Metropolis-Hastings, are used when the posterior distribution is complex.
  \end{itemize}

  Advantage: Scales better with high-dimensional random effects compared to quadrature methods.

  Limitation: Computationally intensive, and variance of estimates can be high without careful tuning.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{choosing-an-integration-method}{%
\subsubsection{Choosing an Integration Method}\label{choosing-an-integration-method}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1855}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2419}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2823}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2903}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Dimensionality
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Accuracy
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Computational Cost
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Gaussian Quadrature & Low-dimensional (\(q \leq 3\)) & High (with sufficient nodes) & High (exponential growth with \(q\)) \\
Laplace Approximation & Moderate-dimensional & Moderate to High & Moderate \\
Monte Carlo Methods & High-dimensional & Variable (depends on sample size) & High (but scalable) \\
\end{longtable}

\begin{itemize}
\tightlist
\item
  For small random effect dimensions, quadrature methods are effective.
\item
  For moderate dimensions, Laplace approximation offers a good balance.
\item
  For high dimensions or complex models, Monte Carlo techniques are often the method of choice.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{sec-estimation-by-linearization-glmm}{%
\subsection{Estimation by Linearization}\label{sec-estimation-by-linearization-glmm}}

When estimating parameters in \protect\hyperlink{sec-nonlinear-mixed-models}{NLMMs} and \protect\hyperlink{sec-generalized-linear-mixed-models}{GLMMs}, one common and effective approach is linearization. This technique approximates the nonlinear or non-Gaussian components with linear counterparts, enabling the use of standard LMM estimation methods. Linearization not only simplifies the estimation process but also allows for leveraging well-established statistical tools and methods developed for linear models.

\hypertarget{concept-of-linearization}{%
\subsubsection{Concept of Linearization}\label{concept-of-linearization}}

The core idea is to create a linearized version of the response variable, known as the \emph{working response} or \emph{pseudo-response}, denoted as \(\tilde{y}_i\). This pseudo-response is designed to approximate the original nonlinear relationship in a linear form, facilitating easier estimation of model parameters. The conditional mean of this pseudo-response is expressed as:

\[
E(\tilde{y}_i \mid \boldsymbol{\alpha}) = \mathbf{x}_i' \boldsymbol{\beta} + \mathbf{z}_i' \boldsymbol{\alpha}
\]

Here:

\begin{itemize}
\item
  \(\mathbf{x}_i\) is the design matrix for fixed effects,
\item
  \(\boldsymbol{\beta}\) represents the fixed effect parameters,
\item
  \(\mathbf{z}_i\) is the design matrix for random effects,
\item
  \(\boldsymbol{\alpha}\) denotes the random effects.
\end{itemize}

In addition to the conditional mean, it is essential to estimate the conditional variance of the pseudo-response to fully characterize the linearized model:

\[
\operatorname{Var}(\tilde{y}_i \mid \boldsymbol{\alpha})
\]

This variance estimation ensures that the model accounts for the inherent variability in the data, maintaining the integrity of statistical inferences.

\hypertarget{application-of-linearization}{%
\subsubsection{Application of Linearization}\label{application-of-linearization}}

Once linearized, the model structure closely resembles that of a \href{(\#sec-linear-mixed-models)}{linear mixed model}, allowing us to apply standard estimation techniques from \protect\hyperlink{sec-linear-mixed-models}{LMMs}. These techniques include methods such as MLE and REML, which are computationally efficient and statistically robust.

The primary difference between various linearization-based methods lies in how the linearization is performed. This often involves expanding the nonlinear function \(f(\mathbf{x}, \boldsymbol{\theta}, \boldsymbol{\alpha})\) or the inverse link function \(g^{-1}(\cdot)\). The goal is to approximate these complex functions with simpler linear expressions while retaining as much of the original model's characteristics as possible.

\hypertarget{taylor-series-expansion}{%
\paragraph{Taylor Series Expansion}\label{taylor-series-expansion}}

A widely used method for linearization is the Taylor series expansion. This approach approximates the nonlinear mean function around initial estimates of the random effects. The first-order Taylor series expansion of the nonlinear function is given by:

\[
 f(\mathbf{x}_{ij}, \boldsymbol{\theta}, \boldsymbol{\alpha}_i) \approx f(\mathbf{x}_{ij}, \boldsymbol{\theta}, \hat{\boldsymbol{\alpha}}_i) + \frac{\partial f}{\partial \boldsymbol{\alpha}_i} \bigg|_{\hat{\boldsymbol{\alpha}}_i} (\boldsymbol{\alpha}_i - \hat{\boldsymbol{\alpha}}_i)
\]

In this expression:

\begin{itemize}
\item
  \(f(\mathbf{x}_{ij}, \boldsymbol{\theta}, \hat{\boldsymbol{\alpha}}_i)\) is the function evaluated at the initial estimates of the random effects \(\hat{\boldsymbol{\alpha}}_i\),
\item
  \(\frac{\partial f}{\partial \boldsymbol{\alpha}_i} \big|_{\hat{\boldsymbol{\alpha}}_i}\) represents the gradient (or derivative) of the function with respect to the random effects, evaluated at \(\hat{\boldsymbol{\alpha}}_i\),
\item
  \((\boldsymbol{\alpha}_i - \hat{\boldsymbol{\alpha}}_i)\) captures the deviation from the initial estimates.
\end{itemize}

The initial estimates \(\hat{\boldsymbol{\alpha}}_i\) are often set to zero for simplicity, especially in the early stages of model fitting. This approximation reduces the model to a linear form, making it amenable to standard LMM estimation techniques.

\hypertarget{advantages-and-considerations}{%
\paragraph{Advantages and Considerations}\label{advantages-and-considerations}}

Linearization offers several advantages:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Simplified Computation: By transforming complex nonlinear relationships into linear forms, linearization reduces computational complexity.
\item
  Flexibility: Despite the simplification, linearized models retain the ability to capture key features of the original data structure.
\item
  Statistical Robustness: The use of established LMM estimation techniques ensures robust parameter estimation.
\end{enumerate}

However, linearization also comes with considerations:

\begin{itemize}
\item
  Approximation Error: The accuracy of the linearized model depends on how well the linear approximation captures the original nonlinear relationship.
\item
  Choice of Expansion Point: The selection of initial estimates \(\hat{\boldsymbol{\alpha}}_i\) can influence the quality of the approximation.
\item
  Higher-Order Terms: In cases where the first-order approximation is insufficient, higher-order Taylor series terms may be needed, increasing model complexity.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{penalized-quasi-likelihood}{%
\subsubsection{Penalized Quasi-Likelihood}\label{penalized-quasi-likelihood}}

Penalized Quasi-Likelihood (PQL) is one of the most popular linearization-based estimation methods for \protect\hyperlink{sec-generalized-linear-mixed-models}{GLMMs}.

The linearization is achieved through a first-order Taylor expansion of the inverse link function around current estimates of the parameters. The working response at the \(k\)-th iteration is given by:

\[
\tilde{y}_i^{(k)} = \hat{\eta}_i^{(k-1)} + \left(y_i - \hat{\mu}_i^{(k-1)}\right) \cdot \left.\frac{d \eta}{d \mu}\right|_{\hat{\eta}_i^{(k-1)}}
\]

Where:

\begin{itemize}
\tightlist
\item
  \(\eta_i = g(\mu_i)\) is the linear predictor,\\
\item
  \(\hat{\eta}_i^{(k-1)}\) and \(\hat{\mu}_i^{(k-1)}\) are the estimates from the previous iteration \((k-1)\),\\
\item
  \(g(\cdot)\) is the link function, and \(\mu_i = g^{-1}(\eta_i)\).
\end{itemize}

PQL Estimation Algorithm

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Initialization:\\
  Start with initial estimates of \(\boldsymbol{\beta}\) and \(\boldsymbol{\alpha}\) (commonly set to zeros).
\item
  Compute the Working Response:\\
  Use the formula above to compute \(\tilde{y}_i^{(k)}\) based on current parameter estimates.
\item
  Fit a \protect\hyperlink{sec-linear-mixed-models}{Linear Mixed Model}:\\
  Apply standard LMM estimation techniques to the pseudo-response \(\tilde{y}_i^{(k)}\) to update estimates of \(\boldsymbol{\beta}\) and \(\boldsymbol{\alpha}\).
\item
  Update Variance Components:\\
  Estimate \(\operatorname{Var}(\tilde{y}_i \mid \boldsymbol{\alpha})\) based on updated parameter estimates.
\item
  Iteration:\\
  Repeat steps 2--4 until the estimates converge.
\end{enumerate}

Comments on PQL:

\begin{itemize}
\tightlist
\item
  Advantages:

  \begin{itemize}
  \tightlist
  \item
    Easy to implement using existing LMM software.
  \item
    Fast convergence for many practical datasets.
  \end{itemize}
\item
  Limitations:

  \begin{itemize}
  \tightlist
  \item
    Inference is only asymptotically correct due to the linearization approximation.
  \item
    Biased estimates are common, especially:

    \begin{itemize}
    \tightlist
    \item
      For binomial responses with small group sizes,
    \item
      In Bernoulli models (worst-case scenario),
    \item
      In Poisson models with small counts. \citep{faraway2016extending}
    \end{itemize}
  \item
    Hypothesis testing and confidence intervals can be unreliable.
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{generalized-estimating-equations}{%
\subsubsection{Generalized Estimating Equations}\label{generalized-estimating-equations}}

Generalized Estimating Equations (GEE) offer an alternative approach to parameter estimation in models with correlated data, particularly for marginal models where the focus is on population-averaged effects rather than subject-specific effects.

GEE estimates are obtained by solving estimating equations rather than maximizing a likelihood function.

Consider a marginal generalized linear model:

\[
\operatorname{logit}(E(\mathbf{y})) = \mathbf{X} \boldsymbol{\beta}
\]

Assuming a working covariance matrix \(\mathbf{V}\) for the elements of \(\mathbf{y}\), the estimating equation for \(\boldsymbol{\beta}\) is:

\[
\mathbf{X}' \mathbf{V}^{-1} (\mathbf{y} - E(\mathbf{y})) = 0
\]

If \(\mathbf{V}\) correctly specifies the covariance structure, the estimator is unbiased. In practice, we often assume a simple structure (e.g., independence) and obtain robust standard errors even when the covariance is misspecified.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{gee-for-repeated-measures}{%
\paragraph{GEE for Repeated Measures}\label{gee-for-repeated-measures}}

Let \(y_{ij}\) denote the \(j\)-th measurement on the \(i\)-th subject, with:

\[
\mathbf{y}_i = 
\begin{pmatrix}
y_{i1} \\
\vdots \\
y_{in_i}
\end{pmatrix},
\quad
\boldsymbol{\mu}_i =
\begin{pmatrix}
\mu_{i1} \\
\vdots \\
\mu_{in_i}
\end{pmatrix},
\quad
\mathbf{x}_{ij} =
\begin{pmatrix}
X_{ij1} \\
\vdots \\
X_{ijp}
\end{pmatrix}
\]

Define the working covariance matrix of \(\mathbf{y}_i\) as:

\[
\mathbf{V}_i = \operatorname{Cov}(\mathbf{y}_i)
\]

Following \citep{liang1986longitudinal}, the GEE for estimating \(\boldsymbol{\beta}\) is:

\[
S(\boldsymbol{\beta}) = \sum_{i=1}^K \frac{\partial \boldsymbol{\mu}_i'}{\partial \boldsymbol{\beta}} \, \mathbf{V}_i^{-1} (\mathbf{y}_i - \boldsymbol{\mu}_i) = 0
\]

Where:

\begin{itemize}
\item
  \(K\) is the number of subjects (or clusters),
\item
  \(\boldsymbol{\mu}_i = E(\mathbf{y}_i)\),
\item
  \(\mathbf{V}_i\) is the working covariance matrix.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{working-correlation-structures}{%
\paragraph{Working Correlation Structures}\label{working-correlation-structures}}

The covariance matrix \(\mathbf{V}_i\) is modeled as:

\[
\mathbf{V}_i = a(\phi) \, \mathbf{B}_i^{1/2} \, \mathbf{R}(\boldsymbol{c}) \, \mathbf{B}_i^{1/2}
\]

\begin{itemize}
\tightlist
\item
  \(a(\phi)\) is a dispersion parameter,
\item
  \(\mathbf{B}_i\) is a diagonal matrix with variance functions \(V(\mu_{ij})\) on the diagonal,
\item
  \(\mathbf{R}(\boldsymbol{c})\) is the working correlation matrix, parameterized by \(\boldsymbol{c}\). If \(\mathbf{R}(\boldsymbol{c})\) is the true correlation matrix of \(\mathbf{y}_i\), then \(\mathbf{V}_i\) is the true covariance matrix.
\end{itemize}

Common Working Correlation Structures:

\begin{itemize}
\tightlist
\item
  Independence: \(\mathbf{R} = \mathbf{I}\) (simplest, but ignores correlation).
\item
  Exchangeable: Constant correlation between all pairs within a cluster.
\item
  Autoregressive (AR(1)): Correlation decreases with time lag.
\item
  Unstructured: Each pair has its own correlation parameter.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{iterative-algorithm-for-gee-estimation}{%
\paragraph{Iterative Algorithm for GEE Estimation}\label{iterative-algorithm-for-gee-estimation}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Initialization:

  \begin{itemize}
  \tightlist
  \item
    Compute an initial estimate of \(\boldsymbol{\beta}\) using a GLM under the independence assumption (\(\mathbf{R} = \mathbf{I}\)).
  \end{itemize}
\item
  Estimate the Working Correlation Matrix:

  \begin{itemize}
  \tightlist
  \item
    Based on residuals from the initial fit, estimate \(\mathbf{R}(\boldsymbol{c})\).
  \end{itemize}
\item
  Update the Covariance Matrix:

  \begin{itemize}
  \tightlist
  \item
    Calculate \(\hat{\mathbf{V}}_i\) using the updated working correlation matrix.
  \end{itemize}
\item
  Update \(\boldsymbol{\beta}\):

  \[
  \boldsymbol{\beta}^{(r+1)} = \boldsymbol{\beta}^{(r)} + \left(\sum_{i=1}^K \frac{\partial \boldsymbol{\mu}_i'}{\partial \boldsymbol{\beta}} \, \hat{\mathbf{V}}_i^{-1} \, \frac{\partial \boldsymbol{\mu}_i}{\partial \boldsymbol{\beta}} \right)^{-1}
  \left( \sum_{i=1}^K \frac{\partial \boldsymbol{\mu}_i'}{\partial \boldsymbol{\beta}} \, \hat{\mathbf{V}}_i^{-1} (\mathbf{y}_i - \boldsymbol{\mu}_i) \right)
  \]
\item
  Iteration:

  \begin{itemize}
  \tightlist
  \item
    Repeat steps 2--4 until convergence (i.e., when changes in \(\boldsymbol{\beta}\) are negligible).
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Comments on GEE:

\begin{itemize}
\tightlist
\item
  Advantages:

  \begin{itemize}
  \tightlist
  \item
    Provides consistent estimates of \(\boldsymbol{\beta}\) even if the working correlation matrix is misspecified.
  \item
    Robust standard errors (also known as ``sandwich'' estimators) account for potential misspecification.
  \end{itemize}
\item
  Limitations:

  \begin{itemize}
  \tightlist
  \item
    Not a likelihood-based method, so likelihood-ratio tests are not appropriate.
  \item
    Efficiency loss if the working correlation matrix is poorly specified.
  \item
    Estimation of random effects is not possible---GEE focuses on marginal (population-averaged) effects.
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{estimation-by-bayesian-hierarchical-models}{%
\subsection{Estimation by Bayesian Hierarchical Models}\label{estimation-by-bayesian-hierarchical-models}}

Bayesian methods provide a flexible framework for estimating parameters in \protect\hyperlink{sec-nonlinear-mixed-models}{NLMMs} and \protect\hyperlink{sec-generalized-linear-mixed-models}{GLMMs}. Unlike frequentist approaches that rely on MLE or linearization techniques, Bayesian estimation fully incorporates prior information and naturally accounts for uncertainty in both parameter estimation and predictions.

In the Bayesian context, we are interested in the posterior distribution of the model parameters, given the observed data \(\mathbf{y}\):

\[
f(\boldsymbol{\alpha}, \boldsymbol{\beta} \mid \mathbf{y}) \propto f(\mathbf{y} \mid \boldsymbol{\alpha}, \boldsymbol{\beta}) \, f(\boldsymbol{\alpha}) \, f(\boldsymbol{\beta})
\]

Where:

\begin{itemize}
\tightlist
\item
  \(f(\mathbf{y} \mid \boldsymbol{\alpha}, \boldsymbol{\beta})\) is the likelihood of the data,
\item
  \(f(\boldsymbol{\alpha})\) is the prior distribution for the random effects,
\item
  \(f(\boldsymbol{\beta})\) is the prior distribution for the fixed effects,
\item
  \(f(\boldsymbol{\alpha}, \boldsymbol{\beta} \mid \mathbf{y})\) is the posterior distribution, which combines prior beliefs with observed data.
\end{itemize}

\textbf{Advantages of Bayesian Estimation}

\begin{itemize}
\tightlist
\item
  \textbf{No Need for Simplifying Approximations:} Bayesian methods do not require linearization or asymptotic approximations.
\item
  \textbf{Full Uncertainty Quantification:} Provides credible intervals for parameters and predictive distributions for new data.
\item
  \textbf{Flexible Modeling:} Easily accommodates complex hierarchical structures, non-standard distributions, and prior information.
\end{itemize}

\textbf{Computational Challenges}

Despite its advantages, Bayesian estimation can be computationally intensive and complex, especially for high-dimensional models. Key implementation issues include:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Non-Valid Joint Distributions:}\\
  In some hierarchical models, specifying valid joint distributions for the data, random effects, and parameters can be challenging.
\item
  \textbf{Constraints from Mean-Variance Relationships:}\\
  The inherent relationship between the mean and variance in GLMMs, combined with random effects, imposes constraints on the marginal covariance structure.
\item
  \textbf{Computational Intensity:}\\
  Fitting Bayesian models often requires advanced numerical techniques like Markov Chain Monte Carlo, which can be slow to converge, especially for large datasets or complex models.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{bayesian-estimation-methods}{%
\subsubsection{Bayesian Estimation Methods}\label{bayesian-estimation-methods}}

Bayesian estimation can proceed through two general approaches:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Approximating the Objective Function (Marginal Likelihood)}
\end{enumerate}

The marginal likelihood is typically intractable because it requires integrating over random effects:

\[
f(\mathbf{y} \mid \boldsymbol{\beta}) = \int f(\mathbf{y} \mid \boldsymbol{\alpha}, \boldsymbol{\beta}) \, f(\boldsymbol{\alpha}) \, d\boldsymbol{\alpha}
\]

Since this integral cannot be solved analytically, we approximate it using the following methods:

\begin{itemize}
\item
  \textbf{Laplace Approximation}

  \begin{itemize}
  \item
    Approximates the integral by expanding the log-likelihood around the mode of the integrand.
  \item
    Provides an efficient, asymptotically accurate approximation when the posterior is approximately Gaussian near its mode.
  \end{itemize}
\item
  \textbf{Quadrature Methods}

  \begin{itemize}
  \item
    \textbf{Gaussian quadrature} (e.g., Gauss-Hermite quadrature) is effective for low-dimensional random effects.
  \item
    Approximates the integral by summing weighted evaluations of the function at specific points.
  \end{itemize}
\item
  \textbf{Monte Carlo Integration}

  \begin{itemize}
  \item
    Uses random sampling to approximate the integral.
  \item
    \textbf{Importance sampling} improves efficiency by drawing samples from a distribution that closely resembles the target distribution.
  \end{itemize}
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Approximating the Model (Linearization)}
\end{enumerate}

Alternatively, we can approximate the model itself using \textbf{Taylor series linearization} around current estimates of the parameters. This approach simplifies the model, making Bayesian estimation computationally more feasible, though at the cost of some approximation error.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{markov-chain-monte-carlo-methods}{%
\subsubsection{Markov Chain Monte Carlo Methods}\label{markov-chain-monte-carlo-methods}}

The most common approach for fully Bayesian estimation is \textbf{MCMC}, which generates samples from the posterior distribution through iterative simulation. Popular MCMC algorithms include:

\begin{itemize}
\item
  \textbf{Gibbs Sampling:}\\
  Efficient when full conditional distributions are available in closed form.
\item
  \textbf{Metropolis-Hastings Algorithm:}\\
  More general and flexible, used when full conditionals are not easily sampled.
\item
  \textbf{Hamiltonian Monte Carlo (HMC):}\\
  Implemented in packages like \texttt{Stan}, provides faster convergence for complex models by leveraging gradient information.
\end{itemize}

The posterior distribution is then approximated using the generated samples:

\[
f(\boldsymbol{\alpha}, \boldsymbol{\beta} \mid \mathbf{y}) \approx \frac{1}{N} \sum_{i=1}^N \delta(\boldsymbol{\alpha} - \boldsymbol{\alpha}^{(i)}, \boldsymbol{\beta} - \boldsymbol{\beta}^{(i)})
\]

Where \(N\) is the number of MCMC samples and \(\delta(\cdot)\) is the Dirac delta function.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{practical-implementation-in-r}{%
\subsection{Practical Implementation in R}\label{practical-implementation-in-r}}

Several R packages facilitate Bayesian estimation for GLMMs and NLMMs:

\begin{itemize}
\tightlist
\item
  \textbf{GLMM Estimation:}

  \begin{itemize}
  \tightlist
  \item
    \texttt{MASS::glmmPQL} --- Penalized Quasi-Likelihood for GLMMs.
  \item
    \texttt{lme4::glmer} --- Frequentist estimation for GLMMs using Laplace approximation.
  \item
    \texttt{glmmTMB} --- Handles complex random effects structures efficiently.
  \end{itemize}
\item
  \textbf{NLMM Estimation:}

  \begin{itemize}
  \tightlist
  \item
    \texttt{nlme::nlme} --- Nonlinear mixed-effects modeling.
  \item
    \texttt{lme4::nlmer} --- Extends \texttt{lme4} for nonlinear mixed models.
  \item
    \texttt{brms::brm} --- Bayesian estimation via \texttt{Stan}, supporting NLMMs.
  \end{itemize}
\item
  \textbf{Bayesian Estimation:}

  \begin{itemize}
  \tightlist
  \item
    \texttt{MCMCglmm} --- Implements MCMC algorithms for GLMMs.
  \item
    \texttt{brms::brm} --- High-level interface for Bayesian regression models, leveraging \texttt{Stan} for efficient MCMC sampling.
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Example: Non-Gaussian Repeated Measurements}

Consider the case of \textbf{repeated measurements}:

\begin{itemize}
\tightlist
\item
  \textbf{If the data are Gaussian:} Use \protect\hyperlink{sec-linear-mixed-models}{Linear Mixed Models}.
\item
  \textbf{If the data are non-Gaussian:} Use \protect\hyperlink{sec-nonlinear-and-generalized-linear-mixed-models}{Nonlinear and Generalized Linear Mixed Models}.
\end{itemize}

\hypertarget{application-nonlinear-and-generalized-linear-mixed-models}{%
\section{Application: Nonlinear and Generalized Linear Mixed Models}\label{application-nonlinear-and-generalized-linear-mixed-models}}

\hypertarget{binomial-data-cbpp-dataset}{%
\subsection{Binomial Data: CBPP Dataset}\label{binomial-data-cbpp-dataset}}

We will use the \textbf{CBPP dataset} from the \texttt{lme4} package to demonstrate different estimation approaches for binomial mixed models.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(lme4)}
\FunctionTok{data}\NormalTok{(cbpp, }\AttributeTok{package =} \StringTok{"lme4"}\NormalTok{)}
\FunctionTok{head}\NormalTok{(cbpp)}
\CommentTok{\#\textgreater{}   herd incidence size period}
\CommentTok{\#\textgreater{} 1    1         2   14      1}
\CommentTok{\#\textgreater{} 2    1         3   12      2}
\CommentTok{\#\textgreater{} 3    1         4    9      3}
\CommentTok{\#\textgreater{} 4    1         0    5      4}
\CommentTok{\#\textgreater{} 5    2         3   22      1}
\CommentTok{\#\textgreater{} 6    2         1   18      2}
\end{Highlighting}
\end{Shaded}

The data contain information about contagious bovine pleuropneumonia (CBPP) cases across different herds and periods.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Penalized Quasi-Likelihood}
\end{enumerate}

\textbf{Pros:}

\begin{itemize}
\item
  Linearizes the response to create a pseudo-response, similar to linear mixed models.
\item
  Computationally efficient.
\end{itemize}

\textbf{Cons:}

\begin{itemize}
\item
  Biased for binary or Poisson data with small counts.
\item
  Random effects must be interpreted on the link scale.
\item
  AIC/BIC values are not interpretable since PQL does not rely on full likelihood.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(MASS)}
\NormalTok{pql\_cbpp }\OtherTok{\textless{}{-}} \FunctionTok{glmmPQL}\NormalTok{(}
    \FunctionTok{cbind}\NormalTok{(incidence, size }\SpecialCharTok{{-}}\NormalTok{ incidence) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ period,}
    \AttributeTok{random  =} \SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{|}\NormalTok{ herd,}
    \AttributeTok{data    =}\NormalTok{ cbpp,}
    \AttributeTok{family  =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"logit"}\NormalTok{),}
    \AttributeTok{verbose =} \ConstantTok{FALSE}
\NormalTok{)}
\FunctionTok{summary}\NormalTok{(pql\_cbpp)}
\CommentTok{\#\textgreater{} Linear mixed{-}effects model fit by maximum likelihood}
\CommentTok{\#\textgreater{}   Data: cbpp }
\CommentTok{\#\textgreater{}   AIC BIC logLik}
\CommentTok{\#\textgreater{}    NA  NA     NA}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Random effects:}
\CommentTok{\#\textgreater{}  Formula: \textasciitilde{}1 | herd}
\CommentTok{\#\textgreater{}         (Intercept) Residual}
\CommentTok{\#\textgreater{} StdDev:   0.5563535 1.184527}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Variance function:}
\CommentTok{\#\textgreater{}  Structure: fixed weights}
\CommentTok{\#\textgreater{}  Formula: \textasciitilde{}invwt }
\CommentTok{\#\textgreater{} Fixed effects:  cbind(incidence, size {-} incidence) \textasciitilde{} period }
\CommentTok{\#\textgreater{}                 Value Std.Error DF   t{-}value p{-}value}
\CommentTok{\#\textgreater{} (Intercept) {-}1.327364 0.2390194 38 {-}5.553372  0.0000}
\CommentTok{\#\textgreater{} period2     {-}1.016126 0.3684079 38 {-}2.758156  0.0089}
\CommentTok{\#\textgreater{} period3     {-}1.149984 0.3937029 38 {-}2.920944  0.0058}
\CommentTok{\#\textgreater{} period4     {-}1.605217 0.5178388 38 {-}3.099839  0.0036}
\CommentTok{\#\textgreater{}  Correlation: }
\CommentTok{\#\textgreater{}         (Intr) perid2 perid3}
\CommentTok{\#\textgreater{} period2 {-}0.399              }
\CommentTok{\#\textgreater{} period3 {-}0.373  0.260       }
\CommentTok{\#\textgreater{} period4 {-}0.282  0.196  0.182}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Standardized Within{-}Group Residuals:}
\CommentTok{\#\textgreater{}        Min         Q1        Med         Q3        Max }
\CommentTok{\#\textgreater{} {-}2.0591168 {-}0.6493095 {-}0.2747620  0.5170492  2.6187632 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Observations: 56}
\CommentTok{\#\textgreater{} Number of Groups: 15}
\end{Highlighting}
\end{Shaded}

Interpretation

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{exp}\NormalTok{(}\FloatTok{0.556}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 1.743684}
\end{Highlighting}
\end{Shaded}

The above result shows how herd-specific odds vary, accounting for random effects.

The fixed effects are interpreted similarly to logistic regression. For example, with the logit link:

\begin{itemize}
\tightlist
\item
  The \textbf{log odds} of having a case in \textbf{period 2} are \textbf{-1.016} less than in \textbf{period 1} (baseline).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(pql\_cbpp)}\SpecialCharTok{$}\NormalTok{tTable}
\CommentTok{\#\textgreater{}                 Value Std.Error DF   t{-}value      p{-}value}
\CommentTok{\#\textgreater{} (Intercept) {-}1.327364 0.2390194 38 {-}5.553372 2.333216e{-}06}
\CommentTok{\#\textgreater{} period2     {-}1.016126 0.3684079 38 {-}2.758156 8.888179e{-}03}
\CommentTok{\#\textgreater{} period3     {-}1.149984 0.3937029 38 {-}2.920944 5.843007e{-}03}
\CommentTok{\#\textgreater{} period4     {-}1.605217 0.5178388 38 {-}3.099839 3.637000e{-}03}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Numerical Integration with \texttt{glmer}}
\end{enumerate}

\textbf{Pros:}

\begin{itemize}
\tightlist
\item
  More accurate estimation since the method directly integrates over random effects.
\end{itemize}

\textbf{Cons:}

\begin{itemize}
\item
  Computationally more expensive, especially with high-dimensional random effects.
\item
  May struggle with convergence for complex models.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{numint\_cbpp }\OtherTok{\textless{}{-}} \FunctionTok{glmer}\NormalTok{(}
    \FunctionTok{cbind}\NormalTok{(incidence, size }\SpecialCharTok{{-}}\NormalTok{ incidence) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ period }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ herd),}
    \AttributeTok{data =}\NormalTok{ cbpp,}
    \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"logit"}\NormalTok{)}
\NormalTok{)}
\FunctionTok{summary}\NormalTok{(numint\_cbpp)}
\CommentTok{\#\textgreater{} Generalized linear mixed model fit by maximum likelihood (Laplace}
\CommentTok{\#\textgreater{}   Approximation) [glmerMod]}
\CommentTok{\#\textgreater{}  Family: binomial  ( logit )}
\CommentTok{\#\textgreater{} Formula: cbind(incidence, size {-} incidence) \textasciitilde{} period + (1 | herd)}
\CommentTok{\#\textgreater{}    Data: cbpp}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}      AIC      BIC   logLik deviance df.resid }
\CommentTok{\#\textgreater{}    194.1    204.2    {-}92.0    184.1       51 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Scaled residuals: }
\CommentTok{\#\textgreater{}     Min      1Q  Median      3Q     Max }
\CommentTok{\#\textgreater{} {-}2.3816 {-}0.7889 {-}0.2026  0.5142  2.8791 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Random effects:}
\CommentTok{\#\textgreater{}  Groups Name        Variance Std.Dev.}
\CommentTok{\#\textgreater{}  herd   (Intercept) 0.4123   0.6421  }
\CommentTok{\#\textgreater{} Number of obs: 56, groups:  herd, 15}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Fixed effects:}
\CommentTok{\#\textgreater{}             Estimate Std. Error z value Pr(\textgreater{}|z|)    }
\CommentTok{\#\textgreater{} (Intercept)  {-}1.3983     0.2312  {-}6.048 1.47e{-}09 ***}
\CommentTok{\#\textgreater{} period2      {-}0.9919     0.3032  {-}3.272 0.001068 ** }
\CommentTok{\#\textgreater{} period3      {-}1.1282     0.3228  {-}3.495 0.000474 ***}
\CommentTok{\#\textgreater{} period4      {-}1.5797     0.4220  {-}3.743 0.000182 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Correlation of Fixed Effects:}
\CommentTok{\#\textgreater{}         (Intr) perid2 perid3}
\CommentTok{\#\textgreater{} period2 {-}0.363              }
\CommentTok{\#\textgreater{} period3 {-}0.340  0.280       }
\CommentTok{\#\textgreater{} period4 {-}0.260  0.213  0.198}
\end{Highlighting}
\end{Shaded}

\textbf{Comparing PQL and Numerical Integration}

For small datasets, the difference between PQL and numerical integration may be minimal.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rbenchmark)}
\FunctionTok{benchmark}\NormalTok{(}
    \StringTok{"PQL (MASS)"} \OtherTok{=}\NormalTok{ \{}
        \FunctionTok{glmmPQL}\NormalTok{(}
            \FunctionTok{cbind}\NormalTok{(incidence, size }\SpecialCharTok{{-}}\NormalTok{ incidence) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ period,}
            \AttributeTok{random =} \SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{|}\NormalTok{ herd,}
            \AttributeTok{data =}\NormalTok{ cbpp,}
            \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"logit"}\NormalTok{),}
            \AttributeTok{verbose =} \ConstantTok{FALSE}
\NormalTok{        )}
\NormalTok{    \},}
    \StringTok{"Numerical Integration (lme4)"} \OtherTok{=}\NormalTok{ \{}
        \FunctionTok{glmer}\NormalTok{(}
            \FunctionTok{cbind}\NormalTok{(incidence, size }\SpecialCharTok{{-}}\NormalTok{ incidence) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ period }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ herd),}
            \AttributeTok{data =}\NormalTok{ cbpp,}
            \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"logit"}\NormalTok{)}
\NormalTok{        )}
\NormalTok{    \},}
    \AttributeTok{replications =} \DecValTok{50}\NormalTok{,}
    \AttributeTok{columns =} \FunctionTok{c}\NormalTok{(}\StringTok{"test"}\NormalTok{, }\StringTok{"replications"}\NormalTok{, }\StringTok{"elapsed"}\NormalTok{, }\StringTok{"relative"}\NormalTok{),}
    \AttributeTok{order =} \StringTok{"relative"}
\NormalTok{)}
\CommentTok{\#\textgreater{}                           test replications elapsed relative}
\CommentTok{\#\textgreater{} 1                   PQL (MASS)           50    4.25    1.000}
\CommentTok{\#\textgreater{} 2 Numerical Integration (lme4)           50    8.61    2.026}
\end{Highlighting}
\end{Shaded}

\textbf{Improving Accuracy with Gauss-Hermite Quadrature}

Setting \texttt{nAGQ\ \textgreater{}\ 1} increases the accuracy of the likelihood approximation:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{numint\_cbpp\_GH }\OtherTok{\textless{}{-}} \FunctionTok{glmer}\NormalTok{(}
    \FunctionTok{cbind}\NormalTok{(incidence, size }\SpecialCharTok{{-}}\NormalTok{ incidence) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ period }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ herd),}
    \AttributeTok{data =}\NormalTok{ cbpp,}
    \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"logit"}\NormalTok{),}
    \AttributeTok{nAGQ =} \DecValTok{20}
\NormalTok{)}
\FunctionTok{summary}\NormalTok{(numint\_cbpp\_GH)}\SpecialCharTok{$}\NormalTok{coefficients[, }\DecValTok{1}\NormalTok{] }\SpecialCharTok{{-}} 
    \FunctionTok{summary}\NormalTok{(numint\_cbpp)}\SpecialCharTok{$}\NormalTok{coefficients[, }\DecValTok{1}\NormalTok{]}
\CommentTok{\#\textgreater{}   (Intercept)       period2       period3       period4 }
\CommentTok{\#\textgreater{} {-}0.0008808634  0.0005160912  0.0004066218  0.0002644629}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Bayesian Approach with \texttt{MCMCglmm}}
\end{enumerate}

\textbf{Pros:}

\begin{itemize}
\item
  Incorporates prior information and handles complex models with intractable likelihoods.
\item
  Provides full posterior distributions for parameters.
\end{itemize}

\textbf{Cons:}

\begin{itemize}
\tightlist
\item
  Computationally intensive, especially with large datasets or complex hierarchical structures.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(MCMCglmm)}
\NormalTok{Bayes\_cbpp }\OtherTok{\textless{}{-}} \FunctionTok{MCMCglmm}\NormalTok{(}
    \FunctionTok{cbind}\NormalTok{(incidence, size }\SpecialCharTok{{-}}\NormalTok{ incidence) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ period,}
    \AttributeTok{random =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ herd,}
    \AttributeTok{data =}\NormalTok{ cbpp,}
    \AttributeTok{family =} \StringTok{"multinomial2"}\NormalTok{,}
    \AttributeTok{verbose =} \ConstantTok{FALSE}
\NormalTok{)}
\FunctionTok{summary}\NormalTok{(Bayes\_cbpp)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Iterations = 3001:12991}
\CommentTok{\#\textgreater{}  Thinning interval  = 10}
\CommentTok{\#\textgreater{}  Sample size  = 1000 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  DIC: 538.7376 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  G{-}structure:  \textasciitilde{}herd}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}      post.mean  l{-}95\% CI u{-}95\% CI eff.samp}
\CommentTok{\#\textgreater{} herd   0.03031 5.345e{-}17   0.1916    87.56}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  R{-}structure:  \textasciitilde{}units}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}       post.mean l{-}95\% CI u{-}95\% CI eff.samp}
\CommentTok{\#\textgreater{} units     1.117   0.2822     2.21    299.9}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Location effects: cbind(incidence, size {-} incidence) \textasciitilde{} period }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}             post.mean l{-}95\% CI u{-}95\% CI eff.samp  pMCMC    }
\CommentTok{\#\textgreater{} (Intercept)   {-}1.5195  {-}2.2597  {-}0.8855   1000.0 \textless{}0.001 ***}
\CommentTok{\#\textgreater{} period2       {-}1.2701  {-}2.3362  {-}0.2688    909.0  0.022 *  }
\CommentTok{\#\textgreater{} period3       {-}1.4085  {-}2.5627  {-}0.3611    755.5  0.006 ** }
\CommentTok{\#\textgreater{} period4       {-}1.9892  {-}3.3839  {-}0.8128    500.7 \textless{}0.001 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \texttt{MCMCglmm} fits a residual variance component (useful with dispersion issues).
\end{itemize}

Variance Component Analysis

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# explains less variability}
\FunctionTok{apply}\NormalTok{(Bayes\_cbpp}\SpecialCharTok{$}\NormalTok{VCV, }\DecValTok{2}\NormalTok{, sd)}
\CommentTok{\#\textgreater{}      herd     units }
\CommentTok{\#\textgreater{} 0.1136323 0.5339901}
\end{Highlighting}
\end{Shaded}

Posterior Summaries

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(Bayes\_cbpp)}\SpecialCharTok{$}\NormalTok{solutions}
\CommentTok{\#\textgreater{}             post.mean  l{-}95\% CI   u{-}95\% CI eff.samp pMCMC}
\CommentTok{\#\textgreater{} (Intercept) {-}1.529801 {-}2.156645 {-}0.8139842 875.5644 0.001}
\CommentTok{\#\textgreater{} period2     {-}1.249981 {-}2.385050 {-}0.2879657 780.4308 0.020}
\CommentTok{\#\textgreater{} period3     {-}1.401787 {-}2.598287 {-}0.3815690 800.8027 0.006}
\CommentTok{\#\textgreater{} period4     {-}1.936025 {-}3.171037 {-}0.7033584 432.9481 0.004}
\end{Highlighting}
\end{Shaded}

MCMC Diagnostics

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(lattice)}
\FunctionTok{xyplot}\NormalTok{(}\FunctionTok{as.mcmc}\NormalTok{(Bayes\_cbpp}\SpecialCharTok{$}\NormalTok{Sol), }\AttributeTok{layout =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{09-nonlinear_generalized_linear_mixed_files/figure-latex/unnamed-chunk-11-1} \end{center}

There is no trend (i.e., well-mixed).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{xyplot}\NormalTok{(}\FunctionTok{as.mcmc}\NormalTok{(Bayes\_cbpp}\SpecialCharTok{$}\NormalTok{VCV), }\AttributeTok{layout =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{09-nonlinear_generalized_linear_mixed_files/figure-latex/unnamed-chunk-12-1} \end{center}

For the herd variable, many of the values are 0, which suggests a problem. To address the instability in the herd effect sampling, we can either:

\begin{itemize}
\item
  Modify prior distributions,
\item
  Increase the number of iterations
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Bayes\_cbpp2 }\OtherTok{\textless{}{-}} \FunctionTok{MCMCglmm}\NormalTok{(}
    \FunctionTok{cbind}\NormalTok{(incidence, size }\SpecialCharTok{{-}}\NormalTok{ incidence) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ period,}
    \AttributeTok{random =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ herd,}
    \AttributeTok{data =}\NormalTok{ cbpp,}
    \AttributeTok{family =} \StringTok{"multinomial2"}\NormalTok{,}
    \AttributeTok{nitt =} \DecValTok{20000}\NormalTok{,}
    \AttributeTok{burnin =} \DecValTok{10000}\NormalTok{,}
    \AttributeTok{prior =} \FunctionTok{list}\NormalTok{(}\AttributeTok{G =} \FunctionTok{list}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{V =} \DecValTok{1}\NormalTok{, }\AttributeTok{nu =} \FloatTok{0.1}\NormalTok{))),}
    \AttributeTok{verbose =} \ConstantTok{FALSE}
\NormalTok{)}
\FunctionTok{xyplot}\NormalTok{(}\FunctionTok{as.mcmc}\NormalTok{(Bayes\_cbpp2}\SpecialCharTok{$}\NormalTok{VCV), }\AttributeTok{layout =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{09-nonlinear_generalized_linear_mixed_files/figure-latex/unnamed-chunk-13-1} \end{center}

To change the shape of priors, in \texttt{MCMCglmm} use:

\begin{itemize}
\item
  \texttt{V} controls for the location of the distribution (default = 1)
\item
  \texttt{nu} controls for the concentration around V (default = 0)
\end{itemize}

\hypertarget{count-data-owl-dataset}{%
\subsection{Count Data: Owl Dataset}\label{count-data-owl-dataset}}

We'll now model count data using the \textbf{Owl dataset}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(glmmTMB)}
\FunctionTok{library}\NormalTok{(dplyr)}
\FunctionTok{data}\NormalTok{(Owls, }\AttributeTok{package =} \StringTok{"glmmTMB"}\NormalTok{)}
\NormalTok{Owls }\OtherTok{\textless{}{-}}\NormalTok{ Owls }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{rename}\NormalTok{(}\AttributeTok{Ncalls =}\NormalTok{ SiblingNegotiation)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Poisson GLMM}
\end{enumerate}

Modeling call counts with a Poisson distribution:

In a typical Poisson model, the Poisson mean \(\lambda\) is modeled as: \[
\log(\lambda) = x' \beta
\] However, if the response variable represents a rate (e.g., counts per \textbf{BroodSize}), we can model it as: \[
\log\left(\frac{\lambda}{b}\right) = x' \beta
\] This is equivalent to: \[
\log(\lambda) = \log(b) + x' \beta
\] where \(b\) represents \textbf{BroodSize}. In this formulation, we ``offset'' the mean by including the logarithm of \(b\) as an offset term in the model. This adjustment accounts for the varying exposure or denominator in rate-based responses.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{owls\_glmer }\OtherTok{\textless{}{-}} \FunctionTok{glmer}\NormalTok{(}
\NormalTok{    Ncalls }\SpecialCharTok{\textasciitilde{}} \FunctionTok{offset}\NormalTok{(}\FunctionTok{log}\NormalTok{(BroodSize)) }\SpecialCharTok{+}\NormalTok{ FoodTreatment }\SpecialCharTok{*}\NormalTok{ SexParent }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ Nest),}
    \AttributeTok{family =}\NormalTok{ poisson,}
    \AttributeTok{data =}\NormalTok{ Owls}
\NormalTok{)}
\FunctionTok{summary}\NormalTok{(owls\_glmer)}
\CommentTok{\#\textgreater{} Generalized linear mixed model fit by maximum likelihood (Laplace}
\CommentTok{\#\textgreater{}   Approximation) [glmerMod]}
\CommentTok{\#\textgreater{}  Family: poisson  ( log )}
\CommentTok{\#\textgreater{} Formula: Ncalls \textasciitilde{} offset(log(BroodSize)) + FoodTreatment * SexParent +  }
\CommentTok{\#\textgreater{}     (1 | Nest)}
\CommentTok{\#\textgreater{}    Data: Owls}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}      AIC      BIC   logLik deviance df.resid }
\CommentTok{\#\textgreater{}   5212.8   5234.8  {-}2601.4   5202.8      594 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Scaled residuals: }
\CommentTok{\#\textgreater{}     Min      1Q  Median      3Q     Max }
\CommentTok{\#\textgreater{} {-}3.5529 {-}1.7971 {-}0.6842  1.2689 11.4312 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Random effects:}
\CommentTok{\#\textgreater{}  Groups Name        Variance Std.Dev.}
\CommentTok{\#\textgreater{}  Nest   (Intercept) 0.2063   0.4542  }
\CommentTok{\#\textgreater{} Number of obs: 599, groups:  Nest, 27}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Fixed effects:}
\CommentTok{\#\textgreater{}                                     Estimate Std. Error z value Pr(\textgreater{}|z|)    }
\CommentTok{\#\textgreater{} (Intercept)                          0.65585    0.09567   6.855 7.12e{-}12 ***}
\CommentTok{\#\textgreater{} FoodTreatmentSatiated               {-}0.65612    0.05606 {-}11.705  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} SexParentMale                       {-}0.03705    0.04501  {-}0.823   0.4104    }
\CommentTok{\#\textgreater{} FoodTreatmentSatiated:SexParentMale  0.13135    0.07036   1.867   0.0619 .  }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Correlation of Fixed Effects:}
\CommentTok{\#\textgreater{}             (Intr) FdTrtS SxPrnM}
\CommentTok{\#\textgreater{} FdTrtmntStt {-}0.225              }
\CommentTok{\#\textgreater{} SexParentMl {-}0.292  0.491       }
\CommentTok{\#\textgreater{} FdTrtmS:SPM  0.170 {-}0.768 {-}0.605}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Nest explains a relatively large proportion of the variability (its standard deviation is larger than some coefficients).
\item
  The model fit isn't great (deviance of 5202 on 594 df).
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Negative Binomial Model}
\end{enumerate}

Addressing overdispersion using the negative binomial distribution:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{owls\_glmerNB }\OtherTok{\textless{}{-}} \FunctionTok{glmer.nb}\NormalTok{(}
\NormalTok{    Ncalls }\SpecialCharTok{\textasciitilde{}} \FunctionTok{offset}\NormalTok{(}\FunctionTok{log}\NormalTok{(BroodSize)) }\SpecialCharTok{+}\NormalTok{ FoodTreatment }\SpecialCharTok{*}\NormalTok{ SexParent }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ Nest),}
    \AttributeTok{data =}\NormalTok{ Owls}
\NormalTok{)}
\FunctionTok{summary}\NormalTok{(owls\_glmerNB)}
\CommentTok{\#\textgreater{} Generalized linear mixed model fit by maximum likelihood (Laplace}
\CommentTok{\#\textgreater{}   Approximation) [glmerMod]}
\CommentTok{\#\textgreater{}  Family: Negative Binomial(0.8423)  ( log )}
\CommentTok{\#\textgreater{} Formula: Ncalls \textasciitilde{} offset(log(BroodSize)) + FoodTreatment * SexParent +  }
\CommentTok{\#\textgreater{}     (1 | Nest)}
\CommentTok{\#\textgreater{}    Data: Owls}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}      AIC      BIC   logLik deviance df.resid }
\CommentTok{\#\textgreater{}   3495.6   3522.0  {-}1741.8   3483.6      593 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Scaled residuals: }
\CommentTok{\#\textgreater{}     Min      1Q  Median      3Q     Max }
\CommentTok{\#\textgreater{} {-}0.8859 {-}0.7737 {-}0.2701  0.4443  6.1432 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Random effects:}
\CommentTok{\#\textgreater{}  Groups Name        Variance Std.Dev.}
\CommentTok{\#\textgreater{}  Nest   (Intercept) 0.1245   0.3529  }
\CommentTok{\#\textgreater{} Number of obs: 599, groups:  Nest, 27}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Fixed effects:}
\CommentTok{\#\textgreater{}                                     Estimate Std. Error z value Pr(\textgreater{}|z|)    }
\CommentTok{\#\textgreater{} (Intercept)                          0.69005    0.13400   5.149 2.61e{-}07 ***}
\CommentTok{\#\textgreater{} FoodTreatmentSatiated               {-}0.76657    0.16509  {-}4.643 3.43e{-}06 ***}
\CommentTok{\#\textgreater{} SexParentMale                       {-}0.02605    0.14575  {-}0.179    0.858    }
\CommentTok{\#\textgreater{} FoodTreatmentSatiated:SexParentMale  0.15680    0.20513   0.764    0.445    }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Correlation of Fixed Effects:}
\CommentTok{\#\textgreater{}             (Intr) FdTrtS SxPrnM}
\CommentTok{\#\textgreater{} FdTrtmntStt {-}0.602              }
\CommentTok{\#\textgreater{} SexParentMl {-}0.683  0.553       }
\CommentTok{\#\textgreater{} FdTrtmS:SPM  0.450 {-}0.744 {-}0.671}
\end{Highlighting}
\end{Shaded}

There is an improvement using negative binomial considering over-dispersion

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(Owls}\SpecialCharTok{$}\NormalTok{Ncalls,}\AttributeTok{breaks=}\DecValTok{30}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{09-nonlinear_generalized_linear_mixed_files/figure-latex/unnamed-chunk-17-1} \end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Zero-Inflated Model}
\end{enumerate}

Handling excess zeros with a zero-inflated Poisson model:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(glmmTMB)}
\NormalTok{owls\_glmm }\OtherTok{\textless{}{-}}
    \FunctionTok{glmmTMB}\NormalTok{(}
\NormalTok{        Ncalls }\SpecialCharTok{\textasciitilde{}}\NormalTok{ FoodTreatment }\SpecialCharTok{*}\NormalTok{ SexParent }\SpecialCharTok{+} \FunctionTok{offset}\NormalTok{(}\FunctionTok{log}\NormalTok{(BroodSize)) }\SpecialCharTok{+}
\NormalTok{            (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ Nest),}
        \AttributeTok{ziformula =}  \SpecialCharTok{\textasciitilde{}} \DecValTok{0}\NormalTok{,}
        \AttributeTok{family =} \FunctionTok{nbinom2}\NormalTok{(}\AttributeTok{link =} \StringTok{"log"}\NormalTok{),}
        \AttributeTok{data =}\NormalTok{ Owls}
\NormalTok{    )}
\NormalTok{owls\_glmm\_zi }\OtherTok{\textless{}{-}}
    \FunctionTok{glmmTMB}\NormalTok{(}
\NormalTok{        Ncalls }\SpecialCharTok{\textasciitilde{}}\NormalTok{ FoodTreatment }\SpecialCharTok{*}\NormalTok{ SexParent }\SpecialCharTok{+} \FunctionTok{offset}\NormalTok{(}\FunctionTok{log}\NormalTok{(BroodSize)) }\SpecialCharTok{+}
\NormalTok{            (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ Nest),}
        \AttributeTok{ziformula =}  \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{,}
        \AttributeTok{family =} \FunctionTok{nbinom2}\NormalTok{(}\AttributeTok{link =} \StringTok{"log"}\NormalTok{),}
        \AttributeTok{data =}\NormalTok{ Owls}
\NormalTok{    )}

\CommentTok{\# Scale Arrival time to use as a covariate for zero{-}inflation parameter}
\NormalTok{Owls}\SpecialCharTok{$}\NormalTok{ArrivalTime }\OtherTok{\textless{}{-}} \FunctionTok{scale}\NormalTok{(Owls}\SpecialCharTok{$}\NormalTok{ArrivalTime)}
\NormalTok{owls\_glmm\_zi\_cov }\OtherTok{\textless{}{-}} \FunctionTok{glmmTMB}\NormalTok{(}
\NormalTok{    Ncalls }\SpecialCharTok{\textasciitilde{}}\NormalTok{ FoodTreatment }\SpecialCharTok{*}\NormalTok{ SexParent }\SpecialCharTok{+}
        \FunctionTok{offset}\NormalTok{(}\FunctionTok{log}\NormalTok{(BroodSize)) }\SpecialCharTok{+}
\NormalTok{        (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ Nest),}
    \AttributeTok{ziformula =}  \SpecialCharTok{\textasciitilde{}}\NormalTok{ ArrivalTime,}
    \AttributeTok{family =} \FunctionTok{nbinom2}\NormalTok{(}\AttributeTok{link =} \StringTok{"log"}\NormalTok{),}
    \AttributeTok{data =}\NormalTok{ Owls}
\NormalTok{)}

\FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{anova}\NormalTok{(owls\_glmm, owls\_glmm\_zi))}
\CommentTok{\#\textgreater{}              Df      AIC      BIC    logLik deviance    Chisq Chi Df}
\CommentTok{\#\textgreater{} owls\_glmm     6 3495.610 3521.981 {-}1741.805 3483.610       NA     NA}
\CommentTok{\#\textgreater{} owls\_glmm\_zi  7 3431.646 3462.413 {-}1708.823 3417.646 65.96373      1}
\CommentTok{\#\textgreater{}                Pr(\textgreater{}Chisq)}
\CommentTok{\#\textgreater{} owls\_glmm              NA}
\CommentTok{\#\textgreater{} owls\_glmm\_zi 4.592983e{-}16}


\FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{anova}\NormalTok{(owls\_glmm\_zi, owls\_glmm\_zi\_cov))}
\CommentTok{\#\textgreater{}                  Df      AIC      BIC    logLik deviance    Chisq Chi Df}
\CommentTok{\#\textgreater{} owls\_glmm\_zi      7 3431.646 3462.413 {-}1708.823 3417.646       NA     NA}
\CommentTok{\#\textgreater{} owls\_glmm\_zi\_cov  8 3422.532 3457.694 {-}1703.266 3406.532 11.11411      1}
\CommentTok{\#\textgreater{}                    Pr(\textgreater{}Chisq)}
\CommentTok{\#\textgreater{} owls\_glmm\_zi               NA}
\CommentTok{\#\textgreater{} owls\_glmm\_zi\_cov 0.0008567362}


\FunctionTok{summary}\NormalTok{(owls\_glmm\_zi\_cov)}
\CommentTok{\#\textgreater{}  Family: nbinom2  ( log )}
\CommentTok{\#\textgreater{} Formula:          }
\CommentTok{\#\textgreater{} Ncalls \textasciitilde{} FoodTreatment * SexParent + offset(log(BroodSize)) +      (1 | Nest)}
\CommentTok{\#\textgreater{} Zero inflation:          \textasciitilde{}ArrivalTime}
\CommentTok{\#\textgreater{} Data: Owls}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}      AIC      BIC   logLik deviance df.resid }
\CommentTok{\#\textgreater{}   3422.5   3457.7  {-}1703.3   3406.5      591 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Random effects:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Conditional model:}
\CommentTok{\#\textgreater{}  Groups Name        Variance Std.Dev.}
\CommentTok{\#\textgreater{}  Nest   (Intercept) 0.07487  0.2736  }
\CommentTok{\#\textgreater{} Number of obs: 599, groups:  Nest, 27}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Dispersion parameter for nbinom2 family (): 2.22 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Conditional model:}
\CommentTok{\#\textgreater{}                                     Estimate Std. Error z value Pr(\textgreater{}|z|)    }
\CommentTok{\#\textgreater{} (Intercept)                          0.84778    0.09961   8.511  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} FoodTreatmentSatiated               {-}0.39529    0.13742  {-}2.877  0.00402 ** }
\CommentTok{\#\textgreater{} SexParentMale                       {-}0.07025    0.10435  {-}0.673  0.50079    }
\CommentTok{\#\textgreater{} FoodTreatmentSatiated:SexParentMale  0.12388    0.16449   0.753  0.45138    }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Zero{-}inflation model:}
\CommentTok{\#\textgreater{}             Estimate Std. Error z value Pr(\textgreater{}|z|)    }
\CommentTok{\#\textgreater{} (Intercept)  {-}1.3018     0.1261  {-}10.32  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} ArrivalTime   0.3545     0.1074    3.30 0.000966 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

\texttt{glmmTMB} can handle ZIP GLMMs since it adds automatic differentiation to existing estimation strategies.

We can see ZIP GLMM with an arrival time covariate on the zero is best.

\begin{itemize}
\item
  Arrival time has a positive effect on observing a nonzero number of calls
\item
  Interactions are non significant, the food treatment is significant (fewer calls after eating)
\item
  Nest variability is large in magnitude (without this, the parameter estimates change)
\end{itemize}

\hypertarget{binomial-example-gotway-hessian-fly-data}{%
\subsection{Binomial Example: Gotway Hessian Fly Data}\label{binomial-example-gotway-hessian-fly-data}}

We will analyze the \textbf{Gotway Hessian Fly} dataset from the \texttt{agridat} package to model binomial outcomes using both frequentist and Bayesian approaches.

\hypertarget{data-visualization}{%
\subsubsection{Data Visualization}\label{data-visualization}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(agridat)}
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(lme4)}
\FunctionTok{library}\NormalTok{(spaMM)}

\FunctionTok{data}\NormalTok{(gotway.hessianfly)}
\NormalTok{dat }\OtherTok{\textless{}{-}}\NormalTok{ gotway.hessianfly}
\NormalTok{dat}\SpecialCharTok{$}\NormalTok{prop }\OtherTok{\textless{}{-}}\NormalTok{ dat}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{/}\NormalTok{ dat}\SpecialCharTok{$}\NormalTok{n  }\CommentTok{\# Proportion of successes}

\FunctionTok{ggplot}\NormalTok{(dat, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ lat, }\AttributeTok{y =}\NormalTok{ long, }\AttributeTok{fill =}\NormalTok{ prop)) }\SpecialCharTok{+}
    \FunctionTok{geom\_tile}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{scale\_fill\_gradient}\NormalTok{(}\AttributeTok{low =} \StringTok{\textquotesingle{}white\textquotesingle{}}\NormalTok{, }\AttributeTok{high =} \StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_text}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =}\NormalTok{ gen, }\AttributeTok{color =}\NormalTok{ block)) }\SpecialCharTok{+}
    \FunctionTok{ggtitle}\NormalTok{(}\StringTok{\textquotesingle{}Gotway Hessian Fly: Proportion of Infestation\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{09-nonlinear_generalized_linear_mixed_files/figure-latex/unnamed-chunk-19-1} \end{center}

\hypertarget{model-specification-2}{%
\subsubsection{Model Specification}\label{model-specification-2}}

\begin{itemize}
\item
  Fixed Effects (\(\boldsymbol{\beta}\)): Genotype (\texttt{gen})
\item
  Random Effects (\(\boldsymbol{\alpha}\)): Block (\texttt{block}), accounting for spatial or experimental design variability
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Frequentist Approach with \texttt{glmer}}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flymodel }\OtherTok{\textless{}{-}} \FunctionTok{glmer}\NormalTok{(}
    \FunctionTok{cbind}\NormalTok{(y, n }\SpecialCharTok{{-}}\NormalTok{ y) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gen }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ block),}
    \AttributeTok{data   =}\NormalTok{ dat,}
    \AttributeTok{family =}\NormalTok{ binomial,}
    \AttributeTok{nAGQ   =} \DecValTok{5}  \CommentTok{\# Using adaptive Gauss{-}Hermite quadrature for accuracy}
\NormalTok{)}
\FunctionTok{summary}\NormalTok{(flymodel)}
\CommentTok{\#\textgreater{} Generalized linear mixed model fit by maximum likelihood (Adaptive}
\CommentTok{\#\textgreater{}   Gauss{-}Hermite Quadrature, nAGQ = 5) [glmerMod]}
\CommentTok{\#\textgreater{}  Family: binomial  ( logit )}
\CommentTok{\#\textgreater{} Formula: cbind(y, n {-} y) \textasciitilde{} gen + (1 | block)}
\CommentTok{\#\textgreater{}    Data: dat}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}      AIC      BIC   logLik deviance df.resid }
\CommentTok{\#\textgreater{}    162.2    198.9    {-}64.1    128.2       47 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Scaled residuals: }
\CommentTok{\#\textgreater{}      Min       1Q   Median       3Q      Max }
\CommentTok{\#\textgreater{} {-}2.38644 {-}1.01188  0.09631  1.03468  2.75479 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Random effects:}
\CommentTok{\#\textgreater{}  Groups Name        Variance Std.Dev.}
\CommentTok{\#\textgreater{}  block  (Intercept) 0.001022 0.03196 }
\CommentTok{\#\textgreater{} Number of obs: 64, groups:  block, 4}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Fixed effects:}
\CommentTok{\#\textgreater{}             Estimate Std. Error z value Pr(\textgreater{}|z|)    }
\CommentTok{\#\textgreater{} (Intercept)   1.5035     0.3914   3.841 0.000122 ***}
\CommentTok{\#\textgreater{} genG02       {-}0.1939     0.5302  {-}0.366 0.714644    }
\CommentTok{\#\textgreater{} genG03       {-}0.5408     0.5103  {-}1.060 0.289260    }
\CommentTok{\#\textgreater{} genG04       {-}1.4342     0.4714  {-}3.043 0.002346 ** }
\CommentTok{\#\textgreater{} genG05       {-}0.2037     0.5429  {-}0.375 0.707486    }
\CommentTok{\#\textgreater{} genG06       {-}0.9783     0.5046  {-}1.939 0.052533 .  }
\CommentTok{\#\textgreater{} genG07       {-}0.6041     0.5111  {-}1.182 0.237235    }
\CommentTok{\#\textgreater{} genG08       {-}1.6774     0.4907  {-}3.418 0.000630 ***}
\CommentTok{\#\textgreater{} genG09       {-}1.3984     0.4725  {-}2.960 0.003078 ** }
\CommentTok{\#\textgreater{} genG10       {-}0.6817     0.5333  {-}1.278 0.201181    }
\CommentTok{\#\textgreater{} genG11       {-}1.4630     0.4843  {-}3.021 0.002522 ** }
\CommentTok{\#\textgreater{} genG12       {-}1.4591     0.4918  {-}2.967 0.003010 ** }
\CommentTok{\#\textgreater{} genG13       {-}3.5528     0.6600  {-}5.383 7.31e{-}08 ***}
\CommentTok{\#\textgreater{} genG14       {-}2.5073     0.5264  {-}4.763 1.90e{-}06 ***}
\CommentTok{\#\textgreater{} genG15       {-}2.0872     0.4851  {-}4.302 1.69e{-}05 ***}
\CommentTok{\#\textgreater{} genG16       {-}2.9697     0.5383  {-}5.517 3.46e{-}08 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

\textbf{Interpretation}:

\begin{itemize}
\item
  The \textbf{fixed effects} (\texttt{gen}) indicate how different genotypes influence the infestation probability.
\item
  The \textbf{random effect} for \texttt{block} captures variability due to experimental blocks, improving model robustness.
\item
  \textbf{Odds Ratios:} Exponentiating coefficients helps interpret the impact on infestation odds.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Bayesian Approach with \texttt{MCMCglmm}}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(MCMCglmm)}
\FunctionTok{library}\NormalTok{(coda)}

\NormalTok{Bayes\_flymodel }\OtherTok{\textless{}{-}} \FunctionTok{MCMCglmm}\NormalTok{(}
    \FunctionTok{cbind}\NormalTok{(y, n }\SpecialCharTok{{-}}\NormalTok{ y) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gen,}
    \AttributeTok{random  =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ block,}
    \AttributeTok{data    =}\NormalTok{ dat,}
    \AttributeTok{family  =} \StringTok{"multinomial2"}\NormalTok{,}
    \AttributeTok{verbose =} \ConstantTok{FALSE}
\NormalTok{)}
\FunctionTok{summary}\NormalTok{(Bayes\_flymodel)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Iterations = 3001:12991}
\CommentTok{\#\textgreater{}  Thinning interval  = 10}
\CommentTok{\#\textgreater{}  Sample size  = 1000 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  DIC: 876.5476 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  G{-}structure:  \textasciitilde{}block}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}       post.mean l{-}95\% CI u{-}95\% CI eff.samp}
\CommentTok{\#\textgreater{} block   0.01028 6.99e{-}17  0.02814    872.7}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  R{-}structure:  \textasciitilde{}units}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}       post.mean l{-}95\% CI u{-}95\% CI eff.samp}
\CommentTok{\#\textgreater{} units     1.005   0.3446      1.9    354.8}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Location effects: cbind(y, n {-} y) \textasciitilde{} gen }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}             post.mean l{-}95\% CI u{-}95\% CI eff.samp  pMCMC    }
\CommentTok{\#\textgreater{} (Intercept)   1.97622  0.54231  3.22591    806.8  0.004 ** }
\CommentTok{\#\textgreater{} genG02       {-}0.42313 {-}2.21175  1.42876    929.8  0.650    }
\CommentTok{\#\textgreater{} genG03       {-}0.73259 {-}2.68958  0.95494   1000.0  0.400    }
\CommentTok{\#\textgreater{} genG04       {-}1.84145 {-}3.69541 {-}0.21353    893.5  0.032 *  }
\CommentTok{\#\textgreater{} genG05       {-}0.36572 {-}2.09198  1.66953    878.2  0.714    }
\CommentTok{\#\textgreater{} genG06       {-}1.31243 {-}3.18999  0.43163    940.8  0.152    }
\CommentTok{\#\textgreater{} genG07       {-}0.74898 {-}2.45451  1.06109    818.6  0.414    }
\CommentTok{\#\textgreater{} genG08       {-}2.13599 {-}3.99461 {-}0.42532   1000.0  0.020 *  }
\CommentTok{\#\textgreater{} genG09       {-}1.86895 {-}3.61008 {-}0.02572    910.3  0.040 *  }
\CommentTok{\#\textgreater{} genG10       {-}0.82455 {-}2.72016  1.07709    833.3  0.336    }
\CommentTok{\#\textgreater{} genG11       {-}1.95852 {-}3.68950 {-}0.11623    812.8  0.024 *  }
\CommentTok{\#\textgreater{} genG12       {-}1.91755 {-}3.65886 {-}0.13231    857.2  0.034 *  }
\CommentTok{\#\textgreater{} genG13       {-}4.39311 {-}6.48591 {-}2.42346    707.8 \textless{}0.001 ***}
\CommentTok{\#\textgreater{} genG14       {-}3.22574 {-}5.33514 {-}1.54308    800.6 \textless{}0.001 ***}
\CommentTok{\#\textgreater{} genG15       {-}2.79275 {-}4.49885 {-}0.91989   1000.0  0.004 ** }
\CommentTok{\#\textgreater{} genG16       {-}3.90025 {-}6.10242 {-}2.10468    804.3  0.004 ** }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

\textbf{MCMC Diagnostics}

\begin{itemize}
\item
  \textbf{Trace Plot:} Checks for chain mixing and convergence.
\item
  \textbf{Autocorrelation Plot:} Evaluates dependency between MCMC samples.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Trace plot for the first fixed effect}
\FunctionTok{plot}\NormalTok{(Bayes\_flymodel}\SpecialCharTok{$}\NormalTok{Sol[, }\DecValTok{1}\NormalTok{],}
     \AttributeTok{main =} \FunctionTok{colnames}\NormalTok{(Bayes\_flymodel}\SpecialCharTok{$}\NormalTok{Sol)[}\DecValTok{1}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{09-nonlinear_generalized_linear_mixed_files/figure-latex/unnamed-chunk-22-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Autocorrelation plot}
\FunctionTok{autocorr.plot}\NormalTok{(Bayes\_flymodel}\SpecialCharTok{$}\NormalTok{Sol[, }\DecValTok{1}\NormalTok{],}
              \AttributeTok{main =} \FunctionTok{colnames}\NormalTok{(Bayes\_flymodel}\SpecialCharTok{$}\NormalTok{Sol)[}\DecValTok{1}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{09-nonlinear_generalized_linear_mixed_files/figure-latex/unnamed-chunk-22-2} \end{center}

Bayesian Interpretation:

\begin{itemize}
\item
  \textbf{Posterior Means:} Represent the central tendency of the parameter estimates.
\item
  \textbf{Credible Intervals:} Unlike frequentist confidence intervals, they can be interpreted directly as the probability that the parameter lies within the interval.
\end{itemize}

\hypertarget{nonlinear-mixed-model-yellow-poplar-data}{%
\subsection{Nonlinear Mixed Model: Yellow Poplar Data}\label{nonlinear-mixed-model-yellow-poplar-data}}

This dataset comes from \citet{Schabenberger_2001}

\hypertarget{data-preparation}{%
\subsubsection{Data Preparation}\label{data-preparation}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat2 }\OtherTok{\textless{}{-}} \FunctionTok{read.table}\NormalTok{(}\StringTok{"images/YellowPoplarData\_r.txt"}\NormalTok{)}
\FunctionTok{names}\NormalTok{(dat2) }\OtherTok{\textless{}{-}}
    \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}tn\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}k\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}dbh\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}totht\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}dob\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}ht\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}maxd\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}cumv\textquotesingle{}}\NormalTok{)}
\NormalTok{dat2}\SpecialCharTok{$}\NormalTok{t }\OtherTok{\textless{}{-}}\NormalTok{ dat2}\SpecialCharTok{$}\NormalTok{dob }\SpecialCharTok{/}\NormalTok{ dat2}\SpecialCharTok{$}\NormalTok{dbh}
\NormalTok{dat2}\SpecialCharTok{$}\NormalTok{r }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ dat2}\SpecialCharTok{$}\NormalTok{dob }\SpecialCharTok{/}\NormalTok{ dat2}\SpecialCharTok{$}\NormalTok{totht}
\end{Highlighting}
\end{Shaded}

\hypertarget{data-visualization-1}{%
\subsubsection{Data Visualization}\label{data-visualization-1}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr)}

\NormalTok{dat2 }\OtherTok{\textless{}{-}}\NormalTok{ dat2 }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{group\_by}\NormalTok{(tn) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{z =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{        totht }\SpecialCharTok{\textless{}} \DecValTok{74} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}a: 0{-}74ft\textquotesingle{}}\NormalTok{,}
\NormalTok{        totht }\SpecialCharTok{\textless{}} \DecValTok{88} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}b: 74{-}88\textquotesingle{}}\NormalTok{,}
\NormalTok{        totht }\SpecialCharTok{\textless{}} \DecValTok{95} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}c: 88{-}95\textquotesingle{}}\NormalTok{,}
\NormalTok{        totht }\SpecialCharTok{\textless{}} \DecValTok{99} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}d: 95{-}99\textquotesingle{}}\NormalTok{,}
\NormalTok{        totht }\SpecialCharTok{\textless{}} \DecValTok{104} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}e: 99{-}104\textquotesingle{}}\NormalTok{,}
\NormalTok{        totht }\SpecialCharTok{\textless{}} \DecValTok{109} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}f: 104{-}109\textquotesingle{}}\NormalTok{,}
\NormalTok{        totht }\SpecialCharTok{\textless{}} \DecValTok{115} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}g: 109{-}115\textquotesingle{}}\NormalTok{,}
\NormalTok{        totht }\SpecialCharTok{\textless{}} \DecValTok{120} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}h: 115{-}120\textquotesingle{}}\NormalTok{,}
\NormalTok{        totht }\SpecialCharTok{\textless{}} \DecValTok{140} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}i: 120{-}150\textquotesingle{}}\NormalTok{,}
        \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}j: 150+\textquotesingle{}}
\NormalTok{    )}
\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(dat2, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ r, }\AttributeTok{y =}\NormalTok{ cumv)) }\SpecialCharTok{+} 
    \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{size =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+} 
    \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(z)) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Cumulative Volume vs. Relative Height by Tree Height Group"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{09-nonlinear_generalized_linear_mixed_files/figure-latex/unnamed-chunk-24-1} \end{center}

\hypertarget{model-specification-3}{%
\subsubsection{Model Specification}\label{model-specification-3}}

The proposed \protect\hyperlink{sec-nonlinear-mixed-models}{Nonlinear Mixed Model} is: \[
V_{ij} = \left(\beta_0 + (\beta_1 + b_{1i})\frac{D_i^2 H_i}{1000}\right) \exp\left[-(\beta_2 + b_{2i}) t_{ij} \exp(\beta_3 t_{ij})\right] + e_{ij}
\] Where:

\begin{itemize}
\item
  \(b_{1i}, b_{2i}\) are random effects for tree \(i\).
\item
  \(e_{ij}\) are residual errors.
\end{itemize}

\hypertarget{fitting-the-nonlinear-mixed-model}{%
\subsubsection{Fitting the Nonlinear Mixed Model}\label{fitting-the-nonlinear-mixed-model}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(nlme)}

\NormalTok{tmp }\OtherTok{\textless{}{-}} \FunctionTok{nlme}\NormalTok{(}
\NormalTok{    cumv }\SpecialCharTok{\textasciitilde{}}\NormalTok{ (b0 }\SpecialCharTok{+}\NormalTok{ (b1 }\SpecialCharTok{+}\NormalTok{ u1) }\SpecialCharTok{*}\NormalTok{ (dbh}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ totht }\SpecialCharTok{/} \DecValTok{1000}\NormalTok{)) }\SpecialCharTok{*} 
        \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{(b2 }\SpecialCharTok{+}\NormalTok{ u2) }\SpecialCharTok{*}\NormalTok{ (t }\SpecialCharTok{/} \DecValTok{1000}\NormalTok{) }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(b3 }\SpecialCharTok{*}\NormalTok{ t)), }
    \AttributeTok{data =}\NormalTok{ dat2,}
    \AttributeTok{fixed =}\NormalTok{ b0 }\SpecialCharTok{+}\NormalTok{ b1 }\SpecialCharTok{+}\NormalTok{ b2 }\SpecialCharTok{+}\NormalTok{ b3 }\SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{,}
    \AttributeTok{random =} \FunctionTok{pdDiag}\NormalTok{(u1 }\SpecialCharTok{+}\NormalTok{ u2 }\SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{),  }\CommentTok{\# Uncorrelated random effects}
    \AttributeTok{groups =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ tn,                 }\CommentTok{\# Grouping by tree}
    \AttributeTok{start =} \FunctionTok{list}\NormalTok{(}\AttributeTok{fixed =} \FunctionTok{c}\NormalTok{(}\AttributeTok{b0 =} \FloatTok{0.25}\NormalTok{, }\AttributeTok{b1 =} \FloatTok{2.3}\NormalTok{, }\AttributeTok{b2 =} \FloatTok{2.87}\NormalTok{, }\AttributeTok{b3 =} \FloatTok{6.7}\NormalTok{))}
\NormalTok{)}
\FunctionTok{summary}\NormalTok{(tmp)}
\CommentTok{\#\textgreater{} Nonlinear mixed{-}effects model fit by maximum likelihood}
\CommentTok{\#\textgreater{}   Model: cumv \textasciitilde{} (b0 + (b1 + u1) * (dbh\^{}2 * totht/1000)) * exp({-}(b2 + u2) *      (t/1000) * exp(b3 * t)) }
\CommentTok{\#\textgreater{}   Data: dat2 }
\CommentTok{\#\textgreater{}        AIC      BIC    logLik}
\CommentTok{\#\textgreater{}   31103.73 31151.33 {-}15544.86}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Random effects:}
\CommentTok{\#\textgreater{}  Formula: list(u1 \textasciitilde{} 1, u2 \textasciitilde{} 1)}
\CommentTok{\#\textgreater{}  Level: tn}
\CommentTok{\#\textgreater{}  Structure: Diagonal}
\CommentTok{\#\textgreater{}                u1       u2 Residual}
\CommentTok{\#\textgreater{} StdDev: 0.1508094 0.447829 2.226361}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Fixed effects:  b0 + b1 + b2 + b3 \textasciitilde{} 1 }
\CommentTok{\#\textgreater{}       Value  Std.Error   DF  t{-}value p{-}value}
\CommentTok{\#\textgreater{} b0 0.249386 0.12894686 6297   1.9340  0.0532}
\CommentTok{\#\textgreater{} b1 2.288832 0.01266804 6297 180.6777  0.0000}
\CommentTok{\#\textgreater{} b2 2.500497 0.05606686 6297  44.5985  0.0000}
\CommentTok{\#\textgreater{} b3 6.848871 0.02140677 6297 319.9395  0.0000}
\CommentTok{\#\textgreater{}  Correlation: }
\CommentTok{\#\textgreater{}    b0     b1     b2    }
\CommentTok{\#\textgreater{} b1 {-}0.639              }
\CommentTok{\#\textgreater{} b2  0.054  0.056       }
\CommentTok{\#\textgreater{} b3 {-}0.011 {-}0.066 {-}0.850}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Standardized Within{-}Group Residuals:}
\CommentTok{\#\textgreater{}           Min            Q1           Med            Q3           Max }
\CommentTok{\#\textgreater{} {-}6.694575e+00 {-}3.081861e{-}01 {-}8.907041e{-}05  3.469469e{-}01  7.855665e+00 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Observations: 6636}
\CommentTok{\#\textgreater{} Number of Groups: 336}
\NormalTok{nlme}\SpecialCharTok{::}\FunctionTok{intervals}\NormalTok{(tmp)}
\CommentTok{\#\textgreater{} Approximate 95\% confidence intervals}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Fixed effects:}
\CommentTok{\#\textgreater{}           lower      est.     upper}
\CommentTok{\#\textgreater{} b0 {-}0.003317833 0.2493858 0.5020894}
\CommentTok{\#\textgreater{} b1  2.264006069 2.2888323 2.3136585}
\CommentTok{\#\textgreater{} b2  2.390620116 2.5004971 2.6103742}
\CommentTok{\#\textgreater{} b3  6.806919325 6.8488712 6.8908232}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Random Effects:}
\CommentTok{\#\textgreater{}   Level: tn }
\CommentTok{\#\textgreater{}            lower      est.     upper}
\CommentTok{\#\textgreater{} sd(u1) 0.1376068 0.1508094 0.1652787}
\CommentTok{\#\textgreater{} sd(u2) 0.4056207 0.4478290 0.4944295}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Within{-}group standard error:}
\CommentTok{\#\textgreater{}    lower     est.    upper }
\CommentTok{\#\textgreater{} 2.187259 2.226361 2.266161}
\end{Highlighting}
\end{Shaded}

\hypertarget{interpretation}{%
\subsubsection{Interpretation:}\label{interpretation}}

\begin{itemize}
\item
  \textbf{Fixed Effects (}\(\beta\)): Describe the average growth pattern across all trees.
\item
  \textbf{Random Effects (}\(b_i\)): Capture tree-specific deviations from the average trend.
\end{itemize}

This result is a bit different from the original study because of different implementation of nonlinear mixed models.

\hypertarget{visualizing-model-predictions}{%
\subsubsection{Visualizing Model Predictions}\label{visualizing-model-predictions}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(cowplot)}

\CommentTok{\# Prediction function}
\NormalTok{nlmmfn }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(fixed, rand, dbh, totht, t) \{}
\NormalTok{    (fixed[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{+}\NormalTok{ (fixed[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{+}\NormalTok{ rand[}\DecValTok{1}\NormalTok{]) }\SpecialCharTok{*}\NormalTok{ (dbh }\SpecialCharTok{\^{}} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ totht }\SpecialCharTok{/} \DecValTok{1000}\NormalTok{)) }\SpecialCharTok{*}
        \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{(fixed[}\DecValTok{3}\NormalTok{] }\SpecialCharTok{+}\NormalTok{ rand[}\DecValTok{2}\NormalTok{]) }\SpecialCharTok{*}\NormalTok{ (t }\SpecialCharTok{/} \DecValTok{1000}\NormalTok{) }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(fixed[}\DecValTok{4}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ t))}
\NormalTok{\}}

\CommentTok{\# Function to generate plots for selected trees}
\NormalTok{plot\_tree }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(tree\_id) \{}
\NormalTok{    pred }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{dob =} \FunctionTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FunctionTok{max}\NormalTok{(dat2}\SpecialCharTok{$}\NormalTok{dob), }\AttributeTok{length.out =} \DecValTok{100}\NormalTok{))}
\NormalTok{    pred}\SpecialCharTok{$}\NormalTok{tn }\OtherTok{\textless{}{-}}\NormalTok{ tree\_id}
\NormalTok{    pred}\SpecialCharTok{$}\NormalTok{dbh }\OtherTok{\textless{}{-}} \FunctionTok{unique}\NormalTok{(dat2}\SpecialCharTok{$}\NormalTok{dbh[dat2}\SpecialCharTok{$}\NormalTok{tn }\SpecialCharTok{==}\NormalTok{ tree\_id])}
\NormalTok{    pred}\SpecialCharTok{$}\NormalTok{t }\OtherTok{\textless{}{-}}\NormalTok{ pred}\SpecialCharTok{$}\NormalTok{dob }\SpecialCharTok{/}\NormalTok{ pred}\SpecialCharTok{$}\NormalTok{dbh}
\NormalTok{    pred}\SpecialCharTok{$}\NormalTok{totht }\OtherTok{\textless{}{-}} \FunctionTok{unique}\NormalTok{(dat2}\SpecialCharTok{$}\NormalTok{totht[dat2}\SpecialCharTok{$}\NormalTok{tn }\SpecialCharTok{==}\NormalTok{ tree\_id])}
\NormalTok{    pred}\SpecialCharTok{$}\NormalTok{r }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ pred}\SpecialCharTok{$}\NormalTok{dob }\SpecialCharTok{/}\NormalTok{ pred}\SpecialCharTok{$}\NormalTok{totht}
    
\NormalTok{    pred}\SpecialCharTok{$}\NormalTok{with\_random }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(tmp, pred)}
\NormalTok{    pred}\SpecialCharTok{$}\NormalTok{without\_random }\OtherTok{\textless{}{-}}
        \FunctionTok{nlmmfn}\NormalTok{(tmp}\SpecialCharTok{$}\NormalTok{coefficients}\SpecialCharTok{$}\NormalTok{fixed,}
               \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{),}
\NormalTok{               pred}\SpecialCharTok{$}\NormalTok{dbh,}
\NormalTok{               pred}\SpecialCharTok{$}\NormalTok{totht,}
\NormalTok{               pred}\SpecialCharTok{$}\NormalTok{t)}
    
    \FunctionTok{ggplot}\NormalTok{(pred) }\SpecialCharTok{+}
        \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ r, }\AttributeTok{y =}\NormalTok{ with\_random, }\AttributeTok{color =} \StringTok{\textquotesingle{}With Random Effects\textquotesingle{}}\NormalTok{)) }\SpecialCharTok{+}
        \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ r, }\AttributeTok{y =}\NormalTok{ without\_random, }\AttributeTok{color =} \StringTok{\textquotesingle{}Without Random Effects\textquotesingle{}}\NormalTok{)) }\SpecialCharTok{+}
        \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{data =}\NormalTok{ dat2[dat2}\SpecialCharTok{$}\NormalTok{tn }\SpecialCharTok{==}\NormalTok{ tree\_id,], }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ r, }\AttributeTok{y =}\NormalTok{ cumv)) }\SpecialCharTok{+}
        \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \FunctionTok{paste}\NormalTok{(}\StringTok{\textquotesingle{}Tree\textquotesingle{}}\NormalTok{, tree\_id), }\AttributeTok{colour =} \StringTok{""}\NormalTok{) }\SpecialCharTok{+}
        \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"bottom"}\NormalTok{)}
\NormalTok{\}}

\CommentTok{\# Plotting for selected trees}
\NormalTok{p1 }\OtherTok{\textless{}{-}} \FunctionTok{plot\_tree}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{p2 }\OtherTok{\textless{}{-}} \FunctionTok{plot\_tree}\NormalTok{(}\DecValTok{151}\NormalTok{)}
\NormalTok{p3 }\OtherTok{\textless{}{-}} \FunctionTok{plot\_tree}\NormalTok{(}\DecValTok{279}\NormalTok{)}

\FunctionTok{plot\_grid}\NormalTok{(p1, p2, p3)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{09-nonlinear_generalized_linear_mixed_files/figure-latex/unnamed-chunk-26-1} \end{center}

\begin{itemize}
\item
  \textbf{Red Line:} Model predictions with tree-specific random effects.
\item
  \textbf{Teal Line:} Model predictions based only on fixed effects (ignoring tree-specific variation).
\item
  \textbf{Dots:} Observed cumulative volume for each tree.
\end{itemize}

\hypertarget{summary-1}{%
\section{Summary}\label{summary-1}}

\includegraphics[width=1\textwidth,height=\textheight]{images/umbrella_of_models.PNG}

As we conclude this exploration of regression analysis, we reflect on the vast landscape we have navigated---from the foundational principles of \textbf{linear regression} to the intricate complexities of \textbf{generalized linear models}, \textbf{linear mixed models}, \textbf{nonlinear mixed models}, and beyond.

Regression analysis is more than a statistical tool; it is a versatile framework that underpins decision-making in disciplines ranging from marketing and finance to healthcare and engineering.

\hypertarget{key-takeaways-1}{%
\subsection{Key Takeaways}\label{key-takeaways-1}}

\begin{itemize}
\item
  \textbf{The Power of Simplicity:} At its core, \textbf{simple linear regression} illustrates how relationships between variables can be modeled with clarity and elegance. Understanding these basics lays the groundwork for more complex techniques.
\item
  \textbf{Beyond Linearity:} \textbf{Nonlinear regression} and \textbf{generalized linear models} extend our analytical capabilities to handle data that defy linear assumptions---whether through curved relationships, non-normal error structures, or diverse outcome distributions.
\item
  \textbf{Accounting for Hierarchies and Dependencies:} Real-world data often exhibit structures such as nested observations or repeated measures. \textbf{Linear mixed models} and \textbf{generalized linear mixed models} enable us to account for both fixed effects and random variability, ensuring robust inferences.
\item
  \textbf{Complex Systems, Flexible Models:} \textbf{Nonlinear mixed models} allow us to capture dynamic, non-linear processes with hierarchical structures.
\end{itemize}

\hypertarget{the-art-and-science-of-regression}{%
\subsection{The Art and Science of Regression}\label{the-art-and-science-of-regression}}

While statistical formulas and algorithms are the backbone of regression analysis, the true art lies in model selection, diagnostic evaluation, and interpretation. No model is inherently perfect; each is an approximation of reality, shaped by the assumptions we make and the data we collect. The most effective analysts are those who approach their models critically---testing assumptions, validating results, and understanding the limitations of their analyses.

\hypertarget{looking-forward}{%
\subsection{Looking Forward}\label{looking-forward}}

The field of regression continues to evolve, with advancements in computational power and methodologies giving rise to \textbf{machine learning algorithms}, \textbf{Bayesian regression techniques}, and \textbf{high-dimensional data analysis}. Although these modern approaches may differ in implementation, many are rooted in the fundamental concepts covered in this book.

As you apply these techniques to your own work, remember that regression is not just about fitting models---it's about \textbf{asking the right questions}, \textbf{interpreting results thoughtfully}, and using data to generate meaningful insights. Whether you're developing marketing strategies, making financial forecasts, or conducting academic research, the tools you've gained here will serve as a strong foundation.

\begin{quote}
In the words of George E.P. Box, \emph{``All models are wrong, but some are useful.''}\\
Our goal as analysts is to find models that are not only useful but also enlightening.
\end{quote}

\hypertarget{part-iii.-ramifications}{%
\part*{III. RAMIFICATIONS}\label{part-iii.-ramifications}}
\addcontentsline{toc}{part}{III. RAMIFICATIONS}

\hypertarget{model-specification-4}{%
\chapter{Model Specification}\label{model-specification-4}}

Test whether underlying assumptions hold true

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{nested-model}{Nested Model} (A1/A3)
\item
  \protect\hyperlink{non-nested-model}{Non-Nested Model} (A1/A3)
\item
  \protect\hyperlink{heteroskedasticity}{Heteroskedasticity} (A4)
\end{itemize}

\hypertarget{nested-model}{%
\section{Nested Model}\label{nested-model}}

\[
\begin{aligned}
y &= \beta_0 + x_1\beta_1 + x_2\beta-2 + x_3\beta_3 + \epsilon & \text{unrestricted model} \\
y &= \beta_0 + x_1\beta_1 + \epsilon & \text{restricted model}
\end{aligned}
\]

Unrestricted model is always longer than the restricted model\\
The restricted model is ``nested'' within the unrestricted model\\
To determine which variables should be included or exclude, we could use the same \protect\hyperlink{sec-wald-test-logistic}{Wald Test}

\textbf{Adjusted} \(R^2\)

\begin{itemize}
\tightlist
\item
  \(R^2\) will always increase with more variables included
\item
  Adjusted \(R^2\) tries to correct by penalizing inclusion of unnecessary variables.
\end{itemize}

\[
\begin{aligned}
{R}^2 &= 1 - \frac{SSR/n}{SST/n} \\
{R}^2_{adj} &= 1 - \frac{SSR/(n-k)}{SST/(n-1)} \\
&= 1 - \frac{(n-1)(1-R^2)}{(n-k)}
\end{aligned}
\]

\begin{itemize}
\tightlist
\item
  \({R}^2_{adj}\) increases if and only if the t-statistic on the additional variable is greater than 1 in absolute value.
\item
  \({R}^2_{adj}\) is valid in models where there is no heteroskedasticity
\item
  there fore it \textbf{should not} be used in determining which variables should be included in the model (the t or F-tests are more appropriate)
\end{itemize}

\hypertarget{chow-test}{%
\subsection{Chow test}\label{chow-test}}

Should we run two different regressions for two groups?

\hypertarget{non-nested-model}{%
\section{Non-Nested Model}\label{non-nested-model}}

compare models with different non-nested specifications

\hypertarget{davidson-mackinnon-test}{%
\subsection{Davidson-Mackinnon test}\label{davidson-mackinnon-test}}

\hypertarget{independent-variable}{%
\subsubsection{Independent Variable}\label{independent-variable}}

Should the independent variables be logged? (decide between non-nested alternatives)

\[
\begin{aligned}
y =  \beta_0 + x_1\beta_1 + x_2\beta_2 + \epsilon && \text{(level eq)} \\
y =  \beta_0 + ln(x_1)\beta_1 + x_2\beta_2 + \epsilon && \text{(log eq)}
\end{aligned}
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Obtain predict outcome when estimating the model in log equation \(\check{y}\) and then estimate the following auxiliary equation,
\end{enumerate}

\[
y = \beta_0 + x_1\beta_1 + x_2\beta_2 + \check{y}\gamma + error
\]

and evaluate the t-statistic for the null hypothesis \(H_0: \gamma = 0\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Obtain predict outcome when estimating the model in the level equation \(\hat{y}\), then estimate the following auxiliary equation,
\end{enumerate}

\[
y = \beta_0 + ln(x_1)\beta_1 + x_2\beta_2 + \check{y}\gamma + error
\]

and evaluate the t-statistic for the null hypothesis \(H_0: \gamma = 0\)

\begin{itemize}
\tightlist
\item
  If you reject the null in the (1) step but fail to reject the null in the second step, then the log equation is preferred.
\item
  If fail to reject the null in the (1) step but reject the null in the (2) step then, level equation is preferred.
\item
  If reject in both steps, then you have statistical evidence that neither model should be used and should re-evaluate the functional form of your model.
\item
  If fail to reject in both steps, you do not have sufficient evidence to prefer one model over the other. You can compare the \(R^2_{adj}\) to choose between the two models.
\end{itemize}

\[
\begin{aligned}
y &= \beta_0 + ln(x)\beta_1 + \epsilon \\
y &= \beta_0 + x(\beta_1) + x^2\beta_2 + \epsilon
\end{aligned}
\]

\begin{itemize}
\tightlist
\item
  Compare which better fits the data
\item
  Compare standard \(R^2\) is unfair because the second model is less parsimonious (more parameters to estimate)
\item
  The \(R_{adj}^2\) will penalize the second model for being less parsimonious + Only valid when there is no heteroskedasticity (\protect\hyperlink{a4-homoskedasticity}{A4} holds)
\item
  Should only compare after a \protect\hyperlink{davidson-mackinnon-test}{Davidson-Mackinnon test}
\end{itemize}

\hypertarget{dependent-variable}{%
\subsubsection{Dependent Variable}\label{dependent-variable}}

\[
\begin{aligned}
y &= \beta_0 + x_1\beta_1 + \epsilon & \text{level eq} \\
ln(y) &= \beta_0 + x_1\beta_1 + \epsilon & \text{log eq} \\
\end{aligned}
\]

\begin{itemize}
\tightlist
\item
  In the level model, regardless of how big y is, x has a constant effect (i.e., one unit change in \(x_1\) results in a \(\beta_1\) unit change in y)
\item
  In the log model, the larger in y is, the effect of x is stronger (i.e., one unit change in \(x_1\) could increase y from 1 to \(1+\beta_1\) or from 100 to 100+100x\(\beta_1\))
\item
  Cannot compare \(R^2\) or \(R^2_{adj}\) because the outcomes are complement different, the scaling is different (SST is different)
\end{itemize}

We need to ``un-transform'' the \(ln(y)\) back to the same scale as y and then compare,

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Estimate the model in the log equation to obtain the predicted outcome \(\hat{ln(y)}\)
\item
  ``Un-transform'' the predicted outcome
\end{enumerate}

\[
\hat{m} = exp(\hat{ln(y)})
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Estimate the following model (without an intercept)
\end{enumerate}

\[
y = \alpha\hat{m} + error
\]

and obtain predicted outcome \(\hat{y}\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Then take the square of the correlation between \(\hat{y}\) and y as a scaled version of the \(R^2\) from the log model that can now compare with the usual \(R^2\) in the level model.
\end{enumerate}

\hypertarget{heteroskedasticity}{%
\section{Heteroskedasticity}\label{heteroskedasticity}}

\begin{itemize}
\item
  Using roust standard errors are always valid
\item
  If there is significant evidence of heteroskedasticity implying \protect\hyperlink{a4-homoskedasticity}{A4} does not hold

  \begin{itemize}
  \tightlist
  \item
    \protect\hyperlink{gauss-markov-theorem}{Gauss-Markov Theorem} no longer holds, OLS is not BLUE.
  \item
    Should consider using a better linear unbiased estimator (\protect\hyperlink{weighted-least-squares}{Weighted Least Squares} or \protect\hyperlink{generalized-least-squares}{Generalized Least Squares})
  \end{itemize}
\end{itemize}

\hypertarget{breusch-pagan-test}{%
\subsection{Breusch-Pagan test}\label{breusch-pagan-test}}

\protect\hyperlink{a4-homoskedasticity}{A4} implies

\[
E(\epsilon_i^2|\mathbf{x_i})=\sigma^2
\]

\[
\epsilon_i^2 = \gamma_0 + x_{i1}\gamma_1 + ... + x_{ik -1}\gamma_{k-1} + error
\]

and determining whether or not \(\mathbf{x}_i\) has any predictive value

\begin{itemize}
\tightlist
\item
  if \(\mathbf{x}_i\) has predictive value, then the variance changes over the levels of \(\mathbf{x}_i\) which is evidence of heteroskedasticity
\item
  if \(\mathbf{x}_i\) does not have predictive value, the variance is constant for all levels of \(\mathbf{x}_i\)
\end{itemize}

The \protect\hyperlink{breusch-pagan-test}{Breusch-Pagan test} for heteroskedasticity would compute the F-test of total significance for the following model

\[
e_i^2 = \gamma_0 + x_{i1}\gamma_1 + ... + x_{ik -1}\gamma_{k-1} + error
\]

A low p-value means we reject the null of homoskedasticity

However, \protect\hyperlink{breusch-pagan-test}{Breusch-Pagan test} cannot detect heteroskedasticity in non-linear form

\hypertarget{white-test}{%
\subsection{White test}\label{white-test}}

test heteroskedasticity would allow for a non-linear relationship by computing the F-test of total significance for the following model (assume there are three independent random variables)

\[
\begin{aligned}
e_i^2 &= \gamma_0 + x_i \gamma_1 + x_{i2}\gamma_2 + x_{i3}\gamma_3 \\
&+ x_{i1}^2\gamma_4 + x_{i2}^2\gamma_5 + x_{i3}^2\gamma_6 \\
&+ (x_{i1} \times x_{i2})\gamma_7 + (x_{i1} \times x_{i3})\gamma_8 + (x_{i2} \times x_{i3})\gamma_9 + error
\end{aligned}
\]

A low p-value means we reject the null of homoskedasticity

Equivalently, we can compute \protect\hyperlink{lagrange-multiplier-score}{LM} as \(LM = nR^2_{e^2}\) where the \(R^2_{e^2}\) come from the regression with the squared residual as the outcome

\begin{itemize}
\tightlist
\item
  The \protect\hyperlink{lagrange-multiplier-score}{LM} statistic has a {[}\(\chi_k^2\){]}{[}Chi-squared{]} distribution
\end{itemize}

\hypertarget{imputation-missing-data}{%
\chapter{Imputation (Missing Data)}\label{imputation-missing-data}}

\hypertarget{introduction-to-missing-data}{%
\section{Introduction to Missing Data}\label{introduction-to-missing-data}}

Missing data is a common problem in statistical analyses and data science, impacting the quality and reliability of insights derived from datasets. One widely used approach to address this issue is \textbf{imputation}, where missing data is replaced with \emph{reasonable estimates}.

\hypertarget{types-of-imputation}{%
\subsection{Types of Imputation}\label{types-of-imputation}}

Imputation can be categorized into:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Unit Imputation}: Replacing an entire missing observation (i.e., all features for a single data point are missing).
\item
  \textbf{Item Imputation}: Replacing missing values for specific variables (features) within a dataset.
\end{enumerate}

While imputation offers a means to make use of incomplete datasets, it has historically been viewed skeptically. This skepticism arises from:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Frequent \textbf{misapplication} of imputation techniques, which can introduce significant \textbf{bias} to estimates.
\item
  Limited \textbf{applicability}, as imputation works well only under certain assumptions about the missing data mechanism and research objectives.
\end{enumerate}

\textbf{Biases in imputation} can arise from various factors, including:

\begin{itemize}
\item
  \textbf{Imputation method}: The chosen method can influence the results and introduce biases.
\item
  \textbf{Missing data mechanism}: The nature of the missing data---whether it is \protect\hyperlink{missing-completely-at-random-mcar}{Missing Completely at Random (MCAR)} or \protect\hyperlink{missing-at-random-mar}{Missing at Random (MAR)}---affects the accuracy of imputation.
\item
  \textbf{Proportion of missing data}: The amount of missing data significantly impacts the reliability of the imputation.
\item
  \textbf{Available information in the dataset}: Limited information reduces the robustness of the imputed values.
\end{itemize}

\hypertarget{when-and-why-to-use-imputation}{%
\subsection{When and Why to Use Imputation}\label{when-and-why-to-use-imputation}}

The appropriateness of imputation depends on the nature of the missing data and the research goal:

\begin{itemize}
\item
  \textbf{Missing Data in the Outcome Variable} (\(y\)): Imputation in such cases is generally problematic, as it can distort statistical models and lead to misleading conclusions. For example, imputing outcomes in regression or classification problems can alter the underlying relationship between the dependent and independent variables.
\item
  \textbf{Missing Data in Predictive Variables} (\(x\)): Imputation is more commonly applied here, especially for \textbf{non-random missing data}. Properly handled, imputation can enable the use of incomplete datasets while minimizing bias.
\end{itemize}

\hypertarget{objectives-of-imputation}{%
\subsubsection{Objectives of Imputation}\label{objectives-of-imputation}}

The utility of imputation methods differs substantially depending on whether the goal of the analysis is \emph{inference/explanation} or \emph{prediction}. Each goal has distinct priorities and tolerances for bias, variance, and assumptions about the missing data mechanism:

\hypertarget{inferenceexplanation}{%
\paragraph{Inference/Explanation}\label{inferenceexplanation}}

In causal inference or explanatory analyses, the primary objective is to ensure valid statistical inference, emphasizing unbiased estimation of parameters and accurate representation of uncertainty. The treatment of missing data must align closely with the assumptions about the mechanism behind the missing data---whether it is Missing Completely at Random (MCAR), Missing at Random (MAR), or Missing Not at Random (MNAR):

\begin{itemize}
\item
  \textbf{Bias Sensitivity:} Inference analyses require that imputed data preserve the integrity of the relationships among variables. Poorly executed imputation can introduce bias, even when it addresses missingness superficially.
\item
  \textbf{Variance and Confidence Intervals:} For inference, the quality of the standard errors, confidence intervals, and test statistics is critical. Naive imputation methods (e.g., mean imputation) often fail to appropriately reflect the uncertainty due to missingness, leading to overconfidence in parameter estimates.
\item
  \textbf{Mechanism Considerations:} Imputation methods, such as multiple imputation (MI), attempt to generate values consistent with the observed data distribution while accounting for missing data uncertainty. However, MI's performance depends heavily on the validity of the MAR assumption. If the missingness mechanism is MNAR and not addressed adequately, the imputed data could yield biased parameter estimates, undermining the purpose of inference.
\end{itemize}

\hypertarget{prediction}{%
\paragraph{Prediction}\label{prediction}}

In predictive modeling, the primary goal is to maximize model accuracy (e.g., minimizing mean squared error for continuous outcomes or maximizing classification accuracy). Here, the focus shifts to optimizing predictive performance rather than ensuring unbiased parameter estimates:

\begin{itemize}
\item
  \textbf{Loss of Information:} Missing data reduces the amount of usable information in a dataset. Imputation allows the model to leverage all available data, rather than excluding incomplete cases via listwise deletion, which can significantly reduce sample size and model performance.
\item
  \textbf{Impact on Model Fit:} In predictive contexts, imputation can reduce standard errors of the predictions and stabilize model coefficients by incorporating plausible estimates for missing values.
\item
  \textbf{Flexibility with Mechanism:} Predictive models are less sensitive to the missing data mechanism than inferential models, as long as the imputed values help reduce variability and align with patterns in the observed data. Methods like K-Nearest Neighbors (KNN), iterative imputation, or even machine learning models (e.g., random forests for imputation) can be valuable, regardless of strict adherence to MAR or MCAR assumptions.
\item
  \textbf{Trade-offs:} Overimputation, where too much noise or complexity is introduced in the imputation process, can harm prediction by introducing artifacts that degrade model generalizability.
\end{itemize}

\hypertarget{key-takeaways-2}{%
\paragraph{Key Takeaways}\label{key-takeaways-2}}

The usefulness of imputation depends on whether the goal of the analysis is \textbf{inference} or \textbf{prediction}:

\begin{itemize}
\item
  \textbf{Inference/Explanation:} The primary concern is valid statistical inference, where biased estimates are unacceptable. Imputation is often of limited value for this purpose, as it may not address the underlying missing data mechanism appropriately \citep{Rubin_1996}.
\item
  \textbf{Prediction:} Imputation can be more useful in predictive modeling, as it reduces the loss of information from incomplete cases. By leveraging observed data, imputation can lower standard errors and improve model accuracy.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{importance-of-missing-data-treatment-in-statistical-modeling}{%
\subsection{Importance of Missing Data Treatment in Statistical Modeling}\label{importance-of-missing-data-treatment-in-statistical-modeling}}

Proper handling of missing data ensures:

\begin{itemize}
\tightlist
\item
  \textbf{Unbiased Estimates:} Avoiding distortions in parameter estimates.
\item
  \textbf{Accurate Standard Errors:} Ensuring valid hypothesis testing and confidence intervals.
\item
  \textbf{Adequate Statistical Power:} Maximizing the use of available data.
\end{itemize}

Ignoring or mishandling missing data can lead to:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Bias:} Systematic errors in parameter estimates, especially under MAR or MNAR mechanisms.
\item
  \textbf{Loss of Power:} Reduced sample size leads to larger standard errors and weaker statistical significance.
\item
  \textbf{Misleading Conclusions:} Over-simplistic imputation methods (e.g., mean substitution) can distort relationships among variables.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{prevalence-of-missing-data-across-domains}{%
\subsection{Prevalence of Missing Data Across Domains}\label{prevalence-of-missing-data-across-domains}}

Missing data affects virtually all fields:

\begin{itemize}
\tightlist
\item
  \textbf{Business:} Non-responses in customer surveys, incomplete sales records, and transactional errors.
\item
  \textbf{Healthcare:} Missing data in electronic health records (EHRs) due to incomplete patient histories or inconsistent data entry.
\item
  \textbf{Social Sciences:} Non-responses or partial responses in large-scale surveys, leading to biased conclusions.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{practical-considerations-for-imputation}{%
\subsection{Practical Considerations for Imputation}\label{practical-considerations-for-imputation}}

\begin{itemize}
\tightlist
\item
  \textbf{Diagnostic Checks:} Always examine the patterns and mechanisms of missing data before applying imputation (\protect\hyperlink{diagnosing-the-missing-data-mechanism}{Diagnosing the Missing Data Mechanism}).
\item
  \textbf{Model Selection:} Align the imputation method with the missing data mechanism and research goal.
\item
  \textbf{Validation:} Assess the impact of imputation on results through sensitivity analyses or cross-validation.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{theoretical-foundations-of-missing-data}{%
\section{Theoretical Foundations of Missing Data}\label{theoretical-foundations-of-missing-data}}

\hypertarget{definition-and-classification-of-missing-data}{%
\subsection{Definition and Classification of Missing Data}\label{definition-and-classification-of-missing-data}}

Missing data refers to the absence of values for some variables in a dataset. The mechanisms underlying missingness significantly impact the validity of statistical analyses and the choice of handling methods. These mechanisms are classified into three categories:

\begin{itemize}
\item
  \protect\hyperlink{missing-completely-at-random-mcar}{Missing Completely at Random} (MCAR): The probability of missingness is independent of both observed and unobserved data. In other words, the missing data occur entirely at random and are unrelated to any values in the dataset.
\item
  \protect\hyperlink{missing-at-random-mar}{Missing at Random} (MAR): The probability of missingness is related to the observed data but not to the missing data itself. This means that, after controlling for observed variables, the missingness is random.
\item
  \protect\hyperlink{missing-not-at-random-mnar}{Missing Not at Random} (MNAR): The probability of missingness depends on unobserved data or the missing values themselves. In this case, the missingness is related to the very information that is missing, making it the most challenging type to handle in analysis.
\end{itemize}

\hypertarget{missing-completely-at-random-mcar}{%
\subsubsection{Missing Completely at Random (MCAR)}\label{missing-completely-at-random-mcar}}

MCAR occurs when the probability of missingness is entirely random and unrelated to either observed or unobserved variables. Under this mechanism, missing data do not introduce bias in parameter estimates when ignored, although statistical efficiency is reduced due to the smaller sample size.

\textbf{Mathematical Definition:} The missingness is independent of all data, both observed and unobserved:

\[
P(Y_{\text{missing}} | Y, X) = P(Y_{\text{missing}})
\]

\textbf{Characteristics of MCAR:}

\begin{itemize}
\tightlist
\item
  Missingness is completely unrelated to both observed and unobserved data.
\item
  Analyses remain unbiased even if missing data are ignored, though they may lack efficiency due to reduced sample size.
\item
  The missing data points represent a random subset of the overall data.
\end{itemize}

\textbf{Examples:}

\begin{itemize}
\tightlist
\item
  A sensor randomly fails at specific time points, unrelated to environmental or operational conditions.
\item
  Survey participants randomly omit responses to certain questions without any systematic pattern.
\end{itemize}

\textbf{Methods for Testing MCAR:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Little's MCAR Test:} A formal statistical test to assess whether data are MCAR. A significant result suggests deviation from MCAR.
\item
  \textbf{Mean Comparison Tests:}

  \begin{itemize}
  \tightlist
  \item
    T-tests or similar approaches compare observed and missing data groups on other variables. Significant differences indicate potential bias.
  \item
    Failure to reject the null hypothesis of no difference does not confirm MCAR but suggests consistency with the MCAR assumption.
  \end{itemize}
\end{enumerate}

\textbf{Handling MCAR:}

Since MCAR data introduce no bias, they can be handled using the following techniques:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Complete Case Analysis (Listwise Deletion):}

  \begin{itemize}
  \tightlist
  \item
    Analyses are performed only on cases with complete data. While unbiased under MCAR, this method reduces sample size and efficiency.
  \end{itemize}
\item
  \textbf{Universal Singular Value Thresholding (USVT):}

  \begin{itemize}
  \tightlist
  \item
    This technique is effective for MCAR data recovery but can only recover the mean structure, not the entire true distribution \citep{chatterjee2015matrix}.
  \end{itemize}
\item
  \textbf{SoftImpute:}

  \begin{itemize}
  \tightlist
  \item
    A matrix completion method useful for some missing data problems but less effective when missingness is not MCAR \citep{hastie2015matrix}.
  \end{itemize}
\item
  \textbf{Synthetic Nearest Neighbor Imputation:}

  \begin{itemize}
  \tightlist
  \item
    A robust method for imputing missing data. While primarily designed for MCAR, it can also handle certain cases of missing not at random (MNAR) \citep{agarwal2023causal}. Available on GitHub: \href{https://github.com/deshen24/syntheticNN}{syntheticNN}.
  \end{itemize}
\end{enumerate}

\textbf{Notes:}

\begin{itemize}
\tightlist
\item
  The ``missingness'' on one variable can be correlated with the ``missingness'' on another variable without violating the MCAR assumption.
\item
  Absence of evidence for bias (e.g., failing to reject a t-test) does not confirm that the data are MCAR.
\end{itemize}

\hypertarget{missing-at-random-mar}{%
\subsubsection{Missing at Random (MAR)}\label{missing-at-random-mar}}

Missing at Random (MAR) occurs when missingness depends on observed variables but not the missing values themselves. This mechanism assumes that observed data provide sufficient information to explain the missingness. In other words, there is a systematic relationship between the propensity of missing values and the observed data, but not the missing data.

\textbf{Mathematical Definition}:

The probability of missingness is conditional only on observed data:

\[
P(Y_{\text{missing}} | Y, X) = P(Y_{\text{missing}} | X)
\]

This implies that whether an observation is missing is unrelated to the missing values themselves but is related to the observed values of other variables.

\textbf{Characteristics of MAR}:

\begin{itemize}
\tightlist
\item
  Missingness is systematically related to observed variables.
\item
  The propensity for a data point to be missing is not related to the missing data but is related to some of the observed data.
\item
  Analyses must account for observed data to mitigate bias.
\end{itemize}

\textbf{Examples}:

\begin{itemize}
\tightlist
\item
  Women are less likely to disclose their weight, but their gender is recorded. In this case, weight is MAR.
\item
  Missing income data is correlated with education, which is observed. For example, individuals with higher education levels might be less likely to reveal their income.
\end{itemize}

\textbf{Challenges in MAR}:

\begin{itemize}
\tightlist
\item
  MAR is weaker than Missing Completely at Random (MCAR).
\item
  It is impossible to directly test for MAR. Evidence for MAR relies on domain expertise and indirect statistical checks rather than direct tests.
\end{itemize}

\textbf{Handling MAR}:

Common methods for handling MAR include:

\begin{itemize}
\item
  \textbf{Multiple Imputation by Chained Equations (MICE):} Iteratively imputes missing values based on observed data.
\item
  \textbf{Maximum Likelihood Estimation:} Estimates model parameters directly while accounting for MAR assumptions.
\item
  \textbf{Regression-Based Imputation:} Predicts missing values using observed covariates.
\end{itemize}

These methods assume that observed variables fully explain the missingness. Effective handling of MAR requires careful modeling and often domain-specific knowledge to validate the assumptions underlying the analysis.

\hypertarget{missing-not-at-random-mnar}{%
\subsubsection{Missing Not at Random (MNAR)}\label{missing-not-at-random-mnar}}

Missing Not at Random (MNAR) is the most complex missing data mechanism. Here, missingness depends on unobserved variables or the values of the missing data themselves. This makes MNAR particularly challenging, as ignoring this dependency introduces significant bias in analyses.

\textbf{Mathematical Definition}:

The probability of missingness depends on the missing values:

\[
P(Y_{\text{missing}} | Y, X) \neq P(Y_{\text{missing}} | X)
\]

\textbf{Characteristics of MNAR}:

\begin{itemize}
\tightlist
\item
  Missingness cannot be fully explained by observed data.
\item
  The cause of missingness is directly related to the unobserved values.
\item
  Ignoring MNAR introduces significant bias in parameter estimates, often leading to invalid conclusions.
\end{itemize}

\textbf{Examples}:

\begin{itemize}
\tightlist
\item
  High-income individuals are less likely to disclose their income, and income itself is unobserved.
\item
  Patients with severe symptoms drop out of a clinical study, leaving their health outcomes unrecorded.
\end{itemize}

\textbf{Challenges in MNAR}:

\begin{itemize}
\tightlist
\item
  MNAR is the most difficult missingness mechanism to address because the missing data mechanism must be explicitly modeled.
\item
  Identifying MNAR often requires domain knowledge and auxiliary information beyond the observed dataset.
\end{itemize}

\textbf{Handling MNAR}:

MNAR requires explicit modeling of the missingness mechanism. Common approaches include:

\begin{itemize}
\item
  \textbf{Heckman Selection Models:} These models explicitly account for the selection process leading to missing data, adjusting for potential bias \citep{Heckman_1976}.
\item
  \textbf{Instrumental Variables:} Variables predictive of missingness but unrelated to the outcome can be used to mitigate bias \citep{sun2018semiparametric, tchetgen2017general}.
\item
  \textbf{Pattern-Mixture Models}: These models separate the data into groups (patterns) based on missingness and model each group separately. They are particularly useful when the relationship between missingness and missing values is complex.
\item
  \textbf{Sensitivity Analysis:} Examines how conclusions change under different assumptions about the missing data mechanism.
\item
  \textbf{Use of Auxiliary Data}

  Auxiliary data refers to external data sources or variables that can help explain the missingness mechanism.

  \begin{itemize}
  \item
    \textbf{Surrogate Variables}: Adding variables that correlate with missing data can improve imputation accuracy and mitigate the MNAR challenge.
  \item
    \textbf{Linking External Datasets}: Merging datasets from different sources can provide additional context or predictors for missingness.
  \item
    \textbf{Applications in Business}: In marketing, customer demographics or transaction histories often serve as auxiliary data to predict missing responses in surveys.
  \end{itemize}
\end{itemize}

Additionally, data collection strategies, such as follow-up surveys or targeted sampling, can help mitigate MNAR effects by collecting information that directly addresses the missingness mechanism. However, such approaches can be resource-intensive and require careful planning.

\hypertarget{missing-data-mechanisms}{%
\subsection{Missing Data Mechanisms}\label{missing-data-mechanisms}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2466}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2466}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2603}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2466}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Mechanism}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Missingness Depends On}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Implications}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Examples}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{MCAR} & Neither observed nor missing data & No bias; simplest to handle; decreases efficiency due to data loss. & Random sensor failure. \\
\textbf{MAR} & Observed data only & Requires observed data to explain missingness; common assumption in imputation methods. & Gender-based missingness of weight. \\
\textbf{MNAR} & Missing data itself or unobserved variables & Requires explicit modeling of the missingness mechanism; significant bias if ignored. & High-income individuals not disclosing income. \\
\end{longtable}

\hypertarget{relationship-between-mechanisms-and-ignorability}{%
\subsection{Relationship Between Mechanisms and Ignorability}\label{relationship-between-mechanisms-and-ignorability}}

The concept of ignorability is central to determining whether the missingness process must be explicitly modeled. Ignorability impacts the choice of methods for handling missing data and whether the missing data mechanism can be safely disregarded or must be explicitly accounted for.

\hypertarget{ignorable-missing-data}{%
\subsubsection{Ignorable Missing Data}\label{ignorable-missing-data}}

Missing data is \textbf{ignorable} under the following conditions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The missing data mechanism is \protect\hyperlink{missing-at-random-mar}{MAR} or \protect\hyperlink{missing-completely-at-random-mcar}{MCAR}.
\item
  The parameters governing the missing data process are unrelated to the parameters of interest in the analysis.
\end{enumerate}

In cases of ignorable missing data, there is no need to model the missingness mechanism explicitly unless you aim to improve the efficiency or precision of parameter estimates. Common imputation techniques, such as multiple imputation or maximum likelihood estimation, rely on the assumption of ignorability to produce unbiased parameter estimates.

\textbf{Practical Considerations for Ignorable Missingness}

Even though ignorable mechanisms simplify analysis, researchers must rigorously assess whether the missingness mechanism meets the MAR or MCAR criteria. Violations can lead to biased results, even if unintentionally overlooked.

For example: A survey on income may assume MAR if missingness is associated with respondent age (observed variable) but not income itself (unobserved variable). However, if income directly influences nonresponse, the assumption of MAR is violated.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{non-ignorable}{%
\subsubsection{Non-Ignorable Missing Data}\label{non-ignorable}}

Missing data is \textbf{non-ignorable} when:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The missingness mechanism depends on the values of the missing data themselves or on unobserved variables.
\item
  The missing data mechanism is related to the parameters of interest, resulting in bias if the mechanism is not modeled explicitly.
\end{enumerate}

This type of missingness (i.e., \protect\hyperlink{missing-not-at-random-mnar}{Missing Not at Random (MNAR)} requires modeling the missing data mechanism directly to produce unbiased estimates.

\textbf{Characteristics of Non-Ignorable Missingness}

\begin{itemize}
\tightlist
\item
  \textbf{Dependence on Missing Values}: The likelihood of missingness is associated with the missing values themselves.

  \begin{itemize}
  \tightlist
  \item
    Example: In a study on health, individuals with more severe conditions are more likely to drop out, leading to an underrepresentation of the sickest individuals in the data.
  \end{itemize}
\item
  \textbf{Bias in Complete Case Analysis}: Analyses based solely on complete cases can lead to substantial bias.

  \begin{itemize}
  \tightlist
  \item
    Example: In income surveys, if wealthier individuals are less likely to report their income, the estimated mean income will be systematically lower than the true population mean.
  \end{itemize}
\item
  \textbf{Need for Explicit Modeling}: To address MNAR, the analyst must model the missing data mechanism. This often involves specifying relationships between observed data, missing data, and the missingness process itself.
\end{itemize}

\hypertarget{implications-of-non-ignorable-missingness}{%
\subsubsection{Implications of Non-Ignorable Missingness}\label{implications-of-non-ignorable-missingness}}

Non-ignorable mechanisms are often associated with sensitive or personal data:

\begin{itemize}
\item
  \textbf{Examples}:

  \begin{itemize}
  \item
    Individuals with lower education levels may omit their education information.
  \item
    Participants with controversial or stigmatized health conditions might opt out of surveys entirely.
  \end{itemize}
\item
  \textbf{Impact on Policy and Decision-Making}:

  \begin{itemize}
  \tightlist
  \item
    Biases introduced by MNAR can have serious consequences for policymaking, such as underestimating the prevalence of poverty or mischaracterizing population health needs.
  \end{itemize}
\end{itemize}

By explicitly addressing non-ignorable missingness, researchers can mitigate biases and ensure that findings accurately reflect the underlying population.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{diagnosing-the-missing-data-mechanism}{%
\section{Diagnosing the Missing Data Mechanism}\label{diagnosing-the-missing-data-mechanism}}

Understanding the mechanism behind missing data is critical to choosing the appropriate methods for handling it. The three main mechanisms for missing data are \textbf{MCAR (Missing Completely at Random)}, \textbf{MAR (Missing at Random)}, and \textbf{MNAR (Missing Not at Random)}. This section discusses methods for diagnosing these mechanisms, including descriptive and inferential approaches.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{descriptive-methods}{%
\subsection{Descriptive Methods}\label{descriptive-methods}}

\hypertarget{visualizing-missing-data-patterns}{%
\subsubsection{Visualizing Missing Data Patterns}\label{visualizing-missing-data-patterns}}

Visualization tools are essential for detecting patterns in missing data. Heatmaps and correlation plots can help identify systematic missingness and provide insights into the underlying mechanism.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example: Visualizing missing data}
\FunctionTok{library}\NormalTok{(Amelia)}
\FunctionTok{missmap}\NormalTok{(}
\NormalTok{    airquality,}
    \AttributeTok{main =} \StringTok{"Missing Data Heatmap"}\NormalTok{,}
    \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"yellow"}\NormalTok{, }\StringTok{"black"}\NormalTok{),}
    \AttributeTok{legend =} \ConstantTok{TRUE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{11-imputation_files/figure-latex/unnamed-chunk-1-1} \end{center}

\begin{itemize}
\item
  \textbf{Heatmaps}: Highlight where missingness occurs in a dataset.
\item
  \textbf{Correlation Plots}: Show relationships between missingness indicators of different variables.
\end{itemize}

\textbf{Exploring Univariate and Multivariate Missingness}

\begin{itemize}
\tightlist
\item
  \textbf{Univariate Analysis}: Calculate the proportion of missing data for each variable.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example: Proportion of missing values}
\NormalTok{missing\_proportions }\OtherTok{\textless{}{-}} \FunctionTok{colSums}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(airquality)) }\SpecialCharTok{/} \FunctionTok{nrow}\NormalTok{(airquality)}
\FunctionTok{print}\NormalTok{(missing\_proportions)}
\CommentTok{\#\textgreater{}      Ozone    Solar.R       Wind       Temp      Month        Day }
\CommentTok{\#\textgreater{} 0.24183007 0.04575163 0.00000000 0.00000000 0.00000000 0.00000000}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \textbf{Multivariate Analysis}: Examine whether missingness in one variable is related to others. This can be visualized using scatterplots of observed vs.~missing values.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example: Missingness correlation}
\FunctionTok{library}\NormalTok{(naniar)}
\FunctionTok{vis\_miss}\NormalTok{(airquality)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{11-imputation_files/figure-latex/unnamed-chunk-3-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gg\_miss\_upset}\NormalTok{(airquality) }\CommentTok{\# Displays a missingness upset plot}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{11-imputation_files/figure-latex/unnamed-chunk-3-2} \end{center}

\hypertarget{statistical-tests-for-missing-data-mechanisms}{%
\subsection{Statistical Tests for Missing Data Mechanisms}\label{statistical-tests-for-missing-data-mechanisms}}

\hypertarget{diagnosing-mcar-littles-test}{%
\subsubsection{Diagnosing MCAR: Little's Test}\label{diagnosing-mcar-littles-test}}

Little's test is a hypothesis test to determine if the missing data mechanism is \textbf{MCAR}. It tests whether the means of observed and missing data are significantly different. The null hypothesis is that the data are MCAR.

\[
\chi^2 = \sum_{i=1}^n \frac{(O_i - E_i)^2}{E_i}
\]

Where:

\begin{itemize}
\item
  \(O_i\)= Observed frequency
\item
  \(E_i\)= Expected frequency under MCAR
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example: Little\textquotesingle{}s test}
\NormalTok{naniar}\SpecialCharTok{::}\FunctionTok{mcar\_test}\NormalTok{(airquality)}
\CommentTok{\#\textgreater{} \# A tibble: 1 x 4}
\CommentTok{\#\textgreater{}   statistic    df p.value missing.patterns}
\CommentTok{\#\textgreater{}       \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}            \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1      35.1    14 0.00142                4}
\NormalTok{misty}\SpecialCharTok{::}\FunctionTok{na.test}\NormalTok{(airquality)}
\CommentTok{\#\textgreater{}  Little\textquotesingle{}s MCAR Test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     n nIncomp nPattern  chi2 df  pval }
\CommentTok{\#\textgreater{}   153      42        4 35.15 14 0.001}
\end{Highlighting}
\end{Shaded}

\hypertarget{diagnosing-mcar-via-dummy-variables}{%
\subsubsection{Diagnosing MCAR via Dummy Variables}\label{diagnosing-mcar-via-dummy-variables}}

Creating a binary indicator for missingness allows you to test whether the presence of missing data is related to observed data. For instance:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Create a dummy variable:

  \begin{itemize}
  \item
    1 = Missing
  \item
    0 = Observed
  \end{itemize}
\item
  Conduct a chi-square test or t-test:

  \begin{itemize}
  \item
    Chi-square: Compare proportions of missingness across groups.
  \item
    T-test: Compare means of (other) observed variables with missingness indicators.
  \end{itemize}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example: Chi{-}square test}
\NormalTok{airquality}\SpecialCharTok{$}\NormalTok{missing\_var }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(}\FunctionTok{ifelse}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(airquality}\SpecialCharTok{$}\NormalTok{Ozone), }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\CommentTok{\# Across groups of months}
\FunctionTok{table}\NormalTok{(airquality}\SpecialCharTok{$}\NormalTok{missing\_var, airquality}\SpecialCharTok{$}\NormalTok{Month)}
\CommentTok{\#\textgreater{}    }
\CommentTok{\#\textgreater{}      5  6  7  8  9}
\CommentTok{\#\textgreater{}   0 26  9 26 26 29}
\CommentTok{\#\textgreater{}   1  5 21  5  5  1}
\FunctionTok{chisq.test}\NormalTok{(}\FunctionTok{table}\NormalTok{(airquality}\SpecialCharTok{$}\NormalTok{missing\_var, airquality}\SpecialCharTok{$}\NormalTok{Month))}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Pearson\textquotesingle{}s Chi{-}squared test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  table(airquality$missing\_var, airquality$Month)}
\CommentTok{\#\textgreater{} X{-}squared = 44.751, df = 4, p{-}value = 4.48e{-}09}

\CommentTok{\# Example: T{-}test (of other variable)}
\FunctionTok{t.test}\NormalTok{(Wind }\SpecialCharTok{\textasciitilde{}}\NormalTok{ missing\_var, }\AttributeTok{data =}\NormalTok{ airquality)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Welch Two Sample t{-}test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  Wind by missing\_var}
\CommentTok{\#\textgreater{} t = {-}0.60911, df = 63.646, p{-}value = 0.5446}
\CommentTok{\#\textgreater{} alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0}
\CommentTok{\#\textgreater{} 95 percent confidence interval:}
\CommentTok{\#\textgreater{}  {-}1.6893132  0.8999377}
\CommentTok{\#\textgreater{} sample estimates:}
\CommentTok{\#\textgreater{} mean in group 0 mean in group 1 }
\CommentTok{\#\textgreater{}        9.862069       10.256757}
\end{Highlighting}
\end{Shaded}

\hypertarget{assessing-mar-and-mnar}{%
\subsection{Assessing MAR and MNAR}\label{assessing-mar-and-mnar}}

\hypertarget{sensitivity-analysis}{%
\subsubsection{Sensitivity Analysis}\label{sensitivity-analysis}}

Sensitivity analysis involves simulating different scenarios of missing data and assessing how the results change. For example, imputing missing values under different assumptions can provide insight into whether the data are MAR or MNAR.

\hypertarget{proxy-variables-and-external-data}{%
\subsubsection{Proxy Variables and External Data}\label{proxy-variables-and-external-data}}

Using proxy variables or external data sources can help assess whether missingness depends on unobserved variables (MNAR). For example, in surveys, follow-ups with non-respondents can reveal systematic differences.

\hypertarget{practical-challenges-in-distinguishing-mar-from-mnar}{%
\subsubsection{Practical Challenges in Distinguishing MAR from MNAR}\label{practical-challenges-in-distinguishing-mar-from-mnar}}

Distinguishing between Missing at Random (MAR) and Missing Not at Random (MNAR) is a critical and challenging task in data analysis. Properly identifying the nature of the missing data has significant implications for the choice of imputation strategies, model robustness, and the validity of conclusions. While statistical tests can sometimes aid in this determination, the process often relies heavily on domain knowledge, intuition, and exploratory analysis. Below, we discuss key considerations and examples that highlight these challenges:

\begin{itemize}
\item
  \textbf{Sensitive Topics}: Missing data related to sensitive or stigmatized topics, such as income, drug use, or health conditions, are often MNAR. For example, individuals with higher incomes might deliberately choose not to report their earnings due to privacy concerns. Similarly, participants in a health survey may avoid answering questions about smoking if they perceive social disapproval. In such cases, the probability of missingness is directly related to the unobserved value itself, making MNAR likely.
\item
  \textbf{Field-Specific Norms}: Understanding norms and typical data collection practices in a specific field can provide insights into missingness patterns. For instance, in marketing surveys, respondents may skip questions about spending habits if they consider the questions intrusive. Prior research or historical data from the same domain can help infer whether missingness is more likely MAR (e.g., random skipping due to survey fatigue) or MNAR (e.g., deliberate omission by higher spenders).
\item
  \textbf{Analyzing Auxiliary Variables}: Leveraging auxiliary variables---those correlated with the missing variable---can help infer the missingness mechanism. For example, if missing income data strongly correlates with employment status, this suggests a MAR mechanism, as the missingness depends on observed variables. However, if missingness persists even after accounting for observable predictors, MNAR might be at play.
\item
  \textbf{Experimental Design and Follow-Up}: In longitudinal studies, dropout rates can signal MAR or MNAR patterns. For example, if dropouts occur disproportionately among participants reporting lower satisfaction in early surveys, this indicates an MNAR mechanism. Designing follow-up surveys to specifically investigate dropout reasons can clarify missingness patterns.
\item
  \textbf{Sensitivity Analysis}: To account for uncertainty in the missingness mechanism, researchers can conduct sensitivity analyses by comparing results under different assumptions (e.g., imputing data using both MAR and MNAR approaches). This process helps to quantify the potential impact of misclassifying the missingness mechanism on study conclusions.
\item
  \textbf{Real-World Examples}:

  \begin{itemize}
  \tightlist
  \item
    In customer feedback surveys, higher ratings might be overrepresented due to non-response bias. Customers with negative experiences might be less likely to complete surveys, leading to an MNAR scenario.
  \item
    In financial reporting, missing audit data might correlate with companies in financial distress, a classic MNAR case where the missingness depends on unobserved financial health metrics.
  \end{itemize}
\end{itemize}

Summary

\begin{itemize}
\item
  \textbf{MCAR}: No pattern in missingness; use Little's test or dummy variable analysis.
\item
  \textbf{MAR}: Missingness related to observed data; requires modeling assumptions or proxy analysis.
\item
  \textbf{MNAR}: Missingness depends on unobserved data; requires external validation or sensitivity analysis.
\end{itemize}

\hypertarget{methods-for-handling-missing-data}{%
\section{Methods for Handling Missing Data}\label{methods-for-handling-missing-data}}

\hypertarget{basic-methods}{%
\subsection{Basic Methods}\label{basic-methods}}

\hypertarget{complete-case-analysis-listwise-deletion}{%
\subsubsection{Complete Case Analysis (Listwise Deletion)}\label{complete-case-analysis-listwise-deletion}}

Listwise deletion retains only cases with complete data for all features, discarding rows with any missing values.

\textbf{Advantages}:

\begin{itemize}
\tightlist
\item
  Universally applicable to various statistical tests (e.g., SEM, multilevel regression).
\item
  When data are Missing Completely at Random (MCAR), parameter estimates and standard errors are unbiased.
\item
  Under specific Missing at Random (MAR) conditions, such as when the probability of missing data depends only on independent variables, listwise deletion can still yield unbiased estimates. For instance, in the model \(y = \beta_{0} + \beta_1X_1 + \beta_2X_2 + \epsilon\), if missingness in \(X_1\) is independent of \(y\) but depends on \(X_1\) and \(X_2\), the estimates remain unbiased \citep{Little_1992}.

  \begin{itemize}
  \tightlist
  \item
    This aligns with principles of stratified sampling, which does not bias estimates.
  \item
    In logistic regression, if missing data depend only on the dependent variable but not on independent variables, listwise deletion produces consistent slope estimates, though the intercept may be biased \citep{Vach_1994}.
  \end{itemize}
\item
  For regression analysis, listwise deletion is more robust than Maximum Likelihood (ML) or Multiple Imputation (MI) when the MAR assumption is violated.
\end{itemize}

\textbf{Disadvantages}:

\begin{itemize}
\tightlist
\item
  Results in larger standard errors compared to advanced methods.
\item
  If data are MAR but not MCAR, biased estimates can occur.
\item
  In non-regression contexts, more sophisticated methods often outperform listwise deletion.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{available-case-analysis-pairwise-deletion}{%
\subsubsection{Available Case Analysis (Pairwise Deletion)}\label{available-case-analysis-pairwise-deletion}}

Pairwise deletion calculates estimates using all available data for each pair of variables, without requiring complete cases. It is particularly suitable for methods like linear regression, factor analysis, and SEM, which rely on correlation or covariance matrices.

\textbf{Advantages}:

\begin{itemize}
\tightlist
\item
  Under MCAR, pairwise deletion produces consistent and unbiased estimates in large samples.
\item
  Compared to listwise deletion \citep{Glasser_1964}:

  \begin{itemize}
  \tightlist
  \item
    When variable correlations are low, pairwise deletion provides more efficient estimates.
  \item
    When correlations are high, listwise deletion becomes more efficient.
  \end{itemize}
\end{itemize}

\textbf{Disadvantages}:

\begin{itemize}
\tightlist
\item
  Yields biased estimates under MAR conditions.
\item
  In small samples, covariance matrices might not be positive definite, rendering coefficient estimation infeasible.
\item
  Software implementation varies in how sample size is handled, potentially affecting standard errors.
\end{itemize}

\textbf{Note}: Carefully review software documentation to understand how sample size is treated, as this influences standard error calculations.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{indicator-method-dummy-variable-adjustment}{%
\subsubsection{Indicator Method (Dummy Variable Adjustment)}\label{indicator-method-dummy-variable-adjustment}}

Also known as the Missing Indicator Method, this approach introduces an additional variable to indicate missingness in the dataset.

\textbf{Implementation}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create an indicator variable:
\end{enumerate}

\[
D =
\begin{cases}
1 & \text{if data on } X \text{ are missing} \\
0 & \text{otherwise}
\end{cases}
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Modify the original variable to accommodate missingness:
\end{enumerate}

\[
X^* =
\begin{cases}
X & \text{if data are available} \\
c & \text{if data are missing}
\end{cases}
\]

\textbf{Note}: A common choice for \(c\) is the mean of \(X\).

\textbf{Interpretation}:

\begin{itemize}
\tightlist
\item
  The coefficient of \(D\) represents the difference in the expected value of \(Y\) between cases with missing data and those without.
\item
  The coefficient of \(X^*\) reflects the effect of \(X\) on \(Y\) for cases with observed data.
\end{itemize}

\textbf{Disadvantages}:

\begin{itemize}
\tightlist
\item
  Produces biased estimates of coefficients, even under MCAR conditions \citep{jones1996indicator}.
\item
  May lead to overinterpretation of the ``missingness effect,'' complicating model interpretation.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{advantages-and-limitations-of-basic-methods}{%
\subsubsection{Advantages and Limitations of Basic Methods}\label{advantages-and-limitations-of-basic-methods}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3750}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3750}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Method}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Advantages}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Disadvantages}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Listwise Deletion} & Simple and universally applicable; unbiased under MCAR; robust in certain MAR scenarios. & Inefficient (larger standard errors); biased under MAR in many cases; discards potentially useful data. \\
\textbf{Pairwise Deletion} & Utilizes all available data; efficient under MCAR with low correlations; avoids discarding all cases. & Biased under MAR; prone to non-positive-definite covariance matrices in small samples. \\
\textbf{Indicator Method} & Simple implementation; explicitly models missingness effect. & Biased even under MCAR; complicates interpretation; may not reflect true underlying relationships. \\
\end{longtable}

\hypertarget{single-imputation-techniques}{%
\subsection{Single Imputation Techniques}\label{single-imputation-techniques}}

Single imputation methods replace missing data with a single value, generating a complete dataset that can be analyzed using standard techniques. While convenient, single imputation generally underestimates variability and risks biasing results.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{deterministic-methods}{%
\subsubsection{Deterministic Methods}\label{deterministic-methods}}

\hypertarget{mean-median-mode-imputation}{%
\paragraph{Mean, Median, Mode Imputation}\label{mean-median-mode-imputation}}

This method replaces missing values with the mean, median, or mode of the observed data.

\textbf{Advantages}:

\begin{itemize}
\tightlist
\item
  Simplicity and ease of implementation.
\item
  Useful for quick exploratory data analysis.
\end{itemize}

\textbf{Disadvantages}:

\begin{itemize}
\tightlist
\item
  \textbf{Bias in Variances and Relationships}: Mean imputation reduces variance and disrupts relationships among variables, leading to biased estimates of variances and covariances \citep{haitovsky1968missing}.
\item
  \textbf{Underestimated Standard Errors}: Results in overly optimistic conclusions and increased risk of Type I errors.
\item
  \textbf{Dependency Structure Ignored}: Particularly problematic in high-dimensional data, as it fails to capture dependencies among features.
\end{itemize}

\hypertarget{forward-and-backward-filling-time-series-contexts}{%
\paragraph{Forward and Backward Filling (Time Series Contexts)}\label{forward-and-backward-filling-time-series-contexts}}

Used in time series analysis, this method replaces missing values using the preceding (forward filling) or succeeding (backward filling) values.

\textbf{Advantages}:

\begin{itemize}
\tightlist
\item
  Simple and preserves temporal ordering.
\item
  Suitable for datasets where adjacent values are strongly correlated.
\end{itemize}

\textbf{Disadvantages}:

\begin{itemize}
\tightlist
\item
  Biased if missingness spans long gaps or occurs systematically.
\item
  Cannot capture trends or changes in the underlying process.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{statistical-prediction-models}{%
\subsubsection{Statistical Prediction Models}\label{statistical-prediction-models}}

\hypertarget{linear-regression-imputation}{%
\paragraph{Linear Regression Imputation}\label{linear-regression-imputation}}

Missing values in a variable are imputed based on a linear regression model using observed values of other variables.

\textbf{Advantages}:

\begin{itemize}
\tightlist
\item
  Preserves relationships between variables.
\item
  More sophisticated than mean or median imputation.
\end{itemize}

\textbf{Disadvantages}:

\begin{itemize}
\tightlist
\item
  Assumes linear relationships, which may not hold in all datasets.
\item
  Fails to capture variability, leading to downwardly biased standard errors.
\end{itemize}

\hypertarget{logistic-regression-for-categorical-variables}{%
\paragraph{Logistic Regression for Categorical Variables}\label{logistic-regression-for-categorical-variables}}

Similar to linear regression imputation but used for categorical variables. The missing category is predicted using a logistic regression model.

\textbf{Advantages}:

\begin{itemize}
\tightlist
\item
  Useful for binary or multinomial categorical data.
\item
  Preserves relationships with other variables.
\end{itemize}

\textbf{Disadvantages}:

\begin{itemize}
\tightlist
\item
  Assumes the underlying logistic model is appropriate.
\item
  Does not account for uncertainty in the imputed values.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{non-parametric-methods}{%
\subsubsection{Non-Parametric Methods}\label{non-parametric-methods}}

\hypertarget{hot-deck-imputation}{%
\paragraph{Hot Deck Imputation}\label{hot-deck-imputation}}

Hot Deck Imputation is a method of handling missing data where missing values are replaced with observed values from ``donor'' cases that are similar in other characteristics. This technique has been widely used in survey data, including by organizations like the U.S. Census Bureau, due to its flexibility and ability to maintain observed data distributions.

\textbf{Advantages of Hot Deck Imputation}

\begin{itemize}
\tightlist
\item
  \textbf{Retains observed data distributions}: Since missing values are imputed using actual observed data, the overall distribution remains realistic.
\item
  \textbf{Flexible}: This method is applicable to both categorical and continuous variables.
\item
  \textbf{Constrained imputations}: Imputed values are always feasible, as they come from observed cases.
\item
  \textbf{Adds variability}: By randomly selecting donors, this method introduces variability, which can aid in robust standard error estimation.
\end{itemize}

\textbf{Disadvantages of Hot Deck Imputation}

\begin{itemize}
\tightlist
\item
  \textbf{Sensitivity to similarity definitions}: The quality of imputed values depends on the criteria used to define similarity between cases.
\item
  \textbf{Computational intensity}: Identifying similar cases and randomly selecting donors can be computationally expensive, especially for large datasets.
\item
  \textbf{Subjectivity}: Deciding how to define ``similar'' can introduce subjectivity or bias.
\end{itemize}

\textbf{Algorithm for Hot Deck Imputation}

Let \(n_1\) represent the number of cases with complete data on the variable \(Y\), and \(n_0\) represent the number of cases with missing data on \(Y\). The steps are as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  From the \(n_1\) cases with complete data, take a random sample (with replacement) of \(n_1\) cases.
\item
  From this sampled pool, take another random sample (with replacement) of size \(n_0\).
\item
  Assign the values from the sampled \(n_0\) cases to the cases with missing data in \(Y\).
\item
  Repeat this process for every variable in the dataset.
\item
  For multiple imputation, repeat the above four steps multiple times to create multiple imputed datasets.
\end{enumerate}

\textbf{Variations and Considerations}

\begin{itemize}
\tightlist
\item
  \textbf{Skipping Step 1}: If Step 1 is skipped, the variability of imputed values is reduced. This approach might not fully account for the uncertainty in missing data, which can underestimate standard errors.
\item
  \textbf{Defining similarity}: A major challenge in this method is deciding what constitutes ``similarity'' between cases. Common approaches include matching based on distance metrics (e.g., Euclidean distance) or grouping cases by strata or clusters.
\end{itemize}

\textbf{Practical Example}

The U.S. Census Bureau employs an approximate Bayesian bootstrap variation of Hot Deck Imputation. In this approach:

\begin{itemize}
\item
  Similar cases are identified based on shared characteristics or grouping variables.
\item
  A randomly chosen value from a similar individual in the sample is used to replace the missing value.
\end{itemize}

This method ensures imputed values are plausible while incorporating variability.

\textbf{Key Notes}

\begin{itemize}
\tightlist
\item
  \textbf{Good aspects}:

  \begin{itemize}
  \tightlist
  \item
    Imputed values are constrained to observed possibilities.
  \item
    Random selection introduces variability, helpful for multiple imputation scenarios.
  \end{itemize}
\item
  \textbf{Challenges}:

  \begin{itemize}
  \tightlist
  \item
    Defining and operationalizing ``similarity'' remains a critical step in applying this method effectively.
  \end{itemize}
\end{itemize}

Below is an example code snippet illustrating Hot Deck Imputation in R:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(Hmisc)}

\CommentTok{\# Example dataset with missing values}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{ID =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{,}
  \AttributeTok{Age =} \FunctionTok{c}\NormalTok{(}\DecValTok{25}\NormalTok{, }\DecValTok{30}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\DecValTok{40}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{60}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\DecValTok{70}\NormalTok{, }\DecValTok{80}\NormalTok{),}
  \AttributeTok{Gender =} \FunctionTok{c}\NormalTok{(}\StringTok{"M"}\NormalTok{, }\StringTok{"F"}\NormalTok{, }\StringTok{"F"}\NormalTok{, }\StringTok{"M"}\NormalTok{, }\StringTok{"M"}\NormalTok{, }\StringTok{"F"}\NormalTok{, }\StringTok{"M"}\NormalTok{, }\StringTok{"F"}\NormalTok{, }\StringTok{"M"}\NormalTok{, }\StringTok{"F"}\NormalTok{)}
\NormalTok{)}

\CommentTok{\# Perform Hot Deck Imputation using Hmisc::impute}
\NormalTok{data}\SpecialCharTok{$}\NormalTok{Age\_imputed }\OtherTok{\textless{}{-}} \FunctionTok{impute}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{Age, }\StringTok{"random"}\NormalTok{)}

\CommentTok{\# Display the imputed dataset}
\FunctionTok{print}\NormalTok{(data)}
\CommentTok{\#\textgreater{}    ID Age Gender Age\_imputed}
\CommentTok{\#\textgreater{} 1   1  25      M          25}
\CommentTok{\#\textgreater{} 2   2  30      F          30}
\CommentTok{\#\textgreater{} 3   3  NA      F          40}
\CommentTok{\#\textgreater{} 4   4  40      M          40}
\CommentTok{\#\textgreater{} 5   5  NA      M          70}
\CommentTok{\#\textgreater{} 6   6  50      F          50}
\CommentTok{\#\textgreater{} 7   7  60      M          60}
\CommentTok{\#\textgreater{} 8   8  NA      F          70}
\CommentTok{\#\textgreater{} 9   9  70      M          70}
\CommentTok{\#\textgreater{} 10 10  80      F          80}
\end{Highlighting}
\end{Shaded}

This code randomly imputes missing values in the \texttt{Age} column based on observed data using the \texttt{Hmisc} package's \texttt{impute} function.

\hypertarget{cold-deck-imputation}{%
\paragraph{Cold Deck Imputation}\label{cold-deck-imputation}}

Cold Deck Imputation is a systematic variant of Hot Deck Imputation where the donor pool is predefined. Instead of selecting donors dynamically from within the same dataset, Cold Deck Imputation relies on an external reference dataset, such as historical data or other high-quality external sources.

\textbf{Advantages of Cold Deck Imputation}

\begin{itemize}
\tightlist
\item
  \textbf{Utilizes high-quality external data}: This method is particularly useful when reliable external reference datasets are available, allowing for accurate and consistent imputations.
\item
  \textbf{Consistency}: If the same donor pool is used across multiple datasets, imputations remain consistent, which can be advantageous in longitudinal studies or standardized processes.
\end{itemize}

\textbf{Disadvantages of Cold Deck Imputation}

\begin{itemize}
\tightlist
\item
  \textbf{Lack of adaptability}: External data may not adequately reflect the unique characteristics or variability of the current dataset.
\item
  \textbf{Potential for systematic bias}: If the donor pool is significantly different from the target dataset, imputations may introduce bias.
\item
  \textbf{Reduces variability}: Unlike Hot Deck Imputation, Cold Deck Imputation systematically selects values, which removes random variation. This can affect the estimation of standard errors and other inferential statistics.
\end{itemize}

\textbf{Key Characteristics}

\begin{itemize}
\tightlist
\item
  \textbf{Systematic Selection}: Cold Deck Imputation selects donor values systematically based on predefined rules or matching criteria, rather than using random sampling.
\item
  \textbf{External Donor Pool}: Donors are typically drawn from a separate dataset or historical records.
\end{itemize}

\textbf{Algorithm for Cold Deck Imputation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Identify an external reference dataset or predefined donor pool.
\item
  Define the matching criteria to find ``similar'' cases between the donor pool and the current dataset (e.g., based on covariates or stratification).
\item
  Systematically assign values from the donor pool to missing values in the current dataset based on the matching criteria.
\item
  Repeat the process for each variable with missing data.
\end{enumerate}

\textbf{Practical Considerations}

\begin{itemize}
\tightlist
\item
  Cold Deck Imputation works well when external data closely resemble the target dataset. However, when there are significant differences in distributions or relationships between variables, imputations may be biased or unrealistic.\\
\item
  This method is less useful for datasets without access to reliable external reference data.
\end{itemize}

Suppose we have a current dataset with missing values and a historical dataset with similar variables. The following example demonstrates how Cold Deck Imputation can be implemented:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Current dataset with missing values}
\NormalTok{current\_data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{ID =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{,}
  \AttributeTok{Age =} \FunctionTok{c}\NormalTok{(}\DecValTok{25}\NormalTok{, }\DecValTok{30}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\DecValTok{45}\NormalTok{, }\ConstantTok{NA}\NormalTok{),}
  \AttributeTok{Gender =} \FunctionTok{c}\NormalTok{(}\StringTok{"M"}\NormalTok{, }\StringTok{"F"}\NormalTok{, }\StringTok{"F"}\NormalTok{, }\StringTok{"M"}\NormalTok{, }\StringTok{"M"}\NormalTok{)}
\NormalTok{)}

\CommentTok{\# External reference dataset (donor pool)}
\NormalTok{reference\_data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{Age =} \FunctionTok{c}\NormalTok{(}\DecValTok{28}\NormalTok{, }\DecValTok{35}\NormalTok{, }\DecValTok{42}\NormalTok{, }\DecValTok{50}\NormalTok{),}
  \AttributeTok{Gender =} \FunctionTok{c}\NormalTok{(}\StringTok{"M"}\NormalTok{, }\StringTok{"F"}\NormalTok{, }\StringTok{"F"}\NormalTok{, }\StringTok{"M"}\NormalTok{)}
\NormalTok{)}

\CommentTok{\# Perform Cold Deck Imputation}
\FunctionTok{library}\NormalTok{(dplyr)}

\CommentTok{\# Define a matching function to find closest donor}
\NormalTok{impute\_cold\_deck }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(missing\_row, reference\_data) \{}
  \CommentTok{\# Filter donors with the same gender}
\NormalTok{  possible\_donors }\OtherTok{\textless{}{-}}\NormalTok{ reference\_data }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{filter}\NormalTok{(Gender }\SpecialCharTok{==}\NormalTok{ missing\_row}\SpecialCharTok{$}\NormalTok{Gender)}
  
  \CommentTok{\# Return the mean age of matching donors as an example of systematic imputation}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{mean}\NormalTok{(possible\_donors}\SpecialCharTok{$}\NormalTok{Age, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{))}
\NormalTok{\}}

\CommentTok{\# Apply Cold Deck Imputation to the missing rows}
\NormalTok{current\_data }\OtherTok{\textless{}{-}}\NormalTok{ current\_data }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rowwise}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{Age\_imputed =} \FunctionTok{ifelse}\NormalTok{(}
      \FunctionTok{is.na}\NormalTok{(Age),}
      \FunctionTok{impute\_cold\_deck}\NormalTok{(}\FunctionTok{cur\_data}\NormalTok{(), reference\_data),}
\NormalTok{      Age}
\NormalTok{    )}
\NormalTok{  )}

\CommentTok{\# Display the imputed dataset}
\FunctionTok{print}\NormalTok{(current\_data)}
\CommentTok{\#\textgreater{} \# A tibble: 5 x 4}
\CommentTok{\#\textgreater{} \# Rowwise: }
\CommentTok{\#\textgreater{}      ID   Age Gender Age\_imputed}
\CommentTok{\#\textgreater{}   \textless{}int\textgreater{} \textless{}dbl\textgreater{} \textless{}chr\textgreater{}        \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1     1    25 M             25  }
\CommentTok{\#\textgreater{} 2     2    30 F             30  }
\CommentTok{\#\textgreater{} 3     3    NA F             38.8}
\CommentTok{\#\textgreater{} 4     4    45 M             45  }
\CommentTok{\#\textgreater{} 5     5    NA M             38.8}
\end{Highlighting}
\end{Shaded}

\textbf{Comparison to Hot Deck Imputation}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3750}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3750}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Feature
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Hot Deck Imputation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Cold Deck Imputation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Donor Pool} & Internal (within the dataset) & External (predefined dataset) \\
\textbf{Selection} & Random & Systematic \\
\textbf{Variability} & Retained & Reduced \\
\textbf{Bias Potential} & Lower & Higher (if donor pool differs) \\
\end{longtable}

This method suits situations where external reference datasets are trusted and representative. However, careful consideration is required to ensure alignment between the donor pool and the target dataset to avoid systematic biases.

\hypertarget{random-draw-from-observed-distribution}{%
\paragraph{Random Draw from Observed Distribution}\label{random-draw-from-observed-distribution}}

This imputation method replaces missing values by randomly sampling from the observed distribution of the variable with missing data. It is a simple, non-parametric approach that retains the variability of the original data.

\textbf{Advantages}

\begin{itemize}
\tightlist
\item
  \textbf{Preserves variability}:

  \begin{itemize}
  \tightlist
  \item
    By randomly drawing values from the observed data, this method ensures that the imputed values reflect the inherent variability of the variable.
  \end{itemize}
\item
  \textbf{Computational simplicity}:

  \begin{itemize}
  \tightlist
  \item
    The process is straightforward and does not require model fitting or complex calculations.
  \end{itemize}
\end{itemize}

\textbf{Disadvantages}

\begin{itemize}
\tightlist
\item
  \textbf{Ignores relationships among variables}:

  \begin{itemize}
  \tightlist
  \item
    Since the imputation is based solely on the observed distribution of the variable, it does not consider relationships or dependencies with other variables.
  \end{itemize}
\item
  \textbf{May not align with trends}:

  \begin{itemize}
  \tightlist
  \item
    Imputed values are random and may fail to align with patterns or trends present in the data, such as time series structures or interactions.
  \end{itemize}
\end{itemize}

\textbf{Steps in Random Draw Imputation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Identify the observed (non-missing) values of the variable.
\item
  For each missing value, randomly sample one value from the observed distribution with or without replacement.
\item
  Replace the missing value with the randomly sampled value.
\end{enumerate}

The following example demonstrates how to use random draw imputation to fill in missing values:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example dataset with missing values}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{ID =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{,}
  \AttributeTok{Value =} \FunctionTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{40}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{60}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\DecValTok{70}\NormalTok{)}
\NormalTok{)}

\CommentTok{\# Perform random draw imputation}
\NormalTok{random\_draw\_impute }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, variable) \{}
\NormalTok{  observed\_values }\OtherTok{\textless{}{-}}\NormalTok{ data[[variable]][}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(data[[variable]])] }\CommentTok{\# Observed values}
\NormalTok{  data[[variable]][}\FunctionTok{is.na}\NormalTok{(data[[variable]])] }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(observed\_values, }
                                                      \FunctionTok{sum}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(data[[variable]])), }
                                                      \AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}
  \FunctionTok{return}\NormalTok{(data)}
\NormalTok{\}}

\CommentTok{\# Apply the imputation}
\NormalTok{imputed\_data }\OtherTok{\textless{}{-}} \FunctionTok{random\_draw\_impute}\NormalTok{(data, }\AttributeTok{variable =} \StringTok{"Value"}\NormalTok{)}

\CommentTok{\# Display the imputed dataset}
\FunctionTok{print}\NormalTok{(imputed\_data)}
\CommentTok{\#\textgreater{}    ID Value}
\CommentTok{\#\textgreater{} 1   1    10}
\CommentTok{\#\textgreater{} 2   2    20}
\CommentTok{\#\textgreater{} 3   3    70}
\CommentTok{\#\textgreater{} 4   4    30}
\CommentTok{\#\textgreater{} 5   5    40}
\CommentTok{\#\textgreater{} 6   6    70}
\CommentTok{\#\textgreater{} 7   7    50}
\CommentTok{\#\textgreater{} 8   8    60}
\CommentTok{\#\textgreater{} 9   9    30}
\CommentTok{\#\textgreater{} 10 10    70}
\end{Highlighting}
\end{Shaded}

\textbf{Considerations}

\begin{itemize}
\item
  \textbf{When to Use}:

  \begin{itemize}
  \tightlist
  \item
    This method is suitable for exploratory analysis or as a quick way to handle missing data in univariate contexts.
  \end{itemize}
\item
  \textbf{Limitations}:

  \begin{itemize}
  \tightlist
  \item
    Random draws may result in values that do not fit well in the broader context of the dataset, especially in cases where the variable has strong relationships with others.
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3056}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3889}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3056}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Feature
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Random Draw from Observed Distribution
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Regression-Based Imputation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Complexity} & Simple & Moderate to High \\
\textbf{Preserves Variability} & Yes & Limited in deterministic forms \\
\textbf{Considers Relationships} & No & Yes \\
\textbf{Risk of Implausible Values} & Low (if observed values are plausible) & Moderate to High \\
\end{longtable}

This method is a quick and computationally efficient way to address missing data but is best complemented by more sophisticated methods when relationships between variables are important.

\hypertarget{semi-parametric-methods}{%
\subsubsection{Semi-Parametric Methods}\label{semi-parametric-methods}}

\hypertarget{predictive-mean-matching-pmm}{%
\paragraph{Predictive Mean Matching (PMM)}\label{predictive-mean-matching-pmm}}

Predictive Mean Matching (PMM) imputes missing values by finding observed values closest in predicted value (based on a regression model) to the missing data. The donor values are then used to fill in the gaps.

\textbf{Advantages}:

\begin{itemize}
\item
  Maintains observed variability in the data.
\item
  Ensures imputed values are realistic since they are drawn from observed data.
\end{itemize}

\textbf{Disadvantages}:

\begin{itemize}
\item
  Requires a suitable predictive model.
\item
  Computationally intensive for large datasets.
\end{itemize}

\textbf{Steps for PMM}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Regress \(Y\) on \(X\) (matrix of covariates) for the \(n_1\) (non-missing cases) to estimate coefficients \(\hat{b}\) and residual variance \(s^2\).
\item
  Draw from the posterior predictive distribution of residual variance: \[s^2_{[1]} = \frac{(n_1-k)s^2}{\chi^2},\] where \(\chi^2\) is a random draw from \(\chi^2_{n_1-k}\).
\item
  Randomly sample from the posterior distribution of \(\hat{b}\): \[b_{[1]} \sim MVN(\hat{b}, s^2_{[1]}(X'X)^{-1}).\]
\item
  Standardize residuals for \(n_1\) cases: \[e_i = \frac{y_i - \hat{b}x_i}{\sqrt{s^2(1-k/n_1)}}.\]
\item
  Randomly draw a sample (with replacement) of \(n_0\) residuals from Step 4.
\item
  Calculate imputed values for \(n_0\) missing cases: \[y_i = b_{[1]}x_i + s_{[1]}e_i.\]
\item
  Repeat Steps 2--6 (except Step 4) to create multiple imputations.
\end{enumerate}

\textbf{Notes}:

\begin{itemize}
\item
  PMM can handle heteroskedasticity
\item
  works for multiple variables, imputing each using all others as predictors.
\end{itemize}

\textbf{Example}:

Example from \href{https://statisticsglobe.com/predictive-mean-matching-imputation-method/}{Statistics Globe}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{) }\CommentTok{\# Seed}
\NormalTok{N  }\OtherTok{\textless{}{-}} \DecValTok{100}                                    \CommentTok{\# Sample size}
\NormalTok{y  }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(}\FunctionTok{runif}\NormalTok{(N,}\SpecialCharTok{{-}}\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{))                 }\CommentTok{\# Target variable Y}
\NormalTok{x1 }\OtherTok{\textless{}{-}}\NormalTok{ y }\SpecialCharTok{+} \FunctionTok{round}\NormalTok{(}\FunctionTok{runif}\NormalTok{(N, }\DecValTok{0}\NormalTok{, }\DecValTok{50}\NormalTok{))              }\CommentTok{\# Auxiliary variable 1}
\NormalTok{x2 }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(y }\SpecialCharTok{+} \FloatTok{0.25} \SpecialCharTok{*}\NormalTok{ x1 }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(N,}\SpecialCharTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{15}\NormalTok{))  }\CommentTok{\# Auxiliary variable 2}
\NormalTok{x3 }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(}\FloatTok{0.1} \SpecialCharTok{*}\NormalTok{ x1 }\SpecialCharTok{+} \FunctionTok{rpois}\NormalTok{(N, }\DecValTok{2}\NormalTok{))           }\CommentTok{\# Auxiliary variable 3}
\CommentTok{\# (categorical variable)}
\NormalTok{x4 }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(}\FunctionTok{round}\NormalTok{(}\FloatTok{0.02} \SpecialCharTok{*}\NormalTok{ y }\SpecialCharTok{+} \FunctionTok{runif}\NormalTok{(N)))   }\CommentTok{\# Auxiliary variable 4 }

\CommentTok{\# Insert 20\% missing data in Y}
\NormalTok{y[}\FunctionTok{rbinom}\NormalTok{(N, }\DecValTok{1}\NormalTok{, }\FloatTok{0.2}\NormalTok{) }\SpecialCharTok{==} \DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NA}               

\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(y, x1, x2, x3, x4)         }\CommentTok{\# Store data in dataset}
\FunctionTok{head}\NormalTok{(data) }\CommentTok{\# First 6 rows of our data}
\CommentTok{\#\textgreater{}    y x1  x2 x3 x4}
\CommentTok{\#\textgreater{} 1 NA 28 {-}10  5  0}
\CommentTok{\#\textgreater{} 2 NA 15  {-}2  2  1}
\CommentTok{\#\textgreater{} 3  1 15 {-}12  6  1}
\CommentTok{\#\textgreater{} 4  8 58  22 10  1}
\CommentTok{\#\textgreater{} 5 NA 26 {-}12  7  0}
\CommentTok{\#\textgreater{} 6 NA 19  36  5  1}

\FunctionTok{library}\NormalTok{(}\StringTok{"mice"}\NormalTok{) }\CommentTok{\# Load mice package}

\DocumentationTok{\#\#\#\#\# Impute data via predictive mean matching (single imputation)\#\#\#\#\#}

\NormalTok{imp\_single }\OtherTok{\textless{}{-}} \FunctionTok{mice}\NormalTok{(data, }\AttributeTok{m =} \DecValTok{1}\NormalTok{, }\AttributeTok{method =} \StringTok{"pmm"}\NormalTok{) }\CommentTok{\# Impute missing values}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  iter imp variable}
\CommentTok{\#\textgreater{}   1   1  y}
\CommentTok{\#\textgreater{}   2   1  y}
\CommentTok{\#\textgreater{}   3   1  y}
\CommentTok{\#\textgreater{}   4   1  y}
\CommentTok{\#\textgreater{}   5   1  y}
\NormalTok{data\_imp\_single }\OtherTok{\textless{}{-}} \FunctionTok{complete}\NormalTok{(imp\_single)         }\CommentTok{\# Store imputed data}
\CommentTok{\# head(data\_imp\_single)}

\CommentTok{\# Since single imputation underestiamtes stnadard errors, }
\CommentTok{\# we use multiple imputaiton}

\DocumentationTok{\#\#\#\#\# Predictive mean matching (multiple imputation) \#\#\#\#\#}

\CommentTok{\# Impute missing values multiple times}
\NormalTok{imp\_multi }\OtherTok{\textless{}{-}} \FunctionTok{mice}\NormalTok{(data, }\AttributeTok{m =} \DecValTok{5}\NormalTok{, }\AttributeTok{method =} \StringTok{"pmm"}\NormalTok{)  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  iter imp variable}
\CommentTok{\#\textgreater{}   1   1  y}
\CommentTok{\#\textgreater{}   1   2  y}
\CommentTok{\#\textgreater{}   1   3  y}
\CommentTok{\#\textgreater{}   1   4  y}
\CommentTok{\#\textgreater{}   1   5  y}
\CommentTok{\#\textgreater{}   2   1  y}
\CommentTok{\#\textgreater{}   2   2  y}
\CommentTok{\#\textgreater{}   2   3  y}
\CommentTok{\#\textgreater{}   2   4  y}
\CommentTok{\#\textgreater{}   2   5  y}
\CommentTok{\#\textgreater{}   3   1  y}
\CommentTok{\#\textgreater{}   3   2  y}
\CommentTok{\#\textgreater{}   3   3  y}
\CommentTok{\#\textgreater{}   3   4  y}
\CommentTok{\#\textgreater{}   3   5  y}
\CommentTok{\#\textgreater{}   4   1  y}
\CommentTok{\#\textgreater{}   4   2  y}
\CommentTok{\#\textgreater{}   4   3  y}
\CommentTok{\#\textgreater{}   4   4  y}
\CommentTok{\#\textgreater{}   4   5  y}
\CommentTok{\#\textgreater{}   5   1  y}
\CommentTok{\#\textgreater{}   5   2  y}
\CommentTok{\#\textgreater{}   5   3  y}
\CommentTok{\#\textgreater{}   5   4  y}
\CommentTok{\#\textgreater{}   5   5  y}
\NormalTok{data\_imp\_multi\_all }\OtherTok{\textless{}{-}}
    \CommentTok{\# Store multiply imputed data}
    \FunctionTok{complete}\NormalTok{(imp\_multi,       }
             \StringTok{"repeated"}\NormalTok{,}
             \AttributeTok{include =} \ConstantTok{TRUE}\NormalTok{)}

\NormalTok{data\_imp\_multi }\OtherTok{\textless{}{-}}
    \CommentTok{\# Combine imputed Y and X1{-}X4 (for convenience)}
    \FunctionTok{data.frame}\NormalTok{(data\_imp\_multi\_all[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{], data[, }\DecValTok{2}\SpecialCharTok{:}\DecValTok{5}\NormalTok{])}

\FunctionTok{head}\NormalTok{(data\_imp\_multi)}
\CommentTok{\#\textgreater{}   y.0 y.1 y.2 y.3 y.4 y.5 x1  x2 x3 x4}
\CommentTok{\#\textgreater{} 1  NA  {-}1   6  {-}1  {-}3   3 28 {-}10  5  0}
\CommentTok{\#\textgreater{} 2  NA {-}10  10   4   0   2 15  {-}2  2  1}
\CommentTok{\#\textgreater{} 3   1   1   1   1   1   1 15 {-}12  6  1}
\CommentTok{\#\textgreater{} 4   8   8   8   8   8   8 58  22 10  1}
\CommentTok{\#\textgreater{} 5  NA   0  {-}1  {-}6   2   0 26 {-}12  7  0}
\CommentTok{\#\textgreater{} 6  NA   4   0   3   3   3 19  36  5  1}
\end{Highlighting}
\end{Shaded}

Example from \href{https://stats.idre.ucla.edu/r/faq/how-do-i-perform-multiple-imputation-using-predictive-mean-matching-in-r/}{UCLA Statistical Consulting}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(mice)}
\FunctionTok{library}\NormalTok{(VIM)}
\FunctionTok{library}\NormalTok{(lattice)}
\FunctionTok{library}\NormalTok{(ggplot2)}
\DocumentationTok{\#\# set observations to NA}
\NormalTok{anscombe }\OtherTok{\textless{}{-}} \FunctionTok{within}\NormalTok{(anscombe, \{}
\NormalTok{    y1[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NA}
\NormalTok{    y4[}\DecValTok{3}\SpecialCharTok{:}\DecValTok{5}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NA}
\NormalTok{\})}
\DocumentationTok{\#\# view}
\FunctionTok{head}\NormalTok{(anscombe)}
\CommentTok{\#\textgreater{}   x1 x2 x3 x4   y1   y2    y3   y4}
\CommentTok{\#\textgreater{} 1 10 10 10  8   NA 9.14  7.46 6.58}
\CommentTok{\#\textgreater{} 2  8  8  8  8   NA 8.14  6.77 5.76}
\CommentTok{\#\textgreater{} 3 13 13 13  8   NA 8.74 12.74   NA}
\CommentTok{\#\textgreater{} 4  9  9  9  8 8.81 8.77  7.11   NA}
\CommentTok{\#\textgreater{} 5 11 11 11  8 8.33 9.26  7.81   NA}
\CommentTok{\#\textgreater{} 6 14 14 14  8 9.96 8.10  8.84 7.04}

\DocumentationTok{\#\# check missing data patterns}
\FunctionTok{md.pattern}\NormalTok{(anscombe)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{11-imputation_files/figure-latex/unnamed-chunk-10-1} \end{center}

\begin{verbatim}
#>   x1 x2 x3 x4 y2 y3 y1 y4  
#> 6  1  1  1  1  1  1  1  1 0
#> 2  1  1  1  1  1  1  1  0 1
#> 2  1  1  1  1  1  1  0  1 1
#> 1  1  1  1  1  1  1  0  0 2
#>    0  0  0  0  0  0  3  3 6

## Number of observations per patterns for all pairs of variables
p <- md.pairs(anscombe)
p 
#> $rr
#>    x1 x2 x3 x4 y1 y2 y3 y4
#> x1 11 11 11 11  8 11 11  8
#> x2 11 11 11 11  8 11 11  8
#> x3 11 11 11 11  8 11 11  8
#> x4 11 11 11 11  8 11 11  8
#> y1  8  8  8  8  8  8  8  6
#> y2 11 11 11 11  8 11 11  8
#> y3 11 11 11 11  8 11 11  8
#> y4  8  8  8  8  6  8  8  8
#> 
#> $rm
#>    x1 x2 x3 x4 y1 y2 y3 y4
#> x1  0  0  0  0  3  0  0  3
#> x2  0  0  0  0  3  0  0  3
#> x3  0  0  0  0  3  0  0  3
#> x4  0  0  0  0  3  0  0  3
#> y1  0  0  0  0  0  0  0  2
#> y2  0  0  0  0  3  0  0  3
#> y3  0  0  0  0  3  0  0  3
#> y4  0  0  0  0  2  0  0  0
#> 
#> $mr
#>    x1 x2 x3 x4 y1 y2 y3 y4
#> x1  0  0  0  0  0  0  0  0
#> x2  0  0  0  0  0  0  0  0
#> x3  0  0  0  0  0  0  0  0
#> x4  0  0  0  0  0  0  0  0
#> y1  3  3  3  3  0  3  3  2
#> y2  0  0  0  0  0  0  0  0
#> y3  0  0  0  0  0  0  0  0
#> y4  3  3  3  3  2  3  3  0
#> 
#> $mm
#>    x1 x2 x3 x4 y1 y2 y3 y4
#> x1  0  0  0  0  0  0  0  0
#> x2  0  0  0  0  0  0  0  0
#> x3  0  0  0  0  0  0  0  0
#> x4  0  0  0  0  0  0  0  0
#> y1  0  0  0  0  3  0  0  1
#> y2  0  0  0  0  0  0  0  0
#> y3  0  0  0  0  0  0  0  0
#> y4  0  0  0  0  1  0  0  3
\end{verbatim}

\begin{itemize}
\tightlist
\item
  \texttt{rr} = number of observations where both pairs of values are observed
\item
  \texttt{rm} = the number of observations where both variables are missing values
\item
  \texttt{mr} = the number of observations where the first variable's value (e.g.~the row variable) is observed and second (or column) variable is missing
\item
  \texttt{mm} = the number of observations where the second variable's value (e.g.~the col variable) is observed and first (or row) variable is missing
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# Margin plot of y1 and y4}
\FunctionTok{marginplot}\NormalTok{(anscombe[}\FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{8}\NormalTok{)], }\AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\StringTok{"red"}\NormalTok{, }\StringTok{"orange"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{11-imputation_files/figure-latex/unnamed-chunk-11-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\DocumentationTok{\#\# 5 imputations for all missing values}
\NormalTok{imp1 }\OtherTok{\textless{}{-}} \FunctionTok{mice}\NormalTok{(anscombe, }\AttributeTok{m =} \DecValTok{5}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  iter imp variable}
\CommentTok{\#\textgreater{}   1   1  y1  y4}
\CommentTok{\#\textgreater{}   1   2  y1  y4}
\CommentTok{\#\textgreater{}   1   3  y1  y4}
\CommentTok{\#\textgreater{}   1   4  y1  y4}
\CommentTok{\#\textgreater{}   1   5  y1  y4}
\CommentTok{\#\textgreater{}   2   1  y1  y4}
\CommentTok{\#\textgreater{}   2   2  y1  y4}
\CommentTok{\#\textgreater{}   2   3  y1  y4}
\CommentTok{\#\textgreater{}   2   4  y1  y4}
\CommentTok{\#\textgreater{}   2   5  y1  y4}
\CommentTok{\#\textgreater{}   3   1  y1  y4}
\CommentTok{\#\textgreater{}   3   2  y1  y4}
\CommentTok{\#\textgreater{}   3   3  y1  y4}
\CommentTok{\#\textgreater{}   3   4  y1  y4}
\CommentTok{\#\textgreater{}   3   5  y1  y4}
\CommentTok{\#\textgreater{}   4   1  y1  y4}
\CommentTok{\#\textgreater{}   4   2  y1  y4}
\CommentTok{\#\textgreater{}   4   3  y1  y4}
\CommentTok{\#\textgreater{}   4   4  y1  y4}
\CommentTok{\#\textgreater{}   4   5  y1  y4}
\CommentTok{\#\textgreater{}   5   1  y1  y4}
\CommentTok{\#\textgreater{}   5   2  y1  y4}
\CommentTok{\#\textgreater{}   5   3  y1  y4}
\CommentTok{\#\textgreater{}   5   4  y1  y4}
\CommentTok{\#\textgreater{}   5   5  y1  y4}

\DocumentationTok{\#\# linear regression for each imputed data set {-} 5 regression are run}
\NormalTok{fitm }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(imp1, }\FunctionTok{lm}\NormalTok{(y1 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ y4 }\SpecialCharTok{+}\NormalTok{ x1))}
\FunctionTok{summary}\NormalTok{(fitm)}
\CommentTok{\#\textgreater{} \# A tibble: 15 x 6}
\CommentTok{\#\textgreater{}    term        estimate std.error statistic p.value  nobs}
\CommentTok{\#\textgreater{}    \textless{}chr\textgreater{}          \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{} \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{}  1 (Intercept)    7.33      2.44       3.01  0.0169    11}
\CommentTok{\#\textgreater{}  2 y4            {-}0.416     0.223     {-}1.86  0.0996    11}
\CommentTok{\#\textgreater{}  3 x1             0.371     0.141      2.63  0.0302    11}
\CommentTok{\#\textgreater{}  4 (Intercept)    7.27      2.90       2.51  0.0365    11}
\CommentTok{\#\textgreater{}  5 y4            {-}0.435     0.273     {-}1.59  0.150     11}
\CommentTok{\#\textgreater{}  6 x1             0.387     0.160      2.41  0.0422    11}
\CommentTok{\#\textgreater{}  7 (Intercept)    6.54      2.80       2.33  0.0479    11}
\CommentTok{\#\textgreater{}  8 y4            {-}0.322     0.255     {-}1.26  0.243     11}
\CommentTok{\#\textgreater{}  9 x1             0.362     0.156      2.32  0.0491    11}
\CommentTok{\#\textgreater{} 10 (Intercept)    5.93      3.08       1.92  0.0907    11}
\CommentTok{\#\textgreater{} 11 y4            {-}0.286     0.282     {-}1.02  0.339     11}
\CommentTok{\#\textgreater{} 12 x1             0.418     0.176      2.37  0.0451    11}
\CommentTok{\#\textgreater{} 13 (Intercept)    8.16      2.67       3.05  0.0158    11}
\CommentTok{\#\textgreater{} 14 y4            {-}0.489     0.251     {-}1.95  0.0867    11}
\CommentTok{\#\textgreater{} 15 x1             0.326     0.151      2.17  0.0622    11}

\DocumentationTok{\#\# pool coefficients and standard errors across all 5 regression models}
\FunctionTok{pool}\NormalTok{(fitm)}
\CommentTok{\#\textgreater{} Class: mipo    m = 5 }
\CommentTok{\#\textgreater{}          term m   estimate       ubar           b          t dfcom       df}
\CommentTok{\#\textgreater{} 1 (Intercept) 5  7.0445966 7.76794670 0.719350800 8.63116766     8 5.805314}
\CommentTok{\#\textgreater{} 2          y4 5 {-}0.3896685 0.06634920 0.006991497 0.07473900     8 5.706243}
\CommentTok{\#\textgreater{} 3          x1 5  0.3727865 0.02473847 0.001134293 0.02609962     8 6.178032}
\CommentTok{\#\textgreater{}          riv     lambda       fmi}
\CommentTok{\#\textgreater{} 1 0.11112601 0.10001207 0.3044313}
\CommentTok{\#\textgreater{} 2 0.12644909 0.11225460 0.3161877}
\CommentTok{\#\textgreater{} 3 0.05502168 0.05215218 0.2586992}

\DocumentationTok{\#\# output parameter estimates}
\FunctionTok{summary}\NormalTok{(}\FunctionTok{pool}\NormalTok{(fitm))}
\CommentTok{\#\textgreater{}          term   estimate std.error statistic       df    p.value}
\CommentTok{\#\textgreater{} 1 (Intercept)  7.0445966 2.9378849  2.397846 5.805314 0.05483678}
\CommentTok{\#\textgreater{} 2          y4 {-}0.3896685 0.2733843 {-}1.425350 5.706243 0.20638512}
\CommentTok{\#\textgreater{} 3          x1  0.3727865 0.1615538  2.307508 6.178032 0.05923999}
\end{Highlighting}
\end{Shaded}

\hypertarget{stochastic-imputation}{%
\paragraph{Stochastic Imputation}\label{stochastic-imputation}}

Stochastic Imputation is an enhancement of regression imputation that introduces randomness into the imputation process by adding a random residual to the predicted values from a regression model. This approach aims to retain the variability of the original data while reducing the bias introduced by deterministic regression imputation.

Stochastic Imputation can be described as:

\[
\text{Imputed Value} = \text{Predicted Value (from regression)} + \text{Random Residual}
\]

This method is commonly used as a foundation for multiple imputation techniques.

\textbf{Advantages of Stochastic Imputation}

\begin{itemize}
\tightlist
\item
  \textbf{Retains all the benefits of regression imputation}:

  \begin{itemize}
  \tightlist
  \item
    Preserves relationships between variables in the dataset.
  \item
    Utilizes information from observed data to inform imputations.
  \end{itemize}
\item
  \textbf{Introduces randomness}:

  \begin{itemize}
  \tightlist
  \item
    Adds variability by including a random residual term, making imputed values more realistic and better representing the uncertainty of missing data.
  \end{itemize}
\item
  \textbf{Supports multiple imputation}:

  \begin{itemize}
  \tightlist
  \item
    By generating different random residuals for each iteration, it facilitates the creation of multiple plausible datasets for robust statistical analysis.
  \end{itemize}
\end{itemize}

\textbf{Disadvantages of Stochastic Imputation}

\begin{itemize}
\tightlist
\item
  \textbf{Implausible values}:

  \begin{itemize}
  \tightlist
  \item
    Depending on the random residuals, imputed values may fall outside the plausible range (e.g., negative values for variables like age or income).
  \end{itemize}
\item
  \textbf{Cannot handle heteroskedasticity}:

  \begin{itemize}
  \tightlist
  \item
    If the data exhibit heteroskedasticity (i.e., non-constant variance of residuals), the randomness added by stochastic imputation may not accurately reflect the underlying variability.
  \end{itemize}
\end{itemize}

\textbf{Steps in Stochastic Imputation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Fit a regression model using cases with complete data for the variable with missing values.
\item
  Predict missing values using the fitted model.
\item
  Generate random residuals based on the distribution of residuals from the regression model.
\item
  Add the random residuals to the predicted values to impute missing values.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example dataset with missing values}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{X =} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{10}\NormalTok{, }\AttributeTok{mean =} \DecValTok{50}\NormalTok{, }\AttributeTok{sd =} \DecValTok{10}\NormalTok{),}
  \AttributeTok{Y =} \FunctionTok{c}\NormalTok{(}\DecValTok{100}\NormalTok{, }\DecValTok{105}\NormalTok{, }\DecValTok{110}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\DecValTok{120}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\DecValTok{130}\NormalTok{, }\DecValTok{135}\NormalTok{, }\DecValTok{140}\NormalTok{, }\ConstantTok{NA}\NormalTok{)}
\NormalTok{)}

\CommentTok{\# Perform stochastic imputation}
\NormalTok{stochastic\_impute }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, predictor, target) \{}
  \CommentTok{\# Subset data with complete cases}
\NormalTok{  complete\_data }\OtherTok{\textless{}{-}}\NormalTok{ data[}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(data[[target]]), ]}
  
  \CommentTok{\# Fit a regression model}
\NormalTok{  model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\FunctionTok{as.formula}\NormalTok{(}\FunctionTok{paste}\NormalTok{(target, }\StringTok{"\textasciitilde{}"}\NormalTok{, predictor)), }\AttributeTok{data =}\NormalTok{ complete\_data)}
  
  \CommentTok{\# Predict missing values}
\NormalTok{  missing\_data }\OtherTok{\textless{}{-}}\NormalTok{ data[}\FunctionTok{is.na}\NormalTok{(data[[target]]), ]}
\NormalTok{  predictions }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(model, }\AttributeTok{newdata =}\NormalTok{ missing\_data)}
  
  \CommentTok{\# Add random residuals}
\NormalTok{  residual\_sd }\OtherTok{\textless{}{-}} \FunctionTok{sd}\NormalTok{(model}\SpecialCharTok{$}\NormalTok{residuals, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{  stochastic\_values }\OtherTok{\textless{}{-}}\NormalTok{ predictions }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{length}\NormalTok{(predictions), }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =}\NormalTok{ residual\_sd)}
  
  \CommentTok{\# Impute missing values}
\NormalTok{  data[}\FunctionTok{is.na}\NormalTok{(data[[target]]), target] }\OtherTok{\textless{}{-}}\NormalTok{ stochastic\_values}
  \FunctionTok{return}\NormalTok{(data)}
\NormalTok{\}}

\CommentTok{\# Apply stochastic imputation}
\NormalTok{imputed\_data }\OtherTok{\textless{}{-}} \FunctionTok{stochastic\_impute}\NormalTok{(data, }\AttributeTok{predictor =} \StringTok{"X"}\NormalTok{, }\AttributeTok{target =} \StringTok{"Y"}\NormalTok{)}

\CommentTok{\# Display the imputed dataset}
\FunctionTok{print}\NormalTok{(imputed\_data)}
\end{Highlighting}
\end{Shaded}

Notes

\begin{itemize}
\item
  \textbf{Multiple Imputation}: Most multiple imputation methods are extensions of stochastic regression imputation. By repeating the imputation process with different random seeds, multiple datasets can be generated to account for uncertainty in the imputed values.
\item
  \textbf{Dealing with Implausible Values}: Additional constraints or transformations (e.g., truncating imputed values to a plausible range) may be necessary to address the issue of implausible values.
\end{itemize}

\textbf{Comparison to Deterministic Regression Imputation}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3472}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4028}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2500}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Feature
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Deterministic Regression Imputation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Stochastic Imputation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Randomness} & None & Adds random residuals \\
\textbf{Preserves Variability} & No & Yes \\
\textbf{Use in Multiple Imputation} & Limited & Well-suited \\
\textbf{Bias Potential} & Higher & Lower \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Income data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)                              }\CommentTok{\# Set seed}
\NormalTok{N }\OtherTok{\textless{}{-}} \DecValTok{1000}                                    \CommentTok{\# Sample size}

\NormalTok{income }\OtherTok{\textless{}{-}}
  \FunctionTok{round}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(N, }\DecValTok{0}\NormalTok{, }\DecValTok{500}\NormalTok{))            }\CommentTok{\# Create some synthetic income data}
\NormalTok{income[income }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ income[income }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ (}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{)}

\NormalTok{x1 }\OtherTok{\textless{}{-}}\NormalTok{ income }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(N, }\DecValTok{1000}\NormalTok{, }\DecValTok{1500}\NormalTok{)          }\CommentTok{\# Auxiliary variables}
\NormalTok{x2 }\OtherTok{\textless{}{-}}\NormalTok{ income }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(N,}\SpecialCharTok{{-}}\DecValTok{5000}\NormalTok{, }\DecValTok{2000}\NormalTok{)}


\CommentTok{\# Create 10\% missingness in income}
\NormalTok{income[}\FunctionTok{rbinom}\NormalTok{(N, }\DecValTok{1}\NormalTok{, }\FloatTok{0.1}\NormalTok{) }\SpecialCharTok{==} \DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NA}

\NormalTok{data\_inc\_miss }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(income, x1, x2)}
\end{Highlighting}
\end{Shaded}

Single stochastic regression imputation

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{imp\_inc\_sri  }\OtherTok{\textless{}{-}} \FunctionTok{mice}\NormalTok{(data\_inc\_miss, }\AttributeTok{method =} \StringTok{"norm.nob"}\NormalTok{, }\AttributeTok{m =} \DecValTok{1}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  iter imp variable}
\CommentTok{\#\textgreater{}   1   1  income}
\CommentTok{\#\textgreater{}   2   1  income}
\CommentTok{\#\textgreater{}   3   1  income}
\CommentTok{\#\textgreater{}   4   1  income}
\CommentTok{\#\textgreater{}   5   1  income}
\NormalTok{data\_inc\_sri }\OtherTok{\textless{}{-}} \FunctionTok{complete}\NormalTok{(imp\_inc\_sri)}
\end{Highlighting}
\end{Shaded}

Single predictive mean matching

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{imp\_inc\_pmm  }\OtherTok{\textless{}{-}} \FunctionTok{mice}\NormalTok{(data\_inc\_miss, }\AttributeTok{method =} \StringTok{"pmm"}\NormalTok{, }\AttributeTok{m =} \DecValTok{1}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  iter imp variable}
\CommentTok{\#\textgreater{}   1   1  income}
\CommentTok{\#\textgreater{}   2   1  income}
\CommentTok{\#\textgreater{}   3   1  income}
\CommentTok{\#\textgreater{}   4   1  income}
\CommentTok{\#\textgreater{}   5   1  income}
\NormalTok{data\_inc\_pmm }\OtherTok{\textless{}{-}} \FunctionTok{complete}\NormalTok{(imp\_inc\_pmm)}
\end{Highlighting}
\end{Shaded}

Stochastic regression imputation contains negative values

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_inc\_sri}\SpecialCharTok{$}\NormalTok{income[data\_inc\_sri}\SpecialCharTok{$}\NormalTok{income }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{]}
\CommentTok{\#\textgreater{}  [1]  {-}23.85404  {-}58.37790  {-}61.86396  {-}57.47909  {-}21.29221  {-}73.26549}
\CommentTok{\#\textgreater{}  [7]  {-}61.76194  {-}42.45942 {-}351.02991 {-}317.69090}
\CommentTok{\# No values below 0}
\NormalTok{data\_inc\_pmm}\SpecialCharTok{$}\NormalTok{income[data\_inc\_pmm}\SpecialCharTok{$}\NormalTok{income }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{] }
\CommentTok{\#\textgreater{} numeric(0)}
\end{Highlighting}
\end{Shaded}

Evidence for heteroskadastic data

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Heteroscedastic data}
 
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)                             }\CommentTok{\# Set seed}
\NormalTok{N }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\DecValTok{1000}                                  \CommentTok{\# Sample size}
 
\NormalTok{a }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{b }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{sigma2 }\OtherTok{\textless{}{-}}\NormalTok{ N}\SpecialCharTok{\^{}}\DecValTok{2}
\NormalTok{eps }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(N, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \FunctionTok{sqrt}\NormalTok{(sigma2))}
 
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ a }\SpecialCharTok{+}\NormalTok{ b }\SpecialCharTok{*}\NormalTok{ N }\SpecialCharTok{+}\NormalTok{ eps                         }\CommentTok{\# Heteroscedastic variable}
\NormalTok{x }\OtherTok{\textless{}{-}} \DecValTok{30} \SpecialCharTok{*}\NormalTok{ N }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(N[}\FunctionTok{length}\NormalTok{(N)], }\DecValTok{1000}\NormalTok{, }\DecValTok{200}\NormalTok{) }\CommentTok{\# Correlated variable}
 
\NormalTok{y[}\FunctionTok{rbinom}\NormalTok{(N[}\FunctionTok{length}\NormalTok{(N)], }\DecValTok{1}\NormalTok{, }\FloatTok{0.3}\NormalTok{) }\SpecialCharTok{==} \DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NA}   \CommentTok{\# 30\% missing}
 
\NormalTok{data\_het\_miss }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(y, x)}
\end{Highlighting}
\end{Shaded}

Single stochastic regression imputation

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{imp\_het\_sri  }\OtherTok{\textless{}{-}} \FunctionTok{mice}\NormalTok{(data\_het\_miss, }\AttributeTok{method =} \StringTok{"norm.nob"}\NormalTok{, }\AttributeTok{m =} \DecValTok{1}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  iter imp variable}
\CommentTok{\#\textgreater{}   1   1  y}
\CommentTok{\#\textgreater{}   2   1  y}
\CommentTok{\#\textgreater{}   3   1  y}
\CommentTok{\#\textgreater{}   4   1  y}
\CommentTok{\#\textgreater{}   5   1  y}
\NormalTok{data\_het\_sri }\OtherTok{\textless{}{-}} \FunctionTok{complete}\NormalTok{(imp\_het\_sri)}
\end{Highlighting}
\end{Shaded}

Single predictive mean matching

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{imp\_het\_pmm  }\OtherTok{\textless{}{-}} \FunctionTok{mice}\NormalTok{(data\_het\_miss, }\AttributeTok{method =} \StringTok{"pmm"}\NormalTok{, }\AttributeTok{m =} \DecValTok{1}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  iter imp variable}
\CommentTok{\#\textgreater{}   1   1  y}
\CommentTok{\#\textgreater{}   2   1  y}
\CommentTok{\#\textgreater{}   3   1  y}
\CommentTok{\#\textgreater{}   4   1  y}
\CommentTok{\#\textgreater{}   5   1  y}
\NormalTok{data\_het\_pmm }\OtherTok{\textless{}{-}} \FunctionTok{complete}\NormalTok{(imp\_het\_pmm)}
\end{Highlighting}
\end{Shaded}

Comparison between predictive mean matching and stochastic regression imputation

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))                              }\CommentTok{\# Both plots in one graphic}

\CommentTok{\# Plot of observed values}
\FunctionTok{plot}\NormalTok{(x[}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(data\_het\_sri}\SpecialCharTok{$}\NormalTok{y)],}
\NormalTok{     data\_het\_sri}\SpecialCharTok{$}\NormalTok{y[}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(data\_het\_sri}\SpecialCharTok{$}\NormalTok{y)],}
     \AttributeTok{main =} \StringTok{""}\NormalTok{,}
     \AttributeTok{xlab =} \StringTok{"X"}\NormalTok{,}
     \AttributeTok{ylab =} \StringTok{"Y"}\NormalTok{)}
\CommentTok{\# Plot of missing values}
\FunctionTok{points}\NormalTok{(x[}\FunctionTok{is.na}\NormalTok{(y)], data\_het\_sri}\SpecialCharTok{$}\NormalTok{y[}\FunctionTok{is.na}\NormalTok{(y)],}
       \AttributeTok{col =} \StringTok{"red"}\NormalTok{)}

\CommentTok{\# Title of plot}
\FunctionTok{title}\NormalTok{(}\StringTok{"Stochastic Regression Imputation"}\NormalTok{,        }
      \AttributeTok{line =} \FloatTok{0.5}\NormalTok{)}

\CommentTok{\# Regression line}
\FunctionTok{abline}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, data\_het\_sri),                   }
       \AttributeTok{col =} \StringTok{"\#1b98e0"}\NormalTok{, }\AttributeTok{lwd =} \FloatTok{2.5}\NormalTok{)}

\CommentTok{\# Legend}
\FunctionTok{legend}\NormalTok{(}
  \StringTok{"topleft"}\NormalTok{,}
  \FunctionTok{c}\NormalTok{(}\StringTok{"Observed Values"}\NormalTok{, }\StringTok{"Imputed Values"}\NormalTok{, }\StringTok{"Regression Y \textasciitilde{} X"}\NormalTok{),}
  \AttributeTok{pch =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\ConstantTok{NA}\NormalTok{),}
  \AttributeTok{lty =} \FunctionTok{c}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\DecValTok{1}\NormalTok{),}
  \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"red"}\NormalTok{, }\StringTok{"\#1b98e0"}\NormalTok{)}
\NormalTok{)}

\CommentTok{\# Plot of observed values}
\FunctionTok{plot}\NormalTok{(x[}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(data\_het\_pmm}\SpecialCharTok{$}\NormalTok{y)],}
\NormalTok{     data\_het\_pmm}\SpecialCharTok{$}\NormalTok{y[}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(data\_het\_pmm}\SpecialCharTok{$}\NormalTok{y)],}
     \AttributeTok{main =} \StringTok{""}\NormalTok{,}
     \AttributeTok{xlab =} \StringTok{"X"}\NormalTok{,}
     \AttributeTok{ylab =} \StringTok{"Y"}\NormalTok{)}


\CommentTok{\# Plot of missing values}
\FunctionTok{points}\NormalTok{(x[}\FunctionTok{is.na}\NormalTok{(y)], data\_het\_pmm}\SpecialCharTok{$}\NormalTok{y[}\FunctionTok{is.na}\NormalTok{(y)],}
       \AttributeTok{col =} \StringTok{"red"}\NormalTok{)}

\CommentTok{\# Title of plot}
\FunctionTok{title}\NormalTok{(}\StringTok{"Predictive Mean Matching"}\NormalTok{,}
      \AttributeTok{line =} \FloatTok{0.5}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, data\_het\_pmm),}
       \AttributeTok{col =} \StringTok{"\#1b98e0"}\NormalTok{, }\AttributeTok{lwd =} \FloatTok{2.5}\NormalTok{)}

\CommentTok{\# Legend}
\FunctionTok{legend}\NormalTok{(}
  \StringTok{"topleft"}\NormalTok{,}
  \FunctionTok{c}\NormalTok{(}\StringTok{"Observed Values"}\NormalTok{, }\StringTok{"Imputed Values"}\NormalTok{, }\StringTok{"Regression Y \textasciitilde{} X"}\NormalTok{),}
  \AttributeTok{pch =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\ConstantTok{NA}\NormalTok{),}
  \AttributeTok{lty =} \FunctionTok{c}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\DecValTok{1}\NormalTok{),}
  \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"red"}\NormalTok{, }\StringTok{"\#1b98e0"}\NormalTok{)}
\NormalTok{)}

\FunctionTok{mtext}\NormalTok{(}
  \StringTok{"Imputation of Heteroscedastic Data"}\NormalTok{,}
  \CommentTok{\# Main title of plot}
  \AttributeTok{side =} \DecValTok{3}\NormalTok{,}
  \AttributeTok{line =} \SpecialCharTok{{-}}\FloatTok{1.5}\NormalTok{,}
  \AttributeTok{outer =} \ConstantTok{TRUE}\NormalTok{,}
  \AttributeTok{cex =} \DecValTok{2}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{11-imputation_files/figure-latex/unnamed-chunk-19-1} \end{center}

\hypertarget{matrix-completion}{%
\subsubsection{Matrix Completion}\label{matrix-completion}}

Matrix completion is a method used to impute missing data in a feature matrix while accounting for dependence between features. This approach leverages principal components to approximate the data matrix, a process referred to as \textbf{matrix completion} \citep[Sec 12.3]{james2013}.

\textbf{Problem Setup}

Consider an \(n \times p\) feature matrix \(\mathbf{X}\), where the element \(x_{ij}\) represents the value for the \(i\)th observation and \(j\)th feature. Some elements of \(\mathbf{X}\) are missing, and we aim to impute these missing values.

Similar to the process described in \ref{principal-components}, the matrix \(\mathbf{X}\) can be approximated using its leading principal components. Specifically, we consider \(M\) principal components that minimize the following objective:

\[
\underset{\mathbf{A} \in \mathbb{R}^{n \times M}, \mathbf{B} \in \mathbb{R}^{p \times M}}{\operatorname{min}} \left\{ \sum_{(i,j) \in \mathcal{O}} (x_{ij} - \sum_{m=1}^M a_{im}b_{jm})^2 \right\}
\]

where \(\mathcal{O}\) is the set of observed indices \((i,j)\), which is a subset of the total \(n \times p\) pairs. Here: - \(\mathbf{A}\) is an \(n \times M\) matrix of principal component scores. - \(\mathbf{B}\) is a \(p \times M\) matrix of principal component loadings.

\textbf{Imputation of Missing Values}

After solving the minimization problem:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Missing observations \(x_{ij}\) can be imputed using the formula: \[
     \hat{x}_{ij} = \sum_{m=1}^M \hat{a}_{im}\hat{b}_{jm}
     \] where \(\hat{a}_{im}\) and \(\hat{b}_{jm}\) are the estimated elements of \(\mathbf{A}\) and \(\mathbf{B}\), respectively.
\item
  The leading \(M\) principal component scores and loadings can be approximately recovered, as is done in complete data scenarios.
\end{enumerate}

\textbf{Iterative Algorithm}

The eigen-decomposition used in standard principal component analysis is not applicable here because of missing values. Instead, an iterative algorithm, as described in \citep[Alg 12.1]{james2013}, is employed:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Initialize the Complete Matrix}: Construct an initial complete matrix \(\tilde{\mathbf{X}}\) of dimension \(n \times p\) where: \[
  \tilde{x}_{ij} =
  \begin{cases}
  x_{ij} & \text{if } (i,j) \in \mathcal{O} \\
  \bar{x}_j & \text{if } (i,j) \notin \mathcal{O}
  \end{cases}
  \] Here, \(\bar{x}_j\) is the mean of the observed values for the \(j\)th variable in the incomplete data matrix \(\mathbf{X}\). \(\mathcal{O}\) indexes the observed elements of \(\mathbf{X}\).
\item
  \textbf{Iterative Steps}: Repeat the following steps until convergence:

  \begin{itemize}
  \item
    \textbf{Minimize the Objective}: Solve the problem: \[
    \underset{\mathbf{A} \in R^{n \times M}, \mathbf{B} \in R^{p \times M}}{\operatorname{min}} \left\{ \sum_{(i,j) \in \mathcal{O}} (x_{ij} - \sum_{m=1}^M a_{im}b_{jm})^2 \right\}
    \] by computing the principal components of the current \(\tilde{\mathbf{X}}\).
  \item
    \textbf{Update Missing Values}: For each missing element \((i,j) \notin \mathcal{O}\), set: \[
    \tilde{x}_{ij} \leftarrow \sum_{m=1}^M \hat{a}_{im}\hat{b}_{jm}
    \]
  \item
    \textbf{Recalculate the Objective}: Compute the objective: \[
    \sum_{(i,j) \in \mathcal{O}} (x_{ij} - \sum_{m=1}^M \hat{a}_{im} \hat{b}_{jm})^2
    \]
  \end{itemize}
\item
  \textbf{Return Imputed Values}: Once the algorithm converges, return the estimated missing entries \(\tilde{x}_{ij}\) for \((i,j) \notin \mathcal{O}\).
\end{enumerate}

\textbf{Key Considerations}

\begin{itemize}
\tightlist
\item
  This approach assumes that the missing data are missing at random (MAR).
\item
  Convergence criteria for the iterative algorithm often involve achieving a threshold for the change in the objective function or limiting the number of iterations.
\item
  The choice of \(M\), the number of principal components, can be guided by cross-validation or other model selection techniques.
\end{itemize}

\hypertarget{comparison-of-single-imputation-techniques}{%
\subsubsection{Comparison of Single Imputation Techniques}\label{comparison-of-single-imputation-techniques}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3056}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4444}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Method}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Advantages}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Disadvantages}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Mean, Median, Mode Imputation} & Simple, quick implementation. & Biased variances and covariances; ignores relationships among variables. \\
\textbf{Forward/Backward Filling} & Preserves temporal ordering. & Biased for systematic gaps or long missing sequences. \\
\textbf{Linear Regression Imputation} & Preserves relationships among variables. & Fails to capture variability; assumes linearity. \\
\textbf{Logistic Regression Imputation} & Handles categorical variables well. & Requires appropriate model assumptions; ignores variability. \\
\textbf{PMM} & Maintains variability; imputes realistic values. & Computationally intensive; requires a good predictive model. \\
\textbf{Hot Deck Imputation} & Flexible; maintains data distribution. & Sensitive to donor selection; computationally demanding. \\
\textbf{Cold Deck Imputation} & Consistent across datasets with predefined donor pools. & Risk of bias if donor data are not representative. \\
\textbf{Random Draw from Observed} & Simple; retains variability in data. & Does not preserve relationships among variables; random imputation may distort trends. \\
\textbf{Matrix Completion} & Captures dependencies; imputes structurally consistent values. & Computationally intensive; assumes principal components capture data relationships. \\
\end{longtable}

Single imputation techniques are straightforward and accessible, but they often underestimate uncertainty and fail to fully leverage relationships among variables. These limitations make them less ideal for rigorous analyses compared to multiple imputation or model-based approaches.

\hypertarget{machine-learning-and-modern-approaches}{%
\subsection{Machine Learning and Modern Approaches}\label{machine-learning-and-modern-approaches}}

\hypertarget{tree-based-methods}{%
\subsubsection{Tree-Based Methods}\label{tree-based-methods}}

\hypertarget{random-forest-imputation-missforest}{%
\paragraph{Random Forest Imputation (missForest)}\label{random-forest-imputation-missforest}}

Random Forest Imputation uses an iterative process where a random forest model predicts missing values for one variable at a time, treating other variables as predictors. This process continues until convergence.

\begin{itemize}
\tightlist
\item
  \textbf{Mathematical Framework}:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    For a variable \(X_j\) with missing values, treat \(X_j\) as the response variable.
  \item
    Fit a random forest model \(f(X_{-j})\) using the other variables \(X_{-j}\) as predictors.
  \item
    Predict missing values \(\hat{X}_j = f(X_{-j})\).
  \item
    Repeat for all variables with missing data until imputed values stabilize.
  \end{enumerate}
\item
  \textbf{Advantages}:

  \begin{itemize}
  \tightlist
  \item
    Captures complex interactions and non-linearities.
  \item
    Handles mixed data types seamlessly.
  \end{itemize}
\item
  \textbf{Limitations}:

  \begin{itemize}
  \tightlist
  \item
    Computationally intensive for large datasets.
  \item
    Sensitive to the quality of data relationships.
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{gradient-boosting-machines-gbm}{%
\paragraph{Gradient Boosting Machines (GBM)}\label{gradient-boosting-machines-gbm}}

Gradient Boosting Machines iteratively build models to minimize loss functions. For imputation, missing values are treated as a target variable to be predicted.

\begin{itemize}
\item
  \textbf{Mathematical Framework}: The GBM algorithm minimizes the loss function: \[
    L = \sum_{i=1}^n \ell(y_i, f(x_i)),
    \] where \(\ell\) is the loss function (e.g., mean squared error), \(y_i\) are observed values, and \(f(x_i)\) are predictions.
\item
  Missing values are treated as the \(y_i\) and predicted iteratively.
\item
  \textbf{Advantages}:

  \begin{itemize}
  \tightlist
  \item
    Highly accurate predictions.
  \item
    Captures variable importance.
  \end{itemize}
\item
  \textbf{Limitations}:

  \begin{itemize}
  \tightlist
  \item
    Overfitting risks.
  \item
    Requires careful parameter tuning.
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{neural-network-based-imputation}{%
\subsubsection{Neural Network-Based Imputation}\label{neural-network-based-imputation}}

\hypertarget{autoencoders}{%
\paragraph{Autoencoders}\label{autoencoders}}

Autoencoders are unsupervised neural networks that compress and reconstruct data. Missing values are estimated during reconstruction.

\begin{itemize}
\item
  \textbf{Mathematical Framework}: An autoencoder consists of:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    An encoder function: \(h = g(Wx + b)\), which compresses the input \(x\).
  \item
    A decoder function: \(\hat{x} = g'(W'h + b')\), which reconstructs the data.
  \end{enumerate}
\item
  The network minimizes the reconstruction loss: \[
    L = \sum_{i=1}^n (x_i - \hat{x}_i)^2.
    \]
\item
  \textbf{Advantages}:

  \begin{itemize}
  \tightlist
  \item
    Handles high-dimensional and non-linear data.
  \item
    Unsupervised learning.
  \end{itemize}
\item
  \textbf{Limitations}:

  \begin{itemize}
  \tightlist
  \item
    Computationally demanding.
  \item
    Requires large datasets for effective training.
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{generative-adversarial-networks-gans-for-data-imputation}{%
\paragraph{Generative Adversarial Networks (GANs) for Data Imputation}\label{generative-adversarial-networks-gans-for-data-imputation}}

GANs consist of a generator and a discriminator. For imputation, the generator fills in missing values, and the discriminator evaluates the quality of the imputations.

\begin{itemize}
\tightlist
\item
  \textbf{Mathematical Framework}: GAN training involves optimizing: \[
    \min_G \max_D \mathbb{E}[\log D(x)] + \mathbb{E}[\log(1 - D(G(z)))].
    \]

  \begin{itemize}
  \tightlist
  \item
    \(D(x)\): Discriminator's probability that \(x\) is real.
  \item
    \(G(z)\): Generator's output for latent input \(z\).
  \end{itemize}
\item
  \textbf{Advantages}:

  \begin{itemize}
  \tightlist
  \item
    Realistic imputations that reflect underlying distributions.
  \item
    Handles complex data types.
  \end{itemize}
\item
  \textbf{Limitations}:

  \begin{itemize}
  \tightlist
  \item
    Difficult to train and tune.
  \item
    Computationally intensive.
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{matrix-factorization-and-matrix-completion}{%
\subsubsection{Matrix Factorization and Matrix Completion}\label{matrix-factorization-and-matrix-completion}}

\hypertarget{singular-value-decomposition-svd}{%
\paragraph{Singular Value Decomposition (SVD)}\label{singular-value-decomposition-svd}}

SVD decomposes a matrix \(A\) into three matrices: \[
A = U\Sigma V^T,
\] where \(U\) and \(V\) are orthogonal matrices, and \(\Sigma\) contains singular values. Missing values are estimated by reconstructing \(A\) using a low-rank approximation: \[
\hat{A} = U_k \Sigma_k V_k^T.
\]

\begin{itemize}
\tightlist
\item
  \textbf{Advantages}:

  \begin{itemize}
  \tightlist
  \item
    Captures global patterns.
  \item
    Efficient for structured data.
  \end{itemize}
\item
  \textbf{Limitations}:

  \begin{itemize}
  \tightlist
  \item
    Assumes linear relationships.
  \item
    Sensitive to sparsity.
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{collaborative-filtering-approaches}{%
\paragraph{Collaborative Filtering Approaches}\label{collaborative-filtering-approaches}}

Collaborative filtering uses similarities between rows (users) or columns (items) to impute missing data. For instance, the value of \(X_{ij}\) is predicted as: \[
\hat{X}_{ij} = \frac{\sum_{k \in N(i)} w_{ik} X_{kj}}{\sum_{k \in N(i)} w_{ik}},
\] where \(w_{ik}\) represents similarity weights and \(N(i)\) is the set of neighbors.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{k-nearest-neighbor-knn-imputation}{%
\subsubsection{K-Nearest Neighbor (KNN) Imputation}\label{k-nearest-neighbor-knn-imputation}}

KNN identifies the \(k\) nearest observations based on a distance metric and imputes missing values using a weighted average (continuous variables) or mode (categorical variables).

\begin{itemize}
\item
  \textbf{Mathematical Framework}: For a missing value \(x\), its imputed value is: \[
    \hat{x} = \frac{\sum_{i=1}^k w_i x_i}{\sum_{i=1}^k w_i},
    \] where \(w_i = \frac{1}{d(x, x_i)}\) and \(d(x, x_i)\) is a distance metric (e.g., Euclidean or Manhattan).
\item
  \textbf{Advantages}:

  \begin{itemize}
  \tightlist
  \item
    Simple and interpretable.
  \item
    Non-parametric.
  \end{itemize}
\item
  \textbf{Limitations}:

  \begin{itemize}
  \tightlist
  \item
    Computationally expensive for large datasets.
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{hybrid-methods}{%
\subsubsection{Hybrid Methods}\label{hybrid-methods}}

Hybrid methods combine statistical and machine learning approaches. For example, mean imputation followed by fine-tuning with machine learning models. These methods aim to leverage the strengths of multiple techniques.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{summary-table}{%
\subsubsection{Summary Table}\label{summary-table}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2466}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2603}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2466}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2466}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Advantages
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Limitations
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Applications
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Random Forest (missForest) & Handles mixed data types, captures interactions & Computationally intensive & Mixed data types \\
Gradient Boosting Machines & High accuracy, feature importance & Sensitive to parameters & Predictive tasks \\
Autoencoders & Handles high-dimensional, non-linear data & Computationally expensive & Complex datasets \\
GANs & Realistic imputations, complex distributions & Difficult to train, resource-intensive & Healthcare, finance \\
SVD & Captures global patterns, efficient & Assumes linear relationships & Recommendation systems \\
Collaborative Filtering & Intuitive for user-item data & Struggles with sparse or new data & Recommender systems \\
KNN Imputation & Simple, interpretable & Computationally intensive, sensitive to k & General-purpose \\
Hybrid Methods & Combines multiple strengths & Complexity in design & Flexible \\
\end{longtable}

\hypertarget{multiple-imputation}{%
\subsection{Multiple Imputation}\label{multiple-imputation}}

Multiple Imputation (MI) is a statistical technique for handling missing data by creating several plausible datasets through imputation, analyzing each dataset separately, and then combining the results to account for uncertainty in the imputations. MI operates under the assumption that missing data is either \protect\hyperlink{missing-completely-at-random-mcar}{Missing Completely at Random (MCAR)} or \protect\hyperlink{missing-at-random-mar}{Missing at Random (MAR)}.

Unlike \protect\hyperlink{single-imputation-techniques}{Single Imputation Techniques}, MI reflects the uncertainty inherent in the missing data by introducing variability in the imputed values. It avoids biases introduced by ad hoc methods and produces more reliable statistical inferences.

The three fundamental steps in MI are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Imputation}: Replace missing values with a set of plausible values to create multiple ``completed'' datasets.
\item
  \textbf{Analysis}: Perform the desired statistical analysis on each imputed dataset.
\item
  \textbf{Combination}: Combine the results using rules to account for within- and between-imputation variability.
\end{enumerate}

\hypertarget{why-multiple-imputation-is-important}{%
\subsubsection{Why Multiple Imputation is Important}\label{why-multiple-imputation-is-important}}

Imputed values are estimates and inherently include random error. However, when these estimates are treated as exact values in subsequent analysis, the software may overlook this additional error. This oversight results in \textbf{underestimated standard errors and overly small p-values}, leading to misleading conclusions.

\textbf{Multiple imputation} addresses this issue by generating multiple estimates for each missing value. These estimates differ slightly due to their random component, which reintroduces variation. This variation helps the software incorporate the uncertainty of imputed values, resulting in:

\begin{itemize}
\item
  \textbf{Unbiased parameter estimates}
\item
  \textbf{Accurate standard errors}
\item
  \textbf{Improved p-values}
\end{itemize}

Multiple imputation was a significant breakthrough in statistics approximately 20 years ago. It provides solutions for many missing data issues (though not all) and, when applied correctly, leads to reliable parameter estimates.

If the proportion of missing data is very small (e.g., 2-3\%), the choice of imputation method is less critical.

\hypertarget{goals-of-multiple-imputation}{%
\subsubsection{Goals of Multiple Imputation}\label{goals-of-multiple-imputation}}

The primary goals of any missing data technique, including multiple imputation, are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Unbiased parameter estimates}: Ensuring accurate regression coefficients, group means, odds ratios, etc.
\item
  \textbf{Accurate standard errors}: This leads to reliable p-values and appropriate statistical inferences.
\item
  \textbf{Adequate power}: To detect meaningful and significant parameter values.
\end{enumerate}

\hypertarget{overview-of-rubins-framework}{%
\subsubsection{Overview of Rubin's Framework}\label{overview-of-rubins-framework}}

Rubin's Framework provides the theoretical foundation for MI. It uses a Bayesian model-based approach for generating imputations and a frequentist approach for evaluating the results. The central goals of Rubin's framework are to ensure that imputations:

\begin{itemize}
\tightlist
\item
  Retain the statistical relationships present in the data.
\item
  Reflect the uncertainty about the true values of the missing data.
\end{itemize}

Under Rubin's framework, MI offers the following advantages:

\begin{itemize}
\tightlist
\item
  \textbf{Generalizability}: Unlike Maximum Likelihood Estimation (MLE), MI can be applied to a wide range of models.
\item
  \textbf{Statistical Properties}: When data is MAR or MCAR, MI estimates are consistent, asymptotically normal, and efficient.
\end{itemize}

Rubin also emphasized the importance of using multiple imputations, as single imputations fail to account for variability in the imputed values, leading to underestimated standard errors and overly optimistic test statistics.

\hypertarget{multivariate-imputation-via-chained-equations-mice}{%
\subsubsection{Multivariate Imputation via Chained Equations (MICE)}\label{multivariate-imputation-via-chained-equations-mice}}

Multivariate Imputation via Chained Equations (MICE) is a widely used algorithm for implementing MI, particularly in datasets with mixed variable types. The steps of MICE include:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Initialization}: Replace missing values with initial guesses, such as the mean or median of the observed data.
\item
  \textbf{Iterative Imputation}:

  \begin{itemize}
  \tightlist
  \item
    For each variable with missing values, regress it on all other variables (or a subset of relevant predictors).
  \item
    Use the regression model to predict missing values, adding a random error term drawn from the residual distribution.
  \end{itemize}
\item
  \textbf{Convergence}: Repeat the imputation process until parameter estimates stabilize.
\end{enumerate}

MICE offers flexibility in specifying regression models for each variable, accommodating continuous, categorical, and binary data.

\hypertarget{bayesian-ridge-regression-for-imputation}{%
\subsubsection{Bayesian Ridge Regression for Imputation}\label{bayesian-ridge-regression-for-imputation}}

Bayesian ridge regression is an advanced imputation method that incorporates prior distributions on the regression coefficients, making it particularly useful when:

\begin{itemize}
\tightlist
\item
  Predictors are highly correlated.
\item
  Sample sizes are small.
\item
  Missingness is substantial.
\end{itemize}

This method treats the regression coefficients as random variables and samples from their posterior distribution, introducing variability into the imputation process. Bayesian ridge regression is more computationally intensive than simpler methods like MICE but offers greater robustness.

\hypertarget{combining-results-from-mi-rubins-rules}{%
\subsubsection{Combining Results from MI (Rubin's Rules)}\label{combining-results-from-mi-rubins-rules}}

Once multiple datasets are imputed and analyzed, Rubin's Rules are used to combine the results. The goal is to properly account for the uncertainty introduced by missing data. For a parameter of interest \(\theta\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Estimate Combination}: \[
  \bar{\theta} = \frac{1}{M} \sum_{m=1}^M \theta_m
  \] where \(\theta_m\) is the estimate from the \(m\)th imputed dataset, and \(M\) is the number of imputations.
\item
  \textbf{Variance Combination}: \[
  T = \bar{W} + \left(1 + \frac{1}{M}\right) B
  \] where:

  \begin{itemize}
  \tightlist
  \item
    \(\bar{W}\) is the average within-imputation variance.
  \item
    \(B\) is the between-imputation variance: \[
    B = \frac{1}{M-1} \sum_{m=1}^M (\theta_m - \bar{\theta})^2
    \]
  \end{itemize}
\end{enumerate}

These formulas adjust the final variance to reflect uncertainty both within and across imputations.

\hypertarget{challenges}{%
\paragraph{Challenges}\label{challenges}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Stochastic Variability}: MI results vary slightly between runs due to its reliance on random draws. To ensure reproducibility, always set a random seed.
\item
  \textbf{Convergence}: Iterative algorithms like MICE may struggle to converge, especially with high proportions of missing data.
\item
  \textbf{Assumption of MAR}: MI assumes that missing data is MAR. If data is Missing Not at Random (MNAR), MI can produce biased results.
\end{enumerate}

\hypertarget{best-practices}{%
\paragraph{Best Practices}\label{best-practices}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Algorithm Selection}:

  \begin{itemize}
  \tightlist
  \item
    Use Multiple Imputation by Chained Equations (MICE) for datasets with mixed data types or when relationships between variables are complex.
  \item
    Apply Bayesian Ridge Regression for small datasets or when predictors are highly correlated.
  \end{itemize}
\item
  \textbf{Diagnostic Checks}:

  \begin{itemize}
  \tightlist
  \item
    Evaluate the quality of imputations and assess convergence using trace plots or diagnostic statistics to ensure reliable results.
  \end{itemize}
\item
  \textbf{Data Transformations}:

  \begin{itemize}
  \tightlist
  \item
    For skewed or proportion data, consider applying log or logit transformations before imputation and inverse-transforming afterward to preserve the data's original scale.
  \end{itemize}
\item
  \textbf{Handling Non-Linear Relationships}:

  \begin{itemize}
  \tightlist
  \item
    For non-linear relationships or interactions, stratify imputations by the levels of the categorical variable involved to ensure accurate estimates.
  \end{itemize}
\item
  \textbf{Number of Imputations}:

  \begin{itemize}
  \tightlist
  \item
    Use at least 20 imputations for small datasets or datasets with high missingness. This ensures robust and reliable results in downstream analyses.
  \end{itemize}
\item
  \textbf{Avoid Rounding Imputations for Dummy Variables}:

  \begin{itemize}
  \tightlist
  \item
    Many imputation methods (e.g., Markov Chain Monte Carlo {[}MCMC{]}) assume normality, even for dummy variables. While it was historically recommended to round imputed values to 0 or 1 for binary variables, research shows that this introduces bias in parameter estimates. Instead, leave imputed values as fractional, even though this may seem counter-intuitive.
  \end{itemize}
\item
  \textbf{Do Not Transform Skewed Variables Before Imputation}:

  \begin{itemize}
  \tightlist
  \item
    Transforming variables to meet normality assumptions before imputation can distort their relationships with other variables, leading to biased imputations and possibly introducing outliers. It is better to directly impute the skewed variable.
  \end{itemize}
\item
  \textbf{Use More Imputations}:

  \begin{itemize}
  \tightlist
  \item
    Traditional advice suggests 5--10 imputations are sufficient for unbiased estimates, but inconsistencies may arise in repeated analyses. {[}@Bodner\_2008{]} suggests using a number of imputations equal to the percentage of missing data. As additional imputations generally do not significantly increase the computational workload, using more imputations is a prudent choice.
  \end{itemize}
\item
  \textbf{Create Multiplicative Terms Before Imputation}:

  \begin{itemize}
  \tightlist
  \item
    When your model includes interaction or quadratic terms, generate these terms before imputing missing values. Imputing first and then generating these terms can introduce bias in their regression parameters, as highlighted by {[}@von\_Hippel\_2009{]}.
  \end{itemize}
\end{enumerate}

\hypertarget{evaluation-of-imputation-methods}{%
\section{Evaluation of Imputation Methods}\label{evaluation-of-imputation-methods}}

\hypertarget{statistical-metrics-for-assessing-imputation-quality}{%
\subsection{Statistical Metrics for Assessing Imputation Quality}\label{statistical-metrics-for-assessing-imputation-quality}}

To evaluate the quality of imputed data, several statistical metrics are commonly used. These metrics compare the imputed values to the observed values (in cases where missingness is simulated or artificially introduced) or assess the overall impact of imputation on the quality of subsequent analyses. Key metrics include:

\begin{itemize}
\item
  \textbf{Root Mean Squared Error (RMSE):} RMSE is calculated as: \[ 
  \text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}
  \] It measures the average magnitude of errors between the true and imputed values. Lower RMSE indicates better imputation accuracy.
\item
  \textbf{Mean Absolute Error (MAE):} MAE measures the average absolute difference between observed and imputed values: \[ 
  \text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
  \] MAE provides a straightforward assessment of imputation performance and is less sensitive to outliers than RMSE.
\item
  \textbf{Log-Likelihood and Deviance Measures:} Log-likelihood can be used to evaluate how well the imputation model fits the data. Deviance measures, based on likelihood comparisons, assess the relative goodness of fit of imputation models. These are particularly useful in evaluating methods like maximum likelihood estimation.
\end{itemize}

In practice, these metrics may be combined with graphical methods such as density plots and residual analysis to understand imputation performance more thoroughly.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{bias-variance-tradeoff-in-imputation}{%
\subsection{Bias-Variance Tradeoff in Imputation}\label{bias-variance-tradeoff-in-imputation}}

Imputation methods must balance bias and variance to achieve reliable results. Simpler methods, such as mean or mode imputation, often lead to biased parameter estimates, particularly if the missingness mechanism is non-random. These methods underestimate variability, shrinking standard errors and potentially leading to overconfidence in statistical inferences.

Conversely, advanced methods like \protect\hyperlink{multiple-imputation}{Multiple Imputation} or Full Information Maximum Likelihood (FIML) typically yield unbiased estimates with appropriately calibrated variances. However, these methods may increase computational complexity and require careful tuning of assumptions and parameters.

The tradeoff is summarized as follows:

\begin{itemize}
\item
  \textbf{High Bias, Low Variance:} Simpler methods (e.g., single imputation, mean imputation).
\item
  \textbf{Low Bias, Moderate Variance:} Advanced methods (e.g., MI, FIML, Bayesian methods).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{sensitivity-analysis-1}{%
\subsection{Sensitivity Analysis}\label{sensitivity-analysis-1}}

Sensitivity analysis is crucial to assess the robustness of imputation methods under varying assumptions. Two primary areas of focus include:

\begin{itemize}
\item
  \textbf{Assessing Robustness to Assumptions:} Imputation models often rely on assumptions about the missingness mechanism (See \protect\hyperlink{definition-and-classification-of-missing-data}{Definition and Classification of Missing Data}). Sensitivity analysis involves testing how results vary when these assumptions are slightly relaxed or modified.
\item
  \textbf{Impact on Downstream Analysis:} The quality of imputation should also be evaluated based on its influence on downstream analyses (\protect\hyperlink{objectives-of-imputation}{Objectives of Imputation}). For instance:

  \begin{itemize}
  \tightlist
  \item
    Does the imputation affect causal inference in regression models?
  \item
    Are the conclusions from hypothesis testing or predictive modeling robust to the imputation technique?
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{validation-using-simulated-data-and-real-world-case-studies}{%
\subsection{Validation Using Simulated Data and Real-World Case Studies}\label{validation-using-simulated-data-and-real-world-case-studies}}

Validation of imputation methods is best performed through a combination of simulated data and real-world examples:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Simulated Data:} - Create datasets with known missingness patterns and true values. - Apply various imputation methods and assess their performance using RMSE, MAE, and other metrics.
\item
  \textbf{Real-World Case Studies:}

  \begin{itemize}
  \tightlist
  \item
    Use datasets from actual studies, such as customer transaction data in marketing or financial data in portfolio analysis.
  \item
    Evaluate the impact of imputation on actionable outcomes (e.g., market segmentation, risk assessment).
  \end{itemize}
\end{enumerate}

Combining these approaches ensures that methods generalize well across different contexts and data structures.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{criteria-for-choosing-an-effective-approach}{%
\section{Criteria for Choosing an Effective Approach}\label{criteria-for-choosing-an-effective-approach}}

Choosing an appropriate imputation method depends on the following criteria:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Unbiased Parameter Estimates:} The technique should ensure that key estimates, such as means, variances, and regression coefficients, are unbiased, particularly in the presence of MAR or MNAR data.
\item
  \textbf{Adequate Power:} The method should preserve statistical power, enabling robust hypothesis testing and model estimation. This ensures that important effects are not missed due to inflated type II error.
\item
  \textbf{Accurate Standard Errors:} Accurate estimation of standard errors is critical for reliable p-values and confidence intervals. Methods like single imputation often underestimate standard errors, leading to overconfident conclusions.
\end{enumerate}

\textbf{Preferred Methods: Multiple Imputation and Full Information Maximum Likelihood}

\textbf{Multiple Imputation (MI):}

\begin{itemize}
\item
  MI replaces missing values with multiple plausible values drawn from a predictive distribution. It generates multiple complete datasets, analyzes each dataset, and combines the results.
\item
  Pros: Handles uncertainty well, provides valid standard errors, and is robust under MAR.
\item
  Cons: Computationally intensive, sensitive to model mis-specification.
\end{itemize}

\textbf{Full Information Maximum Likelihood (FIML):}

\begin{itemize}
\item
  FIML uses all available data to estimate parameters directly, avoiding the need to impute missing values explicitly.
\item
  Pros: Efficient, unbiased under MAR, and computationally elegant.
\item
  Cons: Requires correctly specified models and may be sensitive to MNAR data.
\end{itemize}

\textbf{Methods to Avoid}

\begin{itemize}
\tightlist
\item
  \textbf{Single Imputation (e.g., Mean, Mode):}

  \begin{itemize}
  \tightlist
  \item
    Leads to biased estimates and underestimates variability.
  \end{itemize}
\item
  \textbf{Listwise Deletion:}

  \begin{itemize}
  \tightlist
  \item
    Discards rows with missing data, reducing sample size and potentially introducing bias if the data is not MCAR.
  \end{itemize}
\end{itemize}

\textbf{Practical Considerations}

\begin{itemize}
\tightlist
\item
  Computational efficiency and ease of implementation.
\item
  Compatibility with downstream analysis methods.
\item
  Alignment with the data's missingness mechanism.
\end{itemize}

\hypertarget{challenges-and-ethical-considerations}{%
\section{Challenges and Ethical Considerations}\label{challenges-and-ethical-considerations}}

\hypertarget{challenges-in-high-dimensional-data}{%
\subsection{Challenges in High-Dimensional Data}\label{challenges-in-high-dimensional-data}}

High-dimensional data, where the number of variables exceeds the number of observations, poses unique challenges for missing data analysis.

\begin{itemize}
\item
  \textbf{Curse of Dimensionality}: Standard imputation methods, such as mean or regression imputation, struggle with high-dimensional spaces due to sparse data distribution.
\item
  \textbf{Regularized Methods}: Techniques such as LASSO, Ridge Regression, and Elastic Net can be used to handle high-dimensional missing data. These methods shrink model coefficients, preventing overfitting.
\item
  \textbf{Matrix Factorization}: Methods like Principal Component Analysis (PCA) or Singular Value Decomposition (SVD) are often adapted to impute missing values in high-dimensional datasets by reducing the dimensionality first.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{missing-data-in-big-data-contexts}{%
\subsection{Missing Data in Big Data Contexts}\label{missing-data-in-big-data-contexts}}

The advent of big data introduces additional complexities for missing data handling, including computational scalability and storage constraints.

\hypertarget{distributed-imputation-techniques}{%
\subsubsection{Distributed Imputation Techniques}\label{distributed-imputation-techniques}}

\begin{itemize}
\item
  \textbf{MapReduce Frameworks}: Algorithms like k-nearest neighbor (KNN) imputation or multiple imputation can be adapted for distributed environments using MapReduce or similar frameworks.
\item
  \textbf{Federated Learning}: In scenarios where data is distributed across multiple locations (e.g., in healthcare or banking), federated learning allows imputation without centralizing data, ensuring privacy.
\end{itemize}

\hypertarget{cloud-based-implementations}{%
\subsubsection{Cloud-Based Implementations}\label{cloud-based-implementations}}

\begin{itemize}
\item
  \textbf{Cloud-Native Algorithms}: Cloud platforms like AWS, Google Cloud, and Azure provide scalable solutions for implementing advanced imputation algorithms on large datasets.
\item
  \textbf{AutoML Integration}: Automated Machine Learning (AutoML) pipelines often include missing data handling as a preprocessing step, leveraging cloud-based computational power.
\item
  \textbf{Real-Time Imputation}: In e-commerce, cloud-based solutions enable real-time imputation for recommendation systems or fraud detection, ensuring seamless user experiences.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{ethical-concerns}{%
\subsection{Ethical Concerns}\label{ethical-concerns}}

\hypertarget{bias-amplification}{%
\subsubsection{Bias Amplification}\label{bias-amplification}}

\begin{itemize}
\item
  \textbf{Introduction of Systematic Bias}: Imputation methods can inadvertently reinforce existing biases. For example, imputing salary data based on demographic variables may propagate societal inequalities.
\item
  \textbf{Business Implications}: In credit scoring, biased imputation of missing financial data can lead to unfair credit decisions, disproportionately affecting marginalized groups.
\item
  \textbf{Mitigation Strategies}: Techniques such as fairness-aware machine learning and bias auditing can help identify and reduce bias introduced during imputation.
\end{itemize}

\hypertarget{transparency-in-reporting-imputation-decisions}{%
\subsubsection{Transparency in Reporting Imputation Decisions}\label{transparency-in-reporting-imputation-decisions}}

\begin{itemize}
\item
  \textbf{Reproducibility and Documentation}: Transparent reporting of imputation methods and assumptions is essential for reproducibility. Analysts should provide clear documentation of the imputation pipeline.
\item
  \textbf{Stakeholder Communication}: In business settings, communicating imputation decisions to stakeholders ensures informed decision-making and trust in the results.
\item
  \textbf{Ethical Frameworks}: Ethical guidelines, such as those provided by the European Union's GDPR or industry-specific codes, emphasize the importance of transparency in data handling.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{emerging-trends-in-missing-data-handling}{%
\section{Emerging Trends in Missing Data Handling}\label{emerging-trends-in-missing-data-handling}}

\hypertarget{advances-in-neural-network-approaches}{%
\subsection{Advances in Neural Network Approaches}\label{advances-in-neural-network-approaches}}

Neural networks have transformed the landscape of missing data imputation, offering flexible, scalable, and powerful solutions that go beyond traditional methods.

\hypertarget{variational-autoencoders-vaes}{%
\subsubsection{Variational Autoencoders (VAEs)}\label{variational-autoencoders-vaes}}

\begin{itemize}
\item
  \textbf{Overview}: Variational Autoencoders (VAEs) are generative models that encode data into a latent space and reconstruct it, filling in missing values during reconstruction.
\item
  \textbf{Advantages}:

  \begin{itemize}
  \tightlist
  \item
    Handle complex, non-linear relationships between variables.
  \item
    Scalable to high-dimensional datasets.
  \item
    Generate probabilistic imputations, reflecting uncertainty.
  \end{itemize}
\item
  \textbf{Applications}:

  \begin{itemize}
  \tightlist
  \item
    In marketing, VAEs can impute missing customer behavior data while accounting for seasonal and demographic variations.
  \item
    In finance, VAEs assist in imputing missing stock price data by modeling dependencies among assets.
  \end{itemize}
\end{itemize}

\hypertarget{gans-for-missing-data}{%
\subsubsection{GANs for Missing Data}\label{gans-for-missing-data}}

\begin{itemize}
\item
  \textbf{Generative Adversarial Networks (GANs)}: GANs consist of a generator and a discriminator, with the generator imputing missing data and the discriminator evaluating its quality.
\item
  \textbf{Advantages}:

  \begin{itemize}
  \tightlist
  \item
    Preserve data distributions and avoid over-smoothing.
  \item
    Suitable for imputation in datasets with complex patterns or multi-modal distributions.
  \end{itemize}
\item
  \textbf{Applications}:

  \begin{itemize}
  \tightlist
  \item
    In healthcare, GANs have been used to impute missing patient records while preserving patient privacy and data integrity.
  \item
    In retail, GANs can model missing sales data to predict trends and optimize inventory.
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{integration-with-reinforcement-learning}{%
\subsection{Integration with Reinforcement Learning}\label{integration-with-reinforcement-learning}}

Reinforcement learning (RL) is increasingly being integrated into missing data strategies, particularly in dynamic or sequential data environments.

\begin{itemize}
\item
  \textbf{Markov Decision Processes (MDPs)}: RL models missing data handling as an MDP, where actions (imputations) are optimized based on rewards (accuracy of predictions or decisions).
\item
  \textbf{Active Imputation}:

  \begin{itemize}
  \tightlist
  \item
    RL can be used to actively query for missing data points, prioritizing those with the highest impact on downstream tasks.
  \item
    Example: In customer churn prediction, RL can optimize the imputation of high-value customer records.
  \end{itemize}
\item
  \textbf{Applications}:

  \begin{itemize}
  \tightlist
  \item
    Financial forecasting: RL models are used to impute missing transaction data dynamically, optimizing portfolio decisions.
  \item
    Smart cities: RL-based models handle missing sensor data to enhance real-time decision-making in traffic management.
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{synthetic-data-generation-for-missing-data}{%
\subsection{Synthetic Data Generation for Missing Data}\label{synthetic-data-generation-for-missing-data}}

Synthetic data generation has emerged as a robust solution to address missing data, providing flexibility and privacy.

\begin{itemize}
\item
  \textbf{Data Augmentation}: Synthetic data is generated to augment datasets with missing values, reducing biases introduced by imputation.
\item
  \textbf{Techniques}:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Simulations}: Monte Carlo simulations create plausible data points based on observed distributions.
  \item
    \textbf{Generative Models}: GANs and VAEs generate realistic synthetic data that aligns with existing patterns.
  \end{itemize}
\item
  \textbf{Applications}:

  \begin{itemize}
  \tightlist
  \item
    In fraud detection, synthetic datasets balance the impact of missing values on anomaly detection.
  \item
    In insurance, synthetic data supports pricing models by filling in gaps from incomplete policyholder records.
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{federated-learning-and-privacy-preserving-imputation}{%
\subsection{Federated Learning and Privacy-Preserving Imputation}\label{federated-learning-and-privacy-preserving-imputation}}

Federated learning has gained traction as a method for collaborative analysis while preserving data privacy.

\begin{itemize}
\tightlist
\item
  \textbf{Federated Imputation}:

  \begin{itemize}
  \tightlist
  \item
    Distributed imputation algorithms operate on decentralized data, ensuring that sensitive information remains local.
  \item
    Example: Hospitals collaboratively impute missing patient data without sharing individual records.
  \end{itemize}
\item
  \textbf{Privacy Mechanisms}:

  \begin{itemize}
  \tightlist
  \item
    Differential privacy adds noise to imputed values, protecting individual-level data.
  \item
    Homomorphic encryption allows computations on encrypted data, ensuring privacy throughout the imputation process.
  \end{itemize}
\item
  \textbf{Applications}:

  \begin{itemize}
  \tightlist
  \item
    Healthcare: Federated learning imputes missing diagnostic data across clinics.
  \item
    Banking: Collaborative imputation of financial transaction data supports risk modeling while adhering to regulations.
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{imputation-in-streaming-and-online-data-environments}{%
\subsection{Imputation in Streaming and Online Data Environments}\label{imputation-in-streaming-and-online-data-environments}}

The increasing use of streaming data in business and technology requires real-time imputation methods to ensure uninterrupted analysis.

\begin{itemize}
\tightlist
\item
  \textbf{Challenges}:

  \begin{itemize}
  \tightlist
  \item
    Imputation must occur dynamically as data streams in.
  \item
    Low latency and high accuracy are essential to maintain real-time decision-making.
  \end{itemize}
\item
  \textbf{Techniques}:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Online Learning Algorithms}: Update imputation models incrementally as new data arrives.
  \item
    \textbf{Sliding Window Methods}: Use recent data to estimate and impute missing values in real time.
  \end{itemize}
\item
  \textbf{Applications}:

  \begin{itemize}
  \tightlist
  \item
    IoT devices: Imputation in sensor networks for smart homes or industrial monitoring ensures continuous operation despite data transmission issues.
  \item
    Financial markets: Streaming imputation models predict and fill gaps in real-time stock price feeds to inform trading algorithms.
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{application-of-imputation-in-r}{%
\section{Application of Imputation in R}\label{application-of-imputation-in-r}}

This section demonstrates how to visualize missing data and handle it using different imputation techniques.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Package}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Algorithm}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Cont Var}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Cate Var}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Diagnostics}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Complexity Handling}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Best Use Case}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Limitations}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{missForest} & Random Forest & Yes & Yes & Out-of-bag error (NRMSE, PFC) & Handles complex interactions & Mixed data types with complex interactions & May overfit with small datasets \\
\textbf{Hmisc} & Additive Regression, Bootstrap, Predictive Mean Matching & Yes & Yes & \(R^2\) for imputed values & Basic to intermediate complexity & Simple datasets with low complexity & Limited to simple imputation methods \\
\textbf{mi} & Bayesian Regression & Yes & Yes & Graphical diagnostics,convergence & Detects issues like collinearity & Datasets with irregularities & Computationally intensive for large data \\
\textbf{MICE} & Multivariate Imputation via Chained Equations & Yes & Yes & Density plots, pooling of results & Handles variable interactions & General-purpose imputation for MAR data & Requires proper method selection for variable types \\
\textbf{Amelia} & Bootstrap-based Expectation Maximization (EMB) & Yes & Limited (requires normality) & Diagnostics supported & Works well with large/time-series data & Time-series or datasets approximating MVN & Assumes MVN, requires transformations for non-MVN \\
\end{longtable}

\hypertarget{visualizing-missing-data}{%
\subsection{Visualizing Missing Data}\label{visualizing-missing-data}}

Visualizing missing data is an essential first step in understanding the patterns and extent of missingness in your dataset.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(visdat)}
\FunctionTok{library}\NormalTok{(naniar)}
\FunctionTok{library}\NormalTok{(ggplot2)}

\CommentTok{\# Visualizing missing data}
\FunctionTok{vis\_miss}\NormalTok{(airquality)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{11-imputation_files/figure-latex/unnamed-chunk-20-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Missingness patterns using an upset plot}
\FunctionTok{gg\_miss\_upset}\NormalTok{(airquality)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{11-imputation_files/figure-latex/unnamed-chunk-20-2} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Scatter plot of missing data with faceting}
\FunctionTok{ggplot}\NormalTok{(airquality, }\FunctionTok{aes}\NormalTok{(x, y)) }\SpecialCharTok{+}
  \FunctionTok{geom\_miss\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ group)}

\CommentTok{\# Missing values by variable}
\FunctionTok{gg\_miss\_var}\NormalTok{(data, }\AttributeTok{facet =}\NormalTok{ group)}

\CommentTok{\# Missingness in relation to factors}
\FunctionTok{gg\_miss\_fct}\NormalTok{(}\AttributeTok{x =}\NormalTok{ variable1, }\AttributeTok{fct =}\NormalTok{ variable2)}
\end{Highlighting}
\end{Shaded}

For more details, read \href{https://tmb.njtierney.com/}{The Missing Book by Nicholas Tierney \& Allison Horst}.

\hypertarget{how-many-imputations}{%
\subsection{How Many Imputations?}\label{how-many-imputations}}

\textbf{Usually, 5 imputations} are sufficient unless there is an extremely high proportion of missing data. High proportions require revisiting data collection processes.

\textbf{Rubin's Rule for Relative Efficiency}

According to Rubin, the relative efficiency of an estimate based on \(m\) imputations (relative to infinite imputations) is given by:

\[
\text{Relative Efficiency} = ( 1 + \frac{\lambda}{m})^{-1}
\]

where \(\lambda\) is the rate of missing data.

For example, with 50\% missing data (\(\lambda = 0.5\)), the standard deviation of an estimate based on 5 imputations is only about 5\% wider than that from infinite imputations:

\[
\sqrt{1 + \frac{0.5}{5}} = 1.049
\]

\hypertarget{generating-missing-data-for-demonstration}{%
\subsection{Generating Missing Data for Demonstration}\label{generating-missing-data-for-demonstration}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(missForest)}

\CommentTok{\# Load the data}
\NormalTok{data }\OtherTok{\textless{}{-}}\NormalTok{ iris}

\CommentTok{\# Generate 10\% missing values at random}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{iris.mis }\OtherTok{\textless{}{-}} \FunctionTok{prodNA}\NormalTok{(iris, }\AttributeTok{noNA =} \FloatTok{0.1}\NormalTok{)}

\CommentTok{\# Remove categorical variables for numeric imputation}
\NormalTok{iris.mis.cat }\OtherTok{\textless{}{-}}\NormalTok{ iris.mis}
\NormalTok{iris.mis }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(iris.mis, }\AttributeTok{select =} \SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(Species))}
\end{Highlighting}
\end{Shaded}

\hypertarget{imputation-with-mean-median-and-mode}{%
\subsection{Imputation with Mean, Median, and Mode}\label{imputation-with-mean-median-and-mode}}

Mean, median, or mode imputation is a simple yet commonly used technique.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Imputation for the entire dataset}
\NormalTok{e1071}\SpecialCharTok{::}\FunctionTok{impute}\NormalTok{(iris.mis, }\AttributeTok{what =} \StringTok{"mean"}\NormalTok{)        }\CommentTok{\# Replace with mean}
\NormalTok{e1071}\SpecialCharTok{::}\FunctionTok{impute}\NormalTok{(iris.mis, }\AttributeTok{what =} \StringTok{"median"}\NormalTok{)      }\CommentTok{\# Replace with median}

\CommentTok{\# Imputation by variable}
\NormalTok{Hmisc}\SpecialCharTok{::}\FunctionTok{impute}\NormalTok{(iris.mis}\SpecialCharTok{$}\NormalTok{Sepal.Length, mean)    }\CommentTok{\# Replace with mean}
\NormalTok{Hmisc}\SpecialCharTok{::}\FunctionTok{impute}\NormalTok{(iris.mis}\SpecialCharTok{$}\NormalTok{Sepal.Length, median)  }\CommentTok{\# Replace with median}
\NormalTok{Hmisc}\SpecialCharTok{::}\FunctionTok{impute}\NormalTok{(iris.mis}\SpecialCharTok{$}\NormalTok{Sepal.Length, }\DecValTok{0}\NormalTok{)       }\CommentTok{\# Replace with a specific value}
\end{Highlighting}
\end{Shaded}

Checking Accuracy

Accuracy can be checked by comparing predictions with actual values.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example data}
\NormalTok{actuals }\OtherTok{\textless{}{-}}\NormalTok{ iris}\SpecialCharTok{$}\NormalTok{Sepal.Width[}\FunctionTok{is.na}\NormalTok{(iris.mis}\SpecialCharTok{$}\NormalTok{Sepal.Width)]}
\NormalTok{predicteds }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\FunctionTok{mean}\NormalTok{(iris}\SpecialCharTok{$}\NormalTok{Sepal.Width, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{), }\FunctionTok{length}\NormalTok{(actuals))}

\CommentTok{\# Using MLmetrics package}
\FunctionTok{library}\NormalTok{(MLmetrics)}

\FunctionTok{MAE}\NormalTok{(predicteds, actuals)}
\CommentTok{\#\textgreater{} [1] 0.2870303}
\FunctionTok{MSE}\NormalTok{(predicteds, actuals)}
\CommentTok{\#\textgreater{} [1] 0.1301598}
\FunctionTok{RMSE}\NormalTok{(predicteds, actuals)}
\CommentTok{\#\textgreater{} [1] 0.3607767}
\end{Highlighting}
\end{Shaded}

\hypertarget{k-nearest-neighbors-knn-imputation}{%
\subsection{K-Nearest Neighbors (KNN) Imputation}\label{k-nearest-neighbors-knn-imputation}}

KNN is a more sophisticated method, leveraging similar observations to fill in missing values.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(DMwR2)}
\NormalTok{knnOutput }\OtherTok{\textless{}{-}} \FunctionTok{knnImputation}\NormalTok{(}\AttributeTok{data =}\NormalTok{ iris.mis.cat, }\AttributeTok{meth =} \StringTok{"median"}\NormalTok{)}
\FunctionTok{anyNA}\NormalTok{(knnOutput)  }\CommentTok{\# Check for remaining missing values}
\CommentTok{\#\textgreater{} [1] FALSE}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{actuals }\OtherTok{\textless{}{-}}\NormalTok{ iris}\SpecialCharTok{$}\NormalTok{Sepal.Width[}\FunctionTok{is.na}\NormalTok{(iris.mis}\SpecialCharTok{$}\NormalTok{Sepal.Width)]}
\NormalTok{predicteds }\OtherTok{\textless{}{-}}\NormalTok{ knnOutput[}\FunctionTok{is.na}\NormalTok{(iris.mis}\SpecialCharTok{$}\NormalTok{Sepal.Width), }\StringTok{"Sepal.Width"}\NormalTok{]}
\CommentTok{\# Using MLmetrics package}
\FunctionTok{library}\NormalTok{(MLmetrics)}

\FunctionTok{MAE}\NormalTok{(predicteds, actuals)}
\CommentTok{\#\textgreater{} [1] 0.2318182}
\FunctionTok{MSE}\NormalTok{(predicteds, actuals)}
\CommentTok{\#\textgreater{} [1] 0.1038636}
\FunctionTok{RMSE}\NormalTok{(predicteds, actuals)}
\CommentTok{\#\textgreater{} [1] 0.3222788}
\end{Highlighting}
\end{Shaded}

KNN typically improves upon mean or median imputation in terms of predictive accuracy.

\hypertarget{imputation-with-decision-trees-rpart}{%
\subsection{Imputation with Decision Trees (rpart)}\label{imputation-with-decision-trees-rpart}}

Decision trees, such as those implemented in \texttt{rpart}, are effective for both numeric and categorical variables.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rpart)}

\CommentTok{\# Imputation for a categorical variable}
\NormalTok{class\_mod }\OtherTok{\textless{}{-}} \FunctionTok{rpart}\NormalTok{(}
\NormalTok{  Species }\SpecialCharTok{\textasciitilde{}}\NormalTok{ . }\SpecialCharTok{{-}}\NormalTok{ Sepal.Length,}
  \AttributeTok{data =}\NormalTok{ iris.mis.cat[}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(iris.mis.cat}\SpecialCharTok{$}\NormalTok{Species), ],}
  \AttributeTok{method =} \StringTok{"class"}\NormalTok{,}
  \AttributeTok{na.action =}\NormalTok{ na.omit}
\NormalTok{)}

\CommentTok{\# Imputation for a numeric variable}
\NormalTok{anova\_mod }\OtherTok{\textless{}{-}} \FunctionTok{rpart}\NormalTok{(}
\NormalTok{  Sepal.Width }\SpecialCharTok{\textasciitilde{}}\NormalTok{ . }\SpecialCharTok{{-}}\NormalTok{ Sepal.Length,}
  \AttributeTok{data =}\NormalTok{ iris.mis[}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(iris.mis}\SpecialCharTok{$}\NormalTok{Sepal.Width), ],}
  \AttributeTok{method =} \StringTok{"anova"}\NormalTok{,}
  \AttributeTok{na.action =}\NormalTok{ na.omit}
\NormalTok{)}

\CommentTok{\# Predictions}
\NormalTok{species\_pred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(class\_mod, iris.mis.cat[}\FunctionTok{is.na}\NormalTok{(iris.mis.cat}\SpecialCharTok{$}\NormalTok{Species), ])}
\NormalTok{width\_pred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(anova\_mod, iris.mis[}\FunctionTok{is.na}\NormalTok{(iris.mis}\SpecialCharTok{$}\NormalTok{Sepal.Width), ])}
\end{Highlighting}
\end{Shaded}

\hypertarget{mice-multivariate-imputation-via-chained-equations}{%
\subsection{MICE (Multivariate Imputation via Chained Equations)}\label{mice-multivariate-imputation-via-chained-equations}}

MICE assumes that the data are \protect\hyperlink{missing-at-random-mar}{\textbf{Missing at Random (MAR)}}. It imputes data for each variable by specifying an imputation model tailored to the variable type.

\hypertarget{how-mice-works}{%
\subsubsection{How MICE Works}\label{how-mice-works}}

For a dataset with variables \(X_1, X_2, \dots, X_k\):

\begin{itemize}
\item
  If \(X_1\) has missing data, it is regressed on the other variables.
\item
  This process is repeated for all variables with missing data, using the previously predicted values as needed.
\end{itemize}

By default:

\begin{itemize}
\item
  \textbf{Continuous variables} use linear regression.
\item
  \textbf{Categorical variables} use logistic regression.
\end{itemize}

\hypertarget{methods-available-in-mice}{%
\subsubsection{Methods Available in MICE}\label{methods-available-in-mice}}

\begin{itemize}
\tightlist
\item
  \textbf{\texttt{pmm} (Predictive Mean Matching)}: For numeric variables.
\item
  \textbf{\texttt{logreg} (Logistic Regression)}: For binary variables (2 levels).
\item
  \textbf{\texttt{polyreg} (Bayesian polytomous regression)}: For factor variables (2 levels).
\item
  \textbf{Proportional Odds Model}: For ordered factor variables (2 levels).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load packages}
\FunctionTok{library}\NormalTok{(mice)}
\FunctionTok{library}\NormalTok{(VIM)}

\CommentTok{\# Check missing values pattern}
\FunctionTok{md.pattern}\NormalTok{(iris.mis)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{11-imputation_files/figure-latex/unnamed-chunk-28-1} \end{center}

\begin{verbatim}
#>     Sepal.Width Sepal.Length Petal.Length Petal.Width   
#> 100           1            1            1           1  0
#> 15            1            1            1           0  1
#> 8             1            1            0           1  1
#> 2             1            1            0           0  2
#> 11            1            0            1           1  1
#> 1             1            0            1           0  2
#> 1             1            0            0           1  2
#> 1             1            0            0           0  3
#> 7             0            1            1           1  1
#> 3             0            1            0           1  2
#> 1             0            0            1           1  2
#>              11           15           15          19 60

# Plot missing values
aggr(
  iris.mis,
  col = c('navyblue', 'yellow'),
  numbers = TRUE,
  sortVars = TRUE,
  labels = names(iris.mis),
  cex.axis = 0.7,
  gap = 3,
  ylab = c("Missing data", "Pattern")
)
\end{verbatim}

\begin{center}\includegraphics[width=0.9\linewidth]{11-imputation_files/figure-latex/unnamed-chunk-28-2} \end{center}

\begin{verbatim}
#> 
#>  Variables sorted by number of missings: 
#>      Variable      Count
#>   Petal.Width 0.12666667
#>  Sepal.Length 0.10000000
#>  Petal.Length 0.10000000
#>   Sepal.Width 0.07333333
\end{verbatim}

Imputing Data

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Perform multiple imputation using MICE}
\NormalTok{imputed\_Data }\OtherTok{\textless{}{-}} \FunctionTok{mice}\NormalTok{(}
\NormalTok{  iris.mis,}
  \AttributeTok{m =} \DecValTok{5}\NormalTok{,             }\CommentTok{\# Number of imputed datasets}
  \AttributeTok{maxit =} \DecValTok{10}\NormalTok{,        }\CommentTok{\# Number of iterations}
  \AttributeTok{method =} \StringTok{\textquotesingle{}pmm\textquotesingle{}}\NormalTok{,    }\CommentTok{\# Imputation method}
  \AttributeTok{seed =} \DecValTok{500}         \CommentTok{\# Random seed for reproducibility}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Evaluating Imputed Data

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Summary of imputed data}
\FunctionTok{summary}\NormalTok{(imputed\_Data)}
\CommentTok{\#\textgreater{} Class: mids}
\CommentTok{\#\textgreater{} Number of multiple imputations:  5 }
\CommentTok{\#\textgreater{} Imputation methods:}
\CommentTok{\#\textgreater{} Sepal.Length  Sepal.Width Petal.Length  Petal.Width }
\CommentTok{\#\textgreater{}        "pmm"        "pmm"        "pmm"        "pmm" }
\CommentTok{\#\textgreater{} PredictorMatrix:}
\CommentTok{\#\textgreater{}              Sepal.Length Sepal.Width Petal.Length Petal.Width}
\CommentTok{\#\textgreater{} Sepal.Length            0           1            1           1}
\CommentTok{\#\textgreater{} Sepal.Width             1           0            1           1}
\CommentTok{\#\textgreater{} Petal.Length            1           1            0           1}
\CommentTok{\#\textgreater{} Petal.Width             1           1            1           0}

\CommentTok{\# Density plot: compare imputed values (red) with observed values (blue)}
\FunctionTok{densityplot}\NormalTok{(imputed\_Data)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{11-imputation_files/figure-latex/unnamed-chunk-30-1} \end{center}

Accessing and Using Imputed Data

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Access the complete datasets}
\NormalTok{completeData1 }\OtherTok{\textless{}{-}} \FunctionTok{complete}\NormalTok{(imputed\_Data, }\DecValTok{1}\NormalTok{)  }\CommentTok{\# First imputed dataset}
\NormalTok{completeData2 }\OtherTok{\textless{}{-}} \FunctionTok{complete}\NormalTok{(imputed\_Data, }\DecValTok{2}\NormalTok{)  }\CommentTok{\# Second imputed dataset}
\end{Highlighting}
\end{Shaded}

Regression Model with Imputed Dataset

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fit a regression model using imputed datasets}
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(}\AttributeTok{data =}\NormalTok{ imputed\_Data, }\AttributeTok{exp =} \FunctionTok{lm}\NormalTok{(Sepal.Width }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Sepal.Length }\SpecialCharTok{+}\NormalTok{ Petal.Width))}

\CommentTok{\# Combine results of all 5 models}
\NormalTok{combine }\OtherTok{\textless{}{-}} \FunctionTok{pool}\NormalTok{(fit)}
\FunctionTok{summary}\NormalTok{(combine)}
\CommentTok{\#\textgreater{}           term   estimate  std.error statistic        df      p.value}
\CommentTok{\#\textgreater{} 1  (Intercept)  1.9054698 0.33454626  5.695684 105.12438 1.127064e{-}07}
\CommentTok{\#\textgreater{} 2 Sepal.Length  0.2936285 0.07011405  4.187870  88.69066 6.625536e{-}05}
\CommentTok{\#\textgreater{} 3  Petal.Width {-}0.4742921 0.08138313 {-}5.827892  46.94941 4.915270e{-}07}
\end{Highlighting}
\end{Shaded}

\hypertarget{amelia}{%
\subsection{Amelia}\label{amelia}}

Amelia uses a \textbf{bootstrap-based Expectation-Maximization with Bootstrapping (EMB) algorithm} for imputation, making it faster and suitable for cross-sectional and time-series data.

\hypertarget{assumptions}{%
\subsubsection{Assumptions}\label{assumptions}}

\begin{itemize}
\item
  All variables must follow a \textbf{Multivariate Normal Distribution (MVN)}. Transformations may be required for non-normal data.
\item
  Data must be \protect\hyperlink{missing-at-random-mar}{\textbf{Missing at Random (MAR)}}.
\end{itemize}

\hypertarget{comparison-amelia-vs.-mice}{%
\subsubsection{Comparison: Amelia vs.~MICE}\label{comparison-amelia-vs.-mice}}

\begin{itemize}
\item
  \textbf{MICE} imputes on a variable-by-variable basis using separate models.
\item
  \textbf{Amelia} uses a joint modeling approach based on MVN.
\item
  \textbf{MICE} handles multiple data types, while \textbf{Amelia} requires variables to approximate normality.
\end{itemize}

\hypertarget{imputation-with-amelia}{%
\subsubsection{Imputation with Amelia}\label{imputation-with-amelia}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(Amelia)}
\FunctionTok{data}\NormalTok{(}\StringTok{"iris"}\NormalTok{)}

\CommentTok{\# Seed 10\% missing values}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{iris.mis }\OtherTok{\textless{}{-}} \FunctionTok{prodNA}\NormalTok{(iris, }\AttributeTok{noNA =} \FloatTok{0.1}\NormalTok{)}

\CommentTok{\# Specify columns and run Amelia}
\NormalTok{amelia\_fit }\OtherTok{\textless{}{-}} \FunctionTok{amelia}\NormalTok{(}
\NormalTok{  iris.mis,}
  \AttributeTok{m =} \DecValTok{5}\NormalTok{,                      }\CommentTok{\# Number of imputations}
  \AttributeTok{parallel =} \StringTok{"multicore"}\NormalTok{,     }\CommentTok{\# Use multicore processing}
  \AttributeTok{noms =} \StringTok{"Species"}            \CommentTok{\# Nominal variables}
\NormalTok{)}
\CommentTok{\#\textgreater{} {-}{-} Imputation 1 {-}{-}}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   1  2  3  4  5  6  7}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} {-}{-} Imputation 2 {-}{-}}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   1  2  3  4  5}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} {-}{-} Imputation 3 {-}{-}}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   1  2  3  4  5}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} {-}{-} Imputation 4 {-}{-}}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   1  2  3  4  5  6}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} {-}{-} Imputation 5 {-}{-}}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   1  2  3  4  5  6  7  8  9 10}

\CommentTok{\# Access imputed outputs}
\CommentTok{\# amelia\_fit$imputations[[1]]}
\end{Highlighting}
\end{Shaded}

Amelia's workflow includes bootstrapping multiple imputations to generate robust estimates of means and variances. This process ensures flexibility and speed for large datasets.

\hypertarget{missforest}{%
\subsection{missForest}\label{missforest}}

The \texttt{missForest} package provides a robust non-parametric imputation method using the Random Forest algorithm. It is versatile, handling both continuous and categorical variables without requiring assumptions about the underlying functional forms.

\textbf{Key Features of missForest}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Non-Parametric}: No assumptions about the functional form.
\item
  \textbf{Variable-Specific Models}: Builds a random forest model for each variable to impute missing values.
\item
  \textbf{Error Estimates}: Provides out-of-bag (OOB) imputation error estimates.

  \begin{itemize}
  \tightlist
  \item
    \textbf{NRMSE} (Normalized Root Mean Squared Error) for continuous variables.
  \item
    \textbf{PFC} (Proportion of Falsely Classified) for categorical variables.
  \end{itemize}
\item
  \textbf{High Control}: Offers customizable parameters like \texttt{mtry} and \texttt{ntree}.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(missForest)}

\CommentTok{\# Impute missing values using default parameters}
\NormalTok{iris.imp }\OtherTok{\textless{}{-}} \FunctionTok{missForest}\NormalTok{(iris.mis)}

\CommentTok{\# Check imputed values}
\CommentTok{\# View the imputed dataset}
\CommentTok{\# iris.imp$ximp}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Out{-}of{-}bag error estimates}
\NormalTok{iris.imp}\SpecialCharTok{$}\NormalTok{OOBerror}
\CommentTok{\#\textgreater{}      NRMSE        PFC }
\CommentTok{\#\textgreater{} 0.14004144 0.02877698}

\CommentTok{\# Compare imputed data with original data to calculate error}
\NormalTok{iris.err }\OtherTok{\textless{}{-}} \FunctionTok{mixError}\NormalTok{(iris.imp}\SpecialCharTok{$}\NormalTok{ximp, iris.mis, iris)}
\NormalTok{iris.err}
\CommentTok{\#\textgreater{}      NRMSE        PFC }
\CommentTok{\#\textgreater{} 0.14420833 0.09090909}
\end{Highlighting}
\end{Shaded}

\hypertarget{hmisc}{%
\subsection{Hmisc}\label{hmisc}}

The \texttt{Hmisc} package provides a suite of tools for imputing missing data, offering both simple methods (like mean or median imputation) and more advanced approaches like \texttt{aregImpute}.

\textbf{Features of Hmisc}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{\texttt{impute()}}: Simple imputation using user-defined methods like mean, median, or a random value.
\item
  \textbf{\texttt{aregImpute()}}:

  \begin{itemize}
  \item
    Combines additive regression, bootstrapping, and predictive mean matching.
  \item
    Handles continuous and categorical variables.
  \item
    Automatically recognizes variable types and applies appropriate methods.
  \end{itemize}
\end{enumerate}

\textbf{Assumptions}

\begin{itemize}
\item
  Linearity in the variables being predicted.
\item
  Fisher's optimum scoring is used for categorical variable prediction.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(Hmisc)}

\CommentTok{\# Impute using mean}
\NormalTok{iris.mis}\SpecialCharTok{$}\NormalTok{imputed\_SepalLength }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(iris.mis, }\FunctionTok{impute}\NormalTok{(Sepal.Length, mean))}

\CommentTok{\# Impute using random value}
\NormalTok{iris.mis}\SpecialCharTok{$}\NormalTok{imputed\_SepalLength2 }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(iris.mis, }\FunctionTok{impute}\NormalTok{(Sepal.Length, }\StringTok{\textquotesingle{}random\textquotesingle{}}\NormalTok{))}

\CommentTok{\# Advanced imputation using aregImpute}
\NormalTok{impute\_arg }\OtherTok{\textless{}{-}} \FunctionTok{aregImpute}\NormalTok{(}
  \SpecialCharTok{\textasciitilde{}}\NormalTok{ Sepal.Length }\SpecialCharTok{+}\NormalTok{ Sepal.Width }\SpecialCharTok{+}\NormalTok{ Petal.Length }\SpecialCharTok{+}\NormalTok{ Petal.Width }\SpecialCharTok{+}\NormalTok{ Species,}
  \AttributeTok{data =}\NormalTok{ iris.mis,}
  \AttributeTok{n.impute =} \DecValTok{5}
\NormalTok{)}
\CommentTok{\#\textgreater{} Iteration 1 }
\NormalTok{Iteration }\DecValTok{2} 
\NormalTok{Iteration }\DecValTok{3} 
\NormalTok{Iteration }\DecValTok{4} 
\NormalTok{Iteration }\DecValTok{5} 
\NormalTok{Iteration }\DecValTok{6} 
\NormalTok{Iteration }\DecValTok{7} 
\NormalTok{Iteration }\DecValTok{8} 

\CommentTok{\# Check R{-}squared values for predicted missing values}
\NormalTok{impute\_arg}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Multiple Imputation using Bootstrap and PMM}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} aregImpute(formula = \textasciitilde{}Sepal.Length + Sepal.Width + Petal.Length + }
\CommentTok{\#\textgreater{}     Petal.Width + Species, data = iris.mis, n.impute = 5)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} n: 150   p: 5    Imputations: 5      nk: 3 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of NAs:}
\CommentTok{\#\textgreater{} Sepal.Length  Sepal.Width Petal.Length  Petal.Width      Species }
\CommentTok{\#\textgreater{}           17           19           12           16           11 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}              type d.f.}
\CommentTok{\#\textgreater{} Sepal.Length    s    2}
\CommentTok{\#\textgreater{} Sepal.Width     s    2}
\CommentTok{\#\textgreater{} Petal.Length    s    2}
\CommentTok{\#\textgreater{} Petal.Width     s    2}
\CommentTok{\#\textgreater{} Species         c    2}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Transformation of Target Variables Forced to be Linear}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} R{-}squares for Predicting Non{-}Missing Values for Each Variable}
\CommentTok{\#\textgreater{} Using Last Imputations of Predictors}
\CommentTok{\#\textgreater{} Sepal.Length  Sepal.Width Petal.Length  Petal.Width      Species }
\CommentTok{\#\textgreater{}        0.895        0.536        0.987        0.967        0.984}

\CommentTok{\# Access imputed values for Sepal.Length}
\NormalTok{impute\_arg}\SpecialCharTok{$}\NormalTok{imputed}\SpecialCharTok{$}\NormalTok{Sepal.Length}
\CommentTok{\#\textgreater{}     [,1] [,2] [,3] [,4] [,5]}
\CommentTok{\#\textgreater{} 13   4.4  4.9  4.9  5.0  4.9}
\CommentTok{\#\textgreater{} 14   4.8  4.4  5.0  4.5  4.5}
\CommentTok{\#\textgreater{} 23   4.8  5.1  5.1  5.1  4.8}
\CommentTok{\#\textgreater{} 26   5.0  4.8  4.9  4.9  5.0}
\CommentTok{\#\textgreater{} 34   5.0  5.8  6.0  5.7  5.8}
\CommentTok{\#\textgreater{} 39   4.4  4.9  5.0  4.5  4.6}
\CommentTok{\#\textgreater{} 41   5.2  5.1  4.8  5.0  4.8}
\CommentTok{\#\textgreater{} 69   5.8  6.0  6.3  6.0  6.1}
\CommentTok{\#\textgreater{} 72   5.6  5.7  5.7  5.8  6.1}
\CommentTok{\#\textgreater{} 89   6.1  5.7  5.7  5.6  6.9}
\CommentTok{\#\textgreater{} 90   5.5  6.2  5.2  6.0  5.8}
\CommentTok{\#\textgreater{} 91   5.7  6.9  6.0  6.4  6.4}
\CommentTok{\#\textgreater{} 116  5.9  6.8  6.4  6.6  6.9}
\CommentTok{\#\textgreater{} 118  7.9  7.9  7.9  7.9  7.9}
\CommentTok{\#\textgreater{} 135  6.7  6.7  6.7  6.9  6.7}
\CommentTok{\#\textgreater{} 141  7.0  6.3  5.9  6.7  7.0}
\CommentTok{\#\textgreater{} 143  5.7  6.7  5.8  6.3  5.4}
\end{Highlighting}
\end{Shaded}

\textbf{Note}: While \texttt{missForest} often outperforms \texttt{Hmisc} in terms of accuracy, the latter is useful for datasets with simpler requirements.

\hypertarget{mi}{%
\subsection{mi}\label{mi}}

The \texttt{mi} package is a powerful tool for imputation, using Bayesian methods and providing rich diagnostics for model evaluation and convergence.

\textbf{Features of mi}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Graphical Diagnostics}: Visualize imputation models and convergence.
\item
  \textbf{Bayesian Regression}: Handles separation and other issues in data.
\item
  \textbf{Irregularity Detection}: Automatically detects issues like high collinearity.
\item
  \textbf{Noise Addition}: Adds noise to address additive constraints.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(mi)}

\CommentTok{\# Perform imputation using mi}
\NormalTok{mi\_data }\OtherTok{\textless{}{-}} \FunctionTok{mi}\NormalTok{(iris.mis, }\AttributeTok{seed =} \DecValTok{1}\NormalTok{)}

\CommentTok{\# Summary of the imputation process}
\FunctionTok{summary}\NormalTok{(mi\_data)}
\CommentTok{\#\textgreater{} $Sepal.Length}
\CommentTok{\#\textgreater{} $Sepal.Length$is\_missing}
\CommentTok{\#\textgreater{} missing}
\CommentTok{\#\textgreater{} FALSE  TRUE }
\CommentTok{\#\textgreater{}   133    17 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Sepal.Length$imputed}
\CommentTok{\#\textgreater{}     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. }
\CommentTok{\#\textgreater{} {-}0.29024 {-}0.08355 {-}0.01937 {-}0.01653  0.06134  0.16642 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Sepal.Length$observed}
\CommentTok{\#\textgreater{}     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. }
\CommentTok{\#\textgreater{} {-}0.90110 {-}0.47329 {-}0.04549  0.00000  0.32120  1.23792 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Sepal.Width}
\CommentTok{\#\textgreater{} $Sepal.Width$is\_missing}
\CommentTok{\#\textgreater{} missing}
\CommentTok{\#\textgreater{} FALSE  TRUE }
\CommentTok{\#\textgreater{}   131    19 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Sepal.Width$imputed}
\CommentTok{\#\textgreater{}     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. }
\CommentTok{\#\textgreater{} {-}1.53650 {-}0.25278  0.05430  0.00146  0.25245  1.64863 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Sepal.Width$observed}
\CommentTok{\#\textgreater{}     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. }
\CommentTok{\#\textgreater{} {-}1.01272 {-}0.30642 {-}0.07099  0.00000  0.39988  1.34161 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Petal.Length}
\CommentTok{\#\textgreater{} $Petal.Length$is\_missing}
\CommentTok{\#\textgreater{} missing}
\CommentTok{\#\textgreater{} FALSE  TRUE }
\CommentTok{\#\textgreater{}   138    12 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Petal.Length$imputed}
\CommentTok{\#\textgreater{}     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. }
\CommentTok{\#\textgreater{} {-}0.92285 {-}0.56860  0.32153  0.04637  0.48225  0.73268 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Petal.Length$observed}
\CommentTok{\#\textgreater{}    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. }
\CommentTok{\#\textgreater{} {-}0.7797 {-}0.6088  0.1459  0.0000  0.3880  0.9006 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Petal.Width}
\CommentTok{\#\textgreater{} $Petal.Width$is\_missing}
\CommentTok{\#\textgreater{} missing}
\CommentTok{\#\textgreater{} FALSE  TRUE }
\CommentTok{\#\textgreater{}   134    16 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Petal.Width$imputed}
\CommentTok{\#\textgreater{}     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. }
\CommentTok{\#\textgreater{} {-}0.84088  0.08421  0.29088  0.19524  0.48763  0.84647 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Petal.Width$observed}
\CommentTok{\#\textgreater{}     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. }
\CommentTok{\#\textgreater{} {-}0.69624 {-}0.56602  0.08503  0.00000  0.41055  0.86629 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Species}
\CommentTok{\#\textgreater{} $Species$crosstab}
\CommentTok{\#\textgreater{}             }
\CommentTok{\#\textgreater{}              observed imputed}
\CommentTok{\#\textgreater{}   setosa          180      18}
\CommentTok{\#\textgreater{}   versicolor      192      11}
\CommentTok{\#\textgreater{}   virginica       184      15}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $imputed\_SepalLength}
\CommentTok{\#\textgreater{} $imputed\_SepalLength$is\_missing}
\CommentTok{\#\textgreater{} [1] "all values observed"}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $imputed\_SepalLength$observed}
\CommentTok{\#\textgreater{}    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. }
\CommentTok{\#\textgreater{} {-}0.9574 {-}0.4379  0.0000  0.0000  0.3413  1.3152 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $imputed\_SepalLength2}
\CommentTok{\#\textgreater{} $imputed\_SepalLength2$is\_missing}
\CommentTok{\#\textgreater{} [1] "all values observed"}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $imputed\_SepalLength2$observed}
\CommentTok{\#\textgreater{}     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. }
\CommentTok{\#\textgreater{} {-}0.90570 {-}0.48398 {-}0.06225  0.00000  0.35947  1.20292}
\end{Highlighting}
\end{Shaded}

\hypertarget{data}{%
\chapter{Data}\label{data}}

There are multiple ways to categorize data. For example,

\begin{itemize}
\tightlist
\item
  Qualitative vs.~Quantitative:
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5556}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.4444}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Qualitative
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Quantitative
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
in-depth interviews, documents, focus groups, case study, ethnography. open-ended questions. observations in words & experiments, observation in words, survey with closed-end questions, structured interviews \\
language, descriptive & quantities, numbers \\
Text-based & Numbers-based \\
Subjective & Objectivity \\
\end{longtable}

\hypertarget{cross-sectional}{%
\section{Cross-Sectional}\label{cross-sectional}}

\hypertarget{time-series}{%
\section{Time Series}\label{time-series}}

\[
y_t = \beta_0 + x_{t1}\beta_1 + x_{t2}\beta_2 + ... + x_{t(k-1)}\beta_{k-1} + \epsilon_t
\]

Examples

\begin{itemize}
\item
  Static Model

  \begin{itemize}
  \tightlist
  \item
    \(y_t=\beta_0 + x_1\beta_1 + x_2\beta_2 - x_3\beta_3 - \epsilon_t\)
  \end{itemize}
\item
  Finite Distributed Lag model

  \begin{itemize}
  \tightlist
  \item
    \(y_t=\beta_0 + pe_t\delta_0 + pe_{t-1}\delta_1 +pe_{t-2}\delta_2 + \epsilon_t\)
  \item
    \textbf{Long Run Propensity (LRP)} is \(LRP = \delta_0 + \delta_1 + \delta_2\)
  \end{itemize}
\item
  Dynamic Model

  \begin{itemize}
  \tightlist
  \item
    \(GDP_t = \beta_0 + \beta_1GDP_{t-1} - \epsilon_t\)
  \end{itemize}
\end{itemize}

\protect\hyperlink{finite-sample-properties}{Finite Sample Properties} for \protect\hyperlink{time-series}{Time Series}:

\begin{itemize}
\tightlist
\item
  A1-A3: OLS is unbiased
\item
  A1-A4: usual standard errors are consistent and \protect\hyperlink{gauss-markov-theorem}{Gauss-Markov Theorem} holds (OLS is BLUE)
\item
  A1-A6, A6: Finite Sample \protect\hyperlink{sec-wald-test-logistic}{Wald Test} (t-test and F-test) are valid
\end{itemize}

\protect\hyperlink{a3-exogeneity-of-independent-variables}{A3} might not hold under time series setting

\begin{itemize}
\tightlist
\item
  Spurious Time Trend - solvable
\item
  \protect\hyperlink{a3-exogeneity-of-independent-variables}{Strict} vs Contemporaneous Exogeneity - not solvable
\end{itemize}

In time series data, there are many processes:

\begin{itemize}
\tightlist
\item
  Autoregressive model of order p: AR(p)
\item
  Moving average model of order q: MA(q)
\item
  Autoregressive model of order p and moving average model of order q: ARMA(p,q)
\item
  Autoregressive conditional heteroskedasticity model of order p: ARCH(p)
\item
  Generalized Autoregressive conditional heteroskedasticity of orders p and q; GARCH(p.q)
\end{itemize}

\hypertarget{deterministic-time-trend}{%
\subsection{Deterministic Time trend}\label{deterministic-time-trend}}

Both the dependent and independent variables are trending over time

\textbf{Spurious Time Series Regression}

\[
y_t = \alpha_0 + t\alpha_1 + v_t
\]

and x takes the form

\[
x_t = \lambda_0 + t\lambda_1 + u_t
\]

\begin{itemize}
\tightlist
\item
  \(\alpha_1 \neq 0\) and \(\lambda_1 \neq 0\)
\item
  \(v_t\) and \(u_t\) are independent
\item
  there is no relationship between \(y_t\) and \(x_t\)
\end{itemize}

If we estimate the regression,

\[
y_t = \beta_0 + x_t\beta_1 + \epsilon_t
\]

so the true \(\beta_1=0\)

\begin{itemize}
\tightlist
\item
  Inconsistent: \(plim(\hat{\beta}_1)=\frac{\alpha_1}{\lambda_1}\)
\item
  Invalid Inference: \(|t| \to^d \infty\) for \(H_0: \beta_1=0\), will always reject the null as \(n \to \infty\)
\item
  Uninformative \(R^2\): \(plim(R^2) = 1\) will be able to perfectly predict as \(n \to \infty\)
\end{itemize}

We can rewrite the equation as

\[
\begin{aligned}
y_t &=\beta_0 + \beta_1x_t+\epsilon_t \\
\epsilon_t &= \alpha_1t + v_t
\end{aligned}
\]

where \(\beta_0 = \alpha_0\) and \(\beta_1=0\). Since \(x_t\) is a deterministic function of time, \(\epsilon_t\) is correlated with \(x_t\) and we have the usual omitted variable bias.\\
Even when \(y_t\) and \(x_t\) are related (\(\beta_1 \neq 0\)) but they are both trending over time, we still get spurious results with the simple regression on \(y_t\) on \(x_t\)

\textbf{Solutions to Spurious Trend}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Include time trend \(t\) as an additional control

  \begin{itemize}
  \tightlist
  \item
    consistent parameter estimates and valid inference
  \end{itemize}
\item
  Detrend both dependent and independent variables and then regress the detrended outcome on detrended independent variables (i.e., regress residuals \(\hat{u}_t\) on residuals \(\hat{v}_t\))

  \begin{itemize}
  \item
    Detrending is the same as partialing out in the {[}Frisch-Waugh-Lovell Theorem{]}

    \begin{itemize}
    \tightlist
    \item
      Could allow for non-linear time trends by including \(t\) \(t^2\), and \(\exp(t)\)
    \item
      Allow for seasonality by including indicators for relevant ``seasons'' (quarters, months, weeks).
    \end{itemize}
  \end{itemize}
\end{enumerate}

\protect\hyperlink{a3-exogeneity-of-independent-variables}{A3} does not hold under:

\begin{itemize}
\item
  \protect\hyperlink{feedback-effect}{Feedback Effect}

  \begin{itemize}
  \tightlist
  \item
    \(\epsilon_t\) influences next period's independent variables
  \end{itemize}
\item
  \protect\hyperlink{dynamic-specification}{Dynamic Specification}

  \begin{itemize}
  \tightlist
  \item
    include last time period outcome as an explanatory variable
  \end{itemize}
\item
  \protect\hyperlink{dynamically-complete}{Dynamically Complete}

  \begin{itemize}
  \tightlist
  \item
    For finite distrusted lag model, the number of lags needs to be absolutely correct.
  \end{itemize}
\end{itemize}

\hypertarget{feedback-effect}{%
\subsection{Feedback Effect}\label{feedback-effect}}

\[
y_t = \beta_0 + x_t\beta_1 + \epsilon_t
\]

\protect\hyperlink{a3-exogeneity-of-independent-variables}{A3}

\[
E(\epsilon_t|\mathbf{X})= E(\epsilon_t| x_1,x_2, ...,x_t,x_{t+1},...,x_T)
\]

will not equal 0, because \(y_t\) will likely influence \(x_{t+1},..,x_T\)

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{a3-exogeneity-of-independent-variables}{A3} is violated because we require the error to be uncorrelated with all time observation of the independent regressors (\textbf{strict exogeneity})
\end{itemize}

\hypertarget{dynamic-specification}{%
\subsection{Dynamic Specification}\label{dynamic-specification}}

\[
y_t = \beta_0 + y_{t-1}\beta_1 + \epsilon_t
\]

\[
E(\epsilon_t|\mathbf{X})= E(\epsilon_t| y_1,y_2, ...,y_t,y_{t+1},...,y_T)
\]

will not equal 0, because \(y_t\) and \(\epsilon_t\) are inherently correlated

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{a3-exogeneity-of-independent-variables}{A3} is violated because we require the error to be uncorrelated with all time observation of the independent regressors (\textbf{strict exogeneity})
\item
  \protect\hyperlink{dynamic-specification}{Dynamic Specification} is not allowed under \protect\hyperlink{a3-exogeneity-of-independent-variables}{A3}
\end{itemize}

\hypertarget{dynamically-complete}{%
\subsection{Dynamically Complete}\label{dynamically-complete}}

\[
y_t = \beta_0 + x_t\delta_0 + x_{t-1}\delta_1 + \epsilon_t
\]

\[
E(\epsilon_t|\mathbf{X})= E(\epsilon_t| x_1,x_2, ...,x_t,x_{t+1},...,x_T)
\]

will not equal 0, because if we did not include enough lags, \(x_{t-2}\) and \(\epsilon_t\) are correlated

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{a3-exogeneity-of-independent-variables}{A3} is violated because we require the error to be uncorrelated with all time observation of the independent regressors (strict exogeneity)
\item
  Can be corrected by including more lags (but when stop? )
\end{itemize}

Without \protect\hyperlink{a3-exogeneity-of-independent-variables}{A3}

\begin{itemize}
\tightlist
\item
  OLS is biased
\item
  \protect\hyperlink{gauss-markov-theorem}{Gauss-Markov Theorem}
\item
  \protect\hyperlink{finite-sample-properties}{Finite Sample Properties} are invalid
\end{itemize}

then, we can

\begin{itemize}
\tightlist
\item
  Focus on \protect\hyperlink{large-sample-properties}{Large Sample Properties}
\item
  Can use {[}A3a{]} instead of \protect\hyperlink{a3-exogeneity-of-independent-variables}{A3}
\end{itemize}

{[}A3a{]} in time series become

\[
A3a: E(\mathbf{x}_t'\epsilon_t)= 0
\]

only the regressors in this time period need to be independent from the error in this time period (\textbf{Contemporaneous Exogeneity})

\begin{itemize}
\tightlist
\item
  \(\epsilon_t\) can be correlated with \(...,x_{t-2},x_{t-1},x_{t+1}, x_{t+2},...\)
\item
  can have a dynamic specification \(y_t = \beta_0 + y_{t-1}\beta_1 + \epsilon_t\)
\end{itemize}

Deriving \protect\hyperlink{large-sample-properties}{Large Sample Properties} for Time Series

\begin{itemize}
\item
  Assumptions \protect\hyperlink{a1-linearity}{A1}, \protect\hyperlink{a2-full-rank}{A2}, {[}A3a{]}
\item
  {[}Weak Law{]} and \protect\hyperlink{central-limit-theorem}{Central Limit Theorem} depend on \protect\hyperlink{a5-data-generation-random-sampling}{A5}

  \begin{itemize}
  \tightlist
  \item
    \(x_t\) and \(\epsilon_t\) are dependent over t
  \item
    without {[}Weak Law{]} or \protect\hyperlink{central-limit-theorem}{Central Limit Theorem} depend on \protect\hyperlink{a5-data-generation-random-sampling}{A5}, we cannot have \protect\hyperlink{large-sample-properties}{Large Sample Properties} for \protect\hyperlink{ordinary-least-squares}{OLS}
  \item
    Instead of \protect\hyperlink{a5-data-generation-random-sampling}{A5}, we consider {[}A5a{]}
  \end{itemize}
\item
  Derivation of the Asymptotic Variance depends on \protect\hyperlink{a4-homoskedasticity}{A4}

  \begin{itemize}
  \tightlist
  \item
    time series setting introduces \textbf{Serial Correlation}: \(Cov(\epsilon_t, \epsilon_s) \neq 0\)
  \end{itemize}
\end{itemize}

under \protect\hyperlink{a1-linearity}{A1}, \protect\hyperlink{a2-full-rank}{A2}, {[}A3a{]}, and {[}A5a{]}, \protect\hyperlink{ordinary-least-squares}{OLS estimator} is \textbf{consistent}, and \textbf{asymptotically normal}

\hypertarget{highly-persistent-data}{%
\subsection{Highly Persistent Data}\label{highly-persistent-data}}

If \(y_t, \mathbf{x}_t\) are not weakly dependent stationary process

\begin{itemize}
\item
  \(y_t\) and \(y_{t-h}\) are not almost independent for large h
\item
  {[}A5a{]} does not hold and OLS is not \textbf{consistent} and does not have a limiting distribution.
\item
  Example + Random Walk \(y_t = y_{t-1} + u_t\) + Random Walk with a drift: \(y_t = \alpha+ y_{t-1} + u_t\)
\end{itemize}

\textbf{Solution} First difference is a stationary process

\[
y_t - y_{t-1} = u_t
\]

\begin{itemize}
\tightlist
\item
  If \(u_t\) is a weakly dependent process (also called integrated of order 0) then \(y_t\) is said to be difference-stationary process (integrated of order 1)
\item
  For regression, if \(\{y_t, \mathbf{x}_t \}\) are random walks (integrated at order 1), can consistently estimate the first difference equation
\end{itemize}

\[
\begin{aligned}
y_t - y_{t-1} &= (\mathbf{x}_t - \mathbf{x}_{t-1}\beta + \epsilon_t - \epsilon_{t-1}) \\
\Delta y_t &= \Delta \mathbf{x}\beta + \Delta u_t
\end{aligned}
\]

\textbf{Unit Root Test}

\[
y_t = \alpha + \alpha y_{t-1} + u_t
\]

tests if \(\rho=1\) (integrated of order 1)\\

\begin{itemize}
\tightlist
\item
  Under the null \(H_0: \rho = 1\), OLS is not consistent or asymptotically normal.
\item
  Under the alternative \(H_a: \rho < 1\), OLS is consistent and asymptotically normal.
\item
  usual t-test is not valid, will need to use the transformed equation to produce a valid test.
\end{itemize}

\textbf{Dickey-Fuller Test} \[
\Delta y_t= \alpha + \theta y_{t-1} + v_t
\] where \(\theta = \rho -1\)\\

\begin{itemize}
\tightlist
\item
  \(H_0: \theta = 0\) and \(H_a: \theta < 0\)
\item
  Under the null, \(\Delta y_t\) is weakly dependent but \(y_{t-1}\) is not.
\item
  Dickey and Fuller derived the non-normal asymptotic distribution. If you reject the null then \(y_t\) is not a random walk.
\end{itemize}

Concerns with the standard Dickey Fuller Test\\
1. Only considers a fairly simplistic dynamic relationship

\[
\Delta y_t = \alpha + \theta y_{t-1} + \gamma_1 \Delta_{t-1} + ..+ \gamma_p \Delta_{t-p} +v_t
\]

\begin{itemize}
\tightlist
\item
  with one additional lag, under the null \(\Delta_{y_t}\) is an AR(1) process and under the alternative \(y_t\) is an AR(2) process.
\item
  Solution: include lags of \(\Delta_{y_t}\) as controls.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Does not allow for time trend \[
  \Delta y_t = \alpha + \theta y_{t-1} + \delta t + v_t
  \]
\end{enumerate}

\begin{itemize}
\tightlist
\item
  allows \(y_t\) to have a quadratic relationship with \(t\)
\item
  Solution: include time trend (changes the critical values).
\end{itemize}

\textbf{Adjusted Dickey-Fuller Test} \[
\Delta y_t = \alpha + \theta y_{t-1} + \delta t + \gamma_1 \Delta y_{t-1} + ... + \gamma_p \Delta y_{t-p} + v_t
\] where \(\theta = 1 - \rho\)\\

\begin{itemize}
\tightlist
\item
  \(H_0: \theta_1 = 0\) and \(H_a: \theta_1 < 0\)
\item
  Under the null, \(\Delta y_t\) is weakly dependent but \(y_{t-1}\) is not
\item
  Critical values are different with the time trend, if you reject the null then \(y_t\) is not a random walk.
\end{itemize}

\hypertarget{newey-west-standard-errors}{%
\paragraph{Newey West Standard Errors}\label{newey-west-standard-errors}}

If \protect\hyperlink{a4-homoskedasticity}{A4} does not hold, we can use Newey West Standard Errors (HAC - Heteroskedasticity Autocorrelation Consistent)

\[
\hat{B} = T^{-1} \sum_{t=1}^{T} e_t^2 \mathbf{x'_tx_t} + \sum_{h=1}^{g}(1-\frac{h}{g+1})T^{-1}\sum_{t=h+1}^{T} e_t e_{t-h}(\mathbf{x_t'x_{t-h}+ x_{t-h}'x_t})
\]

\begin{itemize}
\item
  estimates the covariances up to a distance g part
\item
  downweights to insure \(\hat{B}\) is PSD
\item
  How to choose g:

  \begin{itemize}
  \tightlist
  \item
    For yearly data: \(g = 1\) or 2 is likely to account for most of the correlation
  \item
    For quarterly or monthly data: g should be larger (\$g = 4\$ or 8 for quarterly and \(g = 12\) or 14 for monthly)
  \item
    can also take integer part of \(4(T/100)^{2/9}\) or integer part of \(T^{1/4}\)
  \end{itemize}
\end{itemize}

\textbf{Testing for Serial Correlation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Run OLS regression of \(y_t\) on \(\mathbf{x_t}\) and obtain residuals \(e_t\)
\item
  Run OLS regression of \(e_t\) on \(\mathbf{x}_t, e_{t-1}\) and test whether coefficient on \(e_{t-1}\) is significant.
\item
  Reject the null of no serial correlation if the coefficient is significant at the 5\% level.

  \begin{itemize}
  \tightlist
  \item
    Test using heteroskedastic robust standard errors
  \item
    can include \(e_{t-2},e_{t-3},..\) in step 2 to test for higher order serial correlation (t-test would now be an F-test of joint significance)
  \end{itemize}
\end{enumerate}

\hypertarget{repeated-cross-sections}{%
\section{Repeated Cross Sections}\label{repeated-cross-sections}}

For each time point (day, month, year, etc.), a set of data is sampled. This set of data can be different among different time points.

For example, you can sample different groups of students each time you survey.

Allowing structural change in pooled cross section

\[
y_i = \mathbf{x}_i \beta + \delta_1 y_1 + ... + \delta_T y_T + \epsilon_i
\]

Dummy variables for all but one time period

\begin{itemize}
\tightlist
\item
  allows different intercept for each time period
\item
  allows outcome to change on average for each time period
\end{itemize}

Allowing for structural change in pooled cross section

\[
y_i = \mathbf{x}_i \beta + \mathbf{x}_i y_1 \gamma_1 + ... + \mathbf{x}_i y_T \gamma_T + \delta_1 y_1 + ...+ \delta_T y_T + \epsilon_i
\]

Interact \(x_i\) with time period dummy variables

\begin{itemize}
\tightlist
\item
  allows different slopes for each time period
\item
  allows effects to change based on time period (\textbf{structural break})
\item
  Interacting all time period dummies with \(x_i\) can produce many variables - use hypothesis testing to determine which structural breaks are needed.
\end{itemize}

\hypertarget{pooled-cross-section}{%
\subsection{Pooled Cross Section}\label{pooled-cross-section}}

\[
y_i=\mathbf{x_i\beta +x_i \times y1\gamma_1 + ...+ x_i \times yT\gamma_T + \delta_1y_1+...+ \delta_Ty_T + \epsilon_i}
\]

Interact \(x_i\) with time period dummy variables

\begin{itemize}
\item
  allows different slopes for each time period
\item
  allows effect to change based on time period (structural break)

  \begin{itemize}
  \tightlist
  \item
    interacting all time period dummies with \(x_i\) can produce many variables - use hypothesis testing to determine which structural breaks are needed.
  \end{itemize}
\end{itemize}

\hypertarget{panel-data}{%
\section{Panel Data}\label{panel-data}}

Detail notes in R can be found \href{https://cran.r-project.org/web/packages/plm/vignettes/plmPackage.html\#robust}{here}

Follows an individual over T time periods.

Panel data structure is like having n samples of time series data

\textbf{Characteristics}

\begin{itemize}
\item
  Information both across individuals and over time (cross-sectional and time-series)
\item
  N individuals and T time periods
\item
  Data can be either

  \begin{itemize}
  \tightlist
  \item
    Balanced: all individuals are observed in all time periods
  \item
    Unbalanced: all individuals are not observed in all time periods.
  \end{itemize}
\item
  Assume correlation (clustering) over time for a given individual, with independence over individuals.
\end{itemize}

\textbf{Types}

\begin{itemize}
\tightlist
\item
  Short panel: many individuals and few time periods.
\item
  Long panel: many time periods and few individuals
\item
  Both: many time periods and many individuals
\end{itemize}

\textbf{Time Trends and Time Effects}

\begin{itemize}
\tightlist
\item
  Nonlinear
\item
  Seasonality
\item
  Discontinuous shocks
\end{itemize}

\textbf{Regressors}

\begin{itemize}
\tightlist
\item
  Time-invariant regressors \(x_{it}=x_i\) for all t (e.g., gender, race, education) have zero within variation
\item
  Individual-invariant regressors \(x_{it}=x_{t}\) for all i (e.g., time trend, economy trends) have zero between variation
\end{itemize}

\textbf{Variation for the dependent variable and regressors}

\begin{itemize}
\tightlist
\item
  Overall variation: variation over time and individuals.
\item
  Between variation: variation between individuals
\item
  Within variation: variation within individuals (over time).
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.1972}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.8028}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Estimate
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Formula
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Individual mean & \(\bar{x_i}= \frac{1}{T} \sum_{t}x_{it}\) \\
Overall mean & \(\bar{x}=\frac{1}{NT} \sum_{i} \sum_t x_{it}\) \\
Overall Variance & \(s _O^2 = \frac{1}{NT-1} \sum_i \sum_t (x_{it} - \bar{x})^2\) \\
Between variance & \(s_B^2 = \frac{1}{N-1} \sum_i (\bar{x_i} -\bar{x})^2\) \\
Within variance & \(s_W^2= \frac{1}{NT-1} \sum_i \sum_t (x_{it} - \bar{x_i})^2 = \frac{1}{NT-1} \sum_i \sum_t (x_{it} - \bar{x_i} +\bar{x})^2\) \\
\end{longtable}

\textbf{Note}: \(s_O^2 \approx s_B^2 + s_W^2\)

Since we have n observation for each time period t, we can control for each time effect separately by including time dummies (time effects)

\[
y_{it}=\mathbf{x_{it}\beta} + d_1\delta_1+...+d_{T-1}\delta_{T-1} + \epsilon_{it}
\]

\textbf{Note}: we cannot use these many time dummies in time series data because in time series data, our n is 1. Hence, there is no variation, and sometimes not enough data compared to variables to estimate coefficients.

\textbf{Unobserved Effects Model} Similar to group clustering, assume that there is a random effect that captures differences across individuals but is constant in time.

\[
y_it=\mathbf{x_{it}\beta} + d_1\delta_1+...+d_{T-1}\delta_{T-1} + c_i + u_{it}
\]

where

\begin{itemize}
\tightlist
\item
  \(c_i + u_{it} = \epsilon_{it}\)
\item
  \(c_i\) unobserved individual heterogeneity (effect)
\item
  \(u_{it}\) idiosyncratic shock
\item
  \(\epsilon_{it}\) unobserved error term.
\end{itemize}

\hypertarget{pooled-ols-estimator}{%
\subsection{Pooled OLS Estimator}\label{pooled-ols-estimator}}

If \(c_i\) is uncorrelated with \(x_{it}\)

\[
E(\mathbf{x_{it}'}(c_i+u_{it})) = 0
\]

then {[}A3a{]} still holds. And we have Pooled OLS consistent.

If \protect\hyperlink{a4-homoskedasticity}{A4} does not hold, OLS is still consistent, but not efficient, and we need cluster robust SE.

Sufficient for {[}A3a{]} to hold, we need

\begin{itemize}
\tightlist
\item
  \textbf{Exogeneity} for \(u_{it}\) {[}A3a{]} (contemporaneous exogeneity): \(E(\mathbf{x_{it}'}u_{it})=0\) time varying error
\item
  \textbf{Random Effect Assumption} (time constant error): \(E(\mathbf{x_{it}'}c_{i})=0\)
\end{itemize}

Pooled OLS will give you consistent coefficient estimates under \protect\hyperlink{a1-linearity}{A1}, \protect\hyperlink{a2-full-rank}{A2}, {[}A3a{]} (for both \(u_{it}\) and RE assumption), and \protect\hyperlink{a5-data-generation-random-sampling}{A5} (randomly sampling across i).

\hypertarget{individual-specific-effects-model}{%
\subsection{Individual-specific effects model}\label{individual-specific-effects-model}}

\begin{itemize}
\tightlist
\item
  If we believe that there is unobserved heterogeneity across individual (e.g., unobserved ability of an individual affects \(y\)), If the individual-specific effects are correlated with the regressors, then we have the \protect\hyperlink{fixed-effects-estimator}{Fixed Effects Estimator}. and if they are not correlated we have the \protect\hyperlink{random-effects-estimator}{Random Effects Estimator}.
\end{itemize}

\hypertarget{random-effects-estimator}{%
\subsubsection{Random Effects Estimator}\label{random-effects-estimator}}

Random Effects estimator is the Feasible GLS estimator that assumes \(u_{it}\) is serially uncorrelated and homoskedastic

\begin{itemize}
\item
  Under \protect\hyperlink{a1-linearity}{A1}, \protect\hyperlink{a2-full-rank}{A2}, {[}A3a{]} (for both \(u_{it}\) and RE assumption) and \protect\hyperlink{a5-data-generation-random-sampling}{A5} (randomly sampling across i), RE estimator is consistent.

  \begin{itemize}
  \tightlist
  \item
    If \protect\hyperlink{a4-homoskedasticity}{A4} holds for \(u_{it}\), RE is the most efficient estimator
  \item
    If \protect\hyperlink{a4-homoskedasticity}{A4} fails to hold (may be heteroskedasticity across i, and serial correlation over t), then RE is not the most efficient, but still more efficient than pooled OLS.
  \end{itemize}
\end{itemize}

\hypertarget{fixed-effects-estimator}{%
\subsubsection{Fixed Effects Estimator}\label{fixed-effects-estimator}}

also known as \textbf{Within Estimator} uses within variation (over time)

If the \textbf{RE assumption} is not hold (\(E(\mathbf{x_{it}'}c_i) \neq 0\)), then A3a does not hold (\(E(\mathbf{x_{it}'}\epsilon_i) \neq 0\)).

Hence, the OLS and RE are inconsistent/biased (because of omitted variable bias)

However, FE can only fix bias due to time-invariant factors (both observables and unobservables) correlated with treatment (not time-variant factors that correlated with the treatment).

The traditional FE technique is flawed when lagged dependent variables are included in the model. \citep{nickell1981biases} \citep{narayanan2013estimating}

With measurement error in the independent, FE will exacerbate the errors-in-the-variables bias.

\hypertarget{demean-approach}{%
\paragraph{Demean Approach}\label{demean-approach}}

To deal with violation in \(c_i\), we have

\[
y_{it}= \mathbf{x_{it} \beta} + c_i + u_{it}
\]

\[
\bar{y_i}=\bar{\mathbf{x_i}} \beta + c_i + \bar{u_i}
\]

where the second equation is the time averaged equation

using \textbf{within transformation}, we have

\[
y_{it} - \bar{y_i} = \mathbf{(x_{it} - \bar{x_i})}\beta + u_{it} - \bar{u_i}
\]

because \(c_i\) is time constant.

The Fixed Effects estimator uses POLS on the transformed equation

\[
y_{it} - \bar{y_i} = \mathbf{(x_{it} - \bar{x_i})} \beta + d_1\delta_1 + ... + d_{T-2}\delta_{T-2} + u_{it} - \bar{u_i}
\]

\begin{itemize}
\item
  we need \protect\hyperlink{a3-exogeneity-of-independent-variables}{A3} (strict exogeneity) (\(E((\mathbf{x_{it}-\bar{x_i}})'(u_{it}-\bar{u_i})=0\)) to have FE consistent.
\item
  Variables that are time constant will be absorbed into \(c_i\). Hence we cannot make inference on time constant independent variables.

  \begin{itemize}
  \tightlist
  \item
    If you are interested in the effects of time-invariant variables, you could consider the OLS or \textbf{between estimator}
  \end{itemize}
\item
  It's recommended that you should still use cluster robust standard errors.
\end{itemize}

\hypertarget{dummy-approach}{%
\paragraph{Dummy Approach}\label{dummy-approach}}

Equivalent to the within transformation (i.e., mathematically equivalent to \protect\hyperlink{demean-approach}{Demean Approach}), we can have the fixed effect estimator be the same with the dummy regression

\[
y_{it} = x_{it}\beta + d_1\delta_1 + ... + d_{T-2}\delta_{T-2} + c_1\gamma_1 + ... + c_{n-1}\gamma_{n-1} + u_{it}
\]

where

\[
c_i
=
\begin{cases}
1 &\text{if observation is i} \\
0 &\text{otherwise} \\
\end{cases}
\]

\begin{itemize}
\tightlist
\item
  The standard error is incorrectly calculated.
\item
  the FE within transformation is controlling for any difference across individual which is allowed to correlated with observables.
\end{itemize}

\hypertarget{first-difference-approach}{%
\paragraph{First-difference Approach}\label{first-difference-approach}}

Economists typically use this approach

\[
y_{it} - y_{i (t-1)} = (\mathbf{x}_{it} - \mathbf{x}_{i(t-1)}) \beta +  + (u_{it} - u_{i(t-1)})
\]

\hypertarget{fixed-effects-summary}{%
\paragraph{Fixed Effects Summary}\label{fixed-effects-summary}}

\begin{itemize}
\item
  The three approaches are \textbf{almost} equivalent.

  \begin{itemize}
  \item
    \protect\hyperlink{demean-approach}{Demean Approach} is mathematically equivalent to \protect\hyperlink{dummy-approach}{Dummy Approach}
  \item
    If you have only 1 period, all 3 are the same.
  \end{itemize}
\item
  Since fixed effect is a within estimator, only \textbf{status changes} can contribute to \(\beta\) variation.

  \begin{itemize}
  \tightlist
  \item
    Hence, with a small number of changes then the standard error for \(\beta\) will explode
  \end{itemize}
\item
  Status changes mean subjects change from (1) control to treatment group or (2) treatment to control group. Those who have status change, we call them \textbf{switchers}.

  \begin{itemize}
  \item
    Treatment effect is typically \textbf{non-directional}.
  \item
    You can give a parameter for the direction if needed.
  \end{itemize}
\item
  Issues:

  \begin{itemize}
  \item
    You could have fundamental difference between switchers and non-switchers. Even though we can't definitive test this, but providing descriptive statistics on switchers and non-switchers can give us confidence in our conclusion.
  \item
    Because fixed effects focus on bias reduction, you might have larger variance (typically, with fixed effects you will have less df)
  \end{itemize}
\item
  If the true model is \protect\hyperlink{random-effects-estimator}{random effect}, economists typically don't care, especially when \(c_i\) is the random effect and \(c_i \perp x_{it}\) (because RE assumption is that it is unrelated to \(x_{it}\)). The reason why economists don't care is because RE wouldn't correct bias, it only improves efficiency over OLS.
\item
  You can estimate FE for different units (not just individuals).
\item
  FE removes bias from time invariant factors but not without costs because it uses within variation, which imposes strict exogeneity assumption on \(u_{it}\): \(E[(x_{it} - \bar{x}_{i})(u_{it} - \bar{u}_{it})]=0\)
\end{itemize}

Recall

\[
Y_{it} = \beta_0 + X_{it}\beta_1 + \alpha_i + u_{it}
\]

where \(\epsilon_{it} = \alpha_i + u_{it}\)

\[
\hat{\sigma}^2_\epsilon = \frac{SSR_{OLS}}{NT - K}
\]

\[
\hat{\sigma}^2_u = \frac{SSR_{FE}}{NT - (N+K)} = \frac{SSR_{FE}}{N(T-1)-K}
\]

It's ambiguous whether your variance of error changes up or down because SSR can increase while the denominator decreases.

FE can be unbiased, but not consistent (i.e., not converging to the true effect)

\hypertarget{fe-examples}{%
\paragraph{FE Examples}\label{fe-examples}}

\hypertarget{blau1999}{%
\paragraph{\texorpdfstring{\citet{blau1999}}{@blau1999}}\label{blau1999}}

\begin{itemize}
\item
  Intergenerational mobility
\item
  If we transfer resources to low income family, can we generate upward mobility (increase ability)?
\end{itemize}

Mechanisms for intergenerational mobility

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Genetic (policy can't affect) (i.e., ability endowment)
\item
  Environmental indirect
\item
  Environmental direct
\end{enumerate}

\[
\frac{\% \Delta \text{Human capital}}{\% \Delta \text{income}}
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Financial transfer
\end{enumerate}

Income measures:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Total household income
\item
  Wage income
\item
  Non-wage income
\item
  Annual versus permanent income
\end{enumerate}

Core control variables:

\textbf{Bad controls are those jointly determined with dependent variable}

Control by mother = choice by mother

Uncontrolled by mothers:

\begin{itemize}
\item
  mother race
\item
  location of birth
\item
  education of parents
\item
  household structure at age 14
\end{itemize}

\[
Y_{ijt} = X_{jt} \beta_i + I_{jt} \alpha_i + \epsilon_{ijt}
\]

where

\begin{itemize}
\item
  \(i\) = test
\item
  \(j\) = individual (child)
\item
  \(t\) = time
\end{itemize}

Grandmother's model

Since child is nested within mother and mother nested within grandmother, the fixed effect of child is included in the fixed effect of mother, which is included in the fixed-effect of grandmother

\[
Y_{ijgmt} = X_{it} \beta_{i} + I_{jt} \alpha_i + \gamma_g + u_{ijgmt}
\]

where

\begin{itemize}
\item
  \(i\) = test, \(j\) = kid, \(m\) = mother, \(g\) = grandmother
\item
  where \(\gamma_g\) includes \(\gamma_m\) includes \(\gamma_j\)
\end{itemize}

Grandma fixed-effect

Pros:

\begin{itemize}
\item
  control for some genetics + fixed characteristics of how mother are raised
\item
  can estimate effect of parameter income
\end{itemize}

Con:

\begin{itemize}
\tightlist
\item
  Might not be a sufficient control
\end{itemize}

Common to cluster a the fixed-effect level (common correlated component)

\textbf{Fixed effect exaggerates attenuation bias}

Error rate on survey can help you fix this (plug in the number only , but not the uncertainty associated with that number).

\hypertarget{babcock2010}{%
\paragraph{\texorpdfstring{\citet{babcock2010}}{@babcock2010}}\label{babcock2010}}

\[
T_{ijct} = \alpha_0 + S_{jct} \alpha_1 + X_{ijct} \alpha_2 + u_{ijct}
\]

where

\begin{itemize}
\item
  \(S_{jct}\) is the average class expectation
\item
  \(X_{ijct}\alpha_2\) is the individual characteristics
\item
  \(i\) student
\item
  \(j\) instructor
\item
  \(c\) course
\item
  \(t\) time
\end{itemize}

\[
T_{ijct} = \beta_0+ S_{jct} \beta_1+ X_{ijct} \beta_2 +\mu_{jc} + \epsilon_{ijct}
\]

where \(\mu_{jc}\) is instructor by course fixed effect (unique id), which is different from \((\theta_j + \delta_c)\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Decrease course shopping because conditioned on available information (\(\mu_{jc}\)) (class grade and instructor's info).
\item
  Grade expectation change even though class materials stay the same
\end{enumerate}

Identification strategy is

\begin{itemize}
\tightlist
\item
  Under (fixed) time-varying factor that could bias my coefficient (simultaneity)
\end{itemize}

\[
Y_{ijt} = X_{it} \beta_1 + \text{Teacher Experience}_{jt} \beta_2 + \text{Teacher education}_{jt} \beta_3 + \text{Teacher score}_{it}\beta_4 + \dots + \epsilon_{ijt}
\]

Drop teacher characteristics, and include teacher dummy effect

\[
Y_{ijt} = X_{it} \alpha + \Gamma_{it} \theta_j + u_{ijt}
\]

where \(\alpha\) is the within teacher (conditional on teacher fixed effect) and \(j = 1 \to (J-1)\)

Nuisance in the sense that we don't about the interpretation of \(\alpha\)

The least we can say about \(\theta_j\) is the teacher effect conditional on student test score.

\[
Y_{ijt} = X_{it} \gamma + \epsilon_{ijt}
\]

\(\gamma\) is between within (unconditional) and \(e_{ijt}\) is the prediction error

\[
e_{ijt} = T_{it} \delta_j + \tilde{e}_{ijt}
\]

where \(\delta_j\) is the mean for each group

\[
Y_{ijkt} = Y_{ijkt-1} + X_{it} \beta + T_{it} \tau_j + (W_i + P_k + \epsilon_{ijkt})
\]

where

\begin{itemize}
\item
  \(Y_{ijkt-1}\) = lag control
\item
  \(\tau_j\) = teacher fixed time
\item
  \(W_i\) is the student fixed effect
\item
  \(P_k\) is the school fixed effect
\item
  \(u_{ijkt} = W_i + P_k + \epsilon_{ijkt}\)
\end{itemize}

And we worry about selection on class and school

Bias in \(\tau\) (for 1 teacher) is

\[
\frac{1}{N_j} \sum_{i = 1}^N (W_i + P_k + \epsilon_{ijkt})
\]

where \(N_j\) = the number of student in class with teacher \(j\)

then we can \(P_k + \frac{1}{N_j} \sum_{i = 1}^{N_j} (W_i + \epsilon_{ijkt})\)

Shocks from small class can bias \(\tau\)

\[
\frac{1}{N_j} \sum_{i = 1}^{N_j} \epsilon_{ijkt} \neq 0
\]

which will inflate the teacher fixed effect

Even if we create random teacher fixed effect and put it in the model, it still contains bias mentioned above which can still \(\tau\) (but we do not know the way it will affect - whether more positive or negative).

If teachers switch schools, then we can estimate both teacher and school fixed effect (\textbf{mobility web} thin vs.~thick)

Mobility web refers to the web of switchers (i.e., from one status to another).

\[
Y_{ijkt} = Y_{ijk(t-1)} \alpha + X_{it}\beta + T_{it} \tau + P_k + \epsilon_{ijkt}
\]

If we demean (fixed-effect), \(\tau\) (teacher fixed effect) will go away

If you want to examine teacher fixed effect, we have to include teacher fixed effect

Control for school, the article argues that there is no selection bias

For \(\frac{1}{N_j} \sum_{i =1}^{N_j} \epsilon_{ijkt}\) (teacher-level average residuals), \(var(\tau)\) does not change with \(N_j\) (Figure 2 in the paper). In words, the quality of teachers is not a function of the number of students

If \(var(\tau) =0\) it means that teacher quality does not matter

Spin-off of \protect\hyperlink{measurement-error}{Measurement Error}: Sampling error or estimation error

\[
\hat{\tau}_j = \tau_j + \lambda_j
\]

\[
var(\hat{\tau}) = var(\tau + \lambda)
\]

Assume \(cov(\tau_j, \lambda_j)=0\) (reasonable) In words, your randomness in getting children does not correlation with teacher quality.

Hence,

\[
\begin{aligned}
var(\hat{\tau}) &= var(\tau) + var(\lambda) \\
var(\tau) &= var(\hat{\tau}) - var(\lambda) \\
\end{aligned}
\]

We have \(var(\hat{\tau})\) and we need to estimate \(var(\lambda)\)

\[
var(\lambda) = \frac{1}{J} \sum_{j=1}^J \hat{\sigma}^2_j
\] where \(\hat{\sigma}^2_j\) is the squared standard error of the teacher \(j\) (a function of \(n\))

Hence,

\[
\frac{var(\tau)}{var(\hat{\tau})} = \text{reliability} = \text{true variance signal}
\] also known as how much noise in \(\hat{\tau}\) and

\[
1 - \frac{var(\tau)}{var(\hat{\tau})} = \text{noise}
\]

Even in cases where the true relationship is that \(\tau\) is a function of \(N_j\), then our recovery method for \(\lambda\) is still not affected

To examine our assumption

\[
\hat{\tau}_j = \beta_0 + X_j \beta_1 + \epsilon_j
\]

Regressing teacher fixed-effect on teacher characteristics should give us \(R^2\) close to 0, because teacher characteristics cannot predict sampling error (\(\hat{\tau}\) contain sampling error)

\hypertarget{tests-for-assumptions}{%
\subsection{Tests for Assumptions}\label{tests-for-assumptions}}

We typically don't test heteroskedasticity because we will use robust covariance matrix estimation anyway.

Dataset

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"plm"}\NormalTok{)}
\FunctionTok{data}\NormalTok{(}\StringTok{"EmplUK"}\NormalTok{, }\AttributeTok{package=}\StringTok{"plm"}\NormalTok{)}
\FunctionTok{data}\NormalTok{(}\StringTok{"Produc"}\NormalTok{, }\AttributeTok{package=}\StringTok{"plm"}\NormalTok{)}
\FunctionTok{data}\NormalTok{(}\StringTok{"Grunfeld"}\NormalTok{, }\AttributeTok{package=}\StringTok{"plm"}\NormalTok{)}
\FunctionTok{data}\NormalTok{(}\StringTok{"Wages"}\NormalTok{, }\AttributeTok{package=}\StringTok{"plm"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{poolability}{%
\subsubsection{Poolability}\label{poolability}}

also known as an F test of stability (or Chow test) for the coefficients

\(H_0\): All individuals have the same coefficients (i.e., equal coefficients for all individuals).

\(H_a\) Different individuals have different coefficients.

Notes:

\begin{itemize}
\tightlist
\item
  Under a within (i.e., fixed) model, different intercepts for each individual are assumed
\item
  Under random model, same intercept is assumed
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(plm)}
\NormalTok{plm}\SpecialCharTok{::}\FunctionTok{pooltest}\NormalTok{(inv}\SpecialCharTok{\textasciitilde{}}\NormalTok{value}\SpecialCharTok{+}\NormalTok{capital, }\AttributeTok{data=}\NormalTok{Grunfeld, }\AttributeTok{model=}\StringTok{"within"}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  F statistic}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  inv \textasciitilde{} value + capital}
\CommentTok{\#\textgreater{} F = 5.7805, df1 = 18, df2 = 170, p{-}value = 1.219e{-}10}
\CommentTok{\#\textgreater{} alternative hypothesis: unstability}
\end{Highlighting}
\end{Shaded}

Hence, we reject the null hypothesis that coefficients are stable. Then, we should use the random model.

\hypertarget{individual-and-time-effects}{%
\subsubsection{Individual and time effects}\label{individual-and-time-effects}}

use the Lagrange multiplier test to test the presence of individual or time or both (i.e., individual and time).

Types:

\begin{itemize}
\tightlist
\item
  \texttt{honda}: \citep{honda1985testing} Default
\item
  \texttt{bp}: \citep{Breusch_1980} for unbalanced panels
\item
  \texttt{kw}: \citep{King_1997} unbalanced panels, and two-way effects
\item
  \texttt{ghm}: \citep{gourieroux1982likelihood}: two-way effects
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pFtest}\NormalTok{(inv}\SpecialCharTok{\textasciitilde{}}\NormalTok{value}\SpecialCharTok{+}\NormalTok{capital, }\AttributeTok{data=}\NormalTok{Grunfeld, }\AttributeTok{effect=}\StringTok{"twoways"}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  F test for twoways effects}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  inv \textasciitilde{} value + capital}
\CommentTok{\#\textgreater{} F = 17.403, df1 = 28, df2 = 169, p{-}value \textless{} 2.2e{-}16}
\CommentTok{\#\textgreater{} alternative hypothesis: significant effects}
\FunctionTok{pFtest}\NormalTok{(inv}\SpecialCharTok{\textasciitilde{}}\NormalTok{value}\SpecialCharTok{+}\NormalTok{capital, }\AttributeTok{data=}\NormalTok{Grunfeld, }\AttributeTok{effect=}\StringTok{"individual"}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  F test for individual effects}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  inv \textasciitilde{} value + capital}
\CommentTok{\#\textgreater{} F = 49.177, df1 = 9, df2 = 188, p{-}value \textless{} 2.2e{-}16}
\CommentTok{\#\textgreater{} alternative hypothesis: significant effects}
\FunctionTok{pFtest}\NormalTok{(inv}\SpecialCharTok{\textasciitilde{}}\NormalTok{value}\SpecialCharTok{+}\NormalTok{capital, }\AttributeTok{data=}\NormalTok{Grunfeld, }\AttributeTok{effect=}\StringTok{"time"}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  F test for time effects}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  inv \textasciitilde{} value + capital}
\CommentTok{\#\textgreater{} F = 0.23451, df1 = 19, df2 = 178, p{-}value = 0.9997}
\CommentTok{\#\textgreater{} alternative hypothesis: significant effects}
\end{Highlighting}
\end{Shaded}

\hypertarget{cross-sectional-dependencecontemporaneous-correlation}{%
\subsubsection{Cross-sectional dependence/contemporaneous correlation}\label{cross-sectional-dependencecontemporaneous-correlation}}

\begin{itemize}
\tightlist
\item
  Null hypothesis: residuals across entities are not correlated.
\end{itemize}

\hypertarget{global-cross-sectional-dependence}{%
\paragraph{Global cross-sectional dependence}\label{global-cross-sectional-dependence}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pcdtest}\NormalTok{(inv}\SpecialCharTok{\textasciitilde{}}\NormalTok{value}\SpecialCharTok{+}\NormalTok{capital, }\AttributeTok{data=}\NormalTok{Grunfeld, }\AttributeTok{model=}\StringTok{"within"}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Pesaran CD test for cross{-}sectional dependence in panels}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  inv \textasciitilde{} value + capital}
\CommentTok{\#\textgreater{} z = 4.6612, p{-}value = 3.144e{-}06}
\CommentTok{\#\textgreater{} alternative hypothesis: cross{-}sectional dependence}
\end{Highlighting}
\end{Shaded}

\hypertarget{local-cross-sectional-dependence}{%
\paragraph{Local cross-sectional dependence}\label{local-cross-sectional-dependence}}

use the same command, but supply matrix \texttt{w} to the argument.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pcdtest}\NormalTok{(inv}\SpecialCharTok{\textasciitilde{}}\NormalTok{value}\SpecialCharTok{+}\NormalTok{capital, }\AttributeTok{data=}\NormalTok{Grunfeld, }\AttributeTok{model=}\StringTok{"within"}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Pesaran CD test for cross{-}sectional dependence in panels}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  inv \textasciitilde{} value + capital}
\CommentTok{\#\textgreater{} z = 4.6612, p{-}value = 3.144e{-}06}
\CommentTok{\#\textgreater{} alternative hypothesis: cross{-}sectional dependence}
\end{Highlighting}
\end{Shaded}

\hypertarget{serial-correlation-1}{%
\subsubsection{Serial Correlation}\label{serial-correlation-1}}

\begin{itemize}
\item
  Null hypothesis: there is no serial correlation
\item
  usually seen in macro panels with long time series (large N and T), not seen in micro panels (small T and large N)
\item
  Serial correlation can arise from individual effects(i.e., time-invariant error component), or idiosyncratic error terms (e..g, in the case of AR(1) process). But typically, when we refer to serial correlation, we refer to the second one.
\item
  Can be

  \begin{itemize}
  \item
    \textbf{marginal} test: only 1 of the two above dependence (but can be biased towards rejection)
  \item
    \textbf{joint} test: both dependencies (but don't know which one is causing the problem)
  \item
    \textbf{conditional} test: assume you correctly specify one dependence structure, test whether the other departure is present.
  \end{itemize}
\end{itemize}

\hypertarget{unobserved-effect-test}{%
\paragraph{Unobserved effect test}\label{unobserved-effect-test}}

\begin{itemize}
\item
  semi-parametric test (the test statistic \(W \dot{\sim} N\) regardless of the distribution of the errors) with \(H_0: \sigma^2_\mu = 0\) (i.e., no unobserved effects in the residuals), favors pooled OLS.

  \begin{itemize}
  \tightlist
  \item
    Under the null, covariance matrix of the residuals = its diagonal (off-diagonal = 0)
  \end{itemize}
\item
  It is robust against both \textbf{unobserved effects} that are constant within every group, and any kind of \textbf{serial correlation}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pwtest}\NormalTok{(}\FunctionTok{log}\NormalTok{(gsp) }\SpecialCharTok{\textasciitilde{}} \FunctionTok{log}\NormalTok{(pcap) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{(pc) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{(emp) }\SpecialCharTok{+}\NormalTok{ unemp, }\AttributeTok{data =}\NormalTok{ Produc)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Wooldridge\textquotesingle{}s test for unobserved individual effects}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  formula}
\CommentTok{\#\textgreater{} z = 3.9383, p{-}value = 8.207e{-}05}
\CommentTok{\#\textgreater{} alternative hypothesis: unobserved effect}
\end{Highlighting}
\end{Shaded}

Here, we reject the null hypothesis that the no unobserved effects in the residuals. Hence, we will exclude using pooled OLS.

\hypertarget{locally-robust-tests-for-random-effects-and-serial-correlation}{%
\paragraph{Locally robust tests for random effects and serial correlation}\label{locally-robust-tests-for-random-effects-and-serial-correlation}}

\begin{itemize}
\tightlist
\item
  A joint LM test for \textbf{random effects} and \textbf{serial correlation} assuming normality and homoskedasticity of the idiosyncratic errors {[}\citet{baltagi1991joint}{]}\citep{baltagi1995testing}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pbsytest}\NormalTok{(}\FunctionTok{log}\NormalTok{(gsp) }\SpecialCharTok{\textasciitilde{}} \FunctionTok{log}\NormalTok{(pcap) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{(pc) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{(emp) }\SpecialCharTok{+}\NormalTok{ unemp,}
         \AttributeTok{data =}\NormalTok{ Produc,}
         \AttributeTok{test =} \StringTok{"j"}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Baltagi and Li AR{-}RE joint test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  formula}
\CommentTok{\#\textgreater{} chisq = 4187.6, df = 2, p{-}value \textless{} 2.2e{-}16}
\CommentTok{\#\textgreater{} alternative hypothesis: AR(1) errors or random effects}
\end{Highlighting}
\end{Shaded}

Here, we reject the null hypothesis that there is no presence of \textbf{serial correlation,} and \textbf{random effects}. But we still do not know whether it is because of serial correlation, of random effects or of both

To know the departure from the null assumption, we can use \citet{bera2001tests}'s test for first-order serial correlation or random effects (both under normality and homoskedasticity assumption of the error).

BSY for serial correlation

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pbsytest}\NormalTok{(}\FunctionTok{log}\NormalTok{(gsp) }\SpecialCharTok{\textasciitilde{}} \FunctionTok{log}\NormalTok{(pcap) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{(pc) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{(emp) }\SpecialCharTok{+}\NormalTok{ unemp,}
         \AttributeTok{data =}\NormalTok{ Produc)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Bera, Sosa{-}Escudero and Yoon locally robust test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  formula}
\CommentTok{\#\textgreater{} chisq = 52.636, df = 1, p{-}value = 4.015e{-}13}
\CommentTok{\#\textgreater{} alternative hypothesis: AR(1) errors sub random effects}
\end{Highlighting}
\end{Shaded}

BSY for random effects

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pbsytest}\NormalTok{(}\FunctionTok{log}\NormalTok{(gsp)}\SpecialCharTok{\textasciitilde{}}\FunctionTok{log}\NormalTok{(pcap)}\SpecialCharTok{+}\FunctionTok{log}\NormalTok{(pc)}\SpecialCharTok{+}\FunctionTok{log}\NormalTok{(emp)}\SpecialCharTok{+}\NormalTok{unemp, }
         \AttributeTok{data=}\NormalTok{Produc, }
         \AttributeTok{test=}\StringTok{"re"}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Bera, Sosa{-}Escudero and Yoon locally robust test (one{-}sided)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  formula}
\CommentTok{\#\textgreater{} z = 57.914, p{-}value \textless{} 2.2e{-}16}
\CommentTok{\#\textgreater{} alternative hypothesis: random effects sub AR(1) errors}
\end{Highlighting}
\end{Shaded}

Since BSY is only locally robust, if you ``know'' there is no serial correlation, then this test is based on LM test is more superior:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plmtest}\NormalTok{(inv }\SpecialCharTok{\textasciitilde{}}\NormalTok{ value }\SpecialCharTok{+}\NormalTok{ capital, }\AttributeTok{data =}\NormalTok{ Grunfeld, }
        \AttributeTok{type =} \StringTok{"honda"}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Lagrange Multiplier Test {-} (Honda)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  inv \textasciitilde{} value + capital}
\CommentTok{\#\textgreater{} normal = 28.252, p{-}value \textless{} 2.2e{-}16}
\CommentTok{\#\textgreater{} alternative hypothesis: significant effects}
\end{Highlighting}
\end{Shaded}

On the other hand, if you know there is no random effects, to test for serial correlation, use \citep{breusch1978testing}-\citep{godfrey1978testing}'s test

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lmtest}\SpecialCharTok{::}\FunctionTok{bgtest}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

If you ``know'' there are random effects, use \citep{baltagi1995testing}'s. to test for serial correlation in both AR(1) and MA(1) processes.

\(H_0\): Uncorrelated errors.

Note:

\begin{itemize}
\tightlist
\item
  one-sided only has power against positive serial correlation.
\item
  applicable to only balanced panels.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pbltest}\NormalTok{(}
    \FunctionTok{log}\NormalTok{(gsp) }\SpecialCharTok{\textasciitilde{}} \FunctionTok{log}\NormalTok{(pcap) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{(pc) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{(emp) }\SpecialCharTok{+}\NormalTok{ unemp,}
    \AttributeTok{data =}\NormalTok{ Produc,}
    \AttributeTok{alternative =} \StringTok{"onesided"}
\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Baltagi and Li one{-}sided LM test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  log(gsp) \textasciitilde{} log(pcap) + log(pc) + log(emp) + unemp}
\CommentTok{\#\textgreater{} z = 21.69, p{-}value \textless{} 2.2e{-}16}
\CommentTok{\#\textgreater{} alternative hypothesis: AR(1)/MA(1) errors in RE panel model}
\end{Highlighting}
\end{Shaded}

General serial correlation tests

\begin{itemize}
\tightlist
\item
  applicable to random effects model, OLS, and FE (with large T, also known as long panel).
\item
  can also test higher-order serial correlation
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plm}\SpecialCharTok{::}\FunctionTok{pbgtest}\NormalTok{(plm}\SpecialCharTok{::}\FunctionTok{plm}\NormalTok{(inv }\SpecialCharTok{\textasciitilde{}}\NormalTok{ value }\SpecialCharTok{+}\NormalTok{ capital,}
                      \AttributeTok{data =}\NormalTok{ Grunfeld,}
                      \AttributeTok{model =} \StringTok{"within"}\NormalTok{),}
             \AttributeTok{order =} \DecValTok{2}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Breusch{-}Godfrey/Wooldridge test for serial correlation in panel models}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  inv \textasciitilde{} value + capital}
\CommentTok{\#\textgreater{} chisq = 42.587, df = 2, p{-}value = 5.655e{-}10}
\CommentTok{\#\textgreater{} alternative hypothesis: serial correlation in idiosyncratic errors}
\end{Highlighting}
\end{Shaded}

in the case of short panels (small T and large n), we can use

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pwartest}\NormalTok{(}\FunctionTok{log}\NormalTok{(emp) }\SpecialCharTok{\textasciitilde{}} \FunctionTok{log}\NormalTok{(wage) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{(capital), }\AttributeTok{data=}\NormalTok{EmplUK)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Wooldridge\textquotesingle{}s test for serial correlation in FE panels}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  plm.model}
\CommentTok{\#\textgreater{} F = 312.3, df1 = 1, df2 = 889, p{-}value \textless{} 2.2e{-}16}
\CommentTok{\#\textgreater{} alternative hypothesis: serial correlation}
\end{Highlighting}
\end{Shaded}

\hypertarget{unit-rootsstationarity}{%
\subsubsection{Unit roots/stationarity}\label{unit-rootsstationarity}}

\begin{itemize}
\tightlist
\item
  Dickey-Fuller test for stochastic trends.
\item
  Null hypothesis: the series is non-stationary (unit root)
\item
  You would want your test to be less than the critical value (p\textless.5) so that there is evidence there is not unit roots.
\end{itemize}

\hypertarget{heteroskedasticity-1}{%
\subsubsection{Heteroskedasticity}\label{heteroskedasticity-1}}

\begin{itemize}
\item
  Breusch-Pagan test
\item
  Null hypothesis: the data is homoskedastic
\item
  If there is evidence for heteroskedasticity, robust covariance matrix is advised.
\item
  To control for heteroskedasticity: Robust covariance matrix estimation (Sandwich estimator)

  \begin{itemize}
  \tightlist
  \item
    ``white1'' - for general heteroskedasticity but no serial correlation (check serial correlation first). Recommended for random effects.
  \item
    ``white2'' - is ``white1'' restricted to a common variance within groups. Recommended for random effects.
  \item
    ``arellano'' - both heteroskedasticity and serial correlation. Recommended for fixed effects
  \end{itemize}
\end{itemize}

\hypertarget{model-selection}{%
\subsection{Model Selection}\label{model-selection}}

\hypertarget{pols-vs.-re}{%
\subsubsection{POLS vs.~RE}\label{pols-vs.-re}}

The continuum between RE (used FGLS which more assumption ) and POLS check back on the section of FGLS

\textbf{Breusch-Pagan LM} test

\begin{itemize}
\tightlist
\item
  Test for the random effect model based on the OLS residual
\item
  Null hypothesis: variances across entities is zero. In another word, no panel effect.
\item
  If the test is significant, RE is preferable compared to POLS
\end{itemize}

\hypertarget{fe-vs.-re}{%
\subsubsection{FE vs.~RE}\label{fe-vs.-re}}

\begin{itemize}
\tightlist
\item
  RE does not require strict exogeneity for consistency (feedback effect between residual and covariates)
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.3194}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.6806}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Hypothesis
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
If true
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(H_0: Cov(c_i,\mathbf{x_{it}})=0\) & \(\hat{\beta}_{RE}\) is consistent and efficient, while \(\hat{\beta}_{FE}\) is consistent \\
\(H_0: Cov(c_i,\mathbf{x_{it}}) \neq 0\) & \(\hat{\beta}_{RE}\) is inconsistent, while \(\hat{\beta}_{FE}\) is consistent \\
\end{longtable}

\textbf{Hausman Test}

For the Hausman test to run, you need to assume that

\begin{itemize}
\tightlist
\item
  strict exogeneity hold
\item
  A4 to hold for \(u_{it}\)
\end{itemize}

Then,

\begin{itemize}
\tightlist
\item
  Hausman test statistic: \(H=(\hat{\beta}_{RE}-\hat{\beta}_{FE})'(V(\hat{\beta}_{RE})- V(\hat{\beta}_{FE}))(\hat{\beta}_{RE}-\hat{\beta}_{FE}) \sim \chi_{n(X)}^2\) where \(n(X)\) is the number of parameters for the time-varying regressors.
\item
  A low p-value means that we would reject the null hypothesis and prefer FE
\item
  A high p-value means that we would not reject the null hypothesis and consider RE estimator.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gw }\OtherTok{\textless{}{-}} \FunctionTok{plm}\NormalTok{(inv }\SpecialCharTok{\textasciitilde{}}\NormalTok{ value }\SpecialCharTok{+}\NormalTok{ capital, }\AttributeTok{data =}\NormalTok{ Grunfeld, }\AttributeTok{model =} \StringTok{"within"}\NormalTok{)}
\NormalTok{gr }\OtherTok{\textless{}{-}} \FunctionTok{plm}\NormalTok{(inv }\SpecialCharTok{\textasciitilde{}}\NormalTok{ value }\SpecialCharTok{+}\NormalTok{ capital, }\AttributeTok{data =}\NormalTok{ Grunfeld, }\AttributeTok{model =} \StringTok{"random"}\NormalTok{)}
\FunctionTok{phtest}\NormalTok{(gw, gr)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Hausman Test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  inv \textasciitilde{} value + capital}
\CommentTok{\#\textgreater{} chisq = 2.3304, df = 2, p{-}value = 0.3119}
\CommentTok{\#\textgreater{} alternative hypothesis: one model is inconsistent}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Violation Estimator
\item
  Basic Estimator
\item
  Instrumental variable Estimator
\item
  Variable Coefficients estimator
\item
  Generalized Method of Moments estimator
\item
  General FGLS estimator
\item
  Means groups estimator
\item
  CCEMG
\item
  Estimator for limited dependent variables
\end{itemize}

\hypertarget{summary-2}{%
\subsection{Summary}\label{summary-2}}

\begin{itemize}
\item
  All three estimators (POLS, RE, FE) require \protect\hyperlink{a1-linearity}{A1}, \protect\hyperlink{a2-full-rank}{A2}, \protect\hyperlink{a5-data-generation-random-sampling}{A5} (for individuals) to be consistent. Additionally,
\item
  POLS is consistent under A3a(for \(u_{it}\)): \(E(\mathbf{x}_{it}'u_{it})=0\), and RE Assumption \(E(\mathbf{x}_{it}'c_{i})=0\)

  \begin{itemize}
  \tightlist
  \item
    If \protect\hyperlink{a4-homoskedasticity}{A4} does not hold, use cluster robust SE but POLS is not efficient
  \end{itemize}
\item
  RE is consistent under A3a(for \(u_{it}\)): \(E(\mathbf{x}_{it}'u_{it})=0\), and RE Assumption \(E(\mathbf{x}_{it}'c_{i})=0\)

  \begin{itemize}
  \tightlist
  \item
    If \protect\hyperlink{a4-homoskedasticity}{A4} (for \(u_{it}\)) holds then usual SE are valid and RE is most efficient
  \item
    If \protect\hyperlink{a4-homoskedasticity}{A4} (for \(u_{it}\)) does not hold, use cluster robust SE ,and RE is no longer most efficient (but still more efficient than POLS)
  \end{itemize}
\item
  FE is consistent under \protect\hyperlink{a3-exogeneity-of-independent-variables}{A3} \(E((\mathbf{x}_{it}-\bar{\mathbf{x}}_{it})'(u_{it} -\bar{u}_{it}))=0\)

  \begin{itemize}
  \tightlist
  \item
    Cannot estimate effects of time constant variables
  \item
    A4 generally does not hold for \(u_{it} -\bar{u}_{it}\) so cluster robust SE are needed
  \end{itemize}
\end{itemize}

\textbf{Note}: \protect\hyperlink{a5-data-generation-random-sampling}{A5} for individual (not for time dimension) implies that you have {[}A5a{]} for the entire data set.

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Estimator / True Model & POLS & RE & FE \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
POLS & Consistent & Consistent & Inconsistent \\
FE & Consistent & Consistent & Consistent \\
RE & Consistent & Consistent & Inconsistent \\
\end{longtable}

Based on table provided by \href{https://sites.google.com/site/econometricsacademy/econometrics-models/panel-data-models}{Ani Katchova}

\hypertarget{application-1}{%
\subsection{Application}\label{application-1}}

\hypertarget{plm-package}{%
\subsubsection{\texorpdfstring{\texttt{plm} package}{plm package}}\label{plm-package}}

Recommended application of \texttt{plm} can be found \href{https://cran.r-project.org/web/packages/plm/vignettes/B_plmFunction.html}{here} and \href{https://cran.r-project.org/web/packages/plm/vignettes/C_plmModelComponents.html}{here} by Yves Croissant

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#install.packages("plm")}
\FunctionTok{library}\NormalTok{(}\StringTok{"plm"}\NormalTok{)}

\FunctionTok{library}\NormalTok{(foreign)}
\NormalTok{Panel }\OtherTok{\textless{}{-}} \FunctionTok{read.dta}\NormalTok{(}\StringTok{"http://dss.princeton.edu/training/Panel101.dta"}\NormalTok{)}

\FunctionTok{attach}\NormalTok{(Panel)}
\NormalTok{Y }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(y)}
\NormalTok{X }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(x1, x2, x3)}

\CommentTok{\# Set data as panel data}
\NormalTok{pdata }\OtherTok{\textless{}{-}} \FunctionTok{pdata.frame}\NormalTok{(Panel, }\AttributeTok{index =} \FunctionTok{c}\NormalTok{(}\StringTok{"country"}\NormalTok{, }\StringTok{"year"}\NormalTok{))}

\CommentTok{\# Pooled OLS estimator}
\NormalTok{pooling }\OtherTok{\textless{}{-}} \FunctionTok{plm}\NormalTok{(Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X, }\AttributeTok{data =}\NormalTok{ pdata, }\AttributeTok{model =} \StringTok{"pooling"}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(pooling)}

\CommentTok{\# Between estimator}
\NormalTok{between }\OtherTok{\textless{}{-}} \FunctionTok{plm}\NormalTok{(Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X, }\AttributeTok{data =}\NormalTok{ pdata, }\AttributeTok{model =} \StringTok{"between"}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(between)}

\CommentTok{\# First differences estimator}
\NormalTok{firstdiff }\OtherTok{\textless{}{-}} \FunctionTok{plm}\NormalTok{(Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X, }\AttributeTok{data =}\NormalTok{ pdata, }\AttributeTok{model =} \StringTok{"fd"}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(firstdiff)}

\CommentTok{\# Fixed effects or within estimator}
\NormalTok{fixed }\OtherTok{\textless{}{-}} \FunctionTok{plm}\NormalTok{(Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X, }\AttributeTok{data =}\NormalTok{ pdata, }\AttributeTok{model =} \StringTok{"within"}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(fixed)}

\CommentTok{\# Random effects estimator}
\NormalTok{random }\OtherTok{\textless{}{-}} \FunctionTok{plm}\NormalTok{(Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X, }\AttributeTok{data =}\NormalTok{ pdata, }\AttributeTok{model =} \StringTok{"random"}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(random)}

\CommentTok{\# LM test for random effects versus OLS}
\CommentTok{\# Accept Null, then OLS, Reject Null then RE}
\FunctionTok{plmtest}\NormalTok{(pooling, }\AttributeTok{effect =} \StringTok{"individual"}\NormalTok{, }\AttributeTok{type =} \FunctionTok{c}\NormalTok{(}\StringTok{"bp"}\NormalTok{)) }
\CommentTok{\# other type: "honda", "kw"," "ghm"; other effect : "time" "twoways"}


\CommentTok{\# B{-}P/LM and Pesaran CD (cross{-}sectional dependence) test}
\CommentTok{\# Breusch and Pagan\textquotesingle{}s original LM statistic}
\FunctionTok{pcdtest}\NormalTok{(fixed, }\AttributeTok{test =} \FunctionTok{c}\NormalTok{(}\StringTok{"lm"}\NormalTok{)) }
\CommentTok{\# Pesaran\textquotesingle{}s CD statistic}
\FunctionTok{pcdtest}\NormalTok{(fixed, }\AttributeTok{test =} \FunctionTok{c}\NormalTok{(}\StringTok{"cd"}\NormalTok{)) }

\CommentTok{\# Serial Correlation}
\FunctionTok{pbgtest}\NormalTok{(fixed)}

\CommentTok{\# stationary}
\FunctionTok{library}\NormalTok{(}\StringTok{"tseries"}\NormalTok{)}
\FunctionTok{adf.test}\NormalTok{(pdata}\SpecialCharTok{$}\NormalTok{y, }\AttributeTok{k =} \DecValTok{2}\NormalTok{)}

\CommentTok{\# LM test for fixed effects versus OLS}
\FunctionTok{pFtest}\NormalTok{(fixed, pooling)}

\CommentTok{\# Hausman test for fixed versus random effects model}
\FunctionTok{phtest}\NormalTok{(random, fixed)}

\CommentTok{\# Breusch{-}Pagan heteroskedasticity}
\FunctionTok{library}\NormalTok{(lmtest)}
\FunctionTok{bptest}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x1 }\SpecialCharTok{+} \FunctionTok{factor}\NormalTok{(country), }\AttributeTok{data =}\NormalTok{ pdata)}

\CommentTok{\# If there is presence of heteroskedasticity}
\DocumentationTok{\#\# For RE model}
\FunctionTok{coeftest}\NormalTok{(random) }\CommentTok{\#orginal coef}

\CommentTok{\# Heteroskedasticity consistent coefficients}
\FunctionTok{coeftest}\NormalTok{(random, vcovHC) }

\FunctionTok{t}\NormalTok{(}\FunctionTok{sapply}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"HC0"}\NormalTok{, }\StringTok{"HC1"}\NormalTok{, }\StringTok{"HC2"}\NormalTok{, }\StringTok{"HC3"}\NormalTok{, }\StringTok{"HC4"}\NormalTok{), }\ControlFlowTok{function}\NormalTok{(x)}
    \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{diag}\NormalTok{(}
        \FunctionTok{vcovHC}\NormalTok{(random, }\AttributeTok{type =}\NormalTok{ x)}
\NormalTok{    )))) }\CommentTok{\#show HC SE of the coef}
\CommentTok{\# HC0 {-} heteroskedasticity consistent. The default.}
\CommentTok{\# HC1,HC2, HC3  Recommended for small samples. }
\CommentTok{\# HC3 gives less weight to influential observations.}
\CommentTok{\# HC4 {-} small samples with influential observations}
\CommentTok{\# HAC {-} heteroskedasticity and autocorrelation consistent}

\DocumentationTok{\#\# For FE model}
\FunctionTok{coeftest}\NormalTok{(fixed) }\CommentTok{\# Original coefficients}
\FunctionTok{coeftest}\NormalTok{(fixed, vcovHC) }\CommentTok{\# Heteroskedasticity consistent coefficients}

\CommentTok{\# Heteroskedasticity consistent coefficients (Arellano)}
\FunctionTok{coeftest}\NormalTok{(fixed, }\FunctionTok{vcovHC}\NormalTok{(fixed, }\AttributeTok{method =} \StringTok{"arellano"}\NormalTok{)) }

\FunctionTok{t}\NormalTok{(}\FunctionTok{sapply}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"HC0"}\NormalTok{, }\StringTok{"HC1"}\NormalTok{, }\StringTok{"HC2"}\NormalTok{, }\StringTok{"HC3"}\NormalTok{, }\StringTok{"HC4"}\NormalTok{), }\ControlFlowTok{function}\NormalTok{(x)}
    \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{diag}\NormalTok{(}
        \FunctionTok{vcovHC}\NormalTok{(fixed, }\AttributeTok{type =}\NormalTok{ x)}
\NormalTok{    )))) }\CommentTok{\#show HC SE of the coef}
\end{Highlighting}
\end{Shaded}

\textbf{Advanced}

Other methods to estimate the random model:

\begin{itemize}
\tightlist
\item
  \texttt{"swar"}: \emph{default} \citep{swamy1972exact}
\item
  \texttt{"walhus"}: \citep{wallace1969use}
\item
  \texttt{"amemiya"}: \citep{amemiya1971estimation}
\item
  \texttt{"nerlove"}'' \citep{nerlove1971further}
\end{itemize}

Other effects:

\begin{itemize}
\tightlist
\item
  Individual effects: \emph{default}
\item
  Time effects: \texttt{"time"}
\item
  Individual and time effects: \texttt{"twoways"}
\end{itemize}

\textbf{Note}: no random two-ways effect model for \texttt{random.method\ =\ "nerlove"}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{amemiya }\OtherTok{\textless{}{-}}
    \FunctionTok{plm}\NormalTok{(}
\NormalTok{        Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X,}
        \AttributeTok{data =}\NormalTok{ pdata,}
        \AttributeTok{model =} \StringTok{"random"}\NormalTok{,}
        \AttributeTok{random.method =} \StringTok{"amemiya"}\NormalTok{,}
        \AttributeTok{effect =} \StringTok{"twoways"}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

To call the estimation of the variance of the error components

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ercomp}\NormalTok{(Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X,}
       \AttributeTok{data =}\NormalTok{ pdata,}
       \AttributeTok{method =} \StringTok{"amemiya"}\NormalTok{,}
       \AttributeTok{effect =} \StringTok{"twoways"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Check for the unbalancedness. Closer to 1 indicates balanced data \citep{ahrens1981two}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{punbalancedness}\NormalTok{(random)}
\end{Highlighting}
\end{Shaded}

\textbf{Instrumental variable}

\begin{itemize}
\tightlist
\item
  \texttt{"bvk"}: default \citep{balestra1987full}
\item
  \texttt{"baltagi"}: \citep{baltagi1981simultaneous}
\item
  \texttt{"am"} \citep{amemiya1986instrumental}
\item
  \texttt{"bms"}: \citep{breusch1989efficient}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{instr }\OtherTok{\textless{}{-}}
    \FunctionTok{plm}\NormalTok{(}
\NormalTok{        Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X }\SpecialCharTok{|}\NormalTok{ X\_ins,}
        \AttributeTok{data =}\NormalTok{ pdata,}
        \AttributeTok{random.method =} \StringTok{"ht"}\NormalTok{,}
        \AttributeTok{model =} \StringTok{"random"}\NormalTok{,}
        \AttributeTok{inst.method =} \StringTok{"baltagi"}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\hypertarget{other-estimators}{%
\paragraph{Other Estimators}\label{other-estimators}}

\hypertarget{variable-coefficients-model}{%
\subparagraph{Variable Coefficients Model}\label{variable-coefficients-model}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fixed\_pvcm  }\OtherTok{\textless{}{-}} \FunctionTok{pvcm}\NormalTok{(Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X, }\AttributeTok{data =}\NormalTok{ pdata, }\AttributeTok{model =} \StringTok{"within"}\NormalTok{)}
\NormalTok{random\_pvcm }\OtherTok{\textless{}{-}} \FunctionTok{pvcm}\NormalTok{(Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X, }\AttributeTok{data =}\NormalTok{ pdata, }\AttributeTok{model =} \StringTok{"random"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

More details can be found \href{https://cran.r-project.org/web/packages/plm/vignettes/plmPackage.html}{here}

\hypertarget{generalized-method-of-moments-estimator}{%
\subparagraph{Generalized Method of Moments Estimator}\label{generalized-method-of-moments-estimator}}

Typically use in dynamic models. Example is from \href{https://cran.r-project.org/web/packages/plm/vignettes/plmPackage.html}{plm package}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{z2 }\OtherTok{\textless{}{-}} \FunctionTok{pgmm}\NormalTok{(}
    \FunctionTok{log}\NormalTok{(emp) }\SpecialCharTok{\textasciitilde{}} \FunctionTok{lag}\NormalTok{(}\FunctionTok{log}\NormalTok{(emp), }\DecValTok{1}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{lag}\NormalTok{(}\FunctionTok{log}\NormalTok{(wage), }\DecValTok{0}\SpecialCharTok{:}\DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
        \FunctionTok{lag}\NormalTok{(}\FunctionTok{log}\NormalTok{(capital), }\DecValTok{0}\SpecialCharTok{:}\DecValTok{1}\NormalTok{) }\SpecialCharTok{|} \FunctionTok{lag}\NormalTok{(}\FunctionTok{log}\NormalTok{(emp), }\DecValTok{2}\SpecialCharTok{:}\DecValTok{99}\NormalTok{) }\SpecialCharTok{+}
        \FunctionTok{lag}\NormalTok{(}\FunctionTok{log}\NormalTok{(wage), }\DecValTok{2}\SpecialCharTok{:}\DecValTok{99}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{lag}\NormalTok{(}\FunctionTok{log}\NormalTok{(capital), }\DecValTok{2}\SpecialCharTok{:}\DecValTok{99}\NormalTok{),}
    \AttributeTok{data =}\NormalTok{ EmplUK,}
    \AttributeTok{effect =} \StringTok{"twoways"}\NormalTok{,}
    \AttributeTok{model =} \StringTok{"onestep"}\NormalTok{,}
    \AttributeTok{transformation =} \StringTok{"ld"}
\NormalTok{)}
\FunctionTok{summary}\NormalTok{(z2, }\AttributeTok{robust =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{general-feasible-generalized-least-squares-models}{%
\subparagraph{General Feasible Generalized Least Squares Models}\label{general-feasible-generalized-least-squares-models}}

Assume there is no cross-sectional correlation Robust against intragroup heteroskedasticity and serial correlation. Suited when n is much larger than T (long panel) However, inefficient under group-wise heteorskedasticity.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Random Effects}
\NormalTok{zz }\OtherTok{\textless{}{-}}
    \FunctionTok{pggls}\NormalTok{(}\FunctionTok{log}\NormalTok{(emp) }\SpecialCharTok{\textasciitilde{}} \FunctionTok{log}\NormalTok{(wage) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{(capital),}
          \AttributeTok{data =}\NormalTok{ EmplUK,}
          \AttributeTok{model =} \StringTok{"pooling"}\NormalTok{)}

\CommentTok{\# Fixed}
\NormalTok{zz }\OtherTok{\textless{}{-}}
    \FunctionTok{pggls}\NormalTok{(}\FunctionTok{log}\NormalTok{(emp) }\SpecialCharTok{\textasciitilde{}} \FunctionTok{log}\NormalTok{(wage) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{(capital),}
          \AttributeTok{data =}\NormalTok{ EmplUK,}
          \AttributeTok{model =} \StringTok{"within"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{fixest-package}{%
\subsubsection{\texorpdfstring{\texttt{fixest} package}{fixest package}}\label{fixest-package}}

Available functions

\begin{itemize}
\item
  \texttt{feols}: linear models
\item
  \texttt{feglm}: generalized linear models
\item
  \texttt{femlm}: maximum likelihood estimation
\item
  \texttt{feNmlm}: non-linear in RHS parameters
\item
  \texttt{fepois}: Poisson fixed-effect
\item
  \texttt{fenegbin}: negative binomial fixed-effect
\end{itemize}

Notes

\begin{itemize}
\tightlist
\item
  can only work for \texttt{fixest} object
\end{itemize}

Examples by the package's \href{https://cran.r-project.org/web/packages/fixest/vignettes/exporting_tables.html}{authors}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(fixest)}
\FunctionTok{data}\NormalTok{(airquality)}

\CommentTok{\# Setting a dictionary}
\FunctionTok{setFixest\_dict}\NormalTok{(}
    \FunctionTok{c}\NormalTok{(}
        \AttributeTok{Ozone   =} \StringTok{"Ozone (ppb)"}\NormalTok{,}
        \AttributeTok{Solar.R =} \StringTok{"Solar Radiation (Langleys)"}\NormalTok{,}
        \AttributeTok{Wind    =} \StringTok{"Wind Speed (mph)"}\NormalTok{,}
        \AttributeTok{Temp    =} \StringTok{"Temperature"}
\NormalTok{    )}
\NormalTok{)}


\CommentTok{\# On multiple estimations: see the dedicated vignette}
\NormalTok{est }\OtherTok{=} \FunctionTok{feols}\NormalTok{(}
\NormalTok{    Ozone }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Solar.R }\SpecialCharTok{+} \FunctionTok{sw0}\NormalTok{(Wind }\SpecialCharTok{+}\NormalTok{ Temp) }\SpecialCharTok{|} \FunctionTok{csw}\NormalTok{(Month, Day),}
    \AttributeTok{data =}\NormalTok{ airquality,}
    \AttributeTok{cluster =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ Day}
\NormalTok{)}

\FunctionTok{etable}\NormalTok{(est)}
\CommentTok{\#\textgreater{}                                         est.1              est.2}
\CommentTok{\#\textgreater{} Dependent Var.:                   Ozone (ppb)        Ozone (ppb)}
\CommentTok{\#\textgreater{}                                                                 }
\CommentTok{\#\textgreater{} Solar Radiation (Langleys) 0.1148*** (0.0234)   0.0522* (0.0202)}
\CommentTok{\#\textgreater{} Wind Speed (mph)                              {-}3.109*** (0.7986)}
\CommentTok{\#\textgreater{} Temperature                                    1.875*** (0.3671)}
\CommentTok{\#\textgreater{} Fixed{-}Effects:             {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} Month                                     Yes                Yes}
\CommentTok{\#\textgreater{} Day                                        No                 No}
\CommentTok{\#\textgreater{} \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}
\CommentTok{\#\textgreater{} S.E.: Clustered                       by: Day            by: Day}
\CommentTok{\#\textgreater{} Observations                              111                111}
\CommentTok{\#\textgreater{} R2                                    0.31974            0.63686}
\CommentTok{\#\textgreater{} Within R2                             0.12245            0.53154}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}                                        est.3              est.4}
\CommentTok{\#\textgreater{} Dependent Var.:                  Ozone (ppb)        Ozone (ppb)}
\CommentTok{\#\textgreater{}                                                                }
\CommentTok{\#\textgreater{} Solar Radiation (Langleys) 0.1078** (0.0329)   0.0509* (0.0236)}
\CommentTok{\#\textgreater{} Wind Speed (mph)                             {-}3.289*** (0.7777)}
\CommentTok{\#\textgreater{} Temperature                                   2.052*** (0.2415)}
\CommentTok{\#\textgreater{} Fixed{-}Effects:             {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} Month                                    Yes                Yes}
\CommentTok{\#\textgreater{} Day                                      Yes                Yes}
\CommentTok{\#\textgreater{} \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}
\CommentTok{\#\textgreater{} S.E.: Clustered                      by: Day            by: Day}
\CommentTok{\#\textgreater{} Observations                             111                111}
\CommentTok{\#\textgreater{} R2                                   0.58018            0.81604}
\CommentTok{\#\textgreater{} Within R2                            0.12074            0.61471}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes: 0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}

\CommentTok{\# in latex}
\FunctionTok{etable}\NormalTok{(est, }\AttributeTok{tex =}\NormalTok{ T)}
\CommentTok{\#\textgreater{} \textbackslash{}begingroup}
\CommentTok{\#\textgreater{} \textbackslash{}centering}
\CommentTok{\#\textgreater{} \textbackslash{}begin\{tabular\}\{lcccc\}}
\CommentTok{\#\textgreater{}    \textbackslash{}tabularnewline \textbackslash{}midrule \textbackslash{}midrule}
\CommentTok{\#\textgreater{}    Dependent Variable: \& \textbackslash{}multicolumn\{4\}\{c\}\{Ozone (ppb)\}\textbackslash{}\textbackslash{}}
\CommentTok{\#\textgreater{}    Model:                     \& (1)            \& (2)            \& (3)            \& (4)\textbackslash{}\textbackslash{}  }
\CommentTok{\#\textgreater{}    \textbackslash{}midrule}
\CommentTok{\#\textgreater{}    \textbackslash{}emph\{Variables\}\textbackslash{}\textbackslash{}}
\CommentTok{\#\textgreater{}    Solar Radiation (Langleys) \& 0.1148$\^{}\{***\}$ \& 0.0522$\^{}\{**\}$  \& 0.1078$\^{}\{***\}$ \& 0.0509$\^{}\{**\}$\textbackslash{}\textbackslash{}   }
\CommentTok{\#\textgreater{}                               \& (0.0234)       \& (0.0202)       \& (0.0329)       \& (0.0236)\textbackslash{}\textbackslash{}   }
\CommentTok{\#\textgreater{}    Wind Speed (mph)           \&                \& {-}3.109$\^{}\{***\}$ \&                \& {-}3.289$\^{}\{***\}$\textbackslash{}\textbackslash{}   }
\CommentTok{\#\textgreater{}                               \&                \& (0.7986)       \&                \& (0.7777)\textbackslash{}\textbackslash{}   }
\CommentTok{\#\textgreater{}    Temperature                \&                \& 1.875$\^{}\{***\}$  \&                \& 2.052$\^{}\{***\}$\textbackslash{}\textbackslash{}   }
\CommentTok{\#\textgreater{}                               \&                \& (0.3671)       \&                \& (0.2415)\textbackslash{}\textbackslash{}   }
\CommentTok{\#\textgreater{}    \textbackslash{}midrule}
\CommentTok{\#\textgreater{}    \textbackslash{}emph\{Fixed{-}effects\}\textbackslash{}\textbackslash{}}
\CommentTok{\#\textgreater{}    Month                      \& Yes            \& Yes            \& Yes            \& Yes\textbackslash{}\textbackslash{}  }
\CommentTok{\#\textgreater{}    Day                        \&                \&                \& Yes            \& Yes\textbackslash{}\textbackslash{}  }
\CommentTok{\#\textgreater{}    \textbackslash{}midrule}
\CommentTok{\#\textgreater{}    \textbackslash{}emph\{Fit statistics\}\textbackslash{}\textbackslash{}}
\CommentTok{\#\textgreater{}    Observations               \& 111            \& 111            \& 111            \& 111\textbackslash{}\textbackslash{}  }
\CommentTok{\#\textgreater{}    R$\^{}2$                      \& 0.31974        \& 0.63686        \& 0.58018        \& 0.81604\textbackslash{}\textbackslash{}  }
\CommentTok{\#\textgreater{}    Within R$\^{}2$               \& 0.12245        \& 0.53154        \& 0.12074        \& 0.61471\textbackslash{}\textbackslash{}  }
\CommentTok{\#\textgreater{}    \textbackslash{}midrule \textbackslash{}midrule}
\CommentTok{\#\textgreater{}    \textbackslash{}multicolumn\{5\}\{l\}\{\textbackslash{}emph\{Clustered (Day) standard{-}errors in parentheses\}\}\textbackslash{}\textbackslash{}}
\CommentTok{\#\textgreater{}    \textbackslash{}multicolumn\{5\}\{l\}\{\textbackslash{}emph\{Signif. Codes: ***: 0.01, **: 0.05, *: 0.1\}\}\textbackslash{}\textbackslash{}}
\CommentTok{\#\textgreater{} \textbackslash{}end\{tabular\}}
\CommentTok{\#\textgreater{} \textbackslash{}par\textbackslash{}endgroup}


\CommentTok{\# get the fixed{-}effects coefficients for 1 model}
\NormalTok{fixedEffects }\OtherTok{=} \FunctionTok{fixef}\NormalTok{(est[[}\DecValTok{1}\NormalTok{]])}
\FunctionTok{summary}\NormalTok{(fixedEffects)}
\CommentTok{\#\textgreater{} Fixed\_effects coefficients}
\CommentTok{\#\textgreater{} Number of fixed{-}effects for variable Month is 5.}
\CommentTok{\#\textgreater{}  Mean = 19.6 Variance = 272}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} COEFFICIENTS:}
\CommentTok{\#\textgreater{}   Month:     5     6     7     8     9}
\CommentTok{\#\textgreater{}          3.219 8.288 34.26 40.12 12.13}

\CommentTok{\# see the fixed effects for one dimension}
\NormalTok{fixedEffects}\SpecialCharTok{$}\NormalTok{Month}
\CommentTok{\#\textgreater{}         5         6         7         8         9 }
\CommentTok{\#\textgreater{}  3.218876  8.287899 34.260812 40.122257 12.130971}

\FunctionTok{plot}\NormalTok{(fixedEffects)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{12-data_files/figure-latex/unnamed-chunk-24-1} \end{center}

For \href{https://cran.r-project.org/web/packages/fixest/vignettes/multiple_estimations.html}{multiple estimation}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# set up}
\FunctionTok{library}\NormalTok{(fixest)}

\CommentTok{\# let R know the base dataset (the biggest/ultimate }
\CommentTok{\# dataset that includes everything in your analysis)}
\NormalTok{base }\OtherTok{=}\NormalTok{ iris}

\CommentTok{\# rename variables}
\FunctionTok{names}\NormalTok{(base) }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"y1"}\NormalTok{, }\StringTok{"y2"}\NormalTok{, }\StringTok{"x1"}\NormalTok{, }\StringTok{"x2"}\NormalTok{, }\StringTok{"species"}\NormalTok{)}

\NormalTok{res\_multi }\OtherTok{=} \FunctionTok{feols}\NormalTok{(}
    \FunctionTok{c}\NormalTok{(y1, y2) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x1 }\SpecialCharTok{+} \FunctionTok{csw}\NormalTok{(x2, x2 }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{) }\SpecialCharTok{|}
        \FunctionTok{sw0}\NormalTok{(species),}
    \AttributeTok{data =}\NormalTok{ base,}
    \AttributeTok{fsplit =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ species,}
    \AttributeTok{lean =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{vcov =} \StringTok{"hc1"} \CommentTok{\# can also clustered at the fixed effect level}
\NormalTok{)}
\CommentTok{\# it\textquotesingle{}s recommended to use vcov at }
\CommentTok{\# estimation stage, not summary stage}

\FunctionTok{summary}\NormalTok{(res\_multi, }\StringTok{"compact"}\NormalTok{)}
\CommentTok{\#\textgreater{}         sample   fixef lhs               rhs     (Intercept)                x1}
\CommentTok{\#\textgreater{} 1  Full sample 1        y1 x1 + x2           4.19*** (0.104)  0.542*** (0.076)}
\CommentTok{\#\textgreater{} 2  Full sample 1        y1 x1 + x2 + I(x2\^{}2) 4.27*** (0.101)  0.719*** (0.082)}
\CommentTok{\#\textgreater{} 3  Full sample 1        y2 x1 + x2           3.59*** (0.103) {-}0.257*** (0.066)}
\CommentTok{\#\textgreater{} 4  Full sample 1        y2 x1 + x2 + I(x2\^{}2) 3.68*** (0.097)    {-}0.030 (0.078)}
\CommentTok{\#\textgreater{} 5  Full sample species  y1 x1 + x2                            0.906*** (0.076)}
\CommentTok{\#\textgreater{} 6  Full sample species  y1 x1 + x2 + I(x2\^{}2)                  0.900*** (0.077)}
\CommentTok{\#\textgreater{} 7  Full sample species  y2 x1 + x2                              0.155* (0.073)}
\CommentTok{\#\textgreater{} 8  Full sample species  y2 x1 + x2 + I(x2\^{}2)                    0.148. (0.075)}
\CommentTok{\#\textgreater{} 9  setosa      1        y1 x1 + x2           4.25*** (0.474)     0.399 (0.325)}
\CommentTok{\#\textgreater{} 10 setosa      1        y1 x1 + x2 + I(x2\^{}2) 4.00*** (0.504)     0.405 (0.325)}
\CommentTok{\#\textgreater{} 11 setosa      1        y2 x1 + x2           2.89*** (0.416)     0.247 (0.305)}
\CommentTok{\#\textgreater{} 12 setosa      1        y2 x1 + x2 + I(x2\^{}2) 2.82*** (0.423)     0.248 (0.304)}
\CommentTok{\#\textgreater{} 13 setosa      species  y1 x1 + x2                               0.399 (0.325)}
\CommentTok{\#\textgreater{} 14 setosa      species  y1 x1 + x2 + I(x2\^{}2)                     0.405 (0.325)}
\CommentTok{\#\textgreater{} 15 setosa      species  y2 x1 + x2                               0.247 (0.305)}
\CommentTok{\#\textgreater{} 16 setosa      species  y2 x1 + x2 + I(x2\^{}2)                     0.248 (0.304)}
\CommentTok{\#\textgreater{} 17 versicolor  1        y1 x1 + x2           2.38*** (0.423)  0.934*** (0.166)}
\CommentTok{\#\textgreater{} 18 versicolor  1        y1 x1 + x2 + I(x2\^{}2)   0.323 (1.44)   0.901*** (0.164)}
\CommentTok{\#\textgreater{} 19 versicolor  1        y2 x1 + x2           1.25*** (0.275)     0.067 (0.095)}
\CommentTok{\#\textgreater{} 20 versicolor  1        y2 x1 + x2 + I(x2\^{}2)   0.097 (1.01)      0.048 (0.099)}
\CommentTok{\#\textgreater{} 21 versicolor  species  y1 x1 + x2                            0.934*** (0.166)}
\CommentTok{\#\textgreater{} 22 versicolor  species  y1 x1 + x2 + I(x2\^{}2)                  0.901*** (0.164)}
\CommentTok{\#\textgreater{} 23 versicolor  species  y2 x1 + x2                               0.067 (0.095)}
\CommentTok{\#\textgreater{} 24 versicolor  species  y2 x1 + x2 + I(x2\^{}2)                     0.048 (0.099)}
\CommentTok{\#\textgreater{} 25 virginica   1        y1 x1 + x2             1.05. (0.539)  0.995*** (0.090)}
\CommentTok{\#\textgreater{} 26 virginica   1        y1 x1 + x2 + I(x2\^{}2)   {-}2.39 (2.04)   0.994*** (0.088)}
\CommentTok{\#\textgreater{} 27 virginica   1        y2 x1 + x2             1.06. (0.572)     0.149 (0.107)}
\CommentTok{\#\textgreater{} 28 virginica   1        y2 x1 + x2 + I(x2\^{}2)    1.10 (1.76)      0.149 (0.108)}
\CommentTok{\#\textgreater{} 29 virginica   species  y1 x1 + x2                            0.995*** (0.090)}
\CommentTok{\#\textgreater{} 30 virginica   species  y1 x1 + x2 + I(x2\^{}2)                  0.994*** (0.088)}
\CommentTok{\#\textgreater{} 31 virginica   species  y2 x1 + x2                               0.149 (0.107)}
\CommentTok{\#\textgreater{} 32 virginica   species  y2 x1 + x2 + I(x2\^{}2)                     0.149 (0.108)}
\CommentTok{\#\textgreater{}                  x2          I(x2\^{}2)}
\CommentTok{\#\textgreater{} 1   {-}0.320. (0.170)                 }
\CommentTok{\#\textgreater{} 2  {-}1.52*** (0.307) 0.348*** (0.075)}
\CommentTok{\#\textgreater{} 3    0.364* (0.142)                 }
\CommentTok{\#\textgreater{} 4  {-}1.18*** (0.313) 0.446*** (0.074)}
\CommentTok{\#\textgreater{} 5    {-}0.006 (0.163)                 }
\CommentTok{\#\textgreater{} 6     0.290 (0.408)   {-}0.088 (0.117)}
\CommentTok{\#\textgreater{} 7  0.623*** (0.114)                 }
\CommentTok{\#\textgreater{} 8    0.951* (0.472)   {-}0.097 (0.125)}
\CommentTok{\#\textgreater{} 9    0.712. (0.418)                 }
\CommentTok{\#\textgreater{} 10    2.51. (1.47)     {-}2.91 (2.10) }
\CommentTok{\#\textgreater{} 11    0.702 (0.560)                 }
\CommentTok{\#\textgreater{} 12     1.27 (2.39)    {-}0.911 (3.28) }
\CommentTok{\#\textgreater{} 13   0.712. (0.418)                 }
\CommentTok{\#\textgreater{} 14    2.51. (1.47)     {-}2.91 (2.10) }
\CommentTok{\#\textgreater{} 15    0.702 (0.560)                 }
\CommentTok{\#\textgreater{} 16     1.27 (2.39)    {-}0.911 (3.28) }
\CommentTok{\#\textgreater{} 17   {-}0.320 (0.364)                 }
\CommentTok{\#\textgreater{} 18     3.01 (2.31)     {-}1.24 (0.841)}
\CommentTok{\#\textgreater{} 19 0.929*** (0.244)                 }
\CommentTok{\#\textgreater{} 20    2.80. (1.65)    {-}0.695 (0.583)}
\CommentTok{\#\textgreater{} 21   {-}0.320 (0.364)                 }
\CommentTok{\#\textgreater{} 22     3.01 (2.31)     {-}1.24 (0.841)}
\CommentTok{\#\textgreater{} 23 0.929*** (0.244)                 }
\CommentTok{\#\textgreater{} 24    2.80. (1.65)    {-}0.695 (0.583)}
\CommentTok{\#\textgreater{} 25    0.007 (0.205)                 }
\CommentTok{\#\textgreater{} 26    3.50. (2.09)    {-}0.870 (0.519)}
\CommentTok{\#\textgreater{} 27 0.535*** (0.122)                 }
\CommentTok{\#\textgreater{} 28    0.503 (1.56)     0.008 (0.388)}
\CommentTok{\#\textgreater{} 29    0.007 (0.205)                 }
\CommentTok{\#\textgreater{} 30    3.50. (2.09)    {-}0.870 (0.519)}
\CommentTok{\#\textgreater{} 31 0.535*** (0.122)                 }
\CommentTok{\#\textgreater{} 32    0.503 (1.56)     0.008 (0.388)}

\CommentTok{\# call the first 3 estimated models only}
\FunctionTok{etable}\NormalTok{(res\_multi[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{],}
       
       \CommentTok{\# customize the headers}
       \AttributeTok{headers =} \FunctionTok{c}\NormalTok{(}\StringTok{"mod1"}\NormalTok{, }\StringTok{"mod2"}\NormalTok{, }\StringTok{"mod3"}\NormalTok{)) }
\CommentTok{\#\textgreater{}                   res\_multi[1:3].1   res\_multi[1:3].2    res\_multi[1:3].3}
\CommentTok{\#\textgreater{}                               mod1               mod2                mod3}
\CommentTok{\#\textgreater{} Dependent Var.:                 y1                 y1                  y2}
\CommentTok{\#\textgreater{}                                                                          }
\CommentTok{\#\textgreater{} Constant         4.191*** (0.1037)  4.266*** (0.1007)   3.587*** (0.1031)}
\CommentTok{\#\textgreater{} x1              0.5418*** (0.0761) 0.7189*** (0.0815) {-}0.2571*** (0.0664)}
\CommentTok{\#\textgreater{} x2               {-}0.3196. (0.1700) {-}1.522*** (0.3072)    0.3640* (0.1419)}
\CommentTok{\#\textgreater{} x2 square                          0.3479*** (0.0748)                    }
\CommentTok{\#\textgreater{} \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}
\CommentTok{\#\textgreater{} S.E. type       Heteroskedas.{-}rob. Heteroskedas.{-}rob. Heteroskedast.{-}rob.}
\CommentTok{\#\textgreater{} Observations                   150                150                 150}
\CommentTok{\#\textgreater{} R2                         0.76626            0.79456             0.21310}
\CommentTok{\#\textgreater{} Adj. R2                    0.76308            0.79034             0.20240}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes: 0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

\hypertarget{multiple-estimation-left-hand-side}{%
\paragraph{Multiple estimation (Left-hand side)}\label{multiple-estimation-left-hand-side}}

\begin{itemize}
\tightlist
\item
  When you have multiple interested dependent variables
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{etable}\NormalTok{(}\FunctionTok{feols}\NormalTok{(}\FunctionTok{c}\NormalTok{(y1, y2) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x1 }\SpecialCharTok{+}\NormalTok{ x2, base))}
\CommentTok{\#\textgreater{}                 feols(c(y1, y2)..1 feols(c(y1, y2) ..2}
\CommentTok{\#\textgreater{} Dependent Var.:                 y1                  y2}
\CommentTok{\#\textgreater{}                                                       }
\CommentTok{\#\textgreater{} Constant         4.191*** (0.0970)   3.587*** (0.0937)}
\CommentTok{\#\textgreater{} x1              0.5418*** (0.0693) {-}0.2571*** (0.0669)}
\CommentTok{\#\textgreater{} x2               {-}0.3196* (0.1605)    0.3640* (0.1550)}
\CommentTok{\#\textgreater{} \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}
\CommentTok{\#\textgreater{} S.E. type                      IID                 IID}
\CommentTok{\#\textgreater{} Observations                   150                 150}
\CommentTok{\#\textgreater{} R2                         0.76626             0.21310}
\CommentTok{\#\textgreater{} Adj. R2                    0.76308             0.20240}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes: 0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

To input a list of dependent variable

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{depvars }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"y1"}\NormalTok{, }\StringTok{"y2"}\NormalTok{)}

\NormalTok{res }\OtherTok{\textless{}{-}} \FunctionTok{lapply}\NormalTok{(depvars, }\ControlFlowTok{function}\NormalTok{(var) \{}
\NormalTok{    res }\OtherTok{\textless{}{-}} \FunctionTok{feols}\NormalTok{(}\FunctionTok{xpd}\NormalTok{(..lhs }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x1 }\SpecialCharTok{+}\NormalTok{ x2, }\AttributeTok{..lhs =}\NormalTok{ var), }\AttributeTok{data =}\NormalTok{ base)}
    \CommentTok{\# summary(res)}
\NormalTok{\})}
\FunctionTok{etable}\NormalTok{(res)}
\CommentTok{\#\textgreater{}                            model 1             model 2}
\CommentTok{\#\textgreater{} Dependent Var.:                 y1                  y2}
\CommentTok{\#\textgreater{}                                                       }
\CommentTok{\#\textgreater{} Constant         4.191*** (0.0970)   3.587*** (0.0937)}
\CommentTok{\#\textgreater{} x1              0.5418*** (0.0693) {-}0.2571*** (0.0669)}
\CommentTok{\#\textgreater{} x2               {-}0.3196* (0.1605)    0.3640* (0.1550)}
\CommentTok{\#\textgreater{} \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}
\CommentTok{\#\textgreater{} S.E. type                      IID                 IID}
\CommentTok{\#\textgreater{} Observations                   150                 150}
\CommentTok{\#\textgreater{} R2                         0.76626             0.21310}
\CommentTok{\#\textgreater{} Adj. R2                    0.76308             0.20240}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes: 0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

\hypertarget{multiple-estimation-right-hand-side}{%
\paragraph{Multiple estimation (Right-hand side)}\label{multiple-estimation-right-hand-side}}

Options to write the functions

\begin{itemize}
\item
  \texttt{sw} (stepwise): sequentially analyze each elements

  \begin{itemize}
  \tightlist
  \item
    \texttt{y\ \textasciitilde{}\ sw(x1,\ x2)} will be estimated as \texttt{y\ \textasciitilde{}\ x1} and \texttt{y\ \textasciitilde{}\ x2}
  \end{itemize}
\item
  \texttt{sw0} (stepwise 0): similar to \texttt{sw} but also estimate a model without the elements in the set first

  \begin{itemize}
  \tightlist
  \item
    \texttt{y\ \textasciitilde{}\ sw(x1,\ x2)} will be estimated as \texttt{y\ \textasciitilde{}\ 1} and \texttt{y\ \textasciitilde{}\ x1} and \texttt{y\ \textasciitilde{}\ x2}
  \end{itemize}
\item
  \texttt{csw} (cumulative stepwise): sequentially add each element of the set to the formula

  \begin{itemize}
  \tightlist
  \item
    \texttt{y\ \textasciitilde{}\ csw(x1,\ x2)} will be estimated as \texttt{y\ \textasciitilde{}\ x1} and \texttt{y\ \textasciitilde{}\ x1\ +\ x2}
  \end{itemize}
\item
  \texttt{csw0} (cumulative stepwise 0): similar to \texttt{csw} but also estimate a model without the elements in the set first

  \begin{itemize}
  \tightlist
  \item
    \texttt{y\ \textasciitilde{}\ csw(x1,\ x2)} will be estimated as \texttt{y\textasciitilde{}\ 1} \texttt{y\ \textasciitilde{}\ x1} and \texttt{y\ \textasciitilde{}\ x1\ +\ x2}
  \end{itemize}
\item
  \texttt{mvsw} (multiverse stepwise): all possible combination of the elements in the set (it will get large very quick).

  \begin{itemize}
  \tightlist
  \item
    \texttt{mvsw(x1,\ x2,\ x3)} will be \texttt{sw0(x1,\ x2,\ x3,\ x1\ +\ x2,\ x1\ +\ x3,\ x2\ +\ x3,\ x1\ +\ x2\ +\ x3)}
  \end{itemize}
\end{itemize}

\hypertarget{split-sample-estimation}{%
\paragraph{Split sample estimation}\label{split-sample-estimation}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{etable}\NormalTok{(}\FunctionTok{feols}\NormalTok{(y1 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x1 }\SpecialCharTok{+}\NormalTok{ x2, }\AttributeTok{fsplit =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ species, }\AttributeTok{data =}\NormalTok{ base))}
\CommentTok{\#\textgreater{}                  feols(y1 \textasciitilde{} x1 +..1 feols(y1 \textasciitilde{} x1 ..2 feols(y1 \textasciitilde{} x1 +..3}
\CommentTok{\#\textgreater{} Sample (species)        Full sample            setosa         versicolor}
\CommentTok{\#\textgreater{} Dependent Var.:                  y1                y1                 y1}
\CommentTok{\#\textgreater{}                                                                         }
\CommentTok{\#\textgreater{} Constant          4.191*** (0.0970) 4.248*** (0.4114)  2.381*** (0.4493)}
\CommentTok{\#\textgreater{} x1               0.5418*** (0.0693)   0.3990 (0.2958) 0.9342*** (0.1693)}
\CommentTok{\#\textgreater{} x2                {-}0.3196* (0.1605)   0.7121 (0.4874)   {-}0.3200 (0.4024)}
\CommentTok{\#\textgreater{} \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}
\CommentTok{\#\textgreater{} S.E. type                       IID               IID                IID}
\CommentTok{\#\textgreater{} Observations                    150                50                 50}
\CommentTok{\#\textgreater{} R2                          0.76626           0.11173            0.57432}
\CommentTok{\#\textgreater{} Adj. R2                     0.76308           0.07393            0.55620}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}                  feols(y1 \textasciitilde{} x1 +..4}
\CommentTok{\#\textgreater{} Sample (species)          virginica}
\CommentTok{\#\textgreater{} Dependent Var.:                  y1}
\CommentTok{\#\textgreater{}                                    }
\CommentTok{\#\textgreater{} Constant            1.052* (0.5139)}
\CommentTok{\#\textgreater{} x1               0.9946*** (0.0893)}
\CommentTok{\#\textgreater{} x2                  0.0071 (0.1795)}
\CommentTok{\#\textgreater{} \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}
\CommentTok{\#\textgreater{} S.E. type                       IID}
\CommentTok{\#\textgreater{} Observations                     50}
\CommentTok{\#\textgreater{} R2                          0.74689}
\CommentTok{\#\textgreater{} Adj. R2                     0.73612}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes: 0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

\hypertarget{standard-errors-1}{%
\paragraph{Standard Errors}\label{standard-errors-1}}

\begin{itemize}
\item
  \texttt{iid}: errors are homoskedastic and independent and identically distributed
\item
  \texttt{hetero}: errors are heteroskedastic using White correction
\item
  \texttt{cluster}: errors are correlated within the cluster groups
\item
  \texttt{newey\_west}: \citep{newey1986simple} use for time series or panel data. Errors are heteroskedastic and serially correlated.

  \begin{itemize}
  \item
    \texttt{vcov\ =\ newey\_west\ \textasciitilde{}\ id\ +\ period} where \texttt{id} is the subject id and \texttt{period} is time period of the panel.
  \item
    to specify lag period to consider \texttt{vcov\ =\ newey\_west(2)\ \textasciitilde{}\ id\ +\ period} where we're considering 2 lag periods.
  \end{itemize}
\item
  \texttt{driscoll\_kraay} \citep{driscoll1998consistent} use for panel data. Errors are cross-sectionally and serially correlated.

  \begin{itemize}
  \tightlist
  \item
    \texttt{vcov\ =\ discoll\_kraay\ \textasciitilde{}\ period}
  \end{itemize}
\item
  \texttt{conley}: \citep{conley1999gmm} for cross-section data. Errors are spatially correlated

  \begin{itemize}
  \item
    \texttt{vcov\ =\ conley\ \textasciitilde{}\ latitude\ +\ longitude}
  \item
    to specify the distance cutoff, \texttt{vcov\ =\ vcov\_conley(lat\ =\ "lat",\ lon\ =\ "long",\ cutoff\ =\ 100,\ distance\ =\ "spherical")}, which will use the \texttt{conley()} helper function.
  \end{itemize}
\item
  \texttt{hc}: from the \texttt{sandwich} package

  \begin{itemize}
  \tightlist
  \item
    \texttt{vcov\ =\ function(x)\ sandwich::vcovHC(x,\ type\ =\ "HC1"))}
  \end{itemize}
\end{itemize}

To let R know which SE estimation you want to use, insert \texttt{vcov\ =\ vcov\_type\ \textasciitilde{}\ variables}

\hypertarget{small-sample-correction}{%
\paragraph{Small sample correction}\label{small-sample-correction}}

To specify that R needs to use small sample correction add

\texttt{ssc\ =\ ssc(adj\ =\ T,\ cluster.adj\ =\ T)}

\hypertarget{variable-transformation}{%
\chapter{Variable Transformation}\label{variable-transformation}}

\texttt{trafo} \href{https://cran.microsoft.com/snapshot/2018-08-09/web/packages/trafo/vignettes/vignette_trafo.pdf}{vignette}

\hypertarget{continuous-variables}{%
\section{Continuous Variables}\label{continuous-variables}}

Purposes:

\begin{itemize}
\item
  To change the scale of the variables
\item
  To transform skewed data distribution to normal distribution
\end{itemize}

\hypertarget{standardization}{%
\subsection{Standardization}\label{standardization}}

\[
x_i' = \frac{x_i - \bar{x}}{s}
\]

when you have a few large numbers

\hypertarget{min-max-scaling}{%
\subsection{Min-max scaling}\label{min-max-scaling}}

\[
x_i' = \frac{x_i - x_{max}}{x_{max} - x_{min}}
\]

dependent on the min and max values, which makes it sensitive to outliers.

best to use when you have values in a fixed interval.

\hypertarget{square-rootcube-root}{%
\subsection{Square Root/Cube Root}\label{square-rootcube-root}}

\begin{itemize}
\item
  When variables have positive skewness or residuals have positive heteroskasticity.
\item
  Frequency counts variable
\item
  Data have many 0 or extremely small values.
\end{itemize}

\hypertarget{logarithmic}{%
\subsection{Logarithmic}\label{logarithmic}}

\begin{itemize}
\tightlist
\item
  Variables have positively skewed distribution
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5556}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.4444}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Formula
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
In case
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(x_i' = \log(x_i)\) & cannot work zero because \texttt{log(0)\ =\ -Inf} \\
\(x_i' = \log(x_i + 1)\) & variables with 0 \\
\(x_i' = \log(x_i +c)\) & \\
\(x_i' = \frac{x_i}{|x_i|}\log|x_i|\) & variables with negative values \\
\(x_i'^\lambda = \log(x_i + \sqrt{x_i^2 + \lambda})\) & generalized log transformation \\
\end{longtable}

For the general case of \(\log(x_i + c)\), choosing a constant is rather tricky.

The choice of the constant is critically important, especially when you want to do inference. It can dramatically change your model fit (see \citep{ekwaru2018overlooked} for the independent variable case).

\citet{chen2023logs} show that in causal inference problem, \(\log\) transformation of values with meaningful 0 is problematic. But there are solutions for each approach (e.g., DID, IV).

However, assuming that you do not have 0s because of

\begin{itemize}
\item
  Censoring
\item
  No measurement errors (stemming from measurement tools)
\end{itemize}

We can proceed choosing \texttt{c} (it's okay if your 0's are represent really small values).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(cars)}
\NormalTok{cars}\SpecialCharTok{$}\NormalTok{speed }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{} [1] 4 4 7 7 8 9}

\FunctionTok{log}\NormalTok{(cars}\SpecialCharTok{$}\NormalTok{speed) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{} [1] 1.386294 1.386294 1.945910 1.945910 2.079442 2.197225}

\CommentTok{\# log(x+1)}
\FunctionTok{log1p}\NormalTok{(cars}\SpecialCharTok{$}\NormalTok{speed) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{} [1] 1.609438 1.609438 2.079442 2.079442 2.197225 2.302585}
\end{Highlighting}
\end{Shaded}

\hypertarget{exponential}{%
\subsection{Exponential}\label{exponential}}

\begin{itemize}
\item
  Negatively skewed data
\item
  Underlying logarithmic trend (e.g., survival, decay)
\end{itemize}

\hypertarget{power-1}{%
\subsection{Power}\label{power-1}}

\begin{itemize}
\tightlist
\item
  Variables have negatively skewed distribution
\end{itemize}

\hypertarget{inversereciprocal}{%
\subsection{Inverse/Reciprocal}\label{inversereciprocal}}

\begin{itemize}
\item
  Variables have platykurtic distribution
\item
  Data are positively skewed
\item
  Ratio data
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(cars)}
\FunctionTok{head}\NormalTok{(cars}\SpecialCharTok{$}\NormalTok{dist)}
\CommentTok{\#\textgreater{} [1]  2 10  4 22 16 10}
\FunctionTok{plot}\NormalTok{(cars}\SpecialCharTok{$}\NormalTok{dist)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-2-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(}\DecValTok{1}\SpecialCharTok{/}\NormalTok{(cars}\SpecialCharTok{$}\NormalTok{dist))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-2-2} \end{center}

\hypertarget{hyperbolic-arcsine}{%
\subsection{Hyperbolic arcsine}\label{hyperbolic-arcsine}}

\begin{itemize}
\tightlist
\item
  Variables with positively skewed distribution
\end{itemize}

\hypertarget{ordered-quantile-norm}{%
\subsection{Ordered Quantile Norm}\label{ordered-quantile-norm}}

\begin{itemize}
\tightlist
\item
  \citep{bartlett1947use}
\end{itemize}

\[
x_i' = \Phi^{-1} (\frac{rank(x_i) - 1/2}{length(x)})
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ord\_dist }\OtherTok{\textless{}{-}}\NormalTok{ bestNormalize}\SpecialCharTok{::}\FunctionTok{orderNorm}\NormalTok{(cars}\SpecialCharTok{$}\NormalTok{dist)}
\NormalTok{ord\_dist}
\CommentTok{\#\textgreater{} orderNorm Transformation with 50 nonmissing obs and ties}
\CommentTok{\#\textgreater{}  {-} 35 unique values }
\CommentTok{\#\textgreater{}  {-} Original quantiles:}
\CommentTok{\#\textgreater{}   0\%  25\%  50\%  75\% 100\% }
\CommentTok{\#\textgreater{}    2   26   36   56  120}
\NormalTok{ord\_dist}\SpecialCharTok{$}\NormalTok{x.t }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{hist}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-3-1} \end{center}

\hypertarget{arcsinh}{%
\subsection{Arcsinh}\label{arcsinh}}

\begin{itemize}
\tightlist
\item
  Proportion variable (0-1)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cars}\SpecialCharTok{$}\NormalTok{dist }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{hist}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-4-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cars$dist \%\textgreater{}\% MASS::truehist()}

\NormalTok{as\_dist }\OtherTok{\textless{}{-}}\NormalTok{ bestNormalize}\SpecialCharTok{::}\FunctionTok{arcsinh\_x}\NormalTok{(cars}\SpecialCharTok{$}\NormalTok{dist)}
\NormalTok{as\_dist}
\CommentTok{\#\textgreater{} Standardized asinh(x) Transformation with 50 nonmissing obs.:}
\CommentTok{\#\textgreater{}  Relevant statistics:}
\CommentTok{\#\textgreater{}  {-} mean (before standardization) = 4.230843 }
\CommentTok{\#\textgreater{}  {-} sd (before standardization) = 0.7710887}
\NormalTok{as\_dist}\SpecialCharTok{$}\NormalTok{x.t }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{hist}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-4-2} \end{center}

\[
arcsinh(Y) = \log(\sqrt{1 + Y^2} + Y)
\]

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Paper & Interpretation \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\citet{azoulay2019does} & Elasticity \\
\citet{faber2019tourism} & Percentage \\
\citet{hjort2019arrival} & Percentage \\
\citet{johnson2020regulation} & Percentage \\
\citet{beerli2021abolition} & Percentage \\
\citet{norris2021effects} & Percentage \\
\citet{berkouwer2022credit} & Percentage \\
\citet{cabral2022demand} & Elasticity \\
\citet{carranza2022job} & Percentage \\
\citet{mirenda2022economic} & Percentage \\
\end{longtable}

For a simple regression model, \(Y = \beta X\)

When both \(Y\) and \(X\) are transformed, the coefficient estimate represents elasticity, indicating the percentage change in \(Y\) for a 1\% change in \(X\).

When only \(Y\) is in transformed and \(X\) is in raw form, the coefficient estimate represents the percentage change in \(Y\) for a one-unit change in \(X\).

\hypertarget{lambert-w-x-f-transformation}{%
\subsection{Lambert W x F Transformation}\label{lambert-w-x-f-transformation}}

\texttt{LambertW} package

\begin{itemize}
\item
  Using moments to normalize data.
\item
  Usually need to compare with the \protect\hyperlink{box-cox-transformation}{Box-Cox Transformation} and \protect\hyperlink{yeo-johnson-transformation}{Yeo-Johnson Transformation}
\item
  Can handle skewness, heavy-tailed.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(cars)}
\FunctionTok{head}\NormalTok{(cars}\SpecialCharTok{$}\NormalTok{dist)}
\CommentTok{\#\textgreater{} [1]  2 10  4 22 16 10}
\NormalTok{cars}\SpecialCharTok{$}\NormalTok{dist }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{hist}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-5-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]


\NormalTok{l\_dist }\OtherTok{\textless{}{-}}\NormalTok{ LambertW}\SpecialCharTok{::}\FunctionTok{Gaussianize}\NormalTok{(cars}\SpecialCharTok{$}\NormalTok{dist)}
\CommentTok{\# small fix}
\NormalTok{l\_dist }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{hist}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-5-2} \end{center}

\hypertarget{inverse-hyperbolic-sine-ihs-transformation}{%
\subsection{Inverse Hyperbolic Sine (IHS) transformation}\label{inverse-hyperbolic-sine-ihs-transformation}}

\begin{itemize}
\item
  Proposed by \citep{johnson1949}
\item
  Can be applied to real numbers.
\end{itemize}

\[
\begin{aligned}
f(x,\theta) &= \frac{\sinh^{-1} (\theta x)}{\theta} \\
&= \frac{\log(\theta x + (\theta^2 x^2 + 1)^{1/2})}{\theta}
\end{aligned}
\]

\hypertarget{box-cox-transformation}{%
\subsection{Box-Cox Transformation}\label{box-cox-transformation}}

\[
y^\lambda = \beta x+ \epsilon
\]

to fix non-linearity in the error terms

work well between (-3,3) (i.e., small transformation).

or with independent variables

\[
x_i'^\lambda = 
\begin{cases}
\frac{x_i^\lambda-1}{\lambda} & \text{if } \lambda \neq 0\\
\log(x_i) & \text{if } \lambda = 0
\end{cases}
\]

And the two-parameter version is

\[
x_i' (\lambda_1, \lambda_2) = 
\begin{cases}
\frac{(x_i + \lambda_2)^{\lambda_1}-1}{} & \text{if } \lambda_1 \neq 0 \\
\log(x_i + \lambda_2) & \text{if } \lambda_1 = 0
\end{cases}
\]

More advances

\begin{itemize}
\item
  \citep{manly1976exponential}
\item
  \citep{bickel1981analysis, box1981analysis}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(MASS)}
\FunctionTok{data}\NormalTok{(cars)}
\NormalTok{mod }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(cars}\SpecialCharTok{$}\NormalTok{speed }\SpecialCharTok{\textasciitilde{}}\NormalTok{ cars}\SpecialCharTok{$}\NormalTok{dist, }\AttributeTok{data =}\NormalTok{ cars)}
\CommentTok{\# check residuals}
\FunctionTok{plot}\NormalTok{(mod)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-6-1} \end{center}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-6-2} \end{center}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-6-3} \end{center}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-6-4} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{bc }\OtherTok{\textless{}{-}} \FunctionTok{boxcox}\NormalTok{(mod, }\AttributeTok{lambda =} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-6-5} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# best lambda}
\NormalTok{bc}\SpecialCharTok{$}\NormalTok{x[}\FunctionTok{which}\NormalTok{(bc}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{==} \FunctionTok{max}\NormalTok{(bc}\SpecialCharTok{$}\NormalTok{y))]}
\CommentTok{\#\textgreater{} [1] 1.242424}

\CommentTok{\# model with best lambda}
\NormalTok{mod\_lambda }\OtherTok{=} \FunctionTok{lm}\NormalTok{(cars}\SpecialCharTok{$}\NormalTok{speed }\SpecialCharTok{\^{}}\NormalTok{ (bc}\SpecialCharTok{$}\NormalTok{x[}\FunctionTok{which}\NormalTok{(bc}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{==} \FunctionTok{max}\NormalTok{(bc}\SpecialCharTok{$}\NormalTok{y))]) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ cars}\SpecialCharTok{$}\NormalTok{dist, }
                \AttributeTok{data =}\NormalTok{ cars)}
\FunctionTok{plot}\NormalTok{(mod\_lambda)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-6-6} \end{center}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-6-7} \end{center}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-6-8} \end{center}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-6-9} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# 2{-}parameter version}
\NormalTok{two\_bc }\OtherTok{=}\NormalTok{ geoR}\SpecialCharTok{::}\FunctionTok{boxcoxfit}\NormalTok{(cars}\SpecialCharTok{$}\NormalTok{speed)}
\NormalTok{two\_bc}
\CommentTok{\#\textgreater{} Fitted parameters:}
\CommentTok{\#\textgreater{}    lambda      beta   sigmasq }
\CommentTok{\#\textgreater{}  1.028798 15.253008 31.935297 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Convergence code returned by optim: 0}
\FunctionTok{plot}\NormalTok{(two\_bc)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-6-10} \end{center}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-6-11} \end{center}

\begin{Shaded}
\begin{Highlighting}[]


\CommentTok{\# bestNormalize}
\NormalTok{bc\_dist }\OtherTok{\textless{}{-}}\NormalTok{ bestNormalize}\SpecialCharTok{::}\FunctionTok{boxcox}\NormalTok{(cars}\SpecialCharTok{$}\NormalTok{dist)}
\NormalTok{bc\_dist}
\CommentTok{\#\textgreater{} Standardized Box Cox Transformation with 50 nonmissing obs.:}
\CommentTok{\#\textgreater{}  Estimated statistics:}
\CommentTok{\#\textgreater{}  {-} lambda = 0.4950628 }
\CommentTok{\#\textgreater{}  {-} mean (before standardization) = 10.35636 }
\CommentTok{\#\textgreater{}  {-} sd (before standardization) = 3.978036}
\NormalTok{bc\_dist}\SpecialCharTok{$}\NormalTok{x.t }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{hist}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-6-12} \end{center}

\hypertarget{yeo-johnson-transformation}{%
\subsection{Yeo-Johnson Transformation}\label{yeo-johnson-transformation}}

Similar to \protect\hyperlink{box-cox-transformation}{Box-Cox Transformation} (when \(\lambda = 1\)), but allows for negative value

\[
x_i'^\lambda = 
\begin{cases}
\frac{(x_i+1)^\lambda -1}{\lambda} & \text{if } \lambda \neq0, x_i \ge 0 \\
\log(x_i + 1) & \text{if } \lambda = 0, x_i \ge 0 \\
\frac{-[(-x_i+1)^{2-\lambda}-1]}{2 - \lambda} & \text{if } \lambda \neq 2, x_i <0 \\
-\log(-x_i + 1) & \text{if } \lambda = 2, x_i <0 
\end{cases}
\]

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(cars)}
\NormalTok{yj\_speed }\OtherTok{\textless{}{-}}\NormalTok{ bestNormalize}\SpecialCharTok{::}\FunctionTok{yeojohnson}\NormalTok{(cars}\SpecialCharTok{$}\NormalTok{speed)}
\NormalTok{yj\_speed}\SpecialCharTok{$}\NormalTok{x.t }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{hist}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-7-1} \end{center}

\hypertarget{rankgauss}{%
\subsection{RankGauss}\label{rankgauss}}

\begin{itemize}
\tightlist
\item
  Turn values into ranks, then ranks to values under normal distribution.
\end{itemize}

\hypertarget{summary-3}{%
\subsection{Summary}\label{summary-3}}

Automatically choose the best method to normalize data (\href{https://cran.r-project.org/web/packages/bestNormalize/vignettes/bestNormalize.html}{code} by \texttt{bestNormalize})

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bestdist }\OtherTok{\textless{}{-}}\NormalTok{ bestNormalize}\SpecialCharTok{::}\FunctionTok{bestNormalize}\NormalTok{(cars}\SpecialCharTok{$}\NormalTok{dist)}
\NormalTok{bestdist}\SpecialCharTok{$}\NormalTok{x.t }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{hist}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-8-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\FunctionTok{boxplot}\NormalTok{(}\FunctionTok{log10}\NormalTok{(bestdist}\SpecialCharTok{$}\NormalTok{oos\_preds), }\AttributeTok{yaxt =} \StringTok{"n"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-8-2} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# axis(2, at = log10(c(.1, .5, 1, 2, 5, 10)), }
\CommentTok{\#      labels = c(.1, .5, 1, 2, 5, 10))}
\end{Highlighting}
\end{Shaded}

\hypertarget{categorical-variables}{%
\section{Categorical Variables}\label{categorical-variables}}

Purposes

\begin{itemize}
\tightlist
\item
  To transform to continuous variable (for machine learning models) (e.g., encoding/ embedding in text mining)
\end{itemize}

Approaches:

\begin{itemize}
\item
  One-hot encoding
\item
  Label encoding
\item
  Feature hashing
\item
  Binary encoding
\item
  Base N encoding
\item
  Frequency encoding
\item
  Target encoding
\item
  Ordinal encoding
\item
  Helmert encoding
\item
  Mean encoding
\item
  Weight of evidence encoding
\item
  Probability ratio encoding
\item
  Backward difference encoding
\item
  Leave one out encoding
\item
  James-Stein encoding
\item
  M-estimator encoding
\item
  Thermometer encoding
\end{itemize}

\hypertarget{hypothesis-testing}{%
\chapter{Hypothesis Testing}\label{hypothesis-testing}}

Error types:

\begin{itemize}
\item
  Type I Error (False Positive):

  \begin{itemize}
  \tightlist
  \item
    Reality: nope
  \item
    Diagnosis/Analysis: yes
  \end{itemize}
\item
  Type II Error (False Negative):

  \begin{itemize}
  \tightlist
  \item
    Reality: yes
  \item
    Diagnosis/Analysis: nope
  \end{itemize}
\end{itemize}

Power: The probability of rejecting the null hypothesis when it is actually false

\textbf{Note:}

\begin{itemize}
\item
  Always written in terms of the population parameter (\(\beta\)) not the estimator/estimate (\(\hat{\beta}\))
\item
  Sometimes, different disciplines prefer to use \(\beta\) (i.e., standardized coefficient), or \(\mathbf{b}\) (i.e., unstandardized coefficient)

  \begin{itemize}
  \item
    \(\beta\) and \(\mathbf{b}\) are similar in interpretation; however, \(\beta\) is scale free. Hence, you can see the relative contribution of \(\beta\) to the dependent variable. On the other hand, \(\mathbf{b}\) can be more easily used in policy decisions.
  \item
    \[
    \beta_j = \mathbf{b} \frac{s_{x_j}}{s_y}
    \]
  \end{itemize}
\item
  Assuming the null hypothesis is true, what is the (asymptotic) distribution of the estimator
\item
  Two-sided
\end{itemize}

\[
\begin{aligned}
&H_0: \beta_j = 0 \\
&H_1: \beta_j \neq 0 
\end{aligned}
\]

then under the null, the OLS estimator has the following distribution

\[
A1-A3a, A5: \sqrt{n} \hat{\beta_j}  \sim  N(0,Avar(\sqrt{n}\hat{\beta}_j))
\]

\begin{itemize}
\tightlist
\item
  For the one-sided test, the null is a set of values, so now you choose the worst case single value that is hardest to prove and derive the distribution under the null
\item
  One-sided
\end{itemize}

\[
\begin{aligned}
&H_0: \beta_j\ge 0 \\
&H_1: \beta_j < 0 
\end{aligned}
\]

then the hardest null value to prove is \(H_0: \beta_j=0\). Then under this specific null, the OLS estimator has the following asymptotic distribution

\[
A1-A3a, A5: \sqrt{n}\hat{\beta_j} \sim N(0,Avar(\sqrt{n}\hat{\beta}_j))
\]

\hypertarget{types-of-hypothesis-testing}{%
\section{Types of hypothesis testing}\label{types-of-hypothesis-testing}}

\(H_0 : \theta = \theta_0\)

\(H_1 : \theta \neq \theta_0\)

How far away / extreme \(\theta\) can be if our null hypothesis is true

Assume that our likelihood function for q is \(L(q) = q^{30}(1-q)^{70}\) \textbf{Likelihood function}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{q }\OtherTok{=} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{length =} \DecValTok{100}\NormalTok{)}
\NormalTok{L }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(q) \{}
\NormalTok{    q }\SpecialCharTok{\^{}} \DecValTok{30} \SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ q) }\SpecialCharTok{\^{}} \DecValTok{70}
\NormalTok{\}}

\FunctionTok{plot}\NormalTok{(q,}
     \FunctionTok{L}\NormalTok{(q),}
     \AttributeTok{ylab =} \StringTok{"L(q)"}\NormalTok{,}
     \AttributeTok{xlab =} \StringTok{"q"}\NormalTok{,}
     \AttributeTok{type =} \StringTok{"l"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{14-hypothesis_files/figure-latex/unnamed-chunk-1-1} \end{center}

\textbf{Log-Likelihood function}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{q }\OtherTok{=} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{length =} \DecValTok{100}\NormalTok{)}
\NormalTok{l }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(q) \{}
    \DecValTok{30} \SpecialCharTok{*} \FunctionTok{log}\NormalTok{(q) }\SpecialCharTok{+} \DecValTok{70} \SpecialCharTok{*} \FunctionTok{log}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ q)}
\NormalTok{\}}
\FunctionTok{plot}\NormalTok{(q,}
     \FunctionTok{l}\NormalTok{(q) }\SpecialCharTok{{-}} \FunctionTok{l}\NormalTok{(}\FloatTok{0.3}\NormalTok{),}
     \AttributeTok{ylab =} \StringTok{"l(q) {-} l(qhat)"}\NormalTok{,}
     \AttributeTok{xlab =} \StringTok{"q"}\NormalTok{,}
     \AttributeTok{type =} \StringTok{"l"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{v =} \FloatTok{0.2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{14-hypothesis_files/figure-latex/unnamed-chunk-2-1} \end{center}

\includegraphics[width=6.25in,height=4.16667in]{images/nested_tests.jpg}

Figure from\citep{fox1997applied}

typically, \protect\hyperlink{the-likelihood-ratio-test}{The likelihood ratio test} (and \protect\hyperlink{lagrange-multiplier-score}{Lagrange Multiplier (Score)}) performs better with small to moderate sample sizes, but the \protect\hyperlink{sec-wald-test-logistic}{Wald test} only requires one maximization (under the full model).

\hypertarget{wald-test}{%
\section{Wald test}\label{wald-test}}

\[
\begin{aligned}
W &= (\hat{\theta}-\theta_0)'[cov(\hat{\theta})]^{-1}(\hat{\theta}-\theta_0) \\
W &\sim \chi_q^2
\end{aligned}
\]

where \(cov(\hat{\theta})\) is given by the inverse Fisher Information matrix evaluated at \(\hat{\theta}\) and q is the rank of \(cov(\hat{\theta})\), which is the number of non-redundant parameters in \(\theta\)

Alternatively,

\[
t_W=\frac{(\hat{\theta}-\theta_0)^2}{I(\theta_0)^{-1}} \sim \chi^2_{(v)}
\]

where v is the degree of freedom.

Equivalently,

\[
s_W= \frac{\hat{\theta}-\theta_0}{\sqrt{I(\hat{\theta})^{-1}}} \sim Z
\]

How far away in the distribution your sample estimate is from the hypothesized population parameter.

For a null value, what is the probability you would have obtained a realization ``more extreme'' or ``worse'' than the estimate you actually obtained?

Significance Level (\(\alpha\)) and Confidence Level (\(1-\alpha\))

\begin{itemize}
\tightlist
\item
  The significance level is the benchmark in which the probability is so low that we would have to reject the null
\item
  The confidence level is the probability that sets the bounds on how far away the realization of the estimator would have to be to reject the null.
\end{itemize}

\textbf{Test Statistics}

\begin{itemize}
\tightlist
\item
  Standardized (transform) the estimator and null value to a test statistic that always has the same distribution
\item
  Test Statistic for the OLS estimator for a single hypothesis
\end{itemize}

\[
T = \frac{\sqrt{n}(\hat{\beta}_j-\beta_{j0})}{\sqrt{n}SE(\hat{\beta_j})} \sim^a N(0,1)
\]

Equivalently,

\[
T = \frac{(\hat{\beta}_j-\beta_{j0})}{SE(\hat{\beta_j})} \sim^a N(0,1)
\]

the test statistic is another random variable that is a function of the data and null hypothesis.

\begin{itemize}
\tightlist
\item
  T denotes the random variable test statistic
\item
  t denotes the single realization of the test statistic
\end{itemize}

Evaluating Test Statistic: determine whether or not we reject or fail to reject the null hypothesis at a given significance / confidence level

Three equivalent ways

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Critical Value
\item
  P-value
\item
  Confidence Interval
\item
  Critical Value
\end{enumerate}

For a given significance level, will determine the critical value \((c)\)

\begin{itemize}
\tightlist
\item
  One-sided: \(H_0: \beta_j \ge \beta_{j0}\)
\end{itemize}

\[
P(T<c|H_0)=\alpha
\]

Reject the null if \(t<c\)

\begin{itemize}
\tightlist
\item
  One-sided: \(H_0: \beta_j \le \beta_{j0}\)
\end{itemize}

\[
P(T>c|H_0)=\alpha
\]

Reject the null if \(t>c\)

\begin{itemize}
\tightlist
\item
  Two-sided: \(H_0: \beta_j \neq \beta_{j0}\)
\end{itemize}

\[
P(|T|>c|H_0)=\alpha
\]

Reject the null if \(|t|>c\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  p-value
\end{enumerate}

Calculate the probability that the test statistic was worse than the realization you have

\begin{itemize}
\tightlist
\item
  One-sided: \(H_0: \beta_j \ge \beta_{j0}\)
\end{itemize}

\[
\text{p-value} = P(T<t|H_0)
\]

\begin{itemize}
\tightlist
\item
  One-sided: \(H_0: \beta_j \le \beta_{j0}\)
\end{itemize}

\[
\text{p-value} = P(T>t|H_0)
\]

\begin{itemize}
\tightlist
\item
  Two-sided: \(H_0: \beta_j \neq \beta_{j0}\)
\end{itemize}

\[
\text{p-value} = P(|T|<t|H_0)
\]

reject the null if p-value \(< \alpha\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Confidence Interval
\end{enumerate}

Using the critical value associated with a null hypothesis and significance level, create an interval

\[
CI(\hat{\beta}_j)_{\alpha} = [\hat{\beta}_j-(c \times SE(\hat{\beta}_j)),\hat{\beta}_j+(c \times SE(\hat{\beta}_j))]
\]

If the null set lies outside the interval then we reject the null.

\begin{itemize}
\tightlist
\item
  We are not testing whether the true population value is close to the estimate, we are testing that given a field true population value of the parameter, how like it is that we observed this estimate.
\item
  Can be interpreted as we believe with \((1-\alpha)\times 100 \%\) probability that the confidence interval captures the true parameter value.
\end{itemize}

With stronger assumption (A1-A6), we could consider \protect\hyperlink{finite-sample-properties}{Finite Sample Properties}

\[
T = \frac{\hat{\beta}_j-\beta_{j0}}{SE(\hat{\beta}_j)} \sim T(n-k)
\]

\begin{itemize}
\tightlist
\item
  This above distributional derivation is strongly dependent on \protect\hyperlink{a4-homoskedasticity}{A4} and \protect\hyperlink{a5-data-generation-random-sampling}{A5}
\item
  T has a student t-distribution because the numerator is normal and the denominator is \(\chi^2\).
\item
  Critical value and p-values will be calculated from the student t-distribution rather than the standard normal distribution.
\item
  \(n \to \infty\), \(T(n-k)\) is asymptotically standard normal.
\end{itemize}

\textbf{Rule of thumb}

\begin{itemize}
\item
  if \(n-k>120\): the critical values and p-values from the t-distribution are (almost) the same as the critical values and p-values from the standard normal distribution.
\item
  if \(n-k<120\)

  \begin{itemize}
  \tightlist
  \item
    if (A1-A6) hold then the t-test is an exact finite distribution test
  \item
    if (A1-A3a, A5) hold, because the t-distribution is asymptotically normal, computing the critical values from a t-distribution is still a valid asymptotic test (i.e., not quite the right critical values and p0values, the difference goes away as \(n \to \infty\))
  \end{itemize}
\end{itemize}

\hypertarget{multiple-hypothesis}{%
\subsection{Multiple Hypothesis}\label{multiple-hypothesis}}

\begin{itemize}
\item
  test multiple parameters as the same time

  \begin{itemize}
  \tightlist
  \item
    \(H_0: \beta_1 = 0\ \& \ \beta_2 = 0\)
  \item
    \(H_0: \beta_1 = 1\ \& \ \beta_2 = 0\)
  \end{itemize}
\item
  perform a series of simply hypothesis does not answer the question (joint distribution vs.~two marginal distributions).
\item
  The test statistic is based on a restriction written in matrix form.
\end{itemize}

\[
y=\beta_0+x_1\beta_1 + x_2\beta_2 + x_3\beta_3 + \epsilon
\]

Null hypothesis is \(H_0: \beta_1 = 0\) \& \(\beta_2=0\) can be rewritten as \(H_0: \mathbf{R}\beta -\mathbf{q}=0\) where

\begin{itemize}
\tightlist
\item
  \(\mathbf{R}\) is a \(m \times k\) matrix where m is the number of restrictions and \(k\) is the number of parameters. \(\mathbf{q}\) is a \(k \times 1\) vector
\item
  \(\mathbf{R}\) ``picks up'' the relevant parameters while \(\mathbf{q}\) is a the null value of the parameter
\end{itemize}

\[
\mathbf{R}= 
\left(
\begin{array}{cccc}
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
\end{array}
\right),
\mathbf{q} = 
\left(
\begin{array}{c}
0 \\
0 \\
\end{array}
\right)
\]

Test Statistic for OLS estimator for a multiple hypothesis

\[
F = \frac{(\mathbf{R\hat{\beta}-q})\hat{\Sigma}^{-1}(\mathbf{R\hat{\beta}-q})}{m} \sim^a F(m,n-k)
\]

\begin{itemize}
\item
  \(\hat{\Sigma}^{-1}\) is the estimator for the asymptotic variance-covariance matrix

  \begin{itemize}
  \tightlist
  \item
    if \protect\hyperlink{a4-homoskedasticity}{A4} holds, both the homoskedastic and heteroskedastic versions produce valid estimator
  \item
    If \protect\hyperlink{a4-homoskedasticity}{A4} does not hold, only the heteroskedastic version produces valid estimators.
  \end{itemize}
\item
  When \(m = 1\), there is only a single restriction, then the \(F\)-statistic is the \(t\)-statistic squared.
\item
  \(F\) distribution is strictly positive, check {[}F-Distribution{]} for more details.
\end{itemize}

\hypertarget{linear-combination}{%
\subsection{Linear Combination}\label{linear-combination}}

Testing multiple parameters as the same time

\[
\begin{aligned}
H_0&: \beta_1 -\beta_2 = 0 \\
H_0&: \beta_1 - \beta_2 > 0 \\
H_0&: \beta_1 - 2\times\beta_2 =0
\end{aligned}
\]

Each is a single restriction on a function of the parameters.

Null hypothesis:

\[
H_0: \beta_1 -\beta_2 = 0
\]

can be rewritten as

\[
H_0: \mathbf{R}\beta -\mathbf{q}=0
\]

where \(\mathbf{R}\)=(0 1 -1 0 0) and \(\mathbf{q}=0\)

\hypertarget{estimate-difference-in-coefficients}{%
\subsection{Estimate Difference in Coefficients}\label{estimate-difference-in-coefficients}}

There is no package to estimate for the difference between two coefficients and its CI, but a simple function created by \href{https://kzee.github.io/CoeffDiff_Demo.html}{Katherine Zee} can be used to calculate this difference. Some modifications might be needed if you don't use standard \texttt{lm} model in R.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{difftest\_lm }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x1, x2, model) \{}
\NormalTok{    diffest }\OtherTok{\textless{}{-}}
        \FunctionTok{summary}\NormalTok{(model)}\SpecialCharTok{$}\NormalTok{coef[x1, }\StringTok{"Estimate"}\NormalTok{] }\SpecialCharTok{{-}} \FunctionTok{summary}\NormalTok{(model)}\SpecialCharTok{$}\NormalTok{coef[x2, }\StringTok{"Estimate"}\NormalTok{]}
    
\NormalTok{    vardiff }\OtherTok{\textless{}{-}}\NormalTok{ (}\FunctionTok{summary}\NormalTok{(model)}\SpecialCharTok{$}\NormalTok{coef[x1, }\StringTok{"Std. Error"}\NormalTok{] }\SpecialCharTok{\^{}} \DecValTok{2} \SpecialCharTok{+}
                    \FunctionTok{summary}\NormalTok{(model)}\SpecialCharTok{$}\NormalTok{coef[x2, }\StringTok{"Std. Error"}\NormalTok{] }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{) }\SpecialCharTok{{-}}\NormalTok{ (}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ (}\FunctionTok{vcov}\NormalTok{(model)[x1, x2]))}
    \CommentTok{\# variance of x1 + variance of x2 {-} 2*covariance of x1 and x2}
\NormalTok{    diffse }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(vardiff)}
\NormalTok{    tdiff }\OtherTok{\textless{}{-}}\NormalTok{ (diffest) }\SpecialCharTok{/}\NormalTok{ (diffse)}
\NormalTok{    ptdiff }\OtherTok{\textless{}{-}} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{pt}\NormalTok{(}\FunctionTok{abs}\NormalTok{(tdiff), model}\SpecialCharTok{$}\NormalTok{df, }\AttributeTok{lower.tail =}\NormalTok{ T))}
\NormalTok{    upr }\OtherTok{\textless{}{-}}
        \CommentTok{\# will usually be very close to 1.96}
\NormalTok{        diffest }\SpecialCharTok{+} \FunctionTok{qt}\NormalTok{(.}\DecValTok{975}\NormalTok{, }\AttributeTok{df =}\NormalTok{ model}\SpecialCharTok{$}\NormalTok{df) }\SpecialCharTok{*}\NormalTok{ diffse }
\NormalTok{    lwr }\OtherTok{\textless{}{-}}\NormalTok{ diffest }\SpecialCharTok{+} \FunctionTok{qt}\NormalTok{(.}\DecValTok{025}\NormalTok{, }\AttributeTok{df =}\NormalTok{ model}\SpecialCharTok{$}\NormalTok{df) }\SpecialCharTok{*}\NormalTok{ diffse}
\NormalTok{    df }\OtherTok{\textless{}{-}}\NormalTok{ model}\SpecialCharTok{$}\NormalTok{df}
    \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}
        \AttributeTok{est =} \FunctionTok{round}\NormalTok{(diffest, }\AttributeTok{digits =} \DecValTok{2}\NormalTok{),}
        \AttributeTok{t =} \FunctionTok{round}\NormalTok{(tdiff, }\AttributeTok{digits =} \DecValTok{2}\NormalTok{),}
        \AttributeTok{p =} \FunctionTok{round}\NormalTok{(ptdiff, }\AttributeTok{digits =} \DecValTok{4}\NormalTok{),}
        \AttributeTok{lwr =} \FunctionTok{round}\NormalTok{(lwr, }\AttributeTok{digits =} \DecValTok{2}\NormalTok{),}
        \AttributeTok{upr =} \FunctionTok{round}\NormalTok{(upr, }\AttributeTok{digits =} \DecValTok{2}\NormalTok{),}
        \AttributeTok{df =}\NormalTok{ df}
\NormalTok{    ))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{application-2}{%
\subsection{Application}\label{application-2}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"car"}\NormalTok{)}

\CommentTok{\# Multiple hypothesis}
\NormalTok{mod.davis }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(weight }\SpecialCharTok{\textasciitilde{}}\NormalTok{ repwt, }\AttributeTok{data=}\NormalTok{Davis)}
\FunctionTok{linearHypothesis}\NormalTok{(mod.davis, }\FunctionTok{c}\NormalTok{(}\StringTok{"(Intercept) = 0"}\NormalTok{, }\StringTok{"repwt = 1"}\NormalTok{),}\AttributeTok{white.adjust =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\#\textgreater{} Linear hypothesis test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Hypothesis:}
\CommentTok{\#\textgreater{} (Intercept) = 0}
\CommentTok{\#\textgreater{} repwt = 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Model 1: restricted model}
\CommentTok{\#\textgreater{} Model 2: weight \textasciitilde{} repwt}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Note: Coefficient covariance matrix supplied.}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   Res.Df Df      F  Pr(\textgreater{}F)  }
\CommentTok{\#\textgreater{} 1    183                    }
\CommentTok{\#\textgreater{} 2    181  2 3.3896 0.03588 *}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}

\CommentTok{\# Linear Combination}
\NormalTok{mod.duncan }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(prestige }\SpecialCharTok{\textasciitilde{}}\NormalTok{ income }\SpecialCharTok{+}\NormalTok{ education, }\AttributeTok{data=}\NormalTok{Duncan)}
\FunctionTok{linearHypothesis}\NormalTok{(mod.duncan, }\StringTok{"1*income {-} 1*education = 0"}\NormalTok{)}
\CommentTok{\#\textgreater{} Linear hypothesis test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Hypothesis:}
\CommentTok{\#\textgreater{} income {-} education = 0}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Model 1: restricted model}
\CommentTok{\#\textgreater{} Model 2: prestige \textasciitilde{} income + education}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   Res.Df    RSS Df Sum of Sq      F Pr(\textgreater{}F)}
\CommentTok{\#\textgreater{} 1     43 7518.9                           }
\CommentTok{\#\textgreater{} 2     42 7506.7  1    12.195 0.0682 0.7952}
\end{Highlighting}
\end{Shaded}

\hypertarget{nonlinear}{%
\subsection{Nonlinear}\label{nonlinear}}

Suppose that we have q nonlinear functions of the parameters\\
\[
\mathbf{h}(\theta) = \{ h_1 (\theta), ..., h_q (\theta)\}'
\]

The,n, the Jacobian matrix (\(\mathbf{H}(\theta)\)), of rank q is

\[
\mathbf{H}_{q \times p}(\theta) = 
\left(
\begin{array}
{ccc}
\frac{\partial h_1(\theta)}{\partial \theta_1} & ... & \frac{\partial h_1(\theta)}{\partial \theta_p} \\
. & . & . \\
\frac{\partial h_q(\theta)}{\partial \theta_1} & ... & \frac{\partial h_q(\theta)}{\partial \theta_p}
\end{array}
\right)
\]

where the null hypothesis \(H_0: \mathbf{h} (\theta) = 0\) can be tested against the 2-sided alternative with the Wald statistic

\[
W = \frac{\mathbf{h(\hat{\theta})'\{H(\hat{\theta})[F(\hat{\theta})'F(\hat{\theta})]^{-1}H(\hat{\theta})'\}^{-1}h(\hat{\theta})}}{s^2q} \sim F_{q,n-p}
\]

\hypertarget{the-likelihood-ratio-test}{%
\section{The likelihood ratio test}\label{the-likelihood-ratio-test}}

\[
t_{LR} = 2[l(\hat{\theta})-l(\theta_0)] \sim \chi^2_v
\]

where v is the degree of freedom.

Compare the height of the log-likelihood of the sample estimate in relation to the height of log-likelihood of the hypothesized population parameter

Alternatively,

This test considers a ratio of two maximizations,

\[
\begin{aligned}
L_r &= \text{maximized value of the likelihood under $H_0$ (the reduced model)} \\
L_f &= \text{maximized value of the likelihood under $H_0 \cup H_a$ (the full model)}
\end{aligned}
\]

Then, the likelihood ratio is:

\[
\Lambda = \frac{L_r}{L_f}
\]

which can't exceed 1 (since \(L_f\) is always at least as large as \(L-r\) because \(L_r\) is the result of a maximization under a restricted set of the parameter values).

The likelihood ratio statistic is:

\[
\begin{aligned}
-2ln(\Lambda) &= -2ln(L_r/L_f) = -2(l_r - l_f) \\
\lim_{n \to \infty}(-2ln(\Lambda)) &\sim \chi^2_v
\end{aligned}
\]

where \(v\) is the number of parameters in the full model minus the number of parameters in the reduced model.

If \(L_r\) is much smaller than \(L_f\) (the likelihood ratio exceeds \(\chi_{\alpha,v}^2\)), then we reject he reduced model and accept the full model at \(\alpha \times 100 \%\) significance level

\hypertarget{lagrange-multiplier-score}{%
\section{Lagrange Multiplier (Score)}\label{lagrange-multiplier-score}}

\[
t_S= \frac{S(\theta_0)^2}{I(\theta_0)} \sim \chi^2_v
\]

where \(v\) is the degree of freedom.

Compare the slope of the log-likelihood of the sample estimate in relation to the slope of the log-likelihood of the hypothesized population parameter

\hypertarget{two-one-sided-tests-tost-equivalence-testing}{%
\section{Two One-Sided Tests (TOST) Equivalence Testing}\label{two-one-sided-tests-tost-equivalence-testing}}

This is a good way to test whether your population effect size is within a range of practical interest (e.g., if the effect size is 0).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(TOSTER)}
\end{Highlighting}
\end{Shaded}

\hypertarget{marginal-effects}{%
\chapter{Marginal Effects}\label{marginal-effects}}

In cases without polynomials or interactions, it can be easy to interpret the marginal effect.

For example,

\[
Y = \beta_1 X_1 + \beta_2 X_2
\]

where \(\beta\) are the marginal effects.

Numerical derivation is easier than analytical derivation.

\begin{itemize}
\tightlist
\item
  We need to choose values for all the variables to calculate the marginal effect of \(X\)
\end{itemize}

Analytical derivation

\[
f'(x) \equiv \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}
\]

E.g., \(f(x) = X^2\)

\[
\begin{aligned}
f'(x) &= \lim_{h \to 0} \frac{(x+h)^2 - x^2}{h} \\
&= \frac{x^2 + 2xh + h^2 - x^2}{h} \\
&= \frac{2xh + h^2}{h} \\
&= 2x + h \\
&= 2x
\end{aligned}
\]

For numerically approach, we ``just'' need to find a small \(h\) to plug in our function. However, you also need a large enough \(h\) to have numerically accurate computation \citep[chapter 1]{gould2010maximum}

Numerically approach

One-sided derivative

\[
\begin{aligned}
f'(x) &= \lim_{h \to 0} \frac{(x+h)^2 - x^2}{h}  \\
& \approx \frac{f(x+h) -f(x)}{h}
\end{aligned}
\]

Alternatively, two-sided derivative

\[
f'_2(x) \approx \frac{f(x+h) - f(x- h)}{2h}
\]

Marginal effects for

\begin{itemize}
\item
  discrete variables (also known as incremental effects) are the change in \(E[Y|X]\) for a one unit change in \(X\)
\item
  continuous variables are the change in \(E[Y|X]\) for very small changes in \(X\) (not unit changes), because it's a derivative, which is a limit when \(h \to 0\)
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2329}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2329}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5342}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Analytical derivation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Numerical derivation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Marginal Effects & Rules of expectations & Approximate analytical solution \\
Standard Errors & Rules of variances & Delta method using the asymptotic errors (vcov matrix) \\
\end{longtable}

\hypertarget{delta-method}{%
\section{Delta Method}\label{delta-method}}

\begin{itemize}
\tightlist
\item
  approximate the mean and variance of a function of random variables using a first-order Taylor approximation
\item
  A semi-parametric method
\item
  Alternative approaches:

  \begin{itemize}
  \item
    Analytically derive a probability function for the margin
  \item
    Simulation/Bootstrapping
  \end{itemize}
\item
  Resources:

  \begin{itemize}
  \item
    Advanced: \href{https://cran.r-project.org/web/packages/modmarg/vignettes/delta-method.html}{modmarg}
  \item
    Intermediate: \href{https://stats.oarc.ucla.edu/r/faq/how-can-i-estimate-the-standard-error-of-transformed-regression-parameters-in-r-using-the-delta-method/}{UCLA stat}
  \item
    Simple: \href{https://www.alexstephenson.me/post/2022-04-02-standard-errors-and-the-delta-method/}{Another one}
  \end{itemize}
\end{itemize}

Let \(G(\beta)\) be a function of the parameters \(\beta\), then

\[
var(G(\beta)) \approx \nabla G(\beta) cov (\beta) \nabla G(\beta)'
\]

where

\begin{itemize}
\tightlist
\item
  \(\nabla G(\beta)\) = the gradient of the partial derivatives of \(G(\beta)\) (also known as the Jacobian)
\end{itemize}

\hypertarget{average-marginal-effect-algorithm}{%
\section{Average Marginal Effect Algorithm}\label{average-marginal-effect-algorithm}}

For one-sided derivative \(\frac{\partial p(\mathbf{X},\beta)}{\partial X}\) in the probability scale

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Estimate your model
\item
  For each observation \(i\)

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    Calculate \(\hat{Y}_{i0}\) which is the prediction in the probability scale using observed values
  \item
    Increase \(X\) (variable of interest) by a ``small'' amount \(h\) (\(X_{new} = X + h\))

    \begin{itemize}
    \item
      When \(X\) is continuous, \(h = (|\bar{X}| + 0.001) \times 0.001\) where \(\bar{X}\) is the mean value of \(X\)
    \item
      When \(X\) is discrete, \(h = 1\)
    \end{itemize}
  \item
    Calculate \(\hat{Y}_{i1}\) which is the prediction in the probability scale using new \(X\) and other variables' observed vales.
  \item
    Calculate the difference between the two predictions as fraction of \(h\): \(\frac{\bar{Y}_{i1} - \bar{Y}_{i0}}{h}\)
  \end{enumerate}
\item
  Average numerical derivative is \(E[\frac{\bar{Y}_{i1} - \bar{Y}_{i0}}{h}] \approx \frac{\partial p (Y|\mathbf{X}, \beta)}{\partial X}\)
\end{enumerate}

Two-sided derivatives

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Estimate your model
\item
  For each observation \(i\)

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    Calculate \(\hat{Y}_{i0}\) which is the prediction in the probability scale using observed values
  \item
    Increase \(X\) (variable of interest) by a ``small'' amount \(h\) (\(X_{1} = X + h\)) and decrease \(X\) (variable of interest) by a ``small'' amount \(h\) (\(X_{2} = X - h\))

    \begin{itemize}
    \item
      When \(X\) is continuous, \(h = (|\bar{X}| + 0.001) \times 0.001\) where \(\bar{X}\) is the mean value of \(X\)
    \item
      When \(X\) is discrete, \(h = 1\)
    \end{itemize}
  \item
    Calculate \(\hat{Y}_{i1}\) which is the prediction in the probability scale using new \(X_1\) and other variables' observed vales.
  \item
    Calculate \(\hat{Y}_{i2}\) which is the prediction in the probability scale using new \(X_2\) and other variables' observed vales.
  \item
    Calculate the difference between the two predictions as fraction of \(h\): \(\frac{\bar{Y}_{i1} - \bar{Y}_{i2}}{2h}\)
  \end{enumerate}
\item
  Average numerical derivative is \(E[\frac{\bar{Y}_{i1} - \bar{Y}_{i2}}{2h}] \approx \frac{\partial p (Y|\mathbf{X}, \beta)}{\partial X}\)
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(margins)}
\FunctionTok{library}\NormalTok{(tidyverse)}

\FunctionTok{data}\NormalTok{(}\StringTok{"mtcars"}\NormalTok{)}
\NormalTok{mod }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ cyl }\SpecialCharTok{*}\NormalTok{ disp }\SpecialCharTok{*}\NormalTok{ hp, }\AttributeTok{data =}\NormalTok{ mtcars)}
\NormalTok{margins}\SpecialCharTok{::}\FunctionTok{margins}\NormalTok{(mod) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{summary}\NormalTok{()}
\CommentTok{\#\textgreater{}  factor     AME     SE       z      p    lower   upper}
\CommentTok{\#\textgreater{}     cyl {-}4.0592 3.7614 {-}1.0792 0.2805 {-}11.4313  3.3130}
\CommentTok{\#\textgreater{}    disp {-}0.0350 0.0132 {-}2.6473 0.0081  {-}0.0610 {-}0.0091}
\CommentTok{\#\textgreater{}      hp {-}0.0284 0.0185 {-}1.5348 0.1248  {-}0.0647  0.0079}

\CommentTok{\# function for variable}
\NormalTok{get\_mae }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(mod, }\AttributeTok{var =} \StringTok{"disp"}\NormalTok{) \{}
\NormalTok{    data }\OtherTok{=}\NormalTok{ mod}\SpecialCharTok{$}\NormalTok{model}
    
\NormalTok{    pred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(mod)}
    
    \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{class}\NormalTok{(mod}\SpecialCharTok{$}\NormalTok{model[[\{}
\NormalTok{        \{}
\NormalTok{            var}
\NormalTok{        \}}
\NormalTok{    \}]]) }\SpecialCharTok{==} \StringTok{"numeric"}\NormalTok{) \{}
\NormalTok{        h }\OtherTok{=}\NormalTok{ (}\FunctionTok{abs}\NormalTok{(}\FunctionTok{mean}\NormalTok{(mod}\SpecialCharTok{$}\NormalTok{model[[var]])) }\SpecialCharTok{+} \FloatTok{0.01}\NormalTok{) }\SpecialCharTok{*} \FloatTok{0.01}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{        h }\OtherTok{=} \DecValTok{1}
\NormalTok{    \}}
    
\NormalTok{    data[[\{}
\NormalTok{        \{}
\NormalTok{            var}
\NormalTok{        \}}
\NormalTok{    \}]] }\OtherTok{\textless{}{-}}\NormalTok{ data[[\{}
\NormalTok{\{}
\NormalTok{var}
\NormalTok{\}}
\NormalTok{\}]] }\SpecialCharTok{{-}}\NormalTok{ h}

\NormalTok{    pred\_new }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(mod, }\AttributeTok{newdata =}\NormalTok{ data)}

    \FunctionTok{mean}\NormalTok{(pred }\SpecialCharTok{{-}}\NormalTok{ pred\_new) }\SpecialCharTok{/}\NormalTok{ h}
\NormalTok{\}}

\FunctionTok{get\_mae}\NormalTok{(mod, }\AttributeTok{var =} \StringTok{"disp"}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] {-}0.03504546}
\end{Highlighting}
\end{Shaded}

\hypertarget{packages}{%
\section{Packages}\label{packages}}

\hypertarget{marginaleffects}{%
\subsection{MarginalEffects}\label{marginaleffects}}

\texttt{MarginalEffects} package is a successor of \texttt{margins} and \texttt{emtrends} (faster, more efficient, more adaptable). Hence, this is advocated to be used.

\begin{itemize}
\tightlist
\item
  A limitation is that there is no readily function to correct for multiple comparisons. Hence, one can use the \texttt{p.adjust} function to overcome this disadvantage.
\end{itemize}

Definitions from the package:

\begin{itemize}
\item
  \textbf{Marginal effects} are slopes or derivatives (i.e., effect of changes in a variable on the outcome)

  \begin{itemize}
  \tightlist
  \item
    \texttt{margins} package defines marginal effects as ``partial derivatives of the regression equation with respect to each variable in the model for each unit in the data.''
  \end{itemize}
\item
  \textbf{Marginal means} are averages or integrals (i.e., marginalizing across rows of a prediction grid)
\end{itemize}

To customize your plot using \texttt{plot\_cme} (which is a \texttt{ggplot} class), you can check this \href{https://stackoverflow.com/questions/72463092/estimate-marginal-effect-in-triple-interaction}{post} by the author of the \texttt{MarginalEffects} package

Causal inference with the parametric g-formula

\begin{itemize}
\tightlist
\item
  Because the plug-in g estimator is equivalent to the average contrast in the \texttt{marginaleffects} package.
\end{itemize}

To get predicted values

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(marginaleffects)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{data}\NormalTok{(mtcars)}

\NormalTok{mod }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ hp }\SpecialCharTok{*}\NormalTok{ wt }\SpecialCharTok{*}\NormalTok{ am, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{predictions}\NormalTok{(mod) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Estimate Std. Error    z Pr(\textgreater{}|z|)     S 2.5 \% 97.5 \%}
\CommentTok{\#\textgreater{}      22.5      0.884 25.4   \textless{}0.001 471.7  20.8   24.2}
\CommentTok{\#\textgreater{}      20.8      1.194 17.4   \textless{}0.001 223.3  18.5   23.1}
\CommentTok{\#\textgreater{}      25.3      0.709 35.7   \textless{}0.001 922.7  23.9   26.7}
\CommentTok{\#\textgreater{}      20.3      0.704 28.8   \textless{}0.001 601.5  18.9   21.6}
\CommentTok{\#\textgreater{}      17.0      0.712 23.9   \textless{}0.001 416.2  15.6   18.4}
\CommentTok{\#\textgreater{}      19.7      0.875 22.5   \textless{}0.001 368.8  17.9   21.4}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, wt, am}
\CommentTok{\# for specific reference values}
\FunctionTok{predictions}\NormalTok{(mod, }\AttributeTok{newdata =} \FunctionTok{datagrid}\NormalTok{(}\AttributeTok{am =} \DecValTok{0}\NormalTok{, }\AttributeTok{wt =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{)))}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  am wt Estimate Std. Error    z Pr(\textgreater{}|z|)     S 2.5 \% 97.5 \%  hp}
\CommentTok{\#\textgreater{}   0  2     22.0       2.04 10.8   \textless{}0.001  87.4  18.0   26.0 147}
\CommentTok{\#\textgreater{}   0  4     16.6       1.08 15.3   \textless{}0.001 173.8  14.5   18.7 147}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, am, wt}
\FunctionTok{plot\_cap}\NormalTok{(mod, }\AttributeTok{condition =} \FunctionTok{c}\NormalTok{(}\StringTok{"hp"}\NormalTok{,}\StringTok{"wt"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{15-marginal-effect_files/figure-latex/unnamed-chunk-2-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Average Margianl Effects}
\NormalTok{mfx }\OtherTok{\textless{}{-}} \FunctionTok{marginaleffects}\NormalTok{(mod, }\AttributeTok{variables =} \FunctionTok{c}\NormalTok{(}\StringTok{"hp"}\NormalTok{, }\StringTok{"wt"}\NormalTok{))}
\FunctionTok{summary}\NormalTok{(mfx)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Term    Contrast Estimate Std. Error     z Pr(\textgreater{}|z|)   2.5 \% 97.5 \%}
\CommentTok{\#\textgreater{}    hp mean(dY/dX)  {-}0.0381     0.0128 {-}2.98  0.00291 {-}0.0631 {-}0.013}
\CommentTok{\#\textgreater{}    wt mean(dY/dX)  {-}3.9391     1.0858 {-}3.63  \textless{} 0.001 {-}6.0672 {-}1.811}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Columns: term, contrast, estimate, std.error, statistic, p.value, conf.low, conf.high}

\CommentTok{\# Group{-}Average Marginal Effects}
\NormalTok{marginaleffects}\SpecialCharTok{::}\FunctionTok{marginaleffects}\NormalTok{(mod, }\AttributeTok{by =} \StringTok{"hp"}\NormalTok{, }\AttributeTok{variables =} \StringTok{"am"}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Term          Contrast  hp Estimate Std. Error      z Pr(\textgreater{}|z|)   S  2.5 \%}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0)  52    3.976       5.20  0.764    0.445 1.2  {-}6.22}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0)  62   {-}2.774       2.51 {-}1.107    0.268 1.9  {-}7.68}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0)  65    2.999       4.13  0.725    0.468 1.1  {-}5.10}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0)  66    2.025       3.48  0.582    0.561 0.8  {-}4.80}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0)  91    1.858       2.76  0.674    0.500 1.0  {-}3.54}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0)  93    1.201       2.35  0.511    0.609 0.7  {-}3.40}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0)  95   {-}1.832       1.97 {-}0.931    0.352 1.5  {-}5.69}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0)  97    0.708       2.04  0.347    0.728 0.5  {-}3.28}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0) 105   {-}2.682       2.37 {-}1.132    0.258 2.0  {-}7.32}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0) 109   {-}0.237       1.59 {-}0.149    0.881 0.2  {-}3.35}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0) 110   {-}0.640       1.57 {-}0.407    0.684 0.5  {-}3.73}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0) 113    4.081       3.94  1.037    0.300 1.7  {-}3.63}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0) 123   {-}2.098       2.10 {-}0.998    0.318 1.7  {-}6.22}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0) 150   {-}1.429       1.90 {-}0.753    0.452 1.1  {-}5.15}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0) 175   {-}0.416       1.56 {-}0.266    0.790 0.3  {-}3.48}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0) 180   {-}1.381       2.47 {-}0.560    0.576 0.8  {-}6.22}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0) 205   {-}2.873       6.24 {-}0.460    0.645 0.6 {-}15.11}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0) 215   {-}2.534       6.95 {-}0.364    0.716 0.5 {-}16.16}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0) 230   {-}1.477       7.07 {-}0.209    0.835 0.3 {-}15.34}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0) 245    1.115       2.28  0.488    0.625 0.7  {-}3.36}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0) 264    2.106       2.29  0.920    0.358 1.5  {-}2.38}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0) 335    4.027       3.24  1.243    0.214 2.2  {-}2.32}
\CommentTok{\#\textgreater{}  97.5 \%}
\CommentTok{\#\textgreater{}   14.18}
\CommentTok{\#\textgreater{}    2.14}
\CommentTok{\#\textgreater{}   11.10}
\CommentTok{\#\textgreater{}    8.85}
\CommentTok{\#\textgreater{}    7.26}
\CommentTok{\#\textgreater{}    5.80}
\CommentTok{\#\textgreater{}    2.02}
\CommentTok{\#\textgreater{}    4.70}
\CommentTok{\#\textgreater{}    1.96}
\CommentTok{\#\textgreater{}    2.87}
\CommentTok{\#\textgreater{}    2.45}
\CommentTok{\#\textgreater{}   11.79}
\CommentTok{\#\textgreater{}    2.02}
\CommentTok{\#\textgreater{}    2.29}
\CommentTok{\#\textgreater{}    2.64}
\CommentTok{\#\textgreater{}    3.46}
\CommentTok{\#\textgreater{}    9.36}
\CommentTok{\#\textgreater{}   11.09}
\CommentTok{\#\textgreater{}   12.39}
\CommentTok{\#\textgreater{}    5.59}
\CommentTok{\#\textgreater{}    6.59}
\CommentTok{\#\textgreater{}   10.38}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Columns: term, contrast, hp, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted\_lo, predicted\_hi, predicted}

\CommentTok{\# Marginal effects at representative values}
\NormalTok{marginaleffects}\SpecialCharTok{::}\FunctionTok{marginaleffects}\NormalTok{(mod, }
                                 \AttributeTok{newdata =} \FunctionTok{datagrid}\NormalTok{(}\AttributeTok{am =} \DecValTok{0}\NormalTok{, }
                                                    \AttributeTok{wt =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{)))}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Term Contrast am wt Estimate Std. Error      z Pr(\textgreater{}|z|)   S   2.5 \%   97.5 \%}
\CommentTok{\#\textgreater{}    am    1 {-} 0  0  2   2.5465     2.7860  0.914   0.3607 1.5 {-}2.9139  8.00694}
\CommentTok{\#\textgreater{}    am    1 {-} 0  0  4  {-}2.9661     3.0381 {-}0.976   0.3289 1.6 {-}8.9207  2.98852}
\CommentTok{\#\textgreater{}    hp    dY/dX  0  2  {-}0.0598     0.0283 {-}2.115   0.0344 4.9 {-}0.1153 {-}0.00439}
\CommentTok{\#\textgreater{}    hp    dY/dX  0  4  {-}0.0309     0.0187 {-}1.654   0.0981 3.3 {-}0.0676  0.00572}
\CommentTok{\#\textgreater{}    wt    dY/dX  0  2  {-}2.6762     1.4194 {-}1.885   0.0594 4.1 {-}5.4582  0.10587}
\CommentTok{\#\textgreater{}    wt    dY/dX  0  4  {-}2.6762     1.4199 {-}1.885   0.0595 4.1 {-}5.4591  0.10676}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, am, wt, predicted\_lo, predicted\_hi, predicted, mpg, hp}

\CommentTok{\# Marginal Effects at the Mean}
\NormalTok{marginaleffects}\SpecialCharTok{::}\FunctionTok{marginaleffects}\NormalTok{(mod, }\AttributeTok{newdata =} \StringTok{"mean"}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Term Contrast Estimate Std. Error      z Pr(\textgreater{}|z|)    S  2.5 \%  97.5 \%}
\CommentTok{\#\textgreater{}    am    1 {-} 0  {-}0.8086    1.52383 {-}0.531  0.59568  0.7 {-}3.795  2.1781}
\CommentTok{\#\textgreater{}    hp    dY/dX  {-}0.0323    0.00956 {-}3.375  \textless{} 0.001 10.4 {-}0.051 {-}0.0135}
\CommentTok{\#\textgreater{}    wt    dY/dX  {-}3.7959    1.21310 {-}3.129  0.00175  9.2 {-}6.174 {-}1.4183}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted\_lo, predicted\_hi, predicted, mpg, hp, wt, am}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# counterfactual}
\FunctionTok{comparisons}\NormalTok{(mod, }\AttributeTok{variables =} \FunctionTok{list}\NormalTok{(}\AttributeTok{am =} \DecValTok{0}\SpecialCharTok{:}\DecValTok{1}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{summary}\NormalTok{()}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Term          Contrast Estimate Std. Error      z Pr(\textgreater{}|z|) 2.5 \% 97.5 \%}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0)  {-}0.0481       1.85 {-}0.026    0.979 {-}3.68   3.58}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Columns: term, contrast, estimate, std.error, statistic, p.value, conf.low, conf.high}
\end{Highlighting}
\end{Shaded}

\hypertarget{margins}{%
\subsection{margins}\label{margins}}

\begin{itemize}
\tightlist
\item
  Marginal effects are partial derivative of the regression equation with respect to each variable in the model for each unit in the data
\end{itemize}

\begin{itemize}
\item
  Average Partial Effects: the contribution of each variable the outcome scale, conditional on the other variables involved in the link function transformation of the linear predictor
\item
  Average Marginal Effects: the marginal contribution of each variable on the scale of the linear predictor.
\item
  Average marginal effects are the mean of these unit-specific partial derivatives over some sample
\end{itemize}

\texttt{margins} package gives the marginal effects of models (a replication of the \texttt{margins} command in Stata).

\texttt{prediction} package gives the unit-specific and sample average predictions of models (similar to the predictive margins in Stata).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(margins)}

\CommentTok{\# examples by the package\textquotesingle{}s authors}
\NormalTok{mod }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ cyl }\SpecialCharTok{*}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{summary}\NormalTok{(mod)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = mpg \textasciitilde{} cyl * hp + wt, data = mtcars)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}     Min      1Q  Median      3Q     Max }
\CommentTok{\#\textgreater{} {-}3.3440 {-}1.4144 {-}0.6166  1.2160  4.2815 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}              Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept) 52.017520   4.916935  10.579 4.18e{-}11 ***}
\CommentTok{\#\textgreater{} cyl         {-}2.742125   0.800228  {-}3.427  0.00197 ** }
\CommentTok{\#\textgreater{} hp          {-}0.163594   0.052122  {-}3.139  0.00408 ** }
\CommentTok{\#\textgreater{} wt          {-}3.119815   0.661322  {-}4.718 6.51e{-}05 ***}
\CommentTok{\#\textgreater{} cyl:hp       0.018954   0.006645   2.852  0.00823 ** }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 2.242 on 27 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.8795, Adjusted R{-}squared:  0.8616 }
\CommentTok{\#\textgreater{} F{-}statistic: 49.25 on 4 and 27 DF,  p{-}value: 5.065e{-}12}
\end{Highlighting}
\end{Shaded}

In cases where you have interaction or polynomial terms, the coefficient estimates cannot be interpreted as the marginal effects of X on Y. Hence, if you want to know the average marginal effects of each variable then

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(}\FunctionTok{margins}\NormalTok{(mod))}
\CommentTok{\#\textgreater{}  factor     AME     SE       z      p   lower   upper}
\CommentTok{\#\textgreater{}     cyl  0.0381 0.5999  0.0636 0.9493 {-}1.1376  1.2139}
\CommentTok{\#\textgreater{}      hp {-}0.0463 0.0145 {-}3.1909 0.0014 {-}0.0748 {-}0.0179}
\CommentTok{\#\textgreater{}      wt {-}3.1198 0.6613 {-}4.7176 0.0000 {-}4.4160 {-}1.8236}

\CommentTok{\# equivalently }
\FunctionTok{margins\_summary}\NormalTok{(mod)}
\CommentTok{\#\textgreater{}  factor     AME     SE       z      p   lower   upper}
\CommentTok{\#\textgreater{}     cyl  0.0381 0.5999  0.0636 0.9493 {-}1.1376  1.2139}
\CommentTok{\#\textgreater{}      hp {-}0.0463 0.0145 {-}3.1909 0.0014 {-}0.0748 {-}0.0179}
\CommentTok{\#\textgreater{}      wt {-}3.1198 0.6613 {-}4.7176 0.0000 {-}4.4160 {-}1.8236}

\FunctionTok{plot}\NormalTok{(}\FunctionTok{margins}\NormalTok{(mod))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{15-marginal-effect_files/figure-latex/unnamed-chunk-6-1} \end{center}

Marginal effects at the mean (\textbf{MEM}):

\begin{itemize}
\tightlist
\item
  Marginal effects at the mean values of the sample
\item
  For discrete variables, it's called average discrete change (\textbf{ADC})
\end{itemize}

Average Marginal Effect (\textbf{AME})

\begin{itemize}
\tightlist
\item
  An average of the marginal effects at each value of the sample
\end{itemize}

Marginal Effects at representative values (\textbf{MER})

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{margins}\NormalTok{(mod, }\AttributeTok{at =} \FunctionTok{list}\NormalTok{(}\AttributeTok{hp =} \DecValTok{150}\NormalTok{))}
\CommentTok{\#\textgreater{}  at(hp)    cyl       hp    wt}
\CommentTok{\#\textgreater{}     150 0.1009 {-}0.04632 {-}3.12}

\FunctionTok{margins}\NormalTok{(mod, }\AttributeTok{at =} \FunctionTok{list}\NormalTok{(}\AttributeTok{hp =} \DecValTok{150}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{summary}\NormalTok{()}
\CommentTok{\#\textgreater{}  factor       hp     AME     SE       z      p   lower   upper}
\CommentTok{\#\textgreater{}     cyl 150.0000  0.1009 0.6128  0.1647 0.8692 {-}1.1001  1.3019}
\CommentTok{\#\textgreater{}      hp 150.0000 {-}0.0463 0.0145 {-}3.1909 0.0014 {-}0.0748 {-}0.0179}
\CommentTok{\#\textgreater{}      wt 150.0000 {-}3.1198 0.6613 {-}4.7175 0.0000 {-}4.4160 {-}1.8236}
\end{Highlighting}
\end{Shaded}

\hypertarget{mfx}{%
\subsection{mfx}\label{mfx}}

Works well with \protect\hyperlink{generalized-linear-models}{Generalized Linear Models}/\texttt{glm} package

For technical details, see the package \href{https://cran.rstudio.com/web/packages/mfx/vignettes/mfxarticle.pdf}{vignette}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Model & Dependent Variable & Syntax \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Probit & Binary & \texttt{probitmfx} \\
Logit & Binary & \texttt{logitmfx} \\
Poisson & Count & \texttt{poissonmfx} \\
Negative Binomial & Count & \texttt{negbinmfx} \\
Beta & Rate & \texttt{betamfx} \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(mfx)}
\FunctionTok{data}\NormalTok{(}\StringTok{"mtcars"}\NormalTok{)}
\FunctionTok{poissonmfx}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ vs }\SpecialCharTok{\textasciitilde{}}\NormalTok{ mpg }\SpecialCharTok{*}\NormalTok{ cyl }\SpecialCharTok{*}\NormalTok{ disp, }\AttributeTok{data =}\NormalTok{ mtcars)}
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} poissonmfx(formula = vs \textasciitilde{} mpg * cyl * disp, data = mtcars)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Marginal Effects:}
\CommentTok{\#\textgreater{}                    dF/dx   Std. Err.       z  P\textgreater{}|z|}
\CommentTok{\#\textgreater{} mpg           1.4722e{-}03  8.7531e{-}03  0.1682 0.8664}
\CommentTok{\#\textgreater{} cyl           6.6420e{-}03  3.9263e{-}02  0.1692 0.8657}
\CommentTok{\#\textgreater{} disp          1.5899e{-}04  9.4555e{-}04  0.1681 0.8665}
\CommentTok{\#\textgreater{} mpg:cyl      {-}3.4698e{-}04  2.0564e{-}03 {-}0.1687 0.8660}
\CommentTok{\#\textgreater{} mpg:disp     {-}7.6794e{-}06  4.5545e{-}05 {-}0.1686 0.8661}
\CommentTok{\#\textgreater{} cyl:disp     {-}3.3837e{-}05  1.9919e{-}04 {-}0.1699 0.8651}
\CommentTok{\#\textgreater{} mpg:cyl:disp  1.6812e{-}06  9.8919e{-}06  0.1700 0.8650}
\end{Highlighting}
\end{Shaded}

This package can only give the marginal effect for each variable in the \texttt{glm} model, but not the average marginal effect that we might look for.

\hypertarget{prediction-and-estimation}{%
\chapter{Prediction and Estimation}\label{prediction-and-estimation}}

Prediction and Estimation (or Causal Inference) serve distinct roles in understanding and modeling data.

\hypertarget{prediction-1}{%
\section{Prediction}\label{prediction-1}}

\begin{itemize}
\item
  \textbf{Definition}: Prediction, denoted as \(\hat{y}\), is about creating an algorithm for predicting the outcome variable \(y\) from predictors \(x\).
\item
  \textbf{Goal}: The primary goal is loss minimization, aiming for model accuracy on unseen data:

  \[
  \hat{f} \approx \min E_{(y,x)} L(f(x), y)
  \]
\item
  \textbf{Applications in Economics}:

  \begin{itemize}
  \tightlist
  \item
    Measure variables.
  \item
    Embed prediction tasks within parameter estimation or treatment effects.
  \item
    Control for observed confounders.
  \end{itemize}
\end{itemize}

\hypertarget{parameter-estimation}{%
\section{Parameter Estimation}\label{parameter-estimation}}

\begin{itemize}
\item
  \textbf{Definition}: Parameter estimation, represented by \(\hat{\beta}\), focuses on estimating the relationship between \(y\) and \(x\).
\item
  \textbf{Goal}: The aim is consistency, ensuring that models perform well on the training data:

  \[
  E[\hat{f}] = f
  \]
\item
  \textbf{Challenges}:

  \begin{itemize}
  \tightlist
  \item
    High-dimensional spaces can lead to covariance among variables and multicollinearity.
  \item
    This leads to the bias-variance tradeoff \citep{hastie2009elements}.
  \end{itemize}
\end{itemize}

\hypertarget{causation-versus-prediction}{%
\section{Causation versus Prediction}\label{causation-versus-prediction}}

Understanding the relationship between causation and prediction is crucial in statistical modeling.

Let \(Y\) be an outcome variable dependent on \(X\), and our aim is to manipulate \(X\) to maximize a payoff function \(\pi(X, Y)\) \citep{kleinberg2015prediction}. The decision on \(X\) hinges on:

\[ 
\begin{aligned}
\frac{d\pi(X, Y)}{d X} &= \frac{\partial \pi}{\partial X} (Y) + \frac{\partial \pi}{\partial Y} \frac{\partial Y}{\partial X} \\
&= \frac{\partial \pi}{\partial X} \text{(Prediction)} + \frac{\partial \pi}{\partial Y} \text{(Causation)} 
\end{aligned}
\]

Empirical work is essential for estimating the derivatives in this equation:

\begin{itemize}
\item
  \(\frac{\partial Y}{\partial X}\) is required for causal inference to determine \(X\)'s effect on \(Y\),
\item
  \(\frac{\partial \pi}{\partial X}\) is required for prediction of \(Y\).
\end{itemize}

\begin{figure}
\centering
\includegraphics[width=6.25in,height=3.64583in]{images/prediction_causation.PNG}
\caption{(SICSS 2018 - Sendhil Mullainathan's presentation slide)}
\end{figure}

\hypertarget{moderation}{%
\chapter{Moderation}\label{moderation}}

\begin{itemize}
\tightlist
\item
  Spotlight Analysis: Compare the mean of the dependent of the two groups (treatment and control) at every value (\protect\hyperlink{simple-slopes-analysis}{Simple Slopes Analysis})
\item
  Floodlight Analysis: is spotlight analysis on the whole range of the moderator (\protect\hyperlink{johnson-neyman-intervals}{Johnson-Neyman intervals})
\end{itemize}

Other Resources:

\begin{itemize}
\item
  \texttt{BANOVAL} : floodlight analysis on Bayesian ANOVA models
\item
  \texttt{cSEM} : \texttt{doFloodlightAnalysis} in SEM model
\item
  \citep{spiller2013}
\end{itemize}

Terminology:

\begin{itemize}
\item
  Main effects (slopes): coefficients that do no involve interaction terms
\item
  Simple slope: when a continuous independent variable interact with a moderating variable, its slope at a particular level of the moderating variable
\item
  Simple effect: when a categorical independent variable interacts with a moderating variable, its effect at a particular level of the moderating variable.
\end{itemize}

Example:

\[
Y = \beta_0 + \beta_1 X + \beta_2 M + \beta_3 X \times M
\]

where

\begin{itemize}
\item
  \(\beta_0\) = intercept
\item
  \(\beta_1\) = simple effect (slope) of \(X\) (independent variable)
\item
  \(\beta_2\) = simple effect (slope) of \(M\) (moderating variable)
\item
  \(\beta_3\) = interaction of \(X\) and \(M\)
\end{itemize}

Three types of interactions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \protect\hyperlink{continuous-by-continuous}{Continuous by continuous}
\item
  \protect\hyperlink{continuous-by-categorical}{Continuous by categorical}
\item
  \protect\hyperlink{categorical-by-categorical}{Categorical by categorical}
\end{enumerate}

When interpreting the three-way interactions, one can use the slope difference test \citep{dawson2006probing}

\hypertarget{emmeans-package}{%
\section{emmeans package}\label{emmeans-package}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"emmeans"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(emmeans)}
\end{Highlighting}
\end{Shaded}

Data set is from \href{https://stats.oarc.ucla.edu/r/seminars/interactions-r/}{UCLA seminar} where \texttt{gender} and \texttt{prog} are categorical

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{readRDS}\NormalTok{(}\StringTok{"data/exercise.rds"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{prog =} \FunctionTok{factor}\NormalTok{(prog, }\AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"jog"}\NormalTok{, }\StringTok{"swim"}\NormalTok{, }\StringTok{"read"}\NormalTok{))) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{gender =} \FunctionTok{factor}\NormalTok{(gender, }\AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"male"}\NormalTok{, }\StringTok{"female"}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\hypertarget{continuous-by-continuous}{%
\subsection{Continuous by continuous}\label{continuous-by-continuous}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{contcont }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(loss}\SpecialCharTok{\textasciitilde{}}\NormalTok{hours}\SpecialCharTok{*}\NormalTok{effort,}\AttributeTok{data=}\NormalTok{dat)}
\FunctionTok{summary}\NormalTok{(contcont)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = loss \textasciitilde{} hours * effort, data = dat)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}    Min     1Q Median     3Q    Max }
\CommentTok{\#\textgreater{} {-}29.52 {-}10.60  {-}1.78  11.13  34.51 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}              Estimate Std. Error t value Pr(\textgreater{}|t|)  }
\CommentTok{\#\textgreater{} (Intercept)   7.79864   11.60362   0.672   0.5017  }
\CommentTok{\#\textgreater{} hours        {-}9.37568    5.66392  {-}1.655   0.0982 .}
\CommentTok{\#\textgreater{} effort       {-}0.08028    0.38465  {-}0.209   0.8347  }
\CommentTok{\#\textgreater{} hours:effort  0.39335    0.18750   2.098   0.0362 *}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 13.56 on 896 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.07818,    Adjusted R{-}squared:  0.07509 }
\CommentTok{\#\textgreater{} F{-}statistic: 25.33 on 3 and 896 DF,  p{-}value: 9.826e{-}16}
\end{Highlighting}
\end{Shaded}

Simple slopes for a continuous by continuous model

Spotlight analysis \citep{aiken2005interaction}: usually pick 3 values of moderating variable:

\begin{itemize}
\item
  Mean Moderating Variable + \(\sigma \times\) (Moderating variable)
\item
  Mean Moderating Variable
\item
  Mean Moderating Variable - \(\sigma \times\) (Moderating variable)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{effar }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(}\FunctionTok{mean}\NormalTok{(dat}\SpecialCharTok{$}\NormalTok{effort) }\SpecialCharTok{+} \FunctionTok{sd}\NormalTok{(dat}\SpecialCharTok{$}\NormalTok{effort), }\DecValTok{1}\NormalTok{)}
\NormalTok{effr  }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(}\FunctionTok{mean}\NormalTok{(dat}\SpecialCharTok{$}\NormalTok{effort), }\DecValTok{1}\NormalTok{)}
\NormalTok{effbr }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(}\FunctionTok{mean}\NormalTok{(dat}\SpecialCharTok{$}\NormalTok{effort) }\SpecialCharTok{{-}} \FunctionTok{sd}\NormalTok{(dat}\SpecialCharTok{$}\NormalTok{effort), }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# specify list of points}
\NormalTok{mylist }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{effort =} \FunctionTok{c}\NormalTok{(effbr, effr, effar))}

\CommentTok{\# get the estimates}
\FunctionTok{emtrends}\NormalTok{(contcont, }\SpecialCharTok{\textasciitilde{}}\NormalTok{ effort, }\AttributeTok{var =} \StringTok{"hours"}\NormalTok{, }\AttributeTok{at =}\NormalTok{ mylist)}
\CommentTok{\#\textgreater{}  effort hours.trend    SE  df lower.CL upper.CL}
\CommentTok{\#\textgreater{}    24.5       0.261 1.352 896   {-}2.392     2.91}
\CommentTok{\#\textgreater{}    29.7       2.307 0.915 896    0.511     4.10}
\CommentTok{\#\textgreater{}    34.8       4.313 1.308 896    1.745     6.88}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Confidence level used: 0.95}

\CommentTok{\# plot}
\NormalTok{mylist }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{hours =} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{4}\NormalTok{, }\AttributeTok{by =} \FloatTok{0.4}\NormalTok{),}
               \AttributeTok{effort =} \FunctionTok{c}\NormalTok{(effbr, effr, effar))}
\FunctionTok{emmip}\NormalTok{(contcont, effort }\SpecialCharTok{\textasciitilde{}}\NormalTok{ hours, }\AttributeTok{at =}\NormalTok{ mylist, }\AttributeTok{CIs =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{17-moderation_files/figure-latex/unnamed-chunk-6-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# statistical test for slope difference}
\FunctionTok{emtrends}\NormalTok{(}
\NormalTok{    contcont,}
\NormalTok{    pairwise }\SpecialCharTok{\textasciitilde{}}\NormalTok{ effort,}
    \AttributeTok{var =} \StringTok{"hours"}\NormalTok{,}
    \AttributeTok{at =}\NormalTok{ mylist,}
    \AttributeTok{adjust =} \StringTok{"none"}
\NormalTok{)}
\CommentTok{\#\textgreater{} $emtrends}
\CommentTok{\#\textgreater{}  effort hours.trend    SE  df lower.CL upper.CL}
\CommentTok{\#\textgreater{}    24.5       0.261 1.352 896   {-}2.392     2.91}
\CommentTok{\#\textgreater{}    29.7       2.307 0.915 896    0.511     4.10}
\CommentTok{\#\textgreater{}    34.8       4.313 1.308 896    1.745     6.88}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Results are averaged over the levels of: hours }
\CommentTok{\#\textgreater{} Confidence level used: 0.95 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $contrasts}
\CommentTok{\#\textgreater{}  contrast                estimate    SE  df t.ratio p.value}
\CommentTok{\#\textgreater{}  effort24.5 {-} effort29.7    {-}2.05 0.975 896  {-}2.098  0.0362}
\CommentTok{\#\textgreater{}  effort24.5 {-} effort34.8    {-}4.05 1.931 896  {-}2.098  0.0362}
\CommentTok{\#\textgreater{}  effort29.7 {-} effort34.8    {-}2.01 0.956 896  {-}2.098  0.0362}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Results are averaged over the levels of: hours}
\end{Highlighting}
\end{Shaded}

The 3 p-values are the same as the interaction term.

For publication, we use

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}

\CommentTok{\# data}
\NormalTok{mylist }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{hours =} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{4}\NormalTok{, }\AttributeTok{by =} \FloatTok{0.4}\NormalTok{),}
               \AttributeTok{effort =} \FunctionTok{c}\NormalTok{(effbr, effr, effar))}
\NormalTok{contcontdat }\OtherTok{\textless{}{-}}
    \FunctionTok{emmip}\NormalTok{(contcont,}
\NormalTok{          effort }\SpecialCharTok{\textasciitilde{}}\NormalTok{ hours,}
          \AttributeTok{at =}\NormalTok{ mylist,}
          \AttributeTok{CIs =} \ConstantTok{TRUE}\NormalTok{,}
          \AttributeTok{plotit =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{contcontdat}\SpecialCharTok{$}\NormalTok{feffort }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(contcontdat}\SpecialCharTok{$}\NormalTok{effort)}
\FunctionTok{levels}\NormalTok{(contcontdat}\SpecialCharTok{$}\NormalTok{feffort) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"low"}\NormalTok{, }\StringTok{"med"}\NormalTok{, }\StringTok{"high"}\NormalTok{)}

\CommentTok{\# plot}
\NormalTok{p  }\OtherTok{\textless{}{-}}
    \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ contcontdat, }
           \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ hours, }\AttributeTok{y =}\NormalTok{ yvar, }\AttributeTok{color =}\NormalTok{ feffort)) }\SpecialCharTok{+}  
    \FunctionTok{geom\_line}\NormalTok{()}
\NormalTok{p1 }\OtherTok{\textless{}{-}}
\NormalTok{    p }\SpecialCharTok{+} 
    \FunctionTok{geom\_ribbon}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{ymax =}\NormalTok{ UCL, }\AttributeTok{ymin =}\NormalTok{ LCL, }\AttributeTok{fill =}\NormalTok{ feffort), }
                    \AttributeTok{alpha =} \FloatTok{0.4}\NormalTok{)}
\NormalTok{p1  }\SpecialCharTok{+} \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Hours"}\NormalTok{,}
           \AttributeTok{y =} \StringTok{"Weight Loss"}\NormalTok{,}
           \AttributeTok{color =} \StringTok{"Effort"}\NormalTok{,}
           \AttributeTok{fill =} \StringTok{"Effort"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{17-moderation_files/figure-latex/unnamed-chunk-7-1} \end{center}

\hypertarget{continuous-by-categorical}{%
\subsection{Continuous by categorical}\label{continuous-by-categorical}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# use Female as basline}
\NormalTok{dat}\SpecialCharTok{$}\NormalTok{gender }\OtherTok{\textless{}{-}} \FunctionTok{relevel}\NormalTok{(dat}\SpecialCharTok{$}\NormalTok{gender, }\AttributeTok{ref =} \StringTok{"female"}\NormalTok{)}

\NormalTok{contcat }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(loss }\SpecialCharTok{\textasciitilde{}}\NormalTok{ hours }\SpecialCharTok{*}\NormalTok{ gender, }\AttributeTok{data =}\NormalTok{ dat)}
\FunctionTok{summary}\NormalTok{(contcat)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = loss \textasciitilde{} hours * gender, data = dat)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}     Min      1Q  Median      3Q     Max }
\CommentTok{\#\textgreater{} {-}27.118 {-}11.350  {-}1.963  10.001  42.376 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}                  Estimate Std. Error t value Pr(\textgreater{}|t|)  }
\CommentTok{\#\textgreater{} (Intercept)         3.335      2.731   1.221    0.222  }
\CommentTok{\#\textgreater{} hours               3.315      1.332   2.489    0.013 *}
\CommentTok{\#\textgreater{} gendermale          3.571      3.915   0.912    0.362  }
\CommentTok{\#\textgreater{} hours:gendermale   {-}1.724      1.898  {-}0.908    0.364  }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 14.06 on 896 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.008433,   Adjusted R{-}squared:  0.005113 }
\CommentTok{\#\textgreater{} F{-}statistic:  2.54 on 3 and 896 DF,  p{-}value: 0.05523}
\end{Highlighting}
\end{Shaded}

Get simple slopes by each level of the categorical moderator

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{emtrends}\NormalTok{(contcat, }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gender, }\AttributeTok{var =} \StringTok{"hours"}\NormalTok{)}
\CommentTok{\#\textgreater{}  gender hours.trend   SE  df lower.CL upper.CL}
\CommentTok{\#\textgreater{}  female        3.32 1.33 896    0.702     5.93}
\CommentTok{\#\textgreater{}  male          1.59 1.35 896   {-}1.063     4.25}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Confidence level used: 0.95}

\CommentTok{\# test difference in slopes}
\FunctionTok{emtrends}\NormalTok{(contcat, pairwise }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gender, }\AttributeTok{var =} \StringTok{"hours"}\NormalTok{)}
\CommentTok{\#\textgreater{} $emtrends}
\CommentTok{\#\textgreater{}  gender hours.trend   SE  df lower.CL upper.CL}
\CommentTok{\#\textgreater{}  female        3.32 1.33 896    0.702     5.93}
\CommentTok{\#\textgreater{}  male          1.59 1.35 896   {-}1.063     4.25}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Confidence level used: 0.95 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $contrasts}
\CommentTok{\#\textgreater{}  contrast      estimate  SE  df t.ratio p.value}
\CommentTok{\#\textgreater{}  female {-} male     1.72 1.9 896   0.908  0.3639}
\CommentTok{\# which is the same as the interaction term}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# plot}
\NormalTok{(mylist }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
    \AttributeTok{hours =} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{4}\NormalTok{, }\AttributeTok{by =} \FloatTok{0.4}\NormalTok{),}
    \AttributeTok{gender =} \FunctionTok{c}\NormalTok{(}\StringTok{"female"}\NormalTok{, }\StringTok{"male"}\NormalTok{)}
\NormalTok{))}
\CommentTok{\#\textgreater{} $hours}
\CommentTok{\#\textgreater{}  [1] 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 3.6 4.0}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $gender}
\CommentTok{\#\textgreater{} [1] "female" "male"}
\FunctionTok{emmip}\NormalTok{(contcat, gender }\SpecialCharTok{\textasciitilde{}}\NormalTok{ hours, }\AttributeTok{at =}\NormalTok{ mylist, }\AttributeTok{CIs =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{17-moderation_files/figure-latex/unnamed-chunk-10-1} \end{center}

\hypertarget{categorical-by-categorical}{%
\subsection{Categorical by categorical}\label{categorical-by-categorical}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# relevel baseline}
\NormalTok{dat}\SpecialCharTok{$}\NormalTok{prog   }\OtherTok{\textless{}{-}} \FunctionTok{relevel}\NormalTok{(dat}\SpecialCharTok{$}\NormalTok{prog, }\AttributeTok{ref =} \StringTok{"read"}\NormalTok{)}
\NormalTok{dat}\SpecialCharTok{$}\NormalTok{gender }\OtherTok{\textless{}{-}} \FunctionTok{relevel}\NormalTok{(dat}\SpecialCharTok{$}\NormalTok{gender, }\AttributeTok{ref =} \StringTok{"female"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{catcat }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(loss }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gender }\SpecialCharTok{*}\NormalTok{ prog, }\AttributeTok{data =}\NormalTok{ dat)}
\FunctionTok{summary}\NormalTok{(catcat)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = loss \textasciitilde{} gender * prog, data = dat)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}      Min       1Q   Median       3Q      Max }
\CommentTok{\#\textgreater{} {-}19.1723  {-}4.1894  {-}0.0994   3.7506  27.6939 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}                     Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)          {-}3.6201     0.5322  {-}6.802 1.89e{-}11 ***}
\CommentTok{\#\textgreater{} gendermale           {-}0.3355     0.7527  {-}0.446    0.656    }
\CommentTok{\#\textgreater{} progjog               7.9088     0.7527  10.507  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} progswim             32.7378     0.7527  43.494  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} gendermale:progjog    7.8188     1.0645   7.345 4.63e{-}13 ***}
\CommentTok{\#\textgreater{} gendermale:progswim  {-}6.2599     1.0645  {-}5.881 5.77e{-}09 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 6.519 on 894 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.7875, Adjusted R{-}squared:  0.7863 }
\CommentTok{\#\textgreater{} F{-}statistic: 662.5 on 5 and 894 DF,  p{-}value: \textless{} 2.2e{-}16}
\end{Highlighting}
\end{Shaded}

Simple effects

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{emcatcat }\OtherTok{\textless{}{-}} \FunctionTok{emmeans}\NormalTok{(catcat, }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gender}\SpecialCharTok{*}\NormalTok{prog)}

\CommentTok{\# differences in predicted values}
\FunctionTok{contrast}\NormalTok{(emcatcat, }
         \StringTok{"revpairwise"}\NormalTok{, }
         \AttributeTok{by =} \StringTok{"prog"}\NormalTok{, }
         \AttributeTok{adjust =} \StringTok{"bonferroni"}\NormalTok{)}
\CommentTok{\#\textgreater{} prog = read:}
\CommentTok{\#\textgreater{}  contrast      estimate    SE  df t.ratio p.value}
\CommentTok{\#\textgreater{}  male {-} female   {-}0.335 0.753 894  {-}0.446  0.6559}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} prog = jog:}
\CommentTok{\#\textgreater{}  contrast      estimate    SE  df t.ratio p.value}
\CommentTok{\#\textgreater{}  male {-} female    7.483 0.753 894   9.942  \textless{}.0001}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} prog = swim:}
\CommentTok{\#\textgreater{}  contrast      estimate    SE  df t.ratio p.value}
\CommentTok{\#\textgreater{}  male {-} female   {-}6.595 0.753 894  {-}8.762  \textless{}.0001}
\end{Highlighting}
\end{Shaded}

Plot

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{emmip}\NormalTok{(catcat, prog }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gender,}\AttributeTok{CIs=}\ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{17-moderation_files/figure-latex/unnamed-chunk-14-1} \end{center}

Bar graph

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{catcatdat }\OtherTok{\textless{}{-}} \FunctionTok{emmip}\NormalTok{(catcat,}
\NormalTok{                   gender }\SpecialCharTok{\textasciitilde{}}\NormalTok{ prog,}
                   \AttributeTok{CIs =} \ConstantTok{TRUE}\NormalTok{,}
                   \AttributeTok{plotit =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{p }\OtherTok{\textless{}{-}}
    \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ catcatdat,}
           \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ prog, }\AttributeTok{y =}\NormalTok{ yvar, }\AttributeTok{fill =}\NormalTok{ gender)) }\SpecialCharTok{+}
    \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat =} \StringTok{"identity"}\NormalTok{, }\AttributeTok{position =} \StringTok{"dodge"}\NormalTok{)}

\NormalTok{p1 }\OtherTok{\textless{}{-}}
\NormalTok{    p }\SpecialCharTok{+} \FunctionTok{geom\_errorbar}\NormalTok{(}
        \AttributeTok{position =} \FunctionTok{position\_dodge}\NormalTok{(.}\DecValTok{9}\NormalTok{),}
        \AttributeTok{width =}\NormalTok{ .}\DecValTok{25}\NormalTok{,}
        \FunctionTok{aes}\NormalTok{(}\AttributeTok{ymax =}\NormalTok{ UCL, }\AttributeTok{ymin =}\NormalTok{ LCL),}
        \AttributeTok{alpha =} \FloatTok{0.3}
\NormalTok{    )}
\NormalTok{p1  }\SpecialCharTok{+} \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Program"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Weight Loss"}\NormalTok{, }\AttributeTok{fill =} \StringTok{"Gender"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{17-moderation_files/figure-latex/unnamed-chunk-15-1} \end{center}

\hypertarget{probmod-package}{%
\section{probmod package}\label{probmod-package}}

\begin{itemize}
\tightlist
\item
  Not recommend: package has serious problem with subscript.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"probemod"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(probemod)}

\NormalTok{myModel }\OtherTok{\textless{}{-}}
    \FunctionTok{lm}\NormalTok{(loss }\SpecialCharTok{\textasciitilde{}}\NormalTok{ hours }\SpecialCharTok{*}\NormalTok{ gender, }\AttributeTok{data =}\NormalTok{ dat }\SpecialCharTok{\%\textgreater{}\%} 
           \FunctionTok{select}\NormalTok{(loss, hours, gender))}
\NormalTok{jnresults }\OtherTok{\textless{}{-}} \FunctionTok{jn}\NormalTok{(myModel,}
                \AttributeTok{dv =} \StringTok{\textquotesingle{}loss\textquotesingle{}}\NormalTok{,}
                \AttributeTok{iv =} \StringTok{\textquotesingle{}hours\textquotesingle{}}\NormalTok{,}
                \AttributeTok{mod =} \StringTok{\textquotesingle{}gender\textquotesingle{}}\NormalTok{)}


\FunctionTok{pickapoint}\NormalTok{(}
\NormalTok{    myModel,}
    \AttributeTok{dv =} \StringTok{\textquotesingle{}loss\textquotesingle{}}\NormalTok{,}
    \AttributeTok{iv =} \StringTok{\textquotesingle{}hours\textquotesingle{}}\NormalTok{,}
    \AttributeTok{mod =} \StringTok{\textquotesingle{}gender\textquotesingle{}}\NormalTok{,}
    \AttributeTok{alpha =}\NormalTok{ .}\DecValTok{01}
\NormalTok{)}

\FunctionTok{plot}\NormalTok{(jnresults)}
\end{Highlighting}
\end{Shaded}

\hypertarget{interactions-package}{%
\section{interactions package}\label{interactions-package}}

\begin{itemize}
\tightlist
\item
  Recommend
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"interactions"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{continuous-interaction}{%
\subsection{Continuous interaction}\label{continuous-interaction}}

\begin{itemize}
\tightlist
\item
  (at least one of the two variables is continuous)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(interactions)}
\FunctionTok{library}\NormalTok{(jtools) }\CommentTok{\# for summ()}
\NormalTok{states }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(state.x77)}
\NormalTok{fiti }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Income }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Illiteracy }\SpecialCharTok{*}\NormalTok{ Murder }\SpecialCharTok{+} \StringTok{\textasciigrave{}}\AttributeTok{HS Grad}\StringTok{\textasciigrave{}}\NormalTok{, }\AttributeTok{data =}\NormalTok{ states)}
\FunctionTok{summ}\NormalTok{(fiti)}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]
\centering
\begin{tabular}{lr}
\toprule
\cellcolor{gray!6}{Observations} & \cellcolor{gray!6}{50}\\
Dependent variable & Income\\
\cellcolor{gray!6}{Type} & \cellcolor{gray!6}{OLS linear regression}\\
\bottomrule
\end{tabular}
\end{table} \begin{table}[!h]
\centering
\begin{tabular}{lr}
\toprule
\cellcolor{gray!6}{F(4,45)} & \cellcolor{gray!6}{10.65}\\
R & 0.49\\
\cellcolor{gray!6}{Adj. R} & \cellcolor{gray!6}{0.44}\\
\bottomrule
\end{tabular}
\end{table} \begin{table}[!h]
\centering
\begin{threeparttable}
\begin{tabular}{lrrrr}
\toprule
  & Est. & S.E. & t val. & p\\
\midrule
\cellcolor{gray!6}{(Intercept)} & \cellcolor{gray!6}{1414.46} & \cellcolor{gray!6}{737.84} & \cellcolor{gray!6}{1.92} & \cellcolor{gray!6}{0.06}\\
Illiteracy & 753.07 & 385.90 & 1.95 & 0.06\\
\cellcolor{gray!6}{Murder} & \cellcolor{gray!6}{130.60} & \cellcolor{gray!6}{44.67} & \cellcolor{gray!6}{2.92} & \cellcolor{gray!6}{0.01}\\
`HS Grad` & 40.76 & 10.92 & 3.73 & 0.00\\
\cellcolor{gray!6}{Illiteracy:Murder} & \cellcolor{gray!6}{-97.04} & \cellcolor{gray!6}{35.86} & \cellcolor{gray!6}{-2.71} & \cellcolor{gray!6}{0.01}\\
\bottomrule
\end{tabular}
\begin{tablenotes}
\item Standard errors: OLS
\end{tablenotes}
\end{threeparttable}
\end{table}

For continuous moderator, the three values chosen are:

\begin{itemize}
\item
  -1 SD above the mean
\item
  The mean
\item
  -1 SD below the mean
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{interact\_plot}\NormalTok{(fiti,}
              \AttributeTok{pred =}\NormalTok{ Illiteracy,}
              \AttributeTok{modx =}\NormalTok{ Murder,}
              
              \CommentTok{\# if you don\textquotesingle{}t want the plot to mean{-}center}
              \CommentTok{\# centered = "none", }
              
              \CommentTok{\# exclude the mean value of the moderator}
              \CommentTok{\# modx.values = "plus{-}minus", }
              
              \CommentTok{\# split moderator\textquotesingle{}s distribution into 3 groups}
              \CommentTok{\# modx.values = "terciles" }
              
              \AttributeTok{plot.points =}\NormalTok{ T, }\CommentTok{\# overlay data}
              
              
              \CommentTok{\# different shape for differennt levels of the moderator}
              \AttributeTok{point.shape =}\NormalTok{ T, }
              
              \CommentTok{\# if two data points are on top one another, }
              \CommentTok{\# this moves them apart by little}
              \AttributeTok{jitter =} \FloatTok{0.1}\NormalTok{, }
              
              \CommentTok{\# other appearance option}
              \AttributeTok{x.label =} \StringTok{"X label"}\NormalTok{, }
              \AttributeTok{y.label =} \StringTok{"Y label"}\NormalTok{,}
              \AttributeTok{main.title =} \StringTok{"Title"}\NormalTok{,}
              \AttributeTok{legend.main =} \StringTok{"Legend Title"}\NormalTok{,}
              \AttributeTok{colors =} \StringTok{"blue"}\NormalTok{,}
              
              \CommentTok{\# include confidence band}
              \AttributeTok{interval =} \ConstantTok{TRUE}\NormalTok{, }
              \AttributeTok{int.width =} \FloatTok{0.9}\NormalTok{, }
              \AttributeTok{robust =} \ConstantTok{TRUE} \CommentTok{\# use robust SE}
\NormalTok{              ) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{17-moderation_files/figure-latex/unnamed-chunk-20-1} \end{center}

To include weights from the regression inn the plot

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fiti }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Income }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Illiteracy }\SpecialCharTok{*}\NormalTok{ Murder,}
           \AttributeTok{data =}\NormalTok{ states,}
           \AttributeTok{weights =}\NormalTok{ Population)}

\FunctionTok{interact\_plot}\NormalTok{(fiti,}
              \AttributeTok{pred =}\NormalTok{ Illiteracy,}
              \AttributeTok{modx =}\NormalTok{ Murder,}
              \AttributeTok{plot.points =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{17-moderation_files/figure-latex/unnamed-chunk-21-1} \end{center}

Partial Effect Plot

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{data}\NormalTok{(cars)}
\NormalTok{fitc }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(cty }\SpecialCharTok{\textasciitilde{}}\NormalTok{ year }\SpecialCharTok{+}\NormalTok{ cyl }\SpecialCharTok{*}\NormalTok{ displ }\SpecialCharTok{+}\NormalTok{ class }\SpecialCharTok{+}\NormalTok{ fl }\SpecialCharTok{+}\NormalTok{ drv, }
           \AttributeTok{data =}\NormalTok{ mpg)}
\FunctionTok{summ}\NormalTok{(fitc)}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]
\centering
\begin{tabular}{lr}
\toprule
\cellcolor{gray!6}{Observations} & \cellcolor{gray!6}{234}\\
Dependent variable & cty\\
\cellcolor{gray!6}{Type} & \cellcolor{gray!6}{OLS linear regression}\\
\bottomrule
\end{tabular}
\end{table} \begin{table}[!h]
\centering
\begin{tabular}{lr}
\toprule
\cellcolor{gray!6}{F(16,217)} & \cellcolor{gray!6}{99.73}\\
R & 0.88\\
\cellcolor{gray!6}{Adj. R} & \cellcolor{gray!6}{0.87}\\
\bottomrule
\end{tabular}
\end{table} \begin{table}[!h]
\centering
\begin{threeparttable}
\begin{tabular}{lrrrr}
\toprule
  & Est. & S.E. & t val. & p\\
\midrule
\cellcolor{gray!6}{(Intercept)} & \cellcolor{gray!6}{-200.98} & \cellcolor{gray!6}{47.01} & \cellcolor{gray!6}{-4.28} & \cellcolor{gray!6}{0.00}\\
year & 0.12 & 0.02 & 5.03 & 0.00\\
\cellcolor{gray!6}{cyl} & \cellcolor{gray!6}{-1.86} & \cellcolor{gray!6}{0.28} & \cellcolor{gray!6}{-6.69} & \cellcolor{gray!6}{0.00}\\
displ & -3.56 & 0.66 & -5.41 & 0.00\\
\cellcolor{gray!6}{classcompact} & \cellcolor{gray!6}{-2.60} & \cellcolor{gray!6}{0.93} & \cellcolor{gray!6}{-2.80} & \cellcolor{gray!6}{0.01}\\
\addlinespace
classmidsize & -2.63 & 0.93 & -2.82 & 0.01\\
\cellcolor{gray!6}{classminivan} & \cellcolor{gray!6}{-4.41} & \cellcolor{gray!6}{1.04} & \cellcolor{gray!6}{-4.24} & \cellcolor{gray!6}{0.00}\\
classpickup & -4.37 & 0.93 & -4.68 & 0.00\\
\cellcolor{gray!6}{classsubcompact} & \cellcolor{gray!6}{-2.38} & \cellcolor{gray!6}{0.93} & \cellcolor{gray!6}{-2.56} & \cellcolor{gray!6}{0.01}\\
classsuv & -4.27 & 0.87 & -4.92 & 0.00\\
\addlinespace
\cellcolor{gray!6}{fld} & \cellcolor{gray!6}{6.34} & \cellcolor{gray!6}{1.69} & \cellcolor{gray!6}{3.74} & \cellcolor{gray!6}{0.00}\\
fle & -4.57 & 1.66 & -2.75 & 0.01\\
\cellcolor{gray!6}{flp} & \cellcolor{gray!6}{-1.92} & \cellcolor{gray!6}{1.59} & \cellcolor{gray!6}{-1.21} & \cellcolor{gray!6}{0.23}\\
flr & -0.79 & 1.57 & -0.50 & 0.61\\
\cellcolor{gray!6}{drvf} & \cellcolor{gray!6}{1.40} & \cellcolor{gray!6}{0.40} & \cellcolor{gray!6}{3.52} & \cellcolor{gray!6}{0.00}\\
\addlinespace
drvr & 0.49 & 0.46 & 1.06 & 0.29\\
\cellcolor{gray!6}{cyl:displ} & \cellcolor{gray!6}{0.36} & \cellcolor{gray!6}{0.08} & \cellcolor{gray!6}{4.56} & \cellcolor{gray!6}{0.00}\\
\bottomrule
\end{tabular}
\begin{tablenotes}
\item Standard errors: OLS
\end{tablenotes}
\end{threeparttable}
\end{table}

\begin{Shaded}
\begin{Highlighting}[]

\FunctionTok{interact\_plot}\NormalTok{(}
\NormalTok{    fitc,}
    \AttributeTok{pred =}\NormalTok{ displ,}
    \AttributeTok{modx =}\NormalTok{ cyl,}
    \CommentTok{\# the observed data is based on displ, cyl, and model error}
    \AttributeTok{partial.residuals =} \ConstantTok{TRUE}\NormalTok{, }
    \AttributeTok{modx.values =} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{8}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{17-moderation_files/figure-latex/unnamed-chunk-22-1} \end{center}

Check linearity assumption in the model

Plot the lines based on the subsample (red line), and whole sample (black line)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x\_2 }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\AttributeTok{n =} \DecValTok{200}\NormalTok{, }\AttributeTok{min =} \SpecialCharTok{{-}}\DecValTok{3}\NormalTok{, }\AttributeTok{max =} \DecValTok{3}\NormalTok{)}
\NormalTok{w   }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(}\AttributeTok{n =} \DecValTok{200}\NormalTok{, }\AttributeTok{size =} \DecValTok{1}\NormalTok{, }\AttributeTok{prob =} \FloatTok{0.5}\NormalTok{)}
\NormalTok{err }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{200}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{4}\NormalTok{)}
\NormalTok{y\_2 }\OtherTok{\textless{}{-}} \FloatTok{2.5} \SpecialCharTok{{-}}\NormalTok{ x\_2 }\SpecialCharTok{\^{}} \DecValTok{2} \SpecialCharTok{{-}} \DecValTok{5} \SpecialCharTok{*}\NormalTok{ w }\SpecialCharTok{+} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ w }\SpecialCharTok{*}\NormalTok{ (x\_2 }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ err}

\NormalTok{data\_2 }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(x\_2, y\_2, w))}

\NormalTok{model\_2 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y\_2 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x\_2 }\SpecialCharTok{*}\NormalTok{ w, }\AttributeTok{data =}\NormalTok{ data\_2)}
\FunctionTok{summ}\NormalTok{(model\_2)}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]
\centering
\begin{tabular}{lr}
\toprule
\cellcolor{gray!6}{Observations} & \cellcolor{gray!6}{200}\\
Dependent variable & y\_2\\
\cellcolor{gray!6}{Type} & \cellcolor{gray!6}{OLS linear regression}\\
\bottomrule
\end{tabular}
\end{table} \begin{table}[!h]
\centering
\begin{tabular}{lr}
\toprule
\cellcolor{gray!6}{F(3,196)} & \cellcolor{gray!6}{1.40}\\
R & 0.02\\
\cellcolor{gray!6}{Adj. R} & \cellcolor{gray!6}{0.01}\\
\bottomrule
\end{tabular}
\end{table} \begin{table}[!h]
\centering
\begin{threeparttable}
\begin{tabular}{lrrrr}
\toprule
  & Est. & S.E. & t val. & p\\
\midrule
\cellcolor{gray!6}{(Intercept)} & \cellcolor{gray!6}{0.06} & \cellcolor{gray!6}{0.53} & \cellcolor{gray!6}{0.12} & \cellcolor{gray!6}{0.91}\\
x\_2 & 0.55 & 0.31 & 1.79 & 0.07\\
\cellcolor{gray!6}{w} & \cellcolor{gray!6}{0.54} & \cellcolor{gray!6}{0.75} & \cellcolor{gray!6}{0.72} & \cellcolor{gray!6}{0.47}\\
x\_2:w & -0.68 & 0.44 & -1.53 & 0.13\\
\bottomrule
\end{tabular}
\begin{tablenotes}
\item Standard errors: OLS
\end{tablenotes}
\end{threeparttable}
\end{table}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{interact\_plot}\NormalTok{(}
\NormalTok{    model\_2,}
    \AttributeTok{pred =}\NormalTok{ x\_2,}
    \AttributeTok{modx =}\NormalTok{ w,}
    \AttributeTok{linearity.check =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{plot.points =} \ConstantTok{TRUE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{17-moderation_files/figure-latex/unnamed-chunk-23-1} \end{center}

\hypertarget{simple-slopes-analysis}{%
\subsubsection{Simple Slopes Analysis}\label{simple-slopes-analysis}}

\begin{itemize}
\item
  continuous by continuous variable interaction (still work for binary)
\item
  conditional slope of the variable of interest (i.e., the slope of \(X\) when we hold \(M\) constant at a value)
\end{itemize}

Using \texttt{sim\_slopes} it will

\begin{itemize}
\item
  mean-center all variables except the variable of interest
\item
  For moderator that is

  \begin{itemize}
  \item
    Continuous, it will pick mean, and plus/minus 1 SD
  \item
    Categorical, it will use all factor
  \end{itemize}
\end{itemize}

\texttt{sim\_slopes} requires

\begin{itemize}
\item
  A regression model with an interaction term)
\item
  Variable of interest (\texttt{pred\ =})
\item
  Moderator: (\texttt{modx\ =})
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sim\_slopes}\NormalTok{(fiti,}
           \AttributeTok{pred =}\NormalTok{ Illiteracy,}
           \AttributeTok{modx =}\NormalTok{ Murder,}
           \AttributeTok{johnson\_neyman =} \ConstantTok{FALSE}\NormalTok{)}
\CommentTok{\#\textgreater{} SIMPLE SLOPES ANALYSIS }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Slope of Illiteracy when Murder =  5.420973 ({-} 1 SD): }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Est.     S.E.   t val.      p}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}   {-}71.59   268.65    {-}0.27   0.79}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Slope of Illiteracy when Murder =  8.685043 (Mean): }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}      Est.     S.E.   t val.      p}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}   {-}437.12   175.82    {-}2.49   0.02}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Slope of Illiteracy when Murder = 11.949113 (+ 1 SD): }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}      Est.     S.E.   t val.      p}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}   {-}802.66   145.72    {-}5.51   0.00}

\CommentTok{\# plot the coefficients}
\NormalTok{ss }\OtherTok{\textless{}{-}} \FunctionTok{sim\_slopes}\NormalTok{(fiti,}
                 \AttributeTok{pred =}\NormalTok{ Illiteracy,}
                 \AttributeTok{modx =}\NormalTok{ Murder,}
                 \AttributeTok{modx.values =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(ss)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{17-moderation_files/figure-latex/unnamed-chunk-24-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# table }
\NormalTok{ss }\OtherTok{\textless{}{-}} \FunctionTok{sim\_slopes}\NormalTok{(fiti,}
                 \AttributeTok{pred =}\NormalTok{ Illiteracy,}
                 \AttributeTok{modx =}\NormalTok{ Murder,}
                 \AttributeTok{modx.values =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{))}
\FunctionTok{library}\NormalTok{(huxtable)}
\FunctionTok{as\_huxtable}\NormalTok{(ss)}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-24} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l}


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} Value of Murder \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} Slope of Illiteracy \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{1}}->{\huxb{0, 0, 0}{1}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} Value of Murder \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} slope \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} 0.00 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} 535.50 (458.77) \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} 5.00 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} -24.44 (282.48) \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} 10.00 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} -584.38 (152.37)*** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

\hypertarget{johnson-neyman-intervals}{%
\subsubsection{Johnson-Neyman intervals}\label{johnson-neyman-intervals}}

To know all the values of the moderator for which the slope of the variable of interest will be statistically significant, we can use the Johnson-Neyman interval \citep{johnson1936tests}

Even though we kind of know that the alpha level when implementing the Johnson-Neyman interval is not correct \citep{bauer2005probing}, not until recently that there is a correction for the type I and II errors \citep{esarey2018marginal}.

Since Johnson-Neyman inflates the type I error (comparisons across all values of the moderator)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sim\_slopes}\NormalTok{(}
\NormalTok{    fiti,}
    \AttributeTok{pred =}\NormalTok{ Illiteracy,}
    \AttributeTok{modx =}\NormalTok{ Murder,}
    \AttributeTok{johnson\_neyman =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{control.fdr =} \ConstantTok{TRUE}\NormalTok{,}
    \CommentTok{\# correction for type I and II}
    
    \CommentTok{\# include conditional intecepts}
    \CommentTok{\# cond.int = TRUE, }
    
    \AttributeTok{robust =} \StringTok{"HC3"}\NormalTok{,}
    \CommentTok{\# rubust SE}
    
    \CommentTok{\# don\textquotesingle{}t mean{-}centered non{-}focal variables}
    \CommentTok{\# centered = "none",}
    \AttributeTok{jnalpha =} \FloatTok{0.05}
\NormalTok{)}
\CommentTok{\#\textgreater{} JOHNSON{-}NEYMAN INTERVAL }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} When Murder is OUTSIDE the interval [{-}11.70, 8.75], the slope of Illiteracy}
\CommentTok{\#\textgreater{} is p \textless{} .05.}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Note: The range of observed values of Murder is [1.40, 15.10]}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Interval calculated using false discovery rate adjusted t = 2.33 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} SIMPLE SLOPES ANALYSIS }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Slope of Illiteracy when Murder =  5.420973 ({-} 1 SD): }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Est.     S.E.   t val.      p}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}   {-}71.59   256.60    {-}0.28   0.78}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Slope of Illiteracy when Murder =  8.685043 (Mean): }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}      Est.     S.E.   t val.      p}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}   {-}437.12   191.07    {-}2.29   0.03}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Slope of Illiteracy when Murder = 11.949113 (+ 1 SD): }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}      Est.     S.E.   t val.      p}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}   {-}802.66   178.75    {-}4.49   0.00}
\end{Highlighting}
\end{Shaded}

For plotting, we can use \texttt{johnson\_neyman}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{johnson\_neyman}\NormalTok{(fiti,}
               \AttributeTok{pred =}\NormalTok{ Illiteracy,}
               \AttributeTok{modx =}\NormalTok{ Murder,}
               
               \CommentTok{\# correction for type I and II}
               \AttributeTok{control.fdr =} \ConstantTok{TRUE}\NormalTok{, }
               \AttributeTok{alpha =}\NormalTok{ .}\DecValTok{05}\NormalTok{)}
\CommentTok{\#\textgreater{} JOHNSON{-}NEYMAN INTERVAL }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} When Murder is OUTSIDE the interval [{-}22.57, 8.52], the slope of Illiteracy}
\CommentTok{\#\textgreater{} is p \textless{} .05.}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Note: The range of observed values of Murder is [1.40, 15.10]}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Interval calculated using false discovery rate adjusted t = 2.33}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{17-moderation_files/figure-latex/unnamed-chunk-26-1} \end{center}

Note:

\begin{itemize}
\tightlist
\item
  y-axis is the \textbf{conditional slope} of the variable of interest
\end{itemize}

\hypertarget{way-interaction}{%
\subsubsection{3-way interaction}\label{way-interaction}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# fita3 \textless{}{-}}
\CommentTok{\#     lm(rating \textasciitilde{} privileges * critical * learning, }
\CommentTok{\#        data = attitude)}
\CommentTok{\# }
\CommentTok{\# probe\_interaction(}
\CommentTok{\#     fita3,}
\CommentTok{\#     pred = critical,}
\CommentTok{\#     modx = learning,}
\CommentTok{\#     mod2 = privileges,}
\CommentTok{\#     alpha = .1}
\CommentTok{\# )}


\NormalTok{mtcars}\SpecialCharTok{$}\NormalTok{cyl }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{cyl,}
                     \AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"4 cylinder"}\NormalTok{, }\StringTok{"6 cylinder"}\NormalTok{, }\StringTok{"8 cylinder"}\NormalTok{))}
\NormalTok{fitc3 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ hp }\SpecialCharTok{*}\NormalTok{ wt }\SpecialCharTok{*}\NormalTok{ cyl, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{interact\_plot}\NormalTok{(fitc3,}
              \AttributeTok{pred =}\NormalTok{ hp,}
              \AttributeTok{modx =}\NormalTok{ wt,}
              \AttributeTok{mod2 =}\NormalTok{ cyl) }\SpecialCharTok{+}
    \FunctionTok{theme\_apa}\NormalTok{(}\AttributeTok{legend.pos =} \StringTok{"bottomright"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{17-moderation_files/figure-latex/unnamed-chunk-27-1} \end{center}

Johnson-Neyman 3-way interaction

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(survey)}
\FunctionTok{data}\NormalTok{(api)}

\NormalTok{dstrat }\OtherTok{\textless{}{-}} \FunctionTok{svydesign}\NormalTok{(}
    \AttributeTok{id =} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{,}
    \AttributeTok{strata =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ stype,}
    \AttributeTok{weights =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ pw,}
    \AttributeTok{data =}\NormalTok{ apistrat,}
    \AttributeTok{fpc =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ fpc}
\NormalTok{)}

\NormalTok{regmodel3 }\OtherTok{\textless{}{-}}
\NormalTok{    survey}\SpecialCharTok{::}\FunctionTok{svyglm}\NormalTok{(api00 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ avg.ed }\SpecialCharTok{*}\NormalTok{ growth }\SpecialCharTok{*}\NormalTok{ enroll, }\AttributeTok{design =}\NormalTok{ dstrat)}

\FunctionTok{sim\_slopes}\NormalTok{(}
\NormalTok{    regmodel3,}
    \AttributeTok{pred =}\NormalTok{ growth,}
    \AttributeTok{modx =}\NormalTok{ avg.ed,}
    \AttributeTok{mod2 =}\NormalTok{ enroll,}
    \AttributeTok{jnplot =} \ConstantTok{TRUE}
\NormalTok{)}
\CommentTok{\#\textgreater{} \#\#\#\#\#\#\#\#\#\#\#\#\#\#\# While enroll (2nd moderator) =  153.0518 ({-} 1 SD) \#\#\#\#\#\#\#\#\#\#\#\#\#\# }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} JOHNSON{-}NEYMAN INTERVAL }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} When avg.ed is OUTSIDE the interval [2.75, 3.82], the slope of growth is p}
\CommentTok{\#\textgreater{} \textless{} .05.}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Note: The range of observed values of avg.ed is [1.38, 4.44]}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} SIMPLE SLOPES ANALYSIS }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Slope of growth when avg.ed = 2.085002 ({-} 1 SD): }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   Est.   S.E.   t val.      p}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}   1.25   0.32     3.86   0.00}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Slope of growth when avg.ed = 2.787381 (Mean): }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   Est.   S.E.   t val.      p}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}   0.39   0.22     1.75   0.08}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Slope of growth when avg.ed = 3.489761 (+ 1 SD): }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}    Est.   S.E.   t val.      p}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}   {-}0.48   0.35    {-}1.37   0.17}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} \#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\# While enroll (2nd moderator) =  595.2821 (Mean) \#\#\#\#\#\#\#\#\#\#\#\#\#\#\# }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} JOHNSON{-}NEYMAN INTERVAL }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} When avg.ed is OUTSIDE the interval [2.84, 7.83], the slope of growth is p}
\CommentTok{\#\textgreater{} \textless{} .05.}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Note: The range of observed values of avg.ed is [1.38, 4.44]}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} SIMPLE SLOPES ANALYSIS }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Slope of growth when avg.ed = 2.085002 ({-} 1 SD): }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   Est.   S.E.   t val.      p}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}   0.72   0.22     3.29   0.00}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Slope of growth when avg.ed = 2.787381 (Mean): }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   Est.   S.E.   t val.      p}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}   0.34   0.16     2.16   0.03}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Slope of growth when avg.ed = 3.489761 (+ 1 SD): }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}    Est.   S.E.   t val.      p}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}   {-}0.04   0.24    {-}0.16   0.87}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} \#\#\#\#\#\#\#\#\#\#\#\#\#\#\# While enroll (2nd moderator) = 1037.5125 (+ 1 SD) \#\#\#\#\#\#\#\#\#\#\#\#\#\# }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} JOHNSON{-}NEYMAN INTERVAL }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} The Johnson{-}Neyman interval could not be found. Is the p value for your}
\CommentTok{\#\textgreater{} interaction term below the specified alpha?}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} SIMPLE SLOPES ANALYSIS }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Slope of growth when avg.ed = 2.085002 ({-} 1 SD): }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   Est.   S.E.   t val.      p}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}   0.18   0.31     0.58   0.56}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Slope of growth when avg.ed = 2.787381 (Mean): }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   Est.   S.E.   t val.      p}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}   0.29   0.20     1.49   0.14}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Slope of growth when avg.ed = 3.489761 (+ 1 SD): }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   Est.   S.E.   t val.      p}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}   0.40   0.27     1.49   0.14}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{17-moderation_files/figure-latex/unnamed-chunk-28-1} \end{center}

Report

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ss3 }\OtherTok{\textless{}{-}}
    \FunctionTok{sim\_slopes}\NormalTok{(regmodel3,}
               \AttributeTok{pred =}\NormalTok{ growth,}
               \AttributeTok{modx =}\NormalTok{ avg.ed,}
               \AttributeTok{mod2 =}\NormalTok{ enroll)}
\FunctionTok{plot}\NormalTok{(ss3)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{17-moderation_files/figure-latex/unnamed-chunk-29-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{as\_huxtable}\NormalTok{(ss3)}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-29} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l}


\hhline{>{\huxb{0, 0, 0}{1}}->{\huxb{0, 0, 0}{1}}-}
\arrayrulecolor{black}

\multicolumn{2}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textit{enroll = 153} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} Value of avg.ed \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} Slope of growth \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{1}}->{\huxb{0, 0, 0}{1}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} Value of avg.ed \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} slope \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} 2.09 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} 1.25 (0.32)*** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} 2.79 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} 0.39 (0.22)\# \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{1}}->{\huxb{0, 0, 0}{1}}-}
\arrayrulecolor{black}

\multicolumn{2}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textit{enroll = 595.28} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} Value of avg.ed \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} Slope of growth \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{1}}->{\huxb{0, 0, 0}{1}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} 3.49 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} -0.48 (0.35) \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} 2.09 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} 0.72 (0.22)** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} 2.79 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} 0.34 (0.16)* \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{1}}->{\huxb{0, 0, 0}{1}}-}
\arrayrulecolor{black}

\multicolumn{2}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textit{enroll = 1037.51} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} Value of avg.ed \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} Slope of growth \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{1}}->{\huxb{0, 0, 0}{1}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} 3.49 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} -0.04 (0.24) \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} 2.09 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} 0.18 (0.31) \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} 2.79 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} 0.29 (0.20) \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} 3.49 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} 0.40 (0.27) \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

\hypertarget{categorical-interaction}{%
\subsection{Categorical interaction}\label{categorical-interaction}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\NormalTok{mpg2 }\OtherTok{\textless{}{-}}\NormalTok{ mpg }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{cyl =} \FunctionTok{factor}\NormalTok{(cyl))}

\NormalTok{mpg2[}\StringTok{"auto"}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"auto"}
\NormalTok{mpg2}\SpecialCharTok{$}\NormalTok{auto[mpg2}\SpecialCharTok{$}\NormalTok{trans }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"manual(m5)"}\NormalTok{, }\StringTok{"manual(m6)"}\NormalTok{)] }\OtherTok{\textless{}{-}} \StringTok{"manual"}
\NormalTok{mpg2}\SpecialCharTok{$}\NormalTok{auto }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(mpg2}\SpecialCharTok{$}\NormalTok{auto)}
\NormalTok{mpg2[}\StringTok{"fwd"}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"2wd"}
\NormalTok{mpg2}\SpecialCharTok{$}\NormalTok{fwd[mpg2}\SpecialCharTok{$}\NormalTok{drv }\SpecialCharTok{==} \StringTok{"4"}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"4wd"}
\NormalTok{mpg2}\SpecialCharTok{$}\NormalTok{fwd }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(mpg2}\SpecialCharTok{$}\NormalTok{fwd)}
\DocumentationTok{\#\# Drop the two cars with 5 cylinders (rest are 4, 6, or 8)}
\NormalTok{mpg2 }\OtherTok{\textless{}{-}}\NormalTok{ mpg2[mpg2}\SpecialCharTok{$}\NormalTok{cyl }\SpecialCharTok{!=} \StringTok{"5"}\NormalTok{, ]}
\DocumentationTok{\#\# Fit the model}
\NormalTok{fit3 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(cty }\SpecialCharTok{\textasciitilde{}}\NormalTok{ cyl }\SpecialCharTok{*}\NormalTok{ fwd }\SpecialCharTok{*}\NormalTok{ auto, }\AttributeTok{data =}\NormalTok{ mpg2)}

\FunctionTok{library}\NormalTok{(jtools) }\CommentTok{\# for summ()}
\FunctionTok{summ}\NormalTok{(fit3)}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]
\centering
\begin{tabular}{lr}
\toprule
\cellcolor{gray!6}{Observations} & \cellcolor{gray!6}{230}\\
Dependent variable & cty\\
\cellcolor{gray!6}{Type} & \cellcolor{gray!6}{OLS linear regression}\\
\bottomrule
\end{tabular}
\end{table} \begin{table}[!h]
\centering
\begin{tabular}{lr}
\toprule
\cellcolor{gray!6}{F(11,218)} & \cellcolor{gray!6}{61.37}\\
R & 0.76\\
\cellcolor{gray!6}{Adj. R} & \cellcolor{gray!6}{0.74}\\
\bottomrule
\end{tabular}
\end{table} \begin{table}[!h]
\centering
\begin{threeparttable}
\begin{tabular}{lrrrr}
\toprule
  & Est. & S.E. & t val. & p\\
\midrule
\cellcolor{gray!6}{(Intercept)} & \cellcolor{gray!6}{21.37} & \cellcolor{gray!6}{0.39} & \cellcolor{gray!6}{54.19} & \cellcolor{gray!6}{0.00}\\
cyl6 & -4.37 & 0.54 & -8.07 & 0.00\\
\cellcolor{gray!6}{cyl8} & \cellcolor{gray!6}{-8.37} & \cellcolor{gray!6}{0.67} & \cellcolor{gray!6}{-12.51} & \cellcolor{gray!6}{0.00}\\
fwd4wd & -2.91 & 0.76 & -3.83 & 0.00\\
\cellcolor{gray!6}{automanual} & \cellcolor{gray!6}{1.45} & \cellcolor{gray!6}{0.57} & \cellcolor{gray!6}{2.56} & \cellcolor{gray!6}{0.01}\\
\addlinespace
cyl6:fwd4wd & 0.59 & 0.96 & 0.62 & 0.54\\
\cellcolor{gray!6}{cyl8:fwd4wd} & \cellcolor{gray!6}{2.13} & \cellcolor{gray!6}{0.99} & \cellcolor{gray!6}{2.15} & \cellcolor{gray!6}{0.03}\\
cyl6:automanual & -0.76 & 0.90 & -0.84 & 0.40\\
\cellcolor{gray!6}{cyl8:automanual} & \cellcolor{gray!6}{0.71} & \cellcolor{gray!6}{1.18} & \cellcolor{gray!6}{0.60} & \cellcolor{gray!6}{0.55}\\
fwd4wd:automanual & -1.66 & 1.07 & -1.56 & 0.12\\
\addlinespace
\cellcolor{gray!6}{cyl6:fwd4wd:automanual} & \cellcolor{gray!6}{1.29} & \cellcolor{gray!6}{1.52} & \cellcolor{gray!6}{0.85} & \cellcolor{gray!6}{0.40}\\
cyl8:fwd4wd:automanual & -1.39 & 1.76 & -0.79 & 0.43\\
\bottomrule
\end{tabular}
\begin{tablenotes}
\item Standard errors: OLS
\end{tablenotes}
\end{threeparttable}
\end{table}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat\_plot}\NormalTok{(fit3,}
         \AttributeTok{pred =}\NormalTok{ cyl,}
         \AttributeTok{modx =}\NormalTok{ fwd,}
         \AttributeTok{plot.points =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{17-moderation_files/figure-latex/unnamed-chunk-31-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#line plots}
\FunctionTok{cat\_plot}\NormalTok{(}
\NormalTok{    fit3,}
    \AttributeTok{pred =}\NormalTok{ cyl,}
    \AttributeTok{modx =}\NormalTok{ fwd,}
    \AttributeTok{geom =} \StringTok{"line"}\NormalTok{,}
    \AttributeTok{point.shape =} \ConstantTok{TRUE}\NormalTok{,}
    \CommentTok{\# colors = "Set2", \# choose color}
    \AttributeTok{vary.lty =} \ConstantTok{TRUE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{17-moderation_files/figure-latex/unnamed-chunk-31-2} \end{center}

\begin{Shaded}
\begin{Highlighting}[]


\CommentTok{\# bar plot}
\FunctionTok{cat\_plot}\NormalTok{(}
\NormalTok{    fit3,}
    \AttributeTok{pred =}\NormalTok{ cyl,}
    \AttributeTok{modx =}\NormalTok{ fwd,}
    \AttributeTok{geom =} \StringTok{"bar"}\NormalTok{,}
    \AttributeTok{interval =}\NormalTok{ T,}
    \AttributeTok{plot.points =} \ConstantTok{TRUE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{17-moderation_files/figure-latex/unnamed-chunk-31-3} \end{center}

\hypertarget{interactionr-package}{%
\section{interactionR package}\label{interactionr-package}}

\begin{itemize}
\tightlist
\item
  For publication purposes
\item
  Following

  \begin{itemize}
  \item
    \citep{knol2012recommendations} for presentation
  \item
    \citep{hosmer1992confidence} for confidence intervals based on the delta method
  \item
    \citep{zou2008estimation} for variance recovery ``mover'' method
  \item
    \citep{assmann1996confidence} for bootstrapping
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"interactionR"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{sjplot-package}{%
\section{sjPlot package}\label{sjplot-package}}

\begin{itemize}
\item
  For publication purposes (recommend, but more advanced)
\item
  \href{https://strengejacke.github.io/sjPlot/articles/plot_interactions.html}{link}
\end{itemize}

\hypertarget{part-iv.-causal-inference}{%
\part*{IV. CAUSAL INFERENCE}\label{part-iv.-causal-inference}}
\addcontentsline{toc}{part}{IV. CAUSAL INFERENCE}

\hypertarget{causal-inference}{%
\chapter{Causal Inference}\label{causal-inference}}

After all of the mambo jumbo that we have learned so far, I want to now talk about the concept of causality. We usually say that correlation is not causation. Then, what is causation?\\
One of my favorite books has explained this concept beautifully \citep{Pearl_2018}. And I am just going to quickly summarize the gist of it from my understanding. I hope that it can give you an initial grasp on the concept so that later you can continue to read up and develop a deeper understanding.

It's important to have a deep understanding regarding the method research. However, one needs to be aware of its limitation. As mentioned in various sections throughout the book, we see that we need to ask experts for number as our baseline or visit literature to gain insight from past research.

Here, we dive in a more conceptual side statistical analysis as a whole, regardless of particular approach.

You probably heard scientists say correlation doesn't mean causation. There are ridiculous \href{http://www.tylervigen.com/spurious-correlations}{spurious correlations} that give a firm grip on what the previous phrase means. The pioneer who tried to use regression to infer causation in social science was \citet{yule1899} (but it was a fatal attempt where he found relief policy increases poverty). To make a causal inference from statistics, \textbf{the equation (function form) must be stable} under intervention (i.e., variables are manipulated). Statistics is used to be a causality-free enterprise in the past.

Not until the development of path analysis by Sewall Wright in the 1920s that the discipline started to pay attention to causation. Then, it remained dormant until the Causal Revolution (quoted Judea Pearl's words). This revolution introduced the calculus of causation which includes (1) causal diagrams), and (2) a symbolic language

The world has been using \(P(Y|X)\) (statistics use to derive this), but what we want is to compare the difference between

\begin{itemize}
\item
  \(P(Y|do(X))\): treatment group
\item
  \(P(Y|do(not-X))\): control group
\end{itemize}

Hence, we can see a clear difference between \(P(Y|X) \neq P(Y|do(X))\)

The conclusion we want to make from data is counterfactuals: \textbf{What would have happened had we not do X?}

To teach a robot to make inference, we need inference engine

\begin{figure}
\centering
\includegraphics[width=6.25in,height=4.16667in]{images/Figure I.png}
\caption{p.~12 \citep{Pearl_2018}}
\end{figure}

Levels of cognitive ability to be a causal learner:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Seeing
\item
  Doing
\item
  Imagining
\end{enumerate}

Ladder of causation (associated with levels of cognitive ability as well):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Association: conditional probability, correlation, regression
\item
  Intervention
\item
  Counterfactuals
\end{enumerate}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1295}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1007}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3165}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.4388}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Level
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Activity
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Questions
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Examples
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Association

\(P(y|x)\) & Seeing & What is?

How would seeing X change my belief in Y? & What does a symptom tell me about a disease? \\
Intervention

\(P(y|do(x),z)\) & Doing

Intervening & What if?

What if I do X? & What if I spend more time learning, will my result change? \\
Counterfactuals

\(P(y_x|x',y')\) & Imagining & \begin{minipage}[t]{\linewidth}\raggedright
Why?\\
was it X that caused Y?

What if I had acted differently\strut
\end{minipage} & What if I stopped smoking a year ago? \\
\end{longtable}

Table by \citep[p.~57]{pearl2019seven}

You cannot define causation from probability alone

If you say X causes Y if X raises the probability of Y.'' On the surface, it might sound intuitively right. But when we translate it to probability notation: \(P(Y|X) >P(Y)\) , it can't be more wrong. Just because you are seeing X (1st level), it \textbf{doesn't mean} the probability of Y increases.

It could be either that (1) X causes Y, or (2) Z affects both X and Y. Hence, people might use \textbf{control variables}, which translate: \(P(Y|X, Z=z) > P(Y|Z=z)\), then you can be more confident in your probabilistic observation. However, the question is how can you choose \(Z\)

With the invention of the do-operator, now you can represent X causes Y as

\[
P(Y|do(X)) > P(Y)
\]

and with the help of causal diagram, now you can answer questions at the 2nd level (Intervention)

Note: people under econometrics might still use ``Granger causality'' and ``vector autoregression'' to use the probability language to represent causality (but it's not).

The 7 tools for Structural Causal Model framework \citep{pearl2019seven}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Encoding Causal Assumptions - transparency and testability (with graphical representation)
\item
  Do-calculus and the control of confounding: ``back-door''
\item
  The algorithmization of Counterfactuals
\item
  Mediation Analysis and the Assessment of Direct and Indirect Effects
\item
  Adaptability, External validity and Sample Selection Bias: are still researched under ``domain adaptation'', ``transfer learning''
\item
  Recovering from missing data
\item
  Causal Discovery:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    d-separation
  \item
    Functional decomposition \citep{hoyer2008nonlinear}
  \item
    Spontaneous local changes \citep{pearl2014graphical}
  \end{enumerate}
\end{enumerate}

\href{https://cran.r-project.org/web/views/CausalInference.html}{List of packages to do causal inference} in R

Simpson's Paradox:

\begin{itemize}
\tightlist
\item
  A statistical association seen in an entire population is reversed in sub-population.
\end{itemize}

Structural Causal Model accompanies graphical causal model to create more efficient language to represent causality

Structural Causal Model is the solution to the curse of dimensionality (i.e., large numbers of variable \(p\), and small dataset \(n\)) thanks to product decomposition. It allows us to solve problems without knowing the function, parameters, or distributions of the error terms.

Suppose you have a causal chain \(X \to Y \to Z\):

\[
P(X=x,Y=y, Z=z) = P(X=x)P(Y=y|X=x)P(Z=z|Y=y)
\]

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.4949}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5051}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Experimental Design
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Quasi-experimental Design
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Experimentalist & Observationalist \\
Experimental Data & Observational Data \\
Random Assignment (reduce treatment imbalance) & Random Sampling (reduce sample selection error) \\
\end{longtable}

Criticisms of quasi-experimental versus experimental designs:

\begin{itemize}
\item
  Quasi-experimental methods don't approximate well experimental results. For example,

  \begin{itemize}
  \tightlist
  \item
    \citet{lalonde1986evaluating} shows \protect\hyperlink{matching-methods}{Matching Methods}, \protect\hyperlink{difference-in-differences}{Difference-in-differences}, \protect\hyperlink{tobit-2}{Tobit-2} (Heckman-type) can't approximate the experimental estimates.
  \end{itemize}
\end{itemize}

Tools in a hierarchical order

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \protect\hyperlink{experimental-design}{Experimental Design}: Randomized Control Trials (Gold standard): Tier 1
\item
  \protect\hyperlink{quasi-experimental}{Quasi-experimental}

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    \protect\hyperlink{regression-discontinuity}{Regression Discontinuity}
  \item
    \protect\hyperlink{synthetic-difference-in-differences}{Synthetic Difference-in-Differences}
  \item
    \protect\hyperlink{difference-in-differences}{Difference-In-Differences}
  \item
    \protect\hyperlink{synthetic-control}{Synthetic Control}
  \item
    \protect\hyperlink{event-studies}{Event Studies}
  \item
    Fixed Effects Estimator \ref{fixed-effects-estimator}
  \item
    \protect\hyperlink{endogenous-treatment}{Endogenous Treatment}: mostly \protect\hyperlink{instrumental-variables}{Instrumental Variables}
  \item
    \protect\hyperlink{matching-methods}{Matching Methods}
  \item
    \protect\hyperlink{interrupted-time-series}{Interrupted Time Series}
  \item
    Endogenous Sample Selection \ref{endogenous-sample-selection}: mostly Heckman's correction
  \end{enumerate}
\end{enumerate}

Internal vs.~External Validity

\begin{itemize}
\item
  Internal Validity: Economists and applied scientists largely care about.
\item
  External Validity: Localness might affect your external validity.
\end{itemize}

For many economic policies, there is a difference between \textbf{treatment} and \textbf{intention to treat}.

For example, we might have an effective vaccine (i.e., intention to treat), but it does not mean that everybody will take it (i.e., treatment).

There are four types of subjects that we deal with:

\begin{itemize}
\item
  \textbf{Non-switchers}: we don't care about non-switchers because even if we introduce or don't introduce the intervention, it won't affect them.

  \begin{itemize}
  \item
    \textbf{Always takers}
  \item
    \textbf{Never takers}
  \end{itemize}
\item
  \textbf{Switchers}

  \begin{itemize}
  \item
    \textbf{Compliers}: defined as those who respect the intervention.

    \begin{itemize}
    \item
      We only care about compliers because when we introduce the intervention, they will do something. When we don't have any interventions, they won't do it.
    \item
      Tools above are used to identify the causal impact of an intervention on compliers
    \item
      If we have only \textbf{compliers} in our dataset, then \textbf{intention to treatment = treatment effect}.
    \end{itemize}
  \item
    \textbf{Defiers}: those who will go to the opposite direction of your treatment.

    \begin{itemize}
    \tightlist
    \item
      We typically aren't interested in defiers because they will do the opposite of what we want them to do. And they are typically a small group; hence, we just assume they don't exist.
    \end{itemize}
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
& Treatment Assignment & Control Assignment \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Compliers & Treated & No Treated \\
Always-takers & Treated & Treated \\
Never-takers & Not treated & No treated \\
Defiers & Not treated & Treated \\
\end{longtable}

Directional Bias due to selection into treatment comes from 2 general opposite sources

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Mitigation-based}: select into treatment to combat a problem
\item
  \textbf{Preference-based}: select into treatment because units like that kind of treatment.
\end{enumerate}

\hypertarget{treatment-effect-types}{%
\section{Treatment effect types}\label{treatment-effect-types}}

This section is based on \href{https://egap.org/resource/10-types-of-treatment-effect-you-should-know-about/}{Paul Testa's note}

Terminology:

\begin{itemize}
\item
  Quantities of causal interest (i.e., treatment effect types)
\item
  Estimands: parameters of interest
\item
  Estimators: procedures to calculate hesitates for the parameters of interest
\end{itemize}

Sources of bias (\href{https://www.youtube.com/watch?v=CjZnQ3ToJjg}{according to prof. Luke Keele})

\[
\begin{aligned}
&\text{Estimator - True Causal Effect} \\
&= \text{Hidden bias + Misspecification bias + Statistical Noise} \\
&= \text{Due to design + Due to modeling + Due to finite sample}
\end{aligned}
\]

\hypertarget{average-treatment-effects}{%
\subsection{Average Treatment Effects}\label{average-treatment-effects}}

Average treatment effect (ATE) is the difference in means of the treated and control groups

\textbf{Randomization} under \protect\hyperlink{experimental-design}{Experimental Design} can provide an unbiased estimate of ATE.

Let \(Y_i(1)\) denote the outcome of individual \(i\) under treatment and

\(Y_i(0)\) denote the outcome of individual \(i\) under control

Then, the treatment effect for individual \(i\) is the difference between her outcome under treatment and control

\[
\tau_i = Y_i(1) - Y_i(0)
\]

Without a time machine or dimension portal, we can only observe one of the two event: either individual \(i\) experiences the treatment or she doesn't.

Then, the ATE as a quantity of interest can come in handy since we can observe across all individuals

\[
ATE = \frac{1}{N} \sum_{i=1}^N \tau_i = \frac{\sum_1^N Y_i(1)}{N} - \frac{\sum_i^N Y_i(0)}{N}
\]

With random assignment (i.e., treatment assignment is independent of potential outcome and observables and unobservables), the observed means difference between the two groups is an unbiased estimator of the average treatment effect

\[
E(Y_i (1) |D = 1) = E(Y_i(1)|D=0) = E(Y_i(1)) \\
E(Y_i(0) |D = 1) = E(Y_i(0)|D = 0 ) = E(Y_i(0))
\]

\[
ATE = E(Y_i(1)) - E(Y_i(0))
\]

Alternatively, we can write the potential outcomes model in a regression form

\[
Y_i = Y_i(0)  + [Y_i (1) - Y_i(0)] D_i
\]

Let \(\beta_{0i} = Y_i (0) ; \beta_{1i} = Y_i(1) - Y_i(0)\), we have

\[
Y_i = \beta_{0i} + \beta_{1i} D_i
\]

where

\begin{itemize}
\item
  \(\beta_{0i}\) = outcome if the unit did not receive any treatment
\item
  \(\beta_{1i}\) = treatment effect (i.e., random coefficients for each unit \(i\))
\end{itemize}

To understand endogeneity (i.e., nonrandom treatment assignment), we can examine a standard linear model

\[
\begin{aligned}
Y_i &= \beta_{0i} + \beta_{1i} D_i \\
&= ( \bar{\beta}_{0} + \epsilon_{0i} ) + (\bar{\beta}_{1} + \epsilon_{1i} )D_i \\
&=  \bar{\beta}_{0} + \epsilon_{0i} + \bar{\beta}_{1} D_i + \epsilon_{1i} D_i
\end{aligned}
\]

When you have random assignment, \(E(\epsilon_{0i}) = E(\epsilon_{1i}) = 0\)

\begin{itemize}
\item
  No selection bias: \(D_i \perp e_{0i}\)
\item
  Treatment effect is independent of treatment assignment: \(D_i \perp e_{1i}\)
\end{itemize}

But otherwise, residuals can correlate with \(D_i\)

For estimation,

\begin{itemize}
\item
  \(\hat{\beta}_1^{OLS}\) is identical to difference in means (i.e., \(Y_i(1) - Y_i(0)\))
\item
  In case of heteroskedasticity (i.e., \(\epsilon_{0i} + D_i \epsilon_{1i} \neq 0\) ), this residual's variance depends on \(X\) when you have heterogeneous treatment effects (i.e., \(\epsilon_{1i} \neq 0\))

  \begin{itemize}
  \item
    Robust SE should still give consistent estimate of \(\hat{\beta}_1\) in this case
  \item
    Alternatively, one can use two-sample t-test on difference in means with unequal variances.
  \end{itemize}
\end{itemize}

\hypertarget{conditional-average-treatment-effects}{%
\subsection{Conditional Average Treatment Effects}\label{conditional-average-treatment-effects}}

Treatment effects can be different for different groups of people. In words, treatment effects can vary across subgroups.

To examine the heterogeneity across groups (e.g., men vs.~women), we can estimate the conditional average treatment effects (CATE) for each subgroup

\[
CATE = E(Y_i(1) - Y_i(0) |D_i, X_i))
\]

\hypertarget{intent-to-treat-effects}{%
\subsection{Intent-to-treat Effects}\label{intent-to-treat-effects}}

When we encounter non-compliance (either people suppose to receive treatment don't receive it, or people suppose to be in the control group receive the treatment), treatment receipt is not independent of potential outcomes and confounders.

In this case, the difference in observed means between the treatment and control groups is not \protect\hyperlink{average-treatment-effects}{Average Treatment Effects}, but \protect\hyperlink{intent-to-treat-effects}{Intent-to-treat Effects} (ITT). In words, ITT is the treatment effect on those who \textbf{receive} the treatment

\hypertarget{local-average-treatment-effects}{%
\subsection{Local Average Treatment Effects}\label{local-average-treatment-effects}}

Instead of estimating the treatment effects of those who \textbf{receive} the treatment (i.e., \protect\hyperlink{intent-to-treat-effects}{Intent-to-treat Effects}), you want to estimate the treatment effect of those who actually \textbf{comply} with the treatment. This is the local average treatment effects (LATE) or complier average causal effects (CACE). I assume we don't use CATE to denote complier average treatment effect because it was reserved for conditional average treatment effects.

\begin{itemize}
\tightlist
\item
  Using random treatment assignment as an instrument, we can recover the effect of treatment on compliers.
\end{itemize}

\includegraphics[width=6.25in,height=3.75in]{images/iv_late.PNG}

\begin{itemize}
\item
  As the percent of compliers increases, \protect\hyperlink{intent-to-treat-effects}{Intent-to-treat Effects} and \protect\hyperlink{local-average-treatment-effects}{Local Average Treatment Effects} converge
\item
  Rule of thumb: SE(LATE) = SE(ITT)/(share of compliers)
\item
  LATE estimate is always greater than the ITT estimate
\item
  LATE can also be estimated using a pure placebo group \citep{gerber2010}.
\item
  Partial compliance is hard to study, and IV/2SLS estimator is biased, we have to use Bayesian \citep{long2010, jin2009, jin2008}.
\end{itemize}

\hypertarget{one-sided-noncompliance}{%
\subsubsection{One-sided noncompliance}\label{one-sided-noncompliance}}

\begin{itemize}
\item
  One-sided noncompliance is when in the sample, we only have compliers and never-takers
\item
  With the exclusion restriction (i.e., excludability), never-takers have the same results in the treatment or control group (i.e., never treated)
\item
  With random assignment, we can have the same number of never-takers in the treatment and control groups
\item
  Hence,
\end{itemize}

\[
LATE = \frac{ITT}{\text{share of compliers}}
\]

\hypertarget{two-sided-noncompliance}{%
\subsubsection{Two-sided noncompliance}\label{two-sided-noncompliance}}

\begin{itemize}
\item
  Two-sided noncompliance is when in the sample, we have compliers, never-takers, and always-takers
\item
  To estimate LATE, beyond excludability like in the \protect\hyperlink{one-sided-noncompliance}{One-sided noncompliance} case, we need to assume that there is no defiers (i.e., monotonicity assumption) (this is excusable in practical studies)
\end{itemize}

\[
LATE = \frac{ITT}{\text{share of compliers}}
\]

\hypertarget{population-vs.-sample-average-treatment-effects}{%
\subsection{Population vs.~Sample Average Treatment Effects}\label{population-vs.-sample-average-treatment-effects}}

See \citep{imai2008} for when the sample average treatment effect (SATE) diverges from the population average treatment effect (PATE).

To stay consistent, this section uses notations from \citep{imai2008}'s paper.

In a finite population \(N\), we observe \(n\) observations (\(N>>n\)), where half is in the control and half is in the treatment group.

With unknown data generating process, we have

\[
I_i = 
\begin{cases}
1 \text{ if unit i is in the sample} \\
0 \text{ otherwise}
\end{cases}
\]

\[
T_i = 
\begin{cases}
1 \text{ if unit i is in the treatment group} \\
0 \text{ if unit i is in the control group}
\end{cases}
\]

\[
\text{potential outcome} = 
\begin{cases}
Y_i(1) \text{ if } T_i = 1 \\
Y_i(0) \text{ if } T_i = 0
\end{cases}
\]

Observed outcome is

\[
Y_i | I_i = 1= T_i Y_i(1) + (1-T_i)Y_i(0)
\]

Since we can never observed both outcome for the same individual, the treatment effect is always unobserved for unit \(i\)

\[
TE_i = Y_i(1) - Y_i(0)
\]

Sample average treatment effect is

\[
SATE = \frac{1}{n}\sum_{i \in \{I_i = 1\}} TE_i
\]

Population average treatment effect is

\[
PATE = \frac{1}{N}\sum_{i=1}^N TE_i
\]

Let \(X_i\) be observables and \(U_i\) be unobservables for unit \(i\)

The baseline estimator for SATE and PATE is

\[
\begin{aligned}
D &= \frac{1}{n/2} \sum_{i \in (I_i = 1, T_i = 1)} Y_i - \frac{1}{n/2} \sum_{i \in (I_i = 1 , T_i = 0)} Y_i \\
&= \text{observed sample mean of the treatment group} \\
&- \text{observed sample mean of the control group}
\end{aligned}
\]

Let \(\Delta\) be the estimation error (deviation from the truth), under an additive model

\[
Y_i(t) = g_t(X_i) + h_t(U_i)
\]

The decomposition of the estimation error is

\[
\begin{aligned}
PATE - D = \Delta &= \Delta_S + \Delta_T \\
&= (PATE - SATE) + (SATE - D)\\
&= \text{sample selection}+ \text{treatment imbalance} \\
&= (\Delta_{S_X} + \Delta_{S_U}) + (\Delta_{T_X} + \Delta_{T_U}) \\
&= \text{(selection on observed + selection on unobserved)} \\
&+ (\text{treatment imbalance in observed + unobserved})
\end{aligned}
\]

\hypertarget{estimation-error-from-sample-selection}{%
\subsubsection{Estimation Error from Sample Selection}\label{estimation-error-from-sample-selection}}

Also known as sample selection error

\[
\Delta_S = PATE - SATE = \frac{N - n}{N}(NATE - SATE)
\]

where NATE is the non-sample average treatment effect (i.e., average treatment effect for those in the population but not in your sample:

\[
NATE = \sum_{i\in (I_i = 0)} \frac{TE_i}{N-n}
\]

From the equation, to have zero sample selection error (i.e., \(\Delta_S = 0\)), we can either

\begin{itemize}
\item
  Get \(N = n\) by redefining your sample as the population of interest
\item
  \(NATE = SATE\) (e.g., \(TE_i\) is constant over \(i\) in both your selected sample, and those in the population that you did not select)
\end{itemize}

Note

\begin{itemize}
\item
  When you have heterogeneous treatment effects, \textbf{random sampling} can only warrant \textbf{sample selection bias}, not \textbf{sample selection error}.
\item
  Since we can rarely know the true underlying distributions of the observables (\(X\)) and unobservables (\(U\)), we cannot verify whether the empirical distributions of your observables and unobservables for those in your sample is identical to that of your population (to reduce \(\Delta_S\)). For special case,

  \begin{itemize}
  \item
    Say you have census of your population, you can adjust for the observables \(X\) to reduce \(\Delta_{S_X}\), but still you cannot adjust your unobservables (\(U\))
  \item
    Say you are willing to assume \(TE_i\) is constant over

    \begin{itemize}
    \item
      \(X_i\), then \(\Delta_{S_X} = 0\)
    \item
      \(U_i\), then \(\Delta_{U}=0\)
    \end{itemize}
  \end{itemize}
\end{itemize}

\hypertarget{estimation-error-from-treatment-imbalance}{%
\subsubsection{Estimation Error from Treatment Imbalance}\label{estimation-error-from-treatment-imbalance}}

Also known as treatment imbalance error

\[
\Delta_T = SATE - D
\]

\(\Delta_T \to 0\) when treatment and control groups are balanced (i.e., identical empirical distributions) for both observables (\(X\)) and unobservables (\(U\))

However, in reality, we can only readjust for observables, not unobservables.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1289}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3701}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4990}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\protect\hyperlink{randomized-block-designs}{Blocking}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{\protect\hyperlink{matching-methods}{Matching Methods}}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Definition & Random assignment within strata based on pre-treatment observables & Dropping, repeating or grouping observations to balance covariates between the treatment and control group \citep{rubin1973use} \\
Time & Before randomization of treatments & After randomization of treatments \\
What if the set of covariates used to adjust is irrelevant? & Nothing happens & In the worst case scenario (e.g., these variables are uncorrelated with the treatment assignment, but correlated with the post-treatment variables), matching induces bias that is greater than just using the unadjusted difference in means \\
Benefits & \(\Delta_{T_X}=0\) (no imbalance on observables). But we don't know its effect on unobservables imbalance (might reduce if the unobservables are correlated with the observables) & Reduce model dependence, bias, variance, mean-square error \\
\end{longtable}

\hypertarget{average-treatment-effects-on-the-treated-and-control}{%
\subsection{Average Treatment Effects on the Treated and Control}\label{average-treatment-effects-on-the-treated-and-control}}

Average Effect of treatment on the Treated (ATT) is

\[
\begin{aligned}
ATT &= E(Y_i(1) - Y_i(0)|D_i = 1) \\
&= E(Y_i(1)|D_i = 1) - E(Y_i(0) |D_i = 1)
\end{aligned}
\]

Average Effect of treatment on the Control (ATC) (i.e., the effect \textbf{would be} for those weren't treated) is

\[
\begin{aligned}
ATC &= E(Y_i(1) - Y_i (0) |D_i =0) \\
&= E(Y_i(1)|D_i = 0) - E(Y_i(0)|D_i = 0)
\end{aligned}
\]

Under random assignment and full compliance,

\[
ATE = ATT = ATC
\]

\textbf{Sample average treatment effect on the treated} is

\[
SATT = \frac{1}{n} \sum_i TE_i
\]

where

\begin{itemize}
\item
  \(TE_i\) is the treatment effect for unit \(i\)
\item
  \(n\) is the number of treated units in the sample
\item
  \(i\) belongs the subset (i.e., sample) of the population of interest that is treated.
\end{itemize}

\textbf{Population average treatment effect on the treated} is

\[
PATT = \frac{1}{N} \sum_i TE_i
\]

where

\begin{itemize}
\item
  \(TE_i\) is the treatment effect for unit \(i\)
\item
  \(N\) is the number of treated units in the population
\item
  \(i\) belongs to the population of interest that is treated.
\end{itemize}

\hypertarget{quantile-average-treatment-effects}{%
\subsection{Quantile Average Treatment Effects}\label{quantile-average-treatment-effects}}

Instead of the middle point estimate (ATE), we can also understand the changes in the distribution the outcome variable due to the treatment.

Using quantile regression and more assumptions \citep{abadie2002instrumental, chernozhukov2005iv}, we can have consistent estimate of quantile treatment effects (QTE), with which we can make inference regarding a given quantile.

\hypertarget{mediation-effects}{%
\subsection{Mediation Effects}\label{mediation-effects}}

With additional assumptions (i.e., sequential ignorability \citep{imai2010general, bullock2011mediation}), we can examine the mechanism of the treatment on the outcome.

Under the causal framework,

\begin{itemize}
\item
  the indirect effect of treatment via a mediator is called average causal mediation effect (ACME)
\item
  the direct effect of treatment on outcome is the average direct effect (ADE)
\end{itemize}

More in the \protect\hyperlink{mediation}{Mediation} Section \ref{mediation}

\hypertarget{log-odds-treatment-effects}{%
\subsection{Log-odds Treatment Effects}\label{log-odds-treatment-effects}}

For binary outcome variable, we might be interested in the log-odds of success. See \citep{freedman2008randomization} on how to estimate a consistent causal effect.

Alternatively, attributable effects \citep{rosenbaum2002attributing} can also be appropriate for binary outcome.

\hypertarget{part-a.-experimental-design}{%
\part*{A. EXPERIMENTAL DESIGN}\label{part-a.-experimental-design}}
\addcontentsline{toc}{part}{A. EXPERIMENTAL DESIGN}

\hypertarget{experimental-design}{%
\chapter{Experimental Design}\label{experimental-design}}

\begin{itemize}
\tightlist
\item
  Randomized Control Trials (RCT) or Experiments have always been and are likely to continue in the future to be the holy grail of causal inference, because of

  \begin{itemize}
  \item
    unbiased estimates
  \item
    elimination of confounding factors on average (covariate imbalance is always possible. Hence, you want to do \protect\hyperlink{rerandomization}{Rerandomization} to achieve platinum standard set by \citep{tukey1993tightening})
  \end{itemize}
\item
  RCT means you have two group treatment (or experimental) gorp and control group. Hence, as you introduce the treatment (your exogenous variable) to the treatment group, the only expected difference in the outcomes of the two group should be due to the treatment.
\item
  Subjects from the same population will be \textbf{randomly assigned} to either treatment or control group. This random assignment give us the confidence that changes in the outcome variable will be due only the treatment, not any other source (variable).
\item
  It can be easier for hard science to have RCT because they can introduce the treatment, and have control environments. But it's hard for social scientists because their subjects are usually human, and some treatment can be hard to introduce, or environments are uncontrollable. Hence, social scientists have to develop different tools (\protect\hyperlink{quasi-experimental}{Quasi-experimental}) to recover causal inference or to recreate the treatment and control group environment.
\item
  With RCT, you can easily establish internal validity
\item
  Even though random assignment is not the same thing as \emph{ceteris paribus} (i.e., holding everything else constant), it should have the same effect (i.e., under random manipulation, \emph{other things equal} can be observed, on average, across treatment and control groups).
\end{itemize}

\textbf{Selection Problem}

Assume we have

\begin{itemize}
\item
  binary treatment \(D_i =(0,1)\)
\item
  outcome of interest \(Y_i\) for individual \(i\)

  \begin{itemize}
  \item
    \(Y_{0i}\) are those were \textbf{not treated}
  \item
    \(Y_{1i}\) are those were \textbf{treated}
  \end{itemize}
\end{itemize}

\[
\text{Potential Outcome} =
\begin{cases}
Y_{1i} \text{ if } D_i = 1 \\
Y_{0i} \text{ if } D_i = 0
\end{cases}
\]

Then, what we observe in the outcome variable is

\[
Y_i = Y_{0i} + (Y_{1i} - Y_{0i})D_i
\]

It's likely that \(Y_{1i}\) and \(Y_{0i}\) both have their own distributions (i.e., different treatment effect for different people). Since we can't see both outcomes for the same individual (unless we have a time machine), then we can only make inference regarding the average outcome of those who were treated and those who were not.

\[
\begin{aligned}
E[Y_i | D_i = 1] - E[Y_i | D_i = 0] &= (E[Y_{1i} | D_i = 1] - E[Y_{0i}|D_i = 1] ) + (E[Y_{0i} |D_i = 1] - E[Y_{0i} |D_i = 0]) \\
&= (E[Y_{1i}-Y_{0i}|D_i = 1] ) + (E[Y_{0i} |D_i = 1] - E[Y_{0i} |D_i = 0]) \\
\text{Observed difference in treatment} &= \text{Average treatment effect on the treated} + \text{Selection bias}
\end{aligned}
\]

\begin{itemize}
\item
  \textbf{The average treatment effect} is the average between between a person who is treated and the same person (in another parallel universe) who is not treated
\item
  \textbf{The selection bias} is the difference between those who were treated and those who weren't treated
\end{itemize}

With \textbf{random assignment} of treatment (\(D_i\)) under \protect\hyperlink{experimental-design}{Experimental Design}, we can have \(D_i\) independent of potential outcomes

\[
\begin{aligned}
E[Y_i | D_i = 1] - E[Y_i|D_i = 0] &= E[Y_{1i}|D_i = 1]-E[Y_{0i}|D_i = 0)]\\
&= E[Y_{1i}|D_i = 1]-E[Y_{0i}|D_i = 0)] && D_i \perp Y_i \\
&= E[Y_{1i} - Y_{0i}|D_i = 1] \\
&= E[Y_{1i} - Y_{0i}]
\end{aligned}
\]

\textbf{Another representation under regression}

Suppose that you know the effect is

\[
Y_{1i} - Y_{0i} = \rho
\]

The observed outcome variable (for an individual) can be rewritten as

\[
\begin{aligned}
Y_i &= E(Y_{0i}) + (Y_{1i}-Y_{0i})D_i + [Y_{0i} - E(Y_{0i})]\\
&= \alpha + \rho D_i + \eta_i
\end{aligned}
\]

where \(\eta_i\) = random variation of \(Y_{0i}\)

Hence, the conditional expectation of an individual outcome on treatment status is

\[
\begin{aligned}
E[Y_i |D_i = 1] &= \alpha + \rho &+ E[\eta_i |D_i = 1] \\
E[Y_i |D_i = 0] &= \alpha &+ E[\eta_i |D_i = 0]
\end{aligned}
\]

Thus,

\[
E[Y_i |D_i = 1] - E[Y_i |D_i = 0] = \rho + E[\eta_i |D_i = 1] -E[\eta_i |D_i = 0]
\]

where \(E[\eta_i |D_i = 1] -E[\eta_i |D_i = 0]\) is the selection bias - correlation between the regression error term (\(\eta_i\)), and the regressor (\(D_i\))

Under regression, we have

\[
E[\eta_i |D_i = 1] -E[\eta_i |D_i = 0] = E[Y_{0i} |D_i = 1] -E[Y_{0i}|D_i = 0]
\]

which is the difference in outcomes between \textbf{those who weren't treated get treated} and \textbf{those who weren't treated stay untreated}

Say you have control variables (\(X_i\)), that is \textbf{uncorrelated with the treatment} (\(D_i\)), then you can include in your model, and it won't (in principle) affect your estimate of the treatment effect (\(\rho\)) with an added benefit of reducing the residual variance, which subsequently reduces the standard error of other estimates.

\[
Y_i = \alpha + \rho D_i + X_i'\gamma + \eta_i
\]

Examples:

\begin{itemize}
\tightlist
\item
  \citet{bertrand2004emily} randomly assign race to a job application to study the effect of race on callbacks.
\end{itemize}

\hypertarget{notes}{%
\section{Notes}\label{notes}}

For outcomes with 0s, we can't use log-like transformation, because it's sensitive to outcome unit \citep{chen2023logs}. For info on this issue, check {[}Zero-valued Outcomes{]}. We should use:

\begin{itemize}
\item
  \textbf{Percentage changes in the Average}: by using Poisson QMLE, we can interpret the coefficients of the effect of treatment on the treated group relative to the mean of the control group.
\item
  \textbf{Extensive vs.~Intensive Margins}: Distinguish the treatment effect on the intensive (outcome: 10 to 11) vs.~extensive margins (outcome: 0 to 1).

  \begin{itemize}
  \tightlist
  \item
    To get the bounds for the intensive-margin, use \citet{lee2009training} (assuming that treatment has a monotonic effect on outcome)
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{) }\CommentTok{\# For reproducibility}
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{1000} \CommentTok{\# Number of observations}
\NormalTok{p\_treatment }\OtherTok{\textless{}{-}} \FloatTok{0.5} \CommentTok{\# Probability of being treated}

\CommentTok{\# Step 1: Generate the treatment variable D}
\NormalTok{D }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(n, }\DecValTok{1}\NormalTok{, p\_treatment)}

\CommentTok{\# Step 2: Generate potential outcomes}
\CommentTok{\# Untreated potential outcome (mostly zeroes)}
\NormalTok{Y0 }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{*}\NormalTok{ (}\FunctionTok{runif}\NormalTok{(n) }\SpecialCharTok{\textless{}} \FloatTok{0.3}\NormalTok{)}

\CommentTok{\# Treated potential outcome (shifting both the probability of being positive {-} extensive margin and its magnitude {-} intensive margin)}
\NormalTok{Y1 }\OtherTok{\textless{}{-}}\NormalTok{ Y0 }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =} \DecValTok{2}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{*}\NormalTok{ (}\FunctionTok{runif}\NormalTok{(n) }\SpecialCharTok{\textless{}} \FloatTok{0.7}\NormalTok{)}

\CommentTok{\# Step 3: Combine effects based on treatment}
\NormalTok{Y\_observed }\OtherTok{\textless{}{-}}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ D) }\SpecialCharTok{*}\NormalTok{ Y0 }\SpecialCharTok{+}\NormalTok{ D }\SpecialCharTok{*}\NormalTok{ Y1}

\CommentTok{\# Add explicit zeroes to model situations with no effect}
\NormalTok{Y\_observed[Y\_observed }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}


\NormalTok{data }\OtherTok{\textless{}{-}}
    \FunctionTok{data.frame}\NormalTok{(}
        \AttributeTok{ID =} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n,}
        \AttributeTok{Treatment =}\NormalTok{ D,}
        \AttributeTok{Outcome =}\NormalTok{ Y\_observed,}
        \AttributeTok{X =} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{    ) }\SpecialCharTok{|\textgreater{}}
    \CommentTok{\# whether outcome is positive}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{positive =}\NormalTok{ Outcome }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{)}

\CommentTok{\# Viewing the first few rows of the dataset}
\FunctionTok{head}\NormalTok{(data)}
\CommentTok{\#\textgreater{}   ID Treatment   Outcome          X positive}
\CommentTok{\#\textgreater{} 1  1         0 0.0000000  1.4783345    FALSE}
\CommentTok{\#\textgreater{} 2  2         1 2.2369379 {-}1.4067867     TRUE}
\CommentTok{\#\textgreater{} 3  3         0 0.0000000 {-}1.8839721    FALSE}
\CommentTok{\#\textgreater{} 4  4         1 3.2192276 {-}0.2773662     TRUE}
\CommentTok{\#\textgreater{} 5  5         1 0.6649693  0.4304278     TRUE}
\CommentTok{\#\textgreater{} 6  6         0 0.0000000 {-}0.1287867    FALSE}

\FunctionTok{hist}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{Outcome)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{19-experiment_files/figure-latex/unnamed-chunk-1-1} \end{center}

\begin{itemize}
\tightlist
\item
  \textbf{Percentage changes in the Average}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(fixest)}
\NormalTok{res\_pois }\OtherTok{\textless{}{-}}
    \FunctionTok{fepois}\NormalTok{(}
        \AttributeTok{fml =}\NormalTok{ Outcome }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Treatment }\SpecialCharTok{+}\NormalTok{ X,}
        \AttributeTok{data =}\NormalTok{ data, }
        \AttributeTok{vcov =} \StringTok{"hetero"}
\NormalTok{    )}
\FunctionTok{etable}\NormalTok{(res\_pois)}
\CommentTok{\#\textgreater{}                           res\_pois}
\CommentTok{\#\textgreater{} Dependent Var.:            Outcome}
\CommentTok{\#\textgreater{}                                   }
\CommentTok{\#\textgreater{} Constant        {-}2.223*** (0.1440)}
\CommentTok{\#\textgreater{} Treatment        2.579*** (0.1494)}
\CommentTok{\#\textgreater{} X                  0.0235 (0.0406)}
\CommentTok{\#\textgreater{} \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}
\CommentTok{\#\textgreater{} S.E. type       Heteroskedas.{-}rob.}
\CommentTok{\#\textgreater{} Observations                 1,000}
\CommentTok{\#\textgreater{} Squared Cor.               0.33857}
\CommentTok{\#\textgreater{} Pseudo R2                  0.26145}
\CommentTok{\#\textgreater{} BIC                        1,927.9}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes: 0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

To calculate the proportional effect

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# proportional effect}
\FunctionTok{exp}\NormalTok{(}\FunctionTok{coefficients}\NormalTok{(res\_pois)[}\StringTok{"Treatment"}\NormalTok{]) }\SpecialCharTok{{-}} \DecValTok{1}
\CommentTok{\#\textgreater{} Treatment }
\CommentTok{\#\textgreater{}  12.17757}

\CommentTok{\# SE}
\FunctionTok{exp}\NormalTok{(}\FunctionTok{coefficients}\NormalTok{(res\_pois)[}\StringTok{"Treatment"}\NormalTok{]) }\SpecialCharTok{*}
    \FunctionTok{sqrt}\NormalTok{(res\_pois}\SpecialCharTok{$}\NormalTok{cov.scaled[}\StringTok{"Treatment"}\NormalTok{, }\StringTok{"Treatment"}\NormalTok{])}
\CommentTok{\#\textgreater{} Treatment }
\CommentTok{\#\textgreater{}  1.968684}
\end{Highlighting}
\end{Shaded}

Hence, we conclude that the treatment effect is 1215\% higher for the treated group as compared to the control group.

\begin{itemize}
\tightlist
\item
  \textbf{Extensive vs.~Intensive Margins}
\end{itemize}

Here, we can estimate the intensive-margin treatment effect (i.e., the treatment effect for ``always-takers'').

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res }\OtherTok{\textless{}{-}}\NormalTok{ causalverse}\SpecialCharTok{::}\FunctionTok{lee\_bounds}\NormalTok{(}
    \AttributeTok{df =}\NormalTok{ data,}
    \AttributeTok{d =} \StringTok{"Treatment"}\NormalTok{,}
    \AttributeTok{m =} \StringTok{"positive"}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"Outcome"}\NormalTok{,}
    \AttributeTok{numdraws =} \DecValTok{10}
\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
\NormalTok{    causalverse}\SpecialCharTok{::}\FunctionTok{nice\_tab}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\FunctionTok{print}\NormalTok{(res)}
\CommentTok{\#\textgreater{}          term estimate std.error}
\CommentTok{\#\textgreater{} 1 Lower bound    {-}0.22      0.09}
\CommentTok{\#\textgreater{} 2 Upper bound     2.77      0.14}
\end{Highlighting}
\end{Shaded}

Since in this case, the bounds contains 0, we can't say much about the intensive margin for always-takers.

If we aim to examine the sensitivity of always-takers, we should consider scenarios where the average outcome of compliers are \(100 \times c\%\) lower or higher than that of always-takers.

We assume that \(E(Y(1)|Complier) = (1-c)E(Y(1)|Always-taker)\)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{c\_values }\OtherTok{=} \FunctionTok{c}\NormalTok{(.}\DecValTok{1}\NormalTok{, .}\DecValTok{5}\NormalTok{, .}\DecValTok{7}\NormalTok{)}

\NormalTok{combined\_res }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(}\FunctionTok{lapply}\NormalTok{(c\_values, }\ControlFlowTok{function}\NormalTok{(c) \{}
\NormalTok{    res }\OtherTok{\textless{}{-}}\NormalTok{ causalverse}\SpecialCharTok{::}\FunctionTok{lee\_bounds}\NormalTok{(}
        \AttributeTok{df =}\NormalTok{ data,}
        \AttributeTok{d =} \StringTok{"Treatment"}\NormalTok{,}
        \AttributeTok{m =} \StringTok{"positive"}\NormalTok{,}
        \AttributeTok{y =} \StringTok{"Outcome"}\NormalTok{,}
        \AttributeTok{numdraws =} \DecValTok{10}\NormalTok{,}
        \AttributeTok{c\_at\_ratio =}\NormalTok{ c}
\NormalTok{    )}
    
\NormalTok{    res}\SpecialCharTok{$}\NormalTok{c\_value }\OtherTok{\textless{}{-}} \FunctionTok{as.character}\NormalTok{(c)}
    \FunctionTok{return}\NormalTok{(res)}
\NormalTok{\}))}

\NormalTok{combined\_res }\SpecialCharTok{|\textgreater{}} 
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(c\_value, }\FunctionTok{everything}\NormalTok{()) }\SpecialCharTok{|\textgreater{}} 
\NormalTok{    causalverse}\SpecialCharTok{::}\FunctionTok{nice\_tab}\NormalTok{()}
\CommentTok{\#\textgreater{}   c\_value           term estimate std.error}
\CommentTok{\#\textgreater{} 1     0.1 Point estimate     6.60      0.71}
\CommentTok{\#\textgreater{} 2     0.5 Point estimate     2.54      0.13}
\CommentTok{\#\textgreater{} 3     0.7 Point estimate     1.82      0.08}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  If we assume \(c = 0.1\) (i.e., under treatment, compliers would have an outcome equal to 10\% of the outcome of always-takers), then the intensive-margin effect for always-takers is 6.6 more in the unit of the outcome.
\item
  If we assume \(c = 0.5\) (i.e., under treatment, compliers would have an outcome equal to 50\% of the outcome of always-takers), then the intensive-margin effect for always-takers is 2.54 more in the unit of the outcome.
\end{itemize}

\hypertarget{semi-random-experiment}{%
\section{Semi-random Experiment}\label{semi-random-experiment}}

Chicago Open Enrollment Program \citep{cullen2005impact}

\begin{itemize}
\item
  Students can apply to ``choice'' schools
\item
  Many schools are oversubscribed (Demand \textgreater{} Supply)
\item
  Resolve scarcity via random lotteries
\item
  Non-random enrollment, we only have random lottery which mean the above
\end{itemize}

Let

\[
\delta_j = E(Y_i | Enroll_{ij} = 1; Apply_{ij} = 1) - E(Y_i | Enroll_{ij} = 0; Apply_{ij} = 1)
\]

and

\[
\theta_j = E(Y_i | Win_{ij} = 1; Apply_{ij} = 1) - E(Y_i | Win_{ij} = 0; Apply_{ij} = 1)
\]

Hence, we can clearly see that \(\delta_j \neq \theta_j\) because you can only enroll, but you cannot ensure that you will win. Thus, \textbf{intention to treat is different from treatment effect}.

Non-random enrollment, we only have random lottery which means we can only estimate \(\theta_j\)

To recover the true treatment effect, we can use

\[
\delta_j = \frac{E(Y_i|W_{ij} = 1; A_{ij} = 1) - E(Y_i | W_{ij}=0; A_{ij} = 1)}{P(Enroll_{ij} = 1| W_{ij}= 1; A_{ij}=1) - P(Enroll_{ij} = 1| W_{ij}=0; A_{ij}=1)}
\]

where

\begin{itemize}
\item
  \(\delta_j\) = treatment effect
\item
  \(W\) = Whether students win the lottery
\item
  \(A\) = Whether student apply for the lottery
\item
  i = application
\item
  j = school
\end{itemize}

Say that we have

\textbf{10 win}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2073}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1829}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2195}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2195}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1707}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Number students
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Selection effect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Treatment effect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Total effect
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & Always Takers & +0.2 & +1 & +1.2 \\
2 & Compliers & 0 & +1 & +1 \\
7 & Never Takers & -0.1 & 0 & -0.1 \\
\end{longtable}

\textbf{10 lose}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2073}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1829}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2195}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2195}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1707}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Number students
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Selection effect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Treatment effect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Total effect
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & Always Takers & +0.2 & +1 & +1.2 \\
2 & Compliers & 0 & 0 & 0 \\
7 & Never Takers & -0.1 & 0 & -0.1 \\
\end{longtable}

Intent to treatment = Average effect of who you give option to choose

\[
\begin{aligned}
E(Y_i | W_{ij}=1; A_{ij} = 1) &= \frac{1*(1.2)+ 2*(1) + 7 * (-0.1)}{10}\\
&= 0.25
\end{aligned}
\]

\[
\begin{aligned}
E(Y_i | W_{ij}=0; A_{ij} = 1) &= \frac{1*(1.2)+ 2*(0) + 7 * (-0.1)}{10}\\
&= 0.05
\end{aligned}
\]

Hence,

\[
\begin{aligned}
\text{Intent to treatment} &= 0.25 - 0.05 = 0.2 \\
\text{Treatment effect} &= 1
\end{aligned}
\]

\[
\begin{aligned}
P(Enroll_{ij} = 1 | W_{ij} = 1; A_{ij}=1 ) &= \frac{1+2}{10} = 0.3 \\
P(Enroll_{ij} = 1 | W_{ij} = 0; A_{ij}=1 ) &= \frac{1}{10} = 0.1
\end{aligned}
\]

\[
\text{Treatment effect} = \frac{0.2}{0.3-0.1} = 1
\]

After knowing how to recover the treatment effect, we turn our attention to the main model

\[
Y_{ia} = \delta W_{ia} + \lambda L_{ia} + e_{ia}
\]

where

\begin{itemize}
\item
  \(W\) = whether a student wins a lottery
\item
  \(L\) = whether student enrolls in the lottery
\item
  \(\delta\) = intent to treat
\end{itemize}

Hence,

\begin{itemize}
\item
  Conditional on lottery, the \(\delta\) is valid
\item
  But without lottery, your \(\delta\) is not random
\item
  Winning and losing are only identified within lottery
\item
  Each lottery has multiple entries. Thus, we can have within estimator
\end{itemize}

We can also include other control variables (\(X_i \theta\))

\[
Y_{ia} = \delta_1 W_{ia} + \lambda_1 L_{ia} + X_i \theta + u_{ia}
\]

\[
\begin{aligned}
E(\delta) &= E(\delta_1) \\
E(\lambda) &\neq E(\lambda_1) && \text{because choosing a lottery is not random}
\end{aligned}
\]

Including \((X_i \theta)\) just shifts around control variables (i.e., reweighting of lottery), which would not affect your treatment effect \(E(\delta)\)

\hypertarget{rerandomization}{%
\section{Rerandomization}\label{rerandomization}}

\begin{itemize}
\item
  Since randomization only balances baseline covariates on average, imbalance in variables due to random chance can still happen.
\item
  In case that you have a ``bad'' randomization (i.e., imbalance for important baseline covariates), \citep{morgan2012rerandomization} introduce the idea of rerandomization.
\item
  Rerandomization is checking balance during the randomization process (before the experiment), to eliminate bad allocation (i.e., those with unacceptable balance).
\item
  The greater the number of variables, the greater the likelihood that at least one covariate would be imbalanced across treatment groups.

  \begin{itemize}
  \tightlist
  \item
    Example: For 10 covariates, the probability of a significant difference at \(\alpha = .05\) for at least one covariate is \(1 - (1-.05)^{10} = 0.4 = 40\%\)
  \end{itemize}
\item
  Rerandomization increase treatment effect estimate precision if the covariates are correlated with the outcome.

  \begin{itemize}
  \tightlist
  \item
    Improvement in precision for treatment effect estimate depends on (1) improvement in covariate balance and (2) correlation between covariates and the outcome.
  \end{itemize}
\item
  You also need to take into account rerandomization into your analysis when making inference.
\item
  Rerandomization is equivalent to increasing our sample size.
\item
  Alternatives include

  \begin{itemize}
  \item
    Stratified randomization \citep{johansson2022rerandomization}
  \item
    Matched randomization \citep{greevy2004optimal, kapelner2014matching}
  \item
    Minimization \citep{pocock1975sequential}
  \end{itemize}
\end{itemize}

\href{https://healthpolicy.usc.edu/evidence-base/rerandomization-what-is-it-and-why-should-you-use-it-for-random-assignment/}{\includegraphics[width=6.25in,height=5.20833in]{images/The-Randomization-Procedure.png}}

Rerandomization Criterion

\begin{itemize}
\tightlist
\item
  Acceptable randomization is based on a function of covariate matrix \(\mathbf{X}\) and vector of treatment assignments \(\mathbf{W}\)
\end{itemize}

\[
W_i = 
\begin{cases}
1 \text{ if treated} \\
0 \text{ if control} 
\end{cases}
\]

\begin{itemize}
\tightlist
\item
  Mahalanobis Distance, \(M\), can be used as criteria for acceptable balance
\end{itemize}

Let \(M\) be the multivariate distance between groups means

\[
\begin{aligned}
M &= (\bar{\mathbf{X}}_T - \bar{\mathbf{X}}_C)' cov(\bar{\mathbf{X}}_T - \bar{\mathbf{X}}_C)^{-1} (\bar{\mathbf{X}}_T - \bar{\mathbf{X}}_C) \\
&= (\frac{1}{n_T}+ \frac{1}{n_C})^{-1}(\bar{\mathbf{X}}_T - \bar{\mathbf{X}}_C)' cov(\mathbf{X})^{-1}(\bar{\mathbf{X}}_T - \bar{\mathbf{X}}_C)
\end{aligned}
\]

With large sample size and ``pure'' randomization \(M \sim \chi^2_k\) where \(k\) is the number of covariates to be balanced

Then let \(p_a\) be the probability of accepting a randomization. Choosing appropriate \(p_a\) is a tradeoff between balance and time.

Then the rule of thumb is re-randomize when \(M > a\)

\hypertarget{two-stage-randomized-experiments-with-interference-and-noncompliance}{%
\section{Two-Stage Randomized Experiments with Interference and Noncompliance}\label{two-stage-randomized-experiments-with-interference-and-noncompliance}}

\citep{imai2021causal}

\hypertarget{ab-testing-caution}{%
\section{A/B Testing Caution}\label{ab-testing-caution}}

\citet{braun2025where}

\begin{itemize}
\item
  Marketers use online advertising platforms to test user responses to different ad content.
\item
  Platforms' experimentation tools deliver ads to different, dynamically optimized mixes of users, leading to nonrandom exposure.
\item
  This ``divergent delivery'' confounds ad content effects with algorithmic targeting, skewing A/B test results.
\item
  Algorithmic targeting, user heterogeneity, and data aggregation distort the magnitude and direction of A/B test results.
\item
  Platforms have little incentive to help experimenters isolate ad content effects from proprietary targeting mechanisms.
\end{itemize}

\hypertarget{sampling}{%
\chapter{Sampling}\label{sampling}}

\hypertarget{simple-sampling}{%
\section{Simple Sampling}\label{simple-sampling}}

Simple (random) Sampling

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr)}
\NormalTok{iris\_df }\OtherTok{\textless{}{-}}\NormalTok{ iris}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\FunctionTok{sample\_n}\NormalTok{(iris\_df, }\DecValTok{10}\NormalTok{)}
\CommentTok{\#\textgreater{}    Sepal.Length Sepal.Width Petal.Length Petal.Width    Species}
\CommentTok{\#\textgreater{} 1           5.8         2.7          4.1         1.0 versicolor}
\CommentTok{\#\textgreater{} 2           6.4         2.8          5.6         2.1  virginica}
\CommentTok{\#\textgreater{} 3           4.4         3.2          1.3         0.2     setosa}
\CommentTok{\#\textgreater{} 4           4.3         3.0          1.1         0.1     setosa}
\CommentTok{\#\textgreater{} 5           7.0         3.2          4.7         1.4 versicolor}
\CommentTok{\#\textgreater{} 6           5.4         3.0          4.5         1.5 versicolor}
\CommentTok{\#\textgreater{} 7           5.4         3.4          1.7         0.2     setosa}
\CommentTok{\#\textgreater{} 8           7.6         3.0          6.6         2.1  virginica}
\CommentTok{\#\textgreater{} 9           6.1         2.8          4.7         1.2 versicolor}
\CommentTok{\#\textgreater{} 10          4.6         3.4          1.4         0.3     setosa}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(sampling)}
\CommentTok{\# set unique id number for each row}
\NormalTok{iris\_df}\SpecialCharTok{$}\NormalTok{id }\OtherTok{=} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(iris\_df)}

\CommentTok{\# Simple random sampling with replacement}
\FunctionTok{srswor}\NormalTok{(}\DecValTok{10}\NormalTok{, }\FunctionTok{length}\NormalTok{(iris\_df}\SpecialCharTok{$}\NormalTok{id))}
\CommentTok{\#\textgreater{}   [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1}
\CommentTok{\#\textgreater{}  [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0}
\CommentTok{\#\textgreater{}  [75] 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0}
\CommentTok{\#\textgreater{} [112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0}
\CommentTok{\#\textgreater{} [149] 0 0}

\CommentTok{\# Simple random sampling without replacement (sequential method)}
\FunctionTok{srswor1}\NormalTok{(}\DecValTok{10}\NormalTok{, }\FunctionTok{length}\NormalTok{(iris\_df}\SpecialCharTok{$}\NormalTok{id))}
\CommentTok{\#\textgreater{}   [1] 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0}
\CommentTok{\#\textgreater{}  [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0}
\CommentTok{\#\textgreater{}  [75] 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0}
\CommentTok{\#\textgreater{} [112] 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0}
\CommentTok{\#\textgreater{} [149] 0 0}

\CommentTok{\# Simple random sampling with replacement}
\FunctionTok{srswr}\NormalTok{(}\DecValTok{10}\NormalTok{, }\FunctionTok{length}\NormalTok{(iris\_df}\SpecialCharTok{$}\NormalTok{id))}
\CommentTok{\#\textgreater{}   [1] 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0}
\CommentTok{\#\textgreater{}  [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0}
\CommentTok{\#\textgreater{}  [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0}
\CommentTok{\#\textgreater{} [112] 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0}
\CommentTok{\#\textgreater{} [149] 0 0}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(survey)}
\FunctionTok{data}\NormalTok{(}\StringTok{"api"}\NormalTok{)}
\NormalTok{srs\_design }\OtherTok{\textless{}{-}} \FunctionTok{svydesign}\NormalTok{(}\AttributeTok{data =}\NormalTok{ apistrat,}
                        \AttributeTok{weights =} \SpecialCharTok{\textasciitilde{}}\NormalTok{pw, }
                        \AttributeTok{fpc =} \SpecialCharTok{\textasciitilde{}}\NormalTok{fpc, }
                        \AttributeTok{id =} \SpecialCharTok{\textasciitilde{}}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(sampler)}
\FunctionTok{rsamp}\NormalTok{(albania,}
      \AttributeTok{n =} \DecValTok{260}\NormalTok{,}
      \AttributeTok{over =} \FloatTok{0.1}\NormalTok{, }\CommentTok{\# desired oversampling proportion}
      \AttributeTok{rep =}\NormalTok{ F)}
\end{Highlighting}
\end{Shaded}

Identify missing points between sample and collected data

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alsample }\OtherTok{\textless{}{-}} \FunctionTok{rsamp}\NormalTok{(}\AttributeTok{df =}\NormalTok{ albania, }\DecValTok{544}\NormalTok{)}
\NormalTok{alreceived }\OtherTok{\textless{}{-}} \FunctionTok{rsamp}\NormalTok{(}\AttributeTok{df =}\NormalTok{ alsample, }\DecValTok{390}\NormalTok{)}
\FunctionTok{rmissing}\NormalTok{(}\AttributeTok{sampdf =}\NormalTok{ alsample,}
         \AttributeTok{colldf =}\NormalTok{ alreceived,}
         \AttributeTok{col\_name =}\NormalTok{ qvKod)}
\end{Highlighting}
\end{Shaded}

\hypertarget{stratified-sampling}{%
\section{Stratified Sampling}\label{stratified-sampling}}

A stratum is a subset of the population that has at least one common characteristic.

Steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Identify relevant stratums and their representation in the population.
\item
  Randomly sample to select a sufficient number of subjects from each stratum.
\end{enumerate}

Stratified sampling reduces sampling error.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr)}
\CommentTok{\# by number of rows}
\NormalTok{sample\_iris }\OtherTok{\textless{}{-}}\NormalTok{ iris }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{group\_by}\NormalTok{(Species) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{sample\_n}\NormalTok{(}\DecValTok{5}\NormalTok{)}
\NormalTok{sample\_iris}
\CommentTok{\#\textgreater{} \# A tibble: 15 x 5}
\CommentTok{\#\textgreater{} \# Groups:   Species [3]}
\CommentTok{\#\textgreater{}    Sepal.Length Sepal.Width Petal.Length Petal.Width Species   }
\CommentTok{\#\textgreater{}           \textless{}dbl\textgreater{}       \textless{}dbl\textgreater{}        \textless{}dbl\textgreater{}       \textless{}dbl\textgreater{} \textless{}fct\textgreater{}     }
\CommentTok{\#\textgreater{}  1          4.4         3            1.3         0.2 setosa    }
\CommentTok{\#\textgreater{}  2          5.2         3.5          1.5         0.2 setosa    }
\CommentTok{\#\textgreater{}  3          5.1         3.8          1.5         0.3 setosa    }
\CommentTok{\#\textgreater{}  4          5.2         3.4          1.4         0.2 setosa    }
\CommentTok{\#\textgreater{}  5          4.5         2.3          1.3         0.3 setosa    }
\CommentTok{\#\textgreater{}  6          5.5         2.5          4           1.3 versicolor}
\CommentTok{\#\textgreater{}  7          7           3.2          4.7         1.4 versicolor}
\CommentTok{\#\textgreater{}  8          6.7         3            5           1.7 versicolor}
\CommentTok{\#\textgreater{}  9          6.1         2.9          4.7         1.4 versicolor}
\CommentTok{\#\textgreater{} 10          5.5         2.4          3.8         1.1 versicolor}
\CommentTok{\#\textgreater{} 11          6.4         2.7          5.3         1.9 virginica }
\CommentTok{\#\textgreater{} 12          6.4         2.8          5.6         2.1 virginica }
\CommentTok{\#\textgreater{} 13          6.4         3.2          5.3         2.3 virginica }
\CommentTok{\#\textgreater{} 14          6.8         3.2          5.9         2.3 virginica }
\CommentTok{\#\textgreater{} 15          7.2         3.6          6.1         2.5 virginica}

\CommentTok{\# by fraction}
\NormalTok{sample\_iris }\OtherTok{\textless{}{-}}\NormalTok{ iris }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{group\_by}\NormalTok{(Species) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{sample\_frac}\NormalTok{(}\AttributeTok{size =}\NormalTok{ .}\DecValTok{15}\NormalTok{)}
\NormalTok{sample\_iris}
\CommentTok{\#\textgreater{} \# A tibble: 24 x 5}
\CommentTok{\#\textgreater{} \# Groups:   Species [3]}
\CommentTok{\#\textgreater{}    Sepal.Length Sepal.Width Petal.Length Petal.Width Species   }
\CommentTok{\#\textgreater{}           \textless{}dbl\textgreater{}       \textless{}dbl\textgreater{}        \textless{}dbl\textgreater{}       \textless{}dbl\textgreater{} \textless{}fct\textgreater{}     }
\CommentTok{\#\textgreater{}  1          5.5         4.2          1.4         0.2 setosa    }
\CommentTok{\#\textgreater{}  2          5           3            1.6         0.2 setosa    }
\CommentTok{\#\textgreater{}  3          5.2         4.1          1.5         0.1 setosa    }
\CommentTok{\#\textgreater{}  4          4.6         3.1          1.5         0.2 setosa    }
\CommentTok{\#\textgreater{}  5          5.1         3.7          1.5         0.4 setosa    }
\CommentTok{\#\textgreater{}  6          4.8         3.4          1.9         0.2 setosa    }
\CommentTok{\#\textgreater{}  7          5.1         3.3          1.7         0.5 setosa    }
\CommentTok{\#\textgreater{}  8          5.5         3.5          1.3         0.2 setosa    }
\CommentTok{\#\textgreater{}  9          5           2.3          3.3         1   versicolor}
\CommentTok{\#\textgreater{} 10          5.6         2.9          3.6         1.3 versicolor}
\CommentTok{\#\textgreater{} \# i 14 more rows}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(sampler)}
\CommentTok{\# Stratified sample using proportional allocation without replacement}
\FunctionTok{ssamp}\NormalTok{(}\AttributeTok{df=}\NormalTok{albania, }\AttributeTok{n=}\DecValTok{360}\NormalTok{, }\AttributeTok{strata=}\NormalTok{qarku, }\AttributeTok{over=}\FloatTok{0.1}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 395 x 45}
\CommentTok{\#\textgreater{}    qarku  Q\_ID bashkia   BAS\_ID zaz   njesiaAdministrative COM\_ID qvKod zgjedhes}
\CommentTok{\#\textgreater{}    \textless{}fct\textgreater{} \textless{}int\textgreater{} \textless{}fct\textgreater{}      \textless{}int\textgreater{} \textless{}fct\textgreater{} \textless{}fct\textgreater{}                 \textless{}int\textgreater{} \textless{}fct\textgreater{}    \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{}  1 Berat     1 Berat         11 ZAZ \textasciitilde{} "Berat "               1101 "\textbackslash{}"3\textasciitilde{}      558}
\CommentTok{\#\textgreater{}  2 Berat     1 Berat         11 ZAZ \textasciitilde{} "Berat "               1101 "\textbackslash{}"3\textasciitilde{}      815}
\CommentTok{\#\textgreater{}  3 Berat     1 Berat         11 ZAZ \textasciitilde{} "Sinje"                1108 "\textbackslash{}"3\textasciitilde{}      419}
\CommentTok{\#\textgreater{}  4 Berat     1 Kucove        13 ZAZ \textasciitilde{} "Lumas"                1104 "\textbackslash{}"3\textasciitilde{}      237}
\CommentTok{\#\textgreater{}  5 Berat     1 Kucove        13 ZAZ \textasciitilde{} "Kucove"               1201 "\textbackslash{}"3\textasciitilde{}      562}
\CommentTok{\#\textgreater{}  6 Berat     1 Skrapar       17 ZAZ \textasciitilde{} "Corovode"             1303 "\textbackslash{}"3\textasciitilde{}      829}
\CommentTok{\#\textgreater{}  7 Berat     1 Berat         11 ZAZ \textasciitilde{} "Roshnik"              1107 "\textbackslash{}"3\textasciitilde{}      410}
\CommentTok{\#\textgreater{}  8 Berat     1 Ura Vajg\textasciitilde{}     19 ZAZ \textasciitilde{} "Ura Vajgurore"        1110 "\textbackslash{}"3\textasciitilde{}      708}
\CommentTok{\#\textgreater{}  9 Berat     1 Kucove        13 ZAZ \textasciitilde{} "Perondi"              1203 "\textbackslash{}"3\textasciitilde{}      835}
\CommentTok{\#\textgreater{} 10 Berat     1 Kucove        13 ZAZ \textasciitilde{} "Kucove"               1201 "\textbackslash{}"3\textasciitilde{}      907}
\CommentTok{\#\textgreater{} \# i 385 more rows}
\CommentTok{\#\textgreater{} \# i 36 more variables: meshkuj \textless{}int\textgreater{}, femra \textless{}int\textgreater{}, totalSeats \textless{}int\textgreater{},}
\CommentTok{\#\textgreater{} \#   vendndodhja \textless{}fct\textgreater{}, ambienti \textless{}fct\textgreater{}, totalVoters \textless{}int\textgreater{}, femVoters \textless{}int\textgreater{},}
\CommentTok{\#\textgreater{} \#   maleVoters \textless{}int\textgreater{}, unusedBallots \textless{}int\textgreater{}, damagedBallots \textless{}int\textgreater{},}
\CommentTok{\#\textgreater{} \#   ballotsCast \textless{}int\textgreater{}, invalidVotes \textless{}int\textgreater{}, validVotes \textless{}int\textgreater{}, lsi \textless{}int\textgreater{},}
\CommentTok{\#\textgreater{} \#   ps \textless{}int\textgreater{}, pkd \textless{}int\textgreater{}, sfida \textless{}int\textgreater{}, pr \textless{}int\textgreater{}, pd \textless{}int\textgreater{}, pbdksh \textless{}int\textgreater{},}
\CommentTok{\#\textgreater{} \#   adk \textless{}int\textgreater{}, psd \textless{}int\textgreater{}, ad \textless{}int\textgreater{}, frd \textless{}int\textgreater{}, pds \textless{}int\textgreater{}, pdiu \textless{}int\textgreater{}, ...}
\end{Highlighting}
\end{Shaded}

Identify number of missing points by strata between sample and collected data

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alsample }\OtherTok{\textless{}{-}} \FunctionTok{rsamp}\NormalTok{(}\AttributeTok{df =}\NormalTok{ albania, }\DecValTok{544}\NormalTok{)}
\NormalTok{alreceived }\OtherTok{\textless{}{-}} \FunctionTok{rsamp}\NormalTok{(}\AttributeTok{df =}\NormalTok{ alsample, }\DecValTok{390}\NormalTok{)}
\FunctionTok{smissing}\NormalTok{(}
    \AttributeTok{sampdf =}\NormalTok{ alsample,}
    \AttributeTok{colldf =}\NormalTok{ alreceived,}
    \AttributeTok{strata =}\NormalTok{ qarku,}
    \AttributeTok{col\_name =}\NormalTok{ qvKod}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{unequal-probability-sampling}{%
\section{Unequal Probability Sampling}\label{unequal-probability-sampling}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{UPbrewer}\NormalTok{()}
\FunctionTok{UPmaxentropy}\NormalTok{()}
\FunctionTok{UPmidzuno}\NormalTok{()}
\FunctionTok{UPmidzunopi2}\NormalTok{()}
\FunctionTok{UPmultinomial}\NormalTok{()}
\FunctionTok{UPpivotal}\NormalTok{()}
\FunctionTok{UPrandompivotal}\NormalTok{()}
\FunctionTok{UPpoisson}\NormalTok{()}
\FunctionTok{UPsampford}\NormalTok{()}
\FunctionTok{UPsystematic}\NormalTok{()}
\FunctionTok{UPrandomsystematic}\NormalTok{()}
\FunctionTok{UPsystematicpi2}\NormalTok{()}
\FunctionTok{UPtille}\NormalTok{()}
\FunctionTok{UPtillepi2}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\hypertarget{balanced-sampling}{%
\section{Balanced Sampling}\label{balanced-sampling}}

\begin{itemize}
\item
  Purpose: to get the same means in the population and the sample for all the auxiliary variables
\item
  Balanced sampling is different from purposive selection
\end{itemize}

Balancing equations

\[
\sum_{k \in S} \frac{\mathbf{x}_k}{\pi_k} = \sum_{k \in U} \mathbf{x}_k
\]

where \(\mathbf{x}_k\) is a vector of auxiliary variables

\hypertarget{cube}{%
\subsection{Cube}\label{cube}}

\begin{itemize}
\item
  flight phase
\item
  landing phase
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{samplecube}\NormalTok{()}
\FunctionTok{fastflightcube}\NormalTok{()}
\FunctionTok{landingcube}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\hypertarget{stratification}{%
\subsection{Stratification}\label{stratification}}

\begin{itemize}
\tightlist
\item
  Try to replicate the population based on the original multivariate histogram
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(survey)}
\FunctionTok{data}\NormalTok{(}\StringTok{"api"}\NormalTok{)}
\NormalTok{srs\_design }\OtherTok{\textless{}{-}} \FunctionTok{svydesign}\NormalTok{(}\AttributeTok{data =}\NormalTok{ apistrat,}
                        \AttributeTok{weights =} \SpecialCharTok{\textasciitilde{}}\NormalTok{pw, }
                        \AttributeTok{fpc =} \SpecialCharTok{\textasciitilde{}}\NormalTok{fpc, }
                        \AttributeTok{strata =} \SpecialCharTok{\textasciitilde{}}\NormalTok{stype,}
                        \AttributeTok{id =} \SpecialCharTok{\textasciitilde{}}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{balancedstratification}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\hypertarget{cluster}{%
\subsection{Cluster}\label{cluster}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(survey)}
\FunctionTok{data}\NormalTok{(}\StringTok{"api"}\NormalTok{)}
\NormalTok{srs\_design }\OtherTok{\textless{}{-}} \FunctionTok{svydesign}\NormalTok{(}\AttributeTok{data =}\NormalTok{ apiclus1,}
                        \AttributeTok{weights =} \SpecialCharTok{\textasciitilde{}}\NormalTok{pw, }
                        \AttributeTok{fpc =} \SpecialCharTok{\textasciitilde{}}\NormalTok{fpc, }
                        \AttributeTok{id =} \SpecialCharTok{\textasciitilde{}}\NormalTok{dnum)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{balancedcluster}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\hypertarget{two-stage}{%
\subsection{Two-stage}\label{two-stage}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(survey)}
\FunctionTok{data}\NormalTok{(}\StringTok{"api"}\NormalTok{)}
\NormalTok{srs\_design }\OtherTok{\textless{}{-}} \FunctionTok{svydesign}\NormalTok{(}\AttributeTok{data =}\NormalTok{ apiclus2, }
                        \AttributeTok{fpc =} \SpecialCharTok{\textasciitilde{}}\NormalTok{fpc1 }\SpecialCharTok{+}\NormalTok{ fpc2, }
                        \AttributeTok{id =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ dnum }\SpecialCharTok{+}\NormalTok{ snum)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{balancedtwostage}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\hypertarget{sec-analysis-of-variance-anova}{%
\chapter{Analysis of Variance (ANOVA)}\label{sec-analysis-of-variance-anova}}

ANOVA is using the same underlying mechanism as linear regression. However, the angle that ANOVA chooses to look at is slightly different from the traditional linear regression. It can be more useful in the case with \textbf{qualitative variables} and \textbf{designed experiments}.

Experimental Design

\begin{itemize}
\tightlist
\item
  \textbf{Factor}: explanatory or predictor variable to be studied in an investigation
\item
  \textbf{Treatment} (or Factor Level): ``value'' of a factor applied to the experimental unit
\item
  \textbf{Experimental Unit}: person, animal, piece of material, etc. that is subjected to treatment(s) and provides a response
\item
  \textbf{Single Factor Experiment}: one explanatory variable considered
\item
  \textbf{Multifactor Experiment}: more than one explanatory variable
\item
  \textbf{Classification Factor}: A factor that is not under the control of the experimenter (observational data)
\item
  \textbf{Experimental Factor}: assigned by the experimenter
\end{itemize}

Basics of experimental design:

\begin{itemize}
\item
  Choices that a statistician has to make:

  \begin{itemize}
  \tightlist
  \item
    set of treatments
  \item
    set of experimental units
  \item
    treatment assignment (selection bias)
  \item
    measurement (measurement bias, blind experiments)
  \end{itemize}
\item
  Advancements in experimental design:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \item
    \textbf{Factorial Experiments}:\\
    consider multiple factors at the same time (interaction)
  \item
    \textbf{Replication}: repetition of experiment

    \begin{itemize}
    \tightlist
    \item
      assess mean squared error
    \item
      control over precision of experiment (power)
    \end{itemize}
  \item
    \textbf{Randomization}

    \begin{itemize}
    \tightlist
    \item
      Before R.A. Fisher (1900s), treatments were assigned systematically or subjectively
    \item
      randomization: assign treatments to experimental units at random, which averages out systematic effects that cannot be control by the investigator
    \end{itemize}
  \item
    \textbf{Local control}: Blocking or Stratification

    \begin{itemize}
    \tightlist
    \item
      Reduce experimental errors and increase power by placing restrictions on the randomization of treatments to experimental units.
    \end{itemize}
  \end{enumerate}
\end{itemize}

Randomization may also eliminate correlations due to time and space.

\hypertarget{completely-randomized-design-crd}{%
\section{Completely Randomized Design (CRD)}\label{completely-randomized-design-crd}}

Treatment factor A with \(a\ge2\) treatments levels. Experimental units are randomly assigned to each treatment. The number of experimental units in each group can be

\begin{itemize}
\tightlist
\item
  equal (balanced): n
\item
  unequal (unbalanced): \(n_i\) for the i-th group (i = 1,\ldots,a).
\end{itemize}

The total sample size is \(N=\sum_{i=1}^{a}n_i\)

Possible assignments of units to treatments are \(k=\frac{N!}{n_1!n_2!...n_a!}\)

Each has probability 1/k of being selected. Each experimental unit is measured with a response \(Y_{ij}\), in which j denotes unit and i denotes treatment.

Treatment

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1728}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2099}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2099}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1605}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2099}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
1
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
2
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\ldots{}
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
a
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
& \(Y_{11}\) & \(Y_{21}\) & \ldots{} & \(Y_{a1}\) \\
& \(Y_{12}\) & \ldots{} & \ldots{} & \ldots{} \\
& \ldots{} & \ldots{} & \ldots{} & \ldots{} \\
Sample Mean & \(\bar{Y_{1.}}\) & \(\bar{Y_{2.}}\) & \ldots{} & \(\bar{Y_{a.}}\) \\
Sample SD & \(s_1\) & \(s_2\) & \ldots{} & \(s_a\) \\
\end{longtable}

where \(\bar{Y_{i.}}=\frac{1}{n_i}\sum_{j=1}^{n_i}Y_{ij}\)

\(s_i^2=\frac{1}{n_i-1}\sum_{j=1}^{n_i}(Y_{ij}-\bar{Y_i})^2\)

And the grand mean is \(\bar{Y_{..}}=\frac{1}{N}\sum_{i}\sum_{j}Y_{ij}\)

\hypertarget{single-factor-fixed-effects-model}{%
\subsection{Single Factor Fixed Effects Model}\label{single-factor-fixed-effects-model}}

also known as Single Factor (One-Way) ANOVA or ANOVA Type I model.

Partitioning the Variance

The total variability of the \(Y_{ij}\) observation can be measured as the deviation of \(Y_{ij}\) around the overall mean \(\bar{Y_{..}}\): \(Y_{ij} - \bar{Y_{..}}\)

This can be rewritten as:

\[
\begin{aligned}
Y_{ij} - \bar{Y_{..}}&=Y_{ij} - \bar{Y_{..}} + \bar{Y_{i.}} - \bar{Y_{i.}} \\
&= (\bar{Y_{i.}}-\bar{Y_{..}})+(Y_{ij}-\bar{Y_{i.}})
\end{aligned}
\]

where

\begin{itemize}
\tightlist
\item
  the first term is the \emph{between} treatment differences (i.e., the deviation of the treatment mean from the overall mean)
\item
  the second term is \emph{within} treatment differences (i.e., the deviation of the observation around its treatment mean)
\end{itemize}

\[
\begin{aligned}
\sum_{i}\sum_{j}(Y_{ij} - \bar{Y_{..}})^2 &=  \sum_{i}n_i(\bar{Y_{i.}}-\bar{Y_{..}})^2+\sum_{i}\sum_{j}(Y_{ij}-\bar{Y_{i.}})^2 \\
SSTO &= SSTR + SSE \\
total~SS &= treatment~SS + error~SS \\
(N-1)~d.f. &= (a-1)~d.f. + (N - a) ~ d.f.
\end{aligned}
\]

we lose a d.f. for the total corrected SSTO because of the estimation of the mean (\(\sum_{i}\sum_{j}(Y_{ij} - \bar{Y_{..}})=0\))\\
And, for the SSTR \(\sum_{i}n_i(\bar{Y_{i.}}-\bar{Y_{..}})=0\)

Accordingly, \(MSTR= \frac{SST}{a-1}\) and \(MSR=\frac{SSE}{N-a}\)

\textbf{ANOVA Table}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2745}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.4510}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1275}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1275}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Source of Variation
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
SS
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
df
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
MS
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Between Treatments & \(\sum_{i}n_i (\bar{Y_{i.}}-\bar{Y_{..}})^2\) & a-1 & SSTR/(a-1) \\
Error (within treatments) & \(\sum_{i}\sum_{j}(Y_{ij}-\bar{Y_{i.}})^2\) & N-a & SSE/(N-a) \\
Total (corrected) & \(\sum_{i}n_i (\bar{Y_{i.}}-\bar{Y_{..}})^2\) & N-1 & \\
\end{longtable}

Linear Model Explanation of ANOVA

\hypertarget{cell-means-model}{%
\subsubsection{Cell means model}\label{cell-means-model}}

\[
Y_{ij}=\mu_i+\epsilon\_{ij}
\]

where

\begin{itemize}
\item
  \(Y_{ij}\) response variable in \(j\)-th subject for the \(i\)-th treatment
\item
  \(\mu_i\): parameters (fixed) representing the unknown population mean for the i-th treatment
\item
  \(\epsilon_{ij}\) independent \(N(0,\sigma^2)\) errors
\item
  \(E(Y_{ij})=\mu_i\) \(var(Y_{ij})=var(\epsilon_{ij})=\sigma^2\)
\item
  All observations have the same variance
\end{itemize}

Example:

\(a = 3\) (3 treatments) \(n_1=n_2=n_3=2\)

\[
\begin{aligned}
\left(\begin{array}{c} 
Y_{11}\\
Y_{12}\\
Y_{21}\\
Y_{22}\\
Y_{31}\\
Y_{32}\\
\end{array}\right) &= 
\left(\begin{array}{ccc} 
1 & 0 & 0 \\ 
1 & 0 & 0 \\ 
0 & 1 & 0 \\ 
0 & 1 & 0 \\ 
0 & 0 & 1 \\ 
0 & 0 & 1 \\ 
\end{array}\right)
\left(\begin{array}{c}
\mu_1 \\
\mu_2 \\
\mu_3 \\
\end{array}\right) + \left(\begin{array}{c}
\epsilon_{11} \\
\epsilon_{12} \\
\epsilon_{21} \\
\epsilon_{22} \\
\epsilon_{31} \\
\epsilon_{32} \\
\end{array}\right)\\
\mathbf{y} &= \mathbf{X\beta} +\mathbf{\epsilon}
\end{aligned}
\]

\(X_{k,ij}=1\) if the \(k\)-th treatment is used

\(X_{k,ij}=0\) Otherwise

\textbf{Note}: no intercept term.

\begin{equation}
\begin{aligned}
\mathbf{b}= \left[\begin{array}{c}
\mu_1 \\
\mu_2 \\
\mu_3 \\
\end{array}\right] &= 
(\mathbf{x}'\mathbf{x})^{-1}\mathbf{x}'\mathbf{y} \\
& = 
\left[\begin{array}{ccc}
n_1 & 0 & 0\\
0 & n_2 & 0\\
0 & 0 & n_3 \\
\end{array}\right]^{-1}
\left[\begin{array}{c}
Y_1\\
Y_2\\
Y_3\\
\end{array}\right] \\
& = 
\left[\begin{array}{c}
\bar{Y_1}\\
\bar{Y_2}\\
\bar{Y_3}\\
\end{array}\right] 
\end{aligned} 
\label{eq:betaorigin}
\end{equation}

is the BLUE (best linear unbiased estimator) for \(\beta=[\mu_1 \mu_2\mu_3]'\)

\[
E(\mathbf{b})=\beta
\]

\[
var(\mathbf{b})=\sigma^2(\mathbf{X'X})^{-1}=\sigma^2
\left[\begin{array}{ccc}
1/n_1 & 0 & 0\\
0 & 1/n_2 & 0\\
0 & 0 & 1/n_3\\
\end{array}\right]
\]

\(var(b_i)=var(\hat{\mu_i})=\sigma^2/n_i\) where \(\mathbf{b} \sim N(\beta,\sigma^2(\mathbf{X'X})^{-1})\)

\[
\begin{aligned}
MSE &= \frac{1}{N-a} \sum_{i}\sum_{j}(Y_{ij}-\bar{Y_{i.}})^2 \\
    &= \frac{1}{N-a} \sum_{i}[(n_i-1)\frac{\sum_{i}(Y_{ij}-\bar{Y_{i.}})^2}{n_i-1}] \\
    &= \frac{1}{N-a} \sum_{i}(n_i-1)s_1^2
\end{aligned}
\]

We have \(E(s_i^2)=\sigma^2\)

\(E(MSE)=\frac{1}{N-a}\sum_{i}(n_i-1)\sigma^2=\sigma^2\)

Hence, MSE is an unbiased estimator of \(\sigma^2\), regardless of whether the treatment means are equal or not.

\(E(MSTR)=\sigma^2+\frac{\sum_{i}n_i(\mu_i-\mu_.)^2}{a-1}\)\\
where \(\mu_.=\frac{\sum_{i=1}^{a}n_i\mu_i}{\sum_{i=1}^{a}n_i}\)\\
If all treatment means are equals (=\(\mu_.\)), \(E(MSTR)=\sigma^2\).

Then we can use an \(F\)-test for the equality of all treatment means:

\[H_0:\mu_1=\mu_2=..=\mu_a\]

\[H_a: not~al l~ \mu_i ~ are ~ equal \]

\(F=\frac{MSTR}{MSE}\)\\
where large values of F support \(H_a\) (since MSTR will tend to exceed MSE when \(H_a\) holds)\\
and F near 1 support \(H_0\) (upper tail test)

\textbf{Equivalently}, when \(H_0\) is true, \(F \sim f_{(a-1,N-a)}\)

\begin{itemize}
\tightlist
\item
  If \(F \leq f_{(a-1,N-a;1-\alpha)}\), we cannot reject \(H_0\)
\item
  If \(F \geq f_{(a-1,N-a;1-\alpha)}\), we reject \(H_0\)
\end{itemize}

Note: If \(a = 2\) (2 treatments), \(F\)-test = two sample \(t\)-test

\hypertarget{treatment-effects-factor-effects}{%
\subsubsection{Treatment Effects (Factor Effects)}\label{treatment-effects-factor-effects}}

Besides Cell means model, we have another way to formalize one-way ANOVA: \[Y_{ij} = \mu + \tau_i + \epsilon_{ij}\] where

\begin{itemize}
\tightlist
\item
  \(Y_{ij}\) is the \(j\)-th response for the \(i\)-th treatment
\item
  \(\tau_i\) is \(i\)-th treatment effect
\item
  \(\mu\) constant component, common to all observations
\item
  \(\epsilon_{ij}\) independent random errors \textasciitilde{} \(N(0,\sigma^2)\)
\end{itemize}

For example, \(a = 3\), \(n_1=n_2=n_3=2\)

\begin{equation} 
\begin{aligned}
\left(\begin{array}{c} 
Y_{11}\\
Y_{12}\\
Y_{21}\\
Y_{22}\\
Y_{31}\\
Y_{32}\\
\end{array}\right) &= 
\left(\begin{array}{cccc} 
1 & 1 & 0 & 0 \\ 
1 & 1 & 0 & 0 \\ 
1 & 0 & 1 & 0 \\ 
1 & 0 & 1 & 0 \\ 
1 & 0 & 0 & 1 \\ 
1 & 0 & 0 & 1 \\ 
\end{array}\right)
\left(\begin{array}{c}
\mu \\
\tau_1 \\
\tau_2 \\
\tau_3\\
\end{array}\right) + \left(\begin{array}{c}
\epsilon_{11} \\
\epsilon_{12} \\
\epsilon_{21} \\
\epsilon_{22} \\
\epsilon_{31} \\
\epsilon_{32} \\
\end{array}\right)\\
\mathbf{y} &= \mathbf{X\beta} +\mathbf{\epsilon} 
\end{aligned}
\label{eq:unsolvable}
\end{equation}

However,

\[
\mathbf{X'X} = 
\left(
\begin{array}
{cccc}
\sum_{i}n_i & n_1 & n_2 & n_3 \\
n_1 & n_1 & 0 & 0 \\
n_2 & 0 & n_2 & 0 \\
n_3 & 0 & 0 & n_3 \\
\end{array}
\right)
\]

is \textbf{singular} thus does not exist, \(\mathbf{b}\) is insolvable (infinite solutions)

Hence, we have to impose restrictions on the parameters to a model matrix \(\mathbf{X}\) of full rank.

Whatever restriction we use, we still have:

\(E(Y_{ij})=\mu + \tau_i = \mu_i = mean ~ response ~ for ~ i-th ~ treatment\)

\hypertarget{restriction-on-sum-of-tau}{%
\paragraph{Restriction on sum of tau}\label{restriction-on-sum-of-tau}}

\(\sum_{i=1}^{a}\tau_i=0\)

implies

\[
\mu= \mu +\frac{1}{a}\sum_{i=1}^{a}(\mu+\tau_i)
\]

is the average of the treatment mean (grand mean) (overall mean)

\[
\begin{aligned}
\tau_i  &=(\mu+\tau_i) -\mu = \mu_i-\mu \\
        &= \text{treatment  mean} - \text{grand~mean} \\
        &= \text{treatment  effect}
\end{aligned}
\]

\[
\tau_a=-\tau_1-\tau_2-...-\tau_{a-1}
\]

Hence, the mean for the a-th treatment is

\[
\mu_a=\mu+\tau_a=\mu-\tau_1-\tau_2-...-\tau_{a-1}
\]

Hence, the model need only ``a'' parameters:

\[
\mu,\tau_1,\tau_2,..,\tau_{a-1}
\]

Equation \eqref{eq:unsolvable} becomes

\begin{equation}
\begin{aligned}
\left(\begin{array}{c} 
Y_{11}\\
Y_{12}\\
Y_{21}\\
Y_{22}\\
Y_{31}\\
Y_{32}\\
\end{array}\right) &= 
\left(\begin{array}{ccc} 
1 & 1 & 0 \\ 
1 & 1 & 0 \\ 
1 & 0 & 1 \\ 
1 & 0 & 1 \\ 
1 & -1 & -1 \\ 
1 & -1 & -1 \\ 
\end{array}\right)
\left(\begin{array}{c}
\mu \\
\tau_1 \\
\tau_2 \\
\end{array}\right) + \left(\begin{array}{c}
\epsilon_{11} \\
\epsilon_{12} \\
\epsilon_{21} \\
\epsilon_{22} \\
\epsilon_{31} \\
\epsilon_{32} \\
\end{array}\right)\\
\mathbf{y} &= \mathbf{X\beta} +\mathbf{\epsilon}
\end{aligned}
\end{equation}

where \(\beta\equiv[\mu,\tau_1,\tau_2]'\)

Equation \eqref{eq:betaorigin} with \(\sum_{i}\tau_i=0\) becomes

\[
\begin{aligned}
\mathbf{b}= \left[\begin{array}{c}
\hat{\mu} \\
\hat{\tau_1} \\
\hat{\tau_2} \\
\end{array}\right] &= 
(\mathbf{x}'\mathbf{x})^{-1}\mathbf{x}'\mathbf{y} \\
& = 
\left[\begin{array}{ccc}
\sum_{i}n_i & n_1-n_3 & n_2-n_3\\
n_1-n_3 & n_1+n_3 & n_3\\
n_2-n_3 & n_3 & n_2-n_3 \\
\end{array}\right]^{-1}
\left[\begin{array}{c}
Y_{..}\\
Y_{1.}-Y_{3.}\\
Y_{2.}-Y_{3.}\\
\end{array}\right] \\
& =
\left[\begin{array}{c}
\frac{1}{3}\sum_{i=1}^{3}\bar{Y_{i.}}\\
\bar{Y_{1.}}-\frac{1}{3}\sum_{i=1}^{3}\bar{Y_{i.}}\\
\bar{Y_{2.}}-\frac{1}{3}\sum_{i=1}^{3}\bar{Y_{i.}}\\
\end{array}\right]\\
& = 
\left[\begin{array}{c}
\hat{\mu}\\
\hat{\tau_1}\\
\hat{\tau_2}\\
\end{array}\right]
\end{aligned}
\]

and \(\hat{\tau_3}=-\hat{\tau_1}-\hat{\tau_2}=\bar{Y_3}-\frac{1}{3} \sum_{i}\bar{Y_{i.}}\)

\hypertarget{restriction-on-first-tau}{%
\paragraph{Restriction on first tau}\label{restriction-on-first-tau}}

In R, \texttt{lm()} uses the restriction \(\tau_1=0\)

For the previous example, for \(n_1=n_2=n_3=2\), and \(\tau_1=0\).

Then the treatment means can be written as:

\[
\begin{aligned}
\mu_1 &= \mu + \tau_1 = \mu + 0 = \mu  \\
\mu_2 &= \mu + \tau_2 \\
\mu_3 &= \mu + \tau_3
\end{aligned}
\]

Hence, \(\mu\) is the mean response for the first treatment

In the matrix form,

\[
\begin{aligned}
\left(\begin{array}{c} 
Y_{11}\\
Y_{12}\\
Y_{21}\\
Y_{22}\\
Y_{31}\\
Y_{32}\\
\end{array}\right) &= 
\left(\begin{array}{ccc} 
1 & 0 & 0 \\ 
1 & 0 & 0 \\ 
1 & 1 & 0 \\ 
1 & 1 & 0 \\ 
1 & 0 & 1 \\ 
1 & 0 & 1 \\ 
\end{array}\right)
\left(\begin{array}{c}
\mu \\
\tau_2 \\
\tau_3 \\
\end{array}\right) + \left(\begin{array}{c}
\epsilon_{11} \\
\epsilon_{12} \\
\epsilon_{21} \\
\epsilon_{22} \\
\epsilon_{31} \\
\epsilon_{32} \\
\end{array}\right)\\
\mathbf{y} &= \mathbf{X\beta} +\mathbf{\epsilon}
\end{aligned}
\]

\(\beta = [\mu,\tau_2,\tau_3]'\)

\[
\begin{aligned}
\mathbf{b}= \left[\begin{array}{c}
\hat{\mu} \\
\hat{\tau_2} \\
\hat{\tau_3} \\
\end{array}\right] &= 
(\mathbf{x}'\mathbf{x})^{-1}\mathbf{x}'\mathbf{y} \\
& = 
\left[\begin{array}{ccc}
\sum_{i}n_i & n_2 & n_3\\
n_2 & n_2 & 0\\
n_3 & 0 & n_3 \\
\end{array}\right]^{-1}
\left[\begin{array}{c}
Y_{..}\\
Y_{2.}\\
Y_{3.}\\
\end{array}\right] \\
& = 
\left[
\begin{array}{c}
\bar{Y_{1.}} \\
\bar{Y_{2.}} - \bar{Y_{1.}} \\
\bar{Y_{3.}} - \bar{Y_{1.}}\\
\end{array}\right]
\end{aligned}
\]

\[
E(\mathbf{b})= \beta = 
\left[\begin{array}{c}
{\mu}\\
{\tau_2}\\
{\tau_3}\\
\end{array}\right]
=
\left[\begin{array}{c}
\mu_1\\
\mu_2-\mu_1\\
\mu_3-\mu_1\\
\end{array}\right]
\]

\[
\begin{aligned}
var(\mathbf{b}) &= \sigma^2(\mathbf{X'X})^{-1} \\
var(\hat{\mu}) &= var(\bar{Y_{1.}})=\sigma^2/n_1 \\
var(\hat{\tau_2}) &= var(\bar{Y_{2.}}-\bar{Y_{1.}}) = \sigma^2/n_2 + \sigma^2/n_1 \\
var(\hat{\tau_3}) &= var(\bar{Y_{3.}}-\bar{Y_{1.}}) = \sigma^2/n_3 + \sigma^2/n_1
\end{aligned}
\]

\textbf{Note} For all three parameterization, the ANOVA table is the same

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{cell-means-model-1}{Model 1}: \(Y_{ij} = \mu_i + \epsilon_{ij}\)
\item
  \protect\hyperlink{restriction-on-sum-of-tau}{Model 2}: \(Y_{ij} = \mu + \tau_i + \epsilon_{ij}\) where \(\sum_{i} \tau_i=0\)
\item
  \protect\hyperlink{restriction-on-first-tau}{Model 3}: \(Y_{ij}= \mu + \tau_i + \epsilon_{ij}\) where \(\tau_1=0\)
\end{itemize}

All models have the same calculation for \(\hat{Y}\) as

\[
\mathbf{\hat{Y} = X(X'X)^{-1}X'Y=PY = Xb}
\]

\textbf{ANOVA Table}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1410}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.5064}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.0641}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1346}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1346}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Source of Variation
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
SS
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
df
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
MS
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
F
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Between Treatments & \(\sum_{i} n _ i (\bar { Y_ {i .} } -\bar{Y_{..}})^2 = \mathbf{Y ' (P-P_1)Y}\) & a-1 & \(\frac{SSTR}{a-1}\) & \(\frac{MSTR}{MSE}\) \\
Error

(within treatments) & \(\sum_{i}\sum_{j}(Y_{ij} -\bar{Y_{i.}})^2=\mathbf{e'e}\) & N-a & \(\frac{SSE}{N-a}\) & \\
Total (corrected) & \(\sum_{i } n_i(\bar{Y_{i.}}-\bar{Y_{..}})^2=\mathbf{Y'Y - Y'P_1Y}\) & N-1 & & \\
\end{longtable}

where \(\mathbf{P_1} = \frac{1}{n}\mathbf{J}\)

The \(F\)-statistic here has \((a-1,N-a)\) degrees of freedom, which gives the same value for all three parameterization, but the hypothesis test is written a bit different:

\[
\begin{aligned}
&H_0 : \mu_1 = \mu_2 = ... = \mu_a \\
&H_0 : \mu + \tau_1 = \mu + \tau_2 = ... = \mu + \tau_a \\
&H_0 : \tau_1 = \tau_2 = ...= \tau_a 
\end{aligned}
\]

The \(F\)-test here serves as a preliminary analysis, to see if there is any difference at different factors. For more in-depth analysis, we consider different testing of treatment effects.

\hypertarget{testing-of-treatment-effects}{%
\subsubsection{Testing of Treatment Effects}\label{testing-of-treatment-effects}}

\begin{itemize}
\tightlist
\item
  A \protect\hyperlink{single-treatment-mean}{Single Treatment Mean} \(\mu_i\)
\item
  A \protect\hyperlink{differences-between-treatment-means}{Differences Between Treatment Means}
\item
  A \protect\hyperlink{contrast-among-treatment-means}{Contrast Among Treatment Means}
\item
  A \protect\hyperlink{linear-combination-of-treatment-means}{Linear Combination of Treatment Means}
\end{itemize}

\hypertarget{single-treatment-mean}{%
\paragraph{Single Treatment Mean}\label{single-treatment-mean}}

We have \(\hat{\mu_i}=\bar{Y_{i.}}\) where

\begin{itemize}
\tightlist
\item
  \(E(\bar{Y_{i.}})=\mu_i\)
\item
  \(var(\bar{Y_{i}})=\sigma^2/n_i\) estimated by \(s^2(\bar{Y_{i.}})=MSE / n_i\)
\end{itemize}

Since \(\frac{\bar{Y_{i.}}-\mu_i}{s(\bar{Y_{i.}})} \sim t_{N-a}\) and the confidence interval for \(\mu_i\) is \(\bar{Y_{i.}} \pm t_{1-\alpha/2;N-a}s(\bar{Y_{i.}})\),\\
then we can do a t-test for the means difference with some constant \(c\)

\[
\begin{aligned}
&H_0: \mu_i = c \\
&H_1: \mu_i \neq c
\end{aligned}
\]

where

\[
T =\frac{\bar{Y_{i.}}-c}{s(\bar{Y_{i.}})}
\]

follows \(t_{N-a}\) when \(H_0\) is true.\\
If \(|T| > t_{1-\alpha/2;N-a}\), we can reject \(H_0\)

\hypertarget{differences-between-treatment-means}{%
\paragraph{Differences Between Treatment Means}\label{differences-between-treatment-means}}

Let \(D=\mu_i - \mu_i'\), also known as \textbf{pairwise comparison}\\
\(D\) can be estimated by \(\hat{D}=\bar{Y_{i}}-\bar{Y_{i}}'\) is unbiased (\(E(\hat{D})=\mu_i-\mu_i'\))

Since \(\bar{Y_{i}}\) and \(\bar{Y_{i}}'\) are independent, then

\[
var(\hat{D})=var(\bar{Y_{i}}) + var(\bar{Y_{i'}}) = \sigma^2(1/n_i + 1/n_i')
\]

can be estimated with

\[
s^2(\hat{D}) = MSE(1/n_i + 1/n_i')
\]

With the single treatment inference,

\[
\frac{\hat{D}-D}{s(\hat{D})} \sim t_{N-a}
\]

hence,

\[
\hat{D} \pm t_{(1-\alpha/2;N-a)}s(\hat{D})
\]

Hypothesis tests:

\[
\begin{aligned}
&H_0: \mu_i = \mu_i' \\
&H_a: \mu_i \neq \mu_i'
\end{aligned}
\]

can be tested by the following statistic

\[
T = \frac{\hat{D}}{s(\hat{D})} \sim t_{1-\alpha/2;N-a}
\]

reject \(H_0\) if \(|T| > t_{1-\alpha/2;N-a}\)

\hypertarget{contrast-among-treatment-means}{%
\paragraph{Contrast Among Treatment Means}\label{contrast-among-treatment-means}}

generalize the comparison of two means, we have \textbf{contrasts}

A contrast is a linear combination of treatment means:

\[
L = \sum_{i=1}^{a}c_i \mu_i
\]

where each \(c_i\) is non-random constant and sum to 0:

\[
\sum_{i=1}^{a} c_i = 0
\]

An unbiased estimator of a contrast L is

\[
\hat{L} = \sum_{i=1}^{a}c_i \bar{Y}_{i.}
\]

and \(E(\hat{L}) = L\). Since the \(\bar{Y}_{i.}\), i = 1,\ldots, a are independent.

\[
\begin{aligned}
var(\hat{L}) &= var(\sum_{i=1}^a c_i \bar{Y}_{i.}) = \sum_{i=1}^a var(c_i \bar{Y}_i)  \\
&= \sum_{i=1}^a c_i^2 var(\bar{Y}_i) = \sum_{i=1}^a c_i^2 \sigma^2 /n_i \\
&= \sigma^2 \sum_{i=1}^{a} c_i^2 /n_i
\end{aligned}
\]

Estimation of the variance:

\[
s^2(\hat{L}) = MSE \sum_{i=1}^{a} \frac{c_i^2}{n_i}
\]

\(\hat{L}\) is normally distributed (since it is a linear combination of independent normal random variables).

Then, since \(SSE/\sigma^2\) is \(\chi_{N-a}^2\)

\[
\frac{\hat{L}-L}{s(\hat{L})} \sim t_{N-a}
\]

A \(1-\alpha\) confidence limits are given by

\[
\hat{L} \pm t_{1-\alpha/2; N-a}s(\hat{L})
\]

Hypothesis testing

\[
\begin{aligned}
&H_0: L = 0 \\
&H_a: L \neq 0
\end{aligned}
\]

with

\[
T = \frac{\hat{L}}{s(\hat{L})}
\]

reject \(H_0\) if \(|T| > t_{1-\alpha/2;N-a}\)

\hypertarget{linear-combination-of-treatment-means}{%
\paragraph{Linear Combination of Treatment Means}\label{linear-combination-of-treatment-means}}

just like contrast \(L = \sum_{i=1}^a c_i \mu_i\) but no restrictions on the \(c_i\) coefficients.

Tests on a single treatment mean, two treatment means, and contrasts can all be considered form the same perspective.

\[
\begin{aligned}
&H_0: \sum c_i \mu_i = c \\
&H_a: \sum c_i \mu_i \neq c 
\end{aligned}
\]

The test statistics ( \(t\)-stat) can be considered equivalently as \(F\)-tests; \(F = (T)^2\) where \(F \sim F_{1,N-a}\). Since the numerator degrees of freedom is always 1 in these cases, we refer to them as single-degree-of-freedom tests.

\textbf{Multiple Contrasts}

To test simultaneously \(k \ge 2\) contrasts, let \(T_1,...,T_k\) be the t-stat. The joint distribution of these random variables is a multivariate t-distribution (the tests are dependent since they re based on the same data).

Limitations for comparing multiple contrasts:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The confidence coefficient \(1-\alpha\) only applies to a particular estimate, not a series of estimates; similarly, the Type I error rate, \(\alpha\), applies to a particular test, not a series of tests. Example: 3 \(t\)-tests at \(\alpha = 0.05\), if tests are independent (which they are not), \(0.95^3 = 0.857\) (thus \(\alpha - 0.143\) not 0.05)\\
\item
  The confidence coefficient \(1-\alpha\) and significance level \(\alpha\) are appropriate only if the test was not suggest by the data.

  \begin{itemize}
  \tightlist
  \item
    often, the results of an experiment suggest important (i.e.,..g, potential significant) relationships.
  \item
    the process of studying effects suggests by the data is called \textbf{data snooping}
  \end{itemize}
\end{enumerate}

Multiple Comparison Procedures:

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{tukey}{Tukey}
\item
  \protect\hyperlink{scheffe}{Scheffe}
\item
  \protect\hyperlink{bonferroni}{Bonferroni}
\end{itemize}

\hypertarget{tukey}{%
\subparagraph{Tukey}\label{tukey}}

All pairwise comparisons of factor level means. All pairs \(D = \mu_i - \mu_i'\) or all tests of the form:

\[
\begin{aligned}
&H_0: \mu_i -\mu_i' = 0 \\
&H_a: \mu_i - \mu_i' \neq 0
\end{aligned}
\]

\begin{itemize}
\tightlist
\item
  When all sample sizes are equal (\(n_1 = n_2 = ... = n_a\)) then the Tukey method family confidence coefficient is exactly \(1-\alpha\) and the significance level is exactly \(\alpha\)\\
\item
  When the sample sizes are not equal, the family confidence coefficient is greater than \(1-\alpha\) (i.e., the significance level is less than \(\alpha\)) so the test \textbf{conservative}\\
\item
  Tukey considers the \textbf{studentized range distribution}. If we have \(Y_1,..,Y_r\), observations from a normal distribution with mean \(\alpha\) and variance \(\sigma^2\). Define: \[
  w = max(Y_i) - min(Y_i)
  \] as the range of the observations. Let \(s^2\) be an estimate of \(\sigma^2\) with v degrees of freedom. Then, \[
  q(r,v) = \frac{w}{s}
  \] is called the studentized range. The distribution of q uses a special table.
\end{itemize}

\textbf{Notes}

\begin{itemize}
\tightlist
\item
  when we are not interested in testing all pairwise comparison,s the confidence coefficient for the family of comparisons under consideration will be greater than \(1-\alpha\) (with the significance level less than \(\alpha\))
\item
  Tukey can be used for ``data snooping'' as long as the effects to be studied on the basis of preliminary data analysis are pairwise comparisons.
\end{itemize}

\hypertarget{scheffe}{%
\subparagraph{Scheffe}\label{scheffe}}

This method applies when the family of interest is the set of possible contrasts among the treatment means:

\[
L = \sum_{i=1}^a c_i \mu_i
\]

where \(\sum_{i=1}^a c_i =0\)

That is, the family of all possible contrasts \(L\) or

\[
\begin{aligned}
&H_0: L = 0 \\
&H_a: L \neq 0
\end{aligned}
\]

The family confidence level for the Scheffe procedure is exactly \(1-\alpha\) (i.e., significance level = \(\alpha\)) whether the sample sizes are equal or not.

For simultaneous confidence intervals,

\[
\hat{L} \pm Ss(\hat{L})
\]

where \(\hat{L}=\sum c_i \bar{Y}_{i.},s^2(\hat{L}) = MSE \sum c_i^2/n_i\) and \(S^2 = (a-1)f_{1-\alpha;a-1,N-a}\)

The Scheffe procedure considers

\[
F = \frac{\hat{L}^2}{(a-1)s^2(\hat{L})}
\]

where we reject \(H_0\) at the family significance level \(\alpha\) if \(F > f_{(1-\alpha;a-1,N-a)}\)

\textbf{Note}

\begin{itemize}
\tightlist
\item
  Since applications of the Scheffe never involve all conceivable contrasts, the \textbf{finite family} confidence coefficient will be larger than \(1-\alpha\), so \(1-\alpha\) is a lower bound. Thus, people often consider a larger \(\alpha\) (e.g., 90\% confidence interval)
\item
  Scheffe can be used for ``data scooping'' since the family of statements contains all possible contrasts.
\item
  If only pairwise comparisons are to be considered, The Tukey procedure gives narrower confidence limits.
\end{itemize}

\hypertarget{bonferroni}{%
\subparagraph{Bonferroni}\label{bonferroni}}

Applicable whether the sample sizes are equal or unequal.

For the confidence intervals,

\[
\hat{L} \pm B s(\hat{L})
\]

where \(B= t_{(1-\alpha/(2g);N-a)}\) and g is the number of comparisons in the family.

Hypothesis testing

\[
\begin{aligned}
&H_0: L = 0 \\
&H_a: L \neq 0 
\end{aligned}
\]

Let \(T= \frac{\hat{L}}{s(\hat{L})}\) and reject \(H_0\) if \(|T|>t_{1-\alpha/(2g),N-a}\)

\textbf{Notes}

\begin{itemize}
\tightlist
\item
  If all pairwise comparisons are of interest, the Tukey procedure is superior (narrower confidence intervals). If not, Bonferroni may be better.
\item
  Bonferroni is better than Scheffe when the number of contrasts is about the same as the treatment levels (or less).
\item
  Recommendation: compute all threes and pick the smallest.
\item
  Bonferroni can't be used for \textbf{data snooping}
\end{itemize}

\hypertarget{fishers-lsd}{%
\subparagraph{Fisher's LSD}\label{fishers-lsd}}

does not control for family error rate

use \(t\)-stat for testing

\[
H_0: \mu_i = \mu_j
\]

t-stat

\[
t = \frac{\bar{y}_i - \bar{y}_j}{\sqrt{MSE(\frac{1}{n_i}+ \frac{1}{n_j})}}
\]

\hypertarget{newman-keuls}{%
\subparagraph{Newman-Keuls}\label{newman-keuls}}

Do not recommend using this test since it has less power than ANOVA.

\hypertarget{multiple-comparisons-with-a-control}{%
\paragraph{Multiple comparisons with a control}\label{multiple-comparisons-with-a-control}}

\hypertarget{dunnett}{%
\subparagraph{Dunnett}\label{dunnett}}

We have \(a\) groups where the last group is the control group, and the \(a-1\) treatment groups.

Then, we compare treatment groups to the control group. Hence, we have \(a-1\) contrasts (i.e., \(a-1\) pairwise comparisons)

\hypertarget{summary-4}{%
\paragraph{Summary}\label{summary-4}}

When choosing a multiple contrast method:

\begin{itemize}
\item
  Pairwise

  \begin{itemize}
  \tightlist
  \item
    Equal groups sizes: \protect\hyperlink{tukey}{Tukey}
  \item
    Unequal groups sizes: \protect\hyperlink{tukey}{Tukey}, \protect\hyperlink{scheffe}{Scheffe}
  \end{itemize}
\item
  Not pairwise

  \begin{itemize}
  \tightlist
  \item
    with control: \protect\hyperlink{dunnett}{Dunnett}
  \item
    general: \protect\hyperlink{bonferroni}{Bonferroni}, \protect\hyperlink{scheffe}{Scheffe}
  \end{itemize}
\end{itemize}

\hypertarget{single-factor-random-effects-model}{%
\subsection{Single Factor Random Effects Model}\label{single-factor-random-effects-model}}

Also known as ANOVA Type II models.

Treatments are chosen at from from larger population. We extend inference to all treatments in the population and not restrict our inference to those treatments that happened to be selected for the study.

\hypertarget{random-cell-means}{%
\subsubsection{Random Cell Means}\label{random-cell-means}}

\[
Y_{ij} = \mu_i + \epsilon_{ij}
\]

where

\begin{itemize}
\tightlist
\item
  \(\mu_i \sim N(\mu, \sigma^2_{\mu})\) and independent
\item
  \(\epsilon_{ij} \sim N(0,\sigma^2)\) and independent
\end{itemize}

\(\mu_i\) and \(\epsilon_{ij}\) are mutually independent for \(i =1,...,a; j = 1,...,n\)

With all treatment sample sizes are equal

\[
\begin{aligned}
E(Y_{ij}) &= E(\mu_i) = \mu \\
var(Y_{ij}) &= var(\mu_i) + var(\epsilon_i) = \sigma^2_{\mu} + \sigma^2
\end{aligned}
\]

Since \(Y_{ij}\) are not independent

\[
\begin{aligned}
cov(Y_{ij},Y_{ij'}) &= E(Y_{ij}Y_{ij'}) - E(Y_{ij})E(Y_{ij'})  \\
&= E(\mu_i^2 + \mu_i \epsilon_{ij'} + \mu_i \epsilon_{ij} + \epsilon_{ij}\epsilon_{ij'}) - \mu^2 \\
&= \sigma^2_{\mu} + \mu^2 - \mu^2 & \text{if} j \neq j' \\
&= \sigma^2_{\mu} & \text{if} j \neq j' 
\end{aligned}
\]

\[
\begin{aligned}
cov(Y_{ij},Y_{i'j'}) &= E(\mu_i \mu_{i'} + \mu_i \epsilon_{i'j'}+ \mu_{i'}\epsilon_{ij}+ \epsilon_{ij}\epsilon_{i'j'}) - \mu^2 \\
&= \mu^2 - \mu^2 & \text{if } i \neq i' \\
&= 0 \\
\end{aligned}
\]

Hence,

\begin{itemize}
\tightlist
\item
  all observations have the same variance
\item
  any two observations from the same treatment have covariance \(\sigma^2_{\mu}\)
\item
  The correlation between any two responses from the same treatment:\\
  \[
  \begin{aligned}
  \rho(Y_{ij},Y_{ij'}) &= \frac{\sigma^2_{\mu}}{\sigma^2_{\mu}+ \sigma^2} && \text{$j \neq j'$}
  \end{aligned}
  \]
\end{itemize}

\textbf{Inference}

\textbf{Intraclass Correlation Coefficient}

\[
\frac{\sigma^2_{\mu}}{\sigma^2 + \sigma^2_{\mu}}
\]

which measures the proportion of total variability of \(Y_{ij}\) accounted for by the variance of \(\mu_i\)

\[
\begin{aligned}
&H_0: \sigma_{\mu}^2 = 0 \\
&H_a: \sigma_{\mu}^2 \neq 0
\end{aligned}
\]

\(H_0\) implies \(\mu_i = \mu\) for all i, which can be tested by the F-test in ANOVA.

The understandings of the \protect\hyperlink{single-factor-fixed-effects-model}{Single Factor Fixed Effects Model} and the \protect\hyperlink{single-factor-random-effects-model}{Single Factor Random Effects Model} are different, the ANOVA is same for the one factor model. The difference is in the expected mean squares

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.3868}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.6132}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Random Effects} Model
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Fixed Effects} Model
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(E(MSE) = \sigma^2\) & \(E(MSE) = \sigma^2\) \\
\(E(M STR) = \sigma^2 - n \sigma^2_\mu\) & \(E(MSTR) = \sigma^2 + \frac{ \sum_i n_i (\mu_i - \mu)^2}{a-1}\) \\
\end{longtable}

If \(\sigma^2_\mu\), then MSE and MSTR have the same expectation (\(\sigma^2\)). Otherwise, \(E(MSTR) >E(MSE)\). Large values of the statistic

\[
F = \frac{MSTR}{MSE}
\]

suggest we reject \(H_0\).

Since \(F \sim F_{(a-1,a(n-1))}\) when \(H_0\) holds. If \(F > f_{(1-\alpha;a-1,a(n-1))}\) we reject \(H_0\).

If sample sizes are not equal, \(F\)-test can still be used, but the df are \(a-1\) and \(N-a\).

\hypertarget{estimation-of-mu}{%
\paragraph{\texorpdfstring{Estimation of \(\mu\)}{Estimation of \textbackslash mu}}\label{estimation-of-mu}}

An unbiased estimator of \(E(Y_{ij})=\mu\) is the grand mean: \(\hat{\mu} = \hat{Y}_{..}\)

The variance of this estimator is

\[
\begin{aligned}
var(\bar{Y}_{..}) &= var(\sum_i \bar{Y}_{i.}/a) \\
&= \frac{1}{a^2}\sum_ivar(\bar{Y}_{i.}) \\
&= \frac{1}{a^2}\sum_i(\sigma^2_\mu+\sigma^2/n) \\
&= \frac{1}{a^2}(\sigma^2_{\mu}+\sigma^2/n) \\
&= \frac{n\sigma^2_{\mu}+ \sigma^2}{an}
\end{aligned}
\]

An unbiased estimator of this variance is \(s^2(\bar{Y})=\frac{MSTR}{an}\). Thus \(\frac{\bar{Y}_{..}-\mu}{s(\bar{Y}_{..})} \sim t_{a-1}\)

A \(1-\alpha\) confidence interval is \(\bar{Y}_{..} \pm t_{(1-\alpha/2;a-1)}s(\bar{Y}_{..})\)

\hypertarget{estimation-of-sigma2_musigma2_musigma2}{%
\paragraph{\texorpdfstring{Estimation of \(\sigma^2_\mu/(\sigma^2_{\mu}+\sigma^2)\)}{Estimation of \textbackslash sigma\^{}2\_\textbackslash mu/(\textbackslash sigma\^{}2\_\{\textbackslash mu\}+\textbackslash sigma\^{}2)}}\label{estimation-of-sigma2_musigma2_musigma2}}

In the random and fixed effects model, MSTR and MSE are independent. When the sample sizes are equal (\(n_i = n\) for all i),

\[
\frac{\frac{MSTR}{n\sigma^2_\mu+ \sigma^2}}{\frac{MSE}{\sigma^2}} \sim f_{(a-1,a(n-1))}
\]

\[
P(f_{(\alpha/2;a-1,a(n-1))}\le \frac{\frac{MSTR}{n\sigma^2_\mu+ \sigma^2}}{\frac{MSE}{\sigma^2}} \le f_{(1-\alpha/2;a-1,a(n-1))}) = 1-\alpha
\]

\[
\begin{aligned}
L &= \frac{1}{n}(\frac{MSTR}{MSE}(\frac{1}{f_{(1-\alpha/2;a-1,a(n-1))}})-1) \\
U &= \frac{1}{n}(\frac{MSTR}{MSE}(\frac{1}{f_{(\alpha/2;a-1,a(n-1))}})-1)
\end{aligned}
\]

The lower and upper \((L^*,U^*)\) confidence limits for \(\frac{\sigma^2_\mu}{\sigma^2_\mu + \sigma^2}\)

\[
\begin{aligned}
L^* &= \frac{L}{1+L} \\
U^* &= \frac{U}{1+U}
\end{aligned}
\]

If the lower limit for \(\frac{\sigma^2_\mu}{\sigma^2}\) is negative, it is customary to set \(L = 0\).

\hypertarget{estimation-of-sigma2}{%
\paragraph{\texorpdfstring{Estimation of \(\sigma^2\)}{Estimation of \textbackslash sigma\^{}2}}\label{estimation-of-sigma2}}

\(a(n-1)MSE/\sigma^2 \sim \chi^2_{a(n-1)}\), the \((1-\alpha)\) confidence interval for \(\sigma^2\):

\[
\frac{a(n-1)MSE}{\chi^2_{1-\alpha/2;a(n-1)}} \le \sigma^2 \le \frac{a(n-1)MSE}{\chi^2_{\alpha/2;a(n-1)}}
\]

can also be used in case sample sizes are not equal - then df is N-a.

\hypertarget{estimation-of-sigma2_mu}{%
\paragraph{\texorpdfstring{Estimation of \(\sigma^2_\mu\)}{Estimation of \textbackslash sigma\^{}2\_\textbackslash mu}}\label{estimation-of-sigma2_mu}}

\(E(MSE) = \sigma^2\) \(E(MSTR) = \sigma^2 + n\sigma^2_\mu\). Hence,

\[
\sigma^2_{\mu} = \frac{E(MSTR)- E(MSE)}{n}
\]

An unbiased estimator of \(\sigma^2_\mu\) is given by

\[
s^2_\mu =\frac{MSTR-MSE}{n}
\]

if \(s^2_\mu < 0\), set \(s^2_\mu = 0\)

If sample sizes are not equal,

\[
s^2_\mu = \frac{MSTR - MSE}{n'}
\]

where \(n' = \frac{1}{a-1}(\sum_i n_i- \frac{\sum_i n^2_i}{\sum_i n_i})\)

no exact confidence intervals for \(\sigma^2_\mu\), but we can approximate intervals.

\textbf{Satterthewaite Procedure} can be used to construct approximate confidence intervals for linear combination of expected mean squares\\
A linear combination:

\[
\sigma^2_\mu = \frac{1}{n} E(MSTR) + (-\frac{1}{n}) E(MSE)
\]

\[
S = d_1 E(MS_1) + ..+ d_h E(MS_h)
\]

where \(d_i\) are coefficients.

An unbiased estimator of S is

\[
\hat{S} = d_1 MS_1 + ...+ d_h  MS_h 
\]

Let \(df_i\) be the degrees of freedom associated with the mean square \(MS_i\). The \textbf{Satterthwaite} approximation:

\[
\frac{(df)\hat{S}}{S} \sim \chi^2_{df}
\]

where

\[
df = \frac{(d_1MS_1+...+d_hMS_h)^2}{(d_1MS_1)^2/df_1 + ...+ (d_hMS_h)^2/df_h}
\]

An approximate \(1-\alpha\) confidence interval for S:

\[
\frac{(df)\hat{S}}{\chi^2_{1-\alpha/2;df}} \le S \le \frac{(df)\hat{S}}{\chi^2_{\alpha/2;df}}
\]

For the single factor random effects model

\[
\frac{(df)s^2_\mu}{\chi^2_{1-\alpha/2;df}} \le \sigma^2_\mu \le \frac{(df)s^2_\mu}{\chi^2_{\alpha/2;df}}
\]

where

\[
df = \frac{(sn^2_\mu)^2}{\frac{(MSTR)^2}{a-1}+ \frac{(MSE)^2}{a(n-1)}}
\]

\hypertarget{random-treatment-effects-model}{%
\subsubsection{Random Treatment Effects Model}\label{random-treatment-effects-model}}

\[
\tau_i = \mu_i - E(\mu_i) = \mu_i - \mu
\]

we have \(\mu_i = \mu + \tau_i\) and

\[
Y_{ij} = \mu + \tau_i + \epsilon_{ij}
\]

where

\begin{itemize}
\tightlist
\item
  \(\mu\) = constant, common to all observations
\item
  \(\tau_i \sim N(0,\sigma^2_\tau)\) independent (random variables)
\item
  \(\epsilon_{ij} \sim N(0,\sigma^2)\) independent.
\item
  \(\tau_{i}, \epsilon_{ij}\) are independent (i=1,\ldots,a; j =1,..,n)
\item
  our model is concerned with only balanced single factor ANOVA.
\end{itemize}

\textbf{Diagnostics Measures}

\begin{itemize}
\tightlist
\item
  Non-constant error variance (plots, Levene test, Hartley test).
\item
  Non-independence of errors (plots, Durban-Watson test).
\item
  Outliers (plots, regression methods).
\item
  Non-normality of error terms (plots, Shapiro-Wilk, Anderson-Darling).
\item
  Omitted Variable Bias (plots)
\end{itemize}

\textbf{Remedial}

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{weighted-least-squares}{Weighted Least Squares}
\item
  {[}Transformations{]}
\item
  Non-parametric Procedures.
\end{itemize}

\textbf{Note}

\begin{itemize}
\item
  Fixed effect ANOVA is relatively robust to

  \begin{itemize}
  \tightlist
  \item
    non-normality
  \item
    unequal variances when sample sizes are approximately equal; at least the F-test and multiple comparisons. However, single comparisons of treatment means are sensitive to unequal variances.
  \end{itemize}
\item
  Lack of independence can seriously affect both fixed and random effect ANVOA.
\end{itemize}

\hypertarget{two-factor-fixed-effect-anova}{%
\subsection{Two Factor Fixed Effect ANOVA}\label{two-factor-fixed-effect-anova}}

The multi-factor experiment is

\begin{itemize}
\tightlist
\item
  more efficient
\item
  provides more info
\item
  gives more validity to the findings.
\end{itemize}

\hypertarget{balanced}{%
\subsubsection{Balanced}\label{balanced}}

Assumption:

\begin{itemize}
\tightlist
\item
  All treatment sample sizes are equal
\item
  All treatment means are of equal importance
\end{itemize}

Assume:

\begin{itemize}
\tightlist
\item
  Factor \(A\) has \texttt{a} levels and Factor \(B\) has \texttt{b} levels. All \(a \times b\) factor levels are considered.
\item
  The number of treatments for each level is n.~\(N = abn\) observations in the study.
\end{itemize}

\hypertarget{cell-means-model-1}{%
\paragraph{Cell Means Model}\label{cell-means-model-1}}

\[
Y_{ijk} = \mu_{ij} + \epsilon_{ijk}
\]

where

\begin{itemize}
\tightlist
\item
  \(\mu_{ij}\) are fixed parameters (cell means)
\item
  \(i = 1,...,a\) = the levels of Factor A
\item
  \(j = 1,...,b\) = the levels of Factor B.
\item
  \(\epsilon_{ijk} \sim \text{indep } N(0,\sigma^2)\) for \(i = 1,...,a\), \(j = 1,..,b\) and \(k = 1,..,n\)
\end{itemize}

And

\[
\begin{aligned}
E(Y_{ijk}) &= \mu_{ij} \\
var(Y_{ijk}) &= var(\epsilon_{ijk}) = \sigma^2
\end{aligned}
\]

Hence,

\[
Y_{ijk} \sim \text{indep } N(\mu_{ij},\sigma^2)
\]

And the model is\\

\[
\mathbf{Y} = \mathbf{X} \beta + \epsilon
\]

Thus,

\[
\begin{aligned}
E(\mathbf{Y}) &= \mathbf{X}\beta \\
var(\mathbf{Y}) &= \sigma^2 \mathbf{I}
\end{aligned}
\]

\textbf{Interaction}

\[
(\alpha \beta)_{ij} = \mu_{ij} - (\mu_{..}+ \alpha_i + \beta_j)
\]

where

\begin{itemize}
\tightlist
\item
  \(\mu_{..} = \sum_i \sum_j \mu_{ij}/ab\) is the grand mean
\item
  \(\alpha_i = \mu_{i.}-\mu_{..}\) is the main effect for factor \(A\) at the \(i\)-th level
\item
  \(\beta_j = \mu_{.j} - \mu_{..}\) is the main effect for factor \(B\) at the \(j\)-th level
\item
  \((\alpha \beta)_{ij}\) is the interaction effect when factor \(A\) is at the \(i\)-th level and factor \(B\) is at the \(j\)-th level.
\item
  \((\alpha \beta)_{ij} = \mu_{ij} - \mu_{i.}-\mu_{.j}+ \mu_{..}\)
\end{itemize}

Examine interactions:

\begin{itemize}
\tightlist
\item
  Examine whether all \(\mu_{ij}\) can be expressed as the sums \(\mu_{..} + \alpha_i + \beta_j\)
\item
  Examine whether the difference between the mean responses for any two levels of factor \(B\) is the same for all levels of factor \(A\).
\item
  Examine whether the difference between the mean response for any two levels of factor \(A\) is the same for all levels of factor \(B\)
\item
  Examine whether the treatment mean curves for the different factor levels in a treatment plot are parallel.
\end{itemize}

For \(j = 1,...,b\)

\[
\begin{aligned}
\sum_i(\alpha \beta)_{ij} &= \sum_i (\mu_{ij} - \mu_{..} - \alpha_i - \beta_j) \\
&= \sum_i \mu_{ij} - a \mu_{..} - \sum_i \alpha_i - a \beta_j \\
&= a \mu_{.j} - a \mu_{..}- \sum_i (\mu_{i.} - \mu_{..}) - a(\mu_{.j}-\mu_{..}) \\
&= a \mu_{.j} - a \mu_{..} - a \mu_{..}+ a \mu_{..} - a (\mu_{.j} - \mu_{..}) \\
&= 0
\end{aligned}
\]

Similarly, \(\sum_j (\alpha \beta) = 0, i = 1,...,a\) and \(\sum_i \sum_j (\alpha \beta)_{ij} =0\), \(\sum_i \alpha_i = 0\), \(\sum_j \beta_j = 0\)

\hypertarget{factor-effects-model}{%
\paragraph{Factor Effects Model}\label{factor-effects-model}}

\[
\begin{aligned}
\mu_{ij} &= \mu_{..} + \alpha_i + \beta_j + (\alpha \beta)_{ij} \\
Y_{ijk} &= \mu_{..} + \alpha_i + \beta_j + (\alpha \beta)_{ij} + \epsilon_{ijk}
\end{aligned}
\]

where

\begin{itemize}
\tightlist
\item
  \(\mu_{..}\) is a constant
\item
  \(\alpha_i\) are constants subject to the restriction \(\sum_i \alpha_i=0\)
\item
  \(\beta_j\) are constants subject to the restriction \(\sum_j \beta_j = 0\)
\item
  \((\alpha \beta)_{ij}\) are constants subject to the restriction \(\sum_i(\alpha \beta)_{ij} = 0\) for \(j=1,...,b\) and \(\sum_j(\alpha \beta)_{ij} = 0\) for \(i = 1,...,a\)
\item
  \(\epsilon_{ijk} \sim \text{indep } N(0,\sigma^2)\) for \(k = 1,..,n\)
\end{itemize}

We have

\[
\begin{aligned}
E(Y_{ijk}) &= \mu_{..} + \alpha_i + \beta_j + (\alpha \beta)_{ij}\\
var(Y_{ijk}) &= \sigma^2 \\
Y_{ijk} &\sim N (\mu_{..} + \alpha_i + \beta_j + (\alpha \beta)_{ij}, \sigma^2)
\end{aligned}
\]

We have \(1+a+b+ab\) parameters. But there are \(ab\) parameters in the \protect\hyperlink{cell-means-model-1}{Cell Means Model}. In the \protect\hyperlink{factor-effects-model}{Factor Effects Model}, the restrictions limit the number of parameters that can be estimated:

\[
\begin{aligned}
1 &\text{ for } \mu_{..} \\
(a-1) &\text{ for } \alpha_i \\
(b-1) &\text{ for } \beta_j \\
(a-1)(b-1) &\text{ for } (\alpha \beta)_{ij}
\end{aligned}
\]

Hence, there are

\[
1 + a - 1 + b - 1 + ab - a- b + 1 = ab
\]

parameters in the model.

We can have several restrictions when considering the model in the form \(\mathbf{Y} = \mathbf{X} \beta + \epsilon\)

One way:

\[
\begin{aligned}
\alpha_a  &= \alpha_1 - \alpha_2 - ... - \alpha_{a-1} \\
\beta_b &= -\beta_1 - \beta_2 - ... - \beta_{b-1} \\
(\alpha \beta)_{ib} &= -(\alpha \beta)_{i1} -(\alpha \beta)_{i2} -...-(\alpha \beta)_{i,b-1} ; i = 1,..,a \\
(\alpha \beta)_{aj}& = -(\alpha \beta)_{1j}-(\alpha \beta)_{2j} - ... -(\alpha \beta)_{a-1,j}; j = 1,..,b
\end{aligned}
\]

We can fit the model by least squares or maximum likelihood

\textbf{Cell Means Model}\\
minimize\\

\[
Q = \sum_i \sum_j \sum_k (Y_{ijk}-\mu_{ij})^2
\]

estimators

\[
\begin{aligned}
\hat{\mu}_{ij} &= \bar{Y}_{ij} \\
\hat{Y}_{ijk} &= \bar{Y}_{ij} \\
e_{ijk} = Y_{ijk} - \hat{Y}_{ijk} &= Y_{ijk} - \bar{Y}_{ij}
\end{aligned}
\]

\textbf{Factor Effects Model}

\[
Q = \sum_i \sum_j \sum_k (Y_{ijk} - \mu_{..}-\alpha_i = \beta_j - (\alpha \beta)_{ij})^2
\]

subject to the restrictions

\[
\begin{aligned}
\sum_i \alpha_i &= 0 \\
\sum_j \beta_j &= 0 \\
\sum_i (\alpha \beta)_{ij} &= 0 \\
\sum_j (\alpha \beta)_{ij} &= 0
\end{aligned}
\]

estimators

\[
\begin{aligned}
\hat{\mu}_{..} &= \bar{Y}_{...} \\
\hat{\alpha}_i &= \bar{Y}_{i..} - \bar{Y}_{...} \\
\hat{\beta}_j &= \bar{Y}_{.j.}-\bar{Y}_{...} \\
(\hat{\alpha \beta})_{ij} &= \bar{Y}_{ij.} - \bar{Y}_{i..} - \bar{Y}_{.j.}+ \bar{Y}_{...}
\end{aligned}
\]

The fitted values

\[
\hat{Y}_{ijk} = \bar{Y}_{...}+ (\bar{Y}_{i..}- \bar{Y}_{...})+ (\bar{Y}_{.j.}- \bar{Y}_{...}) + (\bar{Y}_{ij.} - \bar{Y}_{i..}-\bar{Y}_{.j.}+\bar{Y}_{...}) = \bar{Y}_{ij.}
\]

where

\[
\begin{aligned}
e_{ijk} &= Y_{ijk} - \bar{Y}_{ij.} \\
e_{ijk} &\sim \text{ indep } (0,\sigma^2)
\end{aligned}
\]

and

\[
\begin{aligned}
s^2_{\hat{\mu}..} &= \frac{MSE}{nab} \\
s^2_{\hat{\alpha}_i} &= MSE(\frac{1}{nb} - \frac{1}{nab}) \\
s^2_{\hat{\beta}_j} &= MSE(\frac{1}{na} - \frac{1}{nab}) \\
s^2_{(\hat{\alpha\beta})_{ij}} &= MSE (\frac{1}{n} - \frac{1}{na}- \frac{1}{nb} + \frac{1}{nab})
\end{aligned}
\]

\hypertarget{partitioning-the-total-sum-of-squares}{%
\subparagraph{Partitioning the Total Sum of Squares}\label{partitioning-the-total-sum-of-squares}}

\[
Y_{ijk} - \bar{Y}_{...} = \bar{Y}_{ij.} - \bar{Y}_{...} + Y_{ijk} - \bar{Y}_{ij.}
\]

\(Y_{ijk} - \bar{Y}_{...}\): Total deviation\\
\(\bar{Y}_{ij.} - \bar{Y}_{...}\): Deviation of treatment mean from overall mean\\
\(Y_{ijk} - \bar{Y}_{ij.}\): Deviation of observation around treatment mean (residual).

\[
\begin{aligned}
\sum_i \sum_j \sum_k (Y_{ijk} - \bar{Y}_{...})^2 &= n \sum_i \sum_j (\bar{Y}_{ij.}- \bar{Y}_{...})^2+ \sum_i \sum_j sum_k (Y_{ijk} - \bar{ij.})^2 \\
SSTO &= SSTR + SSE
\end{aligned}
\]

(cross product terms are 0)

\[
\bar{Y}_{ij.}- \bar{Y}_{...} = \bar{Y}_{i..}-\bar{Y}_{...} + \bar{Y}_{.j.}-\bar{Y}_{...} + \bar{Y}_{ij.} - \bar{Y}_{i..} - \bar{Y}_{.j.} + \bar{Y}_{...}
\]

squaring and summing:\\

\[
\begin{aligned}
n\sum_i \sum_j (\bar{Y}_{ij.}-\bar{Y}_{...})^2 &= nb\sum_i (\bar{Y}_{i..}-\bar{Y}_{...})^2 + na \sum_j (\bar{Y}_{.j.}-\bar{Y}_{...})^2 \\
&+ n \sum_i \sum_j (\bar{Y}_{ij.}-\bar{Y}_{i..}- \bar{Y}_{.j.}+ \bar{Y}_{...})^2 \\
SSTR &= SSA + SSB + SSAB
\end{aligned}
\]

The interaction term from

\[
\begin{aligned}
SSAB &= SSTO - SSE - SSA - SSB \\
SSAB &= SSTR - SSA - SSB 
\end{aligned}
\]

where

\begin{itemize}
\tightlist
\item
  \(SSA\) is the factor \(A\) sum of squares (measures the variability of the estimated factor \(A\) level means \(\bar{Y}_{i..}\))- the more variable, the larger \(SSA\)
\item
  \(SSB\) is the factor \(B\) sum of squares
\item
  \(SSAB\) is the interaction sum of squares, measuring the variability of the estimated interactions.
\end{itemize}

\hypertarget{partitioning-the-df}{%
\subparagraph{Partitioning the df}\label{partitioning-the-df}}

\(N = abn\) cases and \(ab\) treatments.

For one-way ANOVA and regression, the partition has df:

\[
SS: SSTO = SSTR + SSE
\]

\[
df: N-1 = (ab-1) + (N-ab) 
\]

we must further partition the \(ab-1\) df with SSTR

\[
SSTR = SSA + SSB + SSAB
\]

\[
ab-1 = (a-1) + (b-1) + (a-1)(b-1) 
\]

\begin{itemize}
\tightlist
\item
  \(df_{SSA} = a-1\): a treatment deviations but 1 df is lost due to the restriction \(\sum (\bar{Y}_{i..}- \bar{Y}_{...})=0\)\\
\item
  \(df_{SSB} = b-1\): b treatment deviations but 1 df is lost due to the restriction \(\sum (\bar{Y}_{.j.}- \bar{Y}_{...})=0\)\\
\item
  \(df_{SSAB} = (a-1)(b-1)= (ab-1)-(a-1)-(b-1)\): ab interactions, there are (a+b-1) restrictions, so df = ab-a-(b-1)= (a-1)(b-1)
\end{itemize}

\hypertarget{mean-squares}{%
\subparagraph{Mean Squares}\label{mean-squares}}

\[
\begin{aligned}
MSA &= \frac{SSA}{a-1}\\
MSB &= \frac{SSB}{b-1}\\
MSAB &= \frac{SSAB}{(a-1)(b-1)}
\end{aligned}
\]

The expected mean squares are

\[
\begin{aligned}
E(MSE) &= \sigma^2 \\
E(MSA) &= \sigma^2 + nb \frac{\sum \alpha_i^2}{a-1} = \sigma^2 + nb \frac{\sum(\sum_{i.}-\mu_{..})^2}{a-1}  \\
E(MSB) &= \sigma^2 + na \frac{\sum \beta_i^2}{b-1} = \sigma^2 + na \frac{\sum(\sum_{.j}-\mu_{..})^2}{b-1} \\
E(MSAB) &= \sigma^2 + n \frac{\sum \sum (\alpha \beta)_{ij}^2}{(a-1)(b-1)} = \sigma^2 + n \frac{\sum (\mu_{ij}- \mu_{i.}- \mu_{.j}+ \mu_{..} )^2}{(a-1)(b-1)}
\end{aligned}
\]

If there are no factor A main effects (all \(\mu_{i.} = 0\) or \(\alpha_i = 0\)) the MSA and MSE have the same expectation; otherwise MSA \textgreater{} MSE. Same for factor B, and interaction effects. which case we can examine F-statistics.

\textbf{Interaction}

\[
\begin{aligned}
H_0: \mu_{ij}- \mu_{i.} - \mu_{.j} + \mu_{..} = 0 && \text{for all i,j} \\
H_a: \mu_{ij}- \mu_{i.} - \mu_{.j} + \mu_{..} \neq 0 && \text{for some i,j}
\end{aligned}
\]

or

\[
\begin{aligned}
&H_0: \text{All}(\alpha \beta)_{ij} = 0 \\
&H_a: \text{Not all} (\alpha \beta) = 0
\end{aligned}
\]

Let \(F = \frac{MSAB}{MSE}\). When \(H_0\) is true \(F \sim f_{((a-1)(b-1),ab(n-1))}\). So reject \(H_0\) when \(F > f_{((a-1)(b-1),ab(n-1))}\)

Factor A main effects:\\

\[
\begin{aligned}
&H_0: \mu_{1.} = \mu_{2.} = ... = \mu_{a.} \\
&H_a: \text{Not all $\mu_{i.}$ are equal}
\end{aligned}
\]

or

\[
\begin{aligned}
&H_0: \alpha_1 = ... = \alpha_a = 0 \\
&H_a: \text{Not all $\alpha_i$ are equal to 0}
\end{aligned}
\]

\(F= \frac{MSA}{MSE}\) and reject \(H_0\) if \(F>f_{(1-\alpha;a-1,ab(n-1))}\)

\hypertarget{two-way-anova}{%
\subparagraph{Two-way ANOVA}\label{two-way-anova}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1477}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1705}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1477}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Source of Variation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SS
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
df
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MS
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
F
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Factor A & \(SSA\) & \(a-1\) & \(MSA = SSA/(a-1)\) & \(MSA/MSE\) \\
Factor B & \(SSB\) & \(b-1\) & \(MSB = SSB/(b-1)\) & \(MSB/MSE\) \\
AB interactions & \(SSAB\) & \((a-1)(b-1)\) & \(MSAB = SSAB /MSE\) & \\
Error & \(SSE\) & \(ab(n-1)\) & \(MSE = SSE/ab(n-1)\) & \\
Total (corrected) & \(SSTO\) & \(abn - 1\) & & \\
\end{longtable}

Doing 2-way ANOVA means you always check interaction first, because if there are significant interactions, checking the significance of the main effects becomes moot.

The main effects concern the mean responses for levels of one factor averaged over the levels of the other factor. When interaction is present, we can't conclude that a given factor has no effect, even if these averages are the same. It means that the effect of the factor depends on the level of the other factor.

On the other hand, if you can establish that there is no interaction, then you can consider inference on the factor main effects, which are then said to be \textbf{additive}.\\
And we can also compare factor means like the \protect\hyperlink{single-factor-fixed-effects-model}{Single Factor Fixed Effects Model} using \protect\hyperlink{tukey}{Tukey}, \protect\hyperlink{scheffe}{Scheffe}, \protect\hyperlink{bonferroni}{Bonferroni}.

We can also consider contrasts in the 2-way model

\[
L = \sum c_i \mu_i
\]

where \(\sum c_i =0\)\\
which is estimated by

\[
\hat{L} = \sum c_i \bar{Y}_{i..}
\]

with variance

\[
\sigma^2(\hat{L}) = \frac{\sigma^2}{bn} \sum c_i^2
\]

and variance estimate

\[
\frac{MSE}{bn} \sum c_i^2
\]

\textbf{Orthogonal Contrasts}

\[
\begin{aligned}
L_1 &= \sum c_i \mu_i, \sum c_i = 0 \\
L_2 &= \sum d_i \mu_i , \sum d_i = 0
\end{aligned}
\]

these contrasts are said to be \textbf{orthogonal} if

\[
\sum \frac{c_i d_i}{n_i} = 0
\]

in balanced case \(\sum c_i d_i =0\)

\[
\begin{aligned}
cov(\hat{L}_1, \hat{L}_2) &= cov(\sum_i c_i \bar{Y}_{i..}, \sum_l d_l \bar{Y}_{l..}) \\
&= \sum_i \sum_l c_i d_l cov(\bar{Y}_{i..},\bar{Y}_{l..}) \\
&= \sum_i c_i d_i \frac{\sigma^2}{bn} = 0
\end{aligned}
\]

Orthogonal contrasts can be used to further partition the model sum of squares. There are many sets of orthogonal contrasts and thus, many ways to partition the sum of squares.

A special set of orthogonal contrasts that are used when the levels of a factor can be assigned values on a metric scale are called \textbf{orthogonal polynomials}

Coefficients can be found for the special case of

\begin{itemize}
\tightlist
\item
  equal spaced levels (e.g., (0 15 30 45 60))\\
\item
  equal sample sizes (\(n_1 = n_2 = ... = n_{ab}\))
\end{itemize}

We can define the SS for a given contrast:

\[
SS_L = \frac{\hat{L}^2}{\sum_{i=1}^a (c^2_i/bn_i)}
\]

\[
T = \frac{\hat{L}}{\sqrt{MSE\sum_{i=1}^a(c_i^2/bn_i)}} \sim t
\]

Moreover,

\[
t^2_{(1-\alpha/2;df)}=F_{(1-\alpha;1,df)}
\]

So,

\[
\frac{SS_L}{MSE} \sim F_{(1-\alpha;1,df_{MSE})}
\]

all contrasts have d.f = 1

\hypertarget{unbalanced}{%
\subsubsection{Unbalanced}\label{unbalanced}}

We could have unequal numbers of replications for all treatment combinations:

\begin{itemize}
\tightlist
\item
  Observational studies
\item
  Dropouts in designed studies
\item
  Larger sample sizes for inexpensive treatments
\item
  Sample sizes to match population makeup.
\end{itemize}

Assume that each factor combination has at least 1 observation (no empty cells)

Consider the same model as:

\[
Y_{ijk} = \mu_{..} + \alpha_i + \beta_j + (\alpha \beta)_{ij} + \epsilon_{ijk}
\]

where sample sizes are: \(n_{ij}\):

\[
\begin{aligned}
n_{i.} &= \sum_j n_{ij} \\
n_{.j} &= \sum_i n_{ij} \\
n_T &= \sum_i \sum_j n_{ij}
\end{aligned}
\]

Problem here is that

\[
SSTO \neq SSA + SSB + SSAB + SSE
\]

(the design is \textbf{non-orthogonal})

\begin{itemize}
\tightlist
\item
  For \(i = 1,...,a-1,\)
\end{itemize}

\[
u_i = \begin{cases} +1 & \text{if the obs is from the i-th level of Factor 1} \\ -1 & \text{if the obs is from the a-th level of Factor 1} \\ 0 & \text{otherwise} \\ \end{cases}
\]

\begin{itemize}
\tightlist
\item
  For \(j=1,...,b-1\)
\end{itemize}

\[
v_i = 
\begin{cases} +1 & \text{if the obs is from the j-th level of Factor 1} \\ -1 & \text{if the obs is from the b-th level of Factor 1} \\ 0 & \text{otherwise} \\ 
\end{cases}
\]

We can use these indicator variables as predictor variables and \(\mu_{..}, \alpha_i ,\beta_j, (\alpha \beta)_{ij}\) as unknown parameters.

\[
Y = \mu_{..} + \sum_{i=1}^{a-1} \alpha_i u_i + \sum_{j=1}^{b-1} \beta_j v_j + \sum_{i=1}^{a-1} \sum_{j=1}^{b-1}(\alpha \beta)_{ij} u_i v_j + \epsilon
\]

To test hypotheses, we use the extra sum of squares idea.

For interaction effects

\[
\begin{aligned}
&H_0: all (\alpha \beta)_{ij} = 0 \\
&H_a: \text{not all }(\alpha \beta)_{ij} =0
\end{aligned}
\]

Or to test

\[
\begin{aligned}
&H_0: \beta_1 = \beta_2 = \beta_3 = 0 \\
&H_a: \text{not all } \beta_j = 0
\end{aligned}
\]

\textbf{Analysis of Factor Means}

(e.g., contrasts) is analogous to the balanced case, with modifications in the formulas for means and standard errors to account for unequal sample sizes.

Or , we can fit the cell means model and consider it from a regression perspective

If you have empty cells (i.e., some factor combinations have no observation), then the equivalent regression approach can't be used. But you can still do partial analyses

\hypertarget{two-way-random-effects-anova}{%
\subsection{Two-Way Random Effects ANOVA}\label{two-way-random-effects-anova}}

\[
Y_{ijk} = \mu_{..} + \alpha_i + \beta_j + (\alpha \beta)_{ij} + \epsilon_{ij}
\]

where

\begin{itemize}
\tightlist
\item
  \(\mu_{..}\): constant
\item
  \(\alpha_i \sim N(0,\sigma^2_{\alpha}), i = 1,..,a\) (independent)
\item
  \(\beta_j \sim N(0,\sigma^2_{\beta}), j = 1,..,b\) (independent)
\item
  \((\alpha \beta)_{ij} \sim N(0,\sigma^2_{\alpha \beta}),i=1,...,a,j=1,..,b\) (independent)
\item
  \(\epsilon_{ijk} \sim N(0,\sigma^2)\) (independent)
\end{itemize}

All \(\alpha_i, \beta_j, (\alpha \beta)_{ij}\) are pairwise independent

Theoretical means, variances, and covariances are

\[
\begin{aligned}
E(Y_{ijk}) &= \mu_{..} \\
var(Y_{ijk}) &= \sigma^2_Y= \sigma^2_\alpha + \sigma^2_\beta +  \sigma^2_{\alpha \beta} + \sigma^2 
\end{aligned}
\]

So

\(Y_{ijk} \sim N(\mu_{..},\sigma^2_\alpha + \sigma^2_\beta + \sigma^2_{\alpha \beta} + \sigma^2)\)

\[
\begin{aligned}
cov(Y_{ijk},Y_{ij'k'}) &= \sigma^2_{\alpha}, j \neq j' \\
cov(Y_{ijk},Y_{i'jk'}) &= \sigma^2_{\beta}, i \neq i'\\
cov(Y_{ijk},Y_{ijk'}) &= \sigma^2_\alpha + \sigma^2_{\beta} + \sigma^2_{\alpha \beta}, k \neq k' \\
cov(Y_{ijk},Y_{i'j'k'}) &= , i \neq i', j \neq j'
\end{aligned}
\]

\hypertarget{two-way-mixed-effects-anova}{%
\subsection{Two-Way Mixed Effects ANOVA}\label{two-way-mixed-effects-anova}}

\hypertarget{balanced-1}{%
\subsubsection{Balanced}\label{balanced-1}}

One fixed factor, while other is random treatment levels, we have a \textbf{mixed effects model} or a \textbf{mixed model}

\textbf{Restricted mixed model} for 2-way ANOVA:

\[
Y_{ijk} = \mu_{..} + \alpha_i + \beta_j + (\alpha \beta)_{ij} + \epsilon_{ijk}
\]

where

\begin{itemize}
\tightlist
\item
  \(\mu_{..}\): constant
\item
  \(\alpha_i\): fixed effects with constraints subject to restriction \(\sum \alpha_i = 0\)
\item
  \(\beta_j \sim indep N(0,\sigma^2_\beta)\)
\item
  \((\alpha \beta)_{ij} \sim N(0,\frac{a-1}{a}\sigma^2_{\alpha \beta})\) subject to restriction \(\sum_i (\alpha \beta)_{ij} = 0\) for all j, the variance here is written as the proportion for convenience; it makes the expected mean squares simpler (other assumed \(var((\alpha \beta)_{ij}= \sigma^2_{\alpha \beta}\))
\item
  \(cov((\alpha \beta)_{ij},(\alpha \beta)_{i'j'}) = - \frac{1}{a} \sigma^2_{\alpha \beta}, i \neq i'\)
\item
  \(\epsilon_{ijk}\sim indepN(0,\sigma^2)\)
\item
  \(\beta_j, (\alpha \beta)_{ij}, \epsilon_{ijk}\) are pairwise independent
\end{itemize}

Two-way mixed models are written in an ``unrestricted'' form, with no restrictions on the interaction effects \((\alpha \beta)_{ij}\), they are pairwise independent.

Let \(\beta^*, (\alpha \beta)^*_{ij}\) be the unrestricted random effects, and \((\bar{\alpha \beta})_{ij}^*\) the means averaged over the fixed factor for each level of random factor B.

\[
\begin{aligned}
\beta_j &= \beta_j^* + (\bar{\alpha \beta})_{ij}^* \\
(\alpha \beta)_{ij} &= (\alpha \beta)_{ij}^* - (\bar{\alpha \beta})_{ij}^*
\end{aligned}
\]

Some consider the restricted model to be more general. but here we consider the restricted form.

\[
\begin{aligned}
E(Y_{ijk}) &= \mu_{..} + \alpha_i \\
var(Y_{ijk}) &= \sigma^2_\beta + \frac{a-1}{a} \sigma^2_{\alpha \beta} + \sigma^2
\end{aligned}
\]

Responses from the same random factor \((B)\) level are correlated

\[
\begin{aligned}
cov(Y_{ijk},Y_{ijk'}) &= E(Y_{ijk}Y_{ijk'}) - E(Y_{ijk})E(Y_{ijk'}) \\
&= \sigma^2_\beta + \frac{a-1}{a} \sigma^2_{\alpha \beta} , k \neq k'
\end{aligned}
\]

Similarly,

\[
\begin{aligned}
cov(Y_{ijk},Y_{i'jk'}) &= \sigma^2_\beta - \frac{1}{a} \sigma^2_{\alpha\ \beta}, i \neq i' \\
cov(Y_{ijk},Y_{i'j'k'}) &= 0,  j \neq j'
\end{aligned}
\]

Hence, you can see that the only way you don't have dependence in the \(Y\) is when they don't share the same random effect.

An advantage of the \textbf{restricted mixed model} is that 2 observations from the same random factor b level can be positively or negatively correlated. In the \textbf{unrestricted model}, they can only be positively correlated.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.0843}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.0904}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.4337}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3795}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Mean Square
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Fixed ANOVA

(A, B Fixed)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Random ANOVA

(A,B random)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Mixed ANVOA

(A fixed, B random)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
MSA & a - 1 & \(\sigma ^2+ n b \frac{\sum\alpha_i^2}{a-1}\) & \(\sigma^2 + nb\sigma^ 2_ \alpha +n \sigma^ 2_{\alpha \beta}\) \\
MSB & b-1 & \(\sigma^2 + n a \frac{\sum\beta ^2_j}{b-1}\) & \(\sigma^ 2 + na\sigma^2_ \beta +n \sigma^ 2_{\alpha \beta}\) \\
MSAB & ( a-1)(b-1) & \(\sigma^2 + n \frac{\sum \sum(\alpha \beta )^2_ {ij}} { ( a-1)(b-1)}\) & \(\sigma^2+n \sigma^2_{\alpha \beta}\) \\
MSE & (n-1)ab & \(\sigma^2\) & \(\sigma^2\) \\
\end{longtable}

For fixed, random, and mixed models (balanced), the ANOVA table sums of squares calculations are identical. (also true for df and mean squares). The only difference is with the expected mean squares, thus the test statistics.

In Random ANOVA, we test

\[
\begin{aligned}
&H_0: \sigma^2 = 0 \\
&H_a: \sigma^2 > 0 
\end{aligned}
\]

by considering \(F= \frac{MSA}{MSAB} \sim F_{a-1;(a-1)(b-1)}\)

The same test statistic is used for mixed models, but in that case we are testing null hypothesis that all of the \(\alpha_i = 0\)

The test statistic different for the same null hypothesis under the fixed effects model.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2386}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2386}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Test for effects of
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Fixed ANOVA

(A\&B fixed)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Random ANOVA

(A\&B random)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Mixed ANOVA

(A fixed, B random)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Factor A & \(\frac{MSA}{MSE}\) & \(\frac{MSA}{MSAB}\) & \(\frac{MSA}{MSAB}\) \\
Factor B & \(\frac{MSB}{MSE}\) & \(\frac{MSB}{MSAB}\) & \(\frac{MSB}{MSE}\) \\
AB interactions & \(\frac{MSAB}{MSE}\) & \(\frac{MSAB}{MSE}\) & \(\frac{MSAB}{MSE}\) \\
\end{longtable}

\textbf{Estimation Of Variance Components}

In random and mixed effects models, we are interested in estimating the \textbf{variance components}\\
Variance component \(\sigma^2_\beta\) in the mixed ANOVA.

\[
E(\sigma^2_\beta) = \frac{E(MSB)-E(MSE)}{na} = \frac{\sigma^2 + na \sigma^2_\beta - \sigma^2}{na} = \sigma^2_\beta
\]

which can be estimated with

\[
\hat{\sigma}^2_\beta = \frac{MSB - MSE}{na}
\]

Confidence intervals for variance components can be constructed (approximately) by using the \textbf{Satterthwaite} procedure or the MLS procedure (like the 1-way random effects)

\textbf{Estimation of Fixed Effects in Mixed Models}

\[
\begin{aligned}
\hat{\alpha}_i &= \bar{Y}_{i..} - \bar{Y}_{...} \\
\hat{\mu}_{i.} &= \bar{Y}_{...} + (\bar{Y}_{i..}- \bar{Y}_{...}) = \bar{Y}_{i..}  \\
\sigma^2(\hat{\alpha}_i) &= \frac{\sigma^2 + n \sigma^2_{\alpha \beta}}{bn} = \frac{E(MSAB)}{bn} \\
s^2(\hat{\alpha}_i) &= \frac{MSAB}{bn}
\end{aligned}
\]

Contrasts on the \textbf{Fixed Effects}

\[
\begin{aligned}
L &= \sum c_i \alpha_i \\
\sum c_i &= 0 \\
\hat{L} &= \sum c_i \hat{\alpha}_i \\
\sigma^2(\hat{L}) &= \sum c^2_i \sigma^2 (\hat{\alpha}_i) \\
s^2(\hat{L}) &= \frac{MSAB}{bn} \sum c^2_i
\end{aligned}
\]

Confidence intervals and tests can be constructed as usual

\hypertarget{unbalanced-1}{%
\subsubsection{Unbalanced}\label{unbalanced-1}}

For a mixed model with a = 2, b = 4

\[
\begin{aligned}
Y_{ijk} &= \mu_{..} + \alpha_i + \beta_j + (\alpha \beta)_{ij} + \epsilon_{ijk} \\
var(\beta_j)&= \sigma^2_\beta \\
var((\alpha \beta)_{ij})&= \frac{2-1}{2}\sigma^2_{\alpha \beta} = \frac{\sigma^2_{\alpha \beta}}{2} \\
var(\epsilon_{ijk}) &= \sigma^2 \\
E(Y_{ijk}) &= \mu_{..} + \alpha_i \\
var(Y_{ijk}) &= \sigma^2_{\beta} + \frac{\sigma^2_{\alpha \beta}}{2} + \sigma^2 \\
cov(Y_{ijk},Y_{ijk'}) &= \sigma^2 + \frac{\sigma^2_{\alpha \beta}}{2}, k \neq k' \\
cov(Y_{ijk},Y_{i'jk'}) &= \sigma^2_{\beta} - \frac{\sigma^2_{\alpha \beta}}{2}, i \neq i' \\
cov(Y_{ijk},Y_{i'j'k'}) &= 0, j \neq j' 
\end{aligned}
\]

assume

\[
\mathbf{Y} \sim N(\mathbf{X}\beta, M)
\]

where \(M\) is block diagonal

density function

\[
f(\mathbf{Y}) = \frac{1}{(2\pi)^{N/2}|M|^{1/2}}exp(-\frac{1}{2}\mathbf{(Y - X \beta)' M^{-1}(Y-X\beta)})
\]

if we knew the variance components, we could use GLS:

\[
\hat{\beta}_{GLS} = \mathbf{(X'M^{-1}X)^{-1}X'M^{-1}Y}
\]

but we usually don't know the variance components \(\sigma^2, \sigma^2_\beta, \sigma^2_{\alpha \beta}\) that make up \(M\)\\
Another way to get estimates is by \textbf{Maximum likelihood estimation}

we try to maximize its log

\[
\ln L = - \frac{N}{2} \ln (2\pi) - \frac{1}{2}\ln|M| - \frac{1}{2} \mathbf{(Y-X \beta)'\Sigma^{-1}(Y-X\beta)}
\]

\hypertarget{nonparametric-anova}{%
\section{Nonparametric ANOVA}\label{nonparametric-anova}}

\hypertarget{kruskal-wallis}{%
\subsection{Kruskal-Wallis}\label{kruskal-wallis}}

Generalization of independent samples Wilcoxon Rank sum test for 2 independent samples (like F-test of one-way ANOVA is a generalization to several independent samples of the two sample t-test)

Consider the one-way case:

We have

\begin{itemize}
\tightlist
\item
  \(a\ge2\) treatments
\item
  \(n_i\) is the sample size for the \(i\)-th treatment
\item
  \(Y_{ij}\) is the \(j\)-th observation from the \(i\)-th treatment.
\item
  we make \textbf{no} assumption of normality
\item
  We only assume that observations on the \(i\)-th treatment are a random sample from the continuous CDF \(F_i\), i = 1,..,n, and are mutually independent.
\end{itemize}

\[
\begin{aligned}
&H_0: F_1 = F_2 = ... = F_a \\
&H_a: F_i < F_j \text{ for some } i \neq j
\end{aligned}
\]

or if distribution is from the location-scale family, \(H_0: \theta_1 = \theta_2 = ... = \theta_a\))

\textbf{Procedure}

\begin{itemize}
\tightlist
\item
  Rank all \(N = \sum_{i=1}^a n_i\) observations in ascending order. Let \(r_{ij} = rank(Y_{ij})\), note \(\sum_i \sum_j r_{ij} = 1 + 2 .. + N = \frac{N(N+1)}{2}\)\\
\item
  Calculate the rank sums and averages:\\
  \[
  r_{i.} = \sum_{j=1}^{n_i} r_{ij}
  \] and \[
  \bar{r}_{i.} = \frac{r_{i.}}{n_i}, i = 1,..,a
  \]
\item
  Calculate the test statistic on the ranks: \[
  \chi_{KW}^2 = \frac{SSTR}{\frac{SSTO}{N-1}}
  \] where \(SSTR = \sum n_i (\bar{r}_{i.}- \bar{r}_{..})^2\) and \(SSTO = \sum \sum (\bar{r}_{ij}- \bar{r}_{..})^2\)
\item
  For large \(n_i\) (\(\ge 5\) observations) the Kruskal-Wallis statistic is approximated by a \(\chi^2_{a-1}\) distribution when all the treatment means are equal. Hence, reject \(H_0\) if \(\chi^2_{KW} > \chi^2_{(1-\alpha;a-1)}\).\\
\item
  If sample sizes are small, one can exhaustively work out all possible distinct ways of assigning N ranks to the observations from a treatments and calculate the value of the KW statistic in each case (\(\frac{N!}{n_1!..n_a!}\) possible combinations). Under \(H_0\) all of these assignments are equally likely.
\end{itemize}

\hypertarget{friedman-test}{%
\subsection{Friedman Test}\label{friedman-test}}

When the responses \(Y_{ij} = 1,..,n, j = 1,..,r\) in a randomized complete block design are not normally distributed (or do not have constant variance), a nonparametric test is more helpful.

A distribution-free rank-based test for comparing the treatments in this setting is the Friedman test. Let \(F_{ij}\) be the CDF of random \(Y_{ij}\), corresponding to the observed value \(y_{ij}\)

Under the null hypothesis, \(F_{ij}\) are identical for all treatments j separately for each block i.

\[
\begin{aligned}
&H_0: F_{i1} = F_{i2} = ... = F_{ir}  \text{ for all i} \\
&H_a: F_{ij} < F_{ij'} \text{ for some } j \neq j' \text{ for all } i
\end{aligned}
\]

For location parameter distributions, treatment effects can be tested:

\[
\begin{aligned}
&H_0: \tau_1 = \tau_2 = ... = \tau_r \\
&H_a: \tau_j > \tau_{j'} \text{ for some } j \neq j'
\end{aligned}
\]

\textbf{Procedure}

\begin{itemize}
\tightlist
\item
  Rank observations from the r treatments separately within each block (in ascending order; if ties, each tied observation is given the mean of ranks involved). Let the ranks be called \(r_{ij}\)\\
\item
  Calculate the Friedman test statistic\\
  \[
  \chi^2_F = \frac{SSTR}{\frac{SSTR + SSE}{n(r-1)}}
  \] where \[
  \begin{aligned}
  SSTR &= n \sum (\bar{r}_{.j}-\bar{r}_{..})^2 \\
  SSE &= \sum \sum (r_{ij} - \bar{r}_{.j})^2 \\
  \bar{r}_{.j} &= \frac{\sum_i r_{ij}}{n}\\
  \bar{r}_{..} &= \frac{r+1}{2}
  \end{aligned}
  \]
\end{itemize}

If there is no ties, it can be rewritten as

\[
\chi^2_{F} = [\frac{12}{nr(n+1)}\sum_j r_{.j}^2] - 3n(r+1)
\]

with large number of blocks, \(\chi^2_F\) is approximately \(\chi^2_{r-1}\) under \(H_0\). Hence, we reject \(H_0\) if \(\chi^2_F > \chi^2_{(1-\alpha;r-1)}\)\\
The exact null distribution for \(\chi^2_F\) can be derived since there are r! possible ways of assigning ranks 1,2,\ldots,r to the r observations within each block. There are n blocks and thus \((r!)^n\) possible assignments to the ranks, which are equally likely when \(H_0\) is true.

\hypertarget{sample-size-planning-for-anova}{%
\section{Sample Size Planning for ANOVA}\label{sample-size-planning-for-anova}}

\hypertarget{balanced-designs}{%
\subsection{Balanced Designs}\label{balanced-designs}}

\hypertarget{single-factor-studies}{%
\subsubsection{Single Factor Studies}\label{single-factor-studies}}

\hypertarget{fixed-cell-means}{%
\paragraph{Fixed cell means}\label{fixed-cell-means}}

\[
P(F>f_{(1-\alpha;a-1,N-a)}|\phi) = 1 - \beta
\]

where \(\phi\) is the non-centrality \textbf{parameter} (measures how unequal the treatment means \(\mu_i\) are)

\[
\phi = \frac{1}{\sigma}\sqrt{\frac{n}{a}\sum_i (\mu_i - \mu_.)^2} , (n_i \equiv n)
\]

and

\[
\mu_. = \frac{\sum \mu_i}{a}
\]

To decide on the power probabilities we use the non-central F distribution.

We could use the power table directly when effects are fixed and design is balanced by using \textbf{minimum range} of factor level means for your desired differences

\[
\Delta = \max(\mu_i) - \min(\mu_i)
\]

Hence, we need

\begin{itemize}
\tightlist
\item
  \(\alpha\) level
\item
  \(\Delta\)
\item
  \(\sigma\)
\item
  \(\beta\)
\end{itemize}

Notes:

\begin{itemize}
\tightlist
\item
  When \(\Delta/\sigma\) is small greatly affects sample size, but if \(\Delta/\sigma\) is large.
\item
  Reducing \(\alpha\) or \(\beta\) increases the required sample sizes.
\item
  Error in estimating \(\sigma\) can make a large difference.
\end{itemize}

\hypertarget{multi-factor-studies}{%
\subsubsection{Multi-factor Studies}\label{multi-factor-studies}}

The same noncentral \(F\) tables can be used here

For two-factor fixed effect model

Test for interactions:

\[
\begin{aligned}
\phi &= \frac{1}{\sigma} \sqrt{\frac{n \sum \sum (\alpha \beta_{ij})^2}{(a-1)(b-1)+1}} = \frac{1}{\sigma} \sqrt{\frac{n \sum \sum (\mu_{ij}- \mu_{i.} - \mu_{.j} + \mu_{..})^2}{(a-1)(b-1)+1}} \\
\upsilon_1 &= (a-1)(b-1) \\
\upsilon_2 &= ab(n-1)
\end{aligned}
\]

Test for Factor \(A\) main effects:

\[
\begin{aligned}
\phi &= \frac{1}{\sigma} \sqrt{\frac{nb \sum \alpha_i^2}{a}} = \frac{1}{\sigma}\sqrt{\frac{nb \sum (\mu_{i.}- \mu_{..})^2}{a}} \\
\upsilon_1 &= a-1 \\
\upsilon_2 &= ab(n-1)
\end{aligned}
\]

Test for Factor \(B\) main effects:

\[
\begin{aligned}
\phi &= \frac{1}{\sigma} \sqrt{\frac{na \sum \beta_j^2}{b}} = \frac{1}{\sigma}\sqrt{\frac{na \sum (\mu_{.j}- \mu_{..})^2}{b}} \\
\upsilon_1 &= b-1 \\
\upsilon_2 &= ab(n-1)
\end{aligned}
\]

Procedure:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Specify the minimum range of Factor \(A\) means
\item
  Obtain sample sizes with \(r = a\). The resulting sample size is \(bn\), from which \(n\) can be obtained.
\item
  Repeat the first 2 steps for Factor \(B\) minimum range.
\item
  Choose the greater number of sample size between \(A\) and \(B\).
\end{enumerate}

\hypertarget{randomized-block-experiments}{%
\subsection{Randomized Block Experiments}\label{randomized-block-experiments}}

Analogous to completely randomized designs . The power of the F-test for treatment effects for randomized block design uses the same non-centrality parameter as completely randomized design:

\[
\phi = \frac{1}{\sigma} \sqrt{\frac{n}{r} \sum (\mu_i - \mu_.)^2}
\]

However, the power level is different from the randomized block design because

\begin{itemize}
\tightlist
\item
  error variance \(\sigma^2\) is different
\item
  df(MSE) is different.
\end{itemize}

\hypertarget{randomized-block-designs}{%
\section{Randomized Block Designs}\label{randomized-block-designs}}

To improve the precision of treatment comparisons, we can reduce variability among the experimental units. We can group experimental units into \textbf{blocks} so that each block contains relatively homogeneous units.

\begin{itemize}
\tightlist
\item
  Within each block, random assignment treatments to units (separate random assignment for each block)
\item
  The number of units per block is a multiple of the number of factor combinations.
\item
  Commonly, use each treatment once in each block.
\end{itemize}

Benefits of \textbf{Blocking}

\begin{itemize}
\item
  Reduction in variability of estimators for treatment means

  \begin{itemize}
  \tightlist
  \item
    Improved power for t-tests and F-tests
  \item
    Narrower confidence intervals
  \item
    Smaller MSE
  \end{itemize}
\item
  Compare treatments under different conditions (related to different blocks).
\end{itemize}

Loss from \textbf{Blocking} (little to lose)

\begin{itemize}
\tightlist
\item
  If you don't do blocking well, you waste df on negligible block effects that could have been used to estimate \(\sigma^2\)
\item
  Hence, the df for \(t\)-tests and denominator df for \(F\)-tests will be reduced without reducing MSE and small loss of power for both tests.
\end{itemize}

Consider

\[
Y_{ij} = \mu_{..} + \rho_i + \tau_j + \epsilon_{ij}
\]

where

\begin{itemize}
\tightlist
\item
  \(i = 1, 2, \dots, n\)
\item
  \(j = 1, 2, \dots, r\)
\item
  \(\mu_{..}\): overall mean response, averaging across all blocks and treatments
\item
  \(\rho_i\): block effect, average difference in response for i-th block (\(\sum \rho_i =0\))
\item
  \(\tau_j\) treatment effect, average across blocks (\(\sum \tau_j = 0\))
\item
  \(\epsilon_{ij} \sim iid N(0,\sigma^2)\): random experimental error.
\end{itemize}

Here, we assume that the block and treatment effects are additive. The difference in average response for any pair of treatments i the same \textbf{within} each block

\[
(\mu_{..} +  \rho_i + \tau_j) - (\mu_{..} + \rho_i + \tau_j') = \tau_j - \tau_j'
\]

for all \(i=1,..,n\) blocks

\[
\begin{aligned}
\hat{\mu} &= \bar{Y}_{..} \\
\hat{\rho}_i &= \bar{Y}_{i.} - \bar{Y}_{..} \\
\hat{\tau}_j &= \bar{Y}_{.j} - \bar{Y}_{..}
\end{aligned}
\]

Hence,

\[
\begin{aligned}
\hat{Y}_{ij} &= \bar{Y}_{..} + (\bar{Y}_{i.} - \bar{Y}_{..}) + (\bar{Y}_{.j}- \bar{Y}_{..}) = \bar{Y}_{i.} + \bar{Y}_{.j} - \bar{Y}_{..} \\
e_{ij} &= Y_{ij} - \hat{Y}_{ij} = Y_{ij}- \bar{Y}_{i.} - \bar{Y}_{.j} + \bar{Y}_{..}
\end{aligned}
\]

\textbf{ANOVA table}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1063}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.3913}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.0725}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2077}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2077}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Source of Variation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SS
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
df
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Fixed Treatments

E(MS)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Random Treatments

E(MS)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Blocks & \(r \sum_i(\bar{Y}_{i.}-\bar{Y}_{..})^2\) & \(n - 1\) & \(\sigma^2 +r \frac{\sum \rho^2_i}{n-1}\) & \(\sigma^2 + r \frac{\sum \rho^2_i}{n-1}\) \\
Treatments & \(n\sum_ j (\bar{Y} _ {.j}-\bar{ Y}_{..})^2\) & \(r - 1\) & \(\sigma^2 + n \frac{\sum \tau^2_j}{r-1}\) & \(\sigma^2 + n \sigma^2_\tau\) \\
Error & \(\sum_i \sum _j ( Y_{ ij } - \bar { Y}_{i.} - \bar{Y}_{.j} + \bar{ Y}_{..})^2\) & \((n-1)(r-1)\) & \(\sigma^2\) & \(\sigma^2\) \\
Total & \(SSTO\) & \(nr-1\) & & \\
\end{longtable}

\textbf{F-tests}

\[
\begin{aligned}
H_0: \tau_1 = \tau_2 = ... = \tau_r = 0 && \text{Fixed Treatment Effects} \\
H_a: \text{not all } \tau_j = 0 \\
\\
H_0: \sigma^2_{\tau} = 0 && \text{Random Treatment Effects} \\
H_a: \sigma^2_{\tau} \neq 0 
\end{aligned}
\]

In both cases \(F = \frac{MSTR}{MSE}\), reject \(H_0\) if \(F > f_{(1-\alpha; r-1,(n-1)(r-1))}\)

we don't use F-test to compare blocks, because

\begin{itemize}
\tightlist
\item
  We have a priori that blocs are different\\
\item
  Randomization is done ``within'' block.
\end{itemize}

To estimate the efficiency that was gained by blocking (relative to completely randomized design).

\[
\begin{aligned}
\hat{\sigma}^2_{CR} &= \frac{(n-1)MSBL + n(r-1)MSE}{nr-1} \\
\hat{\sigma}^2_{RB} &= MSE \\
\frac{\hat{\sigma}^2_{CR}}{\hat{\sigma}^2_{RB}} &= \text{above 1} \\
\end{aligned}
\]

then a completely randomized experiment would

\[
(\frac{\hat{\sigma}^2_{CR}}{\hat{\sigma}^2_{RB}}-1)\%%
\]

more observations than the randomized block design to get the same MSE

If batches are randomly selected then they are random effects. That is , if the experiment was repeated, a new sample of i batches would be selected,d yielding new values for \(\rho_1, \rho_2,...,\rho_i\) then.

\[
\rho_1, \rho_2,...,\rho_j \sim N(0,\sigma^2_\rho)
\]

Then,

\[
Y_{ij} = \mu_{..} + \rho_i + \tau_j + \epsilon_{ij}
\]

where

\begin{itemize}
\tightlist
\item
  \(\mu_{..}\) fixed
\item
  \(\rho_i\): random iid \(N(0,\sigma^2_p)\)
\item
  \(\tau_j\) fixed (or random) \(\sum \tau_j = 0\)
\item
  \(\epsilon_{ij} \sim iid N(0,\sigma^2)\)
\end{itemize}

\textbf{Fixed Treatment}

\[
\begin{aligned}
E(Y_{ij}) &= \mu_{..} + \tau_j \\
var(Y_{ij}) &= \sigma^2_{\rho} + \sigma^2
\end{aligned}
\]

\[
\begin{aligned}
cov(Y_{ij},Y_{ij'}) &= \sigma^2 , j \neq j' \text{ treatments within same block are correlated} \\
cov(Y_{ij},Y_{i'j'}) &= 0 , i \neq i' , j \neq j'
\end{aligned}
\]

Correlation between 2 observations in the same block

\[
\frac{\sigma^2_{\rho}}{\sigma^2 + \sigma^2_{\rho}}
\]

The expected MS for the additive fixed treatment effect, random block effect is

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Source & SS & E(MS) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Blocks & SSBL & \(\sigma^2 + r \sigma^2_\rho\) \\
Treatment & SSTR & \(\sigma^2 + n \frac{\sum \tau^2_j}{r-1}\) \\
Error & SSE & \(\sigma^2\) \\
\end{longtable}

\textbf{Interactions and Blocks}\\
without replications within each block for each treatment, we can't consider interaction between block and treatment when the block effect is fixed. Hence, only in the random block effect, we have

\[
Y_{ij} = \mu_{..} + \rho_i + \tau_j + (\rho \tau)_{ij} + \epsilon_{ij}
\]

where

\begin{itemize}
\tightlist
\item
  \(\mu_{..}\) constant
\item
  \(\rho_i \sim idd N(0,\sigma^2_{\rho})\) random
\item
  \(\tau_j\) fixed (\(\sum \tau_j = 0\))
\item
  \((\rho \tau)_{ij} \sim N(0,\frac{r-1}{r}\sigma^2_{\rho \tau})\) with \(\sum_j (\rho \tau)_{ij}=0\) for all i
\item
  \(cov((\rho \tau)_{ij},(\rho \tau)_{ij'})= -\frac{1}{r} \sigma^2_{\rho \tau}\) for \(j \neq j'\)
\item
  \(\epsilon_{ij} \sim iid N(0,\sigma^2)\) random
\end{itemize}

Note: a special case of mixed 2-factor model with 1 observation per ``cell''

\[
\begin{aligned}
E(Y_{ij}) &= \mu_{..} + \tau_j \\
var(Y_{ij}) &= \sigma^2_\rho + \frac{r-1}{r} \sigma^2_{\rho \tau} + \sigma^2
\end{aligned}
\]

\[
\begin{aligned}
cov(Y_{ij},Y_{ij'}) &= \sigma^2_\rho - \frac{1}{r} \sigma^2_{\rho \tau}, j \neq j' \text{ obs from the same block are correlated} \\
cov(Y_{ij},Y_{i'j'}) &= 0, i \neq i', j \neq j' \text{ obs from different blocks are independent}
\end{aligned}
\]

The sum of squares and degrees of freedom for interaction model are the same as those for the additive model. The difference exists only with the expected mean squares

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1132}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.0943}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1415}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.6321}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Source
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SS
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
df
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
E(MS)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Blocks & \(SSBL\) & \(n-1\) & \(\sigma^2 + r \sigma^2_\rho\) \\
Treatment & \(SSTR\) & \(r -1\) & \(\sigma^2 + \sigma ^2_{\rho \tau} + n \frac{\sum \tau_j^2}{r-1}\) \\
Error & \(SSE\) & \((n-1)(r-1)\) & \(\sigma^2 + \sigma ^2_{\rho \tau}\) \\
\end{longtable}

\begin{itemize}
\tightlist
\item
  No exact test is possible for block effects when interaction is present (Not important if blocks are used primarily to reduce experimental error variability)\\
\item
  \(E(MSE) = \sigma^2 + \sigma^2_{\rho \tau}\) the error term variance and interaction variance \(\sigma^2_{\rho \tau}\). We can't estimate these components separately with this model. The two are \textbf{confounded}.\\
\item
  If more than 1 observation per treatment block combination, one can consider interaction with fixed block effects, which is called \textbf{generalized randomized block designs} (multifactor analysis).
\end{itemize}

\hypertarget{tukey-test-of-additivity}{%
\subsection{Tukey Test of Additivity}\label{tukey-test-of-additivity}}

(Tukey's 1 df test for additivity)

formal test of interaction effects between blocks and treatments for a randomized block design. can also considered for testing additivity in 2-way analyses when there is only one observation per cell.

we consider a less restricted interaction term

\[
(\rho \tau)_{ij} = D\rho_i \tau_j \text{(D: Constant)}
\]

So,

\[
Y_{ij} = \mu_{..} + \rho_i + \tau_j + D\rho_i \tau_j + \epsilon_{ij}
\]

the least square estimate or MLE for D

\[
\hat{D} = \frac{\sum_i \sum_j \rho_i \tau_j Y_{ij}}{\sum_i \rho_i^2 \sum_j \tau^2_j}
\]

replacing the parameters by their estimates

\[
\hat{D} = \frac{\sum_i \sum_j (\bar{Y}_{i.}- \bar{Y}_{..})(\bar{Y}_{.j}- \bar{Y}_{..})Y_{ij}}{\sum_i (\bar{Y}_{i.}- \bar{Y}_{..})^2 \sum_j(\bar{Y}_{.j}- \bar{Y}_{..})^2}
\]

Thus, the interaction sum of squares

\[
SSint = \sum_i \sum_j \hat{D}^2(\bar{Y}_{i.}- \bar{Y}_{..})^2(\bar{Y}_{.j}- \bar{Y}_{..})^2
\]

The ANOVA decomposition

\[
SSTO = SSBL + SSTR + SSint + SSRem
\]

where \(SSRem\): remainder sum of squares

\[
SSRem = SSTO - SSBL - SSTR - SSint
\]

if \(D = 0\) (i.e., no interactions of the type \(D \rho_i \tau_j\)). \(SSint\) and \(SSRem\) are independent \(\chi^2_{1,rn-r-n}\).

If \(D = 0\),

\[
F = \frac{SSint/1}{SSRem/(rn-r-n)} \sim f_{(1-\alpha;rn-r-n)}
\]

if

\[
\begin{aligned}
&H_0: D = 0 \text{ no interaction present} \\
&H_a: D \neq 0 \text{ interaction of form $D \rho_i \tau_j$ present}
\end{aligned}
\]

we reject \(H_0\) if \(F > f_{(1-\alpha;1,nr-r-n)}\)

\hypertarget{nested-designs}{%
\section{Nested Designs}\label{nested-designs}}

Let \(\mu_{ij}\) be the mean response when factor A is at the i-th level and factor B is at the j-th level.\\
If the factors are crossed, the \(j\)-th level of B is the same for all levels of A.\\
If factor B is nested within A, the j-th level of B when A is at level 1 has nothing in common with the j-th level of B when A is at level 2.

Factors that can't be manipulated are designated as \textbf{classification factors}, as opposed to \textbf{experimental factors} (i.e., you assign to the experimental units).

\hypertarget{two-factor-nested-designs}{%
\subsection{Two-Factor Nested Designs}\label{two-factor-nested-designs}}

\begin{itemize}
\tightlist
\item
  Consider B is nested within A.
\item
  both factors are fixed
\item
  All treatment means are equally important.
\end{itemize}

\textbf{Mean responses}

\[
\mu_{i.} = \sum_j \mu_{ij}/b
\]

Main effect factor A

\[
\alpha_i = \mu_{i.} - \mu_{..}
\]

where \(\mu_{..} = \frac{\mu_{ij}}{ab} = \frac{\sum_i \mu_{i.}}{a}\) and \(\sum_i \alpha_i = 0\)

Individual effects of \(B\) is denoted as \(\beta_{j(i)}\) where \(j(i)\) indicates the \(j\)-th level of factor \(B\) is nested within the it-h level of factor A

\[
\begin{aligned}
\beta_{j(i)} &= \mu_{ij} - \mu_{i.} \\
&= \mu_{ij} - \alpha_i - \mu_{..} \\
\sum_j \beta_{j(i)}&=0 , i = 1,...,a
\end{aligned}
\]

\(\beta_{j(i)}\) is the \textbf{specific effect} of the \(j\)-th level of factor \(B\) nested within the \(i\)-th level of factor \(A\). Hence,

\[
\mu_{ij} \equiv \mu_{..} + \alpha_i + \beta_{j(i)} \equiv \mu_{..} + (\mu_{i.} - \mu_{..}) + (\mu_{ij} - \mu_{i.})
\]

\textbf{Model}

\[
Y_{ijk} = \mu_{..} + \alpha_i + \beta_{j(i)} + \epsilon_{ijk}
\]

where

\begin{itemize}
\tightlist
\item
  \(Y_{ijk}\) response for the \(k\)-th treatment when factor \(A\) is at the \(i\)-th level and factor \(B\) is at the \(j\)-th level \((i = 1,..,a; j = 1,..,b; k = 1,..n)\)
\item
  \(\mu_{..}\) constant
\item
  \(\alpha_i\) constants subject to restriction \(\sum_i \alpha_i = 0\)
\item
  \(\beta_{j(i)}\) constants subject to restriction \(\sum_j \beta_{j(i)} = 0\) for all \(i\)
\item
  \(\epsilon_{ijk} \sim iid N(0,\sigma^2)\)
\end{itemize}

\[
\begin{aligned}
E(Y_{ijk}) &= \mu_{..} + \alpha_i + \beta_{j(i)} \\
var(Y_{ijk}) &= \sigma^2
\end{aligned}
\]

there is no interaction term in a nested model

\textbf{ANOVA for Two-Factor Nested Designs}

Least Squares and MLE estimates

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.3194}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.4722}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Parameter
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Estimator
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(\mu_{..}\) & \(\bar{Y}_{...}\) \\
\(\alpha_i\) & \(\bar{Y}_{i..} - \bar{Y}_{...}\) \\
\(\beta_{j(i)}\) & \(\bar{Y}_{ij.} - \bar{Y}_{i..}\) \\
\(\hat{Y}_{ijk}\) & \(\bar{Y}_{ij.}\) \\
\end{longtable}

residual \(e_{ijk} = Y_{ijk} - \bar{Y}_{ijk}\)

\[
\begin{aligned}
SSTO &= SSA + SSB(A) + SSE \\
\sum_i \sum_j \sum_k (Y_{ijk}- \bar{Y}_{...})^2 &= bn \sum_i (\bar{Y}_{i..}- \bar{Y}_{...})^2 + n \sum_i \sum_j (\bar{Y}_{ij.}- \bar{Y}_{i..})^2  \\
&+ \sum_i \sum_j \sum_k (Y_{ijk} -\bar{Y}_{ij.})^2
\end{aligned}
\]

ANOVA Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1789}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.0894}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.0976}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.0894}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.5203}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Source of Variation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SS
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
df
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MS
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
E(MS)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Factor A & \(SSA\) & \(a-1\) & \(MSA\) & \(\sigma^2 + bn \frac{\sum \alpha_i^2}{a-1}\) \\
Factor B & \(SSB(A)\) & \(a(b-1)\) & \(MSB(A)\) & \(\sigma^2 + n \frac{\  | | | | um \sum e ta_{i)}^ 2}{a(b-1)}\) \\
Error & \(SSE\) & \(ab(n-1)\) & \(MSE\) & \(\sigma^2\) \\
Total & \(SSTO\) & \(abn -1\) & & \\
\end{longtable}

\textbf{Tests For Factor Effects}

\[
\begin{aligned}
&H_0: \text{ All } \alpha_i =0 \\
&H_a: \text{ not all } \alpha_i = 0
\end{aligned}
\]

\(F = \frac{MSA}{MSE} \sim f_{(1-\alpha;a-1,(n-1)ab)}\) reject if \(F > f\)

\[
\begin{aligned}
&H_0: \text{ All } \beta_{j(i)} =0 \\
&H_a: \text{ not all } \beta_{j(i)} = 0
\end{aligned}
\]

\(F = \frac{MSB(A)}{MSE} \sim f_{(1-\alpha;a(b-1),(n-1)ab)}\) reject \(F>f\)

\textbf{Testing Factor Effect Contrasts}

\(L = \sum c_i \mu_i\) where \(\sum c_i =0\)

\[
\begin{aligned}
\hat{L} &= \sum c_i \bar{Y}_{i..} \\
\hat{L} &\pm t_{(1-\alpha/2;df)}s(\hat{L})
\end{aligned}
\]

where \(s^2(\hat{L}) = \sum c_i^2 s^2(\bar{Y}_{i..})\), where \(s^2(\bar{Y}_{i..}) = \frac{MSE}{bn}, df = ab(n-1)\)

\textbf{Testing Treatment Means}

\(L = \sum c_i \mu_{.j}\) estimated by \(\hat{L} = \sum c_i \bar{Y}_{ij}\) with confidence limits:

\[
\hat{L} \pm t_{(1-\alpha/2;(n-1)ab)}s(\hat{L})
\]

where

\[
s^2(\hat{L}) = \frac{MSE}{n}\sum c^2_i
\]

\textbf{Unbalanced Nested Two-Factor Designs}

If there are different number of levels of factor \(B\) for different levels of factor \(A\), then the design is called \textbf{unbalanced}

The model

\[
\begin{aligned}
Y_{ijk} &= \mu_{..} + \alpha_i + \beta_{j(i)} + \epsilon_{ijk} \\
\sum_{i=1}^2 \alpha_i &=0 \\
\sum_{j=1}^3 \beta_{j(1)} &= 0 \\
\sum_{j=1}^2 \beta_{j(2)}&=0
\end{aligned}
\]

where

\begin{itemize}
\item
  \(i = 1,2;j =1,..,b_i;k=1,..,n_{ij}\)
\item
  \(b_1 = 3, b_2= 2, n_{11} = n_{13} =2, n_{12}=1,n_{21} = n_{22} = 2\)
\item
  \(\alpha_1,\beta_{1(1)}, \beta_{2(1)}, \beta_{1(2)}\) are parameters.
\end{itemize}

And constraints: \(\alpha_2 = - \alpha_1, \beta_{3(1)}= - \beta_{1(1)}-\beta_{2(1)}, \beta_{2(2)}=-\beta_{1(2)}\)

4 indicator variables

\begin{equation}
X_1 = 
\begin{cases}
1&\text{if obs from school 1}\\
-1&\text{if obs from school 2}\\
\end{cases}
\end{equation}

\begin{equation}
X_2 = 
\begin{cases}
1&\text{if obs from instructor 1 in school 1}\\
-1&\text{if obs from instructor 3 in school 1}\\
0&\text{otherwise}\\
\end{cases}
\end{equation}

\begin{equation}
X_3 = 
\begin{cases}
1&\text{if obs from instructor 2 in school 1}\\
-1&\text{if obs from instructor 3 in school 1}\\
0&\text{otherwise}\\
\end{cases}
\end{equation}

\begin{equation}
X_4 = 
\begin{cases}
1&\text{if obs from instructor 1 in school 1}\\
-1&\text{if obs from instructor 2 in school 1}\\
0&\text{otherwise}\\
\end{cases}
\end{equation}

Regression Full Model

\[
Y_{ijk} = \mu_{..} + \alpha_1 X_{ijk1} + \beta_{1(1)}X_{ijk2} + \beta_{2(1)}X_{ijk3} + \beta_{1(2)}X_{ijk4} + \epsilon_{ijk}
\]

\textbf{Random Factor Effects}

If

\[
\begin{aligned}
\alpha_1 &\sim iid N(0,\sigma^2_\alpha) \\
\beta_{j(i)} &\sim iid N(0,\sigma^2_\beta)
\end{aligned}
\]

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1029}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4853}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4044}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Mean Square
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Expected Mean Squares

A fixed, B random
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Expected Mean Squares

A random, B random
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
MSA & \(\sigma^ 2 + n \sigma^2_\beta + bn \frac{\sum \alpha_i^2}{a-1}\) & \(\sigma^2 + bn \sigma^2_{\alpha} + n \sigma^2_\beta\) \\
MSB(A) & \(\sigma^2 + n \sigma^2_\beta\) & \(\sigma^2 + n \sigma^2_\beta\) \\
MSE & \(\sigma^2\) & \(\sigma^2\) \\
\end{longtable}

Test Statistics

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Factor A & \(\frac{MSA}{MSB(A)}\) & \(\frac{MSA}{MSB(A)}\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Factor B(A) & \(\frac{MSB(A)}{MSE}\) & \(\frac{MSB(A)}{MSE}\) \\
\end{longtable}

Another way to increase the precision of treatment comparisons by reducing variability is to use regression models to adjust for differences among experimental units (also known as \textbf{analysis of covariance}).

\hypertarget{single-factor-covariance-model}{%
\section{Single Factor Covariance Model}\label{single-factor-covariance-model}}

\[
Y_{ij} = \mu_{.} + \tau_i + \gamma(X_{ij} - \bar{X}_{..}) + \epsilon_{ij} 
\]

for \(i = 1,...,r;j=1,..,n_i\)

where

\begin{itemize}
\tightlist
\item
  \(\mu_.\) overall mean
\item
  \(\tau_i\): fixed treatment effects (\(\sum \tau_i =0\))
\item
  \(\gamma\): fixed regression coefficient effect between X and Y
\item
  \(X_{ij}\) covariate (not random)
\item
  \(\epsilon_{ij} \sim iid N(0,\sigma^2)\): random errors
\end{itemize}

If we just use \(\gamma X_{ij}\) as the regression term (rather than \(\gamma(X_{ij}-\bar{X}_{..})\)), then \(\mu_.\) is no longer the overall mean; thus we need to centered mean.

\[
\begin{aligned}
E(Y_{ij}) &= \mu_. + \tau_i + \gamma(X_{ij}-\bar{X}_{..}) \\
var(Y_{ij}) &= \sigma^2
\end{aligned}
\]

\(Y_{ij} \sim N(\mu_{ij},\sigma^2)\),

where

\[
\begin{aligned}
\mu_{ij} &= \mu_. + \tau_i + \gamma(X_{ij} - \bar{X}_{..}) \\
\sum \tau_i &=0 
\end{aligned}
\]

Thus, the mean response (\(\mu_{ij}\)) is a regression line with intercept \(\mu_. + \tau_i\) and slope \(\gamma\) for each treatment \$\$i.

\textbf{Assumption}:

\begin{itemize}
\tightlist
\item
  All treatment regression lines have the same slope\\
\item
  when treatment interact with covariate \(X\) (non-parallel slopes), covariance analysis is \textbf{not} appropriate. in which case we should use separate regression lines.
\end{itemize}

More complicated regression features (e.g., quadratic, cubic) or additional covariates e.g.,

\[
Y_{ij} = \mu_. + \tau_i + \gamma_1(X_{ij1}-\bar{X}_{..2}) + \gamma_2(X_{ij2}-\bar{X}_{..2}) + \epsilon_{ij}
\]

\textbf{Regression Formulation}

We can use indicator variables for treatments

\[
l_1 =
\begin{cases}
1 & \text{if case is from treatment 1}\\
-1 & \text{if case is from treatment r}\\
0 &\text{otherwise}\\
\end{cases}
\]

\[
.
\]

\[
.
\]

\[
l_{r-1} =
\begin{cases}
1 & \text{if case is from treatment r-1}\\
-1 & \text{if case is from treatment r}\\
0 &\text{otherwise}\\
\end{cases}
\]

Let \(x_{ij} = X_{ij}- \bar{X}_{..}\). the regression model is

\[
Y_{ij} = \mu_. + \tau_1l_{ij,1} + .. + \tau_{r-1}l_{ij,r-1} + \gamma x_{ij}+\epsilon_{ij}
\]

where \(I_{ij,1}\) is the indicator variable \(l_1\) for the j-th case from treatment i. The treatment effect \(\tau_1,..\tau_{r-1}\) are just regression coefficients for the indicator variables.

We could use the same diagnostic tools for this case.

\textbf{Inference}

Treatment effects

\[
\begin{aligned}
&H_0: \tau_1 = \tau_2 = ...= 0 \\
&H_a: \text{not all } \tau_i =0
\end{aligned}
\]

\[
\begin{aligned}
&\text{Full Model}: Y_{ij} = \mu_. + \tau_i + \gamma X_{ij} +\epsilon_{ij}  \\
&\text{Reduced Model}: Y_{ij} = \mu_. + \gamma X_{ij} + \epsilon_{ij}
\end{aligned}
\]

\[
F = \frac{SSE(R) - SSE(F)}{(N-2)-(N-(r+1))} / \frac{SSE(F)}{N-(r+1)} \sim F_{(r-1,N-(r+1))}
\]

If we are interested in comparisons of treatment effects.\\
For example, r - 3. We estimate \(\tau_1,\tau_2, \tau_3 = -\tau_1 - \tau_2\)

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1504}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2556}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5865}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Comparison
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Estimate
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Variance of Estimator
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(\tau_1 - \tau_2\) & \(\hat{\tau}_1 - \hat{\tau}_2\) & \(var(\hat {\tau}_1) + var(\hat{\tau}_2) - 2cov(\hat{ \tau}_1\hat{\tau}_2)\) \\
\(\tau_1 - \tau_3\) & \(2 \hat{\tau}_1 + \hat{\tau}_2\) & \(4var(\hat {\tau}_1) + var(\hat{\tau}_2) - 4cov(\hat{ \tau}_1\hat{\tau}_2)\) \\
\(\tau_2 - \tau_3\) & \(\hat{\tau}_1 + 2 \hat{\tau}_2\) & \(var(\hat{\tau}_1) + 4var(\hat{\tau}_2) - 4cov(\hat{\tau}_1\hat{\tau}_2)\) \\
\end{longtable}

Testing for Parallel Slopes

Example:

r = 3

\[
Y_{ij} = \mu_{.} + \tau_1 I_{ij,1} + \tau_2 I_{ij,2} + \gamma X_{ij} + \beta_1 I_{ij,1}X_{ij} + \beta_2 I_{ij,2}X_{ij} + \epsilon_{ij}
\]

where \(\beta_1,\beta_2\): interaction coefficients.

\[
\begin{aligned}
&H_0: \beta_1 = \beta_2 = 0 \\
&H_a: \text{at least one} \beta \neq 0 
\end{aligned}
\]

If we can't reject \(H_0\) using F-test then we have evidence that the slopes are parallel

\textbf{Adjusted Means}

The means in response after adjusting for the covariate effect

\[
Y_{i.}(adj) = \bar{Y}_{i.} - \hat{\gamma}(\bar{X}_{i.} - \bar{X}_{..})
\]

\hypertarget{multivariate-methods}{%
\chapter{Multivariate Methods}\label{multivariate-methods}}

\(y_1,...,y_p\) are possibly correlated random variables with means \(\mu_1,...,\mu_p\)

\[
\mathbf{y} = 
\left(
\begin{array}
{c}
y_1 \\
. \\
y_p \\
\end{array}
\right)
\]

\[
E(\mathbf{y}) = 
\left(
\begin{array}
{c}
\mu_1 \\
. \\
\mu_p \\
\end{array}
\right)
\]

Let \(\sigma_{ij} = cov(y_i, y_j)\) for \(i,j = 1,,p\)

\[
\mathbf{\Sigma} = (\sigma_{ij}) = 
\left(
\begin{array}
{cccc}
\sigma_{11} & \sigma_{22} & ... &  \sigma_{1p} \\
\sigma_{21} & \sigma_{22} & ... & \sigma_{2p} \\
. & . & . & . \\
\sigma_{p1} & \sigma_{p2} & ... & \sigma_{pp}
\end{array}
\right)
\]

where \(\mathbf{\Sigma}\) (symmetric) is the variance-covariance or dispersion matrix

Let \(\mathbf{u}_{p \times 1}\) and \(\mathbf{v}_{q \times 1}\) be random vectors with means \(\mu_u\) and \(\mu_v\) . Then

\[
\mathbf{\Sigma}_{uv} = cov(\mathbf{u,v}) = E[(\mathbf{u} - \mu_u)(\mathbf{v} - \mu_v)']
\]

in which \(\mathbf{\Sigma}_{uv} \neq \mathbf{\Sigma}_{vu}\) and \(\mathbf{\Sigma}_{uv} = \mathbf{\Sigma}_{vu}'\)

\hfill\break
\textbf{Properties of Covariance Matrices}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Symmetric \(\mathbf{\Sigma}' = \mathbf{\Sigma}\)
\item
  Non-negative definite \(\mathbf{a'\Sigma a} \ge 0\) for any \(\mathbf{a} \in R^p\), which is equivalent to eigenvalues of \(\mathbf{\Sigma}\), \(\lambda_1 \ge \lambda_2 \ge ... \ge \lambda_p \ge 0\)
\item
  \(|\mathbf{\Sigma}| = \lambda_1 \lambda_2 ... \lambda_p \ge 0\) (\textbf{generalized variance}) (the bigger this number is, the more variation there is
\item
  \(trace(\mathbf{\Sigma}) = tr(\mathbf{\Sigma}) = \lambda_1 + ... + \lambda_p = \sigma_{11} + ... + \sigma_{pp} =\) sum of variance (\textbf{total variance})
\end{enumerate}

Note:

\begin{itemize}
\tightlist
\item
  \(\mathbf{\Sigma}\) is typically required to be positive definite, which means all eigenvalues are positive, and \(\mathbf{\Sigma}\) has an inverse \(\mathbf{\Sigma}^{-1}\) such that \(\mathbf{\Sigma}^{-1}\mathbf{\Sigma} = \mathbf{I}_{p \times p} = \mathbf{\Sigma \Sigma}^{-1}\)
\end{itemize}

\textbf{Correlation Matrices}

\[
\rho_{ij} = \frac{\sigma_{ij}}{\sqrt{\sigma_{ii} \sigma_{jj}}}
\]

\[
\mathbf{R} = 
\left(
\begin{array}
{cccc}
\rho_{11} & \rho_{12} & ... & \rho_{1p} \\
\rho_{21} & \rho_{22} & ... & \rho_{2p} \\
. & . & . &. \\
\rho_{p1} & \rho_{p2} & ... & \rho_{pp} \\
\end{array}
\right)
\]

where \(\rho_{ij}\) is the correlation, and \(\rho_{ii} = 1\) for all i

Alternatively,

\[
\mathbf{R} = [diag(\mathbf{\Sigma})]^{-1/2}\mathbf{\Sigma}[diag(\mathbf{\Sigma})]^{-1/2}
\]

where \(diag(\mathbf{\Sigma})\) is the matrix which has the \(\sigma_{ii}\)'s on the diagonal and 0's elsewhere

and \(\mathbf{A}^{1/2}\) (the square root of a symmetric matrix) is a symmetric matrix such as \(\mathbf{A} = \mathbf{A}^{1/2}\mathbf{A}^{1/2}\)

\textbf{Equalities}

Let

\begin{itemize}
\item
  \(\mathbf{x}\) and \(\mathbf{y}\) be random vectors with means \(\mu_x\) and \(\mu_y\) and variance -variance matrices \(\mathbf{\Sigma}_x\) and \(\mathbf{\Sigma}_y\).
\item
  \(\mathbf{A}\) and \(\mathbf{B}\) be matrices of constants and \(\mathbf{c}\) and \(\mathbf{d}\) be vectors of constants
\end{itemize}

Then

\begin{itemize}
\item
  \(E(\mathbf{Ay + c} ) = \mathbf{A} \mu_y + c\)
\item
  \(var(\mathbf{Ay + c}) = \mathbf{A} var(\mathbf{y})\mathbf{A}' = \mathbf{A \Sigma_y A}'\)
\item
  \(cov(\mathbf{Ay + c, By+ d}) = \mathbf{A\Sigma_y B}'\)
\item
  \(E(\mathbf{Ay + Bx + c}) = \mathbf{A \mu_y + B \mu_x + c}\)
\item
  \(var(\mathbf{Ay + Bx + c}) = \mathbf{A \Sigma_y A' + B \Sigma_x B' + A \Sigma_{yx}B' + B\Sigma'_{yx}A'}\)
\end{itemize}

\textbf{Multivariate Normal Distribution}

Let \(\mathbf{y}\) be a multivariate normal (MVN) random variable with mean \(\mu\) and variance \(\mathbf{\Sigma}\). Then the density of \(\mathbf{y}\) is

\[
f(\mathbf{y}) = \frac{1}{(2\pi)^{p/2}|\mathbf{\Sigma}|^{1/2}} \exp(-\frac{1}{2} \mathbf{(y-\mu)'\Sigma^{-1}(y-\mu)} )
\]

\(\mathbf{y} \sim N_p(\mu, \mathbf{\Sigma})\)

\hypertarget{properties-of-mvn}{%
\subsection{Properties of MVN}\label{properties-of-mvn}}

\begin{itemize}
\item
  Let \(\mathbf{A}_{r \times p}\) be a fixed matrix. Then \(\mathbf{Ay} \sim N_r (\mathbf{A \mu, A \Sigma A'})\) . \(r \le p\) and all rows of \(\mathbf{A}\) must be linearly independent to guarantee that \(\mathbf{A \Sigma A}'\) is non-singular.
\item
  Let \(\mathbf{G}\) be a matrix such that \(\mathbf{\Sigma}^{-1} = \mathbf{GG}'\). Then \(\mathbf{G'y} \sim N_p(\mathbf{G' \mu, I})\) and \(\mathbf{G'(y-\mu)} \sim N_p (0,\mathbf{I})\)
\item
  Any fixed linear combination of \(y_1,...,y_p\) (say \(\mathbf{c'y}\)) follows \(\mathbf{c'y} \sim N_1 (\mathbf{c' \mu, c' \Sigma c})\)
\item
  Define a partition, \([\mathbf{y}'_1,\mathbf{y}_2']'\) where

  \begin{itemize}
  \item
    \(\mathbf{y}_1\) is \(p_1 \times 1\)
  \item
    \(\mathbf{y}_2\) is \(p_2 \times 1\),
  \item
    \(p_1 + p_2 = p\)
  \item
    \(p_1,p_2 \ge 1\) Then
  \end{itemize}
\end{itemize}

\[
\left(
\begin{array}
{c}
\mathbf{y}_1 \\
\mathbf{y}_2 \\
\end{array}
\right)
\sim
N
\left(
\left(
\begin{array}
{c}
\mu_1 \\
\mu_2 \\
\end{array}
\right),
\left(
\begin{array}
{cc}
\mathbf{\Sigma}_{11} & \mathbf{\Sigma}_{12} \\
\mathbf{\Sigma}_{21} & \mathbf{\Sigma}_{22}\\
\end{array}
\right)
\right)
\]

\begin{itemize}
\item
  The marginal distributions of \(\mathbf{y}_1\) and \(\mathbf{y}_2\) are \(\mathbf{y}_1 \sim N_{p1}(\mathbf{\mu_1, \Sigma_{11}})\) and \(\mathbf{y}_2 \sim N_{p2}(\mathbf{\mu_2, \Sigma_{22}})\)
\item
  Individual components \(y_1,...,y_p\) are all normally distributed \(y_i \sim N_1(\mu_i, \sigma_{ii})\)
\item
  The conditional distribution of \(\mathbf{y}_1\) and \(\mathbf{y}_2\) is normal

  \begin{itemize}
  \item
    \(\mathbf{y}_1 | \mathbf{y}_2 \sim N_{p1}(\mathbf{\mu_1 + \Sigma_{12} \Sigma_{22}^{-1}(y_2 - \mu_2),\Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \sigma_{21}})\)

    \begin{itemize}
    \tightlist
    \item
      In this formula, we see if we know (have info about) \(\mathbf{y}_2\), we can re-weight \(\mathbf{y}_1\) 's mean, and the variance is reduced because we know more about \(\mathbf{y}_1\) because we know \(\mathbf{y}_2\)
    \end{itemize}
  \item
    which is analogous to \(\mathbf{y}_2 | \mathbf{y}_1\). And \(\mathbf{y}_1\) and \(\mathbf{y}_2\) are independently distrusted only if \(\mathbf{\Sigma}_{12} = 0\)
  \end{itemize}
\item
  If \(\mathbf{y} \sim N(\mathbf{\mu, \Sigma})\) and \(\mathbf{\Sigma}\) is positive definite, then \(\mathbf{(y-\mu)' \Sigma^{-1} (y - \mu)} \sim \chi^2_{(p)}\)
\item
  If \(\mathbf{y}_i\) are independent \(N_p (\mathbf{\mu}_i , \mathbf{\Sigma}_i)\) random variables, then for fixed matrices \(\mathbf{A}_{i(m \times p)}\), \(\sum_{i=1}^k \mathbf{A}_i \mathbf{y}_i \sim N_m (\sum_{i=1}^{k} \mathbf{A}_i \mathbf{\mu}_i, \sum_{i=1}^k \mathbf{A}_i \mathbf{\Sigma}_i \mathbf{A}_i)\)
\end{itemize}

\textbf{Multiple Regression}

\[
\left(
\begin{array}
{c}
Y \\
\mathbf{x}
\end{array}
\right)
\sim 
N_{p+1}
\left(
\left[
\begin{array}
{c}
\mu_y \\
\mathbf{\mu}_x
\end{array}
\right]
,
\left[
\begin{array}
{cc}
\sigma^2_Y & \mathbf{\Sigma}_{yx} \\
\mathbf{\Sigma}_{yx} & \mathbf{\Sigma}_{xx}
\end{array}
\right]
\right)
\]

The conditional distribution of Y given x follows a univariate normal distribution with

\[
\begin{aligned}
E(Y| \mathbf{x}) &= \mu_y + \mathbf{\Sigma}_{yx} \Sigma_{xx}^{-1} (\mathbf{x}- \mu_x) \\
&= \mu_y - \Sigma_{yx} \Sigma_{xx}^{-1}\mu_x + \Sigma_{yx} \Sigma_{xx}^{-1}\mathbf{x} \\
&= \beta_0 + \mathbf{\beta'x}
\end{aligned} 
\]

where \(\beta = (\beta_1,...,\beta_p)' = \mathbf{\Sigma}_{xx}^{-1} \mathbf{\Sigma}_{yx}'\) (e.g., analogous to \(\mathbf{(x'x)^{-1}x'y}\) but not the same if we consider \(Y_i\) and \(\mathbf{x}_i\), \(i = 1,..,n\) and use the empirical covariance formula: \(var(Y|\mathbf{x}) = \sigma^2_Y - \mathbf{\Sigma_{yx}\Sigma^{-1}_{xx} \Sigma'_{yx}}\))

\textbf{Samples from Multivariate Normal Populations}

A random sample of size n, \(\mathbf{y}_1,.., \mathbf{y}_n\) from \(N_p (\mathbf{\mu}, \mathbf{\Sigma})\). Then

\begin{itemize}
\item
  Since \(\mathbf{y}_1,..., \mathbf{y}_n\) are iid, their sample mean, \(\bar{\mathbf{y}} = \sum_{i=1}^n \mathbf{y}_i/n \sim N_p (\mathbf{\mu}, \mathbf{\Sigma}/n)\). that is, \(\bar{\mathbf{y}}\) is an unbiased estimator of \(\mathbf{\mu}\)
\item
  The \(p \times p\) sample variance-covariance matrix, \(\mathbf{S}\) is \(\mathbf{S} = \frac{1}{n-1}\sum_{i=1}^n (\mathbf{y}_i - \bar{\mathbf{y}})(\mathbf{y}_i - \bar{\mathbf{y}})' = \frac{1}{n-1} (\sum_{i=1}^n \mathbf{y}_i \mathbf{y}_i' - n \bar{\mathbf{y}}\bar{\mathbf{y}}')\)

  \begin{itemize}
  \tightlist
  \item
    where \(\mathbf{S}\) is symmetric, unbiased estimator of \(\mathbf{\Sigma}\) and has \(p(p+1)/2\) random variables.
  \end{itemize}
\item
  \((n-1)\mathbf{S} \sim W_p (n-1, \mathbf{\Sigma})\) is a Wishart distribution with n-1 degrees of freedom and expectation \((n-1) \mathbf{\Sigma}\). The Wishart distribution is a multivariate extension of the Chi-squared distribution.
\item
  \(\bar{\mathbf{y}}\) and \(\mathbf{S}\) are independent
\item
  \(\bar{\mathbf{y}}\) and \(\mathbf{S}\) are sufficient statistics. (All of the info in the data about \(\mathbf{\mu}\) and \(\mathbf{\Sigma}\) is contained in \(\bar{\mathbf{y}}\) and \(\mathbf{S}\) , regardless of sample size).
\end{itemize}

\textbf{Large Sample Properties}

\(\mathbf{y}_1,..., \mathbf{y}_n\) are a random sample from some population with mean \(\mathbf{\mu}\) and variance-covariance matrix \(\mathbf{\Sigma}\)

\begin{itemize}
\item
  \(\bar{\mathbf{y}}\) is a consistent estimator for \(\mu\)
\item
  \(\mathbf{S}\) is a consistent estimator for \(\mathbf{\Sigma}\)
\item
  \textbf{Multivariate Central Limit Theorem}: Similar to the univariate case, \(\sqrt{n}(\bar{\mathbf{y}} - \mu) \dot{\sim} N_p (\mathbf{0,\Sigma})\) where n is large relative to p (\(n \ge 25p\)), which is equivalent to \(\bar{\mathbf{y}} \dot{\sim} N_p (\mu, \mathbf{\Sigma}/n)\)
\item
  \textbf{Wald's Theorem}: \(n(\bar{\mathbf{y}} - \mu)' \mathbf{S}^{-1} (\bar{\mathbf{y}} - \mu)\) when n is large relative to p.
\end{itemize}

Maximum Likelihood Estimation for MVN

Suppose iid \(\mathbf{y}_1 ,... \mathbf{y}_n \sim N_p (\mu, \mathbf{\Sigma})\), the likelihood function for the data is

\[
\begin{aligned}
L(\mu, \mathbf{\Sigma}) &= \prod_{j=1}^n (\frac{1}{(2\pi)^{p/2}|\mathbf{\Sigma}|^{1/2}} \exp(-\frac{1}{2}(\mathbf{y}_j -\mu)'\mathbf{\Sigma}^{-1})(\mathbf{y}_j -\mu)) \\
&= \frac{1}{(2\pi)^{np/2}|\mathbf{\Sigma}|^{n/2}} \exp(-\frac{1}{2} \sum_{j=1}^n(\mathbf{y}_j -\mu)'\mathbf{\Sigma}^{-1})(\mathbf{y}_j -\mu)
\end{aligned}
\]

Then, the MLEs are

\[
\hat{\mu} = \bar{\mathbf{y}}
\]

\[
\hat{\mathbf{\Sigma}} = \frac{n-1}{n} \mathbf{S}
\]

using derivatives of the log of the likelihood function with respect to \(\mu\) and \(\mathbf{\Sigma}\)

\textbf{Properties of MLEs}

\begin{itemize}
\item
  Invariance: If \(\hat{\theta}\) is the MLE of \(\theta\), then the MLE of \(h(\theta)\) is \(h(\hat{\theta})\) for any function h(.)
\item
  Consistency: MLEs are consistent estimators, but they are usually biased
\item
  Efficiency: MLEs are efficient estimators (no other estimator has a smaller variance for large samples)
\item
  Asymptotic normality: Suppose that \(\hat{\theta}_n\) is the MLE for \(\theta\) based upon n independent observations. Then \(\hat{\theta}_n \dot{\sim} N(\theta, \mathbf{H}^{-1})\)

  \begin{itemize}
  \item
    \(\mathbf{H}\) is the Fisher Information Matrix, which contains the expected values of the second partial derivatives fo the log-likelihood function. the (i,j)th element of \(\mathbf{H}\) is \(-E(\frac{\partial^2 l(\mathbf{\theta})}{\partial \theta_i \partial \theta_j})\)
  \item
    we can estimate \(\mathbf{H}\) by finding the form determined above, and evaluate it at \(\theta = \hat{\theta}_n\)
  \end{itemize}
\item
  Likelihood ratio testing: for some null hypothesis, \(H_0\) we can form a likelihood ratio test

  \begin{itemize}
  \item
    The statistic is: \(\Lambda = \frac{\max_{H_0}l(\mathbf{\mu}, \mathbf{\Sigma|Y})}{\max l(\mu, \mathbf{\Sigma | Y})}\)
  \item
    For large n, \(-2 \log \Lambda \sim \chi^2_{(v)}\) where v is the number of parameters in the unrestricted space minus the number of parameters under \(H_0\)
  \end{itemize}
\end{itemize}

\textbf{Test of Multivariate Normality}

\begin{itemize}
\item
  Check univariate normality for each trait (X) separately

  \begin{itemize}
  \item
    Can check \[Normality Assessment\]
  \item
    The good thing is that if any of the univariate trait is not normal, then the joint distribution is not normal (see again {[}m{]}). If a joint multivariate distribution is normal, then the marginal distribution has to be normal.
  \item
    However, marginal normality of all traits does not imply joint MVN
  \item
    Easily rule out multivariate normality, but not easy to prove it
  \end{itemize}
\item
  Mardia's tests for multivariate normality

  \begin{itemize}
  \item
    Multivariate skewness is\[
    \beta_{1,p} = E[(\mathbf{y}- \mathbf{\mu})' \mathbf{\Sigma}^{-1} (\mathbf{x} - \mathbf{\mu})]^3
    \]
  \item
    where \(\mathbf{x}\) and \(\mathbf{y}\) are independent, but have the same distribution (note: \(\beta\) here is not regression coefficient)
  \item
    Multivariate kurtosis is defined as
  \item
    \[
    \beta_{2,p} - E[(\mathbf{y}- \mathbf{\mu})' \mathbf{\Sigma}^{-1} (\mathbf{x} - \mathbf{\mu})]^2
    \]
  \item
    For the MVN distribution, we have \(\beta_{1,p} = 0\) and \(\beta_{2,p} = p(p+2)\)
  \item
    For a sample of size n, we can estimate

    \[
    \hat{\beta}_{1,p} = \frac{1}{n^2}\sum_{i=1}^n \sum_{j=1}^n g^2_{ij}
    \]

    \[
    \hat{\beta}_{2,p} = \frac{1}{n} \sum_{i=1}^n g^2_{ii}
    \]

    \begin{itemize}
    \tightlist
    \item
      where \(g_{ij} = (\mathbf{y}_i - \bar{\mathbf{y}})' \mathbf{S}^{-1} (\mathbf{y}_j - \bar{\mathbf{y}})\). Note: \(g_{ii} = d^2_i\) where \(d^2_i\) is the Mahalanobis distance
    \end{itemize}
  \item
    \citep{mardia1970measures} shows for large n

    \[
    \kappa_1 = \frac{n \hat{\beta}_{1,p}}{6} \dot{\sim} \chi^2_{p(p+1)(p+2)/6}
    \]

    \[
    \kappa_2 = \frac{\hat{\beta}_{2,p} - p(p+2)}{\sqrt{8p(p+2)/n}} \sim N(0,1)
    \]

    \begin{itemize}
    \item
      Hence, we can use \(\kappa_1\) and \(\kappa_2\) to test the null hypothesis of MVN.
    \item
      When the data are non-normal, normal theory tests on the mean are sensitive to \(\beta_{1,p}\) , while tests on the covariance are sensitive to \(\beta_{2,p}\)
    \end{itemize}
  \end{itemize}
\item
  Alternatively, Doornik-Hansen test for multivariate normality \citep{doornik2008omnibus}
\item
  Chi-square Q-Q plot

  \begin{itemize}
  \item
    Let \(\mathbf{y}_i, i = 1,...,n\) be a random sample sample from \(N_p(\mathbf{\mu}, \mathbf{\Sigma})\)
  \item
    Then \(\mathbf{z}_i = \mathbf{\Sigma}^{-1/2}(\mathbf{y}_i - \mathbf{\mu}), i = 1,...,n\) are iid \(N_p (\mathbf{0}, \mathbf{I})\). Thus, \(d_i^2 = \mathbf{z}_i' \mathbf{z}_i \sim \chi^2_p , i = 1,...,n\)
  \item
    plot the ordered \(d_i^2\) values against the qualities of the \(\chi^2_p\) distribution. When normality holds, the plot should approximately resemble a straight lien passing through the origin at a 45 degree
  \item
    it requires large sample size (i.e., sensitive to sample size). Even if we generate data from a MVN, the tail of the Chi-square Q-Q plot can still be out of line.
  \end{itemize}
\item
  If the data are not normal, we can

  \begin{itemize}
  \item
    ignore it
  \item
    use nonparametric methods
  \item
    use models based upon an approximate distribution (e.g., GLMM)
  \item
    try performing a transformation
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(heplots)}
\FunctionTok{library}\NormalTok{(ICSNP)}
\FunctionTok{library}\NormalTok{(MVN)}
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{trees }\OtherTok{=} \FunctionTok{read.table}\NormalTok{(}\StringTok{"images/trees.dat"}\NormalTok{)}
\FunctionTok{names}\NormalTok{(trees) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Nitrogen"}\NormalTok{,}\StringTok{"Phosphorous"}\NormalTok{,}\StringTok{"Potassium"}\NormalTok{,}\StringTok{"Ash"}\NormalTok{,}\StringTok{"Height"}\NormalTok{)}
\FunctionTok{str}\NormalTok{(trees)}
\CommentTok{\#\textgreater{} \textquotesingle{}data.frame\textquotesingle{}:    26 obs. of  5 variables:}
\CommentTok{\#\textgreater{}  $ Nitrogen   : num  2.2 2.1 1.52 2.88 2.18 1.87 1.52 2.37 2.06 1.84 ...}
\CommentTok{\#\textgreater{}  $ Phosphorous: num  0.417 0.354 0.208 0.335 0.314 0.271 0.164 0.302 0.373 0.265 ...}
\CommentTok{\#\textgreater{}  $ Potassium  : num  1.35 0.9 0.71 0.9 1.26 1.15 0.83 0.89 0.79 0.72 ...}
\CommentTok{\#\textgreater{}  $ Ash        : num  1.79 1.08 0.47 1.48 1.09 0.99 0.85 0.94 0.8 0.77 ...}
\CommentTok{\#\textgreater{}  $ Height     : int  351 249 171 373 321 191 225 291 284 213 ...}

\FunctionTok{summary}\NormalTok{(trees)}
\CommentTok{\#\textgreater{}     Nitrogen      Phosphorous       Potassium           Ash        }
\CommentTok{\#\textgreater{}  Min.   :1.130   Min.   :0.1570   Min.   :0.3800   Min.   :0.4500  }
\CommentTok{\#\textgreater{}  1st Qu.:1.532   1st Qu.:0.1963   1st Qu.:0.6050   1st Qu.:0.6375  }
\CommentTok{\#\textgreater{}  Median :1.855   Median :0.2250   Median :0.7150   Median :0.9300  }
\CommentTok{\#\textgreater{}  Mean   :1.896   Mean   :0.2506   Mean   :0.7619   Mean   :0.8873  }
\CommentTok{\#\textgreater{}  3rd Qu.:2.160   3rd Qu.:0.2975   3rd Qu.:0.8975   3rd Qu.:0.9825  }
\CommentTok{\#\textgreater{}  Max.   :2.880   Max.   :0.4170   Max.   :1.3500   Max.   :1.7900  }
\CommentTok{\#\textgreater{}      Height     }
\CommentTok{\#\textgreater{}  Min.   : 65.0  }
\CommentTok{\#\textgreater{}  1st Qu.:122.5  }
\CommentTok{\#\textgreater{}  Median :181.0  }
\CommentTok{\#\textgreater{}  Mean   :196.6  }
\CommentTok{\#\textgreater{}  3rd Qu.:276.0  }
\CommentTok{\#\textgreater{}  Max.   :373.0}
\FunctionTok{cor}\NormalTok{(trees, }\AttributeTok{method =} \StringTok{"pearson"}\NormalTok{) }\CommentTok{\# correlation matrix}
\CommentTok{\#\textgreater{}              Nitrogen Phosphorous Potassium       Ash    Height}
\CommentTok{\#\textgreater{} Nitrogen    1.0000000   0.6023902 0.5462456 0.6509771 0.8181641}
\CommentTok{\#\textgreater{} Phosphorous 0.6023902   1.0000000 0.7037469 0.6707871 0.7739656}
\CommentTok{\#\textgreater{} Potassium   0.5462456   0.7037469 1.0000000 0.6710548 0.7915683}
\CommentTok{\#\textgreater{} Ash         0.6509771   0.6707871 0.6710548 1.0000000 0.7676771}
\CommentTok{\#\textgreater{} Height      0.8181641   0.7739656 0.7915683 0.7676771 1.0000000}

\CommentTok{\# qq{-}plot }
\NormalTok{gg }\OtherTok{\textless{}{-}}\NormalTok{ trees }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{pivot\_longer}\NormalTok{(}\FunctionTok{everything}\NormalTok{(), }\AttributeTok{names\_to =} \StringTok{"Var"}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{"Value"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{sample =}\NormalTok{ Value)) }\SpecialCharTok{+}
    \FunctionTok{geom\_qq}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{geom\_qq\_line}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{facet\_wrap}\NormalTok{(}\StringTok{"Var"}\NormalTok{, }\AttributeTok{scales =} \StringTok{"free"}\NormalTok{)}
\NormalTok{gg}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{22-multivariate_files/figure-latex/unnamed-chunk-1-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Univariate normality}
\NormalTok{sw\_tests }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(trees, }\AttributeTok{MARGIN =} \DecValTok{2}\NormalTok{, }\AttributeTok{FUN =}\NormalTok{ shapiro.test)}
\NormalTok{sw\_tests}
\CommentTok{\#\textgreater{} $Nitrogen}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Shapiro{-}Wilk normality test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  newX[, i]}
\CommentTok{\#\textgreater{} W = 0.96829, p{-}value = 0.5794}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Phosphorous}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Shapiro{-}Wilk normality test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  newX[, i]}
\CommentTok{\#\textgreater{} W = 0.93644, p{-}value = 0.1104}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Potassium}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Shapiro{-}Wilk normality test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  newX[, i]}
\CommentTok{\#\textgreater{} W = 0.95709, p{-}value = 0.3375}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Ash}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Shapiro{-}Wilk normality test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  newX[, i]}
\CommentTok{\#\textgreater{} W = 0.92071, p{-}value = 0.04671}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Height}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Shapiro{-}Wilk normality test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  newX[, i]}
\CommentTok{\#\textgreater{} W = 0.94107, p{-}value = 0.1424}
\CommentTok{\# Kolmogorov{-}Smirnov test }
\NormalTok{ks\_tests }\OtherTok{\textless{}{-}} \FunctionTok{map}\NormalTok{(trees, }\SpecialCharTok{\textasciitilde{}} \FunctionTok{ks.test}\NormalTok{(}\FunctionTok{scale}\NormalTok{(.x),}\StringTok{"pnorm"}\NormalTok{))}
\NormalTok{ks\_tests}
\CommentTok{\#\textgreater{} $Nitrogen}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Asymptotic one{-}sample Kolmogorov{-}Smirnov test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  scale(.x)}
\CommentTok{\#\textgreater{} D = 0.12182, p{-}value = 0.8351}
\CommentTok{\#\textgreater{} alternative hypothesis: two{-}sided}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Phosphorous}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Asymptotic one{-}sample Kolmogorov{-}Smirnov test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  scale(.x)}
\CommentTok{\#\textgreater{} D = 0.17627, p{-}value = 0.3944}
\CommentTok{\#\textgreater{} alternative hypothesis: two{-}sided}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Potassium}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Asymptotic one{-}sample Kolmogorov{-}Smirnov test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  scale(.x)}
\CommentTok{\#\textgreater{} D = 0.10542, p{-}value = 0.9348}
\CommentTok{\#\textgreater{} alternative hypothesis: two{-}sided}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Ash}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Asymptotic one{-}sample Kolmogorov{-}Smirnov test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  scale(.x)}
\CommentTok{\#\textgreater{} D = 0.14503, p{-}value = 0.6449}
\CommentTok{\#\textgreater{} alternative hypothesis: two{-}sided}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Height}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Asymptotic one{-}sample Kolmogorov{-}Smirnov test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  scale(.x)}
\CommentTok{\#\textgreater{} D = 0.1107, p{-}value = 0.9076}
\CommentTok{\#\textgreater{} alternative hypothesis: two{-}sided}

\CommentTok{\# Mardia\textquotesingle{}s test, need large sample size for power}
\NormalTok{mardia\_test }\OtherTok{\textless{}{-}}
    \FunctionTok{mvn}\NormalTok{(}
\NormalTok{        trees,}
        \AttributeTok{mvnTest =} \StringTok{"mardia"}\NormalTok{,}
        \AttributeTok{covariance =} \ConstantTok{FALSE}\NormalTok{,}
        \AttributeTok{multivariatePlot =} \StringTok{"qq"}
\NormalTok{    )}

\NormalTok{mardia\_test}\SpecialCharTok{$}\NormalTok{multivariateNormality}
\CommentTok{\#\textgreater{}              Test         Statistic            p value Result}
\CommentTok{\#\textgreater{} 1 Mardia Skewness  29.7248528871795   0.72054426745778    YES}
\CommentTok{\#\textgreater{} 2 Mardia Kurtosis {-}1.67743173185383 0.0934580886477281    YES}
\CommentTok{\#\textgreater{} 3             MVN              \textless{}NA\textgreater{}               \textless{}NA\textgreater{}    YES}

\CommentTok{\# Doornik{-}Hansen\textquotesingle{}s test }
\NormalTok{dh\_test }\OtherTok{\textless{}{-}}
    \FunctionTok{mvn}\NormalTok{(}
\NormalTok{        trees,}
        \AttributeTok{mvnTest =} \StringTok{"dh"}\NormalTok{,}
        \AttributeTok{covariance =} \ConstantTok{FALSE}\NormalTok{,}
        \AttributeTok{multivariatePlot =} \StringTok{"qq"}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{22-multivariate_files/figure-latex/unnamed-chunk-1-2} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dh\_test}\SpecialCharTok{$}\NormalTok{multivariateNormality}
\CommentTok{\#\textgreater{}             Test        E df      p value MVN}
\CommentTok{\#\textgreater{} 1 Doornik{-}Hansen 161.9446 10 1.285352e{-}29  NO}

\CommentTok{\# Henze{-}Zirkler\textquotesingle{}s test }
\NormalTok{hz\_test }\OtherTok{\textless{}{-}}
    \FunctionTok{mvn}\NormalTok{(}
\NormalTok{        trees,}
        \AttributeTok{mvnTest =} \StringTok{"hz"}\NormalTok{,}
        \AttributeTok{covariance =} \ConstantTok{FALSE}\NormalTok{,}
        \AttributeTok{multivariatePlot =} \StringTok{"qq"}
\NormalTok{    )}
\NormalTok{hz\_test}\SpecialCharTok{$}\NormalTok{multivariateNormality}
\CommentTok{\#\textgreater{}            Test        HZ   p value MVN}
\CommentTok{\#\textgreater{} 1 Henze{-}Zirkler 0.7591525 0.6398905 YES}
\CommentTok{\# The last column indicates whether dataset follows a multivariate normality or not (i.e, YES or NO) at significance level 0.05.}

\CommentTok{\# Royston\textquotesingle{}s test}
\CommentTok{\# can only apply for 3 \textless{} obs \textless{} 5000 (because of Shapiro{-}Wilk\textquotesingle{}s test)}
\NormalTok{royston\_test }\OtherTok{\textless{}{-}}
    \FunctionTok{mvn}\NormalTok{(}
\NormalTok{        trees,}
        \AttributeTok{mvnTest =} \StringTok{"royston"}\NormalTok{,}
        \AttributeTok{covariance =} \ConstantTok{FALSE}\NormalTok{,}
        \AttributeTok{multivariatePlot =} \StringTok{"qq"}
\NormalTok{    )}
\NormalTok{royston\_test}\SpecialCharTok{$}\NormalTok{multivariateNormality}
\CommentTok{\#\textgreater{}      Test        H    p value MVN}
\CommentTok{\#\textgreater{} 1 Royston 9.064631 0.08199215 YES}


\CommentTok{\# E{-}statistic}
\NormalTok{estat\_test }\OtherTok{\textless{}{-}}
    \FunctionTok{mvn}\NormalTok{(}
\NormalTok{        trees,}
        \AttributeTok{mvnTest =} \StringTok{"energy"}\NormalTok{,}
        \AttributeTok{covariance =} \ConstantTok{FALSE}\NormalTok{,}
        \AttributeTok{multivariatePlot =} \StringTok{"qq"}
\NormalTok{    )}
\NormalTok{estat\_test}\SpecialCharTok{$}\NormalTok{multivariateNormality}
\CommentTok{\#\textgreater{}          Test Statistic p value MVN}
\CommentTok{\#\textgreater{} 1 E{-}statistic  1.091101   0.526 YES}
\end{Highlighting}
\end{Shaded}

\hypertarget{mean-vector-inference}{%
\subsection{Mean Vector Inference}\label{mean-vector-inference}}

In the univariate normal distribution, we test \(H_0: \mu =\mu_0\) by using

\[
T = \frac{\bar{y}- \mu_0}{s/\sqrt{n}} \sim t_{n-1}
\]

under the null hypothesis. And reject the null if \(|T|\) is large relative to \(t_{(1-\alpha/2,n-1)}\) because it means that seeing a value as large as what we observed is rare if the null is true

Equivalently,

\[
T^2 = \frac{(\bar{y}- \mu_0)^2}{s^2/n} = n(\bar{y}- \mu_0)(s^2)^{-1}(\bar{y}- \mu_0) \sim f_{(1,n-1)}
\]

\hypertarget{natural-multivariate-generalization}{%
\subsubsection{\texorpdfstring{\textbf{Natural Multivariate Generalization}}{Natural Multivariate Generalization}}\label{natural-multivariate-generalization}}

\[
\begin{aligned}
&H_0: \mathbf{\mu} = \mathbf{\mu}_0 \\
&H_a: \mathbf{\mu} \neq \mathbf{\mu}_0
\end{aligned}
\]

Define \textbf{Hotelling's} \(T^2\) by

\[
T^2 = n(\bar{\mathbf{y}} - \mathbf{\mu}_0)'\mathbf{S}^{-1}(\bar{\mathbf{y}} - \mathbf{\mu}_0)
\]

which can be viewed as a generalized distance between \(\bar{\mathbf{y}}\) and \(\mathbf{\mu}_0\)

Under the assumption of normality,

\[
F = \frac{n-p}{(n-1)p} T^2 \sim f_{(p,n-p)}
\]

and reject the null hypothesis when \(F > f_{(1-\alpha, p, n-p)}\)

\begin{itemize}
\item
  The \(T^2\) test is invariant to changes in measurement units.

  \begin{itemize}
  \tightlist
  \item
    If \(\mathbf{z = Cy + d}\) where \(\mathbf{C}\) and \(\mathbf{d}\) do not depend on \(\mathbf{y}\), then \(T^2(\mathbf{z}) - T^2(\mathbf{y})\)
  \end{itemize}
\item
  The \(T^2\) test can be derived as a \textbf{likelihood ratio} test of \(H_0: \mu = \mu_0\)
\end{itemize}

\hypertarget{confidence-intervals}{%
\subsubsection{Confidence Intervals}\label{confidence-intervals}}

\hypertarget{confidence-region}{%
\paragraph{Confidence Region}\label{confidence-region}}

An ``exact'' \(100(1-\alpha)\%\) confidence region for \(\mathbf{\mu}\) is the set of all vectors, \(\mathbf{v}\), which are ``close enough'' to the observed mean vector, \(\bar{\mathbf{y}}\) to satisfy

\[
n(\bar{\mathbf{y}} - \mathbf{\mu}_0)'\mathbf{S}^{-1}(\bar{\mathbf{y}} - \mathbf{\mu}_0) \le \frac{(n-1)p}{n-p} f_{(1-\alpha, p, n-p)}
\]

\begin{itemize}
\tightlist
\item
  \(\mathbf{v}\) are just the mean vectors that are not rejected by the \(T^2\) test when \(\mathbf{\bar{y}}\) is observed.
\end{itemize}

In case that you have 2 parameters, the confidence region is a ``hyper-ellipsoid''.

In this region, it consists of all \(\mathbf{\mu}_0\) vectors for which the \(T^2\) test would not reject \(H_0\) at significance level \(\alpha\)

Even though the confidence region better assesses the joint knowledge concerning plausible values of \(\mathbf{\mu}\) , people typically include confidence statement about the individual component means. We'd like all of the separate confidence statements to hold \textbf{simultaneously} with a specified high probability. Simultaneous confidence intervals: intervals \textbf{against} any statement being incorrect

\hypertarget{simultaneous-confidence-statements}{%
\subparagraph{Simultaneous Confidence Statements}\label{simultaneous-confidence-statements}}

\begin{itemize}
\tightlist
\item
  Intervals based on a rectangular confidence region by projecting the previous region onto the coordinate axes:
\end{itemize}

\[
\bar{y}_{i} \pm \sqrt{\frac{(n-1)p}{n-p}f_{(1-\alpha, p,n-p)}\frac{s_{ii}}{n}}
\]

for all \(i = 1,..,p\)

which implied confidence region is conservative; it has at least \(100(1- \alpha)\%\)

Generally, simultaneous \(100(1-\alpha) \%\) confidence intervals for all linear combinations , \(\mathbf{a}\) of the elements of the mean vector are given by

\[
\mathbf{a'\bar{y}} \pm \sqrt{\frac{(n-1)p}{n-p}f_{(1-\alpha, p,n-p)}\frac{\mathbf{a'Sa}}{n}}
\]

\begin{itemize}
\item
  works for any arbitrary linear combination \(\mathbf{a'\mu} = a_1 \mu_1 + ... + a_p \mu_p\), which is a projection onto the axis in the direction of \(\mathbf{a}\)
\item
  These intervals have the property that the probability that at least one such interval does not contain the appropriate \(\mathbf{a' \mu}\) is no more than \(\alpha\)
\item
  These types of intervals can be used for ``data snooping'' (like \[Scheffe\])
\end{itemize}

\hypertarget{one-mu-at-a-time}{%
\subparagraph{\texorpdfstring{One \(\mu\) at a time}{One \textbackslash mu at a time}}\label{one-mu-at-a-time}}

\begin{itemize}
\tightlist
\item
  One at a time confidence intervals:
\end{itemize}

\[
\bar{y}_i \pm t_{(1 - \alpha/2, n-1} \sqrt{\frac{s_{ii}}{n}}
\]

\begin{itemize}
\item
  Each of these intervals has a probability of \(1-\alpha\) of covering the appropriate \(\mu_i\)
\item
  But they ignore the covariance structure of the \(p\) variables
\item
  If we only care about \(k\) simultaneous intervals, we can use ``one at a time'' method with the \[Bonferroni\] correction.
\item
  This method gets more conservative as the number of intervals \(k\) increases.
\end{itemize}

\hypertarget{general-hypothesis-testing}{%
\subsection{General Hypothesis Testing}\label{general-hypothesis-testing}}

\hypertarget{one-sample-tests}{%
\subsubsection{One-sample Tests}\label{one-sample-tests}}

\[
H_0: \mathbf{C \mu= 0} 
\]

where

\begin{itemize}
\tightlist
\item
  \(\mathbf{C}\) is a \(c \times p\) matrix of rank c where \(c \le p\)
\end{itemize}

We can test this hypothesis using the following statistic

\[
F = \frac{n - c}{(n-1)c} T^2
\]

where \(T^2 = n(\mathbf{C\bar{y}})' (\mathbf{CSC'})^{-1} (\mathbf{C\bar{y}})\)

Example:

\[
H_0: \mu_1 = \mu_2 = ... = \mu_p
\]

Equivalently,

\[
\begin{aligned}
\mu_1 - \mu_2 &= 0 \\
&\vdots \\
\mu_{p-1} - \mu_p &= 0
\end{aligned}
\]

a total of \(p-1\) tests. Hence, we have \(\mathbf{C}\) as the \(p - 1 \times p\) matrix

\[
\mathbf{C} = 
\left(
\begin{array}
{ccccc}
1 & -1 & 0 & \ldots & 0 \\
0 & 1 & -1 & \ldots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & \ldots & 1 & -1 
\end{array}
\right)
\]

number of rows = \(c = p -1\)

Equivalently, we can also compare all of the other means to the first mean. Then, we test \(\mu_1 - \mu_2 = 0, \mu_1 - \mu_3 = 0,..., \mu_1 - \mu_p = 0\), the \((p-1) \times p\) matrix \(\mathbf{C}\) is

\[
\mathbf{C} = 
\left(
\begin{array}
{ccccc}
-1 & 1 & 0 & \ldots & 0 \\
-1 & 0 & 1 & \ldots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
-1 & 0 & \ldots & 0 & 1 
\end{array}
\right)
\]

The value of \(T^2\) is invariant to these equivalent choices of \(\mathbf{C}\)

This is often used for \textbf{repeated measures designs}, where each subject receives each treatment once over successive periods of time (all treatments are administered to each unit).

Example:

Let \(y_{ij}\) be the response from subject i at time j for \(i = 1,..,n, j = 1,...,T\). In this case, \(\mathbf{y}_i = (y_{i1}, ..., y_{iT})', i = 1,...,n\) are a random sample from \(N_T (\mathbf{\mu}, \mathbf{\Sigma})\)

Let \(n=8\) subjects, \(T = 6\). We are interested in \(\mu_1, .., \mu_6\)

\[
H_0: \mu_1 = \mu_2 = ... = \mu_6
\]

Equivalently,

\[
\begin{aligned}
\mu_1 - \mu_2 &= 0 \\
\mu_2 - \mu_3 &= 0 \\
&... \\
\mu_5  - \mu_6 &= 0
\end{aligned}
\]

We can test orthogonal polynomials for 4 equally spaced time points. To test for example the null hypothesis that quadratic and cubic effects are jointly equal to 0, we would define \(\mathbf{C}\)

\[
\mathbf{C} = 
\left(
\begin{array}
{cccc}
1 & -1 & -1 & 1 \\
-1 & 3 & -3 & 1
\end{array}
\right)
\]

\hypertarget{two-sample-tests}{%
\subsubsection{Two-Sample Tests}\label{two-sample-tests}}

Consider the analogous two sample multivariate tests.

Example: we have data on two independent random samples, one sample from each of two populations

\[
\begin{aligned}
\mathbf{y}_{1i} &\sim N_p (\mathbf{\mu_1, \Sigma}) \\
\mathbf{y}_{2j} &\sim N_p (\mathbf{\mu_2, \Sigma})
\end{aligned}
\]

We \textbf{assume}

\begin{itemize}
\item
  normality
\item
  equal variance-covariance matrices
\item
  independent random samples
\end{itemize}

We can summarize our data using the \textbf{sufficient statistics} \(\mathbf{\bar{y}}_1, \mathbf{S}_1, \mathbf{\bar{y}}_2, \mathbf{S}_2\) with respective sample sizes, \(n_1,n_2\)

Since we assume that \(\mathbf{\Sigma}_1 = \mathbf{\Sigma}_2 = \mathbf{\Sigma}\), compute a pooled estimate of the variance-covariance matrix on \(n_1 + n_2 - 2\) df

\[
\mathbf{S} = \frac{(n_1 - 1)\mathbf{S}_1 + (n_2-1) \mathbf{S}_2}{(n_1 -1) + (n_2 - 1)}
\]

\[
\begin{aligned}
&H_0: \mathbf{\mu}_1 = \mathbf{\mu}_2 \\
&H_a: \mathbf{\mu}_1 \neq \mathbf{\mu}_2
\end{aligned}
\]

At least one element of the mean vectors is different

We use

\begin{itemize}
\item
  \(\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2\) to estimate \(\mu_1 - \mu_2\)
\item
  \(\mathbf{S}\) to estimate \(\mathbf{\Sigma}\)

  Note: because we assume the two populations are independent, there is no covariance

  \(cov(\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2) = var(\mathbf{\bar{y}}_1) + var(\mathbf{\bar{y}}_2) = \frac{\mathbf{\Sigma_1}}{n_1} + \frac{\mathbf{\Sigma_2}}{n_2} = \mathbf{\Sigma}(\frac{1}{n_1} + \frac{1}{n_2})\)
\end{itemize}

Reject \(H_0\) if

\[
\begin{aligned}
T^2 &= (\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2)'\{ \mathbf{S} (\frac{1}{n_1} + \frac{1}{n_2})\}^{-1} (\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2)\\
&= \frac{n_1 n_2}{n_1 +n_2} (\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2)'\{ \mathbf{S} \}^{-1} (\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2)\\
& \ge \frac{(n_1 + n_2 -2)p}{n_1 + n_2 - p - 1} f_{(1- \alpha,n_1 + n_2 - p -1)}
\end{aligned}
\]

or equivalently, if

\[
F = \frac{n_1 + n_2 - p -1}{(n_1 + n_2 -2)p} T^2 \ge f_{(1- \alpha, p , n_1 + n_2 -p -1)}
\]

A \(100(1-\alpha) \%\) confidence region for \(\mu_1 - \mu_2\) consists of all vector \(\delta\) which satisfy

\[
\frac{n_1 n_2}{n_1 + n_2} (\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2 - \mathbf{\delta})' \mathbf{S}^{-1}(\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2 - \mathbf{\delta}) \le \frac{(n_1 + n_2 - 2)p}{n_1 + n_2 -p - 1}f_{(1-\alpha, p , n_1 + n_2 - p -1)}
\]

The simultaneous confidence intervals for all linear combinations of \(\mu_1 - \mu_2\) have the form

\[
\mathbf{a'}(\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2) \pm \sqrt{\frac{(n_1 + n_2 -2)p}{n_1 + n_2 - p -1}}f_{(1-\alpha, p, n_1 + n_2 -p -1)} \times \sqrt{\mathbf{a'Sa}(\frac{1}{n_1} + \frac{1}{n_2})}
\]

Bonferroni intervals, for k combinations

\[
(\bar{y}_{1i} - \bar{y}_{2i}) \pm t_{(1-\alpha/2k, n_1 + n_2 - 2)}\sqrt{(\frac{1}{n_1}  + \frac{1}{n_2})s_{ii}}
\]

\hypertarget{model-assumptions}{%
\subsubsection{Model Assumptions}\label{model-assumptions}}

If model assumption are not met

\begin{itemize}
\item
  Unequal Covariance Matrices

  \begin{itemize}
  \item
    If \(n_1 = n_2\) (large samples) there is little effect on the Type I error rate and power fo the two sample test
  \item
    If \(n_1 > n_2\) and the eigenvalues of \(\mathbf{\Sigma}_1 \mathbf{\Sigma}^{-1}_2\) are less than 1, the Type I error level is inflated
  \item
    If \(n_1 > n_2\) and some eigenvalues of \(\mathbf{\Sigma}_1 \mathbf{\Sigma}_2^{-1}\) are greater than 1, the Type I error rate is too small, leading to a reduction in power
  \end{itemize}
\item
  Sample Not Normal

  \begin{itemize}
  \item
    Type I error level of the two sample \(T^2\) test isn't much affect by moderate departures from normality if the two populations being sampled have similar distributions
  \item
    One sample \(T^2\) test is much more sensitive to lack of normality, especially when the distribution is skewed.
  \item
    Intuitively, you can think that in one sample your distribution will be sensitive, but the distribution of the difference between two similar distributions will not be as sensitive.
  \item
    Solutions:

    \begin{itemize}
    \item
      Transform to make the data more normal
    \item
      Large large samples, use the \(\chi^2\) (Wald) test, in which populations don't need to be normal, or equal sample sizes, or equal variance-covariance matrices

      \begin{itemize}
      \tightlist
      \item
        \(H_0: \mu_1 - \mu_2 =0\) use \((\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2)'( \frac{1}{n_1} \mathbf{S}_1 + \frac{1}{n_2}\mathbf{S}_2)^{-1}(\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2) \dot{\sim} \chi^2_{(p)}\)
      \end{itemize}
    \end{itemize}
  \end{itemize}
\end{itemize}

\hypertarget{equal-covariance-matrices-tests}{%
\paragraph{Equal Covariance Matrices Tests}\label{equal-covariance-matrices-tests}}

With independent random samples from k populations of \(p\)-dimensional vectors. We compute the sample covariance matrix for each, \(\mathbf{S}_i\), where \(i = 1,...,k\)

\[
\begin{aligned}
&H_0: \mathbf{\Sigma}_1 = \mathbf{\Sigma}_2 = \ldots = \mathbf{\Sigma}_k = \mathbf{\Sigma} \\
&H_a: \text{at least 2 are different}
\end{aligned}
\]

Assume \(H_0\) is true, we would use a pooled estimate of the common covariance matrix, \(\mathbf{\Sigma}\)

\[
\mathbf{S} = \frac{\sum_{i=1}^k (n_i -1)\mathbf{S}_i}{\sum_{i=1}^k (n_i - 1)}
\]

with \(\sum_{i=1}^k (n_i -1)\)

\hypertarget{bartletts-test-1}{%
\subparagraph{Bartlett's Test}\label{bartletts-test-1}}

(a modification of the likelihood ratio test). Define

\[
N = \sum_{i=1}^k n_i
\]

and (note: \(| |\) are determinants here, not absolute value)

\[
M = (N - k) \log|\mathbf{S}| - \sum_{i=1}^k (n_i - 1)  \log|\mathbf{S}_i|
\]

\[
C^{-1} = 1 - \frac{2p^2 + 3p - 1}{6(p+1)(k-1)} \{\sum_{i=1}^k (\frac{1}{n_i - 1}) - \frac{1}{N-k} \}
\]

\begin{itemize}
\item
  Reject \(H_0\) when \(MC^{-1} > \chi^2_{1- \alpha, (k-1)p(p+1)/2}\)
\item
  If not all samples are from normal populations, \(MC^{-1}\) has a distribution which is often shifted to the right of the nominal \(\chi^2\) distribution, which means \(H_0\) is often rejected even when it is true (the Type I error level is inflated). Hence, it is better to test individual normality first, or then multivariate normality before you do Bartlett's test.
\end{itemize}

\hypertarget{two-sample-repeated-measurements}{%
\subsubsection{Two-Sample Repeated Measurements}\label{two-sample-repeated-measurements}}

\begin{itemize}
\item
  Define \(\mathbf{y}_{hi} = (y_{hi1}, ..., y_{hit})'\) to be the observations from the i-th subject in the h-th group for times 1 through T
\item
  Assume that \(\mathbf{y}_{11}, ..., \mathbf{y}_{1n_1}\) are iid \(N_t(\mathbf{\mu}_1, \mathbf{\Sigma})\) and that \(\mathbf{y}_{21},...,\mathbf{y}_{2n_2}\) are iid \(N_t(\mathbf{\mu}_2, \mathbf{\Sigma})\)
\item
  \(H_0: \mathbf{C}(\mathbf{\mu}_1 - \mathbf{\mu}_2) = \mathbf{0}_c\) where \(\mathbf{C}\) is a \(c \times t\) matrix of rank \(c\) where \(c \le t\)
\item
  The test statistic has the form
\end{itemize}

\[
T^2 = \frac{n_1 n_2}{n_1 + n_2} (\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2)' \mathbf{C}'(\mathbf{CSC}')^{-1}\mathbf{C} (\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2)
\]

where \(\mathbf{S}\) is the pooled covariance estimate. Then,

\[
F = \frac{n_1 + n_2 - c -1}{(n_1 + n_2-2)c} T^2 \sim f_{(c, n_1 + n_2 - c-1)}
\]

when \(H_0\) is true

If the null hypothesis~\(H_0: \mu_1 = \mu_2\) is rejected. A weaker hypothesis is that the profiles for the two groups are parallel.

\[
\begin{aligned}
\mu_{11} - \mu_{21} &= \mu_{12} - \mu_{22} \\
&\vdots \\
\mu_{1t-1} - \mu_{2t-1} &= \mu_{1t} - \mu_{2t}
\end{aligned}
\]

The null hypothesis matrix term is then

\(H_0: \mathbf{C}(\mu_1 - \mu_2) = \mathbf{0}_c\) , where \(c = t - 1\) and

\[
\mathbf{C} = 
\left(
\begin{array}
{ccccc}
1 & -1 & 0 & \ldots & 0 \\
0 & 1 & -1 & \ldots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \ldots & -1 
\end{array}
\right)_{(t-1) \times t}
\]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# One{-}sample Hotelling\textquotesingle{}s T\^{}2 test}
\CommentTok{\#  Create data frame}
\NormalTok{plants }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
    \AttributeTok{y1 =} \FunctionTok{c}\NormalTok{(}\FloatTok{2.11}\NormalTok{, }\FloatTok{2.36}\NormalTok{, }\FloatTok{2.13}\NormalTok{, }\FloatTok{2.78}\NormalTok{, }\FloatTok{2.17}\NormalTok{),}
    \AttributeTok{y2 =} \FunctionTok{c}\NormalTok{(}\FloatTok{10.1}\NormalTok{, }\FloatTok{35.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, }\FloatTok{6.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{),}
    \AttributeTok{y3 =} \FunctionTok{c}\NormalTok{(}\FloatTok{3.4}\NormalTok{, }\FloatTok{4.1}\NormalTok{, }\FloatTok{1.9}\NormalTok{, }\FloatTok{3.8}\NormalTok{, }\FloatTok{1.7}\NormalTok{)}
\NormalTok{)}

\CommentTok{\# Center the data with }
\CommentTok{\# the hypothesized means and make a matrix}
\NormalTok{plants\_ctr }\OtherTok{\textless{}{-}}\NormalTok{ plants }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{transmute}\NormalTok{(}\AttributeTok{y1\_ctr =}\NormalTok{ y1 }\SpecialCharTok{{-}} \FloatTok{2.85}\NormalTok{,}
              \AttributeTok{y2\_ctr =}\NormalTok{ y2 }\SpecialCharTok{{-}} \FloatTok{15.0}\NormalTok{,}
              \AttributeTok{y3\_ctr =}\NormalTok{ y3 }\SpecialCharTok{{-}} \FloatTok{6.0}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{as.matrix}\NormalTok{()}

\CommentTok{\# Use anova.mlm to calculate Wilks\textquotesingle{} lambda}
\NormalTok{onesamp\_fit }\OtherTok{\textless{}{-}} \FunctionTok{anova}\NormalTok{(}\FunctionTok{lm}\NormalTok{(plants\_ctr }\SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{), }\AttributeTok{test =} \StringTok{"Wilks"}\NormalTok{)}
\NormalTok{onesamp\_fit}
\CommentTok{\#\textgreater{} Analysis of Variance Table}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}             Df    Wilks approx F num Df den Df  Pr(\textgreater{}F)  }
\CommentTok{\#\textgreater{} (Intercept)  1 0.054219   11.629      3      2 0.08022 .}
\CommentTok{\#\textgreater{} Residuals    4                                          }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

can't reject the null of hypothesized vector of means

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Paired{-}Sample Hotelling\textquotesingle{}s T\^{}2 test}
\FunctionTok{library}\NormalTok{(ICSNP)}

\CommentTok{\#  Create data frame}
\NormalTok{waste }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
    \AttributeTok{case =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{11}\NormalTok{,}
    \AttributeTok{com\_y1 =} \FunctionTok{c}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{18}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{11}\NormalTok{, }\DecValTok{34}\NormalTok{, }\DecValTok{28}\NormalTok{, }\DecValTok{71}\NormalTok{, }\DecValTok{43}\NormalTok{, }\DecValTok{33}\NormalTok{, }\DecValTok{20}\NormalTok{),}
    \AttributeTok{com\_y2 =} \FunctionTok{c}\NormalTok{(}\DecValTok{27}\NormalTok{, }\DecValTok{23}\NormalTok{, }\DecValTok{64}\NormalTok{, }\DecValTok{44}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{75}\NormalTok{, }\DecValTok{26}\NormalTok{, }\DecValTok{124}\NormalTok{, }\DecValTok{54}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{14}\NormalTok{),}
    \AttributeTok{state\_y1 =} \FunctionTok{c}\NormalTok{(}\DecValTok{25}\NormalTok{, }\DecValTok{28}\NormalTok{, }\DecValTok{36}\NormalTok{, }\DecValTok{35}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{44}\NormalTok{, }\DecValTok{42}\NormalTok{, }\DecValTok{54}\NormalTok{, }\DecValTok{34}\NormalTok{, }\DecValTok{29}\NormalTok{, }\DecValTok{39}\NormalTok{),}
    \AttributeTok{state\_y2 =} \FunctionTok{c}\NormalTok{(}\DecValTok{15}\NormalTok{, }\DecValTok{13}\NormalTok{, }\DecValTok{22}\NormalTok{, }\DecValTok{29}\NormalTok{, }\DecValTok{31}\NormalTok{, }\DecValTok{64}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{64}\NormalTok{, }\DecValTok{56}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{21}\NormalTok{)}
\NormalTok{)}

\CommentTok{\# Calculate the difference between commercial and state labs}
\NormalTok{waste\_diff }\OtherTok{\textless{}{-}}\NormalTok{ waste }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{transmute}\NormalTok{(}\AttributeTok{y1\_diff =}\NormalTok{ com\_y1 }\SpecialCharTok{{-}}\NormalTok{ state\_y1,}
              \AttributeTok{y2\_diff =}\NormalTok{ com\_y2 }\SpecialCharTok{{-}}\NormalTok{ state\_y2)}
\CommentTok{\# Run the test}
\NormalTok{paired\_fit }\OtherTok{\textless{}{-}} \FunctionTok{HotellingsT2}\NormalTok{(waste\_diff)}
\CommentTok{\# value T.2 in the output corresponds to }
\CommentTok{\# the approximate F{-}value in the output from anova.mlm}
\NormalTok{paired\_fit }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Hotelling\textquotesingle{}s one sample T2{-}test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  waste\_diff}
\CommentTok{\#\textgreater{} T.2 = 6.1377, df1 = 2, df2 = 9, p{-}value = 0.02083}
\CommentTok{\#\textgreater{} alternative hypothesis: true location is not equal to c(0,0)}
\end{Highlighting}
\end{Shaded}

reject the null that the two labs' measurements are equal

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Independent{-}Sample Hotelling\textquotesingle{}s T\^{}2 test with Bartlett\textquotesingle{}s test}

\CommentTok{\# Read in data}
\NormalTok{steel }\OtherTok{\textless{}{-}} \FunctionTok{read.table}\NormalTok{(}\StringTok{"images/steel.dat"}\NormalTok{)}
\FunctionTok{names}\NormalTok{(steel) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Temp"}\NormalTok{, }\StringTok{"Yield"}\NormalTok{, }\StringTok{"Strength"}\NormalTok{)}
\FunctionTok{str}\NormalTok{(steel)}
\CommentTok{\#\textgreater{} \textquotesingle{}data.frame\textquotesingle{}:    12 obs. of  3 variables:}
\CommentTok{\#\textgreater{}  $ Temp    : int  1 1 1 1 1 2 2 2 2 2 ...}
\CommentTok{\#\textgreater{}  $ Yield   : int  33 36 35 38 40 35 36 38 39 41 ...}
\CommentTok{\#\textgreater{}  $ Strength: int  60 61 64 63 65 57 59 59 61 63 ...}

\CommentTok{\# Plot the data}
\FunctionTok{ggplot}\NormalTok{(steel, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Yield, }\AttributeTok{y =}\NormalTok{ Strength)) }\SpecialCharTok{+}
    \FunctionTok{geom\_text}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =}\NormalTok{ Temp), }\AttributeTok{size =} \DecValTok{5}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_segment}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}
        \AttributeTok{x =} \DecValTok{33}\NormalTok{,}
        \AttributeTok{y =} \FloatTok{57.5}\NormalTok{,}
        \AttributeTok{xend =} \DecValTok{42}\NormalTok{,}
        \AttributeTok{yend =} \DecValTok{65}
\NormalTok{    ), }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{22-multivariate_files/figure-latex/unnamed-chunk-4-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]


\CommentTok{\# Bartlett\textquotesingle{}s test for equality of covariance matrices}
\CommentTok{\# same thing as Box\textquotesingle{}s M test in the multivariate setting}
\NormalTok{bart\_test }\OtherTok{\textless{}{-}} \FunctionTok{boxM}\NormalTok{(steel[, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{], steel}\SpecialCharTok{$}\NormalTok{Temp)}
\NormalTok{bart\_test }\CommentTok{\# fail to reject the null of equal covariances }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Box\textquotesingle{}s M{-}test for Homogeneity of Covariance Matrices}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  steel[, {-}1]}
\CommentTok{\#\textgreater{} Chi{-}Sq (approx.) = 0.38077, df = 3, p{-}value = 0.9442}

\CommentTok{\# anova.mlm}
\NormalTok{twosamp\_fit }\OtherTok{\textless{}{-}}
    \FunctionTok{anova}\NormalTok{(}\FunctionTok{lm}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(Yield, Strength) }\SpecialCharTok{\textasciitilde{}} \FunctionTok{factor}\NormalTok{(Temp), }
             \AttributeTok{data =}\NormalTok{ steel), }
          \AttributeTok{test =} \StringTok{"Wilks"}\NormalTok{)}
\NormalTok{twosamp\_fit}
\CommentTok{\#\textgreater{} Analysis of Variance Table}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}              Df    Wilks approx F num Df den Df    Pr(\textgreater{}F)    }
\CommentTok{\#\textgreater{} (Intercept)   1 0.001177   3818.1      2      9 6.589e{-}14 ***}
\CommentTok{\#\textgreater{} factor(Temp)  1 0.294883     10.8      2      9  0.004106 ** }
\CommentTok{\#\textgreater{} Residuals    10                                              }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}

\CommentTok{\# ICSNP package}
\NormalTok{twosamp\_fit2 }\OtherTok{\textless{}{-}}
    \FunctionTok{HotellingsT2}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(steel}\SpecialCharTok{$}\NormalTok{Yield, steel}\SpecialCharTok{$}\NormalTok{Strength) }\SpecialCharTok{\textasciitilde{}} 
                     \FunctionTok{factor}\NormalTok{(steel}\SpecialCharTok{$}\NormalTok{Temp))}
\NormalTok{twosamp\_fit2}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Hotelling\textquotesingle{}s two sample T2{-}test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  cbind(steel$Yield, steel$Strength) by factor(steel$Temp)}
\CommentTok{\#\textgreater{} T.2 = 10.76, df1 = 2, df2 = 9, p{-}value = 0.004106}
\CommentTok{\#\textgreater{} alternative hypothesis: true location difference is not equal to c(0,0)}
\end{Highlighting}
\end{Shaded}

reject null. Hence, there is a difference in the means of the bivariate normal distributions

\hypertarget{manova}{%
\section{MANOVA}\label{manova}}

Multivariate Analysis of Variance

One-way MANOVA

Compare treatment means for h different populations

Population 1: \(\mathbf{y}_{11}, \mathbf{y}_{12}, \dots, \mathbf{y}_{1n_1} \sim idd N_p (\mathbf{\mu}_1, \mathbf{\Sigma})\)

\(\vdots\)

Population h: \(\mathbf{y}_{h1}, \mathbf{y}_{h2}, \dots, \mathbf{y}_{hn_h} \sim idd N_p (\mathbf{\mu}_h, \mathbf{\Sigma})\)

\textbf{Assumptions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Independent random samples from \(h\) different populations
\item
  Common covariance matrices
\item
  Each population is multivariate \textbf{normal}
\end{enumerate}

Calculate the summary statistics \(\mathbf{\bar{y}}_i, \mathbf{S}\) and the pooled estimate of the covariance matrix \(\mathbf{S}\)

Similar to the univariate one-way ANVOA, we can use the effects model formulation \(\mathbf{\mu}_i = \mathbf{\mu} + \mathbf{\tau}_i\), where

\begin{itemize}
\item
  \(\mathbf{\mu}_i\) is the population mean for population i
\item
  \(\mathbf{\mu}\) is the overall mean effect
\item
  \(\mathbf{\tau}_i\) is the treatment effect of the i-th treatment.
\end{itemize}

For the one-way model: \(\mathbf{y}_{ij} = \mu + \tau_i + \epsilon_{ij}\) for \(i = 1,..,h; j = 1,..., n_i\) and \(\epsilon_{ij} \sim N_p(\mathbf{0, \Sigma})\)

However, the above model is over-parameterized (i.e., infinite number of ways to define \(\mathbf{\mu}\) and the \(\mathbf{\tau}_i\)'s such that they add up to \(\mu_i\). Thus we can constrain by having

\[
\sum_{i=1}^h n_i \tau_i = 0 
\]

or

\[
\mathbf{\tau}_h = 0
\]

The observational equivalent of the effects model is

\[
\begin{aligned}
\mathbf{y}_{ij} &= \mathbf{\bar{y}} + (\mathbf{\bar{y}}_i - \mathbf{\bar{y}}) + (\mathbf{y}_{ij} - \mathbf{\bar{y}}_i) \\
&= \text{overall sample mean} + \text{treatement effect} + \text{residual} \text{ (under univariate ANOVA)}
\end{aligned} 
\]

After manipulation

\[
\sum_{i = 1}^h \sum_{j = 1}^{n_i} (\mathbf{\bar{y}}_{ij} - \mathbf{\bar{y}})(\mathbf{\bar{y}}_{ij} - \mathbf{\bar{y}})' = \sum_{i = 1}^h n_i (\mathbf{\bar{y}}_i - \mathbf{\bar{y}})(\mathbf{\bar{y}}_i - \mathbf{\bar{y}})' + \sum_{i=1}^h \sum_{j = 1}^{n_i} (\mathbf{\bar{y}}_{ij} - \mathbf{\bar{y}})(\mathbf{\bar{y}}_{ij} - \mathbf{\bar{y}}_i)'
\]

LHS = Total corrected sums of squares and cross products (SSCP) matrix

RHS =

\begin{itemize}
\item
  1st term = Treatment (or between subjects) sum of squares and cross product matrix (denoted H;B)
\item
  2nd term = residual (or within subject) SSCP matrix denoted (E;W)
\end{itemize}

Note:

\[
\mathbf{E} = (n_1 - 1)\mathbf{S}_1  + ... + (n_h -1) \mathbf{S}_h = (n-h) \mathbf{S}
\]

MANOVA table

\begin{longtable}[]{@{}lll@{}}
\caption{MONOVA table}\tabularnewline
\toprule\noalign{}
Source & SSCP & df \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
Source & SSCP & df \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Treatment & \(\mathbf{H}\) & \(h -1\) \\
Residual (error) & \(\mathbf{E}\) & \(\sum_{i= 1}^h n_i - h\) \\
Total Corrected & \(\mathbf{H + E}\) & \(\sum_{i=1}^h n_i -1\) \\
\end{longtable}

\[
H_0: \tau_1 = \tau_2 = \dots = \tau_h = \mathbf{0}
\]

We consider the relative ``sizes'' of \(\mathbf{E}\) and \(\mathbf{H+E}\)

Wilk's Lambda

Define Wilk's Lambda

\[
\Lambda^* = \frac{|\mathbf{E}|}{|\mathbf{H+E}|}
\]

Properties:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Wilk's Lambda is equivalent to the F-statistic in the univariate case
\item
  The exact distribution of \(\Lambda^*\) can be determined for especial cases.
\item
  For large sample sizes, reject \(H_0\) if
\end{enumerate}

\[
-(\sum_{i=1}^h n_i - 1 - \frac{p+h}{2}) \log(\Lambda^*) > \chi^2_{(1-\alpha, p(h-1))}
\]

\hypertarget{testing-general-hypotheses}{%
\subsection{Testing General Hypotheses}\label{testing-general-hypotheses}}

\begin{itemize}
\item
  \(h\) different treatments
\item
  with the i-th treatment
\item
  applied to \(n_i\) subjects that
\item
  are observed for \(p\) repeated measures.
\end{itemize}

Consider this a \(p\) dimensional obs on a random sample from each of \(h\) different treatment populations.

\[
\mathbf{y}_{ij} = \mathbf{\mu} + \mathbf{\tau}_i + \mathbf{\epsilon}_{ij}
\]

for \(i = 1,..,h\) and \(j = 1,..,n_i\)

Equivalently,

\[
\mathbf{Y} = \mathbf{XB} + \mathbf{\epsilon}
\]

where \(n = \sum_{i = 1}^h n_i\) and with restriction \(\mathbf{\tau}_h = 0\)

\[
\mathbf{Y}_{(n \times p)} = 
\left[
\begin{array}
{c}
\mathbf{y}_{11}' \\
\vdots \\
\mathbf{y}_{1n_1}' \\
\vdots \\
\mathbf{y}_{hn_h}'
\end{array}
\right],
\mathbf{B}_{(h \times p)} = 
\left[
\begin{array}
{c}
\mathbf{\mu}' \\
\mathbf{\tau}_1' \\
\vdots \\
\mathbf{\tau}_{h-1}'
\end{array}
\right],
\mathbf{\epsilon}_{(n \times p)} = 
\left[
\begin{array}
{c}
\epsilon_{11}' \\
\vdots \\
\epsilon_{1n_1}' \\
\vdots \\
\epsilon_{hn_h}'
\end{array}
\right]
\]

\[
\mathbf{X}_{(n \times h)} = 
\left[
\begin{array}
{ccccc}
1 & 1 & 0 & \ldots & 0 \\
\vdots & \vdots & \vdots & & \vdots \\
1 & 1 & 0 & \ldots & 0 \\
\vdots & \vdots & \vdots & \ldots & \vdots \\
1 & 0 & 0 & \ldots & 0 \\
\vdots & \vdots & \vdots & & \vdots \\
1 & 0 & 0 & \ldots & 0 
\end{array}
\right]
\]

Estimation

\[
\mathbf{\hat{B}} = (\mathbf{X'X})^{-1} \mathbf{X'Y}
\]

Rows of \(\mathbf{Y}\) are independent (i.e., \(var(\mathbf{Y}) = \mathbf{I}_n \otimes \mathbf{\Sigma}\) , an \(np \times np\) matrix, where \(\otimes\) is the Kronecker product).

\[
\begin{aligned}
&H_0: \mathbf{LBM} = 0 \\
&H_a: \mathbf{LBM} \neq 0
\end{aligned}
\]

where

\begin{itemize}
\item
  \(\mathbf{L}\) is a \(g \times h\) matrix of full row rank (\(g \le h\)) = comparisons across groups
\item
  \(\mathbf{M}\) is a \(p \times u\) matrix of full column rank (\(u \le p\)) = comparisons across traits
\end{itemize}

The general treatment corrected sums of squares and cross product is

\[
\mathbf{H} = \mathbf{M'Y'X(X'X)^{-1}L'[L(X'X)^{-1}L']^{-1}L(X'X)^{-1}X'YM}
\]

or for the null hypothesis \(H_0: \mathbf{LBM} = \mathbf{D}\)

\[
\mathbf{H} = (\mathbf{\hat{LBM}} - \mathbf{D})'[\mathbf{X(X'X)^{-1}L}]^{-1}(\mathbf{\hat{LBM}} - \mathbf{D})
\]

The general matrix of residual sums of squares and cross product

\[
\mathbf{E} = \mathbf{M'Y'[I-X(X'X)^{-1}X']YM} = \mathbf{M'[Y'Y - \hat{B}'(X'X)^{-1}\hat{B}]M}
\]

We can compute the following statistic eigenvalues of \(\mathbf{HE}^{-1}\)

\begin{itemize}
\item
  Wilk's Criterion: \(\Lambda^* = \frac{|\mathbf{E}|}{|\mathbf{H} + \mathbf{E}|}\) . The df depend on the rank of \(\mathbf{L}, \mathbf{M}, \mathbf{X}\)
\item
  Lawley-Hotelling Trace: \(U = tr(\mathbf{HE}^{-1})\)
\item
  Pillai Trace: \(V = tr(\mathbf{H}(\mathbf{H}+ \mathbf{E}^{-1})\)
\item
  Roy's Maximum Root: largest eigenvalue of \(\mathbf{HE}^{-1}\)
\end{itemize}

If \(H_0\) is true and n is large, \(-(n-1- \frac{p+h}{2})\ln \Lambda^* \sim \chi^2_{p(h-1)}\). Some special values of p and h can give exact F-dist under \(H_0\)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# One{-}way MANOVA}

\FunctionTok{library}\NormalTok{(car)}
\FunctionTok{library}\NormalTok{(emmeans)}
\FunctionTok{library}\NormalTok{(profileR)}
\FunctionTok{library}\NormalTok{(tidyverse)}

\DocumentationTok{\#\# Read in the data}
\NormalTok{gpagmat }\OtherTok{\textless{}{-}} \FunctionTok{read.table}\NormalTok{(}\StringTok{"images/gpagmat.dat"}\NormalTok{)}

\DocumentationTok{\#\# Change the variable names}
\FunctionTok{names}\NormalTok{(gpagmat) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"y1"}\NormalTok{, }\StringTok{"y2"}\NormalTok{, }\StringTok{"admit"}\NormalTok{)}

\DocumentationTok{\#\# Check the structure}
\FunctionTok{str}\NormalTok{(gpagmat)}
\CommentTok{\#\textgreater{} \textquotesingle{}data.frame\textquotesingle{}:    85 obs. of  3 variables:}
\CommentTok{\#\textgreater{}  $ y1   : num  2.96 3.14 3.22 3.29 3.69 3.46 3.03 3.19 3.63 3.59 ...}
\CommentTok{\#\textgreater{}  $ y2   : int  596 473 482 527 505 693 626 663 447 588 ...}
\CommentTok{\#\textgreater{}  $ admit: int  1 1 1 1 1 1 1 1 1 1 ...}


\DocumentationTok{\#\# Plot the data}
\NormalTok{gg }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(gpagmat, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ y1, }\AttributeTok{y =}\NormalTok{ y2)) }\SpecialCharTok{+}
    \FunctionTok{geom\_text}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =}\NormalTok{ admit, }\AttributeTok{col =} \FunctionTok{as.character}\NormalTok{(admit))) }\SpecialCharTok{+}
    \FunctionTok{scale\_color\_discrete}\NormalTok{(}\AttributeTok{name =} \StringTok{"Admission"}\NormalTok{,}
                         \AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Admit"}\NormalTok{, }\StringTok{"Do not admit"}\NormalTok{, }\StringTok{"Borderline"}\NormalTok{)) }\SpecialCharTok{+}
    \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{name =} \StringTok{"GPA"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{name =} \StringTok{"GMAT"}\NormalTok{)}

\DocumentationTok{\#\# Fit one{-}way MANOVA}
\NormalTok{oneway\_fit }\OtherTok{\textless{}{-}} \FunctionTok{manova}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(y1, y2) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ admit, }\AttributeTok{data =}\NormalTok{ gpagmat)}
\FunctionTok{summary}\NormalTok{(oneway\_fit, }\AttributeTok{test =} \StringTok{"Wilks"}\NormalTok{)}
\CommentTok{\#\textgreater{}           Df  Wilks approx F num Df den Df    Pr(\textgreater{}F)    }
\CommentTok{\#\textgreater{} admit      1 0.6126   25.927      2     82 1.881e{-}09 ***}
\CommentTok{\#\textgreater{} Residuals 83                                            }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

reject the null of equal multivariate mean vectors between the three admmission groups

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Repeated Measures MANOVA}


\DocumentationTok{\#\# Create data frame}
\NormalTok{stress }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
    \AttributeTok{subject =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{8}\NormalTok{,}
    \AttributeTok{begin =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{),}
    \AttributeTok{middle =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{),}
    \AttributeTok{final =} \FunctionTok{c}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  If independent = time with 3 levels -\textgreater{} univariate ANOVA (require sphericity assumption (i.e., the variances for all differences are equal))
\item
  If each level of independent time as a separate variable -\textgreater{} MANOVA (does not require sphericity assumption)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# MANOVA}
\NormalTok{stress\_mod }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(begin, middle, final) }\SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }\AttributeTok{data =}\NormalTok{ stress)}
\NormalTok{idata }\OtherTok{\textless{}{-}}
    \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{time =} \FunctionTok{factor}\NormalTok{(}
        \FunctionTok{c}\NormalTok{(}\StringTok{"begin"}\NormalTok{, }\StringTok{"middle"}\NormalTok{, }\StringTok{"final"}\NormalTok{),}
        \AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{"begin"}\NormalTok{, }\StringTok{"middle"}\NormalTok{, }\StringTok{"final"}\NormalTok{)}
\NormalTok{    ))}
\NormalTok{repeat\_fit }\OtherTok{\textless{}{-}}
    \FunctionTok{Anova}\NormalTok{(}
\NormalTok{        stress\_mod,}
        \AttributeTok{idata =}\NormalTok{ idata,}
        \AttributeTok{idesign =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ time,}
        \AttributeTok{icontrasts =} \StringTok{"contr.poly"}
\NormalTok{    )}
\FunctionTok{summary}\NormalTok{(repeat\_fit) }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Type III Repeated Measures MANOVA Tests:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}  }
\CommentTok{\#\textgreater{} Term: (Intercept) }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Response transformation matrix:}
\CommentTok{\#\textgreater{}        (Intercept)}
\CommentTok{\#\textgreater{} begin            1}
\CommentTok{\#\textgreater{} middle           1}
\CommentTok{\#\textgreater{} final            1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Sum of squares and products for the hypothesis:}
\CommentTok{\#\textgreater{}             (Intercept)}
\CommentTok{\#\textgreater{} (Intercept)        1352}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Multivariate Tests: (Intercept)}
\CommentTok{\#\textgreater{}                  Df test stat approx F num Df den Df     Pr(\textgreater{}F)    }
\CommentTok{\#\textgreater{} Pillai            1  0.896552 60.66667      1      7 0.00010808 ***}
\CommentTok{\#\textgreater{} Wilks             1  0.103448 60.66667      1      7 0.00010808 ***}
\CommentTok{\#\textgreater{} Hotelling{-}Lawley  1  8.666667 60.66667      1      7 0.00010808 ***}
\CommentTok{\#\textgreater{} Roy               1  8.666667 60.66667      1      7 0.00010808 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}  }
\CommentTok{\#\textgreater{} Term: time }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Response transformation matrix:}
\CommentTok{\#\textgreater{}               time.L     time.Q}
\CommentTok{\#\textgreater{} begin  {-}7.071068e{-}01  0.4082483}
\CommentTok{\#\textgreater{} middle {-}7.850462e{-}17 {-}0.8164966}
\CommentTok{\#\textgreater{} final   7.071068e{-}01  0.4082483}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Sum of squares and products for the hypothesis:}
\CommentTok{\#\textgreater{}           time.L   time.Q}
\CommentTok{\#\textgreater{} time.L 18.062500 6.747781}
\CommentTok{\#\textgreater{} time.Q  6.747781 2.520833}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Multivariate Tests: time}
\CommentTok{\#\textgreater{}                  Df test stat approx F num Df den Df   Pr(\textgreater{}F)  }
\CommentTok{\#\textgreater{} Pillai            1 0.7080717 7.276498      2      6 0.024879 *}
\CommentTok{\#\textgreater{} Wilks             1 0.2919283 7.276498      2      6 0.024879 *}
\CommentTok{\#\textgreater{} Hotelling{-}Lawley  1 2.4254992 7.276498      2      6 0.024879 *}
\CommentTok{\#\textgreater{} Roy               1 2.4254992 7.276498      2      6 0.024879 *}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Univariate Type III Repeated{-}Measures ANOVA Assuming Sphericity}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}             Sum Sq num Df Error SS den Df F value    Pr(\textgreater{}F)    }
\CommentTok{\#\textgreater{} (Intercept) 450.67      1    52.00      7 60.6667 0.0001081 ***}
\CommentTok{\#\textgreater{} time         20.58      2    24.75     14  5.8215 0.0144578 *  }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Mauchly Tests for Sphericity}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}      Test statistic p{-}value}
\CommentTok{\#\textgreater{} time         0.7085 0.35565}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Greenhouse{-}Geisser and Huynh{-}Feldt Corrections}
\CommentTok{\#\textgreater{}  for Departure from Sphericity}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}       GG eps Pr(\textgreater{}F[GG])  }
\CommentTok{\#\textgreater{} time 0.77429    0.02439 *}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}         HF eps Pr(\textgreater{}F[HF])}
\CommentTok{\#\textgreater{} time 0.9528433 0.01611634}
\end{Highlighting}
\end{Shaded}

can't reject the null hypothesis of sphericity, hence univariate ANOVA is also appropriate.We also see linear significant time effect, but no quadratic time effect

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# Polynomial contrasts}
\CommentTok{\# What is the reference for the marginal means?}
\FunctionTok{ref\_grid}\NormalTok{(stress\_mod, }\AttributeTok{mult.name =} \StringTok{"time"}\NormalTok{)}
\CommentTok{\#\textgreater{} \textquotesingle{}emmGrid\textquotesingle{} object with variables:}
\CommentTok{\#\textgreater{}     1 = 1}
\CommentTok{\#\textgreater{}     time = multivariate response levels: begin, middle, final}

\CommentTok{\# marginal means for the levels of time}
\NormalTok{contr\_means }\OtherTok{\textless{}{-}} \FunctionTok{emmeans}\NormalTok{(stress\_mod, }\SpecialCharTok{\textasciitilde{}}\NormalTok{ time, }\AttributeTok{mult.name =} \StringTok{"time"}\NormalTok{)}
\FunctionTok{contrast}\NormalTok{(contr\_means, }\AttributeTok{method =} \StringTok{"poly"}\NormalTok{)}
\CommentTok{\#\textgreater{}  contrast  estimate    SE df t.ratio p.value}
\CommentTok{\#\textgreater{}  linear        2.12 0.766  7   2.773  0.0276}
\CommentTok{\#\textgreater{}  quadratic     1.38 0.944  7   1.457  0.1885}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# MANOVA}


\DocumentationTok{\#\# Read in Data}
\NormalTok{heart }\OtherTok{\textless{}{-}} \FunctionTok{read.table}\NormalTok{(}\StringTok{"images/heart.dat"}\NormalTok{)}
\FunctionTok{names}\NormalTok{(heart) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"drug"}\NormalTok{, }\StringTok{"y1"}\NormalTok{, }\StringTok{"y2"}\NormalTok{, }\StringTok{"y3"}\NormalTok{, }\StringTok{"y4"}\NormalTok{)}
\DocumentationTok{\#\# Create a subject ID nested within drug}
\NormalTok{heart }\OtherTok{\textless{}{-}}\NormalTok{ heart }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{group\_by}\NormalTok{(drug) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{subject =} \FunctionTok{row\_number}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{ungroup}\NormalTok{()}
\FunctionTok{str}\NormalTok{(heart)}
\CommentTok{\#\textgreater{} tibble [24 x 6] (S3: tbl\_df/tbl/data.frame)}
\CommentTok{\#\textgreater{}  $ drug   : chr [1:24] "ax23" "ax23" "ax23" "ax23" ...}
\CommentTok{\#\textgreater{}  $ y1     : int [1:24] 72 78 71 72 66 74 62 69 85 82 ...}
\CommentTok{\#\textgreater{}  $ y2     : int [1:24] 86 83 82 83 79 83 73 75 86 86 ...}
\CommentTok{\#\textgreater{}  $ y3     : int [1:24] 81 88 81 83 77 84 78 76 83 80 ...}
\CommentTok{\#\textgreater{}  $ y4     : int [1:24] 77 82 75 69 66 77 70 70 80 84 ...}
\CommentTok{\#\textgreater{}  $ subject: int [1:24] 1 2 3 4 5 6 7 8 1 2 ...}

\DocumentationTok{\#\# Create means summary for profile plot,}
\CommentTok{\# pivot longer for plotting with ggplot}
\NormalTok{heart\_means }\OtherTok{\textless{}{-}}\NormalTok{ heart }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{group\_by}\NormalTok{(drug) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{summarize\_at}\NormalTok{(}\FunctionTok{vars}\NormalTok{(}\FunctionTok{starts\_with}\NormalTok{(}\StringTok{"y"}\NormalTok{)), mean) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{pivot\_longer}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{drug, }\AttributeTok{names\_to =} \StringTok{"time"}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{"mean"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{time =} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{as.factor}\NormalTok{(time)))}
\NormalTok{gg\_profile }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(heart\_means, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ time, }\AttributeTok{y =}\NormalTok{ mean)) }\SpecialCharTok{+}
    \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{col =}\NormalTok{ drug)) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{col =}\NormalTok{ drug)) }\SpecialCharTok{+}
    \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Profile Plot"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{name =} \StringTok{"Response"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{scale\_x\_discrete}\NormalTok{(}\AttributeTok{name =} \StringTok{"Time"}\NormalTok{)}
\NormalTok{gg\_profile}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{22-multivariate_files/figure-latex/unnamed-chunk-9-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# Fit model}
\NormalTok{heart\_mod }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(y1, y2, y3, y4) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ drug, }\AttributeTok{data =}\NormalTok{ heart)}
\NormalTok{man\_fit }\OtherTok{\textless{}{-}}\NormalTok{ car}\SpecialCharTok{::}\FunctionTok{Anova}\NormalTok{(heart\_mod)}
\FunctionTok{summary}\NormalTok{(man\_fit)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Type II MANOVA Tests:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Sum of squares and products for error:}
\CommentTok{\#\textgreater{}        y1      y2      y3     y4}
\CommentTok{\#\textgreater{} y1 641.00 601.750 535.250 426.00}
\CommentTok{\#\textgreater{} y2 601.75 823.875 615.500 534.25}
\CommentTok{\#\textgreater{} y3 535.25 615.500 655.875 555.25}
\CommentTok{\#\textgreater{} y4 426.00 534.250 555.250 674.50}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}  }
\CommentTok{\#\textgreater{} Term: drug }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Sum of squares and products for the hypothesis:}
\CommentTok{\#\textgreater{}        y1       y2       y3    y4}
\CommentTok{\#\textgreater{} y1 567.00 335.2500  42.7500 387.0}
\CommentTok{\#\textgreater{} y2 335.25 569.0833 404.5417 367.5}
\CommentTok{\#\textgreater{} y3  42.75 404.5417 391.0833 171.0}
\CommentTok{\#\textgreater{} y4 387.00 367.5000 171.0000 316.0}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Multivariate Tests: drug}
\CommentTok{\#\textgreater{}                  Df test stat  approx F num Df den Df     Pr(\textgreater{}F)    }
\CommentTok{\#\textgreater{} Pillai            2  1.283456  8.508082      8     38 1.5010e{-}06 ***}
\CommentTok{\#\textgreater{} Wilks             2  0.079007 11.509581      8     36 6.3081e{-}08 ***}
\CommentTok{\#\textgreater{} Hotelling{-}Lawley  2  7.069384 15.022441      8     34 3.9048e{-}09 ***}
\CommentTok{\#\textgreater{} Roy               2  6.346509 30.145916      4     19 5.4493e{-}08 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

reject the null hypothesis of no difference in means between treatments

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# Contrasts}
\NormalTok{heart}\SpecialCharTok{$}\NormalTok{drug }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(heart}\SpecialCharTok{$}\NormalTok{drug)}
\NormalTok{L }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{,}
              \DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{), }\AttributeTok{nrow =} \DecValTok{3}\NormalTok{, }\AttributeTok{byrow =}\NormalTok{ T)}
\FunctionTok{colnames}\NormalTok{(L) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"bww9:ctrl"}\NormalTok{, }\StringTok{"ax23:rest"}\NormalTok{)}
\FunctionTok{rownames}\NormalTok{(L) }\OtherTok{\textless{}{-}} \FunctionTok{unique}\NormalTok{(heart}\SpecialCharTok{$}\NormalTok{drug)}
\FunctionTok{contrasts}\NormalTok{(heart}\SpecialCharTok{$}\NormalTok{drug) }\OtherTok{\textless{}{-}}\NormalTok{ L}
\FunctionTok{contrasts}\NormalTok{(heart}\SpecialCharTok{$}\NormalTok{drug)}
\CommentTok{\#\textgreater{}      bww9:ctrl ax23:rest}
\CommentTok{\#\textgreater{} ax23         0         2}
\CommentTok{\#\textgreater{} bww9         1        {-}1}
\CommentTok{\#\textgreater{} ctrl        {-}1        {-}1}

\CommentTok{\# do not set contrast L if you do further analysis (e.g., Anova, lm)}
\CommentTok{\# do M matrix instead}

\NormalTok{M }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{,}
              \DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{,}
              \DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{), }\AttributeTok{nrow =} \DecValTok{4}\NormalTok{)}
\DocumentationTok{\#\# update model to test contrasts}
\NormalTok{heart\_mod2 }\OtherTok{\textless{}{-}} \FunctionTok{update}\NormalTok{(heart\_mod)}
\FunctionTok{coef}\NormalTok{(heart\_mod2)}
\CommentTok{\#\textgreater{}                  y1         y2        y3    y4}
\CommentTok{\#\textgreater{} (Intercept)   75.00 78.9583333 77.041667 74.75}
\CommentTok{\#\textgreater{} drugbww9:ctrl  4.50  5.8125000  3.562500  4.25}
\CommentTok{\#\textgreater{} drugax23:rest {-}2.25  0.7708333  1.979167 {-}0.75}

\CommentTok{\# Hypothesis test for bww9 vs control after transformation M}
\CommentTok{\# same as linearHypothesis(heart\_mod, hypothesis.matrix = c(0,1,{-}1), P = M)}
\NormalTok{bww9vctrl }\OtherTok{\textless{}{-}}
\NormalTok{    car}\SpecialCharTok{::}\FunctionTok{linearHypothesis}\NormalTok{(heart\_mod2,}
                     \AttributeTok{hypothesis.matrix =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{),}
                     \AttributeTok{P =}\NormalTok{ M)}
\NormalTok{bww9vctrl}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Response transformation matrix:}
\CommentTok{\#\textgreater{}    [,1] [,2] [,3]}
\CommentTok{\#\textgreater{} y1    1    0    0}
\CommentTok{\#\textgreater{} y2   {-}1    1    0}
\CommentTok{\#\textgreater{} y3    0   {-}1    1}
\CommentTok{\#\textgreater{} y4    0    0   {-}1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Sum of squares and products for the hypothesis:}
\CommentTok{\#\textgreater{}          [,1]   [,2]     [,3]}
\CommentTok{\#\textgreater{} [1,]  27.5625 {-}47.25  14.4375}
\CommentTok{\#\textgreater{} [2,] {-}47.2500  81.00 {-}24.7500}
\CommentTok{\#\textgreater{} [3,]  14.4375 {-}24.75   7.5625}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Sum of squares and products for error:}
\CommentTok{\#\textgreater{}          [,1]     [,2]    [,3]}
\CommentTok{\#\textgreater{} [1,]  261.375 {-}141.875  28.000}
\CommentTok{\#\textgreater{} [2,] {-}141.875  248.750 {-}19.375}
\CommentTok{\#\textgreater{} [3,]   28.000  {-}19.375 219.875}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Multivariate Tests: }
\CommentTok{\#\textgreater{}                  Df test stat approx F num Df den Df Pr(\textgreater{}F)}
\CommentTok{\#\textgreater{} Pillai            1 0.2564306 2.184141      3     19 0.1233}
\CommentTok{\#\textgreater{} Wilks             1 0.7435694 2.184141      3     19 0.1233}
\CommentTok{\#\textgreater{} Hotelling{-}Lawley  1 0.3448644 2.184141      3     19 0.1233}
\CommentTok{\#\textgreater{} Roy               1 0.3448644 2.184141      3     19 0.1233}

\NormalTok{bww9vctrl }\OtherTok{\textless{}{-}}
\NormalTok{    car}\SpecialCharTok{::}\FunctionTok{linearHypothesis}\NormalTok{(heart\_mod,}
                     \AttributeTok{hypothesis.matrix =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{),}
                     \AttributeTok{P =}\NormalTok{ M)}
\NormalTok{bww9vctrl}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Response transformation matrix:}
\CommentTok{\#\textgreater{}    [,1] [,2] [,3]}
\CommentTok{\#\textgreater{} y1    1    0    0}
\CommentTok{\#\textgreater{} y2   {-}1    1    0}
\CommentTok{\#\textgreater{} y3    0   {-}1    1}
\CommentTok{\#\textgreater{} y4    0    0   {-}1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Sum of squares and products for the hypothesis:}
\CommentTok{\#\textgreater{}          [,1]   [,2]     [,3]}
\CommentTok{\#\textgreater{} [1,]  27.5625 {-}47.25  14.4375}
\CommentTok{\#\textgreater{} [2,] {-}47.2500  81.00 {-}24.7500}
\CommentTok{\#\textgreater{} [3,]  14.4375 {-}24.75   7.5625}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Sum of squares and products for error:}
\CommentTok{\#\textgreater{}          [,1]     [,2]    [,3]}
\CommentTok{\#\textgreater{} [1,]  261.375 {-}141.875  28.000}
\CommentTok{\#\textgreater{} [2,] {-}141.875  248.750 {-}19.375}
\CommentTok{\#\textgreater{} [3,]   28.000  {-}19.375 219.875}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Multivariate Tests: }
\CommentTok{\#\textgreater{}                  Df test stat approx F num Df den Df Pr(\textgreater{}F)}
\CommentTok{\#\textgreater{} Pillai            1 0.2564306 2.184141      3     19 0.1233}
\CommentTok{\#\textgreater{} Wilks             1 0.7435694 2.184141      3     19 0.1233}
\CommentTok{\#\textgreater{} Hotelling{-}Lawley  1 0.3448644 2.184141      3     19 0.1233}
\CommentTok{\#\textgreater{} Roy               1 0.3448644 2.184141      3     19 0.1233}
\end{Highlighting}
\end{Shaded}

there is no significant difference in means between the control and \texttt{bww9} drug

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Hypothesis test for ax23 vs rest after transformation M}
\NormalTok{axx23vrest }\OtherTok{\textless{}{-}}
\NormalTok{    car}\SpecialCharTok{::}\FunctionTok{linearHypothesis}\NormalTok{(heart\_mod2,}
                     \AttributeTok{hypothesis.matrix =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{),}
                     \AttributeTok{P =}\NormalTok{ M)}
\NormalTok{axx23vrest}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Response transformation matrix:}
\CommentTok{\#\textgreater{}    [,1] [,2] [,3]}
\CommentTok{\#\textgreater{} y1    1    0    0}
\CommentTok{\#\textgreater{} y2   {-}1    1    0}
\CommentTok{\#\textgreater{} y3    0   {-}1    1}
\CommentTok{\#\textgreater{} y4    0    0   {-}1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Sum of squares and products for the hypothesis:}
\CommentTok{\#\textgreater{}           [,1]       [,2]      [,3]}
\CommentTok{\#\textgreater{} [1,]  438.0208  175.20833 {-}395.7292}
\CommentTok{\#\textgreater{} [2,]  175.2083   70.08333 {-}158.2917}
\CommentTok{\#\textgreater{} [3,] {-}395.7292 {-}158.29167  357.5208}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Sum of squares and products for error:}
\CommentTok{\#\textgreater{}          [,1]     [,2]    [,3]}
\CommentTok{\#\textgreater{} [1,]  261.375 {-}141.875  28.000}
\CommentTok{\#\textgreater{} [2,] {-}141.875  248.750 {-}19.375}
\CommentTok{\#\textgreater{} [3,]   28.000  {-}19.375 219.875}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Multivariate Tests: }
\CommentTok{\#\textgreater{}                  Df test stat approx F num Df den Df     Pr(\textgreater{}F)    }
\CommentTok{\#\textgreater{} Pillai            1  0.855364 37.45483      3     19 3.5484e{-}08 ***}
\CommentTok{\#\textgreater{} Wilks             1  0.144636 37.45483      3     19 3.5484e{-}08 ***}
\CommentTok{\#\textgreater{} Hotelling{-}Lawley  1  5.913921 37.45483      3     19 3.5484e{-}08 ***}
\CommentTok{\#\textgreater{} Roy               1  5.913921 37.45483      3     19 3.5484e{-}08 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}

\NormalTok{axx23vrest }\OtherTok{\textless{}{-}}
\NormalTok{    car}\SpecialCharTok{::}\FunctionTok{linearHypothesis}\NormalTok{(heart\_mod,}
                     \AttributeTok{hypothesis.matrix =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{),}
                     \AttributeTok{P =}\NormalTok{ M)}
\NormalTok{axx23vrest}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Response transformation matrix:}
\CommentTok{\#\textgreater{}    [,1] [,2] [,3]}
\CommentTok{\#\textgreater{} y1    1    0    0}
\CommentTok{\#\textgreater{} y2   {-}1    1    0}
\CommentTok{\#\textgreater{} y3    0   {-}1    1}
\CommentTok{\#\textgreater{} y4    0    0   {-}1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Sum of squares and products for the hypothesis:}
\CommentTok{\#\textgreater{}           [,1]       [,2]      [,3]}
\CommentTok{\#\textgreater{} [1,]  402.5208  127.41667 {-}390.9375}
\CommentTok{\#\textgreater{} [2,]  127.4167   40.33333 {-}123.7500}
\CommentTok{\#\textgreater{} [3,] {-}390.9375 {-}123.75000  379.6875}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Sum of squares and products for error:}
\CommentTok{\#\textgreater{}          [,1]     [,2]    [,3]}
\CommentTok{\#\textgreater{} [1,]  261.375 {-}141.875  28.000}
\CommentTok{\#\textgreater{} [2,] {-}141.875  248.750 {-}19.375}
\CommentTok{\#\textgreater{} [3,]   28.000  {-}19.375 219.875}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Multivariate Tests: }
\CommentTok{\#\textgreater{}                  Df test stat approx F num Df den Df     Pr(\textgreater{}F)    }
\CommentTok{\#\textgreater{} Pillai            1  0.842450 33.86563      3     19 7.9422e{-}08 ***}
\CommentTok{\#\textgreater{} Wilks             1  0.157550 33.86563      3     19 7.9422e{-}08 ***}
\CommentTok{\#\textgreater{} Hotelling{-}Lawley  1  5.347205 33.86563      3     19 7.9422e{-}08 ***}
\CommentTok{\#\textgreater{} Roy               1  5.347205 33.86563      3     19 7.9422e{-}08 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

there is a significant difference in means between ax23 drug treatment and the rest of the treatments

\hypertarget{profile-analysis}{%
\subsection{Profile Analysis}\label{profile-analysis}}

Examine similarities between the treatment effects (between subjects), which is useful for longitudinal analysis. Null is that all treatments have the same average effect.

\[
H_0: \mu_1 = \mu_2 = \dots = \mu_h
\]

Equivalently,

\[
H_0: \tau_1 = \tau_2 = \dots = \tau_h
\]

The exact nature of the similarities and differences between the treatments can be examined under this analysis.

Sequential steps in profile analysis:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Are the profiles \textbf{parallel}? (i.e., is there no interaction between treatment and time)
\item
  Are the profiles \textbf{coincidental}? (i.e., are the profiles identical?)
\item
  Are the profiles \textbf{horizontal}? (i.e., are there no differences between any time points?)
\end{enumerate}

If we reject the null hypothesis that the profiles are parallel, we can test

\begin{itemize}
\item
  Are there differences among groups within some subset of the total time points?
\item
  Are there differences among time points in a particular group (or groups)?
\item
  Are there differences within some subset of the total time points in a particular group (or groups)?
\end{itemize}

Example

\begin{itemize}
\item
  4 times (p = 4)
\item
  3 treatments (h=3)
\end{itemize}

\hypertarget{parallel-profile}{%
\subsubsection{Parallel Profile}\label{parallel-profile}}

Are the profiles for each population identical expect for a mean shift?

\[
\begin{aligned}
H_0: \mu_{11} - \mu_{21} - \mu_{12} - \mu_{22} = &\dots = \mu_{1t} - \mu_{2t} \\
\mu_{11} - \mu_{31} - \mu_{12} - \mu_{32} = &\dots = \mu_{1t} - \mu_{3t} \\
&\dots
\end{aligned}
\]

for \(h-1\) equations

Equivalently,

\[
H_0: \mathbf{LBM = 0}
\]

\[
\mathbf{LBM} =
\left[
\begin{array}
{ccc}
1 & -1 & 0 \\
1 & 0 & -1
\end{array}
\right]
\left[
\begin{array}
{ccc}
\mu_{11} & \dots & \mu_{14} \\
\mu_{21} & \dots & \mu_{24} \\
\mu_{31} & \dots & \mu_{34} 
\end{array}
\right]
\left[
\begin{array}
{ccc}
1 & 1 & 1 \\
-1 & 0 & 0 \\
0 & -1 & 0 \\
0 & 0 & -1
\end{array}
\right]
= 
\mathbf{0}
\]

where this is the cell means parameterization of \(\mathbf{B}\)

The multiplication of the first 2 matrices \(\mathbf{LB}\) is

\[
\left[
\begin{array}
{cccc}
\mu_{11} - \mu_{21} & \mu_{12} - \mu_{22} & \mu_{13} - \mu_{23} & \mu_{14} - \mu_{24}\\
\mu_{11} - \mu_{31} & \mu_{12} - \mu_{32} & \mu_{13} - \mu_{33} & \mu_{14} - \mu_{34} 
\end{array}
\right]
\]

which is the differences in treatment means at the same time

Multiplying by \(\mathbf{M}\), we get the comparison across time

\[
\left[
\begin{array}
{ccc}
(\mu_{11} - \mu_{21}) - (\mu_{12} - \mu_{22}) & (\mu_{11} - \mu_{21}) -(\mu_{13} - \mu_{23}) & (\mu_{11} - \mu_{21}) - (\mu_{14} - \mu_{24}) \\
(\mu_{11} - \mu_{31}) - (\mu_{12} - \mu_{32}) & (\mu_{11} - \mu_{31}) - (\mu_{13} - \mu_{33}) & (\mu_{11} - \mu_{31}) -(\mu_{14} - \mu_{34}) 
\end{array}
\right]
\]

Alternatively, we can also use the effects parameterization

\[
\mathbf{LBM} =
\left[
\begin{array}
{cccc}
0 & 1 & -1 & 0 \\
0 & 1 & 0 & -1 
\end{array}
\right]
\left[
\begin{array}
{c}
\mu' \\
\tau'_1 \\
\tau_2' \\
\tau_3'
\end{array}
\right]
\left[
\begin{array}
{ccc}
1 & 1 & 1 \\
-1 & 0 & 0 \\
0 & -1 & 0 \\
0 & 0 & -1
\end{array}
\right]
= \mathbf{0}
\]

In both parameterizations, \(rank(\mathbf{L}) = h-1\) and \(rank(\mathbf{M}) = p-1\)

We could also choose \(\mathbf{L}\) and \(\mathbf{M}\) in other forms

\[
\mathbf{L} = \left[
\begin{array}
{cccc}
0 & 1 & 0 & -1 \\
0 & 0 & 1 & -1 
\end{array}
\right]
\]

and

\[
\mathbf{M} = \left[
\begin{array}
{ccc}
1 & 0 & 0 \\
-1 & 1 & 0 \\
0 & -1 & 1 \\
0 & 0 & -1
\end{array}
\right]
\]

and still obtain the same result.

\hypertarget{coincidental-profiles}{%
\subsubsection{Coincidental Profiles}\label{coincidental-profiles}}

After we have evidence that the profiles are parallel (i.e., fail to reject the parallel profile test), we can ask whether they are identical?

Given profiles are \textbf{parallel}, then if the sums of the components of \(\mu_i\) are identical for all the treatments, then the profiles are \textbf{identical}.

\[
H_0: \mathbf{1'}_p \mu_1 = \mathbf{1'}_p \mu_2 = \dots = \mathbf{1'}_p \mu_h 
\]

Equivalently,

\[
H_0: \mathbf{LBM} = \mathbf{0}
\]

where for the cell means parameterization

\[
\mathbf{L} = 
\left[
\begin{array}
{ccc}
1 & 0 & -1 \\
0 & 1 & -1
\end{array}
\right]
\]

and

\[
\mathbf{M} = 
\left[
\begin{array}
{cccc}
1 & 1 & 1 & 1
\end{array}
\right]'
\]

multiplication yields

\[
\left[
\begin{array}
{c}
(\mu_{11} + \mu_{12} + \mu_{13} + \mu_{14}) - (\mu_{31} + \mu_{32} + \mu_{33} + \mu_{34}) \\
(\mu_{21} + \mu_{22} + \mu_{23} + \mu_{24}) - (\mu_{31} + \mu_{32} + \mu_{33} + \mu_{34})
\end{array}
\right]
=
\left[
\begin{array}
{c}
0 \\
0 
\end{array}
\right]
\]

Different choices of \(\mathbf{L}\) and \(\mathbf{M}\) can yield the same result

\hypertarget{horizontal-profiles}{%
\subsubsection{Horizontal Profiles}\label{horizontal-profiles}}

Given that we can't reject the null hypothesis that all \(h\) profiles are the same, we can ask whether all of the elements of the common profile equal? (i.e., horizontal)

\[
H_0: \mathbf{LBM} = \mathbf{0}
\]

\[
\mathbf{L} = 
\left[
\begin{array}
{ccc}
1 & 0 & 0 
\end{array}
\right]
\]

and

\[
\mathbf{M} = \left[
\begin{array}
{ccc}
1 & 0 & 0 \\
-1 & 1 & 0 \\
0 & -1 & 1 \\
0 & 0 & -1
\end{array}
\right]
\]

hence,

\[
\left[
\begin{array}
{ccc}
(\mu_{11} - \mu_{12}) & (\mu_{12} - \mu_{13}) & (\mu_{13} + \mu_{14}) 
\end{array}
\right]
=
\left[
\begin{array}
{ccc}
0 & 0 & 0
\end{array}
\right]
\]

Note:

\begin{itemize}
\tightlist
\item
  If we fail to reject all 3 hypotheses, then we fail to reject the null hypotheses of both no difference between treatments and no differences between traits.
\end{itemize}

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Test & Equivalent test for \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Parallel profile & Interaction \\
Coincidental profile & main effect of between-subjects factor \\
Horizontal profile & main effect of repeated measures factor \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{profile\_fit }\OtherTok{\textless{}{-}}
    \FunctionTok{pbg}\NormalTok{(}
        \AttributeTok{data =} \FunctionTok{as.matrix}\NormalTok{(heart[, }\DecValTok{2}\SpecialCharTok{:}\DecValTok{5}\NormalTok{]),}
        \AttributeTok{group =} \FunctionTok{as.matrix}\NormalTok{(heart[, }\DecValTok{1}\NormalTok{]),}
        \AttributeTok{original.names =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{profile.plot =} \ConstantTok{FALSE}
\NormalTok{    )}
\FunctionTok{summary}\NormalTok{(profile\_fit)}
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} pbg(data = as.matrix(heart[, 2:5]), group = as.matrix(heart[, }
\CommentTok{\#\textgreater{}     1]), original.names = TRUE, profile.plot = FALSE)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Hypothesis Tests:}
\CommentTok{\#\textgreater{} $\textasciigrave{}Ho: Profiles are parallel\textasciigrave{}}
\CommentTok{\#\textgreater{}   Multivariate.Test Statistic  Approx.F num.df den.df      p.value}
\CommentTok{\#\textgreater{} 1             Wilks 0.1102861 12.737599      6     38 7.891497e{-}08}
\CommentTok{\#\textgreater{} 2            Pillai 1.0891707  7.972007      6     40 1.092397e{-}05}
\CommentTok{\#\textgreater{} 3  Hotelling{-}Lawley 6.2587852 18.776356      6     36 9.258571e{-}10}
\CommentTok{\#\textgreater{} 4               Roy 5.9550887 39.700592      3     20 1.302458e{-}08}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $\textasciigrave{}Ho: Profiles have equal levels\textasciigrave{}}
\CommentTok{\#\textgreater{}             Df Sum Sq Mean Sq F value  Pr(\textgreater{}F)   }
\CommentTok{\#\textgreater{} group        2  328.7  164.35   5.918 0.00915 **}
\CommentTok{\#\textgreater{} Residuals   21  583.2   27.77                   }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $\textasciigrave{}Ho: Profiles are flat\textasciigrave{}}
\CommentTok{\#\textgreater{}          F df1 df2      p{-}value}
\CommentTok{\#\textgreater{} 1 14.30928   3  19 4.096803e{-}05}
\CommentTok{\# reject null hypothesis of parallel profiles}
\CommentTok{\# reject the null hypothesis of coincidental profiles}
\CommentTok{\# reject the null hypothesis that the profiles are flat}
\end{Highlighting}
\end{Shaded}

\hypertarget{summary-5}{%
\subsection{Summary}\label{summary-5}}

\includegraphics[width=6.25in,height=4.16667in]{images/MANOVA_summary.PNG}

\hypertarget{principal-components}{%
\section{Principal Components}\label{principal-components}}

\begin{itemize}
\tightlist
\item
  Unsupervised learning
\item
  find important features
\item
  reduce the dimensions of the data set
\item
  ``decorrelate'' multivariate vectors that have dependence.
\item
  uses eigenvector/eigvenvalue decomposition of covariance (correlation) matrices.
\end{itemize}

According to the ``spectral decomposition theorem'', if \(\mathbf{\Sigma}_{p \times p}\) i s a positive semi-definite, symmetric, real matrix, then there exists an orthogonal matrix \(\mathbf{A}\) such that \(\mathbf{A'\Sigma A} = \Lambda\) where \(\Lambda\) is a diagonal matrix containing the eigenvalues \(\mathbf{\Sigma}\)

\[
\mathbf{\Lambda} = 
\left(
\begin{array}
{cccc}
\lambda_1 & 0 & \ldots & 0 \\
0 & \lambda_2 & \ldots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \ldots & \lambda_p
\end{array}
\right)
\]

\[
\mathbf{A} =
\left(
\begin{array}
{cccc}
\mathbf{a}_1 & \mathbf{a}_2 & \ldots & \mathbf{a}_p
\end{array}
\right)
\]

the i-th column of \(\mathbf{A}\) , \(\mathbf{a}_i\), is the i-th \(p \times 1\) eigenvector of \(\mathbf{\Sigma}\) that corresponds to the eigenvalue, \(\lambda_i\) , where \(\lambda_1 \ge \lambda_2 \ge \ldots \ge \lambda_p\) . Alternatively, express in matrix decomposition:

\[
\mathbf{\Sigma} = \mathbf{A \Lambda A}'
\]

\[
\mathbf{\Sigma} = \mathbf{A}
\left(
\begin{array}
{cccc}
\lambda_1 & 0 & \ldots & 0 \\
0 & \lambda_2 & \ldots & 0 \\
\vdots & \vdots& \ddots & \vdots \\
0 & 0 & \ldots & \lambda_p
\end{array}
\right)
\mathbf{A}'
= \sum_{i=1}^p \lambda_i \mathbf{a}_i \mathbf{a}_i'
\]

where the outer product \(\mathbf{a}_i \mathbf{a}_i'\) is a \(p \times p\) matrix of rank 1.

For example,

\(\mathbf{x} \sim N_2(\mathbf{\mu}, \mathbf{\Sigma})\)

\[
\mathbf{\mu} = 
\left(
\begin{array}
{c}
5 \\ 
12 
\end{array} 
\right);
\mathbf{\Sigma} = 
\left(
\begin{array}
{cc}
4 & 1 \\
1 & 2 
\end{array}
\right)
\]

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(MASS)}
\NormalTok{mu }\OtherTok{=} \FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{12}\NormalTok{))}
\NormalTok{Sigma }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{), }\AttributeTok{nrow =} \DecValTok{2}\NormalTok{, }\AttributeTok{byrow =}\NormalTok{ T)}
\NormalTok{sim }\OtherTok{\textless{}{-}} \FunctionTok{mvrnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{1000}\NormalTok{, }\AttributeTok{mu =}\NormalTok{ mu, }\AttributeTok{Sigma =}\NormalTok{ Sigma)}
\FunctionTok{plot}\NormalTok{(sim[, }\DecValTok{1}\NormalTok{], sim[, }\DecValTok{2}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{22-multivariate_files/figure-latex/unnamed-chunk-14-1} \end{center}

Here,

\[
\mathbf{A} = 
\left(
\begin{array}
{cc}
0.9239 & -0.3827 \\
0.3827 & 0.9239 \\
\end{array}
\right)
\]

Columns of \(\mathbf{A}\) are the eigenvectors for the decomposition

Under matrix multiplication (\(\mathbf{A'\Sigma A}\) or \(\mathbf{A'A}\) ), the off-diagonal elements equal to 0

Multiplying data by this matrix (i.e., projecting the data onto the orthogonal axes); the distribution of the resulting data (i.e., ``scores'') is

\[
N_2 (\mathbf{A'\mu,A'\Sigma A}) = N_2 (\mathbf{A'\mu, \Lambda})
\]

Equivalently,

\[
\mathbf{y} = \mathbf{A'x} \sim N
\left[
\left(
\begin{array}
{c}
9.2119 \\
9.1733 
\end{array}
\right),
\left(
\begin{array}
{cc}
4.4144 & 0 \\
0 & 1.5859 
\end{array}
\right)
\right]
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A\_matrix }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FloatTok{0.9239}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{0.3827}\NormalTok{, }\FloatTok{0.3827}\NormalTok{, }\FloatTok{0.9239}\NormalTok{),}
                  \AttributeTok{nrow =} \DecValTok{2}\NormalTok{,}
                  \AttributeTok{byrow =}\NormalTok{ T)}
\FunctionTok{t}\NormalTok{(A\_matrix) }\SpecialCharTok{\%*\%}\NormalTok{ A\_matrix}
\CommentTok{\#\textgreater{}          [,1]     [,2]}
\CommentTok{\#\textgreater{} [1,] 1.000051 0.000000}
\CommentTok{\#\textgreater{} [2,] 0.000000 1.000051}

\NormalTok{sim1 }\OtherTok{\textless{}{-}}
    \FunctionTok{mvrnorm}\NormalTok{(}
        \AttributeTok{n =} \DecValTok{1000}\NormalTok{,}
        \AttributeTok{mu =} \FunctionTok{t}\NormalTok{(A\_matrix) }\SpecialCharTok{\%*\%}\NormalTok{ mu,}
        \AttributeTok{Sigma =} \FunctionTok{t}\NormalTok{(A\_matrix) }\SpecialCharTok{\%*\%}\NormalTok{ Sigma }\SpecialCharTok{\%*\%}\NormalTok{ A\_matrix}
\NormalTok{    )}
\FunctionTok{plot}\NormalTok{(sim1[, }\DecValTok{1}\NormalTok{], sim1[, }\DecValTok{2}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{22-multivariate_files/figure-latex/unnamed-chunk-15-1} \end{center}

No more dependence in the data structure, plot

Notes:

\begin{itemize}
\item
  The i-th eigenvalue is the variance of a linear combination of the elements of \(\mathbf{x}\) ; \(var(y_i) = var(\mathbf{a'_i x}) = \lambda_i\)
\item
  The values on the transformed set of axes (i.e., the \(y_i\)'s) are called the scores. These are the orthogonal projections of the data onto the ``new principal component axes
\item
  Variances of \(y_1\) are greater than those for any other possible projection
\end{itemize}

Covariance matrix decomposition and projection onto orthogonal axes = PCA

\hypertarget{population-principal-components}{%
\subsection{Population Principal Components}\label{population-principal-components}}

\(p \times 1\) vectors \(\mathbf{x}_1, \dots , \mathbf{x}_n\) which are iid with \(var(\mathbf{x}_i) = \mathbf{\Sigma}\)

\begin{itemize}
\item
  The first PC is the linear combination \(y_1 = \mathbf{a}_1' \mathbf{x} = a_{11}x_1 + \dots + a_{1p}x_p\) with \(\mathbf{a}_1' \mathbf{a}_1 = 1\) such that \(var(y_1)\) is the maximum of all linear combinations of \(\mathbf{x}\) which have unit length
\item
  The second PC is the linear combination \(y_1 = \mathbf{a}_2' \mathbf{x} = a_{21}x_1 + \dots + a_{2p}x_p\) with \(\mathbf{a}_2' \mathbf{a}_2 = 1\) such that \(var(y_1)\) is the maximum of all linear combinations of \(\mathbf{x}\) which have unit length and uncorrelated with \(y_1\) (i.e., \(cov(\mathbf{a}_1' \mathbf{x}, \mathbf{a}'_2 \mathbf{x}) =0\)
\item
  continues for all \(y_i\) to \(y_p\)
\end{itemize}

\(\mathbf{a}_i\)'s are those that make up the matrix \(\mathbf{A}\) in the symmetric decomposition \(\mathbf{A'\Sigma A} = \mathbf{\Lambda}\) , where \(var(y_1) = \lambda_1, \dots , var(y_p) = \lambda_p\) And the total variance of \(\mathbf{x}\) is

\[
\begin{aligned}
var(x_1) + \dots + var(x_p) &= tr(\Sigma) = \lambda_1 + \dots + \lambda_p \\
&= var(y_1) + \dots + var(y_p) 
\end{aligned}
\]

Data Reduction

To reduce the dimension of data from p (original) to k dimensions without much ``loss of information'', we can use properties of the population principal components

\begin{itemize}
\item
  Suppose \(\mathbf{\Sigma} \approx \sum_{i=1}^k \lambda_i \mathbf{a}_i \mathbf{a}_i'\) . Even thought the true variance-covariance matrix has rank \(p\) , it can be be well approximate by a matrix of rank k (k \textless p)
\item
  New ``traits'' are linear combinations of the measured traits. We can attempt to make meaningful interpretation fo the combinations (with orthogonality constraints).
\item
  The proportion of the total variance accounted for by the j-th principal component is
\end{itemize}

\[
\frac{var(y_j)}{\sum_{i=1}^p var(y_i)} = \frac{\lambda_j}{\sum_{i=1}^p \lambda_i}
\]

\begin{itemize}
\item
  The proportion of the total variation accounted for by the first k principal components is \(\frac{\sum_{i=1}^k \lambda_i}{\sum_{i=1}^p \lambda_i}\)
\item
  Above example , we have \(4.4144/(4+2) = .735\) of the total variability can be explained by the first principal component
\end{itemize}

\hypertarget{sample-principal-components}{%
\subsection{Sample Principal Components}\label{sample-principal-components}}

Since \(\mathbf{\Sigma}\) is unknown, we use

\[
\mathbf{S} = \frac{1}{n-1}\sum_{i=1}^n (\mathbf{x}_i - \bar{\mathbf{x}})(\mathbf{x}_i - \bar{\mathbf{x}})'
\]

Let \(\hat{\lambda}_1 \ge \hat{\lambda}_2 \ge \dots \ge \hat{\lambda}_p \ge 0\) be the eigenvalues of \(\mathbf{S}\) and \(\hat{\mathbf{a}}_1, \hat{\mathbf{a}}_2, \dots, \hat{\mathbf{a}}_p\) denote the eigenvectors of \(\mathbf{S}\)

Then, the i-th sample principal component score (or principal component or score) is

\[
\hat{y}_{ij} = \sum_{k=1}^p \hat{a}_{ik}x_{kj} = \hat{\mathbf{a}}_i'\mathbf{x}_j
\]

\textbf{Properties of Sample Principal Components}

\begin{itemize}
\item
  The estimated variance of \(y_i = \hat{\mathbf{a}}_i'\mathbf{x}_j\) is \(\hat{\lambda}_i\)
\item
  The sample covariance between \(\hat{y}_i\) and \(\hat{y}_{i'}\) is 0 when \(i \neq i'\)
\item
  The proportion of the total sample variance accounted for by the i-th sample principal component is \(\frac{\hat{\lambda}_i}{\sum_{k=1}^p \hat{\lambda}_k}\)
\item
  The estimated correlation between the \(i\)-th principal component score and the \(l\)-th attribute of \(\mathbf{x}\) is
\end{itemize}

\[
r_{x_l , \hat{y}_i} = \frac{\hat{a}_{il}\sqrt{\lambda_i}}{\sqrt{s_{ll}}}
\]

\begin{itemize}
\item
  The correlation coefficient is typically used to interpret the components (i.e., if this correlation is high then it suggests that the l-th original trait is important in the i-th principle component). According to \citet{johnson2002applied}, pp.433-434, \(r_{x_l, \hat{y}_i}\) only measures the univariate contribution of an individual X to a component Y without taking into account the presence of the other X's. Hence, some prefer \(\hat{a}_{il}\) coefficient to interpret the principal component.
\item
  \(r_{x_l, \hat{y}_i} ; \hat{a}_{il}\) are referred to as ``loadings''
\end{itemize}

To use k principal components, we must calculate the scores for each data vector in the sample

\[
\mathbf{y}_j = 
\left(
\begin{array}
{c}
y_{1j} \\
y_{2j} \\
\vdots \\
y_{kj} 
\end{array}
\right) = 
\left(
\begin{array}
{c}
\hat{\mathbf{a}}_1' \mathbf{x}_j \\
\hat{\mathbf{a}}_2' \mathbf{x}_j \\
\vdots \\
\hat{\mathbf{a}}_k' \mathbf{x}_j
\end{array}
\right) = 
\left(
\begin{array}
{c}
\hat{\mathbf{a}}_1' \\
\hat{\mathbf{a}}_2' \\
\vdots \\
\hat{\mathbf{a}}_k'
\end{array}
\right) \mathbf{x}_j
\]

Issues:

\begin{itemize}
\item
  Large sample theory exists for eigenvalues and eigenvectors of sample covariance matrices if inference is necessary. But we do not do inference with PCA, we only use it as exploratory or descriptive analysis.
\item
  PC is not invariant to changes in scale (Exception: if all trait are rescaled by multiplying by the same constant, such as feet to inches).

  \begin{itemize}
  \item
    PCA based on the correlation matrix \(\mathbf{R}\) is different than that based on the covariance matrix \(\mathbf{\Sigma}\)
  \item
    PCA for the correlation matrix is just rescaling each trait to have unit variance
  \item
    Transform \(\mathbf{x}\) to \(\mathbf{z}\) where \(z_{ij} = (x_{ij} - \bar{x}_i)/\sqrt{s_{ii}}\) where the denominator affects the PCA
  \item
    After transformation, \(cov(\mathbf{z}) = \mathbf{R}\)
  \item
    PCA on \(\mathbf{R}\) is calculated in the same way as that on \(\mathbf{S}\) (where \(\hat{\lambda}{}_1 + \dots + \hat{\lambda}{}_p = p\) )
  \item
    The use of \(\mathbf{R}, \mathbf{S}\) depends on the purpose of PCA.

    \begin{itemize}
    \tightlist
    \item
      If the scale of the observations if different, covariance matrix is more preferable. but if they are dramatically different, analysis can still be dominated by the large variance traits.
    \end{itemize}
  \item
    How many PCs to use can be guided by

    \begin{itemize}
    \item
      Scree Graphs: plot the eigenvalues against their indices. Look for the ``elbow'' where the steep decline in the graph suddenly flattens out; or big gaps.
    \item
      minimum Percent of total variation (e.g., choose enough components to have 50\% or 90\%). can be used for interpretations.
    \item
      Kaiser's rule: use only those PC with eigenvalues larger than 1 (applied to PCA on the correlation matrix) - ad hoc
    \item
      Compare to the eigenvalue scree plot of data to the scree plot when the data are randomized.
    \end{itemize}
  \end{itemize}
\end{itemize}

\hypertarget{application-3}{%
\subsection{Application}\label{application-3}}

PCA on the covariance matrix is usually not preferred due to the fact that PCA is not invariant to changes in scale. Hence, PCA on the correlation matrix is more preferred

This also addresses the problem of multicollinearity

The eigvenvectors may differ by a multiplication of -1 for different implementation, but same interpretation.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\DocumentationTok{\#\# Read in and check data}
\NormalTok{stock }\OtherTok{\textless{}{-}} \FunctionTok{read.table}\NormalTok{(}\StringTok{"images/stock.dat"}\NormalTok{)}
\FunctionTok{names}\NormalTok{(stock) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"allied"}\NormalTok{, }\StringTok{"dupont"}\NormalTok{, }\StringTok{"carbide"}\NormalTok{, }\StringTok{"exxon"}\NormalTok{, }\StringTok{"texaco"}\NormalTok{)}
\FunctionTok{str}\NormalTok{(stock)}
\CommentTok{\#\textgreater{} \textquotesingle{}data.frame\textquotesingle{}:    100 obs. of  5 variables:}
\CommentTok{\#\textgreater{}  $ allied : num  0 0.027 0.1228 0.057 0.0637 ...}
\CommentTok{\#\textgreater{}  $ dupont : num  0 {-}0.04485 0.06077 0.02995 {-}0.00379 ...}
\CommentTok{\#\textgreater{}  $ carbide: num  0 {-}0.00303 0.08815 0.06681 {-}0.03979 ...}
\CommentTok{\#\textgreater{}  $ exxon  : num  0.0395 {-}0.0145 0.0862 0.0135 {-}0.0186 ...}
\CommentTok{\#\textgreater{}  $ texaco : num  0 0.0435 0.0781 0.0195 {-}0.0242 ...}

\DocumentationTok{\#\# Covariance matrix of data}
\FunctionTok{cov}\NormalTok{(stock)}
\CommentTok{\#\textgreater{}               allied       dupont      carbide        exxon       texaco}
\CommentTok{\#\textgreater{} allied  0.0016299269 0.0008166676 0.0008100713 0.0004422405 0.0005139715}
\CommentTok{\#\textgreater{} dupont  0.0008166676 0.0012293759 0.0008276330 0.0003868550 0.0003109431}
\CommentTok{\#\textgreater{} carbide 0.0008100713 0.0008276330 0.0015560763 0.0004872816 0.0004624767}
\CommentTok{\#\textgreater{} exxon   0.0004422405 0.0003868550 0.0004872816 0.0008023323 0.0004084734}
\CommentTok{\#\textgreater{} texaco  0.0005139715 0.0003109431 0.0004624767 0.0004084734 0.0007587370}

\DocumentationTok{\#\# Correlation matrix of data}
\FunctionTok{cor}\NormalTok{(stock)}
\CommentTok{\#\textgreater{}            allied    dupont   carbide     exxon    texaco}
\CommentTok{\#\textgreater{} allied  1.0000000 0.5769244 0.5086555 0.3867206 0.4621781}
\CommentTok{\#\textgreater{} dupont  0.5769244 1.0000000 0.5983841 0.3895191 0.3219534}
\CommentTok{\#\textgreater{} carbide 0.5086555 0.5983841 1.0000000 0.4361014 0.4256266}
\CommentTok{\#\textgreater{} exxon   0.3867206 0.3895191 0.4361014 1.0000000 0.5235293}
\CommentTok{\#\textgreater{} texaco  0.4621781 0.3219534 0.4256266 0.5235293 1.0000000}

\CommentTok{\# cov(scale(stock)) \# give the same result}

\DocumentationTok{\#\# PCA with covariance}
\NormalTok{cov\_pca }\OtherTok{\textless{}{-}} \FunctionTok{prcomp}\NormalTok{(stock) }
\CommentTok{\# uses singular value decomposition for calculation and an N {-}1 divisor}
\CommentTok{\# alternatively, princomp can do PCA via spectral decomposition, }
\CommentTok{\# but it has worse numerical accuracy}

\CommentTok{\# eigen values}
\NormalTok{cov\_results }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{eigen\_values =}\NormalTok{ cov\_pca}\SpecialCharTok{$}\NormalTok{sdev }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{)}
\NormalTok{cov\_results }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{proportion =}\NormalTok{ eigen\_values }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(eigen\_values),}
           \AttributeTok{cumulative =} \FunctionTok{cumsum}\NormalTok{(proportion)) }
\CommentTok{\#\textgreater{}   eigen\_values proportion cumulative}
\CommentTok{\#\textgreater{} 1 0.0035953867 0.60159252  0.6015925}
\CommentTok{\#\textgreater{} 2 0.0007921798 0.13255027  0.7341428}
\CommentTok{\#\textgreater{} 3 0.0007364426 0.12322412  0.8573669}
\CommentTok{\#\textgreater{} 4 0.0005086686 0.08511218  0.9424791}
\CommentTok{\#\textgreater{} 5 0.0003437707 0.05752091  1.0000000}
\CommentTok{\# first 2 PCs account for 73\% variance in the data}

\CommentTok{\# eigen vectors}
\NormalTok{cov\_pca}\SpecialCharTok{$}\NormalTok{rotation }\CommentTok{\# prcomp calls rotation}
\CommentTok{\#\textgreater{}               PC1         PC2        PC3         PC4         PC5}
\CommentTok{\#\textgreater{} allied  0.5605914  0.73884565 {-}0.1260222  0.28373183 {-}0.20846832}
\CommentTok{\#\textgreater{} dupont  0.4698673 {-}0.09286987 {-}0.4675066 {-}0.68793190  0.28069055}
\CommentTok{\#\textgreater{} carbide 0.5473322 {-}0.65401929 {-}0.1140581  0.50045312 {-}0.09603973}
\CommentTok{\#\textgreater{} exxon   0.2908932 {-}0.11267353  0.6099196 {-}0.43808002 {-}0.58203935}
\CommentTok{\#\textgreater{} texaco  0.2842017  0.07103332  0.6168831  0.06227778  0.72784638}
\CommentTok{\# princomp calls loadings.}

\CommentTok{\# first PC = overall average}
\CommentTok{\# second PC compares Allied to Carbide}

\DocumentationTok{\#\# PCA with correlation}
\CommentTok{\#same as scale(stock) \%\textgreater{}\% prcomp}
\NormalTok{cor\_pca }\OtherTok{\textless{}{-}} \FunctionTok{prcomp}\NormalTok{(stock, }\AttributeTok{scale =}\NormalTok{ T)}



\CommentTok{\# eigen values}
\NormalTok{cor\_results }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{eigen\_values =}\NormalTok{ cor\_pca}\SpecialCharTok{$}\NormalTok{sdev }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{)}
\NormalTok{cor\_results }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{proportion =}\NormalTok{ eigen\_values }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(eigen\_values),}
           \AttributeTok{cumulative =} \FunctionTok{cumsum}\NormalTok{(proportion))}
\CommentTok{\#\textgreater{}   eigen\_values proportion cumulative}
\CommentTok{\#\textgreater{} 1    2.8564869 0.57129738  0.5712974}
\CommentTok{\#\textgreater{} 2    0.8091185 0.16182370  0.7331211}
\CommentTok{\#\textgreater{} 3    0.5400440 0.10800880  0.8411299}
\CommentTok{\#\textgreater{} 4    0.4513468 0.09026936  0.9313992}
\CommentTok{\#\textgreater{} 5    0.3430038 0.06860076  1.0000000}

\CommentTok{\# first egiven values corresponds to less variance }
\CommentTok{\# than PCA based on the covariance matrix}

\CommentTok{\# eigen vectors}
\NormalTok{cor\_pca}\SpecialCharTok{$}\NormalTok{rotation}
\CommentTok{\#\textgreater{}               PC1        PC2        PC3        PC4        PC5}
\CommentTok{\#\textgreater{} allied  0.4635405 {-}0.2408499  0.6133570 {-}0.3813727  0.4532876}
\CommentTok{\#\textgreater{} dupont  0.4570764 {-}0.5090997 {-}0.1778996 {-}0.2113068 {-}0.6749814}
\CommentTok{\#\textgreater{} carbide 0.4699804 {-}0.2605774 {-}0.3370355  0.6640985  0.3957247}
\CommentTok{\#\textgreater{} exxon   0.4216770  0.5252647 {-}0.5390181 {-}0.4728036  0.1794482}
\CommentTok{\#\textgreater{} texaco  0.4213291  0.5822416  0.4336029  0.3812273 {-}0.3874672}
\CommentTok{\# interpretation of PC2 is different from above: }
\CommentTok{\# it is a comparison of Allied, Dupont and Carbid to Exxon and Texaco }
\end{Highlighting}
\end{Shaded}

Covid Example

To reduce collinearity problem in this dataset, we can use principal components as regressors.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{load}\NormalTok{(}\StringTok{\textquotesingle{}images/MOcovid.RData\textquotesingle{}}\NormalTok{)}
\NormalTok{covidpca }\OtherTok{\textless{}{-}} \FunctionTok{prcomp}\NormalTok{(ndat[,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{],}\AttributeTok{scale =}\NormalTok{ T,}\AttributeTok{center =}\NormalTok{ T)}

\NormalTok{covidpca}\SpecialCharTok{$}\NormalTok{rotation[,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{]}
\CommentTok{\#\textgreater{}                                                          PC1         PC2}
\CommentTok{\#\textgreater{} X..Population.in.Rural.Areas                      0.32865838  0.05090955}
\CommentTok{\#\textgreater{} Area..sq..miles.                                  0.12014444 {-}0.28579183}
\CommentTok{\#\textgreater{} Population.density..sq..miles.                   {-}0.29670124  0.28312922}
\CommentTok{\#\textgreater{} Literacy.rate                                    {-}0.12517700 {-}0.08999542}
\CommentTok{\#\textgreater{} Families                                         {-}0.25856941  0.16485752}
\CommentTok{\#\textgreater{} Area.of.farm.land..sq..miles.                     0.02101106 {-}0.31070363}
\CommentTok{\#\textgreater{} Number.of.farms                                  {-}0.03814582 {-}0.44809679}
\CommentTok{\#\textgreater{} Average.value.of.all.property.per.farm..dollars. {-}0.05410709  0.14404306}
\CommentTok{\#\textgreater{} Estimation.of.rurality..                         {-}0.19040210  0.12089501}
\CommentTok{\#\textgreater{} Male..                                            0.02182394 {-}0.09568768}
\CommentTok{\#\textgreater{} Number.of.Physcians.per.100.000                  {-}0.31451606  0.13598026}
\CommentTok{\#\textgreater{} average.age                                       0.29414708  0.35593459}
\CommentTok{\#\textgreater{} X0.4.age.proportion                              {-}0.11431336 {-}0.23574057}
\CommentTok{\#\textgreater{} X20.44.age.proportion                            {-}0.32802128 {-}0.22718550}
\CommentTok{\#\textgreater{} X65.and.over.age.proportion                       0.30585033  0.32201626}
\CommentTok{\#\textgreater{} prop..White..nonHisp                              0.35627561 {-}0.14142646}
\CommentTok{\#\textgreater{} prop..Hispanic                                   {-}0.16655381 {-}0.15105342}
\CommentTok{\#\textgreater{} prop..Black                                      {-}0.33333359  0.24405802}


\CommentTok{\# Variability of each principal component: pr.var}
\NormalTok{pr.var }\OtherTok{\textless{}{-}}\NormalTok{ covidpca}\SpecialCharTok{$}\NormalTok{sdev }\SpecialCharTok{\^{}} \DecValTok{2}
\CommentTok{\# Variance explained by each principal component: pve}
\NormalTok{pve }\OtherTok{\textless{}{-}}\NormalTok{ pr.var }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(pr.var)}
\FunctionTok{plot}\NormalTok{(}
\NormalTok{    pve,}
    \AttributeTok{xlab =} \StringTok{"Principal Component"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"Proportion of Variance Explained"}\NormalTok{,}
    \AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.5}\NormalTok{),}
    \AttributeTok{type =} \StringTok{"b"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{22-multivariate_files/figure-latex/unnamed-chunk-17-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\FunctionTok{plot}\NormalTok{(}
    \FunctionTok{cumsum}\NormalTok{(pve),}
    \AttributeTok{xlab =} \StringTok{"Principal Component"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"Cumulative Proportion of Variance Explained"}\NormalTok{,}
    \AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{),}
    \AttributeTok{type =} \StringTok{"b"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{22-multivariate_files/figure-latex/unnamed-chunk-17-2} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# the first six principe account for around 80\% of the variance. }


\CommentTok{\#using base lm function for PC regression}
\NormalTok{pcadat }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(covidpca}\SpecialCharTok{$}\NormalTok{x[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{])}
\NormalTok{pcadat}\SpecialCharTok{$}\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ ndat}\SpecialCharTok{$}\NormalTok{Y}
\NormalTok{pcr.man }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(y) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., pcadat)}
\FunctionTok{mean}\NormalTok{(pcr.man}\SpecialCharTok{$}\NormalTok{residuals }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 0.03453371}

\CommentTok{\#comparison to lm w/o prin comps}
\NormalTok{lm.fit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(Y) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ ndat)}
\FunctionTok{mean}\NormalTok{(lm.fit}\SpecialCharTok{$}\NormalTok{residuals }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 0.02335128}
\end{Highlighting}
\end{Shaded}

MSE for the PC-based model is larger than regular regression, because models with a large degree of collinearity can still perform well.

\texttt{pcr} function in \texttt{pls} can be used for fitting PC regression (it will select the optimal number of components in the model).

\hypertarget{factor-analysis}{%
\section{Factor Analysis}\label{factor-analysis}}

Purpose

\begin{itemize}
\item
  Using a few linear combinations of underlying unobservable (latent) traits, we try to describe the covariance relationship among a large number of measured traits
\item
  Similar to \protect\hyperlink{principal-components}{PCA}, but factor analysis is \textbf{model based}
\end{itemize}

More details can be found on \href{https://online.stat.psu.edu/stat505/book/export/html/691}{PSU stat} or \href{http://users.stat.umn.edu/~helwig/notes/factanal-Notes.pdf}{UMN stat}

Let \(\mathbf{y}\) be the set of \(p\) measured variables

\(E(\mathbf{y}) = \mathbf{\mu}\)

\(var(\mathbf{y}) = \mathbf{\Sigma}\)

We have

\[
\begin{aligned}
\mathbf{y} - \mathbf{\mu} &= \mathbf{Lf} + \epsilon \\
&= 
\left(
\begin{array}
{c}
l_{11}f_1 + l_{12}f_2 + \dots + l_{tm}f_m \\
\vdots \\
l_{p1}f_1 + l_{p2}f_2 + \dots + l_{pm} f_m
\end{array}
\right)
+ 
\left(
\begin{array}
{c}
\epsilon_1 \\
\vdots \\
\epsilon_p
\end{array}
\right)
\end{aligned}
\]

where

\begin{itemize}
\item
  \(\mathbf{y} - \mathbf{\mu}\) = the p centered measurements
\item
  \(\mathbf{L}\) = \(p \times m\) matrix of factor loadings
\item
  \(\mathbf{f}\) = unobserved common factors for the population
\item
  \(\mathbf{\epsilon}\) = random errors (i.e., variation that is not accounted for by the common factors).
\end{itemize}

We want \(m\) (the number of factors) to be much smaller than \(p\) (the number of measured attributes)

\textbf{Restrictions on the model}

\begin{itemize}
\item
  \(E(\epsilon) = \mathbf{0}\)
\item
  \(var(\epsilon) = \Psi_{p \times p} = diag( \psi_1, \dots, \psi_p)\)
\item
  \(\mathbf{\epsilon}, \mathbf{f}\) are independent
\item
  Additional assumption could be \(E(\mathbf{f}) = \mathbf{0}, var(\mathbf{f}) = \mathbf{I}_{m \times m}\) (known as the orthogonal factor model) , which imposes the following covariance structure on \(\mathbf{y}\)
\end{itemize}

\[
\begin{aligned}
var(\mathbf{y}) = \mathbf{\Sigma} &=  var(\mathbf{Lf} + \mathbf{\epsilon}) \\
&= var(\mathbf{Lf}) + var(\epsilon) \\
&= \mathbf{L} var(\mathbf{f}) \mathbf{L}' + \mathbf{\Psi} \\
&= \mathbf{LIL}' + \mathbf{\Psi} \\
&= \mathbf{LL}' + \mathbf{\Psi}
\end{aligned}
\]

Since \(\mathbf{\Psi}\) is diagonal, the off-diagonal elements of \(\mathbf{LL}'\) are \(\sigma_{ij}\), the co variances in \(\mathbf{\Sigma}\), which means \(cov(y_i, y_j) = \sum_{k=1}^m l_{ik}l_{jk}\) and the covariance of \(\mathbf{y}\) is completely determined by the m factors ( \(m <<p\))

\(var(y_i) = \sum_{k=1}^m l_{ik}^2 + \psi_i\) where \(\psi_i\) is the \textbf{specific variance} and the summation term is the i-th \textbf{communality} (i.e., portion of the variance of the i-th variable contributed by the \(m\) common factors (\(h_i^2 = \sum_{k=1}^m l_{ik}^2\))

The factor model is only uniquely determined up to an orthogonal transformation of the factors.

Let \(\mathbf{T}_{m \times m}\) be an orthogonal matrix \(\mathbf{TT}' = \mathbf{T'T} = \mathbf{I}\) then

\[
\begin{aligned}
\mathbf{y} - \mathbf{\mu} &= \mathbf{Lf} + \epsilon \\
&= \mathbf{LTT'f} + \epsilon \\
&= \mathbf{L}^*(\mathbf{T'f}) + \epsilon & \text{where } \mathbf{L}^* = \mathbf{LT}
\end{aligned}
\]

and

\[
\begin{aligned}
\mathbf{\Sigma} &= \mathbf{LL}' + \mathbf{\Psi} \\
&= \mathbf{LTT'L} + \mathbf{\Psi} \\
&= (\mathbf{L}^*)(\mathbf{L}^*)' + \mathbf{\Psi}
\end{aligned}
\]

Hence, any orthogonal transformation of the factors is an equally good description of the correlations among the observed traits.

Let \(\mathbf{y} = \mathbf{Cx}\) , where \(\mathbf{C}\) is any diagonal matrix, then \(\mathbf{L}_y = \mathbf{CL}_x\) and \(\mathbf{\Psi}_y = \mathbf{C\Psi}_x\mathbf{C}\)

Hence, we can see that factor analysis is also invariant to changes in scale

\hypertarget{methods-of-estimation}{%
\subsection{Methods of Estimation}\label{methods-of-estimation}}

To estimate \(\mathbf{L}\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \protect\hyperlink{principal-component-method}{Principal Component Method}
\item
  \protect\hyperlink{principal-factor-method}{Principal Factor Method}
\item
  \ref{maximum-likelihood-method-factor-analysis}
\end{enumerate}

\hypertarget{principal-component-method}{%
\subsubsection{Principal Component Method}\label{principal-component-method}}

Spectral decomposition

\[
\begin{aligned}
\mathbf{\Sigma} &= \lambda_1 \mathbf{a}_1 \mathbf{a}_1' + \dots + \lambda_p \mathbf{a}_p \mathbf{a}_p' \\
&= \mathbf{A\Lambda A}' \\
&= \sum_{k=1}^m \lambda+k \mathbf{a}_k \mathbf{a}_k' + \sum_{k= m+1}^p \lambda_k \mathbf{a}_k \mathbf{a}_k' \\
&= \sum_{k=1}^m l_k l_k' + \sum_{k=m+1}^p \lambda_k \mathbf{a}_k \mathbf{a}_k'
\end{aligned}
\]

where \(l_k = \mathbf{a}_k \sqrt{\lambda_k}\) and the second term is not diagonal in general.

Assume

\[
\psi_i = \sigma_{ii} - \sum_{k=1}^m l_{ik}^2 = \sigma_{ii} -  \sum_{k=1}^m \lambda_i a_{ik}^2
\]

then

\[
\mathbf{\Sigma} \approx \mathbf{LL}' + \mathbf{\Psi}
\]

To estimate \(\mathbf{L}\) and \(\Psi\) , we use the expected eigenvalues and eigenvectors from \(\mathbf{S}\) or \(\mathbf{R}\)

\begin{itemize}
\item
  The estimated factor loadings don't change as the number of actors increases
\item
  The diagonal elements of \(\hat{\mathbf{L}}\hat{\mathbf{L}}' + \hat{\mathbf{\Psi}}\) are equal to the diagonal elements of \(\mathbf{S}\) and \(\mathbf{R}\), but the covariances may not be exactly reproduced
\item
  We select \(m\) so that the off-diagonal elements close to the values in \(\mathbf{S}\) (or to make the off-diagonal elements of \(\mathbf{S} - \hat{\mathbf{L}} \hat{\mathbf{L}}' + \hat{\mathbf{\Psi}}\) small)
\end{itemize}

\hypertarget{principal-factor-method}{%
\subsubsection{Principal Factor Method}\label{principal-factor-method}}

Consider modeling the correlation matrix, \(\mathbf{R} = \mathbf{L} \mathbf{L}' + \mathbf{\Psi}\) . Then

\[
\mathbf{L} \mathbf{L}' = \mathbf{R} - \mathbf{\Psi} =
\left(
\begin{array}
{cccc}
h_1^2 & r_{12} & \dots & r_{1p} \\
r_{21} & h_2^2 & \dots & r_{2p} \\
\vdots & \vdots & \ddots & \vdots \\
r_{p1} & r_{p2} & \dots & h_p^2
\end{array}
\right)
\]

where \(h_i^2 = 1- \psi_i\) (the communality)

Suppose that initial estimates are available for the communalities, \((h_1^*)^2,(h_2^*)^2, \dots , (h_p^*)^2\), then we can regress each trait on all the others, and then use the \(r^2\) as \(h^2\)

The estimate of \(\mathbf{R} - \mathbf{\Psi}\) at step k is

\[
(\mathbf{R} - \mathbf{\Psi})_k = 
\left(
\begin{array}
{cccc}
(h_1^*)^2 & r_{12} & \dots & r_{1p} \\
r_{21} & (h_2^*)^2 & \dots & r_{2p} \\
\vdots & \vdots & \ddots & \vdots \\
r_{p1} & r_{p2} & \dots & (h_p^*)^2
\end{array}
\right) = 
\mathbf{L}_k^*(\mathbf{L}_k^*)' 
\]

where

\[
\mathbf{L}_k^* = (\sqrt{\hat{\lambda}_1^*\hat{\mathbf{a}}_1^* , \dots \hat{\lambda}_m^*\hat{\mathbf{a}}_m^*})
\]

and

\[
\hat{\psi}_{i,k}^* = 1 - \sum_{j=1}^m \hat{\lambda}_i^* (\hat{a}_{ij}^*)^2
\]

we used the spectral decomposition on the estimated matrix \((\mathbf{R}- \mathbf{\Psi})\) to calculate the \(\hat{\lambda}_i^* s\) and the \(\mathbf{\hat{a}}_i^* s\)

After updating the values of \((\hat{h}_i^*)^2 = 1 - \hat{\psi}_{i,k}^*\) we will use them to form a new \(\mathbf{L}_{k+1}^*\) via another spectral decomposition. Repeat the process

Notes:

\begin{itemize}
\item
  The matrix \((\mathbf{R} - \mathbf{\Psi})_k\) is not necessarily positive definite
\item
  The principal component method is similar to principal factor if one considers the initial communalities are \(h^2 = 1\)
\item
  if \(m\) is too large, some communalities may become larger than 1, causing the iterations to terminate. To combat, we can

  \begin{itemize}
  \item
    fix any communality that is greater than 1 at 1 and then continues.
  \item
    continue iterations regardless of the size of the communalities. However, results can be outside fo the parameter space.
  \end{itemize}
\end{itemize}

\hypertarget{maximum-likelihood-method-factor-analysis}{%
\subsubsection{Maximum Likelihood Method}\label{maximum-likelihood-method-factor-analysis}}

Since we need the likelihood function, we make the additional (critical) assumption that

\begin{itemize}
\item
  \(\mathbf{y}_j \sim N(\mathbf{\mu},\mathbf{\Sigma})\) for \(j = 1,..,n\)
\item
  \(\mathbf{f} \sim N(\mathbf{0}, \mathbf{I})\)
\item
  \(\epsilon_j \sim N(\mathbf{0}, \mathbf{\Psi})\)
\end{itemize}

and restriction

\begin{itemize}
\tightlist
\item
  \(\mathbf{L}' \mathbf{\Psi}^{-1}\mathbf{L} = \mathbf{\Delta}\) where \(\mathbf{\Delta}\) is a diagonal matrix. (since the factor loading matrix is not unique, we need this restriction).
\end{itemize}

Notes:

\begin{itemize}
\item
  Finding MLE can be computationally expensive
\item
  we typically use other methods for exploratory data analysis
\item
  Likelihood ratio tests could be used for testing hypotheses in this framework (i.e., Confirmatory Factor Analysis)
\end{itemize}

\hypertarget{factor-rotation}{%
\subsection{Factor Rotation}\label{factor-rotation}}

\(\mathbf{T}_{m \times m}\) is an orthogonal matrix that has the property that

\[
\hat{\mathbf{L}} \hat{\mathbf{L}}' + \hat{\mathbf{\Psi}} = \hat{\mathbf{L}}^*(\hat{\mathbf{L}}^*)' + \hat{\mathbf{\Psi}}
\]

where \(\mathbf{L}^* = \mathbf{LT}\)

This means that estimated specific variances and communalities are not altered by the orthogonal transformation.

Since there are an infinite number of choices for \(\mathbf{T}\), some selection criterion is necessary

For example, we can find the orthogonal transformation that maximizes the objective function

\[
\sum_{j = 1}^m [\frac{1}{p}\sum_{i=1}^p (\frac{l_{ij}^{*2}}{h_i})^2 - \{\frac{\gamma}{p} \sum_{i=1}^p (\frac{l_{ij}^{*2}}{h_i})^2 \}^2]
\]

where \(\frac{l_{ij}^{*2}}{h_i}\) are ``scaled loadings'', which gives variables with small communalities more influence.

Different choices of \(\gamma\) in the objective function correspond to different orthogonal rotation found in the literature;

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Varimax \(\gamma = 1\) (rotate the factors so that each of the \(p\) variables should have a high loading on only one factor, but this is not always possible).
\item
  Quartimax \(\gamma = 0\)
\item
  Equimax \(\gamma = m/2\)
\item
  Parsimax \(\gamma = \frac{p(m-1)}{p+m-2}\)
\item
  Promax: non-orthogonal or olique transformations
\item
  Harris-Kaiser (HK): non-orthogonal or oblique transformations
\end{enumerate}

\hypertarget{estimation-of-factor-scores}{%
\subsection{Estimation of Factor Scores}\label{estimation-of-factor-scores}}

Recall

\[
(\mathbf{y}_j - \mathbf{\mu}) = \mathbf{L}_{p \times m}\mathbf{f}_j + \epsilon_j
\]

If the factor model is correct then

\[
var(\epsilon_j) = \mathbf{\Psi} = diag (\psi_1, \dots , \psi_p)
\]

Thus we could consider using weighted least squares to estimate \(\mathbf{f}_j\) , the vector of factor scores for the j-th sampled unit by

\[
\begin{aligned}
\hat{\mathbf{f}} &= (\mathbf{L}'\mathbf{\Psi}^{-1} \mathbf{L})^{-1} \mathbf{L}' \mathbf{\Psi}^{-1}(\mathbf{y}_j - \mathbf{\mu}) \\
& \approx (\mathbf{L}'\mathbf{\Psi}^{-1} \mathbf{L})^{-1} \mathbf{L}' \mathbf{\Psi}^{-1}(\mathbf{y}_j - \mathbf{\bar{y}})
\end{aligned}
\]

\hypertarget{the-regression-method}{%
\subsubsection{The Regression Method}\label{the-regression-method}}

Alternatively, we can use the regression method to estimate the factor scores

Consider the joint distribution of \((\mathbf{y}_j - \mathbf{\mu})\) and \(\mathbf{f}_j\) assuming multivariate normality, as in the maximum likelihood approach. then,

\[
\left(
\begin{array}
{c}
\mathbf{y}_j - \mathbf{\mu} \\
\mathbf{f}_j
\end{array}
\right) \sim
N_{p + m}
\left(
\left[
\begin{array}
{cc}
\mathbf{LL}' + \mathbf{\Psi} & \mathbf{L} \\
\mathbf{L}' & \mathbf{I}_{m\times m}
\end{array}
\right]
\right)
\]

when the \(m\) factor model is correct

Hence,

\[
E(\mathbf{f}_j | \mathbf{y}_j - \mathbf{\mu}) = \mathbf{L}' (\mathbf{LL}' + \mathbf{\Psi})^{-1}(\mathbf{y}_j - \mathbf{\mu})
\]

notice that \(\mathbf{L}' (\mathbf{LL}' + \mathbf{\Psi})^{-1}\) is an \(m \times p\) matrix of regression coefficients

Then, we use the estimated conditional mean vector to estimate the factor scores

\[
\mathbf{\hat{f}}_j = \mathbf{\hat{L}}'(\mathbf{\hat{L}}\mathbf{\hat{L}}' + \mathbf{\hat{\Psi}})^{-1}(\mathbf{y}_j - \mathbf{\bar{y}})
\]

Alternatively, we could reduce the effect of possible incorrect determination fo the number of factors \(m\) by using \(\mathbf{S}\) as a substitute for \(\mathbf{\hat{L}}\mathbf{\hat{L}}' + \mathbf{\hat{\Psi}}\) then

\[
\mathbf{\hat{f}}_j = \mathbf{\hat{L}}'\mathbf{S}^{-1}(\mathbf{y}_j - \mathbf{\bar{y}})
\]

where \(j = 1,\dots,n\)

\hypertarget{model-diagnostic}{%
\subsection{Model Diagnostic}\label{model-diagnostic}}

\begin{itemize}
\item
  Plots
\item
  Check for outliers (recall that \(\mathbf{f}_j \sim iid N(\mathbf{0}, \mathbf{I}_{m \times m})\))
\item
  Check for multivariate normality assumption
\item
  Use univariate tests for normality to check the factor scores
\item
  \textbf{Confirmatory Factor Analysis}: formal testing of hypotheses about loadings, use MLE and full/reduced model testing paradigm and measures of model fit
\end{itemize}

\hypertarget{application-4}{%
\subsection{Application}\label{application-4}}

In the \texttt{psych} package,

\begin{itemize}
\item
  h2 = the communalities
\item
  u2 = the uniqueness
\item
  com = the complexity
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(psych)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\DocumentationTok{\#\# Load the data from the psych package}
\FunctionTok{data}\NormalTok{(Harman}\FloatTok{.5}\NormalTok{)}
\NormalTok{Harman}\FloatTok{.5}
\CommentTok{\#\textgreater{}         population schooling employment professional housevalue}
\CommentTok{\#\textgreater{} Tract1        5700      12.8       2500          270      25000}
\CommentTok{\#\textgreater{} Tract2        1000      10.9        600           10      10000}
\CommentTok{\#\textgreater{} Tract3        3400       8.8       1000           10       9000}
\CommentTok{\#\textgreater{} Tract4        3800      13.6       1700          140      25000}
\CommentTok{\#\textgreater{} Tract5        4000      12.8       1600          140      25000}
\CommentTok{\#\textgreater{} Tract6        8200       8.3       2600           60      12000}
\CommentTok{\#\textgreater{} Tract7        1200      11.4        400           10      16000}
\CommentTok{\#\textgreater{} Tract8        9100      11.5       3300           60      14000}
\CommentTok{\#\textgreater{} Tract9        9900      12.5       3400          180      18000}
\CommentTok{\#\textgreater{} Tract10       9600      13.7       3600          390      25000}
\CommentTok{\#\textgreater{} Tract11       9600       9.6       3300           80      12000}
\CommentTok{\#\textgreater{} Tract12       9400      11.4       4000          100      13000}

\CommentTok{\# Correlation matrix}
\NormalTok{cor\_mat }\OtherTok{\textless{}{-}} \FunctionTok{cor}\NormalTok{(Harman}\FloatTok{.5}\NormalTok{)}
\NormalTok{cor\_mat}
\CommentTok{\#\textgreater{}              population  schooling employment professional housevalue}
\CommentTok{\#\textgreater{} population   1.00000000 0.00975059  0.9724483    0.4388708 0.02241157}
\CommentTok{\#\textgreater{} schooling    0.00975059 1.00000000  0.1542838    0.6914082 0.86307009}
\CommentTok{\#\textgreater{} employment   0.97244826 0.15428378  1.0000000    0.5147184 0.12192599}
\CommentTok{\#\textgreater{} professional 0.43887083 0.69140824  0.5147184    1.0000000 0.77765425}
\CommentTok{\#\textgreater{} housevalue   0.02241157 0.86307009  0.1219260    0.7776543 1.00000000}

\DocumentationTok{\#\# Principal Component Method with Correlation}
\NormalTok{cor\_pca }\OtherTok{\textless{}{-}} \FunctionTok{prcomp}\NormalTok{(Harman}\FloatTok{.5}\NormalTok{, }\AttributeTok{scale =}\NormalTok{ T)}
\CommentTok{\# eigen values}
\NormalTok{cor\_results }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{eigen\_values =}\NormalTok{ cor\_pca}\SpecialCharTok{$}\NormalTok{sdev }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{)}

\NormalTok{cor\_results }\OtherTok{\textless{}{-}}\NormalTok{ cor\_results }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}
        \AttributeTok{proportion =}\NormalTok{ eigen\_values }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(eigen\_values),}
        \AttributeTok{cumulative =} \FunctionTok{cumsum}\NormalTok{(proportion),}
        \AttributeTok{number =} \FunctionTok{row\_number}\NormalTok{()}
\NormalTok{    )}
\NormalTok{cor\_results}
\CommentTok{\#\textgreater{}   eigen\_values  proportion cumulative number}
\CommentTok{\#\textgreater{} 1   2.87331359 0.574662719  0.5746627      1}
\CommentTok{\#\textgreater{} 2   1.79666009 0.359332019  0.9339947      2}
\CommentTok{\#\textgreater{} 3   0.21483689 0.042967377  0.9769621      3}
\CommentTok{\#\textgreater{} 4   0.09993405 0.019986811  0.9969489      4}
\CommentTok{\#\textgreater{} 5   0.01525537 0.003051075  1.0000000      5}

\CommentTok{\# Scree plot of Eigenvalues}
\NormalTok{scree\_gg }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(cor\_results, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ number, }\AttributeTok{y =}\NormalTok{ eigen\_values)) }\SpecialCharTok{+}
    \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_text}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =}\NormalTok{ number)) }\SpecialCharTok{+}
    \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{name =} \StringTok{"Number"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{name =} \StringTok{"Eigenvalue"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{theme\_bw}\NormalTok{()}
\NormalTok{scree\_gg}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{22-multivariate_files/figure-latex/unnamed-chunk-18-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\FunctionTok{screeplot}\NormalTok{(cor\_pca, }\AttributeTok{type =} \StringTok{\textquotesingle{}lines\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{22-multivariate_files/figure-latex/unnamed-chunk-18-2} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\DocumentationTok{\#\# Keep 2 factors based on scree plot and eigenvalues}
\NormalTok{factor\_pca }\OtherTok{\textless{}{-}} \FunctionTok{principal}\NormalTok{(Harman}\FloatTok{.5}\NormalTok{, }\AttributeTok{nfactors =} \DecValTok{2}\NormalTok{, }\AttributeTok{rotate =} \StringTok{"none"}\NormalTok{)}
\NormalTok{factor\_pca}
\CommentTok{\#\textgreater{} Principal Components Analysis}
\CommentTok{\#\textgreater{} Call: principal(r = Harman.5, nfactors = 2, rotate = "none")}
\CommentTok{\#\textgreater{} Standardized loadings (pattern matrix) based upon correlation matrix}
\CommentTok{\#\textgreater{}               PC1   PC2   h2    u2 com}
\CommentTok{\#\textgreater{} population   0.58  0.81 0.99 0.012 1.8}
\CommentTok{\#\textgreater{} schooling    0.77 {-}0.54 0.89 0.115 1.8}
\CommentTok{\#\textgreater{} employment   0.67  0.73 0.98 0.021 2.0}
\CommentTok{\#\textgreater{} professional 0.93 {-}0.10 0.88 0.120 1.0}
\CommentTok{\#\textgreater{} housevalue   0.79 {-}0.56 0.94 0.062 1.8}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}                        PC1  PC2}
\CommentTok{\#\textgreater{} SS loadings           2.87 1.80}
\CommentTok{\#\textgreater{} Proportion Var        0.57 0.36}
\CommentTok{\#\textgreater{} Cumulative Var        0.57 0.93}
\CommentTok{\#\textgreater{} Proportion Explained  0.62 0.38}
\CommentTok{\#\textgreater{} Cumulative Proportion 0.62 1.00}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Mean item complexity =  1.7}
\CommentTok{\#\textgreater{} Test of the hypothesis that 2 components are sufficient.}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} The root mean square of the residuals (RMSR) is  0.03 }
\CommentTok{\#\textgreater{}  with the empirical chi square  0.29  with prob \textless{}  0.59 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Fit based upon off diagonal values = 1}

\CommentTok{\# factor 1 = overall socioeconomic health}
\CommentTok{\# factor 2 = contrast of the population and employment against school and house value}


\DocumentationTok{\#\# Ssquared multiple correlation (SMC) prior, no rotation}
\NormalTok{factor\_pca\_smc }\OtherTok{\textless{}{-}} \FunctionTok{fa}\NormalTok{(}
\NormalTok{    Harman}\FloatTok{.5}\NormalTok{,}
    \AttributeTok{nfactors =} \DecValTok{2}\NormalTok{,}
    \AttributeTok{fm =} \StringTok{"pa"}\NormalTok{,}
    \AttributeTok{rotate =} \StringTok{"none"}\NormalTok{,}
    \AttributeTok{SMC =} \ConstantTok{TRUE}
\NormalTok{)}
\NormalTok{factor\_pca\_smc}
\CommentTok{\#\textgreater{} Factor Analysis using method =  pa}
\CommentTok{\#\textgreater{} Call: fa(r = Harman.5, nfactors = 2, rotate = "none", SMC = TRUE, fm = "pa")}
\CommentTok{\#\textgreater{} Standardized loadings (pattern matrix) based upon correlation matrix}
\CommentTok{\#\textgreater{}               PA1   PA2   h2      u2 com}
\CommentTok{\#\textgreater{} population   0.62  0.78 1.00 {-}0.0027 1.9}
\CommentTok{\#\textgreater{} schooling    0.70 {-}0.53 0.77  0.2277 1.9}
\CommentTok{\#\textgreater{} employment   0.70  0.68 0.96  0.0413 2.0}
\CommentTok{\#\textgreater{} professional 0.88 {-}0.15 0.80  0.2017 1.1}
\CommentTok{\#\textgreater{} housevalue   0.78 {-}0.60 0.96  0.0361 1.9}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}                        PA1  PA2}
\CommentTok{\#\textgreater{} SS loadings           2.76 1.74}
\CommentTok{\#\textgreater{} Proportion Var        0.55 0.35}
\CommentTok{\#\textgreater{} Cumulative Var        0.55 0.90}
\CommentTok{\#\textgreater{} Proportion Explained  0.61 0.39}
\CommentTok{\#\textgreater{} Cumulative Proportion 0.61 1.00}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Mean item complexity =  1.7}
\CommentTok{\#\textgreater{} Test of the hypothesis that 2 factors are sufficient.}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} df null model =  10  with the objective function =  6.38 with Chi Square =  54.25}
\CommentTok{\#\textgreater{} df of  the model are 1  and the objective function was  0.34 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} The root mean square of the residuals (RMSR) is  0.01 }
\CommentTok{\#\textgreater{} The df corrected root mean square of the residuals is  0.03 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} The harmonic n.obs is  12 with the empirical chi square  0.02  with prob \textless{}  0.88 }
\CommentTok{\#\textgreater{} The total n.obs was  12  with Likelihood Chi Square =  2.44  with prob \textless{}  0.12 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Tucker Lewis Index of factoring reliability =  0.596}
\CommentTok{\#\textgreater{} RMSEA index =  0.336  and the 90 \% confidence intervals are  0 0.967}
\CommentTok{\#\textgreater{} BIC =  {-}0.04}
\CommentTok{\#\textgreater{} Fit based upon off diagonal values = 1}

\DocumentationTok{\#\# SMC prior, Promax rotation}
\NormalTok{factor\_pca\_smc\_pro }\OtherTok{\textless{}{-}} \FunctionTok{fa}\NormalTok{(}
\NormalTok{    Harman}\FloatTok{.5}\NormalTok{,}
    \AttributeTok{nfactors =} \DecValTok{2}\NormalTok{,}
    \AttributeTok{fm =} \StringTok{"pa"}\NormalTok{,}
    \AttributeTok{rotate =} \StringTok{"Promax"}\NormalTok{,}
    \AttributeTok{SMC =} \ConstantTok{TRUE}
\NormalTok{)}
\NormalTok{factor\_pca\_smc\_pro}
\CommentTok{\#\textgreater{} Factor Analysis using method =  pa}
\CommentTok{\#\textgreater{} Call: fa(r = Harman.5, nfactors = 2, rotate = "Promax", SMC = TRUE, }
\CommentTok{\#\textgreater{}     fm = "pa")}
\CommentTok{\#\textgreater{} Standardized loadings (pattern matrix) based upon correlation matrix}
\CommentTok{\#\textgreater{}                PA1   PA2   h2      u2 com}
\CommentTok{\#\textgreater{} population   {-}0.11  1.02 1.00 {-}0.0027 1.0}
\CommentTok{\#\textgreater{} schooling     0.90 {-}0.11 0.77  0.2277 1.0}
\CommentTok{\#\textgreater{} employment    0.02  0.97 0.96  0.0413 1.0}
\CommentTok{\#\textgreater{} professional  0.75  0.33 0.80  0.2017 1.4}
\CommentTok{\#\textgreater{} housevalue    1.01 {-}0.14 0.96  0.0361 1.0}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}                        PA1  PA2}
\CommentTok{\#\textgreater{} SS loadings           2.38 2.11}
\CommentTok{\#\textgreater{} Proportion Var        0.48 0.42}
\CommentTok{\#\textgreater{} Cumulative Var        0.48 0.90}
\CommentTok{\#\textgreater{} Proportion Explained  0.53 0.47}
\CommentTok{\#\textgreater{} Cumulative Proportion 0.53 1.00}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  With factor correlations of }
\CommentTok{\#\textgreater{}      PA1  PA2}
\CommentTok{\#\textgreater{} PA1 1.00 0.25}
\CommentTok{\#\textgreater{} PA2 0.25 1.00}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Mean item complexity =  1.1}
\CommentTok{\#\textgreater{} Test of the hypothesis that 2 factors are sufficient.}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} df null model =  10  with the objective function =  6.38 with Chi Square =  54.25}
\CommentTok{\#\textgreater{} df of  the model are 1  and the objective function was  0.34 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} The root mean square of the residuals (RMSR) is  0.01 }
\CommentTok{\#\textgreater{} The df corrected root mean square of the residuals is  0.03 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} The harmonic n.obs is  12 with the empirical chi square  0.02  with prob \textless{}  0.88 }
\CommentTok{\#\textgreater{} The total n.obs was  12  with Likelihood Chi Square =  2.44  with prob \textless{}  0.12 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Tucker Lewis Index of factoring reliability =  0.596}
\CommentTok{\#\textgreater{} RMSEA index =  0.336  and the 90 \% confidence intervals are  0 0.967}
\CommentTok{\#\textgreater{} BIC =  {-}0.04}
\CommentTok{\#\textgreater{} Fit based upon off diagonal values = 1}

\DocumentationTok{\#\# SMC prior, varimax rotation}
\NormalTok{factor\_pca\_smc\_var }\OtherTok{\textless{}{-}} \FunctionTok{fa}\NormalTok{(}
\NormalTok{    Harman}\FloatTok{.5}\NormalTok{,}
    \AttributeTok{nfactors =} \DecValTok{2}\NormalTok{,}
    \AttributeTok{fm =} \StringTok{"pa"}\NormalTok{,}
    \AttributeTok{rotate =} \StringTok{"varimax"}\NormalTok{,}
    \AttributeTok{SMC =} \ConstantTok{TRUE}
\NormalTok{)}
\DocumentationTok{\#\# Make a data frame of the loadings for ggplot2}
\NormalTok{factors\_df }\OtherTok{\textless{}{-}}
    \FunctionTok{bind\_rows}\NormalTok{(}
        \FunctionTok{data.frame}\NormalTok{(}
            \AttributeTok{y =} \FunctionTok{rownames}\NormalTok{(factor\_pca\_smc}\SpecialCharTok{$}\NormalTok{loadings),}
            \FunctionTok{unclass}\NormalTok{(factor\_pca\_smc}\SpecialCharTok{$}\NormalTok{loadings)}
\NormalTok{        ),}
        \FunctionTok{data.frame}\NormalTok{(}
            \AttributeTok{y =} \FunctionTok{rownames}\NormalTok{(factor\_pca\_smc\_pro}\SpecialCharTok{$}\NormalTok{loadings),}
            \FunctionTok{unclass}\NormalTok{(factor\_pca\_smc\_pro}\SpecialCharTok{$}\NormalTok{loadings)}
\NormalTok{        ),}
        \FunctionTok{data.frame}\NormalTok{(}
            \AttributeTok{y =} \FunctionTok{rownames}\NormalTok{(factor\_pca\_smc\_var}\SpecialCharTok{$}\NormalTok{loadings),}
            \FunctionTok{unclass}\NormalTok{(factor\_pca\_smc\_var}\SpecialCharTok{$}\NormalTok{loadings)}
\NormalTok{        ),}
        \AttributeTok{.id =} \StringTok{"Rotation"}
\NormalTok{    )}
\NormalTok{flag\_gg }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(factors\_df) }\SpecialCharTok{+}
    \FunctionTok{geom\_vline}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{xintercept =} \DecValTok{0}\NormalTok{)) }\SpecialCharTok{+}
    \FunctionTok{geom\_hline}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{)) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}
        \AttributeTok{x =}\NormalTok{ PA2,}
        \AttributeTok{y =}\NormalTok{ PA1,}
        \AttributeTok{col =}\NormalTok{ y,}
        \AttributeTok{shape =}\NormalTok{ y}
\NormalTok{    ), }\AttributeTok{size =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{name =} \StringTok{"Factor 2"}\NormalTok{, }\AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{1.1}\NormalTok{, }\FloatTok{1.1}\NormalTok{)) }\SpecialCharTok{+}
    \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{name =} \StringTok{"Factor1"}\NormalTok{, }\AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{1.1}\NormalTok{, }\FloatTok{1.1}\NormalTok{)) }\SpecialCharTok{+}
    \FunctionTok{facet\_wrap}\NormalTok{(}\StringTok{"Rotation"}\NormalTok{, }\AttributeTok{labeller =} \FunctionTok{labeller}\NormalTok{(}\AttributeTok{Rotation =} \FunctionTok{c}\NormalTok{(}
        \StringTok{"1"} \OtherTok{=} \StringTok{"Original"}\NormalTok{, }\StringTok{"2"} \OtherTok{=} \StringTok{"Promax"}\NormalTok{, }\StringTok{"3"} \OtherTok{=} \StringTok{"Varimax"}
\NormalTok{    ))) }\SpecialCharTok{+}
    \FunctionTok{coord\_fixed}\NormalTok{(}\AttributeTok{ratio =} \DecValTok{1}\NormalTok{) }\CommentTok{\# make aspect ratio of each facet 1}

\NormalTok{flag\_gg}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{22-multivariate_files/figure-latex/unnamed-chunk-18-3} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# promax and varimax did a good job to assign trait to a particular factor}

\NormalTok{factor\_mle\_1 }\OtherTok{\textless{}{-}} \FunctionTok{fa}\NormalTok{(}
\NormalTok{    Harman}\FloatTok{.5}\NormalTok{,}
    \AttributeTok{nfactors =} \DecValTok{1}\NormalTok{,}
    \AttributeTok{fm =} \StringTok{"mle"}\NormalTok{,}
    \AttributeTok{rotate =} \StringTok{"none"}\NormalTok{,}
    \AttributeTok{SMC =} \ConstantTok{TRUE}
\NormalTok{)}
\NormalTok{factor\_mle\_1}
\CommentTok{\#\textgreater{} Factor Analysis using method =  ml}
\CommentTok{\#\textgreater{} Call: fa(r = Harman.5, nfactors = 1, rotate = "none", SMC = TRUE, fm = "mle")}
\CommentTok{\#\textgreater{} Standardized loadings (pattern matrix) based upon correlation matrix}
\CommentTok{\#\textgreater{}               ML1    h2     u2 com}
\CommentTok{\#\textgreater{} population   0.97 0.950 0.0503   1}
\CommentTok{\#\textgreater{} schooling    0.14 0.021 0.9791   1}
\CommentTok{\#\textgreater{} employment   1.00 0.995 0.0049   1}
\CommentTok{\#\textgreater{} professional 0.51 0.261 0.7388   1}
\CommentTok{\#\textgreater{} housevalue   0.12 0.014 0.9864   1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}                 ML1}
\CommentTok{\#\textgreater{} SS loadings    2.24}
\CommentTok{\#\textgreater{} Proportion Var 0.45}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Mean item complexity =  1}
\CommentTok{\#\textgreater{} Test of the hypothesis that 1 factor is sufficient.}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} df null model =  10  with the objective function =  6.38 with Chi Square =  54.25}
\CommentTok{\#\textgreater{} df of  the model are 5  and the objective function was  3.14 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} The root mean square of the residuals (RMSR) is  0.41 }
\CommentTok{\#\textgreater{} The df corrected root mean square of the residuals is  0.57 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} The harmonic n.obs is  12 with the empirical chi square  39.41  with prob \textless{}  2e{-}07 }
\CommentTok{\#\textgreater{} The total n.obs was  12  with Likelihood Chi Square =  24.56  with prob \textless{}  0.00017 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Tucker Lewis Index of factoring reliability =  0.022}
\CommentTok{\#\textgreater{} RMSEA index =  0.564  and the 90 \% confidence intervals are  0.374 0.841}
\CommentTok{\#\textgreater{} BIC =  12.14}
\CommentTok{\#\textgreater{} Fit based upon off diagonal values = 0.5}
\CommentTok{\#\textgreater{} Measures of factor score adequacy             }
\CommentTok{\#\textgreater{}                                                    ML1}
\CommentTok{\#\textgreater{} Correlation of (regression) scores with factors   1.00}
\CommentTok{\#\textgreater{} Multiple R square of scores with factors          1.00}
\CommentTok{\#\textgreater{} Minimum correlation of possible factor scores     0.99}

\NormalTok{factor\_mle\_2 }\OtherTok{\textless{}{-}} \FunctionTok{fa}\NormalTok{(}
\NormalTok{    Harman}\FloatTok{.5}\NormalTok{,}
    \AttributeTok{nfactors =} \DecValTok{2}\NormalTok{,}
    \AttributeTok{fm =} \StringTok{"mle"}\NormalTok{,}
    \AttributeTok{rotate =} \StringTok{"none"}\NormalTok{,}
    \AttributeTok{SMC =} \ConstantTok{TRUE}
\NormalTok{)}
\NormalTok{factor\_mle\_2}
\CommentTok{\#\textgreater{} Factor Analysis using method =  ml}
\CommentTok{\#\textgreater{} Call: fa(r = Harman.5, nfactors = 2, rotate = "none", SMC = TRUE, fm = "mle")}
\CommentTok{\#\textgreater{} Standardized loadings (pattern matrix) based upon correlation matrix}
\CommentTok{\#\textgreater{}                ML2  ML1   h2    u2 com}
\CommentTok{\#\textgreater{} population   {-}0.03 1.00 1.00 0.005 1.0}
\CommentTok{\#\textgreater{} schooling     0.90 0.04 0.81 0.193 1.0}
\CommentTok{\#\textgreater{} employment    0.09 0.98 0.96 0.036 1.0}
\CommentTok{\#\textgreater{} professional  0.78 0.46 0.81 0.185 1.6}
\CommentTok{\#\textgreater{} housevalue    0.96 0.05 0.93 0.074 1.0}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}                        ML2  ML1}
\CommentTok{\#\textgreater{} SS loadings           2.34 2.16}
\CommentTok{\#\textgreater{} Proportion Var        0.47 0.43}
\CommentTok{\#\textgreater{} Cumulative Var        0.47 0.90}
\CommentTok{\#\textgreater{} Proportion Explained  0.52 0.48}
\CommentTok{\#\textgreater{} Cumulative Proportion 0.52 1.00}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Mean item complexity =  1.1}
\CommentTok{\#\textgreater{} Test of the hypothesis that 2 factors are sufficient.}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} df null model =  10  with the objective function =  6.38 with Chi Square =  54.25}
\CommentTok{\#\textgreater{} df of  the model are 1  and the objective function was  0.31 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} The root mean square of the residuals (RMSR) is  0.01 }
\CommentTok{\#\textgreater{} The df corrected root mean square of the residuals is  0.05 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} The harmonic n.obs is  12 with the empirical chi square  0.05  with prob \textless{}  0.82 }
\CommentTok{\#\textgreater{} The total n.obs was  12  with Likelihood Chi Square =  2.22  with prob \textless{}  0.14 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Tucker Lewis Index of factoring reliability =  0.658}
\CommentTok{\#\textgreater{} RMSEA index =  0.307  and the 90 \% confidence intervals are  0 0.945}
\CommentTok{\#\textgreater{} BIC =  {-}0.26}
\CommentTok{\#\textgreater{} Fit based upon off diagonal values = 1}
\CommentTok{\#\textgreater{} Measures of factor score adequacy             }
\CommentTok{\#\textgreater{}                                                    ML2  ML1}
\CommentTok{\#\textgreater{} Correlation of (regression) scores with factors   0.98 1.00}
\CommentTok{\#\textgreater{} Multiple R square of scores with factors          0.95 1.00}
\CommentTok{\#\textgreater{} Minimum correlation of possible factor scores     0.91 0.99}

\NormalTok{factor\_mle\_3 }\OtherTok{\textless{}{-}} \FunctionTok{fa}\NormalTok{(}
\NormalTok{    Harman}\FloatTok{.5}\NormalTok{,}
    \AttributeTok{nfactors =} \DecValTok{3}\NormalTok{,}
    \AttributeTok{fm =} \StringTok{"mle"}\NormalTok{,}
    \AttributeTok{rotate =} \StringTok{"none"}\NormalTok{,}
    \AttributeTok{SMC =} \ConstantTok{TRUE}
\NormalTok{)}
\NormalTok{factor\_mle\_3}
\CommentTok{\#\textgreater{} Factor Analysis using method =  ml}
\CommentTok{\#\textgreater{} Call: fa(r = Harman.5, nfactors = 3, rotate = "none", SMC = TRUE, fm = "mle")}
\CommentTok{\#\textgreater{} Standardized loadings (pattern matrix) based upon correlation matrix}
\CommentTok{\#\textgreater{}                ML2  ML1   ML3   h2     u2 com}
\CommentTok{\#\textgreater{} population   {-}0.12 0.98 {-}0.11 0.98 0.0162 1.1}
\CommentTok{\#\textgreater{} schooling     0.89 0.15  0.29 0.90 0.0991 1.3}
\CommentTok{\#\textgreater{} employment    0.00 1.00  0.04 0.99 0.0052 1.0}
\CommentTok{\#\textgreater{} professional  0.72 0.52 {-}0.10 0.80 0.1971 1.9}
\CommentTok{\#\textgreater{} housevalue    0.97 0.13 {-}0.09 0.97 0.0285 1.1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}                        ML2  ML1  ML3}
\CommentTok{\#\textgreater{} SS loadings           2.28 2.26 0.11}
\CommentTok{\#\textgreater{} Proportion Var        0.46 0.45 0.02}
\CommentTok{\#\textgreater{} Cumulative Var        0.46 0.91 0.93}
\CommentTok{\#\textgreater{} Proportion Explained  0.49 0.49 0.02}
\CommentTok{\#\textgreater{} Cumulative Proportion 0.49 0.98 1.00}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Mean item complexity =  1.2}
\CommentTok{\#\textgreater{} Test of the hypothesis that 3 factors are sufficient.}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} df null model =  10  with the objective function =  6.38 with Chi Square =  54.25}
\CommentTok{\#\textgreater{} df of  the model are {-}2  and the objective function was  0 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} The root mean square of the residuals (RMSR) is  0 }
\CommentTok{\#\textgreater{} The df corrected root mean square of the residuals is  NA }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} The harmonic n.obs is  12 with the empirical chi square  0  with prob \textless{}  NA }
\CommentTok{\#\textgreater{} The total n.obs was  12  with Likelihood Chi Square =  0  with prob \textless{}  NA }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Tucker Lewis Index of factoring reliability =  1.318}
\CommentTok{\#\textgreater{} Fit based upon off diagonal values = 1}
\CommentTok{\#\textgreater{} Measures of factor score adequacy             }
\CommentTok{\#\textgreater{}                                                    ML2  ML1  ML3}
\CommentTok{\#\textgreater{} Correlation of (regression) scores with factors   0.99 1.00 0.82}
\CommentTok{\#\textgreater{} Multiple R square of scores with factors          0.98 1.00 0.68}
\CommentTok{\#\textgreater{} Minimum correlation of possible factor scores     0.96 0.99 0.36}
\end{Highlighting}
\end{Shaded}

The output info for the null hypothesis of no common factors is in the statement ``The degrees of freedom for the null model ..''

The output info for the null hypothesis that number of factors is sufficient is in the statement ``The total number of observations was \ldots{}''

One factor is not enough, two is sufficient, and not enough data for 3 factors (df of -2 and NA for p-value). Hence, we should use 2-factor model.

\hypertarget{discriminant-analysis}{%
\section{Discriminant Analysis}\label{discriminant-analysis}}

Suppose we have two or more different populations from which observations could come from. Discriminant analysis seeks to determine which of the possible population an observation comes from while making as few mistakes as possible

\begin{itemize}
\item
  This is an alternative to logistic approaches with the following advantages:

  \begin{itemize}
  \item
    when there is clear separation between classes, the parameter estimates for the logic regression model can be \textbf{surprisingly} unstable, while discriminant approaches do not suffer
  \item
    If X is normal in each of the classes and the sample size is small, then discriminant approaches can be more accurate
  \end{itemize}
\end{itemize}

Notation

Similar to MANOVA, let \(\mathbf{y}_{j1},\mathbf{y}_{j2},\dots, \mathbf{y}_{in_j} \sim iid f_j (\mathbf{y})\) for \(j = 1,\dots, h\)

Let \(f_j(\mathbf{y})\) be the density function for population j . Note that each vector \(\mathbf{y}\) contain measurements on all \(p\) traits

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Assume that each observation is from one of \(h\) possible populations.
\item
  We want to form a discriminant rule that will allocate an observation \(\mathbf{y}\) to population j when \(\mathbf{y}\) is in fact from this population
\end{enumerate}

\hypertarget{known-populations}{%
\subsection{Known Populations}\label{known-populations}}

The maximum likelihood discriminant rule for assigning an observation \(\mathbf{y}\) to one of the \(h\) populations allocates \(\mathbf{y}\) to the population that gives the largest likelihood to \(\mathbf{y}\)

Consider the likelihood for a single observation \(\mathbf{y}\), which has the form \(f_j (\mathbf{y})\) where j is the true population.

Since \(j\) is unknown, to make the likelihood as large as possible, we should choose the value j which causes \(f_j (\mathbf{y})\) to be as large as possible

Consider a simple univariate example. Suppose we have data from one of two binomial populations.

\begin{itemize}
\item
  The first population has \(n= 10\) trials with success probability \(p = .5\)
\item
  The second population has \(n= 10\) trials with success probability \(p = .7\)
\item
  to which population would we assign an observation of \(y = 7\)
\item
  Note:

  \begin{itemize}
  \item
    \(f(y = 7|n = 10, p = .5) = .117\)
  \item
    \(f(y = 7|n = 10, p = .7) = .267\) where \(f(.)\) is the binomial likelihood.
  \item
    Hence, we choose the second population
  \end{itemize}
\end{itemize}

Another example

We have 2 populations, where

\begin{itemize}
\item
  First population: \(N(\mu_1, \sigma^2_1)\)
\item
  Second population: \(N(\mu_2, \sigma^2_2)\)
\end{itemize}

The likelihood for a single observation is

\[
f_j (y) = (2\pi \sigma^2_j)^{-1/2} \exp\{ -\frac{1}{2}(\frac{y - \mu_j}{\sigma_j})^2\}
\]

Consider a likelihood ratio rule

\[
\begin{aligned}
\Lambda &= \frac{\text{likelihood of y from pop 1}}{\text{likelihood of y from pop 2}} \\
&= \frac{f_1(y)}{f_2(y)} \\
&= \frac{\sigma_2}{\sigma_1} \exp\{-\frac{1}{2}[(\frac{y - \mu_1}{\sigma_1})^2- (\frac{y - \mu_2}{\sigma_2})^2] \}
\end{aligned}
\]

Hence, we classify into

\begin{itemize}
\item
  pop 1 if \(\Lambda >1\)
\item
  pop 2 if \(\Lambda <1\)
\item
  for ties, flip a coin
\end{itemize}

Another way to think:

we classify into population 1 if the ``standardized distance'' of y from \(\mu_1\) is less than the ``standardized distance'' of y from \(\mu_2\) which is referred to as a \textbf{quadratic discriminant rule}.

(Significant simplification occurs in th special case where \(\sigma_1 = \sigma_2 = \sigma^2\))

Thus, we classify into population 1 if

\[
(y - \mu_2)^2 > (y - \mu_1)^2
\]

or

\[
|y- \mu_2| > |y - \mu_1|
\]

and

\[
-2 \log (\Lambda) = -2y  \frac{(\mu_1 - \mu_2)}{\sigma^2} + \frac{(\mu_1^2 - \mu_2^2)}{\sigma^2} = \beta y + \alpha
\]

Thus, we classify into population 1 if this is less than 0.

Discriminant classification rule is linear in y in this case.

\hypertarget{multivariate-expansion}{%
\subsubsection{Multivariate Expansion}\label{multivariate-expansion}}

Suppose that there are 2 populations

\begin{itemize}
\item
  \(N_p(\mathbf{\mu}_1, \mathbf{\Sigma}_1)\)
\item
  \(N_p(\mathbf{\mu}_2, \mathbf{\Sigma}_2)\)
\end{itemize}

\[
\begin{aligned}
-2 \log(\frac{f_1 (\mathbf{x})}{f_2 (\mathbf{x})}) &= \log|\mathbf{\Sigma}_1| + (\mathbf{x} - \mathbf{\mu}_1)' \mathbf{\Sigma}^{-1}_1 (\mathbf{x} - \mathbf{\mu}_1) \\
&- [\log|\mathbf{\Sigma}_2|+ (\mathbf{x} - \mathbf{\mu}_2)' \mathbf{\Sigma}^{-1}_2 (\mathbf{x} - \mathbf{\mu}_2) ]
\end{aligned}
\]

Again, we classify into population 1 if this is less than 0, otherwise, population 2. And like the univariate case with non-equal variances, this is a quadratic discriminant rule.

And if the covariance matrices are equal: \(\mathbf{\Sigma}_1 = \mathbf{\Sigma}_2 = \mathbf{\Sigma}_1\) classify into population 1 if

\[
(\mathbf{\mu}_1 - \mathbf{\mu}_2)' \mathbf{\Sigma}^{-1}\mathbf{x} - \frac{1}{2} (\mathbf{\mu}_1 - \mathbf{\mu}_2)' \mathbf{\Sigma}^{-1} (\mathbf{\mu}_1 - \mathbf{\mu}_2) \ge 0
\]

This linear discriminant rule is also referred to as \textbf{Fisher's linear discriminant function}

By \textbf{assuming the covariance matrices are equal, we assume that the shape and orientation fo the two populations must be the same (which can be a strong restriction)}

In other words, for each variable, it can have different mean but the same variance.

\begin{itemize}
\tightlist
\item
  Note: LDA Bayes decision boundary is linear. Hence, quadratic decision boundary might lead to better classification. Moreover, the assumption of same variance/covariance matrix across all classes for Gaussian densities imposes the linear rule, if we allow the predictors in each class to follow MVN distribution with class-specific mean vectors and variance/covariance matrices, then it is \textbf{Quadratic Discriminant Analysis.} But then, you will have more parameters to estimate (which gives more flexibility than LDA) at the cost of more variance (bias -variance tradeoff).
\end{itemize}

When \(\mathbf{\mu}_1, \mathbf{\mu}_2, \mathbf{\Sigma}\) are known, the probability of misclassification can be determined:

\[
\begin{aligned}
P(2|1) &= P(\text{calssify into pop 2| x is from pop 1}) \\
&= P((\mathbf{\mu}_1 - \mathbf{\mu}_2)' \mathbf{\Sigma}^{-1} \mathbf{x} \le \frac{1}{2} (\mathbf{\mu}_1 - \mathbf{\mu}_2)' \mathbf{\Sigma}^{-1} (\mathbf{\mu}_1 - \mathbf{\mu}_2)|\mathbf{x} \sim N(\mu_1, \mathbf{\Sigma}) \\
&= \Phi(-\frac{1}{2} \delta)
\end{aligned}
\]

where

\begin{itemize}
\item
  \(\delta^2 = (\mathbf{\mu}_1 - \mathbf{\mu}_2)' \mathbf{\Sigma}^{-1} (\mathbf{\mu}_1 - \mathbf{\mu}_2)\)
\item
  \(\Phi\) is the standard normal CDF
\end{itemize}

Suppose there are \(h\) possible populations, which are distributed as \(N_p (\mathbf{\mu}_p, \mathbf{\Sigma})\). Then, the maximum likelihood (linear) discriminant rule allocates \(\mathbf{y}\) to population j where j minimizes the squared Mahalanobis distance

\[
(\mathbf{y} - \mathbf{\mu}_j)' \mathbf{\Sigma}^{-1} (\mathbf{y} - \mathbf{\mu}_j)
\]

\hypertarget{bayes-discriminant-rules}{%
\subsubsection{Bayes Discriminant Rules}\label{bayes-discriminant-rules}}

If we know that population j has prior probabilities \(\pi_j\) (assume \(\pi_j >0\)) we can form the Bayes discriminant rule.

This rule allocates an observation \(\mathbf{y}\) to the population for which \(\pi_j f_j (\mathbf{y})\) is maximized.

Note:

\begin{itemize}
\tightlist
\item
  \textbf{Maximum likelihood discriminant rule} is a special case of the \textbf{Bayes discriminant rule}, where it sets all the \(\pi_j = 1/h\)
\end{itemize}

Optimal Properties of Bayes Discriminant Rules

\begin{itemize}
\item
  let \(p_{ii}\) be the probability of correctly assigning an observation from population i
\item
  then one rule (with probabilities \(p_{ii}\) ) is as good as another rule (with probabilities \(p_{ii}'\) ) if \(p_{ii} \ge p_{ii}'\) for all \(i = 1,\dots, h\)
\item
  The first rule is better than the alternative if \(p_{ii} > p_{ii}'\) for at least one i.
\item
  A rule for which there is no better alternative is called admissible
\item
  Bayes Discriminant Rules are admissible
\item
  If we utilized prior probabilities, then we can form the posterior probability of a correct allocation, \(\sum_{i=1}^h \pi_i p_{ii}\)
\item
  Bayes Discriminant Rules have the largest possible posterior probability of correct allocation with respect to the prior
\item
  These properties show that \textbf{Bayes Discriminant rule is our best approach}.
\end{itemize}

Unequal Cost

\begin{itemize}
\item
  We want to consider the cost misallocation

  \begin{itemize}
  \tightlist
  \item
    Define \(c_{ij}\) to be the cost associated with allocation a member of population j to population i.
  \end{itemize}
\item
  Assume that

  \begin{itemize}
  \item
    \(c_{ij} >0\) for all \(i \neq j\)
  \item
    \(c_{ij} = 0\) if \(i = j\)
  \end{itemize}
\item
  We could determine the expected amount of loss for an observation allocated to population i as \(\sum_j c_{ij} p_{ij}\) where the \(p_{ij}s\) are the probabilities of allocating an observation from population j into population i
\item
  We want to minimize the amount of loss expected for our rule. Using a Bayes Discrimination, allocate \(\mathbf{y}\) to the population j which minimizes \(\sum_{k \neq j} c_{ij} \pi_k f_k(\mathbf{y})\)
\item
  We could assign equal probabilities to each group and get a maximum likelihood type rule. here, we would allocate \(\mathbf{y}\) to population j which minimizes \(\sum_{k \neq j}c_{jk} f_k(\mathbf{y})\)
\end{itemize}

\textbf{Example}:

Two binomial populations, each of size 10, with probabilities \(p_1 = .5\) and \(p_2 = .7\)

And the probability of being in the first population is .9

However, suppose the cost of inappropriately allocating into the first population is 1 and the cost of incorrectly allocating into the second population is 5.

In this case, we pick population 1 over population 2

In general, we consider two regions, \(R_1\) and \(R_2\) associated with population 1 and 2:

\[
R_1: \frac{f_1 (\mathbf{x})}{f_2 (\mathbf{x})} \ge \frac{c_{12} \pi_2}{c_{21} \pi_1}
\]

\[
R_2: \frac{f_1 (\mathbf{x})}{f_2 (\mathbf{x})} < \frac{c_{12} \pi_2}{c_{21} \pi_1}
\]

where \(c_{12}\) is the cost of assigning a member of population 2 to population 1.

\hypertarget{discrimination-under-estimation}{%
\subsubsection{Discrimination Under Estimation}\label{discrimination-under-estimation}}

Suppose we know the form of the distributions for populations of interests, but we still have to estimate the parameters.

Example:

we know the distributions are multivariate normal, but we have to estimate the means and variances

The maximum likelihood discriminant rule allocates an observation \(\mathbf{y}\) to population j when j maximizes the function

\[
f_j (\mathbf{y} |\hat{\theta})
\]

where \(\hat{\theta}\) are the maximum likelihood estimates of the unknown parameters

For instance, we have 2 multivariate normal populations with distinct means, but common variance covariance matrix

MLEs for \(\mathbf{\mu}_1\) and \(\mathbf{\mu}_2\) are \(\mathbf{\bar{y}}_1\) and \(\mathbf{\bar{y}}_2\)and common \(\mathbf{\Sigma}\) is \(\mathbf{S}\).

Thus, an estimated discriminant rule could be formed by substituting these sample values for the population values

\hypertarget{native-bayes}{%
\subsubsection{Native Bayes}\label{native-bayes}}

\begin{itemize}
\item
  The challenge with classification using Bayes' is that we don't know the (true) densities, \(f_k, k = 1, \dots, K\), while LDA and QDA make \textbf{strong multivariate normality assumptions} to deal with this.
\item
  Naive Bayes makes only one assumption: \textbf{within the k-th class, the p predictors are independent (i.e,, for} \(k = 1,\dots, K\)
\end{itemize}

\[
f_k(x) = f_{k1}(x_1) \times f_{k2}(x_2) \times \dots \times f_{kp}(x_p)
\]

where \(f_{kj}\) is the density function of the j-th predictor among observation in the k-th class.

This assumption allows the use of joint distribution without the need to account for dependence between observations. However, this (native) assumption can be unrealistic, but still works well in cases where the number of sample (n) is not large relative to the number of features (p).

With this assumption, we have

\[
P(Y=k|X=x) = \frac{\pi_k \times f_{k1}(x_1) \times \dots \times f_{kp}(x_p)}{\sum_{l=1}^K \pi_l \times f_{l1}(x_1)\times \dots f_{lp}(x_p)}
\]

we only need to estimate the one-dimensional density function \(f_{kj}\) with either of these approaches:

\begin{itemize}
\item
  When \(X_j\) is quantitative, assume it has a univariate normal distribution (with independence): \(X_j | Y = k \sim N(\mu_{jk}, \sigma^2_{jk})\) which is more restrictive than QDA because it assumes predictors are independent (e.g., a diagonal covariance matrix)
\item
  When \(X_j\) is quantitative, use a kernel density estimator \protect\hyperlink{kernel-methods}{Kernel Methods} ; which is a smoothed histogram
\item
  When \(X_j\) is qualitative, we count the promotion of training observations for the j-th predictor corresponding to each class.
\end{itemize}

\hypertarget{comparison-of-classification-methods}{%
\subsubsection{Comparison of Classification Methods}\label{comparison-of-classification-methods}}

Assuming we have K classes and K is the baseline from (James , Witten, Hastie, and Tibshirani book)

Comparing the log odds relative to the K class

\hypertarget{logistic-regression-1}{%
\paragraph{Logistic Regression}\label{logistic-regression-1}}

\[
\log(\frac{P(Y=k|X = x)}{P(Y = K| X = x)}) = \beta_{k0} + \sum_{j=1}^p \beta_{kj}x_j
\]

\hypertarget{lda}{%
\paragraph{LDA}\label{lda}}

\[
\log(\frac{P(Y = k | X = x)}{P(Y = K | X = x)} = a_k + \sum_{j=1}^p b_{kj} x_j
\]

where \(a_k\) and \(b_{kj}\) are functions of \(\pi_k, \pi_K, \mu_k , \mu_K, \mathbf{\Sigma}\)

Similar to logistic regression, LDA assumes the log odds is linear in \(x\)

Even though they look like having the same form, the parameters in logistic regression are estimated by MLE, where as LDA linear parameters are specified by the prior and normal distributions

We expect LDA to outperform logistic regression when the normality assumption (approximately) holds, and logistic regression to perform better when it does not

\hypertarget{qda}{%
\paragraph{QDA}\label{qda}}

\[
\log(\frac{P(Y=k|X=x}{P(Y=K | X = x}) = a_k + \sum_{j=1}^{p}b_{kj}x_{j} + \sum_{j=1}^p \sum_{l=1}^p c_{kjl}x_j x_l 
\]

where \(a_k, b_{kj}, c_{kjl}\) are functions \(\pi_k , \pi_K, \mu_k, \mu_K ,\mathbf{\Sigma}_k, \mathbf{\Sigma}_K\)

\hypertarget{naive-bayes}{%
\paragraph{Naive Bayes}\label{naive-bayes}}

\[
\log (\frac{P(Y = k | X = x)}{P(Y = K | X = x}) = a_k + \sum_{j=1}^p g_{kj} (x_j)
\]

where \(a_k = \log (\pi_k / \pi_K)\) and \(g_{kj}(x_j) = \log(\frac{f_{kj}(x_j)}{f_{Kj}(x_j)})\) which is the form of generalized additive model

\hypertarget{summary-6}{%
\paragraph{Summary}\label{summary-6}}

\begin{itemize}
\item
  LDA is a special case of QDA
\item
  LDA is robust when it comes to high dimensions
\item
  Any classifier with a linear decision boundary is a special case of naive Bayes with \(g_{kj}(x_j) = b_{kj} x_j\), which means LDA is a special case of naive Bayes. LDA assumes that the features are normally distributed with a common within-class covariance matrix, and naive Bayes assumes independence of the features.
\item
  Naive bayes is also a special case of LDA with \(\mathbf{\Sigma}\) restricted to a diagonal matrix with diagonals, \(\sigma^2\) (another notation \(diag (\mathbf{\Sigma})\) ) assuming \(f_{kj}(x_j) = N(\mu_{kj}, \sigma^2_j)\)
\item
  QDA and naive Bayes are not special case of each other. In principal,e naive Bayes can produce a more flexible fit by the choice of \(g_{kj}(x_j)\) , but it's restricted to only purely additive fit, but QDA includes multiplicative terms of the form \(c_{kjl}x_j x_l\)
\item
  None of these methods uniformly dominates the others: the choice of method depends on the true distribution of the predictors in each of the K classes, n and p (i.e., related to the bias-variance tradeoff).
\end{itemize}

Compare to the non-parametric method (KNN)

\begin{itemize}
\item
  KNN would outperform both LDA and logistic regression when the decision boundary is highly nonlinear, but can't say which predictors are most important, and requires many observations
\item
  KNN is also limited in high-dimensions due to the curse of dimensionality
\item
  Since QDA is a special type of nonlinear decision boundary (quadratic), it can be considered as a compromise between the linear methods and KNN classification. QDA can have fewer training observations than KNN but not as flexible.
\end{itemize}

From simulation:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.6111}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.3889}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
True decision boundary
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Best performance
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Linear & LDA + Logistic regression \\
Moderately nonlinear & QDA + Naive Bayes \\
Highly nonlinear (many training, p is not large) & KNN \\
\end{longtable}

\begin{itemize}
\tightlist
\item
  like linear regression, we can also introduce flexibility by including transformed features \(\sqrt{X}, X^2, X^3\)
\end{itemize}

\hypertarget{probabilities-of-misclassification}{%
\subsection{Probabilities of Misclassification}\label{probabilities-of-misclassification}}

When the distribution are exactly known, we can determine the misclassification probabilities exactly. however, when we need to estimate the population parameters, we have to estimate the probability of misclassification

\begin{itemize}
\item
  Naive method

  \begin{itemize}
  \item
    Plugging the parameters estimates into the form for the misclassification probabilities results to derive at the estimates of the misclassification probability.
  \item
    But this will tend to be optimistic when the number of samples in one or more populations is small.
  \end{itemize}
\item
  Resubstitution method

  \begin{itemize}
  \item
    Use the proportion of the samples from population i that would be allocated to another population as an estimate of the misclassification probability
  \item
    But also optimistic when the number of samples is small
  \end{itemize}
\item
  Jack-knife estimates:

  \begin{itemize}
  \item
    The above two methods use observation to estimate both parameters and also misclassification probabilities based upon the discriminant rule
  \item
    Alternatively, we determine the discriminant rule based upon all of the data except the k-th observation from the j-th population
  \item
    then, determine if the k-th observation would be misclassified under this rule
  \item
    perform this process for all \(n_j\) observation in population j . An estimate fo the misclassification probability would be the fraction of \(n_j\) observations which were misclassified
  \item
    repeat the process for other \(i \neq j\) populations
  \item
    This method is more reliable than the others, but also computationally intensive
  \end{itemize}
\item
  Cross-Validation
\end{itemize}

\textbf{Summary}

Consider the group-specific densities \(f_j (\mathbf{x})\) for multivariate vector \(\mathbf{x}\).

Assume equal misclassifications costs, the Bayes classification probability of \(\mathbf{x}\) belonging to the j-th population is

\[
p(j |\mathbf{x}) = \frac{\pi_j f_j (\mathbf{x})}{\sum_{k=1}^h \pi_k f_k (\mathbf{x})}
\]

\(j = 1,\dots, h\)

where there are \(h\) possible groups.

We then classify into the group for which this probability of membership is largest

Alternatively, we can write this in terms of a \textbf{generalized squared distance} formation

\[
D_j^2 (\mathbf{x}) = d_j^2 (\mathbf{x})+ g_1(j) + g_2 (j)
\]

where

\begin{itemize}
\item
  \(d_j^2(\mathbf{x}) = (\mathbf{x} - \mathbf{\mu}_j)' \mathbf{V}_j^{-1} (\mathbf{x} - \mathbf{\mu}_j)\) is the squared Mahalanobis distance from \(\mathbf{x}\) to the centroid of group j, and

  \begin{itemize}
  \item
    \(\mathbf{V}_j = \mathbf{S}_j\) if the within group covariance matrices are not equal
  \item
    \(\mathbf{V}_j = \mathbf{S}_p\) if a pooled covariance estimate is appropriate
  \end{itemize}
\end{itemize}

and

\[
g_1(j) =
\begin{cases}
\ln |\mathbf{S}_j| & \text{within group covariances are not equal} \\
0 & \text{pooled covariance}
\end{cases}
\]

\[
g_2(j) = 
\begin{cases}
-2 \ln \pi_j & \text{prior probabilities are not equal} \\
0 & \text{prior probabilities are equal}
\end{cases}
\]

then, the posterior probability of belonging to group j is

\[
p(j| \mathbf{x})  = \frac{\exp(-.5 D_j^2(\mathbf{x}))}{\sum_{k=1}^h \exp(-.5 D^2_k (\mathbf{x}))}
\]

where \(j = 1,\dots , h\)

and \(\mathbf{x}\) is classified into group j if \(p(j | \mathbf{x})\) is largest for \(j = 1,\dots,h\) (or, \(D_j^2(\mathbf{x})\) is smallest).

\hypertarget{assessing-classification-performance}{%
\subsubsection{Assessing Classification Performance}\label{assessing-classification-performance}}

For binary classification, confusion matrix

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1765}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2353}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2353}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1029}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Predicted class
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
& & - or Null & + or Null & Total \\
True Class & - or Null & True Neg (TN) & False Pos (FP) & N \\
& + or Null & False Neg (FN) & True Pos (TP) & P \\
& Total & N* & P* & \\
\end{longtable}

and table 4.6 from \citep{james2013}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2535}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2535}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4930}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Synonyms
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
False Pos rate & FP/N & Type I error, 1 0 Specificity \\
True Pos. rate & TP/P & 1 - Type II error, power, sensitivity, recall \\
Pos Pred. value & TP/P* & Precision, 1 - false discovery promotion \\
Neg. Pred. value & TN/N* & \\
\end{longtable}

ROC curve (receiver Operating Characteristics) is a graphical comparison between \textbf{sensitivity} (true positive) and \textbf{specificity} ( = 1 - false positive)

y-axis = true positive rate

x-axis = false positive rate

as we change the threshold rate for classifying an observation as from 0 to 1

AUC (area under the ROC) ideally would equal to 1, a bad classifier would have AUC = 0.5 (pure chance)

\hypertarget{unknown-populations-nonparametric-discrimination}{%
\subsection{Unknown Populations/ Nonparametric Discrimination}\label{unknown-populations-nonparametric-discrimination}}

When your multivariate data are not Gaussian, or known distributional form at all, we can use the following methods

\hypertarget{kernel-methods}{%
\subsubsection{Kernel Methods}\label{kernel-methods}}

We approximate \(f_j (\mathbf{x})\) by a kernel density estimate

\[
\hat{f}_j(\mathbf{x}) = \frac{1}{n_j} \sum_{i = 1}^{n_j} K_j (\mathbf{x} - \mathbf{x}_i)
\]

where

\begin{itemize}
\item
  \(K_j (.)\) is a kernel function satisfying \(\int K_j(\mathbf{z})d\mathbf{z} =1\)
\item
  \(\mathbf{x}_i\) , \(i = 1,\dots , n_j\) is a random sample from the j-th population.
\end{itemize}

Thus, after finding \(\hat{f}_j (\mathbf{x})\) for each of the \(h\) populations, the posterior probability of group membership is

\[
p(j |\mathbf{x}) = \frac{\pi_j \hat{f}_j (\mathbf{x})}{\sum_{k-1}^h \pi_k \hat{f}_k (\mathbf{x})}
\]

where \(j = 1,\dots, h\)

There are different choices for the kernel function:

\begin{itemize}
\item
  Uniform
\item
  Normal
\item
  Epanechnikov
\item
  Biweight
\item
  Triweight
\end{itemize}

We these kernels, we have to pick the ``radius'' (or variance, width, window width, bandwidth) of the kernel, which is a smoothing parameter (the larger the radius, the more smooth the kernel estimate of the density).

To select the smoothness parameter, we can use the following method

If we believe the populations were close to multivariate normal, then

\[
R = (\frac{4/(2p+1)}{n_j})^{1/(p+1}
\]

But since we do not know for sure, we might choose several different values and select one that vies the best out of sample or cross-validation discrimination.

Moreover, you also have to decide whether to use different kernel smoothness for different populations, which is similar to the individual and pooled covariances in the classical methodology.

\hypertarget{nearest-neighbor-methods}{%
\subsubsection{Nearest Neighbor Methods}\label{nearest-neighbor-methods}}

The nearest neighbor (also known as k-nearest neighbor) method performs the classification of a new observation vector based on the group membership of its nearest neighbors. In practice, we find

\[
d_{ij}^2 (\mathbf{x}, \mathbf{x}_i) = (\mathbf{x}, \mathbf{x}_i) V_j^{-1}(\mathbf{x}, \mathbf{x}_i)
\]

which is the distance between the vector \(\mathbf{x}\) and the \(i\)-th observation in group \(j\)

We consider different choices for \(\mathbf{V}_j\)

For example,

\[
\begin{aligned}
\mathbf{V}_j &= \mathbf{S}_p \\
\mathbf{V}_j &= \mathbf{S}_j \\
\mathbf{V}_j &= \mathbf{I} \\
\mathbf{V}_j &= diag (\mathbf{S}_p)
\end{aligned}
\]

We find the \(k\) observations that are closest to \(\mathbf{x}\) (where users pick \(k\)). Then we classify into the most common population, weighted by the prior.

\hypertarget{modern-discriminant-methods}{%
\subsubsection{Modern Discriminant Methods}\label{modern-discriminant-methods}}

\textbf{Note}:

Logistic regression (with or without random effects) is a flexible model-based procedure for classification between two populations.

The extension of logistic regression to the multi-group setting is polychotomous logistic regression (or, mulinomial regression).

The machine learning and pattern recognition are growing with strong focus on nonlinear discriminant analysis methods such as:

\begin{itemize}
\item
  radial basis function networks
\item
  support vector machines
\item
  multiplayer perceptrons (neural networks)
\end{itemize}

The general framework

\[
g_j (\mathbf{x}) = \sum_{l = 1}^m w_{jl}\phi_l (\mathbf{x}; \mathbf{\theta}_l) + w_{j0}
\]

where

\begin{itemize}
\item
  \(j = 1,\dots, h\)
\item
  \(m\) nonlinear basis functions \(\phi_l\), each of which has \(n_m\) parameters given by \(\theta_l = \{ \theta_{lk}: k = 1, \dots , n_m \}\)
\end{itemize}

We assign \(\mathbf{x}\) to the \(j\)-th population if \(g_j(\mathbf{x})\) is the maximum for all \(j = 1,\dots, h\)

Development usually focuses on the choice and estimation of the basis functions, \(\phi_l\) and the estimation of the weights \(w_{jl}\)

More details can be found \citep{webb2011statistical}

\hypertarget{application-5}{%
\subsection{Application}\label{application-5}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(class)}
\FunctionTok{library}\NormalTok{(klaR)}
\FunctionTok{library}\NormalTok{(MASS)}
\FunctionTok{library}\NormalTok{(tidyverse)}

\DocumentationTok{\#\# Read in the data}
\NormalTok{crops }\OtherTok{\textless{}{-}} \FunctionTok{read.table}\NormalTok{(}\StringTok{"images/crops.txt"}\NormalTok{)}
\FunctionTok{names}\NormalTok{(crops) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"crop"}\NormalTok{, }\StringTok{"y1"}\NormalTok{, }\StringTok{"y2"}\NormalTok{, }\StringTok{"y3"}\NormalTok{, }\StringTok{"y4"}\NormalTok{)}
\FunctionTok{str}\NormalTok{(crops)}
\CommentTok{\#\textgreater{} \textquotesingle{}data.frame\textquotesingle{}:    36 obs. of  5 variables:}
\CommentTok{\#\textgreater{}  $ crop: chr  "Corn" "Corn" "Corn" "Corn" ...}
\CommentTok{\#\textgreater{}  $ y1  : int  16 15 16 18 15 15 12 20 24 21 ...}
\CommentTok{\#\textgreater{}  $ y2  : int  27 23 27 20 15 32 15 23 24 25 ...}
\CommentTok{\#\textgreater{}  $ y3  : int  31 30 27 25 31 32 16 23 25 23 ...}
\CommentTok{\#\textgreater{}  $ y4  : int  33 30 26 23 32 15 73 25 32 24 ...}


\DocumentationTok{\#\# Read in test data}
\NormalTok{crops\_test }\OtherTok{\textless{}{-}} \FunctionTok{read.table}\NormalTok{(}\StringTok{"images/crops\_test.txt"}\NormalTok{)}
\FunctionTok{names}\NormalTok{(crops\_test) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"crop"}\NormalTok{, }\StringTok{"y1"}\NormalTok{, }\StringTok{"y2"}\NormalTok{, }\StringTok{"y3"}\NormalTok{, }\StringTok{"y4"}\NormalTok{)}
\FunctionTok{str}\NormalTok{(crops\_test)}
\CommentTok{\#\textgreater{} \textquotesingle{}data.frame\textquotesingle{}:    5 obs. of  5 variables:}
\CommentTok{\#\textgreater{}  $ crop: chr  "Corn" "Soybeans" "Cotton" "Sugarbeets" ...}
\CommentTok{\#\textgreater{}  $ y1  : int  16 21 29 54 32}
\CommentTok{\#\textgreater{}  $ y2  : int  27 25 24 23 32}
\CommentTok{\#\textgreater{}  $ y3  : int  31 23 26 21 62}
\CommentTok{\#\textgreater{}  $ y4  : int  33 24 28 54 16}
\end{Highlighting}
\end{Shaded}

\hypertarget{lda-1}{%
\subsubsection{LDA}\label{lda-1}}

Default prior is proportional to sample size and \texttt{lda} and \texttt{qda} do not fit a constant or intercept term

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# Linear discriminant analysis}
\NormalTok{lda\_mod }\OtherTok{\textless{}{-}} \FunctionTok{lda}\NormalTok{(crop }\SpecialCharTok{\textasciitilde{}}\NormalTok{ y1 }\SpecialCharTok{+}\NormalTok{ y2 }\SpecialCharTok{+}\NormalTok{ y3 }\SpecialCharTok{+}\NormalTok{ y4,}
               \AttributeTok{data =}\NormalTok{ crops)}
\NormalTok{lda\_mod}
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lda(crop \textasciitilde{} y1 + y2 + y3 + y4, data = crops)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Prior probabilities of groups:}
\CommentTok{\#\textgreater{}     Clover       Corn     Cotton   Soybeans Sugarbeets }
\CommentTok{\#\textgreater{}  0.3055556  0.1944444  0.1666667  0.1666667  0.1666667 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Group means:}
\CommentTok{\#\textgreater{}                  y1       y2       y3       y4}
\CommentTok{\#\textgreater{} Clover     46.36364 32.63636 34.18182 36.63636}
\CommentTok{\#\textgreater{} Corn       15.28571 22.71429 27.42857 33.14286}
\CommentTok{\#\textgreater{} Cotton     34.50000 32.66667 35.00000 39.16667}
\CommentTok{\#\textgreater{} Soybeans   21.00000 27.00000 23.50000 29.66667}
\CommentTok{\#\textgreater{} Sugarbeets 31.00000 32.16667 20.00000 40.50000}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients of linear discriminants:}
\CommentTok{\#\textgreater{}              LD1          LD2         LD3          LD4}
\CommentTok{\#\textgreater{} y1 {-}6.147360e{-}02  0.009215431 {-}0.02987075 {-}0.014680566}
\CommentTok{\#\textgreater{} y2 {-}2.548964e{-}02  0.042838972  0.04631489  0.054842132}
\CommentTok{\#\textgreater{} y3  1.642126e{-}02 {-}0.079471595  0.01971222  0.008938745}
\CommentTok{\#\textgreater{} y4  5.143616e{-}05 {-}0.013917423  0.05381787 {-}0.025717667}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Proportion of trace:}
\CommentTok{\#\textgreater{}    LD1    LD2    LD3    LD4 }
\CommentTok{\#\textgreater{} 0.7364 0.1985 0.0576 0.0075}

\DocumentationTok{\#\# Look at accuracy on the training data}
\NormalTok{lda\_fitted }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(lda\_mod,}\AttributeTok{newdata =}\NormalTok{ crops)}
\CommentTok{\# Contingency table}
\NormalTok{lda\_table }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ crops}\SpecialCharTok{$}\NormalTok{crop, }\AttributeTok{fitted =}\NormalTok{ lda\_fitted}\SpecialCharTok{$}\NormalTok{class)}
\NormalTok{lda\_table}
\CommentTok{\#\textgreater{}             fitted}
\CommentTok{\#\textgreater{} truth        Clover Corn Cotton Soybeans Sugarbeets}
\CommentTok{\#\textgreater{}   Clover          6    0      3        0          2}
\CommentTok{\#\textgreater{}   Corn            0    6      0        1          0}
\CommentTok{\#\textgreater{}   Cotton          3    0      1        2          0}
\CommentTok{\#\textgreater{}   Soybeans        0    1      1        3          1}
\CommentTok{\#\textgreater{}   Sugarbeets      1    1      0        2          2}
\CommentTok{\# accuracy of 0.5 is just random (not good)}

\DocumentationTok{\#\# Posterior probabilities of membership}
\NormalTok{crops\_post }\OtherTok{\textless{}{-}} \FunctionTok{cbind.data.frame}\NormalTok{(crops,}
                               \AttributeTok{crop\_pred =}\NormalTok{ lda\_fitted}\SpecialCharTok{$}\NormalTok{class,}
\NormalTok{                               lda\_fitted}\SpecialCharTok{$}\NormalTok{posterior)}
\NormalTok{crops\_post }\OtherTok{\textless{}{-}}\NormalTok{ crops\_post }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{missed =}\NormalTok{ crop }\SpecialCharTok{!=}\NormalTok{ crop\_pred)}
\FunctionTok{head}\NormalTok{(crops\_post)}
\CommentTok{\#\textgreater{}   crop y1 y2 y3 y4 crop\_pred     Clover      Corn    Cotton  Soybeans}
\CommentTok{\#\textgreater{} 1 Corn 16 27 31 33      Corn 0.08935164 0.4054296 0.1763189 0.2391845}
\CommentTok{\#\textgreater{} 2 Corn 15 23 30 30      Corn 0.07690181 0.4558027 0.1420920 0.2530101}
\CommentTok{\#\textgreater{} 3 Corn 16 27 27 26      Corn 0.09817815 0.3422454 0.1365315 0.3073105}
\CommentTok{\#\textgreater{} 4 Corn 18 20 25 23      Corn 0.10521511 0.3633673 0.1078076 0.3281477}
\CommentTok{\#\textgreater{} 5 Corn 15 15 31 32      Corn 0.05879921 0.5753907 0.1173332 0.2086696}
\CommentTok{\#\textgreater{} 6 Corn 15 32 32 15  Soybeans 0.09723648 0.3278382 0.1318370 0.3419924}
\CommentTok{\#\textgreater{}   Sugarbeets missed}
\CommentTok{\#\textgreater{} 1 0.08971545  FALSE}
\CommentTok{\#\textgreater{} 2 0.07219340  FALSE}
\CommentTok{\#\textgreater{} 3 0.11573442  FALSE}
\CommentTok{\#\textgreater{} 4 0.09546233  FALSE}
\CommentTok{\#\textgreater{} 5 0.03980738  FALSE}
\CommentTok{\#\textgreater{} 6 0.10109590   TRUE}
\CommentTok{\# posterior shows that posterior of corn membership is much higher than the prior}

\DocumentationTok{\#\# LOOCV}
\CommentTok{\# leave{-}one{-}out cross validation for linear discriminant analysis}
\CommentTok{\# cannot run the predict function using the object with CV = TRUE }
\CommentTok{\# because it returns the within sample predictions}
\NormalTok{lda\_cv }\OtherTok{\textless{}{-}} \FunctionTok{lda}\NormalTok{(crop }\SpecialCharTok{\textasciitilde{}}\NormalTok{ y1 }\SpecialCharTok{+}\NormalTok{ y2 }\SpecialCharTok{+}\NormalTok{ y3 }\SpecialCharTok{+}\NormalTok{ y4,}
              \AttributeTok{data =}\NormalTok{ crops, }\AttributeTok{CV =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\# Contingency table}
\NormalTok{lda\_table\_cv }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ crops}\SpecialCharTok{$}\NormalTok{crop, }\AttributeTok{fitted =}\NormalTok{ lda\_cv}\SpecialCharTok{$}\NormalTok{class)}
\NormalTok{lda\_table\_cv}
\CommentTok{\#\textgreater{}             fitted}
\CommentTok{\#\textgreater{} truth        Clover Corn Cotton Soybeans Sugarbeets}
\CommentTok{\#\textgreater{}   Clover          4    3      1        0          3}
\CommentTok{\#\textgreater{}   Corn            0    4      1        2          0}
\CommentTok{\#\textgreater{}   Cotton          3    0      0        2          1}
\CommentTok{\#\textgreater{}   Soybeans        0    1      1        3          1}
\CommentTok{\#\textgreater{}   Sugarbeets      2    1      0        2          1}

\DocumentationTok{\#\# Predict the test data}
\NormalTok{lda\_pred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(lda\_mod, }\AttributeTok{newdata =}\NormalTok{ crops\_test)}

\DocumentationTok{\#\# Make a contingency table with truth and most likely class}
\FunctionTok{table}\NormalTok{(}\AttributeTok{truth=}\NormalTok{crops\_test}\SpecialCharTok{$}\NormalTok{crop, }\AttributeTok{predict=}\NormalTok{lda\_pred}\SpecialCharTok{$}\NormalTok{class)}
\CommentTok{\#\textgreater{}             predict}
\CommentTok{\#\textgreater{} truth        Clover Corn Cotton Soybeans Sugarbeets}
\CommentTok{\#\textgreater{}   Clover          0    0      1        0          0}
\CommentTok{\#\textgreater{}   Corn            0    1      0        0          0}
\CommentTok{\#\textgreater{}   Cotton          0    0      0        1          0}
\CommentTok{\#\textgreater{}   Soybeans        0    0      0        1          0}
\CommentTok{\#\textgreater{}   Sugarbeets      1    0      0        0          0}
\end{Highlighting}
\end{Shaded}

LDA didn't do well on both within sample and out-of-sample data.

\hypertarget{qda-1}{%
\subsubsection{QDA}\label{qda-1}}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# Quadratic discriminant analysis}
\NormalTok{qda\_mod }\OtherTok{\textless{}{-}} \FunctionTok{qda}\NormalTok{(crop }\SpecialCharTok{\textasciitilde{}}\NormalTok{ y1 }\SpecialCharTok{+}\NormalTok{ y2 }\SpecialCharTok{+}\NormalTok{ y3 }\SpecialCharTok{+}\NormalTok{ y4,}
               \AttributeTok{data =}\NormalTok{ crops)}

\DocumentationTok{\#\# Look at accuracy on the training data}
\NormalTok{qda\_fitted }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(qda\_mod, }\AttributeTok{newdata =}\NormalTok{ crops)}
\CommentTok{\# Contingency table}
\NormalTok{qda\_table }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ crops}\SpecialCharTok{$}\NormalTok{crop, }\AttributeTok{fitted =}\NormalTok{ qda\_fitted}\SpecialCharTok{$}\NormalTok{class)}
\NormalTok{qda\_table}
\CommentTok{\#\textgreater{}             fitted}
\CommentTok{\#\textgreater{} truth        Clover Corn Cotton Soybeans Sugarbeets}
\CommentTok{\#\textgreater{}   Clover          9    0      0        0          2}
\CommentTok{\#\textgreater{}   Corn            0    7      0        0          0}
\CommentTok{\#\textgreater{}   Cotton          0    0      6        0          0}
\CommentTok{\#\textgreater{}   Soybeans        0    0      0        6          0}
\CommentTok{\#\textgreater{}   Sugarbeets      0    0      1        1          4}

\DocumentationTok{\#\# LOOCV}
\NormalTok{qda\_cv }\OtherTok{\textless{}{-}} \FunctionTok{qda}\NormalTok{(crop }\SpecialCharTok{\textasciitilde{}}\NormalTok{ y1 }\SpecialCharTok{+}\NormalTok{ y2 }\SpecialCharTok{+}\NormalTok{ y3 }\SpecialCharTok{+}\NormalTok{ y4,}
              \AttributeTok{data =}\NormalTok{ crops, }\AttributeTok{CV =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\# Contingency table}
\NormalTok{qda\_table\_cv }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ crops}\SpecialCharTok{$}\NormalTok{crop, }\AttributeTok{fitted =}\NormalTok{ qda\_cv}\SpecialCharTok{$}\NormalTok{class)}
\NormalTok{qda\_table\_cv}
\CommentTok{\#\textgreater{}             fitted}
\CommentTok{\#\textgreater{} truth        Clover Corn Cotton Soybeans Sugarbeets}
\CommentTok{\#\textgreater{}   Clover          9    0      0        0          2}
\CommentTok{\#\textgreater{}   Corn            3    2      0        0          2}
\CommentTok{\#\textgreater{}   Cotton          3    0      2        0          1}
\CommentTok{\#\textgreater{}   Soybeans        3    0      0        2          1}
\CommentTok{\#\textgreater{}   Sugarbeets      3    0      1        1          1}

\DocumentationTok{\#\# Predict the test data}
\NormalTok{qda\_pred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(qda\_mod, }\AttributeTok{newdata =}\NormalTok{ crops\_test)}
\DocumentationTok{\#\# Make a contingency table with truth and most likely class}
\FunctionTok{table}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ crops\_test}\SpecialCharTok{$}\NormalTok{crop, }\AttributeTok{predict =}\NormalTok{ qda\_pred}\SpecialCharTok{$}\NormalTok{class)}
\CommentTok{\#\textgreater{}             predict}
\CommentTok{\#\textgreater{} truth        Clover Corn Cotton Soybeans Sugarbeets}
\CommentTok{\#\textgreater{}   Clover          1    0      0        0          0}
\CommentTok{\#\textgreater{}   Corn            0    1      0        0          0}
\CommentTok{\#\textgreater{}   Cotton          0    0      1        0          0}
\CommentTok{\#\textgreater{}   Soybeans        0    0      0        1          0}
\CommentTok{\#\textgreater{}   Sugarbeets      0    0      0        0          1}
\end{Highlighting}
\end{Shaded}

\hypertarget{knn}{%
\subsubsection{KNN}\label{knn}}

\texttt{knn} uses design matrices of the features.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# Design matrices}
\NormalTok{X\_train }\OtherTok{\textless{}{-}}\NormalTok{ crops }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{crop)}
\NormalTok{X\_test }\OtherTok{\textless{}{-}}\NormalTok{ crops\_test }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{crop)}
\NormalTok{Y\_train }\OtherTok{\textless{}{-}}\NormalTok{ crops}\SpecialCharTok{$}\NormalTok{crop}
\NormalTok{Y\_test }\OtherTok{\textless{}{-}}\NormalTok{ crops\_test}\SpecialCharTok{$}\NormalTok{crop}

\DocumentationTok{\#\# Nearest neighbors with 2 neighbors}
\NormalTok{knn\_2 }\OtherTok{\textless{}{-}} \FunctionTok{knn}\NormalTok{(X\_train, X\_train, Y\_train, }\AttributeTok{k =} \DecValTok{2}\NormalTok{)}
\FunctionTok{table}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ Y\_train, }\AttributeTok{fitted =}\NormalTok{ knn\_2)}
\CommentTok{\#\textgreater{}             fitted}
\CommentTok{\#\textgreater{} truth        Clover Corn Cotton Soybeans Sugarbeets}
\CommentTok{\#\textgreater{}   Clover          8    0      2        1          0}
\CommentTok{\#\textgreater{}   Corn            0    6      0        1          0}
\CommentTok{\#\textgreater{}   Cotton          0    0      5        0          1}
\CommentTok{\#\textgreater{}   Soybeans        0    0      0        3          3}
\CommentTok{\#\textgreater{}   Sugarbeets      0    0      1        1          4}

\DocumentationTok{\#\# Accuracy}
\FunctionTok{mean}\NormalTok{(Y\_train}\SpecialCharTok{==}\NormalTok{knn\_2)}
\CommentTok{\#\textgreater{} [1] 0.7222222}

\DocumentationTok{\#\# Performance on test data}
\NormalTok{knn\_2\_test }\OtherTok{\textless{}{-}} \FunctionTok{knn}\NormalTok{(X\_train, X\_test, Y\_train, }\AttributeTok{k =} \DecValTok{2}\NormalTok{)}
\FunctionTok{table}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ Y\_test, }\AttributeTok{predict =}\NormalTok{ knn\_2\_test)}
\CommentTok{\#\textgreater{}             predict}
\CommentTok{\#\textgreater{} truth        Clover Corn Cotton Soybeans Sugarbeets}
\CommentTok{\#\textgreater{}   Clover          1    0      0        0          0}
\CommentTok{\#\textgreater{}   Corn            0    1      0        0          0}
\CommentTok{\#\textgreater{}   Cotton          0    0      0        0          1}
\CommentTok{\#\textgreater{}   Soybeans        0    0      0        1          0}
\CommentTok{\#\textgreater{}   Sugarbeets      0    0      0        0          1}

\DocumentationTok{\#\# Accuracy}
\FunctionTok{mean}\NormalTok{(Y\_test}\SpecialCharTok{==}\NormalTok{knn\_2\_test)}
\CommentTok{\#\textgreater{} [1] 0.8}

\DocumentationTok{\#\# Nearest neighbors with 3 neighbors}
\NormalTok{knn\_3 }\OtherTok{\textless{}{-}} \FunctionTok{knn}\NormalTok{(X\_train, X\_train, Y\_train, }\AttributeTok{k =} \DecValTok{3}\NormalTok{)}
\FunctionTok{table}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ Y\_train, }\AttributeTok{fitted =}\NormalTok{ knn\_3)}
\CommentTok{\#\textgreater{}             fitted}
\CommentTok{\#\textgreater{} truth        Clover Corn Cotton Soybeans Sugarbeets}
\CommentTok{\#\textgreater{}   Clover          8    0      2        1          0}
\CommentTok{\#\textgreater{}   Corn            0    6      0        1          0}
\CommentTok{\#\textgreater{}   Cotton          1    1      4        0          0}
\CommentTok{\#\textgreater{}   Soybeans        1    0      0        4          1}
\CommentTok{\#\textgreater{}   Sugarbeets      0    0      0        2          4}

\DocumentationTok{\#\# Accuracy}
\FunctionTok{mean}\NormalTok{(Y\_train}\SpecialCharTok{==}\NormalTok{knn\_3)}
\CommentTok{\#\textgreater{} [1] 0.7222222}

\DocumentationTok{\#\# Performance on test data}
\NormalTok{knn\_3\_test }\OtherTok{\textless{}{-}} \FunctionTok{knn}\NormalTok{(X\_train, X\_test, Y\_train, }\AttributeTok{k =} \DecValTok{3}\NormalTok{)}
\FunctionTok{table}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ Y\_test, }\AttributeTok{predict =}\NormalTok{ knn\_3\_test)}
\CommentTok{\#\textgreater{}             predict}
\CommentTok{\#\textgreater{} truth        Clover Corn Cotton Soybeans Sugarbeets}
\CommentTok{\#\textgreater{}   Clover          1    0      0        0          0}
\CommentTok{\#\textgreater{}   Corn            0    1      0        0          0}
\CommentTok{\#\textgreater{}   Cotton          0    0      1        0          0}
\CommentTok{\#\textgreater{}   Soybeans        0    0      0        1          0}
\CommentTok{\#\textgreater{}   Sugarbeets      0    0      0        0          1}

\DocumentationTok{\#\# Accuracy}
\FunctionTok{mean}\NormalTok{(Y\_test}\SpecialCharTok{==}\NormalTok{knn\_3\_test)}
\CommentTok{\#\textgreater{} [1] 1}
\end{Highlighting}
\end{Shaded}

\hypertarget{stepwise}{%
\subsubsection{Stepwise}\label{stepwise}}

Stepwise discriminant analysis using the \texttt{stepclass} in function in the \texttt{klaR} package.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{step }\OtherTok{\textless{}{-}} \FunctionTok{stepclass}\NormalTok{(}
\NormalTok{    crop }\SpecialCharTok{\textasciitilde{}}\NormalTok{ y1 }\SpecialCharTok{+}\NormalTok{ y2 }\SpecialCharTok{+}\NormalTok{ y3 }\SpecialCharTok{+}\NormalTok{ y4,}
    \AttributeTok{data =}\NormalTok{ crops,}
    \AttributeTok{method =} \StringTok{"qda"}\NormalTok{,}
    \AttributeTok{improvement =} \FloatTok{0.15}
\NormalTok{)}
\CommentTok{\#\textgreater{} correctness rate: 0.45;  in: "y1";  variables (1): y1 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  hr.elapsed min.elapsed sec.elapsed }
\CommentTok{\#\textgreater{}        0.00        0.00        0.16}

\NormalTok{step}\SpecialCharTok{$}\NormalTok{process}
\CommentTok{\#\textgreater{}    step var varname result.pm}
\CommentTok{\#\textgreater{} 0 start   0      {-}{-}      0.00}
\CommentTok{\#\textgreater{} 1    in   1      y1      0.45}

\NormalTok{step}\SpecialCharTok{$}\NormalTok{performance.measure}
\CommentTok{\#\textgreater{} [1] "correctness rate"}
\end{Highlighting}
\end{Shaded}

Iris Data

\begin{Shaded}
\begin{Highlighting}[]

\FunctionTok{library}\NormalTok{(dplyr)}
\FunctionTok{data}\NormalTok{(}\StringTok{\textquotesingle{}iris\textquotesingle{}}\NormalTok{)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{samp }\OtherTok{\textless{}{-}}
    \FunctionTok{sample.int}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(iris), }\AttributeTok{size =} \FunctionTok{floor}\NormalTok{(}\FloatTok{0.70} \SpecialCharTok{*} \FunctionTok{nrow}\NormalTok{(iris)), }\AttributeTok{replace =}\NormalTok{ F)}

\NormalTok{train.iris }\OtherTok{\textless{}{-}}\NormalTok{ iris[samp,] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate\_if}\NormalTok{(is.numeric,scale)}
\NormalTok{test.iris }\OtherTok{\textless{}{-}}\NormalTok{ iris[}\SpecialCharTok{{-}}\NormalTok{samp,] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate\_if}\NormalTok{(is.numeric,scale)}

\FunctionTok{library}\NormalTok{(ggplot2)}
\NormalTok{iris.model }\OtherTok{\textless{}{-}} \FunctionTok{lda}\NormalTok{(Species }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ train.iris)}
\CommentTok{\#pred}
\NormalTok{pred.lda }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(iris.model, test.iris)}
\FunctionTok{table}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ test.iris}\SpecialCharTok{$}\NormalTok{Species, }\AttributeTok{prediction =}\NormalTok{ pred.lda}\SpecialCharTok{$}\NormalTok{class)}
\CommentTok{\#\textgreater{}             prediction}
\CommentTok{\#\textgreater{} truth        setosa versicolor virginica}
\CommentTok{\#\textgreater{}   setosa         15          0         0}
\CommentTok{\#\textgreater{}   versicolor      0         17         0}
\CommentTok{\#\textgreater{}   virginica       0          0        13}

\FunctionTok{plot}\NormalTok{(iris.model)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{22-multivariate_files/figure-latex/unnamed-chunk-24-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{iris.model.qda }\OtherTok{\textless{}{-}} \FunctionTok{qda}\NormalTok{(Species}\SpecialCharTok{\textasciitilde{}}\NormalTok{.,}\AttributeTok{data=}\NormalTok{train.iris)}
\CommentTok{\#pred}
\NormalTok{pred.qda }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(iris.model.qda,test.iris)}
\FunctionTok{table}\NormalTok{(}\AttributeTok{truth=}\NormalTok{test.iris}\SpecialCharTok{$}\NormalTok{Species,}\AttributeTok{prediction=}\NormalTok{pred.qda}\SpecialCharTok{$}\NormalTok{class)}
\CommentTok{\#\textgreater{}             prediction}
\CommentTok{\#\textgreater{} truth        setosa versicolor virginica}
\CommentTok{\#\textgreater{}   setosa         15          0         0}
\CommentTok{\#\textgreater{}   versicolor      0         16         1}
\CommentTok{\#\textgreater{}   virginica       0          0        13}
\end{Highlighting}
\end{Shaded}

\hypertarget{pca-with-discriminant-analysis}{%
\subsubsection{PCA with Discriminant Analysis}\label{pca-with-discriminant-analysis}}

we can use both PCA for dimension reduction in discriminant analysis

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{zeros }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{read.table}\NormalTok{(}\StringTok{"images/mnist0\_train\_b.txt"}\NormalTok{))}
\NormalTok{nines }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{read.table}\NormalTok{(}\StringTok{"images/mnist9\_train\_b.txt"}\NormalTok{))}
\NormalTok{train }\OtherTok{\textless{}{-}} \FunctionTok{rbind}\NormalTok{(zeros[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{1000}\NormalTok{, ], nines[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{1000}\NormalTok{, ])}
\NormalTok{train }\OtherTok{\textless{}{-}}\NormalTok{ train }\SpecialCharTok{/} \DecValTok{255} \CommentTok{\#divide by 255 per notes (so ranges from 0 to 1)}
\NormalTok{train }\OtherTok{\textless{}{-}} \FunctionTok{t}\NormalTok{(train) }\CommentTok{\#each column is an observation}
\FunctionTok{image}\NormalTok{(}\FunctionTok{matrix}\NormalTok{(train[, }\DecValTok{1}\NormalTok{], }\AttributeTok{nrow =} \DecValTok{28}\NormalTok{), }\AttributeTok{main =} \StringTok{\textquotesingle{}Example image, unrotated\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{22-multivariate_files/figure-latex/unnamed-chunk-25-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]


\NormalTok{test }\OtherTok{\textless{}{-}} \FunctionTok{rbind}\NormalTok{(zeros[}\DecValTok{2501}\SpecialCharTok{:}\DecValTok{3000}\NormalTok{, ], nines[}\DecValTok{2501}\SpecialCharTok{:}\DecValTok{3000}\NormalTok{, ])}
\NormalTok{test }\OtherTok{\textless{}{-}}\NormalTok{ test }\SpecialCharTok{/} \DecValTok{255}
\NormalTok{test }\OtherTok{\textless{}{-}} \FunctionTok{t}\NormalTok{(test)}
\NormalTok{y.train }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1000}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\DecValTok{9}\NormalTok{, }\DecValTok{1000}\NormalTok{))}
\NormalTok{y.test }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{500}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\DecValTok{9}\NormalTok{, }\DecValTok{500}\NormalTok{))}


\FunctionTok{library}\NormalTok{(MASS)}
\NormalTok{pc }\OtherTok{\textless{}{-}} \FunctionTok{prcomp}\NormalTok{(}\FunctionTok{t}\NormalTok{(train))}
\NormalTok{train.large }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(y.train, pc}\SpecialCharTok{$}\NormalTok{x[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{]))}
\NormalTok{large }\OtherTok{\textless{}{-}} \FunctionTok{lda}\NormalTok{(y.train }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ train.large)}
\CommentTok{\#the test data set needs to be constucted w/ the same 10 princomps}
\NormalTok{test.large }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(y.test, }\FunctionTok{predict}\NormalTok{(pc, }\FunctionTok{t}\NormalTok{(test))[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{]))}
\NormalTok{pred.lda }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(large, test.large)}
\FunctionTok{table}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ test.large}\SpecialCharTok{$}\NormalTok{y.test, }\AttributeTok{prediction =}\NormalTok{ pred.lda}\SpecialCharTok{$}\NormalTok{class)}
\CommentTok{\#\textgreater{}      prediction}
\CommentTok{\#\textgreater{} truth   0   9}
\CommentTok{\#\textgreater{}     0 491   9}
\CommentTok{\#\textgreater{}     9   5 495}

\NormalTok{large.qda }\OtherTok{\textless{}{-}} \FunctionTok{qda}\NormalTok{(y.train}\SpecialCharTok{\textasciitilde{}}\NormalTok{.,}\AttributeTok{data=}\NormalTok{train.large)}
\CommentTok{\#prediction}
\NormalTok{pred.qda }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(large.qda,test.large)}
\FunctionTok{table}\NormalTok{(}\AttributeTok{truth=}\NormalTok{test.large}\SpecialCharTok{$}\NormalTok{y.test,}\AttributeTok{prediction=}\NormalTok{pred.qda}\SpecialCharTok{$}\NormalTok{class)}
\CommentTok{\#\textgreater{}      prediction}
\CommentTok{\#\textgreater{} truth   0   9}
\CommentTok{\#\textgreater{}     0 493   7}
\CommentTok{\#\textgreater{}     9   3 497}
\end{Highlighting}
\end{Shaded}

\hypertarget{part-b.-quasi-experimental-design}{%
\part*{B. QUASI-EXPERIMENTAL DESIGN}\label{part-b.-quasi-experimental-design}}
\addcontentsline{toc}{part}{B. QUASI-EXPERIMENTAL DESIGN}

\hypertarget{quasi-experimental}{%
\chapter{Quasi-experimental}\label{quasi-experimental}}

In most cases, it means that you have pre- and post-intervention data.

Great resources for causal inference include \href{https://mixtape.scunning.com/introduction.html}{Causal Inference Mixtape} and \href{https://christinecai.github.io/PublicGoods/applied_micro_methods.pdf}{Recent Advances in Micro}, especially if you like to read about the history of causal inference as a field as well (codes for Stata, R, and Python).

Libraries in R:

\begin{itemize}
\item
  \href{https://cran.r-project.org/web/views/Econometrics.html}{Econometrics}
\item
  \href{https://cran.r-project.org/web/views/CausalInference.html}{Causal Inference}
\end{itemize}

\textbf{Identification strategy} for any quasi-experiment (No ways to prove or formal statistical test, but you can provide plausible argument and evidence)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Where the exogenous variation comes from (by argument and institutional knowledge)
\item
  Exclusion restriction: Evidence that the variation in the exogenous shock and the outcome is due to no other factors

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    The stable unit treatment value assumption (SUTVA) states that the treatment of unit \(i\) affect only the outcome of unit \(i\) (i.e., no spillover to the control groups)
  \end{enumerate}
\end{enumerate}

All quasi-experimental methods involve a tradeoff between power and support for the exogeneity assumption (i.e., discard variation in the data that is not exogenous).

Consequently, we don't usually look at \(R^2\) \citep{ebbes2011sense}. And it can even be misleading to use \(R^2\) as the basis for model comparison.

Clustering should be based on the design, not the expectations of correlation \citep{abadie2023should}. With a \textbf{small sample}, you should use the \textbf{wild bootstrap procedure} \citep{cameron2008bootstrap} to correct for the downward bias (see \citep{cai2022implementation}for additional assumptions).

Typical robustness check: recommended by \citep{goldfarb2022conducting}

\begin{itemize}
\item
  Different controls: show models with and without controls. Typically, we want to see the change in the estimate of interest. See \citep{altonji2005selection} for a formal assessment based on Rosenbaum bounds (i.e., changes in the estimate and threat of Omitted variables on the estimate). For specific applications in marketing, see \citep{manchanda2015social} \citep{shin2012fire}
\item
  Different functional forms
\item
  Different window of time (in longitudinal setting)
\item
  Different dependent variables (those that are related) or different measures of the dependent variables
\item
  Different control group size (matched vs.~un-matched samples)
\item
  Placebo tests: see each placebo test for each setting below.
\end{itemize}

Showing the mechanism:

\begin{itemize}
\item
  \protect\hyperlink{mediation}{Mediation} analysis
\item
  \protect\hyperlink{moderation}{Moderation} analysis

  \begin{itemize}
  \item
    Estimate the model separately (for different groups)
  \item
    Assess whether the three-way interaction between the source of variation (e.g., under DID, cross-sectional and time series) and group membership is significant.
  \end{itemize}
\end{itemize}

External Validity:

\begin{itemize}
\item
  Assess how representative your sample is
\item
  Explain the limitation of the design.
\item
  Use quasi-experimental results in conjunction with structural models: see \citep{anderson2015growth, einav2010beyond, chung2014bonuses}
\end{itemize}

Limitation

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What is your identifying assumptions or identification strategy
\item
  What are threats to the validity of your assumptions?
\item
  What you do to address it? And maybe how future research can do to address it.
\end{enumerate}

\hypertarget{natural-experiments}{%
\section{Natural Experiments}\label{natural-experiments}}

Reusing the same natural experiments for research, particularly when employing identical methods to determine the treatment effect in a given setting, can pose problems for hypothesis testing.

Simulations show that when \(N_{\text{Outcome}} >> N_{\text{True effect}}\), more than 50\% of statistically significant findings may be false positives \citep[p.2331]{heath2023reusing}.

\textbf{Solutions:}

\begin{itemize}
\item
  Bonferroni correction
\item
  \citet{romano2005stepwise} and \citet{romano2016efficient} correction: recommended
\item
  \citet{benjamini2001control} correction
\item
  Alternatively, refer to the rules of thumb from Table AI \citep[p.2356]{heath2023reusing}.
\end{itemize}

When applying multiple testing corrections, we can either use (but they will give similar results anyway \citep[p.2335]{heath2023reusing}):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Chronological Sequencing}: Outcomes are ordered by the date they were first reported, with multiple testing corrections applied in this sequence. This method progressively raises the statistical significance threshold as more outcomes are reviewed over time.
\item
  \textbf{Best Foot Forward Policy}: Outcomes are ordered from most to least likely to be rejected based on experimental data. Used primarily in clinical trials, this approach gives priority to intended treatment effects, which are subjected to less stringent statistical requirements. New outcomes are added to the sequence as they are linked to the primary treatment effect.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Romano{-}Wolf correction}
\FunctionTok{library}\NormalTok{(fixest)}
\FunctionTok{library}\NormalTok{(wildrwolf)}

\FunctionTok{head}\NormalTok{(iris)}
\CommentTok{\#\textgreater{}   Sepal.Length Sepal.Width Petal.Length Petal.Width Species}
\CommentTok{\#\textgreater{} 1          5.1         3.5          1.4         0.2  setosa}
\CommentTok{\#\textgreater{} 2          4.9         3.0          1.4         0.2  setosa}
\CommentTok{\#\textgreater{} 3          4.7         3.2          1.3         0.2  setosa}
\CommentTok{\#\textgreater{} 4          4.6         3.1          1.5         0.2  setosa}
\CommentTok{\#\textgreater{} 5          5.0         3.6          1.4         0.2  setosa}
\CommentTok{\#\textgreater{} 6          5.4         3.9          1.7         0.4  setosa}

\NormalTok{fit1 }\OtherTok{\textless{}{-}} \FunctionTok{feols}\NormalTok{(Sepal.Width }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Sepal.Length , }\AttributeTok{data =}\NormalTok{ iris)}
\NormalTok{fit2 }\OtherTok{\textless{}{-}} \FunctionTok{feols}\NormalTok{(Petal.Length }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Sepal.Length, }\AttributeTok{data =}\NormalTok{ iris)}
\NormalTok{fit3 }\OtherTok{\textless{}{-}} \FunctionTok{feols}\NormalTok{(Petal.Width }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Sepal.Length, }\AttributeTok{data =}\NormalTok{ iris)}

\NormalTok{res }\OtherTok{\textless{}{-}} \FunctionTok{rwolf}\NormalTok{(}
  \AttributeTok{models =} \FunctionTok{list}\NormalTok{(fit1, fit2, fit3), }
  \AttributeTok{param =} \StringTok{"Sepal.Length"}\NormalTok{,  }
  \AttributeTok{B =} \DecValTok{500}
\NormalTok{)}
\CommentTok{\#\textgreater{} }
  \SpecialCharTok{|}                                                                            
  \ErrorTok{|}                                                                      \ErrorTok{|}   \DecValTok{0}\NormalTok{\%}
  \SpecialCharTok{|}                                                                            
  \ErrorTok{|=======================}                                               \ErrorTok{|}  \DecValTok{33}\NormalTok{\%}
  \SpecialCharTok{|}                                                                            
  \ErrorTok{|===============================================}                       \ErrorTok{|}  \DecValTok{67}\NormalTok{\%}
  \SpecialCharTok{|}                                                                            
  \ErrorTok{|======================================================================|} \DecValTok{100}\NormalTok{\%}

\NormalTok{res}
\CommentTok{\#\textgreater{}   model   Estimate Std. Error   t value     Pr(\textgreater{}|t|) RW Pr(\textgreater{}|t|)}
\CommentTok{\#\textgreater{} 1     1 {-}0.0618848 0.04296699 {-}1.440287    0.1518983 0.131736527}
\CommentTok{\#\textgreater{} 2     2   1.858433 0.08585565  21.64602 1.038667e{-}47 0.001996008}
\CommentTok{\#\textgreater{} 3     3  0.7529176 0.04353017  17.29645 2.325498e{-}37 0.001996008}
\end{Highlighting}
\end{Shaded}

For all other tests, one can use \texttt{multtest::mt.rawp2adjp} which includes:

\begin{itemize}
\tightlist
\item
  Bonferroni
\item
  \citet{holm1979simple}
\item
  \citet{vsidak1967rectangular}
\item
  \citet{hochberg1988sharper}
\item
  \citet{benjamini1995controlling}
\item
  \citet{benjamini2001control}
\item
  Adaptive \citet{benjamini2000adaptive}
\item
  Two-stage \citet{benjamini2006adaptive}
\end{itemize}

Permutation adjusted p-values for simple multiple testing procedures

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# BiocManager::install("multtest")}
\FunctionTok{library}\NormalTok{(multtest)}

\NormalTok{procs }\OtherTok{\textless{}{-}}
    \FunctionTok{c}\NormalTok{(}\StringTok{"Bonferroni"}\NormalTok{,}
      \StringTok{"Holm"}\NormalTok{,}
      \StringTok{"Hochberg"}\NormalTok{,}
      \StringTok{"SidakSS"}\NormalTok{,}
      \StringTok{"SidakSD"}\NormalTok{,}
      \StringTok{"BH"}\NormalTok{,}
      \StringTok{"BY"}\NormalTok{,}
      \StringTok{"ABH"}\NormalTok{,}
      \StringTok{"TSBH"}\NormalTok{)}

\FunctionTok{mt.rawp2adjp}\NormalTok{(}
    \CommentTok{\# p{-}values}
    \FunctionTok{runif}\NormalTok{(}\DecValTok{10}\NormalTok{),}
\NormalTok{    procs) }\SpecialCharTok{|\textgreater{}}\NormalTok{ causalverse}\SpecialCharTok{::}\FunctionTok{nice\_tab}\NormalTok{()}
\CommentTok{\#\textgreater{}    adjp.rawp adjp.Bonferroni adjp.Holm adjp.Hochberg adjp.SidakSS adjp.SidakSD}
\CommentTok{\#\textgreater{} 1       0.12               1         1          0.75         0.72         0.72}
\CommentTok{\#\textgreater{} 2       0.22               1         1          0.75         0.92         0.89}
\CommentTok{\#\textgreater{} 3       0.24               1         1          0.75         0.94         0.89}
\CommentTok{\#\textgreater{} 4       0.29               1         1          0.75         0.97         0.91}
\CommentTok{\#\textgreater{} 5       0.36               1         1          0.75         0.99         0.93}
\CommentTok{\#\textgreater{} 6       0.38               1         1          0.75         0.99         0.93}
\CommentTok{\#\textgreater{} 7       0.44               1         1          0.75         1.00         0.93}
\CommentTok{\#\textgreater{} 8       0.59               1         1          0.75         1.00         0.93}
\CommentTok{\#\textgreater{} 9       0.65               1         1          0.75         1.00         0.93}
\CommentTok{\#\textgreater{} 10      0.75               1         1          0.75         1.00         0.93}
\CommentTok{\#\textgreater{}    adjp.BH adjp.BY adjp.ABH adjp.TSBH\_0.05 index h0.ABH h0.TSBH}
\CommentTok{\#\textgreater{} 1     0.63       1     0.63           0.63     2     10      10}
\CommentTok{\#\textgreater{} 2     0.63       1     0.63           0.63     6     10      10}
\CommentTok{\#\textgreater{} 3     0.63       1     0.63           0.63     8     10      10}
\CommentTok{\#\textgreater{} 4     0.63       1     0.63           0.63     3     10      10}
\CommentTok{\#\textgreater{} 5     0.63       1     0.63           0.63    10     10      10}
\CommentTok{\#\textgreater{} 6     0.63       1     0.63           0.63     1     10      10}
\CommentTok{\#\textgreater{} 7     0.63       1     0.63           0.63     7     10      10}
\CommentTok{\#\textgreater{} 8     0.72       1     0.72           0.72     9     10      10}
\CommentTok{\#\textgreater{} 9     0.72       1     0.72           0.72     5     10      10}
\CommentTok{\#\textgreater{} 10    0.75       1     0.75           0.75     4     10      10}
\end{Highlighting}
\end{Shaded}

\hypertarget{regression-discontinuity}{%
\chapter{Regression Discontinuity}\label{regression-discontinuity}}

\begin{itemize}
\item
  A regression discontinuity occurs when there is a discrete change (jump) in treatment likelihood in the distribution of a continuous (or roughly continuous) variable (i.e., \textbf{running/forcing/assignment variable}).

  \begin{itemize}
  \tightlist
  \item
    Running variable can also be time, but the argument for time to be continuous is hard to argue because usually we do not see increment of time (e.g., quarterly or annual data). Unless we have minute or hour data, then we might be able to argue for it.
  \end{itemize}
\item
  Review paper \citep{imbens2008regression, lee2010regression}
\item
  Other readings:

  \begin{itemize}
  \item
    \url{https://ies.ed.gov/ncee/wwc/Docs/ReferenceResources/wwc_rd.pdf}
  \item
    \url{https://ies.ed.gov/ncee/wwc/Docs/ReferenceResources/wwc_rdd_standards_122315.pdf}
  \end{itemize}
\item
  \citep{thistlethwaite1960}: first paper to use RD in the context of merit awards on future academic outcomes.
\item
  RD is a localized experiment at the cutoff point

  \begin{itemize}
  \tightlist
  \item
    Hence, we always have to qualify (perfunctory) our statement in research articles that ``our research might not generalize to beyond the bandwidth.''
  \end{itemize}
\item
  In reality, RD and experimental (from random assignment) estimates are very similar (\citep{chaplin2018internal}; \href{https://www.mathematica.org/publications/replicating-experimental-impact-estimates-using-a-regression-discontinuity-approach}{Mathematica}). But still, it's hard to prove empirically for every context (there might be future study that finds a huge difference between local estimate - causal - and overall estimate - random assignment.
\item
  Threats: only valid near threshold: inference at threshold is valid on average. Interestingly, random experiment showed the validity already.
\item
  Tradeoff between efficiency and bias
\item
  Regression discontinuity is under the framework of \protect\hyperlink{instrumental-variable}{Instrumental Variable} (structural IV) argued by \citep{angrist1999using} and a special case of the \protect\hyperlink{matching-methods}{Matching Methods} (matching at one point) argued by \citep{heckman1999economics}.
\item
  The hard part is to find a setting that can apply, but once you find one, it's easy to apply
\item
  We can also have multiple cutoff lines. However, for each cutoff line, there can only be one breakup point
\item
  RD can have multiple coinciding effects (i.e., joint distribution or bundled treatment), then RD effect in this case would be the joint effect.
\item
  As the running variable becomes more discrete your framework should be \protect\hyperlink{interrupted-time-series}{Interrupted Time Series}, but for more granular levels you can use RD. When you have infinite data (or substantially large) the two frameworks are identical. RD is always better than \protect\hyperlink{interrupted-time-series}{Interrupted Time Series}
\item
  Multiple alternative model specifications that produce consistent results are more reliable (parametric - linear regression with polynomials terms, and non-parametric - local linear regression). This is according to \citep{lee2010regression}, one straightforward method to ease the linearity assumption is by incorporating polynomial functions of the forcing variable. The choice of polynomial terms can be determined based on the data.

  \begin{itemize}
  \tightlist
  \item
    . According to \citep{gelman2019high}, accounting for global high-order polynomials presents three issues: (1) imprecise estimates due to noise, (2) sensitivity to the polynomial's degree, and (3) inadequate coverage of confidence intervals. To address this, researchers should instead employ estimators that rely on local linear or quadratic polynomials or other smooth functions.
  \end{itemize}
\item
  RD should be viewed more as a description of a data generating process, rather than a method or approach (similar to a randomized experiment)
\item
  RD is close to

  \begin{itemize}
  \item
    other quasi-experimental methods in the sense that it's based on the discontinuity at a threshold
  \item
    randomized experiments in the sense that it's local randomization.
  \end{itemize}
\end{itemize}

There are several types of Regression Discontinuity:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Sharp RD: Change in treatment probability at the cutoff point is 1

  \begin{itemize}
  \tightlist
  \item
    Kink design: Instead of a discontinuity in the level of running variable, we have a discontinuity in the slope of the function (while the function/level can remain continuous) \citep{nielsen2010estimating}. See \citep{bockerman2018kink} for application, and \citep{card2015inference} for theory.
  \end{itemize}
\item
  Kink RD
\item
  Fuzzy RD: Change in treatment probability less than 1
\item
  Fuzzy Kink RD
\item
  RDiT: running variable is time.
\end{enumerate}

Others:

\begin{itemize}
\item
  Multiple cutoff
\item
  Multiple Scores
\item
  Geographic RD
\item
  Dynamic Treatments
\item
  Continuous Treatments
\end{itemize}

Consider

\[
D_i = 1_{X_i > c}
\]

\[
D_i = 
\begin{cases}
D_i = 1 \text{ if } X_i > C \\
D_i = 0 \text{ if } X_i < C
\end{cases}
\]

where

\begin{itemize}
\item
  \(D_i\) = treatment effect
\item
  \(X_i\) = score variable (continuous)
\item
  \(c\) = cutoff point
\end{itemize}

\textbf{Identification (Identifying assumption}s) of RD:

Average Treatment Effect at the cutoff (\protect\hyperlink{continuity-based}{Continuity-based})

\[
\begin{aligned}
\alpha_{SRDD} &= E[Y_{1i} - Y_{0i} | X_i = c] \\
&= E[Y_{1i}|X_i = c] - E[Y_{0i}|X_i = c]\\
&= \lim_{x \to c^+} E[Y_{1i}|X_i = c] - \lim_{x \to c^=} E[Y_{0i}|X_i = c]
\end{aligned}
\]

Average Treatment Effect in a neighborhood (\protect\hyperlink{local-randomization-based}{Local Randomization-based}):

\[
\begin{aligned}
\alpha_{LR} &= E[Y_{1i} - Y_{0i}|X_i \in W] \\
&= \frac{1}{N_1} \sum_{X_i \in W, T_i = 1}Y_i - \frac{1}{N_0}\sum_{X_i \in W, T_i =0} Y_i
\end{aligned}
\]

RDD estimates the local average treatment effect (LATE), at the cutoff point which is not at the individual or population levels.

Since researchers typically care more about the internal validity, than external validity, localness affects only external validity.

\textbf{Assumptions}:

\begin{itemize}
\item
  Independent assignment
\item
  Continuity of conditional regression functions

  \begin{itemize}
  \tightlist
  \item
    \(E[Y(0)|X=x]\) and \(E[Y(1)|X=x]\) are continuous in x.
  \end{itemize}
\item
  RD is valid if cutpoint is \textbf{exogenous (i.e., no endogenous selection)} and running variable is \textbf{not manipulable}
\item
  Only treatment(s) (e.g., could be joint distribution of multiple treatments) cause discontinuity or jump in the outcome variable
\item
  All other factors are \textbf{smooth} through the cutoff (i.e., threshold) value. (we can also test this assumption by seeing no discontinuity in other factors). If they ``jump'', they will bias your causal estimate
\end{itemize}

\textbf{Threats to RD}

\begin{itemize}
\item
  Variables (other than treatment) change discontinuously at the cutoff

  \begin{itemize}
  \tightlist
  \item
    We can test for jumps in these variables (including pre-treatment outcome)
  \end{itemize}
\item
  Multiple discontinuities for the assignment variable
\item
  Manipulation of the assignment variable

  \begin{itemize}
  \tightlist
  \item
    At the cutoff point, check for continuity in the density of the assignment variable.
  \end{itemize}
\end{itemize}

\hypertarget{estimation-and-inference}{%
\section{Estimation and Inference}\label{estimation-and-inference}}

\hypertarget{local-randomization-based}{%
\subsection{Local Randomization-based}\label{local-randomization-based}}

\textbf{Additional Assumption}: Local Randomization approach assumes that inside the chosen window \(W = [c-w, c+w]\) are assigned to treatment as good as random:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Joint probability distribution of scores for units inside the chosen window \(W\) is known
\item
  Potential outcomes are not affected by value of the score
\end{enumerate}

This approach is stronger than the \protect\hyperlink{continuity-based}{Continuity-based} because we assume the regressions are continuously at \(c\) and unaffected by the running variable within window \(W\)

Because we can choose the window \(W\) (within which random assignment is plausible), the sample size can typically be small.

To choose the window \(W\), we can base on either

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  where the pre-treatment covariate-balance is observed
\item
  independent tests between outcome and score
\item
  domain knowledge
\end{enumerate}

To make inference, we can either use

\begin{itemize}
\item
  (Fisher) randomization inference
\item
  (Neyman) design-based
\end{itemize}

\hypertarget{continuity-based}{%
\subsection{Continuity-based}\label{continuity-based}}

\begin{itemize}
\item
  also known as the local polynomial method

  \begin{itemize}
  \tightlist
  \item
    as the name suggests, global polynomial regression is not recommended (because of lack of robustness, and over-fitting and Runge's phenomenon)
  \end{itemize}
\end{itemize}

Step to estimate local polynomial regression

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Choose polynomial order and weighting scheme
\item
  Choose bandwidth that has optimal MSE or coverage error
\item
  Estimate the parameter of interest
\item
  Examine robust bias-correct inference
\end{enumerate}

\hypertarget{specification-checks}{%
\section{Specification Checks}\label{specification-checks}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \protect\hyperlink{balance-checks}{Balance Checks}
\item
  \protect\hyperlink{sortingbunchingmanipulation}{Sorting/Bunching/Manipulation}
\item
  \protect\hyperlink{placebo-tests}{Placebo Tests}
\item
  \protect\hyperlink{sensitivity-to-bandwidth-choice}{Sensitivity to Bandwidth Choice}
\end{enumerate}

\hypertarget{balance-checks}{%
\subsection{Balance Checks}\label{balance-checks}}

\begin{itemize}
\item
  Also known as checking for Discontinuities in Average Covariates
\item
  Null Hypothesis: The average effect of covariates on pseudo outcomes (i.e., those qualitatively cannot be affected by the treatment) is 0.
\item
  If this hypothesis is rejected, you better have a good reason to why because it can cast serious doubt on your RD design.
\end{itemize}

\hypertarget{sortingbunchingmanipulation}{%
\subsection{Sorting/Bunching/Manipulation}\label{sortingbunchingmanipulation}}

\begin{itemize}
\item
  Also known as checking for A Discontinuity in the Distribution of the Forcing Variable
\item
  Also known as clustering or density test
\item
  Formal test is McCrary sorting test \citep{mccrary2008manipulation} or \citep{cattaneo2019practical}
\item
  Since human subjects can manipulate the running variable to be just above or below the cutoff (assuming that the running variable is manipulable), especially when the cutoff point is known in advance for all subjects, this can result in a discontinuity in the distribution of the running variable at the cutoff (i.e., we will see ``bunching'' behavior right before or after the cutoff)\textgreater{}

  \begin{itemize}
  \item
    People would like to sort into treatment if it's desirable. The density of the running variable would be 0 just below the threshold
  \item
    People would like to be out of treatment if it's undesirable
  \end{itemize}
\item
  \citep{mccrary2008manipulation} proposes a density test (i.e., a formal test for manipulation of the assignment variable).

  \begin{itemize}
  \item
    \(H_0\): The continuity of the density of the running variable (i.e., the covariate that underlies the assignment at the discontinuity point)
  \item
    \(H_a\): A jump in the density function at that point
  \item
    Even though it's not a requirement that the density of the running must be continuous at the cutoff, but a discontinuity can suggest manipulations.
  \end{itemize}
\item
  \citep{zhang2003estimation, lee2009training, aronow2019note} offers a guide to know when you should warrant the manipulation
\item
  Usually it's better to know your research design inside out so that you can suspect any manipulation attempts.

  \begin{itemize}
  \tightlist
  \item
    We would suspect the direction of the manipulation. And typically, it's one-way manipulation. In cases where we might have both ways, theoretically they would cancel each other out.
  \end{itemize}
\item
  We could also observe partial manipulation in reality (e.g., when subjects can only imperfectly manipulate). But typically, as we treat it like fuzzy RD, we would not have identification problems. But complete manipulation would lead to serious identification issues.
\item
  Remember: even in cases where we fail to reject the null hypothesis for the density test, we could not rule out completely that identification problem exists (just like any other hypotheses)
\item
  Bunching happens when people self-select to a specific value in the range of a variable (e.g., key policy thresholds).
\item
  Review paper \citep{kleven2016bunching}
\item
  \textbf{This test can only detect manipulation that changes the distribution of the running variable}. If you can choose the cutoff point or you have 2-sided manipulation, this test will fail to detect it.
\item
  Histogram in bunching is similar to a density curve (we want narrower bins, wider bins bias elasticity estimates)
\item
  We can also use bunching method to study individuals' or firm's responsiveness to changes in policy.
\item
  Under RD, we assume that we don't have any manipulation in the running variable. However, bunching behavior is a manipulation by firms or individuals. Thus, violating this assumption.

  \begin{itemize}
  \item
    Bunching can fix this problem by estimating what densities of individuals would have been without manipulation (i.e., manipulation-free counterfactual).
  \item
    \textbf{The fraction of persons who manipulated} is then calculated by comparing the observed distribution to manipulation-free counterfactual distributions.
  \item
    Under RD, we do not need this step because the observed and manipulation-free counterfactual distributions are assumed to be the same. RD assume there is no manipulation (i.e., assume the manipulation-free counterfactual distribution)
  \end{itemize}
\end{itemize}

When running variable and outcome variable are simultaneously determined, we can use a modified RDD estimator to have consistent estimate. \citep{bajari2011regression}

\begin{itemize}
\item
  \textbf{Assumptions}:

  \begin{itemize}
  \item
    Manipulation is \textbf{one-sided}: People move one way (i.e., either below the threshold to above the threshold or vice versa, but not to or away the threshold), which is similar to the monotonicity assumption under instrumental variable \ref{instrumental-variable}
  \item
    Manipulation is \textbf{bounded} (also known as regularity assumption): so that we can use people far away from this threshold to derive at our counterfactual distribution {[}\citet{blomquist2021bunching}{]}\citep{bertanha2021better}
  \end{itemize}
\end{itemize}

Steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Identify the window in which the running variable contains bunching behavior. We can do this step empirically based on \citet{bosch2020data}. Additionally robustness test is needed (i.e., varying the manipulation window).
\item
  Estimate the manipulation-free counterfactual
\item
  Calculating the standard errors for inference can follow \citep{chetty2016effects} where we bootstrap re-sampling residuals in the estimation of the counts of individuals within bins (large data can render this step unnecessary).
\end{enumerate}

If we pass the bunching test, we can move on to the \protect\hyperlink{placebo-test}{Placebo Test}

\citet{mccrary2008manipulation} test

A jump in the density at the threshold (i.e., discontinuity) hold can serve as evidence for sorting around the cutoff point

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rdd)}

\CommentTok{\# you only need the runing variable and the cutoff point}

\CommentTok{\# Example by the package\textquotesingle{}s authors}
\CommentTok{\#No discontinuity}
\NormalTok{x}\OtherTok{\textless{}{-}}\FunctionTok{runif}\NormalTok{(}\DecValTok{1000}\NormalTok{,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\FunctionTok{DCdensity}\NormalTok{(x,}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{24-regression-discontinuity_files/figure-latex/unnamed-chunk-1-1} \end{center}

\begin{verbatim}
#> [1] 0.8254815

#Discontinuity
x<-runif(1000,-1,1)
x<-x+2*(runif(1000,-1,1)>0&x<0)
DCdensity(x,0)
\end{verbatim}

\begin{center}\includegraphics[width=0.9\linewidth]{24-regression-discontinuity_files/figure-latex/unnamed-chunk-1-2} \end{center}

\begin{verbatim}
#> [1] 0.02607091
\end{verbatim}

\citet{cattaneo2019practical} test

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rddensity)}

\CommentTok{\# Example by the package\textquotesingle{}s authors}
\CommentTok{\# Continuous Density}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{2000}\NormalTok{, }\AttributeTok{mean =} \SpecialCharTok{{-}}\FloatTok{0.5}\NormalTok{)}
\NormalTok{rdd }\OtherTok{\textless{}{-}} \FunctionTok{rddensity}\NormalTok{(}\AttributeTok{X =}\NormalTok{ x, }\AttributeTok{vce =} \StringTok{"jackknife"}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(rdd)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Manipulation testing using local polynomial density estimation.}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of obs =       2000}
\CommentTok{\#\textgreater{} Model =               unrestricted}
\CommentTok{\#\textgreater{} Kernel =              triangular}
\CommentTok{\#\textgreater{} BW method =           estimated}
\CommentTok{\#\textgreater{} VCE method =          jackknife}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} c = 0                 Left of c           Right of c          }
\CommentTok{\#\textgreater{} Number of obs         1376                624                 }
\CommentTok{\#\textgreater{} Eff. Number of obs    354                 345                 }
\CommentTok{\#\textgreater{} Order est. (p)        2                   2                   }
\CommentTok{\#\textgreater{} Order bias (q)        3                   3                   }
\CommentTok{\#\textgreater{} BW est. (h)           0.514               0.609               }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Method                T                   P \textgreater{} |T|             }
\CommentTok{\#\textgreater{} Robust                {-}0.6798             0.4966              }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} P{-}values of binomial tests (H0: p=0.5).}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Window Length / 2          \textless{}c     \textgreater{}=c    P\textgreater{}|T|}
\CommentTok{\#\textgreater{} 0.036                      28      20    0.3123}
\CommentTok{\#\textgreater{} 0.072                      46      39    0.5154}
\CommentTok{\#\textgreater{} 0.107                      68      59    0.4779}
\CommentTok{\#\textgreater{} 0.143                      94      79    0.2871}
\CommentTok{\#\textgreater{} 0.179                     122     103    0.2301}
\CommentTok{\#\textgreater{} 0.215                     145     130    0.3986}
\CommentTok{\#\textgreater{} 0.250                     163     156    0.7370}
\CommentTok{\#\textgreater{} 0.286                     190     176    0.4969}
\CommentTok{\#\textgreater{} 0.322                     214     200    0.5229}
\CommentTok{\#\textgreater{} 0.358                     249     218    0.1650}

\CommentTok{\# you have to specify your own plot (read package manual)}
\end{Highlighting}
\end{Shaded}

\hypertarget{placebo-tests}{%
\subsection{Placebo Tests}\label{placebo-tests}}

\begin{itemize}
\item
  Also known as Discontinuities in Average Outcomes at Other Values
\item
  We should not see any jumps at other values (either \(X_i <c\) or \(X_i \ge c\))

  \begin{itemize}
  \tightlist
  \item
    Use the same bandwidth you use for the cutoff, and move it along the running variable: testing for a jump in the conditional mean of the outcome at the median of the running variable.
  \end{itemize}
\item
  Also known as falsification checks
\item
  Before and after the cutoff point, we can run the placebo test to see whether X's are different).
\item
  The placebo test is where you expect your coefficients to be not different from 0.
\item
  This test can be used for

  \begin{itemize}
  \item
    Testing no discontinuity in predetermined variables:
  \item
    Testing other discontinuities
  \item
    Placebo outcomes: we should see any changes in other outcomes that shouldn't have changed.
  \item
    Inclusion and exclusion of covariates: RDD parameter estimates should not be sensitive to the inclusion or exclusion of other covariates.
  \end{itemize}
\item
  This is analogous to \protect\hyperlink{experimental-design}{Experimental Design} where we cannot only test whether the observables are similar in both treatment and control groups (if we reject this, then we don't have random assignment), but we cannot test unobservables.
\end{itemize}

Balance on observable characteristics on both sides

\[
Z_i = \alpha_0 + \alpha_1 f(x_i) + [I(x_i \ge c)] \alpha_2 + [f(x_i) \times I(x_i \ge c)]\alpha_3 + u_i
\]

where

\begin{itemize}
\item
  \(x_i\) is the running variable
\item
  \(Z_i\) is other characteristics of people (e.g., age, etc)
\end{itemize}

Theoretically, \(Z_i\) should no be affected by treatment. Hence, \(E(\alpha_2) = 0\)

Moreover, when you have multiple \(Z_i\), you typically have to simulate joint distribution (to avoid having significant coefficient based on chance).

The only way that you don't need to generate joint distribution is when all \(Z_i\)'s are independent (unlikely in reality).

Under RD, you shouldn't have to do any \protect\hyperlink{matching-methods}{Matching Methods}. Because just like when you have random assignment, there is no need to make balanced dataset before and after the cutoff. If you have to do balancing, then your RD assumptions are probably wrong in the first place.

\hypertarget{sensitivity-to-bandwidth-choice}{%
\subsection{Sensitivity to Bandwidth Choice}\label{sensitivity-to-bandwidth-choice}}

\begin{itemize}
\item
  Methods for bandwidth selection

  \begin{itemize}
  \item
    Ad-hoc or substantively driven
  \item
    Data driven: cross validation
  \item
    Conservative approach: \citep{calonico2020optimal}
  \end{itemize}
\item
  The objective is to minimize the mean squared error between the estimated and actual treatment effects.
\item
  Then, we need to see how sensitive our results will be dependent on the choice of bandwidth.
\item
  In some cases, the best bandwidth for testing covariates may not be the best bandwidth for treating them, but it may be close.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# find optimal bandwidth by Imbens{-}Kalyanaraman}
\NormalTok{rdd}\SpecialCharTok{::}\FunctionTok{IKbandwidth}\NormalTok{(running\_var,}
\NormalTok{                 outcome\_var,}
                 \AttributeTok{cutpoint =} \StringTok{""}\NormalTok{,}
                 \AttributeTok{kernel =} \StringTok{"triangular"}\NormalTok{) }\CommentTok{\# can also pick other kernels}
\end{Highlighting}
\end{Shaded}

\hypertarget{manipulation-robust-regression-discontinuity-bounds}{%
\subsection{Manipulation Robust Regression Discontinuity Bounds}\label{manipulation-robust-regression-discontinuity-bounds}}

\begin{itemize}
\item
  \citet{mccrary2008manipulation} linked density jumps at cutoffs in RD studies to potential manipulation.

  \begin{itemize}
  \item
    If no jump is detected, researchers proceed with RD analysis; if detected, they halt using the cutoff for inference.
  \item
    Some studies use the ``doughnut-hole'' method, excluding near-cutoff observations and extrapolating, which contradicts RD principles.

    \begin{itemize}
    \item
      False negative could be due to a small sample size and can lead to biased estimates, as units near the cutoff may still differ in unobserved ways.
    \item
      Even correct rejections of no manipulation may overlook that the data can still be informative despite modest manipulation.
    \item
      \citet{gerard2020bounds} introduces a systematic approach to handle potentially manipulated variables in RD designs, addressing both concerns.
    \end{itemize}
  \end{itemize}
\item
  The model introduces two types of unobservable units in RD designs:

  \begin{itemize}
  \item
    \textbf{always-assigned} units, which are always on one side of the cutoff,
  \item
    \textbf{potentially-assigned} units, which fit traditional RD assumptions.

    \begin{itemize}
    \tightlist
    \item
      The standard RD model is a subset of this broader model, which assumes no always-assigned units.
    \end{itemize}
  \end{itemize}
\item
  Identifying assumption: manipulation occurs through one-sided selection.
\item
  The approach does not make a binary decision on manipulation in RD designs but assesses its extent and worst-case impact.
\end{itemize}

Two steps are used:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Determining the proportion of always-assigned units using the discontinuity at the cutoff
\item
  Bounding treatment effects based on the most extreme feasible outcomes for these units.
\end{enumerate}

\begin{itemize}
\item
  For sharp RD designs, bounds are established by trimming extreme outcomes near the cutoff; for fuzzy designs, the process involves more complex adjustments due to additional model constraints.
\item
  Extensions of the study use covariate information and economic behavior assumptions to refine these bounds and identify covariate distributions among unit types at the cutoff.
\end{itemize}

\textbf{Setup}

Independent data points \((X_i, Y_i, D_i)\), where \(X_i\) is the running variable, \(Y_i\) is the outcome, and \(D_i\) indicates treatment status (1 if treated, 0 otherwise). Treatment is assigned based on \(X_i \geq c\).

The design is \emph{sharp} if \(D_i = I(X_i \geq c)\) and \emph{fuzzy} otherwise.

The population is divided into:

\begin{itemize}
\item
  \textbf{Potentially-assigned units} (\(M_i = 0\)): Follow the standard RD framework, with potential outcomes \(Y_i(d)\) and potential treatment states \(D_i(x)\).
\item
  \textbf{Always-assigned units} (\(M_i = 1\)): These units do not require potential outcomes or states, and always have \(X_i\) values beyond the cutoff.
\end{itemize}

\textbf{Assumptions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Local Independence and Continuity}:

  \begin{itemize}
  \tightlist
  \item
    \(P(D = 1|X = c^+, M = 0) > P(D = 1|X = c^-, M = 0)\)
  \item
    No defiers: \(P(D^+ \geq D^-|X = c, M = 0) = 1\)
  \item
    Continuity in potential outcomes and states at \(c\).
  \item
    \(F_{X|M=0}(x)\) is differentiable at \(c\), with a positive derivative.
  \end{itemize}
\item
  \textbf{Smoothness of the Running Variable among Potentially-Assigned Units}:

  \begin{itemize}
  \tightlist
  \item
    The derivative of \(F_{X|M=0}(x)\) is continuous at \(c\).
  \end{itemize}
\item
  \textbf{Restrictions on Always-Assigned Units}:

  \begin{itemize}
  \tightlist
  \item
    \(P(X \geq c|M = 1) = 1\) and \(F_{X|M=1}(x)\) is right-differentiable (or left-differentiable) at \(c\).
  \item
    This (local) one-sided manipulation assumption allows identification of the proportion of always-assigned units among all units close to the cutoff.
  \end{itemize}
\end{enumerate}

When always-assigned unit exist, the RD design is fuzzy because we have

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Treated and untreated units among the potentially-assigned (below and above the cutoff)
\item
  Always-assigned units (above the cutoff).
\end{enumerate}

Causal Effects of Interest

causal effects among potentially-assigned units:

\[
\Gamma = E[Y(1) - Y(0) | X = c, D^+ > D^-, M = 0]
\]

This parameter represents the local average treatment effect (LATE) for the subgroup of ``compliers''---units that receive treatment if and only if their running variable \(X_i\) exceeds a certain cutoff.

The parameter \(\Gamma\) captures the causal effect of changes in the cutoff level on treatment status among potentially-assigned compliers.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.4291}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5709}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
RD designs with a manipulated running variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{``Doughnut-Hole'' RD Designs}:
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\begin{minipage}[t]{\linewidth}\raggedright
\begin{itemize}
\tightlist
\item
  Focuses on actual observations at the cutoff, not hypothetical true values.
\item
  Provides a direct and observable estimate of causal effects, without reliance on hypothetical constructs.
\end{itemize}
\end{minipage} & \begin{minipage}[t]{\linewidth}\raggedright
\begin{itemize}
\tightlist
\item
  Exclude observations around the cutoff and use extrapolation from the trends outside this excluded range to infer causal effects at the cutoff
\item
  Assumes a hypothetical population existing in a counterfactual scenario without manipulation.
\item
  Requires strong assumptions about the nature of manipulation and the minimal impact of extrapolation biases.
\end{itemize}
\end{minipage} \\
\end{longtable}

Identification of \(\tau\) in RD Designs

\begin{itemize}
\item
  Identification challenges arise due to the inability to distinguish always-assigned from potentially-assigned units, thus  is not point identified. We establish sharp bounds on 
\item
  These bounds are supported by the stochastic dominance of the potential outcome CDFs over observed distributions.
\end{itemize}

Unit Types and Notation:

\begin{itemize}
\tightlist
\item
  \(C_0\): Potentially-assigned compliers.
\item
  \(A_0\): Potentially-assigned always-takers.
\item
  \(N_0\): Potentially-assigned never-takers.
\item
  \(T_1\): Always-assigned treated units.
\item
  \(U_1\): Always-assigned untreated units.
\end{itemize}

The measure \(\tau\) , representing the proportion of always-assigned units near the cutoff, is point identified by the discontinuity in the observed running variable density \(f_X\) at the cutoff

Sharp RD:

\begin{itemize}
\item
  Units to the left of the cutoff are potentially assigned units. The distribution of their observed outcomes (\(Y\)) are the outcomes \(Y(0)\) of potentially-assigned compliers (\(C_0\)) at the cutoff.
\item
  To determine the bounds on the treatment effect (\(\Gamma\)), we need to assess the distribution of treated outcomes (\(Y(1)\)) for the same potentially-assigned compliers at the cutoff.
\item
  Information regarding the treated outcomes (\(Y(1)\)) comes exclusively from the subpopulation of treated units, which includes both potentially-assigned compliers (\(C_0\)) and those always assigned units (\(T_1\)).
\item
  With \(\tau\) point identified, we can estimate sharp bounds on \(\Gamma\).
\end{itemize}

Fuzzy RD:

\begin{longtable}[]{@{}ll@{}}
\caption{Note: Table on page 848 \citep{gerard2020bounds}}\tabularnewline
\toprule\noalign{}
Subpopulation & Types of units \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
Subpopulation & Types of units \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(X = c^+, D = 1\) & \(C_0, A_0, T_1\) \\
\(X = c^-, D = 1\) & \(A_0\) \\
\(X= c^+, D = 0\) & \(N_0, U_1\) \\
\(X = c^-, D = 0\) & \(C_0, N_0\) \\
\end{longtable}

\begin{itemize}
\item
  \textbf{Unit Types and Combinations}: There are five distinct unit types and four combinations of treatment assignments and decisions relevant to the analysis. These distinctions are important because they affect how potential outcomes are analyzed and bounded.
\item
  \textbf{Outcome Distributions}: The analysis involves estimating the distribution of potential outcomes (both treated and untreated) among potentially-assigned compliers at the cutoff.
\item
  \textbf{Three-Step Process}:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \item
    \textbf{Potential Outcomes Under Treatment}: Bounds on the distribution of treated outcomes are determined using data from treated units.
  \item
    \textbf{Potential Outcomes Under Non-Treatment}: Bounds on the distribution of untreated outcomes are derived using data from untreated units.
  \item
    \textbf{Bounds on Parameters of Interest}: Using the bounds from the first two steps, sharp upper and lower bounds on the local average treatment effect are derived.
  \end{enumerate}
\item
  \textbf{Extreme Value Consideration}: The bounds for treatment effects are based on ``extreme'' scenarios under worst-case assumptions about the distribution of potential outcomes, making them sharp but empirically relevant within the data constraints.
\end{itemize}

Extensions:

\begin{itemize}
\item
  \textbf{Quantile Treatment Effects}: alternative to average effects by focusing on different quantiles of the outcome distribution, which are less affected by extreme values.
\item
  \textbf{Applicability to Discrete Outcomes}
\item
  \textbf{Behavioral Assumptions Impact}: Assuming a high likelihood of treatment among always-assigned units can narrow the bounds of treatment effects by refining the analysis of potential outcomes.
\item
  \textbf{Utilization of Covariates}: Incorporating covariates measured prior to treatment can refine the bounds on treatment effects and help target policies by identifying covariate distributions among different unit types.
\end{itemize}

\textbf{Notes}:

\begin{itemize}
\item
  Quantile Treatment Effects (QTEs): QTE bounds are less sensitive to the tails of the outcome distribution, making them tighter than ATE bounds.

  \begin{itemize}
  \item
    Inference on ATEs is sensitive to the extent of manipulation, with confidence intervals widening significantly with small degrees of assumed manipulation.
  \item
    Inference on QTEs is less affected by manipulation, remaining meaningful even with larger degrees of manipulation.
  \end{itemize}
\item
  Alternative Inference Strategy when manipulation is believed to be unlikely. Try different hypothetical values of \(\tau\)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{devtools}\SpecialCharTok{::}\FunctionTok{install\_github}\NormalTok{(}\StringTok{"francoisgerard/rdbounds/R"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(formattable)}
\FunctionTok{library}\NormalTok{(data.table)}
\FunctionTok{library}\NormalTok{(rdbounds)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{rdbounds\_sampledata}\NormalTok{(}\DecValTok{10000}\NormalTok{, }\AttributeTok{covs =} \ConstantTok{FALSE}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] "True tau: 0.117999815082062"}
\CommentTok{\#\textgreater{} [1] "True treatment effect on potentially{-}assigned: 2"}
\CommentTok{\#\textgreater{} [1] "True treatment effect on right side of cutoff: 2.35399944524618"}
\FunctionTok{head}\NormalTok{(df)}
\CommentTok{\#\textgreater{}            x        y treatment}
\CommentTok{\#\textgreater{} 1 {-}1.2532616 3.489563         0}
\CommentTok{\#\textgreater{} 2 {-}0.5146925 3.365232         0}
\CommentTok{\#\textgreater{} 3  3.4853777 6.193533         0}
\CommentTok{\#\textgreater{} 4  0.1576616 8.820440         1}
\CommentTok{\#\textgreater{} 5  0.2890962 4.791972         0}
\CommentTok{\#\textgreater{} 6  3.8350019 7.316907         0}

\NormalTok{rdbounds\_est }\OtherTok{\textless{}{-}}
    \FunctionTok{rdbounds}\NormalTok{(}
        \AttributeTok{y =}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{y,}
        \AttributeTok{x =}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{x,}
        \CommentTok{\# covs = as.factor(df$cov),}
        \AttributeTok{treatment =}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{treatment,}
        \AttributeTok{c =} \DecValTok{0}\NormalTok{,}
        \AttributeTok{discrete\_x =} \ConstantTok{FALSE}\NormalTok{,}
        \AttributeTok{discrete\_y =} \ConstantTok{FALSE}\NormalTok{,}
        \AttributeTok{bwsx =} \FunctionTok{c}\NormalTok{(.}\DecValTok{2}\NormalTok{, .}\DecValTok{5}\NormalTok{),}
        \AttributeTok{bwy =} \DecValTok{1}\NormalTok{,}
        
        \CommentTok{\# for median effect use }
        \CommentTok{\# type = "qte", }
        \CommentTok{\# percentiles = .5, }
        
        \AttributeTok{kernel =} \StringTok{"epanechnikov"}\NormalTok{,}
        \AttributeTok{orders =} \DecValTok{1}\NormalTok{,}
        \AttributeTok{evaluation\_ys =} \FunctionTok{seq}\NormalTok{(}\AttributeTok{from =} \DecValTok{0}\NormalTok{, }\AttributeTok{to =} \DecValTok{15}\NormalTok{, }\AttributeTok{by =} \DecValTok{1}\NormalTok{),}
        \AttributeTok{refinement\_A =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{refinement\_B =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{right\_effects =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{yextremes =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{15}\NormalTok{),}
        \AttributeTok{num\_bootstraps =} \DecValTok{5}
\NormalTok{    )}
\CommentTok{\#\textgreater{} [1] "The proportion of always{-}assigned units just to the right of the cutoff is estimated to be 0.04209"}
\CommentTok{\#\textgreater{} [1] "2024{-}05{-}13 19:26:14 Estimating CDFs for point estimates"}
\CommentTok{\#\textgreater{} [1] "2024{-}05{-}13 19:26:14 .....Estimating CDFs for units just to the right of the cutoff"}
\CommentTok{\#\textgreater{} [1] "2024{-}05{-}13 19:26:16 Estimating CDFs with nudged tau (tau\_star)"}
\CommentTok{\#\textgreater{} [1] "2024{-}05{-}13 19:26:16 .....Estimating CDFs for units just to the right of the cutoff"}
\CommentTok{\#\textgreater{} [1] "2024{-}05{-}13 19:26:18 Beginning parallelized output by bootstrap.."}
\CommentTok{\#\textgreater{} [1] "2024{-}05{-}13 19:26:24 Computing Confidence Intervals"}
\CommentTok{\#\textgreater{} [1] "2024{-}05{-}13 19:26:33 Time taken:0.32 minutes"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rdbounds\_summary}\NormalTok{(rdbounds\_est, }\AttributeTok{title\_prefix =} \StringTok{"Sample Data Results"}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] "Time taken: 0.32 minutes"}
\CommentTok{\#\textgreater{} [1] "Sample size: 10000"}
\CommentTok{\#\textgreater{} [1] "Local Average Treatment Effect:"}
\CommentTok{\#\textgreater{} $tau\_hat}
\CommentTok{\#\textgreater{} [1] 0.04209028}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $tau\_hat\_CI}
\CommentTok{\#\textgreater{} [1] 0.1671043 0.7765031}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $takeup\_increase}
\CommentTok{\#\textgreater{} [1] 0.7521208}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $takeup\_increase\_CI}
\CommentTok{\#\textgreater{} [1] 0.7065353 0.7977063}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $TE\_SRD\_naive}
\CommentTok{\#\textgreater{} [1] 1.770963}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $TE\_SRD\_naive\_CI}
\CommentTok{\#\textgreater{} [1] 1.541314 2.000612}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $TE\_SRD\_bounds}
\CommentTok{\#\textgreater{} [1] 1.569194 1.912681}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $TE\_SRD\_CI}
\CommentTok{\#\textgreater{} [1] {-}0.1188634  3.5319468}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $TE\_SRD\_covs\_bounds}
\CommentTok{\#\textgreater{} [1] NA NA}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $TE\_SRD\_covs\_CI}
\CommentTok{\#\textgreater{} [1] NA NA}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $TE\_FRD\_naive}
\CommentTok{\#\textgreater{} [1] 2.356601}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $TE\_FRD\_naive\_CI}
\CommentTok{\#\textgreater{} [1] 1.995430 2.717772}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $TE\_FRD\_bounds}
\CommentTok{\#\textgreater{} [1] 1.980883 2.362344}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $TE\_FRD\_CI}
\CommentTok{\#\textgreater{} [1] {-}0.6950823  4.6112538}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $TE\_FRD\_bounds\_refinementA}
\CommentTok{\#\textgreater{} [1] 1.980883 2.357499}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $TE\_FRD\_refinementA\_CI}
\CommentTok{\#\textgreater{} [1] {-}0.6950823  4.6112538}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $TE\_FRD\_bounds\_refinementB}
\CommentTok{\#\textgreater{} [1] 1.980883 2.351411}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $TE\_FRD\_refinementB\_CI}
\CommentTok{\#\textgreater{} [1] {-}0.6152215  4.2390830}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $TE\_FRD\_covs\_bounds}
\CommentTok{\#\textgreater{} [1] NA NA}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $TE\_FRD\_covs\_CI}
\CommentTok{\#\textgreater{} [1] NA NA}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $TE\_SRD\_CIs\_manipulation}
\CommentTok{\#\textgreater{} [1] NA NA}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $TE\_FRD\_CIs\_manipulation}
\CommentTok{\#\textgreater{} [1] NA NA}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $TE\_SRD\_right\_bounds}
\CommentTok{\#\textgreater{} [1] 1.376392 2.007746}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $TE\_SRD\_right\_CI}
\CommentTok{\#\textgreater{} [1] {-}5.036752  5.889137}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $TE\_FRD\_right\_bounds}
\CommentTok{\#\textgreater{} [1] 1.721121 2.511504}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $TE\_FRD\_right\_CI}
\CommentTok{\#\textgreater{} [1] {-}6.663269  7.414185}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rdbounds\_est\_tau }\OtherTok{\textless{}{-}}
    \FunctionTok{rdbounds}\NormalTok{(}
        \AttributeTok{y =}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{y,}
        \AttributeTok{x =}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{x,}
        \CommentTok{\# covs = as.factor(df$cov),}
        \AttributeTok{treatment =}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{treatment,}
        \AttributeTok{c =} \DecValTok{0}\NormalTok{,}
        \AttributeTok{discrete\_x =} \ConstantTok{FALSE}\NormalTok{,}
        \AttributeTok{discrete\_y =} \ConstantTok{FALSE}\NormalTok{,}
        \AttributeTok{bwsx =} \FunctionTok{c}\NormalTok{(.}\DecValTok{2}\NormalTok{, .}\DecValTok{5}\NormalTok{),}
        \AttributeTok{bwy =} \DecValTok{1}\NormalTok{,}
        \AttributeTok{kernel =} \StringTok{"epanechnikov"}\NormalTok{,}
        \AttributeTok{orders =} \DecValTok{1}\NormalTok{,}
        \AttributeTok{evaluation\_ys =} \FunctionTok{seq}\NormalTok{(}\AttributeTok{from =} \DecValTok{0}\NormalTok{, }\AttributeTok{to =} \DecValTok{15}\NormalTok{, }\AttributeTok{by =} \DecValTok{1}\NormalTok{),}
        \AttributeTok{refinement\_A =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{refinement\_B =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{right\_effects =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{potential\_taus =} \FunctionTok{c}\NormalTok{(.}\DecValTok{025}\NormalTok{, .}\DecValTok{05}\NormalTok{, .}\DecValTok{1}\NormalTok{, .}\DecValTok{2}\NormalTok{),}
        \AttributeTok{yextremes =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{15}\NormalTok{),}
        \AttributeTok{num\_bootstraps =} \DecValTok{5}
\NormalTok{    )}
\CommentTok{\#\textgreater{} [1] "The proportion of always{-}assigned units just to the right of the cutoff is estimated to be 0.04209"}
\CommentTok{\#\textgreater{} [1] "2024{-}05{-}13 19:26:34 Estimating CDFs for point estimates"}
\CommentTok{\#\textgreater{} [1] "2024{-}05{-}13 19:26:34 .....Estimating CDFs for units just to the right of the cutoff"}
\CommentTok{\#\textgreater{} [1] "2024{-}05{-}13 19:26:35 Estimating CDFs with nudged tau (tau\_star)"}
\CommentTok{\#\textgreater{} [1] "2024{-}05{-}13 19:26:35 .....Estimating CDFs for units just to the right of the cutoff"}
\CommentTok{\#\textgreater{} [1] "2024{-}05{-}13 19:26:38 Beginning parallelized output by bootstrap.."}
\CommentTok{\#\textgreater{} [1] "2024{-}05{-}13 19:26:44 Estimating CDFs with fixed tau value of: 0.025"}
\CommentTok{\#\textgreater{} [1] "2024{-}05{-}13 19:26:44 Estimating CDFs with fixed tau value of: 0.05"}
\CommentTok{\#\textgreater{} [1] "2024{-}05{-}13 19:26:44 Estimating CDFs with fixed tau value of: 0.1"}
\CommentTok{\#\textgreater{} [1] "2024{-}05{-}13 19:26:44 Estimating CDFs with fixed tau value of: 0.2"}
\CommentTok{\#\textgreater{} [1] "2024{-}05{-}13 19:26:45 Beginning parallelized output by bootstrap x fixed tau.."}
\CommentTok{\#\textgreater{} [1] "2024{-}05{-}13 19:26:49 Computing Confidence Intervals"}
\CommentTok{\#\textgreater{} [1] "2024{-}05{-}13 19:27:00 Time taken:0.43 minutes"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{causalverse}\SpecialCharTok{::}\FunctionTok{plot\_rd\_aa\_share}\NormalTok{(rdbounds\_est\_tau) }\CommentTok{\# For SRD (default)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{24-regression-discontinuity_files/figure-latex/unnamed-chunk-8-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# causalverse::plot\_rd\_aa\_share(rdbounds\_est\_tau, rd\_type = "FRD")  \# For FRD}
\end{Highlighting}
\end{Shaded}

\hypertarget{fuzzy-rd-design}{%
\section{Fuzzy RD Design}\label{fuzzy-rd-design}}

When you have cutoff that does not perfectly determine treatment, but creates a discontinuity in the likelihood of receiving the treatment, you need another instrument

For those that are close to the cutoff, we create an instrument for \(D_i\)

\[
Z_i=
\begin{cases}
1 & \text{if } X_i \ge c \\
0 & \text{if } X_c < c
\end{cases}
\]

Then, we can estimate the effect of the treatment for compliers only (i.e., those treatment \(D_i\) depends on \(Z_i\))

The LATE parameter

\[
\lim_{c - \epsilon \le X \le c + \epsilon, \epsilon \to 0}( \frac{E(Y |Z = 1) - E(Y |Z=0)}{E(D|Z = 1) - E(D|Z = 0)})
\]

equivalently, the canonical parameter:

\[
\frac{lim_{x \downarrow c}E(Y|X = x) - \lim_{x \uparrow c} E(Y|X = x)}{\lim_{x \downarrow c } E(D |X = x) - \lim_{x \uparrow c}E(D |X=x)}
\]

Two equivalent ways to estimate

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  First

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    Sharp RDD for \(Y\)
  \item
    Sharp RDD for \(D\)
  \item
    Take the estimate from step 1 divide by that of step 2
  \end{enumerate}
\item
  Second: Subset those observations that are close to \(c\) and run instrumental variable \(Z\)
\end{enumerate}

\hypertarget{regression-kink-design}{%
\section{Regression Kink Design}\label{regression-kink-design}}

\begin{itemize}
\item
  If the slope of the treatment intensity changes at the cutoff (instead of the level of treatment assignment), we can have regression kink design
\item
  Example: unemployment benefits
\end{itemize}

Sharp Kink RD parameter

\[
\alpha_{KRD} = \frac{\lim_{x \downarrow c} \frac{d}{dx}E[Y_i |X_i = x]- \lim_{x \uparrow c} \frac{d}{dx}E[Y_i |X_i = x]}{\lim_{x \downarrow c} \frac{d}{dx}b(x) - \lim_{x \uparrow c} \frac{d}{dx}b(x)}
\]

where \(b(x)\) is a known function inducing ``kink''

Fuzzy Kink RD parameter

\[
\alpha_{KRD} = \frac{\lim_{x \downarrow c} \frac{d}{dx}E[Y_i |X_i = x]- \lim_{x \uparrow c} \frac{d}{dx}E[Y_i |X_i = x]}{\lim_{x \downarrow c} \frac{d}{dx}E[D_i |X_i = x]- \lim_{x \uparrow c} \frac{d}{dx}E[D_i |X_i = x]}
\]

\hypertarget{multi-cutoff}{%
\section{Multi-cutoff}\label{multi-cutoff}}

\[
\tau (x,c)= E[Y_{1i} - Y_{0i}|X_i = x, C_i = c]
\]

\hypertarget{multi-score}{%
\section{Multi-score}\label{multi-score}}

Multi-score (in multiple dimensions) (e.g., math and English cutoff for certain honor class):

\[
\tau (x_1, x_2) = E[Y_{1i} - Y_{0i}|X_{1i} = x_1, X_{2i} = x]
\]

\hypertarget{steps-for-sharp-rd}{%
\section{Steps for Sharp RD}\label{steps-for-sharp-rd}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Graph the data by computing the average value of the outcome variable over a set of bins (large enough to see a smooth graph, and small enough to make the jump around the cutoff clear).
\item
  Run regression on both sides of the cutoff to get the treatment effect
\item
  Robustness checks:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    Assess possible jumps in other variables around the cutoff
  \item
    Hypothesis testing for bunching
  \item
    Placebo tests
  \item
    Varying bandwidth
  \end{enumerate}
\end{enumerate}

\hypertarget{steps-for-fuzzy-rd}{%
\section{Steps for Fuzzy RD}\label{steps-for-fuzzy-rd}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Graph the data by computing the average value of the outcome variable over a set of bins (large enough to see a smooth graph, and small enough to make the jump around the cutoff clear).
\item
  Graph the probability of treatment
\item
  Estimate the treatment effect using 2SLS
\item
  Robustness checks:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    Assess possible jumps in other variables around the cutoff
  \item
    Hypothesis testing for bunching
  \item
    Placebo tests
  \item
    Varying bandwidth
  \end{enumerate}
\end{enumerate}

\hypertarget{steps-for-rdit-regression-discontinuity-in-time}{%
\section{Steps for RDiT (Regression Discontinuity in Time)}\label{steps-for-rdit-regression-discontinuity-in-time}}

Notes:

\begin{itemize}
\tightlist
\item
  Additional assumption: Time-varying confounders change smoothly across the cutoff date
\item
  Typically used in policy implementation in the same date for all subjects, but can also be used for cases where implementation dates are different between subjects. In the second case, researchers typically use different RDiT specification for each time series.
\item
  Sometimes the date of implementation is not randomly assigned by chosen strategically. Hence, RDiT should be thought of as the ``discontinuity at a threshold'' interpretation of RD (not as ``local randomization''). \citep[p.~8]{hausman2018}
\item
  Normal RD uses variation in the \(N\) dimension, while RDiT uses variation in the \(T\) dimension
\item
  Choose polynomials based on BIC typically. And can have either global polynomial or pre-period and post-period polynomial for each time series (but usually the global one will perform better)
\item
  Could use \textbf{augmented local linear} outlined by \citep[p.~12]{hausman2018}, where estimate the model with all the control first then take the residuals to include in the model with the RDiT treatment (remember to use bootstrapping method to account for the first-stage variance in the second stage).
\end{itemize}

Pros:

\begin{itemize}
\item
  can overcome cases where there is no cross-sectional variation in treatment implementation (DID is not feasible)

  \begin{itemize}
  \tightlist
  \item
    There are papers that use both RDiT and DID to (1) see the differential treatment effects across individuals/ space \citep{auffhammer2011clearing} or (2) compare the 2 estimates where the control group's validity is questionable \citep{gallego2013effect}.
  \end{itemize}
\item
  Better than pre/post comparison because it can include flexible controls
\item
  Better than event studies because it can use long-time horizons (may not be too relevant now since the development long-time horizon event studies), and it can use higher-order polynomials time control variables.
\end{itemize}

Cons:

\begin{itemize}
\item
  Taking observation for from the threshold (in time) can bias your estimates because of unobservables and time-series properties of the data generating process.
\item
  \citep{mccrary2008manipulation} test is not possible (see \protect\hyperlink{sortingbunchingmanipulation}{Sorting/Bunching/Manipulation}) because when the density of the running (time) is uniform, you can't use the test.
\item
  Time-varying unobservables may impact the dependent variable discontinuously
\item
  Error terms are likely to include persistence (serially correlated errors)
\item
  Researchers cannot model time-varying treatment under RDiT

  \begin{itemize}
  \tightlist
  \item
    In a small enough window, the local linear specification is fine, but the global polynomials can either be too big or too small \citep{hausman2018}
  \end{itemize}
\end{itemize}

Biases

\begin{itemize}
\item
  Time-Varying treatment Effects

  \begin{itemize}
  \item
    increase sample size either by

    \begin{itemize}
    \item
      more granular data (greater frequency): will not increase power because of the problem of serial correlation
    \item
      increasing time window: increases bias from other confounders
    \end{itemize}
  \item
    2 additional assumption:

    \begin{itemize}
    \item
      Model is correctly specified (with all confoudners or global polynomial approximation)
    \item
      Treatment effect is correctly specified (whether it's smooth and constant, or varies)
    \item
      These 2 assumptions do not interact ( we don't want them to interact - i.e., we don't want the polynomial correlated with the unobserved variation in the treatment effect)
    \end{itemize}
  \item
    There usually a difference between short-run and long-run treatment effects, but it's also possibly that the bias can stem from the over-fitting problem of the polynomial specification. \citep[p.~544]{hausman2018}
  \end{itemize}
\item
  Autoregression (serial dependence)

  \begin{itemize}
  \item
    Need to use \textbf{clustered standard errors} to account for serial dependence in the residuals
  \item
    In the case of serial dependence in \(\epsilon_{it}\), we don't have a solution, including a lagged dependent variable would misspecify the model (probably find another research project)
  \item
    In the case of serial dependence in \(y_{it}\), with long window, it becomes fuzzy to what you try to recover. You can include the \textbf{lagged dependent variable} (bias can still come from the time-varying treatment or over-fitting of the global polynomial)
  \end{itemize}
\item
  Sorting and Anticipation Effects

  \begin{itemize}
  \item
    Cannot run the \citep{mccrary2008manipulation} because the density of the time running variable is uniform
  \item
    Can still run tests to check discontinuities in other covariates (you want no discontinuities) and discontinuities in the outcome variable at other placebo thresholds ( you don't want discontinuities)
  \item
    Hence, it's hard to argue for the causal effect here because it could be the total effect of the causal treatment and the unobserved sorting/anticipation/adaptation/avoidance effects. You can only argue that there is no such behavior
  \end{itemize}
\end{itemize}

Recommendations for robustness check following \citep[p.~549]{hausman2018}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Plot the raw data and residuals (after removing confounders or trend). With varying polynomial and local linear controls, inconsistent results can be a sign of time-varying treatment effects.
\item
  Using global polynomial, you could overfit, then show polynomial with different order and alternative local linear bandwidths. If the results are consistent, you're okay
\item
  \protect\hyperlink{placebo-tests}{Placebo Tests}: estimate another RD (1) on another location or subject (that did not receive the treatment) or (2) use another date.
\item
  Plot RD discontinuity on continuous controls
\item
  Donut RD to see if avoiding the selection close to the cutoff would yield better results \citep{barreca2011saving}
\item
  Test for auto-regression (using only pre-treatment data). If there is evidence for autoregression, include the lagged dependent variable
\item
  Augmented local linear (no need to use global polynomial and avoid over-fitting)

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    Use full sample to exclude the effect of important predictors
  \item
    Estimate the conditioned second stage on a smaller sample bandwidth
  \end{enumerate}
\end{enumerate}

Examples from \citep[p.~534]{hausman2018} in

econ

\begin{itemize}
\item
  \citep{davis2008effect}: Air quality
\item
  \citep{auffhammer2011clearing}: Air quality
\item
  \citep{chen2018effect}: Air quality
\item
  \citep{de2013deterrent}: car accidents
\item
  \citep{gallego2013effect}: air quality
\item
  \citep{bento2014effects}: Traffic
\item
  \citep{anderson2014subways}: Traffic
\item
  \citep{burger2014did}: Car accidents
\item
  \citep{brodeur2021covid}: Covid19 lock-downs on well-being
\end{itemize}

marketing

\begin{itemize}
\item
  \citep[\citet{busse2013estimating}]{busse20061}: Vehicle prices
\item
  \citep{chen2009learning}: Customer Satisfaction
\item
  \citep{busse2010best}: Vehicle prices
\item
  \citep{davis2010international}: vehicle prices
\end{itemize}

\hypertarget{evaluation-of-an-rd}{%
\section{Evaluation of an RD}\label{evaluation-of-an-rd}}

\begin{itemize}
\item
  Evidence for (either formal tests or graphs)

  \begin{itemize}
  \item
    Treatment and outcomes change discontinuously at the cutoff, while other variables and pre-treatment outcomes do not.
  \item
    No manipulation of the assignment variable.
  \end{itemize}
\item
  Results are robust to various functional forms of the forcing variable
\item
  Is there any other (unobserved) confound that could cause the discontinuous change at the cutoff (i.e., multiple forcing variables / bundling of institutions)?
\item
  External Validity: How likely the result at the cutoff will generalize?
\end{itemize}

\textbf{General Model}

\[
Y_i = \beta_0 + f(x_i) \beta_1 + [I(x_i \ge c)]\beta_2 + \epsilon_i
\]

where \(f(x_i)\) is any functional form of \(x_i\)

\textbf{Simple case}

When \(f(x_i) = x_i\) (linear function)

\[
Y_i = \beta_0 + x_i \beta_1 + [I(x_i \ge c)]\beta_2 + \epsilon_i
\]

\includegraphics[width=6.25in,height=3.125in]{images/rd1.PNG}

RD gives you \(\beta_2\) (causal effect) of \(X\) on \(Y\) at the cutoff point

In practice, everyone does

\[
Y_i = \alpha_0 + f(x) \alpha _1 + [I(x_i \ge c)]\alpha_2 + [f(x_i)\times [I(x_i \ge c)]\alpha_3 + u_i
\]

\includegraphics[width=6.25in,height=3.125in]{images/rd2.PNG}

where we estimate different slope on different sides of the line

and if you estimate \(\alpha_3\) to be no different from 0 then we return to the simple case

\textbf{Notes}:

\begin{itemize}
\item
  Sparse data can make \(\alpha_3\) large differential effect
\item
  People are very skeptical when you have complex \(f(x_i)\), usual simple function forms (e.g., linear, squared term, etc.) should be good. However, if you still insist, then \textbf{non-parametric estimation} can be your best bet.
\end{itemize}

Bandwidth of \(c\) (window)

\begin{itemize}
\item
  Closer to \(c\) can give you lower bias, but also efficiency
\item
  Wider \(c\) can increase bias, but higher efficiency.
\item
  Optimal bandwidth is very controversial, but usually we have to do it in the appendix for research article anyway.
\item
  We can either

  \begin{itemize}
  \item
    drop observations outside of bandwidth or
  \item
    weight depends on how far and close to \(c\)
  \end{itemize}
\end{itemize}

\hypertarget{applications}{%
\section{Applications}\label{applications}}

Examples in marketing:

\begin{itemize}
\item
  \citep{narayanan2015position}
\item
  \citep{hartmann2011identifying}: nonparametric estimation and guide to identifying causal marketing mix effects
\end{itemize}

\href{https://rdpackages.github.io/}{Packages} in R (see \citep{thoemmes2017analysis} for detailed comparisons): all can handle both sharp and fuzzy RD

\begin{itemize}
\item
  \texttt{rdd}
\item
  \texttt{rdrobust} estimation, inference and plot
\item
  \texttt{rddensity} discontinuity in density tests (\protect\hyperlink{sortingbunchingmanipulation}{Sorting/Bunching/Manipulation}) using local polynomials and binomial test
\item
  \texttt{rdlocrand} covariate balance, binomial tests, window selection
\item
  \texttt{rdmulti} multiple cutoffs and multiple scores
\item
  \texttt{rdpower} power, sample selection
\item
  \texttt{rddtools}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1860}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2016}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2326}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3643}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Package
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
rdd
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
rdrobust
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
rddtools
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Coefficient estimator & Local linear regression & local polynomial regression & local polynomial regression \\
bandwidth selectors & \citep{imbens2012optimal} & \citep{calonico2020optimal}

\citep{imbens2012optimal}

\citep{calonico2014robust} & \citep{imbens2012optimal} \\
\begin{minipage}[t]{\linewidth}\raggedright
Kernel functions

\begin{itemize}
\item
  Triangular
\item
  Rectangular
\end{itemize}
\end{minipage} & Epanechnikov

Gaussian & Epanechnikov & Gaussian \\
Bias Correction & & Local polynomial regression & \\
Covariate options & Include & Include & Include

Residuals \\
Assumptions testing & McCrary sorting & & McCrary sorting

Equality of covariates distribution and mean \\
\end{longtable}

based on table 1 \citep{thoemmes2017analysis} (p.~347)

\hypertarget{example-1}{%
\subsection{Example 1}\label{example-1}}

Example by \href{https://towardsdatascience.com/the-crown-jewel-of-causal-inference-regression-discontinuity-design-rdd-bad37a68e786}{Leihua Ye}

\[
Y_i = \beta_0 + \beta_1 X_i + \beta_2 W_i + u_i
\]

\[
X_i = 
\begin{cases}
1, W_i \ge c \\
0, W_i < c
\end{cases}
\]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#cutoff point = 3.5}
\NormalTok{GPA }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{4}\NormalTok{)}
\NormalTok{future\_success }\OtherTok{\textless{}{-}} \DecValTok{10} \SpecialCharTok{+} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ GPA }\SpecialCharTok{+} \DecValTok{10} \SpecialCharTok{*}\NormalTok{ (GPA }\SpecialCharTok{\textgreater{}=} \FloatTok{3.5}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{1000}\NormalTok{)}
\CommentTok{\#install and load the package rddtools}
\CommentTok{\#install.packages(rddtools)}
\FunctionTok{library}\NormalTok{(rddtools)}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{rdd\_data}\NormalTok{(future\_success, GPA, }\AttributeTok{cutpoint =} \FloatTok{3.5}\NormalTok{)}
\CommentTok{\# plot the dataset}
\FunctionTok{plot}\NormalTok{(}
\NormalTok{    data,}
    \AttributeTok{col =}  \StringTok{"red"}\NormalTok{,}
    \AttributeTok{cex =} \FloatTok{0.1}\NormalTok{,}
    \AttributeTok{xlab =}  \StringTok{"GPA"}\NormalTok{,}
    \AttributeTok{ylab =}  \StringTok{"future\_success"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{24-regression-discontinuity_files/figure-latex/unnamed-chunk-9-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# estimate the sharp RDD model}
\NormalTok{rdd\_mod }\OtherTok{\textless{}{-}} \FunctionTok{rdd\_reg\_lm}\NormalTok{(}\AttributeTok{rdd\_object =}\NormalTok{ data, }\AttributeTok{slope =}  \StringTok{"same"}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(rdd\_mod)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = y \textasciitilde{} ., data = dat\_step1, weights = weights)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}      Min       1Q   Median       3Q      Max }
\CommentTok{\#\textgreater{} {-}2.90364 {-}0.70348  0.00278  0.66828  3.00603 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept) 16.90704    0.06637  254.75   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} D           10.09058    0.11063   91.21   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} x            1.97078    0.03281   60.06   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 0.9908 on 997 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.9654, Adjusted R{-}squared:  0.9654 }
\CommentTok{\#\textgreater{} F{-}statistic: 1.392e+04 on 2 and 997 DF,  p{-}value: \textless{} 2.2e{-}16}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# plot the RDD model along with binned observations}
\FunctionTok{plot}\NormalTok{(}
\NormalTok{    rdd\_mod,}
    \AttributeTok{cex =} \FloatTok{0.1}\NormalTok{,}
    \AttributeTok{col =}  \StringTok{"red"}\NormalTok{,}
    \AttributeTok{xlab =}  \StringTok{"GPA"}\NormalTok{,}
    \AttributeTok{ylab =}  \StringTok{"future\_success"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{24-regression-discontinuity_files/figure-latex/unnamed-chunk-11-1} \end{center}

\hypertarget{example-2}{%
\subsection{Example 2}\label{example-2}}

\citet{bowblis2021occupational}

Occupational licensing can either increase or decrease market efficiency:

\begin{itemize}
\item
  More information means more efficiency
\item
  Increased entry barriers (i.e., friction) increase efficiency
\end{itemize}

Components of RD

\begin{itemize}
\tightlist
\item
  Running variable
\item
  Cutoff: 120 beds or above
\item
  Treatment: you have to have the treatment before the cutoff point.
\end{itemize}

Under OLS

\[
Y_i = \alpha_0 + X_i \alpha_1 + LW_i \alpha_2 + \epsilon_i
\]

where

\begin{itemize}
\item
  \(LW_i\) Licensed/certified workers (in fraction format for each center).
\item
  \(Y_i\) = Quality of service
\end{itemize}

Bias in \(\alpha_2\)

\begin{itemize}
\item
  Mitigation-based: terrible quality can lead to more hiring, which negatively bias \(\alpha_2\)
\item
  Preference-based: places that have higher quality staff want to keep high quality staffs.
\end{itemize}

Under RD

\[
\begin{aligned}
Y_{ist} &= \beta_0 + [I(Bed \ge121)_{ist}]\beta_1 + f(Size_{ist}) \beta_2\\
&+ [f(Size_{ist}) \times I(Bed \ge 121)_{ist}] \beta_3 \\
&+ X_{it} \delta + \gamma_s + \theta_t + \epsilon_{ist}
\end{aligned}
\]

where

\begin{itemize}
\item
  \(s\) = state
\item
  \(t\) = year
\item
  \(i\) = hospital
\end{itemize}

This RD is fuzzy

\begin{itemize}
\item
  If right near the threshold (bandwidth), we have states with different sorting (i.e., non-random), then we need the fixed-effect for state \(s\). But then your RD assumption wrong anyway, then you won't do it in the first place
\item
  Technically, we could also run the fixed-effect regression, but because it's lower in the causal inference hierarchy. Hence, we don't do it.
\item
  Moreover, in the RD framework, we don't include \(t\) before treatment (but in the FE we have to include before and after)
\item
  If we include \(\pi_i\) for each hospital, then we don't have variation in the causal estimates (because hardly any hospital changes their bed size in the panel)
\item
  When you have \(\beta_1\) as the intent to treat (because the treatment effect does not coincide with the intent to treat)
\item
  You cannot take those fuzzy cases out, because it will introduce the selection bias.
\item
  Note that we cannot drop cases based on behavioral choice (because we will exclude non-compliers), but we can drop when we have particular behaviors ((e.g., people like round numbers).
\end{itemize}

Thus, we have to use Instrument variable \ref{instrumental-variable}

\textbf{Stage 1:}

\[
\begin{aligned}
QSW_{ist} &= \alpha_0 + [I(Bed \ge121)_{ist}]\alpha_1 + f(Size_{ist}) \alpha_2\\
&+ [f(Size_{ist}) \times I(Bed \ge 121)_{ist}] \alpha_3 \\
&+ X_{it} \delta + \gamma_s + \theta_t + \epsilon_{ist}
\end{aligned}
\]

(Note: you should have different fixed effects and error term - \(\delta, \gamma_s, \theta_t, \epsilon_{ist}\) from the first equation, but I ran out of Greek letters)

\textbf{Stage 2:}

\[
\begin{aligned}
Y_{ist} &= \gamma_0 + \gamma_1 \hat{QWS}_{ist} + f(Size_{ist}) \delta_2 \\
&+ [f(Size_{ist}) \times I(Bed \ge 121)] \delta_3 \\
&+ X_{it} \lambda + \eta_s + \tau_t + u_{ist}
\end{aligned}
\]

\begin{itemize}
\item
  The bigger the jump (discontinuity), the more similar the 2 coefficients (\(\gamma_1 \approx \beta_1\)) where \(\gamma_1\) is the average treatment effect (of exposing to the policy)
\item
  \(\beta_1\) will always be closer to 0 than \(\gamma_1\)
\item
  Figure 1 shows bunching at every 5 units cutoff, but 120 is still out there.
\item
  If we have manipulable bunching, there should be decrease at 130
\item
  Since we have limited number of mass points (at the round numbers), we should clustered standard errors by the mass point
\end{itemize}

\hypertarget{example-3}{%
\subsection{Example 3}\label{example-3}}

Replication of \citep{carpenter2009effect} by \href{https://rpubs.com/phle/r_tutorial_regression_discontinuity_design}{Philipp Leppert}, dataset from \href{https://www.openicpsr.org/openicpsr/project/113550/version/V1/view?flag=follow\&pageSize=100\&sortOrder=(?title)\&sortAsc=true}{here}

\hypertarget{example-4}{%
\subsection{Example 4}\label{example-4}}

For a detailed application, see \citep{thoemmes2017analysis} where they use \texttt{rdd}, \texttt{rdrobust}, \texttt{rddtools}

\hypertarget{synthetic-difference-in-differences}{%
\chapter{Synthetic Difference-in-Differences}\label{synthetic-difference-in-differences}}

by \citep{arkhangelsky2021synthetic}

also known as weighted double-differencing estimators

\begin{itemize}
\item
  Setting: Researchers use panel data to study effects of policy changes.

  \begin{itemize}
  \item
    Panel data: repeated observations across time for various units.
  \item
    Some units exposed to policy at different times than others.
  \end{itemize}
\item
  Policy changes often aren't random across units or time.

  \begin{itemize}
  \tightlist
  \item
    Challenge: Observed covariates might not lead to credible conclusions of no confounding \citep{imbens2015causal}
  \end{itemize}
\item
  To estimate the effects, either

  \begin{itemize}
  \item
    \protect\hyperlink{difference-in-differences}{Difference-in-differences} (DID) method widely used in applied economics.
  \item
    \protect\hyperlink{synthetic-control}{Synthetic Control} (SC) methods offer alternative approach for comparative case studies.
  \end{itemize}
\item
  Difference between DID and SC:

  \begin{itemize}
  \item
    DID: used with many policy-exposed units; relies on ``parallel trends'' assumption.
  \item
    SC: used with few policy-exposed units; compensates lack of parallel trends by reweighting units based on pre-exposure trends.
  \end{itemize}
\item
  \textbf{New proposition}: Synthetic Difference in Differences (SDID).

  \begin{itemize}
  \item
    Combines features of DID and SC.
  \item
    Reweights and matches pre-exposure trends (similar to SC).
  \item
    Invariant to additive unit-level shifts, valid for large-panel inference (like DID).
  \end{itemize}
\item
  Attractive features:

  \begin{itemize}
  \item
    SDID provides consistent and asymptotically normal estimates.
  \item
    SDID performs on par with or better than DID in traditional DID settings.

    \begin{itemize}
    \tightlist
    \item
      where DID can only handle completely random treatment assignment, SDID can handle cases where treatment assignment is correlated with some time or unit latent factors.
    \end{itemize}
  \item
    Similarly, SDID is as good as or better than SC in traditional SC settings.
  \item
    Uniformly random treatment assignment results in unbiased outcomes for all methods, but SDID is more precise.
  \item
    SDID reduces bias effectively for non-uniformly random assignments.
  \item
    SDID's double robustness is akin to the augmented inverse probability weighting estimator \citep[\citet{scharfstein1999adjusting}]{ben2021augmented}.
  \item
    Very much similar to augmented SC estimator by \citep[p.~4112]{ben2021augmented, arkhangelsky2021synthetic}
  \end{itemize}
\end{itemize}

Ideal case to use SDID estimator is when

\begin{itemize}
\item
  \(N_{ctr} \approx T_{pre}\)
\item
  Small \(T_{post}\)
\item
  \(N_{tr} <\sqrt{N_{ctr}}\)
\end{itemize}

Applications in marketing:

\begin{itemize}
\item
  \citet{lambrecht2024tv}: TV ads on online browsing and sales.
\item
  \citet{keller2024soda}: soda tax on marketing effectiveness.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{understanding}{%
\section{Understanding}\label{understanding}}

Consider a traditional time-series cross-sectional data

Let \(Y_{it}\) denote the outcome for unit \(i\) in period \(t\)

A balanced panel of \(N\) units and \(T\) time periods

\begin{itemize}
\item
  \(W_{it} \in \{0, 1\}\) is the binary treatment
\item
  \(N_c\) never-treated units (control)
\item
  \(N_t\) treated units after time \(T_{pre}\)
\end{itemize}

\textbf{Steps}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Find unit weights \(\hat{w}^{sdid}\) such that \(\sum_{i = 1}^{N_c} \hat{w}_i^{sdid} Y_{it} \approx N_t^{-1} \sum_{i = N_c + 1}^N Y_{it} \forall t = 1, \dots, T_{pre}\) (i.e., pre-treatment trends in outcome of the treated similar to those of control units) (similar to SC).
\item
  Find time weights \(\hat{\lambda}_t\) such that we have a balanced window (i.e., posttreatment outcomes for control units differ consistently from their weighted average pretreatment outcomes).
\item
  Estimate the average causal effect of treatment
\end{enumerate}

\[
(\hat{\tau}^{sdid}, \hat{\mu}, \hat{\alpha}, \hat{\beta}) = \arg \min_{\tau, \mu, \alpha, \beta} \{ \sum_{i = 1}^N \sum_{t = 1}^T (Y_{it} - \mu - \alpha_i - \beta_ t - W_{it} \tau)^2 \hat{w}_i^{sdid} \hat{\lambda}_t^{sdid} \}
\]

Better than DiD estimator because \(\tau^{did}\) does not consider time or unit weights

\[
(\hat{\tau}^{did}, \hat{\mu}, \hat{\alpha}, \hat{\beta}) = \arg \min_{\tau, \mu, \alpha, \beta} \{ \sum_{i = 1}^N \sum_{t = 1}^T (Y_{it} - \mu - \alpha_i - \beta_ t - W_{it} \tau)^2 \}
\]

Better than SC estimator because \(\tau^{sc}\) lacks unit fixed effete and time weights

\[
(\hat{\tau}^{sc}, \hat{\mu}, \hat{\beta}) = \arg \min_{\tau, \mu, \beta} \{ \sum_{i = 1}^N \sum_{t = 1}^T (Y_{it} - \mu - \beta_ t - W_{it} \tau)^2 \hat{w}_i^{sdid}  \}
\]

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1119}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2413}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2867}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3531}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{DID}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{SC}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{SDID}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Primary Assumption} & Absence of intervention leads to parallel evolution across states. & Reweights unexposed states to match pre-intervention outcomes of treated state. & Reweights control units to ensure a parallel time trend with the treated pre-intervention trend. \\
\textbf{Reliability Concern} & Can be unreliable when pre-intervention trends aren't parallel. & Accounts for non-parallel pre-intervention trends by reweighting. & Uses reweighting to adjust for non-parallel pre-intervention trends. \\
\textbf{Treatment of Time Periods} & All pre-treatment periods are given equal weight. & Doesn't specifically emphasize equal weight for pre-treatment periods. & Focuses only on a subset of pre-intervention time periods, selected based on historical outcomes. \\
\textbf{Goal with Reweighting} & N/A (doesn't use reweighting). & To match treated state as closely as possible before the intervention. & Make trends of control units parallel (not necessarily identical) to the treated pre-intervention. \\
\end{longtable}

Alternatively, think of our parameter of interest as:

\[
\hat{\tau} = \hat{\delta}_t - \sum_{i = 1}^{N_c} \hat{w}_i \hat{\delta}_i
\]

where \(\hat{\delta}_t = \frac{1}{N_t} \sum_{i = N_c + 1}^N \hat{\delta}_i\)

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.0492}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2121}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.4167}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3144}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Sample Weight
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Adjusted outcomes (\(\hat{\delta}_i\))
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Interpretation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
SC & \(\hat{w}^{sc} = \min_{w \in R}l_{unit}(w)\) & \(\frac{1}{T_{post}} \sum_{t = T_{pre} + 1}^T Y_{it}\) & Unweighted treatment period averages \\
DID & \(\hat{w}_i^{did} = N_c^{-1}\) & \(\frac{1}{T_{post}} \sum_{t = T_{pre}+ 1}^T Y_{it} - \frac{1}{T_{pre}} \sum_{t = 1}^{T_{pre}}Y_{it}\) & Unweighted differences between average treatment period and pretreatment outcome \\
SDID & \((\hat{w}_0, \hat{w}^{sdid}) = \min l_{unit}(w_0, w)\) & \(\frac{1}{T_{post}} \sum_{t = T_{pre} + 1}^T Y_{it} - \sum_{t = 1}^{T_{pre}} \hat{\lambda}_t^{sdid} Y_{it}\) & Weighted differences between average treatment period and pretreatment outcome \\
\end{longtable}

\begin{itemize}
\item
  The SDID estimator uses weights:

  \begin{itemize}
  \item
    Makes two-way fixed effect regression ``local.''
  \item
    Emphasizes units similar in their past to treated units.
  \item
    Prioritizes periods resembling treated periods.
  \end{itemize}
\item
  Benefits of this localization:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \item
    \textbf{Robustness}: Using similar units and periods boosts estimator's robustness.
  \item
    \textbf{Improved Precision}: Weights can eliminate predictable outcome components.

    \begin{itemize}
    \item
      The SEs of SDID are smaller than those of SC and DID
    \item
      Caveat: If there's minor systematic heterogeneity in outcomes, unequal weighting might reduce precision compared to standard DID.
    \end{itemize}
  \end{enumerate}
\item
  Weight Design:

  \begin{itemize}
  \item
    \textbf{Unit Weights}: Makes average outcome for treated units roughly parallel to the weighted average for control units.
  \item
    \textbf{Time Weights}: Ensures posttreatment outcomes for control units differ consistently from their weighted average pretreatment outcomes.
  \end{itemize}
\item
  Weights enhance DID's plausibility:

  \begin{itemize}
  \item
    Raw data often lacks parallel time trends for treated/control units.
  \item
    Similar techniques (e.g., adjusting for covariates or selecting specific time periods) were used before \citep{callaway2021difference}.
  \item
    SDID automates this process, applying a similar logic to weight both units and time periods.
  \end{itemize}
\item
  Time Weights in SDID:

  \begin{itemize}
  \tightlist
  \item
    Removes bias and boosts precision (i.e., minimizes the influence of time periods vastly different from posttreatment periods).
  \end{itemize}
\item
  Argument for Unit Fixed Effects:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \item
    \textbf{Flexibility}: Increases model flexibility and thereby bolsters robustness.
  \item
    \textbf{Enhanced Precision}: Unit fixed effects explain a significant portion of outcome variation.
  \end{enumerate}
\item
  SC Weighting \& Unit Fixed Effects:

  \begin{itemize}
  \item
    Under certain conditions, SC weighting can inherently account for unit fixed effects.

    \begin{itemize}
    \tightlist
    \item
      For example, when the weighted average outcome for control units in pretreatment is the same as that of the treated units. (unlikely in reality)
    \end{itemize}
  \item
    The use of unit fixed effect in synthetic control regression (i.e., synthetic control with intercept) was proposed before in \citet{doudchenko2016balancing} and \citet{ferman2021synthetic} (called DIFP)
  \end{itemize}
\end{itemize}

More details on application

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Choose unit weights
\end{enumerate}

\begin{itemize}
\item
  Regularization Parameter:

  \begin{itemize}
  \tightlist
  \item
    Equal to the size of a typical one-period outcome change for control units in the pre-period, then multiplied by a scaling factor \citep[p.~4092]{arkhangelsky2021synthetic}.
  \end{itemize}
\item
  Relation to SC Weights:

  \begin{itemize}
  \item
    SDID weights are similar to those used in \citep{abadie2010synthetic} except two distinctions:

    \begin{enumerate}
    \def\labelenumi{\arabic{enumi}.}
    \item
      Inclusion of an Intercept Term:

      \begin{itemize}
      \item
        The weights in SynthDiD do not necessarily make the control pre-trends perfectly match the treated trends, just make them parallel.
      \item
        This flexibility comes from the use of unit fixed effects, which can absorb any consistent differences between units.
      \end{itemize}
    \item
      Regularization Penalty:

      \begin{itemize}
      \item
        Adopted from \citet{doudchenko2016balancing} .
      \item
        Enhances the dispersion and ensures the uniqueness of the weights.
      \end{itemize}
    \end{enumerate}
  \item
    DID weights are identical to those used in \citep{abadie2010synthetic} without intercept and regularization penalty and 1 treated unit.
  \end{itemize}
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Choose time weights
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Also include an intercept term, but no regularization (because correlated observations within time periods for the same unit is plausible, but not across units within the same period).
\end{itemize}

\textbf{Note}: To account for time-varying variables in the weights, one can use the residuals of the regression of the observed outcome on these time-varying variables, instead of the observed outcomes themselves (\(Y_{it}^{res} = Y_{it} - X_{it} \hat{\beta}\), where \(\hat{\beta}\) come from \(Y = \beta X_{it}\)).

The SDID method can account for systematic effects, often referred to as unit effects or unit heterogeneity, which influence treatment assignment (i.e., when treatment assignment is correlated with these systematic effects). Consequently, it provides unbiased estimates, especially valuable when there's a suspicion that the treatment might be influenced by persistent, unit-specific attributes.

Even in cases where we have completely random assignment, SDID, DiD, and SC are unbiased, but SynthDiD has the smallest SE.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{application-6}{%
\section{Application}\label{application-6}}

\textbf{SDID Algorithm}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute regularization parameter \(\zeta\)
\end{enumerate}

\[
\zeta = (N_{t}T_{post})^{1/4} \hat{\sigma}
\]

where

\[
\hat{\sigma}^2 = \frac{1}{N_c(T_{pre}- 1)} \sum_{i = 1}^{N_c} \sum_{t = 1}^{T_{re}-1}(\Delta_{it} - \hat{\Delta})^2
\]

\begin{itemize}
\item
  \(\Delta_{it} = Y_{i(t + 1)} - Y_{it}\)
\item
  \(\hat{\Delta} = \frac{1}{N_c(T_{pre} - 1)}\sum_{i = 1}^{N_c}\sum_{t = 1}^{T_{pre}-1} \Delta_{it}\)
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Compute unit weights \(\hat{w}^{sdid}\)
\end{enumerate}

\[
(\hat{w}_0, \hat{w}^{sidid}) = \arg \min_{w_0 \in R, w \in \Omega}l_{unit}(w_0, w)
\]

where

\begin{itemize}
\item
  \(l_{unit} (w_0, w) = \sum_{t = 1}^{T_{pre}}(w_0 + \sum_{i = 1}^{N_c}w_i Y_{it} - \frac{1}{N_t}\sum_{i = N_c + 1}^NY_{it})^2 + \zeta^2 T_{pre}||w||_2^2\)
\item
  \(\Omega = \{w \in R_+^N: \sum_{i = 1}^{N_c} w_i = 1, w_i = N_t^{-1} \forall i = N_c + 1, \dots, N \}\)
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Compute time weights \(\hat{\lambda}^{sdid}\)
\end{enumerate}

\[
(\hat{\lambda}_0 , \hat{\lambda}^{sdid}) = \arg \min_{\lambda_0 \in R, \lambda \in \Lambda} l_{time}(\lambda_0, \lambda)
\]

where

\begin{itemize}
\item
  \(l_{time} (\lambda_0, \lambda) = \sum_{i = 1}^{N_c}(\lambda_0 + \sum_{t = 1}^{T_{pre}} \lambda_t Y_{it} - \frac{1}{T_{post}} \sum_{t = T_{pre} + 1}^T Y_{it})^2\)
\item
  \(\Lambda = \{ \lambda \in R_+^T: \sum_{t = 1}^{T_{pre}} \lambda_t = 1, \lambda_t = T_{post}^{-1} \forall t = T_{pre} + 1, \dots, T\}\)
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Compute the SDID estimator
\end{enumerate}

\[
(\hat{\tau}^{sdid}, \hat{\mu}, \hat{\alpha}, \hat{\beta}) = \arg \min_{\tau, \mu, \alpha, \beta}\{ \sum_{i = 1}^N \sum_{t = 1}^T (Y_{it} - \mu - \alpha_i - \beta_t - W_{it} \tau)^2 \hat{w}_i^{sdid}\hat{\lambda}_t^{sdid}
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{SE Estimation}

\begin{itemize}
\item
  Under certain assumptions (errors, samples, and interaction properties between time and unit fixed effects) detailed in \citep[p.~4107]{arkhangelsky2019synthetic}, SDID is asymptotically normal and zero-centered
\item
  Using its asymptotic variance, conventional confidence intervals can be applied to SDID.
\end{itemize}

\[
\tau \in \hat{\tau}^{sdid} \pm z_{\alpha/2}\sqrt{\hat{V}_\tau}
\]

\begin{itemize}
\item
  There are 3 approaches for variance estimation in confidence intervals:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \item
    \textbf{Clustered Bootstrap \citep{efron1992bootstrap}:}

    \begin{itemize}
    \item
      Independently resample units.
    \item
      Advantages: Simple to use; robust performance in large panels due to natural approach to inference with panel data where observations of the same unit might be correlated.
    \item
      Disadvantage: Computationally expensive.
    \end{itemize}
  \item
    \textbf{Jackknife \citep{miller1974jackknife}:}

    \begin{itemize}
    \item
      Applied to weighted SDID regression with fixed weights.
    \item
      Generally conservative and precise when treated and control units are sufficiently similar.
    \item
      Not recommended for some methods, like the SC estimator, due to potential biases.
    \item
      Appropriate for jackknifing DID without random weights.
    \end{itemize}
  \item
    \textbf{Placebo Variance Estimation:}

    \begin{itemize}
    \item
      Can used in cases with only one treated unit or large panels.
    \item
      Placebo evaluations swap out the treated unit for untreated ones to estimate noise.
    \item
      Relies on homoskedasticity across units.
    \item
      Depends on homoskedasticity across units. It hinges on the empirical distribution of residuals from placebo estimators on control units.
    \item
      The validity of the placebo method hinges on consistent noise distribution across units. One treated unit makes nonparametric variance estimation difficult, necessitating homoskedasticity for feasible inference. Detailed analysis available in \citet{conley2011inference}.
    \end{itemize}
  \end{enumerate}
\end{itemize}

All algorithms are from \citet{arkhangelsky2021synthetic}, p.~4109:

\begin{quote}
\textbf{Bootstrap Variance Estimation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  For each \(b\) from \(1 \to B\):

  \begin{itemize}
  \item
    Sample \(N\) rows from \((\mathbf{Y}, \mathbf{W})\) to get (\(\mathbf{Y}^{(b)}, \mathbf{W}^{(b)}\)) with replacement.
  \item
    If the sample lacks treated or control units, resample.
  \item
    Calculate \(\tau^{(b)}\) using (\(\mathbf{Y}^{(b)}, \mathbf{W}^{(b)}\)).
  \end{itemize}
\item
  Calculate variance: \(\hat{V}_\tau = \frac{1}{B} \sum_{b = 1}^B (\hat{\tau}^{b} - \frac{1}{B} \sum_{b = 1}^B \hat{\tau}^b)^2\)
\end{enumerate}
\end{quote}

\begin{quote}
\textbf{Jackknife Variance Estimation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  For each \(i\) from \(1 \to N\):

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Calculate \(\hat{\tau}^{(-i)}\): \(\arg\min_{\tau, \{\alpha_j, \beta_t\}} \sum_{j \neq, i, t}(\mathbf{Y}_{jt} - \alpha_j - \beta_t - \tau \mathbf{W}_{it})^2 \hat{w}_j \hat{\lambda}_t\)
  \end{enumerate}
\item
  Calculate: \(\hat{V}_{\tau} = (N - 1) N^{-1} \sum_{i = 1}^N (\hat{\tau}^{(-i)} - \hat{\tau})^2\)
\end{enumerate}
\end{quote}

\begin{quote}
\textbf{Placebo Variance Estimation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  For each \(b\) from \(1 \to B\)

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Sample \(N_t\) out of \(N_c\) without replacement to get the ``placebo'' treatment
  \item
    Construct a placebo treatment matrix \(\mathbf{W}_c^b\) for the controls
  \item
    Calculate \(\hat{\tau}\) based on~\((\mathbf{Y}_c, \mathbf{W}_c^b)\)
  \end{enumerate}
\item
  Calculate \(\hat{V}_\tau = \frac{1}{B}\sum_{b = 1}^B (\hat{\tau}^b - \frac{1}{B} \sum_{b = 1}^B \hat{\tau}^b)^2\)
\end{enumerate}
\end{quote}

\hypertarget{block-treatment}{%
\subsection{Block Treatment}\label{block-treatment}}

Code provided by the \texttt{synthdid} package

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(synthdid)}
\FunctionTok{library}\NormalTok{(tidyverse)}

\CommentTok{\# Estimate the effect of California Proposition 99 on cigarette consumption}
\FunctionTok{data}\NormalTok{(}\StringTok{\textquotesingle{}california\_prop99\textquotesingle{}}\NormalTok{)}

\NormalTok{setup }\OtherTok{=}\NormalTok{ synthdid}\SpecialCharTok{::}\FunctionTok{panel.matrices}\NormalTok{(synthdid}\SpecialCharTok{::}\NormalTok{california\_prop99)}

\NormalTok{tau.hat }\OtherTok{=}\NormalTok{ synthdid}\SpecialCharTok{::}\FunctionTok{synthdid\_estimate}\NormalTok{(setup}\SpecialCharTok{$}\NormalTok{Y, setup}\SpecialCharTok{$}\NormalTok{N0, setup}\SpecialCharTok{$}\NormalTok{T0)}

\CommentTok{\# se = sqrt(vcov(tau.hat, method = \textquotesingle{}placebo\textquotesingle{}))}

\FunctionTok{plot}\NormalTok{(tau.hat) }\SpecialCharTok{+}\NormalTok{ causalverse}\SpecialCharTok{::}\FunctionTok{ama\_theme}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{25-synthdid_files/figure-latex/unnamed-chunk-1-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{setup }\OtherTok{=}\NormalTok{ synthdid}\SpecialCharTok{::}\FunctionTok{panel.matrices}\NormalTok{(synthdid}\SpecialCharTok{::}\NormalTok{california\_prop99)}

\CommentTok{\# Run for specific estimators}
\NormalTok{results\_selected }\OtherTok{=}\NormalTok{ causalverse}\SpecialCharTok{::}\FunctionTok{panel\_estimate}\NormalTok{(setup,}
                                               \AttributeTok{selected\_estimators =} \FunctionTok{c}\NormalTok{(}\StringTok{"synthdid"}\NormalTok{, }\StringTok{"did"}\NormalTok{, }\StringTok{"sc"}\NormalTok{))}

\NormalTok{results\_selected}
\CommentTok{\#\textgreater{} $synthdid}
\CommentTok{\#\textgreater{} $synthdid$estimate}
\CommentTok{\#\textgreater{} synthdid: {-}15.604 +{-} NA. Effective N0/N0 = 16.4/38\textasciitilde{}0.4. Effective T0/T0 = 2.8/19\textasciitilde{}0.1. N1,T1 = 1,12. }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $synthdid$std.error}
\CommentTok{\#\textgreater{} [1] 10.05324}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $did}
\CommentTok{\#\textgreater{} $did$estimate}
\CommentTok{\#\textgreater{} synthdid: {-}27.349 +{-} NA. Effective N0/N0 = 38.0/38\textasciitilde{}1.0. Effective T0/T0 = 19.0/19\textasciitilde{}1.0. N1,T1 = 1,12. }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $did$std.error}
\CommentTok{\#\textgreater{} [1] 15.81479}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $sc}
\CommentTok{\#\textgreater{} $sc$estimate}
\CommentTok{\#\textgreater{} synthdid: {-}19.620 +{-} NA. Effective N0/N0 = 3.8/38\textasciitilde{}0.1. Effective T0/T0 = Inf/19\textasciitilde{}Inf. N1,T1 = 1,12. }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $sc$std.error}
\CommentTok{\#\textgreater{} [1] 11.16422}

\CommentTok{\# to access more details in the estimate object}
\FunctionTok{summary}\NormalTok{(results\_selected}\SpecialCharTok{$}\NormalTok{did}\SpecialCharTok{$}\NormalTok{estimate)}
\CommentTok{\#\textgreater{} $estimate}
\CommentTok{\#\textgreater{} [1] {-}27.34911}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $se}
\CommentTok{\#\textgreater{}      [,1]}
\CommentTok{\#\textgreater{} [1,]   NA}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $controls}
\CommentTok{\#\textgreater{}                estimate 1}
\CommentTok{\#\textgreater{} Wyoming             0.026}
\CommentTok{\#\textgreater{} Wisconsin           0.026}
\CommentTok{\#\textgreater{} West Virginia       0.026}
\CommentTok{\#\textgreater{} Virginia            0.026}
\CommentTok{\#\textgreater{} Vermont             0.026}
\CommentTok{\#\textgreater{} Utah                0.026}
\CommentTok{\#\textgreater{} Texas               0.026}
\CommentTok{\#\textgreater{} Tennessee           0.026}
\CommentTok{\#\textgreater{} South Dakota        0.026}
\CommentTok{\#\textgreater{} South Carolina      0.026}
\CommentTok{\#\textgreater{} Rhode Island        0.026}
\CommentTok{\#\textgreater{} Pennsylvania        0.026}
\CommentTok{\#\textgreater{} Oklahoma            0.026}
\CommentTok{\#\textgreater{} Ohio                0.026}
\CommentTok{\#\textgreater{} North Dakota        0.026}
\CommentTok{\#\textgreater{} North Carolina      0.026}
\CommentTok{\#\textgreater{} New Mexico          0.026}
\CommentTok{\#\textgreater{} New Hampshire       0.026}
\CommentTok{\#\textgreater{} Nevada              0.026}
\CommentTok{\#\textgreater{} Nebraska            0.026}
\CommentTok{\#\textgreater{} Montana             0.026}
\CommentTok{\#\textgreater{} Missouri            0.026}
\CommentTok{\#\textgreater{} Mississippi         0.026}
\CommentTok{\#\textgreater{} Minnesota           0.026}
\CommentTok{\#\textgreater{} Maine               0.026}
\CommentTok{\#\textgreater{} Louisiana           0.026}
\CommentTok{\#\textgreater{} Kentucky            0.026}
\CommentTok{\#\textgreater{} Kansas              0.026}
\CommentTok{\#\textgreater{} Iowa                0.026}
\CommentTok{\#\textgreater{} Indiana             0.026}
\CommentTok{\#\textgreater{} Illinois            0.026}
\CommentTok{\#\textgreater{} Idaho               0.026}
\CommentTok{\#\textgreater{} Georgia             0.026}
\CommentTok{\#\textgreater{} Delaware            0.026}
\CommentTok{\#\textgreater{} Connecticut         0.026}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $periods}
\CommentTok{\#\textgreater{}      estimate 1}
\CommentTok{\#\textgreater{} 1988      0.053}
\CommentTok{\#\textgreater{} 1987      0.053}
\CommentTok{\#\textgreater{} 1986      0.053}
\CommentTok{\#\textgreater{} 1985      0.053}
\CommentTok{\#\textgreater{} 1984      0.053}
\CommentTok{\#\textgreater{} 1983      0.053}
\CommentTok{\#\textgreater{} 1982      0.053}
\CommentTok{\#\textgreater{} 1981      0.053}
\CommentTok{\#\textgreater{} 1980      0.053}
\CommentTok{\#\textgreater{} 1979      0.053}
\CommentTok{\#\textgreater{} 1978      0.053}
\CommentTok{\#\textgreater{} 1977      0.053}
\CommentTok{\#\textgreater{} 1976      0.053}
\CommentTok{\#\textgreater{} 1975      0.053}
\CommentTok{\#\textgreater{} 1974      0.053}
\CommentTok{\#\textgreater{} 1973      0.053}
\CommentTok{\#\textgreater{} 1972      0.053}
\CommentTok{\#\textgreater{} 1971      0.053}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $dimensions}
\CommentTok{\#\textgreater{}           N1           N0 N0.effective           T1           T0 T0.effective }
\CommentTok{\#\textgreater{}            1           38           38           12           19           19}

\NormalTok{causalverse}\SpecialCharTok{::}\FunctionTok{process\_panel\_estimate}\NormalTok{(results\_selected)}
\CommentTok{\#\textgreater{}     Method Estimate    SE}
\CommentTok{\#\textgreater{} 1 SYNTHDID   {-}15.60 10.05}
\CommentTok{\#\textgreater{} 2      DID   {-}27.35 15.81}
\CommentTok{\#\textgreater{} 3       SC   {-}19.62 11.16}
\end{Highlighting}
\end{Shaded}

\hypertarget{staggered-adoption}{%
\subsection{Staggered Adoption}\label{staggered-adoption}}

To apply to staggered adoption settings using the SDID estimator (see examples in \citet{arkhangelsky2021synthetic}, p.~4115 similar to \citet{ben2022synthetic}), we can:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Apply the SDID estimator repeatedly, once for every adoption date.
\item
  Using \citet{ben2022synthetic} 's method, form matrices for each adoption date. Apply SDID and average based on treated unit/time-period fractions.
\item
  Create multiple samples by splitting the data up by time periods. Each sample should have a consistent adoption date.
\end{enumerate}

For a formal note on this special case, see \citet{porreca2022synthetic}. It compares the outcomes from using SynthDiD with those from other estimators:

\begin{itemize}
\item
  Two-Way Fixed Effects (TWFE),
\item
  The group time average treatment effect estimator from \citet{callaway2021difference},
\item
  The partially pooled synthetic control method estimator from \citet{ben2021augmented}, in a staggered treatment adoption context.
\end{itemize}

\begin{itemize}
\item
  The findings reveal that SynthDiD produces a different estimate of the average treatment effect compared to the other methods.

  \begin{itemize}
  \tightlist
  \item
    Simulation results suggest that these differences could be due to the SynthDiD's data generating process assumption (a latent factor model) aligning more closely with the actual data than the additive fixed effects model assumed by traditional DiD methods.
  \end{itemize}
\end{itemize}

To explore heterogeneity of treatment effect, we can do subgroup analysis \citep[p.~1092]{berman2022value}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1783}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1757}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3902}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2506}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Method}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Advantages}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Disadvantages}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Procedure}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Split Data into Subsets & Compares treated units to control units within the same subgroup. & Each subset uses a different synthetic control, making it challenging to compare effects across subgroups. & \begin{minipage}[t]{\linewidth}\raggedright
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Split the data into separate subsets for each subgroup.
\item
  Compute synthetic DID effects for each subset.
\end{enumerate}
\end{minipage} \\
Control Group Comprising All Non-adopters & Control weights match pretrends well for each treated subgroup. & Each control unit receives a different weight for each treatment subgroup, making it difficult to compare results due to varying synthetic controls. & \begin{minipage}[t]{\linewidth}\raggedright
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Use a control group consisting of all non-adopters in each balanced panel cohort analysis.
\item
  Switch treatment units to the subgroup being analyzed.
\item
  Perform \texttt{synthdid} analysis.
\end{enumerate}
\end{minipage} \\
Use All Data to Estimate Synthetic Control Weights \textbf{(recommend)} & All units have the same synthetic control. & Pretrend match may not be as accurate since it aims to match the average outcome of all treated units, not just a specific subgroup. & \begin{minipage}[t]{\linewidth}\raggedright
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Use all the data to estimate the synthetic DID control weights.
\item
  Compute treatment effects using only the treated subgroup units as the treatment units.
\end{enumerate}
\end{minipage} \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{df }\OtherTok{\textless{}{-}}\NormalTok{ fixest}\SpecialCharTok{::}\NormalTok{base\_stagg }\SpecialCharTok{|\textgreater{}}
\NormalTok{   dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{treatvar =} \FunctionTok{if\_else}\NormalTok{(time\_to\_treatment }\SpecialCharTok{\textgreater{}=} \DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
\NormalTok{   dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{treatvar =} \FunctionTok{as.integer}\NormalTok{(}\FunctionTok{if\_else}\NormalTok{(year\_treated }\SpecialCharTok{\textgreater{}}\NormalTok{ (}\DecValTok{5} \SpecialCharTok{+} \DecValTok{2}\NormalTok{), }\DecValTok{0}\NormalTok{, treatvar)))}


\NormalTok{est }\OtherTok{\textless{}{-}}\NormalTok{ causalverse}\SpecialCharTok{::}\FunctionTok{synthdid\_est\_ate}\NormalTok{(}
  \AttributeTok{data               =}\NormalTok{ df,}
  \AttributeTok{adoption\_cohorts   =} \DecValTok{5}\SpecialCharTok{:}\DecValTok{7}\NormalTok{,}
  \AttributeTok{lags               =} \DecValTok{2}\NormalTok{,}
  \AttributeTok{leads              =} \DecValTok{2}\NormalTok{,}
  \AttributeTok{time\_var           =} \StringTok{"year"}\NormalTok{,}
  \AttributeTok{unit\_id\_var        =} \StringTok{"id"}\NormalTok{,}
  \AttributeTok{treated\_period\_var =} \StringTok{"year\_treated"}\NormalTok{,}
  \AttributeTok{treat\_stat\_var     =} \StringTok{"treatvar"}\NormalTok{,}
  \AttributeTok{outcome\_var        =} \StringTok{"y"}
\NormalTok{)}
\CommentTok{\#\textgreater{} adoption\_cohort: 5 }
\CommentTok{\#\textgreater{} Treated units: 5 Control units: 65 }
\CommentTok{\#\textgreater{} adoption\_cohort: 6 }
\CommentTok{\#\textgreater{} Treated units: 5 Control units: 60 }
\CommentTok{\#\textgreater{} adoption\_cohort: 7 }
\CommentTok{\#\textgreater{} Treated units: 5 Control units: 55}

\FunctionTok{data.frame}\NormalTok{(}
    \AttributeTok{Period =} \FunctionTok{names}\NormalTok{(est}\SpecialCharTok{$}\NormalTok{TE\_mean\_w),}
    \AttributeTok{ATE    =}\NormalTok{ est}\SpecialCharTok{$}\NormalTok{TE\_mean\_w,}
    \AttributeTok{SE     =}\NormalTok{ est}\SpecialCharTok{$}\NormalTok{SE\_mean\_w}
\NormalTok{) }\SpecialCharTok{|\textgreater{}}
\NormalTok{    causalverse}\SpecialCharTok{::}\FunctionTok{nice\_tab}\NormalTok{()}
\CommentTok{\#\textgreater{}    Period   ATE   SE}
\CommentTok{\#\textgreater{} 1      {-}2 {-}0.05 0.22}
\CommentTok{\#\textgreater{} 2      {-}1  0.05 0.22}
\CommentTok{\#\textgreater{} 3       0 {-}5.07 0.80}
\CommentTok{\#\textgreater{} 4       1 {-}4.68 0.51}
\CommentTok{\#\textgreater{} 5       2 {-}3.70 0.79}
\CommentTok{\#\textgreater{} 6 cumul.0 {-}5.07 0.80}
\CommentTok{\#\textgreater{} 7 cumul.1 {-}4.87 0.55}
\CommentTok{\#\textgreater{} 8 cumul.2 {-}4.48 0.53}


\NormalTok{causalverse}\SpecialCharTok{::}\FunctionTok{synthdid\_plot\_ate}\NormalTok{(est)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{25-synthdid_files/figure-latex/unnamed-chunk-2-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{est\_sub }\OtherTok{\textless{}{-}}\NormalTok{ causalverse}\SpecialCharTok{::}\FunctionTok{synthdid\_est\_ate}\NormalTok{(}
  \AttributeTok{data               =}\NormalTok{ df,}
  \AttributeTok{adoption\_cohorts   =} \DecValTok{5}\SpecialCharTok{:}\DecValTok{7}\NormalTok{,}
  \AttributeTok{lags               =} \DecValTok{2}\NormalTok{,}
  \AttributeTok{leads              =} \DecValTok{2}\NormalTok{,}
  \AttributeTok{time\_var           =} \StringTok{"year"}\NormalTok{,}
  \AttributeTok{unit\_id\_var        =} \StringTok{"id"}\NormalTok{,}
  \AttributeTok{treated\_period\_var =} \StringTok{"year\_treated"}\NormalTok{,}
  \AttributeTok{treat\_stat\_var     =} \StringTok{"treatvar"}\NormalTok{,}
  \AttributeTok{outcome\_var        =} \StringTok{"y"}\NormalTok{,}
  \CommentTok{\# a vector of subgroup id (from unit id)}
  \AttributeTok{subgroup           =}  \FunctionTok{c}\NormalTok{(}
    \CommentTok{\# some are treated}
    \StringTok{"11"}\NormalTok{, }\StringTok{"30"}\NormalTok{, }\StringTok{"49"}\NormalTok{ ,}
    \CommentTok{\# some are control within this period}
    \StringTok{"20"}\NormalTok{, }\StringTok{"25"}\NormalTok{, }\StringTok{"21"}\NormalTok{)}
\NormalTok{)}
\CommentTok{\#\textgreater{} adoption\_cohort: 5 }
\CommentTok{\#\textgreater{} Treated units: 3 Control units: 65 }
\CommentTok{\#\textgreater{} adoption\_cohort: 6 }
\CommentTok{\#\textgreater{} Treated units: 0 Control units: 60 }
\CommentTok{\#\textgreater{} adoption\_cohort: 7 }
\CommentTok{\#\textgreater{} Treated units: 0 Control units: 55}

\FunctionTok{data.frame}\NormalTok{(}
    \AttributeTok{Period =} \FunctionTok{names}\NormalTok{(est\_sub}\SpecialCharTok{$}\NormalTok{TE\_mean\_w),}
    \AttributeTok{ATE =}\NormalTok{ est\_sub}\SpecialCharTok{$}\NormalTok{TE\_mean\_w,}
    \AttributeTok{SE =}\NormalTok{ est\_sub}\SpecialCharTok{$}\NormalTok{SE\_mean\_w}
\NormalTok{) }\SpecialCharTok{|\textgreater{}}
\NormalTok{    causalverse}\SpecialCharTok{::}\FunctionTok{nice\_tab}\NormalTok{()}
\CommentTok{\#\textgreater{}    Period   ATE   SE}
\CommentTok{\#\textgreater{} 1      {-}2  0.32 0.44}
\CommentTok{\#\textgreater{} 2      {-}1 {-}0.32 0.44}
\CommentTok{\#\textgreater{} 3       0 {-}4.29 1.68}
\CommentTok{\#\textgreater{} 4       1 {-}4.00 1.52}
\CommentTok{\#\textgreater{} 5       2 {-}3.44 2.90}
\CommentTok{\#\textgreater{} 6 cumul.0 {-}4.29 1.68}
\CommentTok{\#\textgreater{} 7 cumul.1 {-}4.14 1.52}
\CommentTok{\#\textgreater{} 8 cumul.2 {-}3.91 1.82}

\NormalTok{causalverse}\SpecialCharTok{::}\FunctionTok{synthdid\_plot\_ate}\NormalTok{(est)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{25-synthdid_files/figure-latex/synthdid subgroup analysis-1} \end{center}

Plot different estimators

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(causalverse)}
\NormalTok{methods }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"synthdid"}\NormalTok{, }\StringTok{"did"}\NormalTok{, }\StringTok{"sc"}\NormalTok{, }\StringTok{"sc\_ridge"}\NormalTok{, }\StringTok{"difp"}\NormalTok{, }\StringTok{"difp\_ridge"}\NormalTok{)}

\NormalTok{estimates }\OtherTok{\textless{}{-}} \FunctionTok{lapply}\NormalTok{(methods, }\ControlFlowTok{function}\NormalTok{(method) \{}
  \FunctionTok{synthdid\_est\_ate}\NormalTok{(}
    \AttributeTok{data               =}\NormalTok{ df,}
    \AttributeTok{adoption\_cohorts   =} \DecValTok{5}\SpecialCharTok{:}\DecValTok{7}\NormalTok{,}
    \AttributeTok{lags               =} \DecValTok{2}\NormalTok{,}
    \AttributeTok{leads              =} \DecValTok{2}\NormalTok{,}
    \AttributeTok{time\_var           =} \StringTok{"year"}\NormalTok{,}
    \AttributeTok{unit\_id\_var        =} \StringTok{"id"}\NormalTok{,}
    \AttributeTok{treated\_period\_var =} \StringTok{"year\_treated"}\NormalTok{,}
    \AttributeTok{treat\_stat\_var     =} \StringTok{"treatvar"}\NormalTok{,}
    \AttributeTok{outcome\_var        =} \StringTok{"y"}\NormalTok{,}
    \AttributeTok{method =}\NormalTok{ method}
\NormalTok{  )}
\NormalTok{\})}

\NormalTok{plots }\OtherTok{\textless{}{-}} \FunctionTok{lapply}\NormalTok{(}\FunctionTok{seq\_along}\NormalTok{(estimates), }\ControlFlowTok{function}\NormalTok{(i) \{}
\NormalTok{  causalverse}\SpecialCharTok{::}\FunctionTok{synthdid\_plot\_ate}\NormalTok{(estimates[[i]],}
                                 \AttributeTok{title =}\NormalTok{ methods[i],}
                                 \AttributeTok{theme =}\NormalTok{ causalverse}\SpecialCharTok{::}\FunctionTok{ama\_theme}\NormalTok{(}\AttributeTok{base\_size =} \DecValTok{6}\NormalTok{))}
\NormalTok{\})}

\NormalTok{gridExtra}\SpecialCharTok{::}\FunctionTok{grid.arrange}\NormalTok{(}\AttributeTok{grobs =}\NormalTok{ plots, }\AttributeTok{ncol =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{difference-in-differences}{%
\chapter{Difference-in-differences}\label{difference-in-differences}}

\href{https://github.com/lnsongxf/DiD-1}{List of packages}

Examples in marketing

\begin{itemize}
\tightlist
\item
  \citep{liaukonyte2015television}: TV ad on online shopping
\item
  \citep{wang2018border}: political ad source and message tone on vote shares and turnout using discontinuities in the level of political ads at the borders
\item
  \citep{datta2018changing}: streaming service on total music consumption using timing of users adoption of a music streaming service
\item
  \citep{janakiraman2018effect}: data breach announcement affect customer spending using timing of data breach and variation whether customer info was breached in that event
\item
  \citep{israeli2018online}: digital monitoring and enforcement on violations using enforcement of min ad price policies
\item
  \citep{ramani2019effects}: firms respond to foreign direct investment liberalization using India's reform in 1991.
\item
  \citep{pattabhiramaiah2019paywalls}: paywall affects readership
\item
  \citep{akca2020value}: aggregators for airlines business effect
\item
  \citep{lim2020competitive}: nutritional labels on nutritional quality for other brands in a category using variation in timing of adoption of nutritional labels across categories
\item
  \citep{guo2020let}: payment disclosure laws effect on physician prescription behavior using Timing of the Massachusetts open payment law as the exogenous shock
\item
  \citep{he2022market}: using Amazon policy change to examine the causal impact of fake reviews on sales, average ratings.
\item
  \citep{peukert2022regulatory}: using European General data protection Regulation, examine the impact of policy change on website usage.
\end{itemize}

Examples in econ:

\begin{itemize}
\item
  \citep{rosenzweig2000natural}
\item
  \citep{angrist2001instrumental}
\item
  \citep{fuchs2016natural}: macro
\end{itemize}

Show the mechanism via

\begin{itemize}
\item
  \protect\hyperlink{mediation-under-did}{Mediation Under DiD} analysis: see \citep{habel2021variable}
\item
  Moderation analysis: see \citep{goldfarb2011online}
\end{itemize}

Steps to trust DID:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Visualize the treatment rollout (e.g., \texttt{panelView}).
\item
  Document the number of treated units in each cohort (e.g., control and treated).
\item
  Visualize the trajectory of average outcomes across cohorts (if you have multiple periods).
\item
  \protect\hyperlink{prior-parallel-trends-test}{Parallel Trends} Conduct an event-study analysis with and without covariates.
\item
  For the case with covariates, check for overlap in covariates between treated and control groups to ensure control group validity (e.g., if the control is relatively small than the treated group, you might not have overlap, and you have to make extrapolation).
\item
  Conduct sensitivity analysis for parallel trend violations (e.g., \texttt{honestDiD}).
\end{enumerate}

\hypertarget{visualization}{%
\section{Visualization}\label{visualization}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(panelView)}
\FunctionTok{library}\NormalTok{(fixest)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{base\_stagg }\OtherTok{\textless{}{-}}\NormalTok{ fixest}\SpecialCharTok{::}\NormalTok{base\_stagg }\SpecialCharTok{|\textgreater{}}
    \CommentTok{\# treatment status}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{treat\_stat =}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{if\_else}\NormalTok{(time\_to\_treatment }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} 
    \FunctionTok{select}\NormalTok{(id, year, treat\_stat, y)}

\FunctionTok{head}\NormalTok{(base\_stagg)}
\CommentTok{\#\textgreater{}   id year treat\_stat           y}
\CommentTok{\#\textgreater{} 2 90    1          0  0.01722971}
\CommentTok{\#\textgreater{} 3 89    1          0 {-}4.58084528}
\CommentTok{\#\textgreater{} 4 88    1          0  2.73817174}
\CommentTok{\#\textgreater{} 5 87    1          0 {-}0.65103066}
\CommentTok{\#\textgreater{} 6 86    1          0 {-}5.33381664}
\CommentTok{\#\textgreater{} 7 85    1          0  0.49562631}

\NormalTok{panelView}\SpecialCharTok{::}\FunctionTok{panelview}\NormalTok{(}
\NormalTok{    y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ treat\_stat,}
    \AttributeTok{data =}\NormalTok{ base\_stagg,}
    \AttributeTok{index =} \FunctionTok{c}\NormalTok{(}\StringTok{"id"}\NormalTok{, }\StringTok{"year"}\NormalTok{),}
    \AttributeTok{xlab =} \StringTok{"Year"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"Unit"}\NormalTok{,}
    \AttributeTok{display.all =}\NormalTok{ F,}
    \AttributeTok{gridOff =}\NormalTok{ T,}
    \AttributeTok{by.timing =}\NormalTok{ T}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-1-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# alternatively specification}
\NormalTok{panelView}\SpecialCharTok{::}\FunctionTok{panelview}\NormalTok{(}
    \AttributeTok{Y =} \StringTok{"y"}\NormalTok{,}
    \AttributeTok{D =} \StringTok{"treat\_stat"}\NormalTok{,}
    \AttributeTok{data =}\NormalTok{ base\_stagg,}
    \AttributeTok{index =} \FunctionTok{c}\NormalTok{(}\StringTok{"id"}\NormalTok{, }\StringTok{"year"}\NormalTok{),}
    \AttributeTok{xlab =} \StringTok{"Year"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"Unit"}\NormalTok{,}
    \AttributeTok{display.all =}\NormalTok{ F,}
    \AttributeTok{gridOff =}\NormalTok{ T,}
    \AttributeTok{by.timing =}\NormalTok{ T}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-1-2} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Average outcomes for each cohort}
\NormalTok{panelView}\SpecialCharTok{::}\FunctionTok{panelview}\NormalTok{(}
    \AttributeTok{data =}\NormalTok{ base\_stagg, }
    \AttributeTok{Y =} \StringTok{"y"}\NormalTok{,}
    \AttributeTok{D =} \StringTok{"treat\_stat"}\NormalTok{,}
    \AttributeTok{index =} \FunctionTok{c}\NormalTok{(}\StringTok{"id"}\NormalTok{, }\StringTok{"year"}\NormalTok{),}
    \AttributeTok{by.timing =}\NormalTok{ T,}
    \AttributeTok{display.all =}\NormalTok{ F,}
    \AttributeTok{type =} \StringTok{"outcome"}\NormalTok{, }
    \AttributeTok{by.cohort =}\NormalTok{ T}
\NormalTok{)}
\CommentTok{\#\textgreater{} Number of unique treatment histories: 10}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-1-3} \end{center}

\hypertarget{simple-dif-n-dif}{%
\section{Simple Dif-n-dif}\label{simple-dif-n-dif}}

\begin{itemize}
\item
  A tool developed intuitively to study ``natural experiment'', but its uses are much broader.
\item
  \protect\hyperlink{fixed-effects-estimator}{Fixed Effects Estimator} is the foundation for DID
\item
  Why is dif-in-dif attractive? Identification strategy: Inter-temporal variation between groups

  \begin{itemize}
  \item
    \textbf{Cross-sectional estimator} helps avoid omitted (unobserved) \textbf{common trends}
  \item
    \textbf{Time-series estimator} helps overcome omitted (unobserved) \textbf{cross-sectional differences}
  \end{itemize}
\end{itemize}

Consider

\begin{itemize}
\item
  \(D_i = 1\) treatment group
\item
  \(D_i = 0\) control group
\item
  \(T= 1\) After the treatment
\item
  \(T =0\) Before the treatment
\end{itemize}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
& After (T = 1) & Before (T = 0) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Treated \(D_i =1\) & \(E[Y_{1i}(1)|D_i = 1]\) & \(E[Y_{0i}(0)|D)i=1]\) \\
Control \(D_i = 0\) & \(E[Y_{0i}(1) |D_i =0]\) & \(E[Y_{0i}(0)|D_i=0]\) \\
\end{longtable}

missing \(E[Y_{0i}(1)|D=1]\)

\textbf{The Average Treatment Effect on Treated (ATT)}

\[
\begin{aligned}
E[Y_1(1) - Y_0(1)|D=1] &= \{E[Y(1)|D=1] - E[Y(1)|D=0] \} \\
&- \{E[Y(0)|D=1] - E[Y(0)|D=0] \}
\end{aligned}
\]

More elaboration:

\begin{itemize}
\tightlist
\item
  For the treatment group, we isolate the difference between being treated and not being treated. If the untreated group would have been affected in a different way, the DiD design and estimate would tell us nothing.
\item
  Alternatively, because we can't observe treatment variation in the control group, we can't say anything about the treatment effect on this group.
\end{itemize}

\textbf{Extension}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{More than 2 groups} (multiple treatments and multiple controls), and more than 2 period (pre and post)
\end{enumerate}

\[
Y_{igt} = \alpha_g + \gamma_t + \beta I_{gt} + \delta X_{igt} + \epsilon_{igt}
\]

where

\begin{itemize}
\item
  \(\alpha_g\) is the group-specific fixed effect
\item
  \(\gamma_t\) = time specific fixed effect
\item
  \(\beta\) = dif-in-dif effect
\item
  \(I_{gt}\) = interaction terms (n treatment indicators x n post-treatment dummies) (capture effect heterogeneity over time)
\end{itemize}

This specification is the ``two-way fixed effects DiD'' - \textbf{TWFE} (i.e., 2 sets of fixed effects: group + time).

\begin{itemize}
\tightlist
\item
  However, if you have \protect\hyperlink{staggered-dif-n-dif}{Staggered Dif-n-dif} (i.e., treatment is applied at different times to different groups). TWFE is really bad.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Long-term Effects}
\end{enumerate}

To examine the dynamic treatment effects (that are not under rollout/staggered design), we can create a centered time variable,

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.3378}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.6622}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Centered Time Variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Period
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\ldots{} & \\
\(t = -1\) & 2 periods before treatment period \\
\(t = 0\) & Last period right before treatment period

Remember to use this period as reference group \\
\(t = 1\) & Treatment period \\
\ldots{} & \\
\end{longtable}

By interacting this factor variable, we can examine the dynamic effect of treatment (i.e., whether it's fading or intensifying)

\[
\begin{aligned}
Y &= \alpha_0 + \alpha_1 Group + \alpha_2 Time  \\
&+ \beta_{-T_1} Treatment+  \beta_{-(T_1 -1)} Treatment + \dots +  \beta_{-1} Treatment \\
&+ \beta_1 + \dots + \beta_{T_2} Treatment
\end{aligned}
\]

where

\begin{itemize}
\item
  \(\beta_0\) is used as the reference group (i.e., drop from the model)
\item
  \(T_1\) is the pre-treatment period
\item
  \(T_2\) is the post-treatment period
\end{itemize}

With more variables (i.e., interaction terms), coefficients estimates can be less precise (i.e., higher SE).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  DiD on the relationship, not levels. Technically, we can apply DiD research design not only on variables, but also on coefficients estimates of some other regression models with before and after a policy is implemented.
\end{enumerate}

Goal:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Pre-treatment coefficients should be non-significant \(\beta_{-T_1}, \dots, \beta_{-1} = 0\) (similar to the \protect\hyperlink{placebo-test}{Placebo Test})
\item
  Post-treatment coefficients are expected to be significant \(\beta_1, \dots, \beta_{T_2} \neq0\)

  \begin{itemize}
  \tightlist
  \item
    You can now examine the trend in post-treatment coefficients (i.e., increasing or decreasing)
  \end{itemize}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(fixest)}

\NormalTok{od }\OtherTok{\textless{}{-}}\NormalTok{ causaldata}\SpecialCharTok{::}\NormalTok{organ\_donations }\SpecialCharTok{\%\textgreater{}\%}
    
    \CommentTok{\# Treatment variable}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{California =}\NormalTok{ State }\SpecialCharTok{==} \StringTok{\textquotesingle{}California\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \CommentTok{\# centered time variable}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{center\_time =} \FunctionTok{as.factor}\NormalTok{(Quarter\_Num }\SpecialCharTok{{-}} \DecValTok{3}\NormalTok{))  }
\CommentTok{\# where 3 is the reference period precedes the treatment period}

\FunctionTok{class}\NormalTok{(od}\SpecialCharTok{$}\NormalTok{California)}
\CommentTok{\#\textgreater{} [1] "logical"}
\FunctionTok{class}\NormalTok{(od}\SpecialCharTok{$}\NormalTok{State)}
\CommentTok{\#\textgreater{} [1] "character"}

\NormalTok{cali }\OtherTok{\textless{}{-}} \FunctionTok{feols}\NormalTok{(Rate }\SpecialCharTok{\textasciitilde{}} \FunctionTok{i}\NormalTok{(center\_time, California, }\AttributeTok{ref =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{|}
\NormalTok{                  State }\SpecialCharTok{+}\NormalTok{ center\_time,}
              \AttributeTok{data =}\NormalTok{ od)}

\FunctionTok{etable}\NormalTok{(cali)}
\CommentTok{\#\textgreater{}                                              cali}
\CommentTok{\#\textgreater{} Dependent Var.:                              Rate}
\CommentTok{\#\textgreater{}                                                  }
\CommentTok{\#\textgreater{} California x center\_time = {-}2    {-}0.0029 (0.0051)}
\CommentTok{\#\textgreater{} California x center\_time = {-}1   0.0063** (0.0023)}
\CommentTok{\#\textgreater{} California x center\_time = 1  {-}0.0216*** (0.0050)}
\CommentTok{\#\textgreater{} California x center\_time = 2  {-}0.0203*** (0.0045)}
\CommentTok{\#\textgreater{} California x center\_time = 3    {-}0.0222* (0.0100)}
\CommentTok{\#\textgreater{} Fixed{-}Effects:                {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} State                                         Yes}
\CommentTok{\#\textgreater{} center\_time                                   Yes}
\CommentTok{\#\textgreater{} \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}
\CommentTok{\#\textgreater{} S.E.: Clustered                         by: State}
\CommentTok{\#\textgreater{} Observations                                  162}
\CommentTok{\#\textgreater{} R2                                        0.97934}
\CommentTok{\#\textgreater{} Within R2                                 0.00979}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes: 0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}

\FunctionTok{iplot}\NormalTok{(cali, }\AttributeTok{pt.join =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-2-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{coefplot}\NormalTok{(cali)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-2-2} \end{center}

\hypertarget{notes-1}{%
\section{Notes}\label{notes-1}}

\begin{itemize}
\item
  \protect\hyperlink{matching-methods}{Matching Methods}

  \begin{itemize}
  \item
    Match treatment and control based on pre-treatment observables
  \item
    Modify SEs appropriately \citep{heckman1997matching}. It's might be easier to just use the \protect\hyperlink{doubly-robust-did}{Doubly Robust DiD} \citep{sant2020doubly} where you just need either matching or regression to work in order to identify your treatment effect
  \item
    Whereas the group fixed effects control for the group time-invariant effects, it does not control for selection bias (i.e., certain groups are more likely to be treated than others). Hence, with these backdoor open (i.e., selection bias) between (1) propensity to be treated and (2) dynamics evolution of the outcome post-treatment, matching can potential close these backdoor.
  \item
    Be careful when matching time-varying covariates because you might encounter ``regression to the mean'' problem, where pre-treatment periods can have an unusually bad or good time (that is out of the ordinary), then the post-treatment period outcome can just be an artifact of the regression to the mean \citep{daw2018matching}. This problem is not of concern to time-invariant variables.
  \item
    Matching and DiD can use pre-treatment outcomes to correct for selection bias. From real world data and simulation, \citep{chabe2015analysis} found that matching generally underestimates the average causal effect and gets closer to the true effect with more number of pre-treatment outcomes. When selection bias is symmetric around the treatment date, DID is still consistent when implemented symmetrically (i.e., the same number of period before and after treatment). In cases where selection bias is asymmetric, the MC simulations show that Symmetric DiD still performs better than Matching.
  \item
    Forward DID is a simple algo that helps choose control units \citep{li2024frontiers}.
  \end{itemize}
\item
  It's always good to show results with and without controls because

  \begin{itemize}
  \item
    If the controls are fixed within group or within time, then those should be absorbed under those fixed effects
  \item
    If the controls are dynamic across group and across, then your parallel trends assumption is not plausible.
  \end{itemize}
\item
  Under causal inference, \(R^2\) is not so important.
\end{itemize}

For count data, one can use the fixed-effects Poisson pseudo-maximum likelihood estimator (PPML) \citep[\citet{puhani2012treatment}]{athey2006identification} (For applied papers, see \citet{burtch2018can} in management and \citet{he2021end} in marketing). This also allows for robust standard errors under over-dispersion \citep{wooldridge1999quasi}.

\begin{itemize}
\item
  This estimator outperforms a log OLS when data have many 0s\citep{silva2011further}, since log-OLS can produce biased estimates \citep{o2010not} under heteroskedascity \citep{silva2006log}.
\item
  For those thinking of negative binomial with fixed effects, there isn't an estimator right now \citep{allison20027}.
\end{itemize}

For {[}Zero-valued Outcomes{]}, we have to distinguish the treatment effect on the intensive (outcome: 10 to 11) vs.~extensive margins (outcome: 0 to 1), and we can't readily interpret the treatment coefficient of log-transformed outcome regression as percentage change \citep{chen2023logs}. Alternatively, we can either focus on

\begin{itemize}
\item
  \textbf{Proportional treatment effects}: \(\theta_{ATT\%} = \frac{E(Y_{it}(1) | D_i = 1, Post_t = 1) - E(Y_{it}(0) |D_i = 1, Post_t = 1)}{E(Y_{it}(0) | D_i = 1 , Post_t = 1}\) (i.e., percentage change in treated group's average post-treatment outcome). Instead of relying on the parallel trends assumption in levels, we could also rely on parallel trends assumption in ratio \citep{wooldridge2023simple}.

  \begin{itemize}
  \item
    We can use Poisson QMLE to estimate the treatment effect: \(Y_{it} = \exp(\beta_0 + D_i \times \beta_1 Post_t + \beta_2 D_i + \beta_3 Post_t + X_{it}) \epsilon_{it}\) and \(\hat{\theta}_{ATT \%} = \exp(\hat{\beta}_1-1)\).
  \item
    To examine the parallel trends assumption in ratio holds, we can also estimate a dynamic version of the Poisson QMLE: \(Y_{it} = \exp(\lambda_t + \beta_2 D_i + \sum_{r \neq -1} \beta_r D_i \times (RelativeTime_t = r)\), we would expect \(\exp(\hat{\beta_r}) - 1 = 0\) for \(r < 0\).
  \item
    Even if we see the plot of these coefficients are 0, we still should run sensitivity analysis \citep{rambachan2023more} to examine violation of this assumption (see \protect\hyperlink{prior-parallel-trends-test}{Prior Parallel Trends Test}).
  \end{itemize}
\item
  \textbf{Log Effects with Calibrated Extensive-margin value}: due to problem with the mean value interpretation of the proportional treatment effects with outcomes that are heavy-tailed, we might be interested in the extensive margin effect. Then, we can explicit model how much weight we put on the intensive vs.~extensive margin \citep[p.~39]{chen2023logs}.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Proportional treatment effects}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{) }\CommentTok{\# For reproducibility}

\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{500} \CommentTok{\# Number of observations per group (treated and control)}
\CommentTok{\# Generating IDs for a panel setup}
\NormalTok{ID }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n, }\AttributeTok{times =} \DecValTok{2}\NormalTok{)}

\CommentTok{\# Defining groups and periods}
\NormalTok{Group }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"Control"}\NormalTok{, }\StringTok{"Treated"}\NormalTok{), }\AttributeTok{each =}\NormalTok{ n)}
\NormalTok{Time }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"Before"}\NormalTok{, }\StringTok{"After"}\NormalTok{), }\AttributeTok{times =}\NormalTok{ n)}
\NormalTok{Treatment }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(Group }\SpecialCharTok{==} \StringTok{"Treated"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{Post }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(Time }\SpecialCharTok{==} \StringTok{"After"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}

\CommentTok{\# Step 1: Generate baseline outcomes with a zero{-}inflated model}
\NormalTok{lambda }\OtherTok{\textless{}{-}} \DecValTok{20} \CommentTok{\# Average rate of occurrence}
\NormalTok{zero\_inflation }\OtherTok{\textless{}{-}} \FloatTok{0.5} \CommentTok{\# Proportion of zeros}
\NormalTok{Y\_baseline }\OtherTok{\textless{}{-}}
    \FunctionTok{ifelse}\NormalTok{(}\FunctionTok{runif}\NormalTok{(}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ n) }\SpecialCharTok{\textless{}}\NormalTok{ zero\_inflation, }\DecValTok{0}\NormalTok{, }\FunctionTok{rpois}\NormalTok{(}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ n, lambda))}

\CommentTok{\# Step 2: Apply DiD treatment effect on the treated group in the post{-}treatment period}
\NormalTok{Treatment\_Effect }\OtherTok{\textless{}{-}}\NormalTok{ Treatment }\SpecialCharTok{*}\NormalTok{ Post}
\NormalTok{Y\_treatment }\OtherTok{\textless{}{-}}
    \FunctionTok{ifelse}\NormalTok{(Treatment\_Effect }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, }\FunctionTok{rpois}\NormalTok{(n, }\AttributeTok{lambda =} \DecValTok{2}\NormalTok{), }\DecValTok{0}\NormalTok{)}

\CommentTok{\# Incorporating a simple time trend, ensuring outcomes are non{-}negative}
\NormalTok{Time\_Trend }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(Time }\SpecialCharTok{==} \StringTok{"After"}\NormalTok{, }\FunctionTok{rpois}\NormalTok{(}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ n, }\AttributeTok{lambda =} \DecValTok{1}\NormalTok{), }\DecValTok{0}\NormalTok{)}

\CommentTok{\# Step 3: Combine to get the observed outcomes}
\NormalTok{Y\_observed }\OtherTok{\textless{}{-}}\NormalTok{ Y\_baseline }\SpecialCharTok{+}\NormalTok{ Y\_treatment }\SpecialCharTok{+}\NormalTok{ Time\_Trend}

\CommentTok{\# Ensure no negative outcomes after the time trend}
\NormalTok{Y\_observed }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(Y\_observed }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, Y\_observed)}

\CommentTok{\# Create the final dataset}
\NormalTok{data }\OtherTok{\textless{}{-}}
    \FunctionTok{data.frame}\NormalTok{(}
        \AttributeTok{ID =}\NormalTok{ ID,}
        \AttributeTok{Treatment =}\NormalTok{ Treatment,}
        \AttributeTok{Period =}\NormalTok{ Post,}
        \AttributeTok{Outcome =}\NormalTok{ Y\_observed}
\NormalTok{    )}

\CommentTok{\# Viewing the first few rows of the dataset}
\FunctionTok{head}\NormalTok{(data)}
\CommentTok{\#\textgreater{}   ID Treatment Period Outcome}
\CommentTok{\#\textgreater{} 1  1         0      0       0}
\CommentTok{\#\textgreater{} 2  2         0      1      25}
\CommentTok{\#\textgreater{} 3  3         0      0       0}
\CommentTok{\#\textgreater{} 4  4         0      1      20}
\CommentTok{\#\textgreater{} 5  5         0      0      19}
\CommentTok{\#\textgreater{} 6  6         0      1       0}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(fixest)}
\NormalTok{res\_pois }\OtherTok{\textless{}{-}}
    \FunctionTok{fepois}\NormalTok{(Outcome }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Treatment }\SpecialCharTok{+}\NormalTok{ Period }\SpecialCharTok{+}\NormalTok{ Treatment }\SpecialCharTok{*}\NormalTok{ Period,}
           \AttributeTok{data =}\NormalTok{ data,}
           \AttributeTok{vcov =} \StringTok{"hetero"}\NormalTok{)}
\FunctionTok{etable}\NormalTok{(res\_pois)}
\CommentTok{\#\textgreater{}                             res\_pois}
\CommentTok{\#\textgreater{} Dependent Var.:              Outcome}
\CommentTok{\#\textgreater{}                                     }
\CommentTok{\#\textgreater{} Constant           2.249*** (0.0717)}
\CommentTok{\#\textgreater{} Treatment           0.1743. (0.0932)}
\CommentTok{\#\textgreater{} Period               0.0662 (0.0960)}
\CommentTok{\#\textgreater{} Treatment x Period   0.0314 (0.1249)}
\CommentTok{\#\textgreater{} \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}
\CommentTok{\#\textgreater{} S.E. type          Heteroskeda.{-}rob.}
\CommentTok{\#\textgreater{} Observations                   1,000}
\CommentTok{\#\textgreater{} Squared Cor.                 0.01148}
\CommentTok{\#\textgreater{} Pseudo R2                    0.00746}
\CommentTok{\#\textgreater{} BIC                         15,636.8}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes: 0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}

\CommentTok{\# Average percentage change}
\FunctionTok{exp}\NormalTok{(}\FunctionTok{coefficients}\NormalTok{(res\_pois)[}\StringTok{"Treatment:Period"}\NormalTok{]) }\SpecialCharTok{{-}} \DecValTok{1}
\CommentTok{\#\textgreater{} Treatment:Period }
\CommentTok{\#\textgreater{}       0.03191643}

\CommentTok{\# SE using delta method}
\FunctionTok{exp}\NormalTok{(}\FunctionTok{coefficients}\NormalTok{(res\_pois)[}\StringTok{"Treatment:Period"}\NormalTok{]) }\SpecialCharTok{*}
    \FunctionTok{sqrt}\NormalTok{(res\_pois}\SpecialCharTok{$}\NormalTok{cov.scaled[}\StringTok{"Treatment:Period"}\NormalTok{, }\StringTok{"Treatment:Period"}\NormalTok{])}
\CommentTok{\#\textgreater{} Treatment:Period }
\CommentTok{\#\textgreater{}        0.1288596}
\end{Highlighting}
\end{Shaded}

In this example, the DID coefficient is not significant. However, say that it's significant, we can interpret the coefficient as 3 percent increase in posttreatment period due to the treatment.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(fixest)}

\NormalTok{base\_did\_log0 }\OtherTok{\textless{}{-}}\NormalTok{ base\_did }\SpecialCharTok{|\textgreater{}} 
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{y =} \FunctionTok{if\_else}\NormalTok{(y }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{, y, }\DecValTok{0}\NormalTok{))}

\NormalTok{res\_pois\_es }\OtherTok{\textless{}{-}}
    \FunctionTok{fepois}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x1 }\SpecialCharTok{+} \FunctionTok{i}\NormalTok{(period, treat, }\DecValTok{5}\NormalTok{) }\SpecialCharTok{|}\NormalTok{ id }\SpecialCharTok{+}\NormalTok{ period,}
           \AttributeTok{data =}\NormalTok{ base\_did\_log0,}
           \AttributeTok{vcov =} \StringTok{"hetero"}\NormalTok{)}

\FunctionTok{etable}\NormalTok{(res\_pois\_es)}
\CommentTok{\#\textgreater{}                            res\_pois\_es}
\CommentTok{\#\textgreater{} Dependent Var.:                      y}
\CommentTok{\#\textgreater{}                                       }
\CommentTok{\#\textgreater{} x1                  0.1895*** (0.0108)}
\CommentTok{\#\textgreater{} treat x period = 1    {-}0.2769 (0.3545)}
\CommentTok{\#\textgreater{} treat x period = 2    {-}0.2699 (0.3533)}
\CommentTok{\#\textgreater{} treat x period = 3     0.1737 (0.3520)}
\CommentTok{\#\textgreater{} treat x period = 4    {-}0.2381 (0.3249)}
\CommentTok{\#\textgreater{} treat x period = 6     0.3724 (0.3086)}
\CommentTok{\#\textgreater{} treat x period = 7    0.7739* (0.3117)}
\CommentTok{\#\textgreater{} treat x period = 8    0.5028. (0.2962)}
\CommentTok{\#\textgreater{} treat x period = 9   0.9746** (0.3092)}
\CommentTok{\#\textgreater{} treat x period = 10  1.310*** (0.3193)}
\CommentTok{\#\textgreater{} Fixed{-}Effects:      {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} id                                 Yes}
\CommentTok{\#\textgreater{} period                             Yes}
\CommentTok{\#\textgreater{} \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}
\CommentTok{\#\textgreater{} S.E. type           Heteroskedas.{-}rob.}
\CommentTok{\#\textgreater{} Observations                     1,080}
\CommentTok{\#\textgreater{} Squared Cor.                   0.51131}
\CommentTok{\#\textgreater{} Pseudo R2                      0.34836}
\CommentTok{\#\textgreater{} BIC                            5,868.8}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes: 0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\FunctionTok{iplot}\NormalTok{(res\_pois\_es)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/estiamte teh proprortion treatment effects for event-study form-1} \end{center}

This parallel trend is the ``ratio'' version as in \citet{wooldridge2023simple} :

\[
\frac{E(Y_{it}(0) |D_i = 1, Post_t = 1)}{E(Y_{it}(0) |D_i = 1, Post_t = 0)} = \frac{E(Y_{it}(0) |D_i = 0, Post_t = 1)}{E(Y_{it}(0) |D_i =0, Post_t = 0)}
\]

which means without treatment, the average percentage change in the mean outcome for treated group is identical to that of the control group.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Log Effects with Calibrated Extensive-margin value}
\end{enumerate}

If we want to study the treatment effect on a concave transformation of the outcome that is less influenced by those in the distribution's tail, then we can perform this analysis.

Steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Normalize the outcomes such that 1 represents the minimum non-zero and positve value (i.e., divide the outcome by its minimum non-zero and positive value).
\item
  Estimate the treatment effects for the new outcome
\end{enumerate}

\[
m(y) =
\begin{cases}
\log(y) & \text{for } y >0 \\
-x & \text{for } y = 0
\end{cases}
\]

The choice of \(x\) depends on what the researcher is interested in:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.0777}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.9223}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Value of \(x\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Interest
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(x = 0\) & The treatment effect in logs where all zero-valued outcomes are set to equal the minimum non-zero value (i.e., we exclude the extensive-margin change between 0 and \(y_{min}\) ) \\
\(x>0\) & Setting the change between 0 and \(y_{min}\) to be valued as the equivalent of a \(x\) log point change along the intensive margin. \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(fixest)}
\NormalTok{base\_did\_log0\_cali }\OtherTok{\textless{}{-}}\NormalTok{ base\_did\_log0 }\SpecialCharTok{|\textgreater{}} 
    \CommentTok{\# get min }
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{min\_y =} \FunctionTok{min}\NormalTok{(y[y }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{])) }\SpecialCharTok{|\textgreater{}} 
    
    \CommentTok{\# normalized the outcome }
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{y\_norm =}\NormalTok{ y }\SpecialCharTok{/}\NormalTok{ min\_y)}

\NormalTok{my\_regression }\OtherTok{\textless{}{-}}
    \ControlFlowTok{function}\NormalTok{(x) \{}
\NormalTok{        base\_did\_log0\_cali }\OtherTok{\textless{}{-}}
\NormalTok{            base\_did\_log0\_cali }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{(}\AttributeTok{my =} \FunctionTok{ifelse}\NormalTok{(y\_norm }\SpecialCharTok{==} \DecValTok{0}\NormalTok{,}\SpecialCharTok{{-}}\NormalTok{x,}
                                                      \FunctionTok{log}\NormalTok{(y\_norm)))}
\NormalTok{        my\_reg }\OtherTok{\textless{}{-}}
            \FunctionTok{feols}\NormalTok{(}
                \AttributeTok{fml =}\NormalTok{ my }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x1 }\SpecialCharTok{+} \FunctionTok{i}\NormalTok{(period, treat, }\DecValTok{5}\NormalTok{) }\SpecialCharTok{|}\NormalTok{ id }\SpecialCharTok{+}\NormalTok{ period,}
                \AttributeTok{data =}\NormalTok{ base\_did\_log0\_cali,}
                \AttributeTok{vcov =} \StringTok{"hetero"}
\NormalTok{            )}
        
        \FunctionTok{return}\NormalTok{(my\_reg)}
\NormalTok{    \}}

\NormalTok{xvec }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, .}\DecValTok{1}\NormalTok{, .}\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{reg\_list }\OtherTok{\textless{}{-}}\NormalTok{ purrr}\SpecialCharTok{::}\FunctionTok{map}\NormalTok{(}\AttributeTok{.x =}\NormalTok{ xvec, }\AttributeTok{.f =}\NormalTok{ my\_regression)}


\FunctionTok{iplot}\NormalTok{(reg\_list, }
      \AttributeTok{pt.col =}  \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(xvec),}
      \AttributeTok{pt.pch =} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(xvec))}
\FunctionTok{legend}\NormalTok{(}\StringTok{"topleft"}\NormalTok{, }
       \AttributeTok{col =} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(xvec),}
       \AttributeTok{pch =} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(xvec),}
       \AttributeTok{legend =} \FunctionTok{as.character}\NormalTok{(xvec))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-3-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]


\FunctionTok{etable}\NormalTok{(}
\NormalTok{    reg\_list,}
    \AttributeTok{headers =} \FunctionTok{list}\NormalTok{(}\StringTok{"Extensive{-}margin value (x)"} \OtherTok{=} \FunctionTok{as.character}\NormalTok{(xvec)),}
    \AttributeTok{digits =} \DecValTok{2}\NormalTok{,}
    \AttributeTok{digits.stats =} \DecValTok{2}
\NormalTok{)}
\CommentTok{\#\textgreater{}                                   model 1        model 2        model 3}
\CommentTok{\#\textgreater{} Extensive{-}margin value (x)              0            0.1            0.5}
\CommentTok{\#\textgreater{} Dependent Var.:                        my             my             my}
\CommentTok{\#\textgreater{}                                                                        }
\CommentTok{\#\textgreater{} x1                         0.43*** (0.02) 0.44*** (0.02) 0.46*** (0.03)}
\CommentTok{\#\textgreater{} treat x period = 1           {-}0.92 (0.67)   {-}0.94 (0.69)    {-}1.0 (0.73)}
\CommentTok{\#\textgreater{} treat x period = 2           {-}0.41 (0.66)   {-}0.42 (0.67)   {-}0.43 (0.71)}
\CommentTok{\#\textgreater{} treat x period = 3           {-}0.34 (0.67)   {-}0.35 (0.68)   {-}0.38 (0.73)}
\CommentTok{\#\textgreater{} treat x period = 4            {-}1.0 (0.67)    {-}1.0 (0.68)    {-}1.1 (0.73)}
\CommentTok{\#\textgreater{} treat x period = 6            0.44 (0.66)    0.44 (0.67)    0.45 (0.72)}
\CommentTok{\#\textgreater{} treat x period = 7            1.1. (0.64)    1.1. (0.65)    1.2. (0.70)}
\CommentTok{\#\textgreater{} treat x period = 8            1.1. (0.64)    1.1. (0.65)     1.1 (0.69)}
\CommentTok{\#\textgreater{} treat x period = 9           1.7** (0.65)   1.7** (0.66)    1.8* (0.70)}
\CommentTok{\#\textgreater{} treat x period = 10         2.4*** (0.62)  2.4*** (0.63)  2.5*** (0.68)}
\CommentTok{\#\textgreater{} Fixed{-}Effects:             {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} id                                    Yes            Yes            Yes}
\CommentTok{\#\textgreater{} period                                Yes            Yes            Yes}
\CommentTok{\#\textgreater{} \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_}
\CommentTok{\#\textgreater{} S.E. type                  Heterosk.{-}rob. Heterosk.{-}rob. Heterosk.{-}rob.}
\CommentTok{\#\textgreater{} Observations                        1,080          1,080          1,080}
\CommentTok{\#\textgreater{} R2                                   0.43           0.43           0.43}
\CommentTok{\#\textgreater{} Within R2                            0.26           0.26           0.25}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}                                   model 4        model 5}
\CommentTok{\#\textgreater{} Extensive{-}margin value (x)              1              3}
\CommentTok{\#\textgreater{} Dependent Var.:                        my             my}
\CommentTok{\#\textgreater{}                                                         }
\CommentTok{\#\textgreater{} x1                         0.49*** (0.03) 0.62*** (0.04)}
\CommentTok{\#\textgreater{} treat x period = 1            {-}1.1 (0.79)     {-}1.5 (1.0)}
\CommentTok{\#\textgreater{} treat x period = 2           {-}0.44 (0.77)   {-}0.51 (0.99)}
\CommentTok{\#\textgreater{} treat x period = 3           {-}0.43 (0.78)    {-}0.60 (1.0)}
\CommentTok{\#\textgreater{} treat x period = 4            {-}1.2 (0.78)     {-}1.5 (1.0)}
\CommentTok{\#\textgreater{} treat x period = 6            0.45 (0.77)     0.46 (1.0)}
\CommentTok{\#\textgreater{} treat x period = 7             1.2 (0.75)     1.3 (0.97)}
\CommentTok{\#\textgreater{} treat x period = 8             1.2 (0.74)     1.3 (0.96)}
\CommentTok{\#\textgreater{} treat x period = 9            1.8* (0.75)    2.1* (0.97)}
\CommentTok{\#\textgreater{} treat x period = 10         2.7*** (0.73)  3.2*** (0.94)}
\CommentTok{\#\textgreater{} Fixed{-}Effects:             {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} id                                    Yes            Yes}
\CommentTok{\#\textgreater{} period                                Yes            Yes}
\CommentTok{\#\textgreater{} \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_}
\CommentTok{\#\textgreater{} S.E. type                  Heterosk.{-}rob. Heterosk.{-}rob.}
\CommentTok{\#\textgreater{} Observations                        1,080          1,080}
\CommentTok{\#\textgreater{} R2                                   0.42           0.41}
\CommentTok{\#\textgreater{} Within R2                            0.25           0.24}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes: 0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

We have the dynamic treatment effects for different hypothesized extensive-margin value of \(x \in (0, .1, .5, 1, 3, 5)\)

The first column is when the zero-valued outcome equal to \(y_{min, y>0}\) (i.e., there is no different between the minimum outcome and zero outcome - \(x = 0\))

For this particular example, as the extensive margin increases, we see an increase in the effect magnitude. The second column is when we assume an extensive-margin change from 0 to \(y_{min, y >0}\) is equivalent to a 10 (i.e., \(0.1 \times 100\)) log point change along the intensive margin.

\hypertarget{standard-errors-2}{%
\section{Standard Errors}\label{standard-errors-2}}

Serial correlation is a big problem in DiD because \citep{bertrand2004much}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  DiD often uses long time series
\item
  Outcomes are often highly positively serially correlated
\item
  Minimal variation in the treatment variable over time within a group (e.g., state).
\end{enumerate}

To overcome this problem:

\begin{itemize}
\tightlist
\item
  Using parametric correction (standard AR correction) is not good.
\item
  Using nonparametric (e.g., \textbf{block bootstrap}- keep all obs from the same group such as state together) is good when number of groups is large.
\item
  Remove time series dimension (i.e., aggregate data into 2 periods: pre and post). This still works with small number of groups (See \citep{donald2007inference} for more notes on small-sample aggregation).
\item
  Empirical and arbitrary variance-covariance matrix corrections work only in large samples.
\end{itemize}

\hypertarget{examples}{%
\section{Examples}\label{examples}}

Example by \href{https://rpubs.com/phle/r_tutorial_difference_in_differences}{Philipp Leppert} replicating \href{https://davidcard.berkeley.edu/data_sets.html}{Card and Krueger (1994)}

Example by \href{https://bookdown.org/aschmi11/causal_inf/difference-in-differences.html}{Anthony Schmidt}

\hypertarget{example-by-doleac2020unintended}{%
\subsection{\texorpdfstring{Example by \citet{doleac2020unintended}}{Example by @doleac2020unintended}}\label{example-by-doleac2020unintended}}

\begin{itemize}
\item
  The purpose of banning a checking box for ex-criminal was banned because we thought that it gives more access to felons
\item
  Even if we ban the box, employers wouldn't just change their behaviors. But then the unintended consequence is that employers statistically discriminate based on race
\end{itemize}

3 types of ban the box

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Public employer only
\item
  Private employer with government contract
\item
  All employers
\end{enumerate}

Main identification strategy

\begin{itemize}
\tightlist
\item
  If any county in the Metropolitan Statistical Area (MSA) adopts ban the box, it means the whole MSA is treated. Or if the state adopts ``ban the ban,'' every county is treated
\end{itemize}

Under \protect\hyperlink{simple-dif-n-dif}{Simple Dif-n-dif}

\[ Y_{it} = \beta_0 + \beta_1 Post_t + \beta_2 treat_i + \beta_2 (Post_t \times Treat_i) + \epsilon_{it} \]

But if there is no common post time, then we should use \protect\hyperlink{staggered-dif-n-dif}{Staggered Dif-n-dif}

\[ \begin{aligned} E_{imrt} &= \alpha + \beta_1 BTB_{imt} W_{imt} + \beta_2 BTB_{mt} + \beta_3 BTB_{mt} H_{imt}\\  &+ \delta_m + D_{imt} \beta_5 + \lambda_{rt} + \delta_m\times f(t) \beta_7 + e_{imrt} \end{aligned} \]

where

\begin{itemize}
\item
  \(i\) = person; \(m\) = MSA; \(r\) = region (US regions e.g., Midwest) ; \(r\) = region; \(t\) = year
\item
  \(W\) = White; \(B\) = Black; \(H\) = Hispanic
\item
  \(\beta_1 BTB_{imt} W_{imt} + \beta_2 BTB_{mt} + \beta_3 BTB_{mt} H_{imt}\) are the 3 dif-n-dif variables (\(BTB\) = ``ban the box'')
\item
  \(\delta_m\) = dummy for MSI
\item
  \(D_{imt}\) = control for people
\item
  \(\lambda_{rt}\) = region by time fixed effect
\item
  \(\delta_m \times f(t)\) = linear time trend within MSA (but we should not need this if we have good pre-trend)
\end{itemize}

If we put \(\lambda_r - \lambda_t\) (separately) we will more broad fixed effect, while \(\lambda_{rt}\) will give us deeper and narrower fixed effect.

Before running this model, we have to drop all other races. And \(\beta_1, \beta_2, \beta_3\) are not collinear because there are all interaction terms with \(BTB_{mt}\)

If we just want to estimate the model for black men, we will modify it to be

\[ E_{imrt} = \alpha + BTB_{mt} \beta_1 + \delta_m + D_{imt} \beta_5 + \lambda_{rt} + (\delta_m \times f(t)) \beta_7 + e_{imrt} \]

\[ \begin{aligned} E_{imrt} &= \alpha + BTB_{m (t - 3t)} \theta_1 + BTB_{m(t-2)} \theta_2 + BTB_{mt} \theta_4 \\ &+ BTB_{m(t+1)}\theta_5 + BTB_{m(t+2)}\theta_6 + BTB_{m(t+3t)}\theta_7 \\ &+ [\delta_m + D_{imt}\beta_5 + \lambda_r + (\delta_m \times (f(t))\beta_7 + e_{imrt}] \end{aligned} \]

We have to leave \(BTB_{m(t-1)}\theta_3\) out for the category would not be perfect collinearity

So the year before BTB (\(\theta_1, \theta_2, \theta_3\)) should be similar to each other (i.e., same pre-trend). Remember, we only run for places with BTB.

If \(\theta_2\) is statistically different from \(\theta_3\) (baseline), then there could be a problem, but it could also make sense if we have pre-trend announcement.

\hypertarget{example-from-princeton}{%
\subsection{\texorpdfstring{Example from \href{https://www.princeton.edu/~otorres/DID101R.pdf}{Princeton}}{Example from Princeton}}\label{example-from-princeton}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(foreign)}
\NormalTok{mydata }\OtherTok{=} \FunctionTok{read.dta}\NormalTok{(}\StringTok{"http://dss.princeton.edu/training/Panel101.dta"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \CommentTok{\# create a dummy variable to indicate the time when the treatment started}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{time =} \FunctionTok{ifelse}\NormalTok{(year }\SpecialCharTok{\textgreater{}=} \DecValTok{1994}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
    \CommentTok{\# create a dummy variable to identify the treatment group}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{treated =} \FunctionTok{ifelse}\NormalTok{(country }\SpecialCharTok{==} \StringTok{"E"} \SpecialCharTok{|}
\NormalTok{                                country }\SpecialCharTok{==} \StringTok{"F"} \SpecialCharTok{|}\NormalTok{ country }\SpecialCharTok{==} \StringTok{"G"}\NormalTok{ ,}
                            \DecValTok{1}\NormalTok{,}
                            \DecValTok{0}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
    \CommentTok{\# create an interaction between time and treated}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{did =}\NormalTok{ time }\SpecialCharTok{*}\NormalTok{ treated)}
\end{Highlighting}
\end{Shaded}

estimate the DID estimator

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{didreg }\OtherTok{=} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ treated }\SpecialCharTok{+}\NormalTok{ time }\SpecialCharTok{+}\NormalTok{ did, }\AttributeTok{data =}\NormalTok{ mydata)}
\FunctionTok{summary}\NormalTok{(didreg)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = y \textasciitilde{} treated + time + did, data = mydata)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}        Min         1Q     Median         3Q        Max }
\CommentTok{\#\textgreater{} {-}9.768e+09 {-}1.623e+09  1.167e+08  1.393e+09  6.807e+09 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}               Estimate Std. Error t value Pr(\textgreater{}|t|)  }
\CommentTok{\#\textgreater{} (Intercept)  3.581e+08  7.382e+08   0.485   0.6292  }
\CommentTok{\#\textgreater{} treated      1.776e+09  1.128e+09   1.575   0.1200  }
\CommentTok{\#\textgreater{} time         2.289e+09  9.530e+08   2.402   0.0191 *}
\CommentTok{\#\textgreater{} did         {-}2.520e+09  1.456e+09  {-}1.731   0.0882 .}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 2.953e+09 on 66 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.08273,    Adjusted R{-}squared:  0.04104 }
\CommentTok{\#\textgreater{} F{-}statistic: 1.984 on 3 and 66 DF,  p{-}value: 0.1249}
\end{Highlighting}
\end{Shaded}

The \texttt{did} coefficient is the differences-in-differences estimator. Treat has a negative effect

\hypertarget{example-by-card1993minimum}{%
\subsection{\texorpdfstring{Example by \citet{card1993minimum}}{Example by @card1993minimum}}\label{example-by-card1993minimum}}

found that increase in minimum wage increases employment

Experimental Setting:

\begin{itemize}
\item
  New Jersey (treatment) increased minimum wage
\item
  Penn (control) did not increase minimum wage
\end{itemize}

\begin{longtable}[]{@{}lllll@{}}
\toprule\noalign{}
& & After & Before & \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Treatment & NJ & A & B & A - B \\
Control & PA & C & D & C - D \\
& & A - C & B - D & (A - B) - (C - D) \\
\end{longtable}

where

\begin{itemize}
\item
  A - B = treatment effect + effect of time (additive)
\item
  C - D = effect of time
\item
  (A - B) - (C - D) = dif-n-dif
\end{itemize}

\textbf{The identifying assumptions}:

\begin{itemize}
\item
  Can't have \textbf{switchers}
\item
  PA is the control group

  \begin{itemize}
  \item
    is a good counter factual
  \item
    is what NJ would look like if they hadn't had the treatment
  \end{itemize}
\end{itemize}

\[
Y_{jt} = \beta_0 + NJ_j \beta_1 + POST_t \beta_2 + (NJ_j \times POST_t)\beta_3+ X_{jt}\beta_4 + \epsilon_{jt}
\]

where

\begin{itemize}
\item
  \(j\) = restaurant
\item
  \(NJ\) = dummy where \(1 = NJ\), and \(0 = PA\)
\item
  \(POST\) = dummy where \(1 = post\), and \(0 = pre\)
\end{itemize}

Notes:

\begin{itemize}
\item
  We don't need \(\beta_4\) in our model to have unbiased \(\beta_3\), but including it would give our coefficients efficiency
\item
  If we use \(\Delta Y_{jt}\) as the dependent variable, we don't need \(POST_t \beta_2\) anymore
\item
  Alternative model specification is that the authors use NJ high wage restaurant as control group (still choose those that are close to the border)
\item
  The reason why they can't control for everything (PA + NJ high wage) is because it's hard to interpret the causal treatment
\item
  Dif-n-dif utilizes similarity in pretrend of the dependent variables. However, this is neither a necessary nor sufficient for the identifying assumption.

  \begin{itemize}
  \item
    It's not sufficient because they can have multiple treatments (technically, you could include more control, but your treatment can't interact)
  \item
    It's not necessary because trends can be parallel after treatment
  \end{itemize}
\item
  However, we can't never be certain; we just try to find evidence consistent with our theory so that dif-n-dif can work.
\item
  Notice that we don't need before treatment the \textbf{levels of the dependent variable} to be the same (e.g., same wage average in both NJ and PA), dif-n-dif only needs \textbf{pre-trend (i.e., slope)} to be the same for the two groups.
\end{itemize}

\hypertarget{example-by-butcher2014effects}{%
\subsection{\texorpdfstring{Example by \citet{butcher2014effects}}{Example by @butcher2014effects}}\label{example-by-butcher2014effects}}

Theory:

\begin{itemize}
\item
  Highest achieving students are usually in hard science. Why?

  \begin{itemize}
  \item
    Hard to give students students the benefit of doubt for hard science
  \item
    How unpleasant and how easy to get a job. Degrees with lower market value typically want to make you feel more pleasant
  \end{itemize}
\end{itemize}

Under OLS

\[
E_{ij} = \beta_0 + X_i \beta_1 + G_j \beta_2 + \epsilon_{ij}
\]

where

\begin{itemize}
\item
  \(X_i\) = student attributes
\item
  \(\beta_2\) = causal estimate (from grade change)
\item
  \(E_{ij}\) = Did you choose to enroll in major \(j\)
\item
  \(G_j\) = grade given in major \(j\)
\end{itemize}

Examine \(\hat{\beta}_2\)

\begin{itemize}
\item
  Negative bias: Endogenous response because department with lower enrollment rate will give better grade
\item
  Positive bias: hard science is already having best students (i.e., ability), so if they don't their grades can be even lower
\end{itemize}

Under dif-n-dif

\[
Y_{idt} = \beta_0 + POST_t \beta_1 + Treat_d \beta_2 + (POST_t \times Treat_d)\beta_3 + X_{idt} + \epsilon_{idt}
\]

where

\begin{itemize}
\tightlist
\item
  \(Y_{idt}\) = grade average
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.4000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1222}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1222}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1556}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Intercept
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Treat
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Post
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Treat*Post
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Treat Pre & 1 & 1 & 0 & 0 \\
Treat Post & 1 & 1 & 1 & 1 \\
Control Pre & 1 & 0 & 0 & 0 \\
Control Post & 1 & 0 & 1 & 0 \\
& Average for pre-control \(\beta_0\) & & & \\
\end{longtable}

A more general specification of the dif-n-dif is that

\[
Y_{idt} = \alpha_0 + (POST_t \times Treat_d) \alpha_1 + \theta_d + \delta_t + X_{idt} + u_{idt}
\]

where

\begin{itemize}
\item
  \((\theta_d + \delta_t)\) richer , more df than \(Treat_d \beta_2 + Post_t \beta_1\) (because fixed effects subsume Post and treat)
\item
  \(\alpha_1\) should be equivalent to \(\beta_3\) (if your model assumptions are correct)
\end{itemize}

\hypertarget{one-difference}{%
\section{One Difference}\label{one-difference}}

The regression formula is as follows \citep{liaukonyte2023frontiers}:

\[
y_{ut} = \beta \text{Post}_t + \gamma_u + \gamma_w(t) + \gamma_l + \gamma_g(u)p(t) + \epsilon_{ut}
\]

where

\begin{itemize}
\tightlist
\item
  \(y_{ut}\): Outcome of interest for unit u in time t.
\item
  \(\text{Post}_t\): Dummy variable representing a specific post-event period.
\item
  \(\beta\): Coefficient measuring the average change in the outcome after the event relative to the pre-period.
\item
  \(\gamma_u\): Fixed effects for each unit.
\item
  \(\gamma_w(t)\): Time-specific fixed effects to account for periodic variations.
\item
  \(\gamma_l\): Dummy variable for a specific significant period (e.g., a major event change).
\item
  \(\gamma_g(u)p(t)\): Group x period fixed effects for flexible trends that may vary across different categories (e.g., geographical regions) and periods.
\item
  \(\epsilon_{ut}\): Error term.
\end{itemize}

This model can be used to analyze the impact of an event on the outcome of interest while controlling for various fixed effects and time-specific variations, but using units themselves pre-treatment as controls.

\hypertarget{two-way-fixed-effects}{%
\section{Two-way Fixed-effects}\label{two-way-fixed-effects}}

A generalization of the dif-n-dif model is the two-way fixed-effects models where you have multiple groups and time effects. But this is not a designed-based, non-parametric causal estimator \citep{imai2021use}

When applying TWFE to multiple groups and multiple periods, the supposedly causal coefficient is the weighted average of all two-group/two-period DiD estimators in the data where some of the weights can be negative. More specifically, the weights are proportional to group sizes and treatment indicator's variation in each pair, where units in the middle of the panel have the highest weight.

The canonical/standard TWFE only works when

\begin{itemize}
\item
  Effects are homogeneous across units and across time periods (i.e., no dynamic changes in the effects of treatment). See \citep{goodman2021difference, de2020two, sun2021estimating, borusyak2021revisiting} for details. Similarly, it relies on the assumption of \textbf{linear additive effects} \citep{imai2021use}

  \begin{itemize}
  \item
    Have to argue why treatment heterogeneity is not a problem (e.g., plot treatment timing and decompose treatment coefficient using \protect\hyperlink{goodman-bacon-decomposition}{Goodman-Bacon Decomposition}) know the percentage of observation are never treated (because as the never-treated group increases, the bias of TWFE decreases, with 80\% sample to be never-treated, bias is negligible). The problem is worsen when you have long-run effects.
  \item
    Need to manually drop two relative time periods if everyone is eventually treated (to avoid multicollinearity). Programs might do this randomly and if it chooses to drop a post-treatment period, it will create biases. The choice usually -1, and -2 periods.
  \item
    Treatment heterogeneity can come in because (1) it might take some time for a treatment to have measurable changes in outcomes or (2) for each period after treatment, the effect can be different (phase in or increasing effects).
  \end{itemize}
\item
  2 time periods.
\end{itemize}

Within this setting, TWFE works because, using the baseline (e.g., control units where their treatment status is unchanged across time periods), the comparison can be

\begin{itemize}
\item
  Good for

  \begin{itemize}
  \item
    Newly treated units vs.~control
  \item
    Newly treated units vs not-yet treated
  \end{itemize}
\item
  Bad for

  \begin{itemize}
  \tightlist
  \item
    Newly treated vs.~already treated (because already treated cannot serve as the potential outcome for the newly treated).
  \item
    Strict exogeneity (i.e., time-varying confounders, feedback from past outcome to treatment) \citep{imai2019should}
  \item
    Specific functional forms (i.e., treatment effect homogeneity and no carryover effects or anticipation effects) \citep{imai2019should}
  \end{itemize}
\end{itemize}

Note: Notation for this section is consistent with \citep{arkhangelsky2021double}

\[
Y_{it} = \alpha_i + \lambda_t + \tau W_{it} + \beta X_{it} + \epsilon_{it}
\]

where

\begin{itemize}
\item
  \(Y_{it}\) is the outcome
\item
  \(\alpha_i\) is the unit FE
\item
  \(\lambda_t\) is the time FE
\item
  \(\tau\) is the causal effect of treatment
\item
  \(W_{it}\) is the treatment indicator
\item
  \(X_{it}\) are covariates
\end{itemize}

When \(T = 2\), the TWFE is the traditional DiD model

Under the following assumption, \(\hat{\tau}_{OLS}\) is unbiased:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  homogeneous treatment effect
\item
  parallel trends assumptions
\item
  linear additive effects \citep{imai2021use}
\end{enumerate}

\textbf{Remedies for TWFE's shortcomings}

\begin{itemize}
\item
  \citep{goodman2021difference}: diagnostic robustness tests of the TWFE DiD and identify influential observations to the DiD estimate (\protect\hyperlink{goodman-bacon-decomposition}{Goodman-Bacon Decomposition})
\item
  \citep{callaway2021difference}: 2-step estimation with a bootstrap procedure that can account for autocorrelation and clustering,

  \begin{itemize}
  \item
    the parameters of interest are the group-time average treatment effects, where each group is defined by when it was first treated (\protect\hyperlink{multiple-periods-and-variation-in-treatment-timing}{Multiple periods and variation in treatment timing})
  \item
    Comparing post-treatment outcomes fo groups treated in a period against a similar group that is never treated (using matching).
  \item
    Treatment status cannot switch (once treated, stay treated for the rest of the panel)
  \item
    Package: \texttt{did}
  \end{itemize}
\item
  \citep{sun2021estimating}: a specialization of \citep{callaway2021difference} in the event-study context.

  \begin{itemize}
  \item
    They include lags and leads in their design
  \item
    have cohort-specific estimates (similar to group-time estimates in \citep{callaway2021difference}
  \item
    They propose the ``interaction-weighted'' estimator.
  \item
    Package: \texttt{fixest}
  \end{itemize}
\item
  \citep{imai2021use}

  \begin{itemize}
  \item
    Different from \citep{callaway2021difference} because they allow units to switch in and out of treatment.
  \item
    Based on matching methods, to have weighted TWFE
  \item
    Package: \texttt{wfe} and \texttt{PanelMatch}
  \end{itemize}
\item
  \citep{gardner2022two}: two-stage DiD

  \begin{itemize}
  \tightlist
  \item
    \texttt{did2s}
  \end{itemize}
\item
  In cases with an unaffected unit (i.e., never-treated), using the exposure-adjusted difference-in-differences estimators can recover the average treatment effect \citep{de2020two}. However, if you want to see the treatment effect heterogeneity (in cases where the true heterogeneous treatment effects vary by the exposure rate), exposure-adjusted did still fails \citep{sun2022linear}.
\item
  \citep{arkhangelsky2021double}: see below
\end{itemize}

To be robust against

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  time- and unit-varying effects
\end{enumerate}

We can use the reshaped inverse probability weighting (RIPW)- TWFE estimator

With the following assumptions:

\begin{itemize}
\item
  SUTVA
\item
  Binary treatment: \(\mathbf{W}_i = (W_{i1}, \dots, W_{it})\) where \(\mathbf{W}_i \sim \mathbf{\pi}_i\) generalized propensity score (i.e., each person treatment likelihood follow \(\pi\) regardless of the period)
\end{itemize}

Then, the unit-time specific effect is \(\tau_{it} = Y_{it}(1) - Y_{it}(0)\)

Then the Doubly Average Treatment Effect (DATE) is

\[
\tau(\xi) = \sum_{T=1}^T \xi_t \left(\frac{1}{n} \sum_{i = 1}^n \tau_{it} \right)
\]

where

\begin{itemize}
\item
  \(\frac{1}{n} \sum_{i = 1}^n \tau_{it}\) is the unweighted effect of treatment across units (i.e., time-specific ATE).
\item
  \(\xi = (\xi_1, \dots, \xi_t)\) are user-specific weights for each time period.
\item
  This estimand is called DATE because it's weighted (averaged) across both time and units.
\end{itemize}

A special case of DATE is when both time and unit-weights are equal

\[
\tau_{eq} = \frac{1}{nT} \sum_{t=1}^T \sum_{i = 1}^n \tau_{it} 
\]

Borrowing the idea of inverse propensity-weighted least squares estimator in the cross-sectional case that we reweight the objective function via the treatment assignment mechanism:

\[
\hat{\tau} \triangleq \arg \min_{\tau} \sum_{i = 1}^n (Y_i -\mu - W_i \tau)^2 \frac{1}{\pi_i (W_i)}
\]

where

\begin{itemize}
\item
  the first term is the least squares objective
\item
  the second term is the propensity score
\end{itemize}

In the panel data case, the IPW estimator will be

\[
\hat{\tau}_{IPW} \triangleq \arg \min_{\tau} \sum_{i = 1}^n \sum_{t =1}^T (Y_{i t}-\alpha_i - \lambda_t - W_{it} \tau)^2 \frac{1}{\pi_i (W_i)}
\]

Then, to have DATE that users can specify the structure of time weight, we use reshaped IPW estimator \citep{arkhangelsky2021double}

\[
\hat{\tau}_{RIPW} (\Pi) \triangleq \arg \min_{\tau} \sum_{i = 1}^n \sum_{t =1}^T (Y_{i t}-\alpha_i - \lambda_t - W_{it} \tau)^2 \frac{\Pi(W_i)}{\pi_i (W_i)}
\]

where it's a function of a data-independent distribution \(\Pi\) that depends on the support of the treatment path \(\mathbb{S} = \cup_i Supp(W_i)\)

This generalization can transform to

\begin{itemize}
\item
  IPW-TWFE estimator when \(\Pi \sim Unif(\mathbb{S})\)
\item
  randomized experiment when \(\Pi = \pi_i\)
\end{itemize}

To choose \(\Pi\), we don't need to data, we just need possible assignments in your setting.

\begin{itemize}
\item
  For most practical problems (DiD, staggered, transient), we have closed form solutions
\item
  For generic solver, we can use nonlinear programming (e..g, BFGS algorithm)
\end{itemize}

As argued in \citep{imai2021use} that TWFE is not a non-parametric approach, it can be subjected to incorrect model assumption (i.e., model dependence).

\begin{itemize}
\item
  Hence, they advocate for matching methods for time-series cross-sectional data \citep{imai2021use}
\item
  Use \texttt{wfe} and \texttt{PanelMatch} to apply their paper.
\end{itemize}

This package is based on \citep{somaini2016algorithm}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# dataset}
\FunctionTok{library}\NormalTok{(bacondecomp)}
\NormalTok{df }\OtherTok{\textless{}{-}}\NormalTok{ bacondecomp}\SpecialCharTok{::}\NormalTok{castle}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# devtools::install\_github("paulosomaini/xtreg2way")}

\FunctionTok{library}\NormalTok{(xtreg2way)}
\CommentTok{\# output \textless{}{-} xtreg2way(y,}
\CommentTok{\#                     data.frame(x1, x2),}
\CommentTok{\#                     iid,}
\CommentTok{\#                     tid,}
\CommentTok{\#                     w,}
\CommentTok{\#                     noise = "1",}
\CommentTok{\#                     se = "1")}

\CommentTok{\# equilvalently}
\NormalTok{output }\OtherTok{\textless{}{-}} \FunctionTok{xtreg2way}\NormalTok{(l\_homicide }\SpecialCharTok{\textasciitilde{}}\NormalTok{ post,}
\NormalTok{                    df,}
                    \AttributeTok{iid =}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{state, }\CommentTok{\# group id}
                    \AttributeTok{tid =}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{year, }\CommentTok{\# time id}
                    \CommentTok{\# w, \# vector of weight}
                    \AttributeTok{se =} \StringTok{"1"}\NormalTok{)}
\NormalTok{output}\SpecialCharTok{$}\NormalTok{betaHat}
\CommentTok{\#\textgreater{}                  [,1]}
\CommentTok{\#\textgreater{} l\_homicide 0.08181162}
\NormalTok{output}\SpecialCharTok{$}\NormalTok{aVarHat}
\CommentTok{\#\textgreater{}             [,1]}
\CommentTok{\#\textgreater{} [1,] 0.003396724}

\CommentTok{\# to save time, you can use your structure in the }
\CommentTok{\# last output for a new set of variables}
\CommentTok{\# output2 \textless{}{-} xtreg2way(y, x1, struc=output$struc)}
\end{Highlighting}
\end{Shaded}

Standard errors estimation options

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.1966}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.8034}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Set
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Estimation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{se\ =\ "0"} & Assume homoskedasticity and no within group correlation or serial correlation \\
\texttt{se\ =\ "1"} (default) & robust to heteroskadasticity and serial correlation \citep{arellano1987computing} \\
\texttt{se\ =\ "2"} & robust to heteroskedasticity, but assumes no correlation within group or serial correlation \\
\texttt{se\ =\ "11"} & Aerllano SE with df correction performed by Stata xtreg \citep{somaini2021twfem} \\
\end{longtable}

Alternatively, you can also do it manually or with the \texttt{plm} package, but you have to be careful with how the SEs are estimated

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(multiwayvcov) }\CommentTok{\# get vcov matrix }
\FunctionTok{library}\NormalTok{(lmtest) }\CommentTok{\# robust SEs estimation}

\CommentTok{\# manual}
\NormalTok{output3 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(l\_homicide }\SpecialCharTok{\textasciitilde{}}\NormalTok{ post }\SpecialCharTok{+} \FunctionTok{factor}\NormalTok{(state) }\SpecialCharTok{+} \FunctionTok{factor}\NormalTok{(year),}
              \AttributeTok{data =}\NormalTok{ df)}

\CommentTok{\# get variance{-}covariance matrix}
\NormalTok{vcov\_tw }\OtherTok{\textless{}{-}}\NormalTok{ multiwayvcov}\SpecialCharTok{::}\FunctionTok{cluster.vcov}\NormalTok{(output3,}
                        \FunctionTok{cbind}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{state, df}\SpecialCharTok{$}\NormalTok{year),}
                        \AttributeTok{use\_white =}\NormalTok{ F,}
                        \AttributeTok{df\_correction =}\NormalTok{ F)}

\CommentTok{\# get coefficients}
\FunctionTok{coeftest}\NormalTok{(output3, vcov\_tw)[}\DecValTok{2}\NormalTok{,] }
\CommentTok{\#\textgreater{}   Estimate Std. Error    t value   Pr(\textgreater{}|t|) }
\CommentTok{\#\textgreater{} 0.08181162 0.05671410 1.44252696 0.14979397}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# using the plm package}
\FunctionTok{library}\NormalTok{(plm)}

\NormalTok{output4 }\OtherTok{\textless{}{-}} \FunctionTok{plm}\NormalTok{(l\_homicide }\SpecialCharTok{\textasciitilde{}}\NormalTok{ post, }
               \AttributeTok{data =}\NormalTok{ df, }
               \AttributeTok{index =} \FunctionTok{c}\NormalTok{(}\StringTok{"state"}\NormalTok{, }\StringTok{"year"}\NormalTok{), }
               \AttributeTok{model =} \StringTok{"within"}\NormalTok{, }
               \AttributeTok{effect =} \StringTok{"twoways"}\NormalTok{)}

\CommentTok{\# get coefficients}
\FunctionTok{coeftest}\NormalTok{(output4, }\AttributeTok{vcov =}\NormalTok{ vcovHC, }\AttributeTok{type =} \StringTok{"HC1"}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} t test of coefficients:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}      Estimate Std. Error t value Pr(\textgreater{}|t|)}
\CommentTok{\#\textgreater{} post 0.081812   0.057748  1.4167   0.1572}
\end{Highlighting}
\end{Shaded}

As you can see, differences stem from SE estimation, not the coefficient estimate.

\hypertarget{multiple-periods-and-variation-in-treatment-timing}{%
\section{Multiple periods and variation in treatment timing}\label{multiple-periods-and-variation-in-treatment-timing}}

This is an extension of the DiD framework to settings where you have

\begin{itemize}
\item
  more than 2 time periods
\item
  different treatment timing
\end{itemize}

When treatment effects are heterogeneous across time or units, the standard \protect\hyperlink{two-way-fixed-effects}{Two-way Fixed-effects} is inappropriate.

Notation is consistent with \texttt{did} \href{https://cran.r-project.org/web/packages/did/vignettes/multi-period-did.html}{package} \citep{callaway2021difference}

\begin{itemize}
\item
  \(Y_{it}(0)\) is the potential outcome for unit \(i\)
\item
  \(Y_{it}(g)\) is the potential outcome for unit \(i\) in time period \(t\) if it's treated in period \(g\)
\item
  \(Y_{it}\) is the observed outcome for unit \(i\) in time period \(t\)
\end{itemize}

\[
Y_{it} = 
\begin{cases}
Y_{it} = Y_{it}(0) & \forall i \in \text{never-treated group} \\
Y_{it} = 1\{G_i > t\} Y_{it}(0) +  1\{G_i \le t \}Y_{it}(G_i) & \forall i \in \text{other groups}
\end{cases}
\]

\begin{itemize}
\item
  \(G_i\) is the time period when \(i\) is treated
\item
  \(C_i\) is a dummy when \(i\) belongs to the \textbf{never-treated} group
\item
  \(D_{it}\) is a dummy for whether \(i\) is treated in period \(t\)
\end{itemize}

\textbf{Assumptions}:

\begin{itemize}
\item
  Staggered treatment adoption: once treated, a unit cannot be untreated (revert)
\item
  Parallel trends assumptions (conditional on covariates):

  \begin{itemize}
  \item
    Based on never-treated units: \(E[Y_t(0)- Y_{t-1}(0)|G= g] = E[Y_t(0) - Y_{t-1}(0)|C=1]\)

    \begin{itemize}
    \tightlist
    \item
      Without treatment, the average potential outcomes for group \(g\) equals the average potential outcomes for the never-treated group (i.e., control group), which means that we have (1) enough data on the never-treated group (2) the control group is similar to the eventually treated group.
    \end{itemize}
  \item
    Based on not-yet treated units: \(E[Y_t(0) - Y_{t-1}(0)|G = g] = E[Y_t(0) - Y_{t-1}(0)|D_s = 0, G \neq g]\)

    \begin{itemize}
    \item
      Not-yet treated units by time \(s\) ( \(s \ge t\)) can be used as comparison groups to calculate the average treatment effects for the group first treated in time \(g\)
    \item
      Additional assumption: pre-treatment trends across groups \citep{marcus2021role}
    \end{itemize}
  \end{itemize}
\item
  Random sampling
\item
  Irreversibility of treatment (once treated, cannot be untreated)
\item
  Overlap (the treatment propensity \(e \in [0,1]\))
\end{itemize}

Group-Time ATE

\begin{itemize}
\tightlist
\item
  This is the equivalent of the average treatment effect in the standard case (2 groups, 2 periods) under multiple time periods.
\end{itemize}

\[
ATT(g,t) = E[Y_t(g) - Y_t(0) |G = g]
\]

which is the average treatment effect for group \(g\) in period \(t\)

\begin{itemize}
\item
  Identification: When the parallel trends assumption based on

  \begin{itemize}
  \item
    Never-treated units: \(ATT(g,t) = E[Y_t - Y_{g-1} |G = g] - E[Y_t - Y_{g-1}|C=1] \forall t \ge g\)
  \item
    Not-yet-treated units: \(ATT(g,t) = E[Y_t - Y_{g-1}|G= g] - E[Y_t - Y_{g-1}|D_t = 0, G \neq g] \forall t \ge g\)
  \end{itemize}
\item
  Identification: when the parallel trends assumption only holds conditional on covariates and based on

  \begin{itemize}
  \item
    Never-treated units: \(ATT(g,t) = E[Y_t - Y_{g-1} |X, G = g] - E[Y_t - Y_{g-1}|X, C=1] \forall t \ge g\)
  \item
    Not-yet-treated units: \(ATT(g,t) = E[Y_t - Y_{g-1}|X, G= g] - E[Y_t - Y_{g-1}|X, D_t = 0, G \neq g] \forall t \ge g\)
  \item
    This is plausible when you have suspected selection bias that can be corrected by using covariates (i.e., very much similar to matching methods to have plausible parallel trends).
  \end{itemize}
\end{itemize}

Possible parameters of interest are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Average treatment effect per group
\end{enumerate}

\[
\theta_S(g) = \frac{1}{\tau - g + 1} \sum_{t = 2}^\tau \mathbb{1} \{ \le t \} ATT(g,t)
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Average treatment effect across groups (that were treated) (similar to average treatment effect on the treated in the canonical case)
\end{enumerate}

\[
\theta_S^O := \sum_{g=2}^\tau \theta_S(g) P(G=g)
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Average treatment effect dynamics (i.e., average treatment effect for groups that have been exposed to the treatment for \(e\) time periods):
\end{enumerate}

\[
\theta_D(e) := \sum_{g=2}^\tau \mathbb{1} \{g + e \le \tau \}ATT(g,g + e) P(G = g|G + e \le \tau)
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Average treatment effect in period \(t\) for all groups that have treated by period \(t\))
\end{enumerate}

\[
\theta_C(t) = \sum_{g=2}^\tau \mathbb{1}\{g \le t\} ATT(g,t) P(G = g|g \le t)
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Average treatment effect by calendar time
\end{enumerate}

\[
\theta_C = \frac{1}{\tau-1}\sum_{t=2}^\tau \theta_C(t)
\]

\hypertarget{staggered-dif-n-dif}{%
\section{Staggered Dif-n-dif}\label{staggered-dif-n-dif}}

See \citet{wing2024designing} checklist.

Recommendations by \citet{baker2022much}

\begin{itemize}
\item
  TWFE DiD regressions are suitable for single treatment periods or when treatment effects are homogeneous, provided there's a solid rationale for effect homogeneity.
\item
  For TWFE staggered DiD, researchers should evaluate bias risks, plot treatment timings to check for variations, and use decompositions like \citet{goodman2021difference} when possible. If decompositions aren't feasible (e.g., unbalanced panel), the percentage of never-treated units can indicate bias severity. Expected treatment effect variability should also be discussed.
\item
  In TWFE staggered DiD event studies, avoid binning time periods without evidence of uniform effects. Use full relative-time indicators, justify reference periods, and be wary of multicollinearity causing bias.
\item
  To address treatment timing and bias concerns, use alternative estimators like stacked regressions, \citet{sun2021estimating}, \citet{callaway2021difference}, or separate regressions for each event with ``clean'' controls.
\item
  Justify the selection of comparison groups (not-yet treated, last treated, never treated) and ensure the parallel-trends assumption holds, especially when anticipating no effects for certain groups.
\end{itemize}

Notes:

\begin{itemize}
\tightlist
\item
  When subjects are treated at different point in time (variation in treatment timing across units), we have to use staggered DiD (also known as DiD event study or dynamic DiD).
\item
  For design where a treatment is applied and units are exposed to this treatment at all time afterward, see \citep{athey2022design}
\end{itemize}

For example, basic design \citep{stevenson2006bargaining}

\[
\begin{aligned}
Y_{it} &= \sum_k \beta_k Treatment_{it}^k + \sum_i \eta_i  State_i \\
&+ \sum_t \lambda_t Year_t + Controls_{it} + \epsilon_{it}
\end{aligned}
\]

where

\begin{itemize}
\item
  \(Treatment_{it}^k\) is a series of dummy variables equal to 1 if state \(i\) is treated \(k\) years ago in period \(t\)
\item
  SE is usually clustered at the group level (occasionally time level).
\item
  To avoid collinearity, the period right before treatment is usually chosen to drop.
\end{itemize}

The more general form of TWFE \citep{sun2021estimating}:

First, define the relative period bin indicator as

\[
D_{it}^l = \mathbf{1}(t - E_i = l)
\]

where it's an indicator function of unit \(i\) being \(l\) periods from its first treatment at time \(t\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Static} specification
\end{enumerate}

\[
Y_{it} = \alpha_i + \lambda_t + \mu_g \sum_{l \ge0} D_{it}^l + \epsilon_{it}
\]

where

\begin{itemize}
\item
  \(\alpha_i\) is the the unit FE
\item
  \(\lambda_t\) is the time FE
\item
  \(\mu_g\) is the coefficient of interest \(g = [0,T)\)
\item
  we exclude all periods before first adoption.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Dynamic} specification
\end{enumerate}

\[
Y_{it} = \alpha_i + \lambda_t + \sum_{\substack{l = -K \\ l \neq -1}}^{L} \mu_l D_{it}^l + \epsilon_{it}
\]

where we have to exclude some relative periods to avoid multicollinearity problem (e.g., either period right before treatment, or the treatment period).

In this setting, we try to show that the treatment and control groups are not statistically different (i.e., the coefficient estimates before treatment are not different from 0) to show pre-treatment parallel trends.

However, this two-way fixed effects design has been criticized by \citet{sun2021estimating}; \citet{callaway2021difference}; \citet{goodman2021difference}. When researchers include leads and lags of the treatment to see the long-term effects of the treatment, these leads and lags can be biased by effects from other periods, and pre-trends can falsely arise due to treatment effects heterogeneity.

Applying the new proposed method, finance and accounting researchers find that in many cases, the causal estimates turn out to be null \citep{baker2022much}.

\textbf{Assumptions of Staggered DID}

\begin{itemize}
\item
  \textbf{Rollout Exogeneity} (i.e., exogeneity of treatment adoption): if the treatment is randomly implemented over time (i.e., unrelated to variables that could also affect our dependent variables)

  \begin{itemize}
  \tightlist
  \item
    Evidence: Regress adoption on pre-treatment variables. And if you find evidence of correlation, include linear trends interacted with pre-treatment variables \citep{hoynes2009consumption}
  \item
    Evidence: \citep[p.~223]{deshpande2019screened}

    \begin{itemize}
    \tightlist
    \item
      Treatment is random: Regress treatment status at the unit level to all pre-treatment observables. If you have some that are predictive of treatment status, you might have to argue why it's not a worry. At best, you want this.
    \item
      Treatment timing is random: Conditional on treatment, regress timing of the treatment on pre-treatment observables. At least, you want this.
    \end{itemize}
  \end{itemize}
\item
  \textbf{No confounding events}
\item
  \textbf{Exclusion restrictions}

  \begin{itemize}
  \item
    \textbf{\emph{No-anticipation assumption}}: future treatment time do not affect current outcomes
  \item
    \textbf{\emph{Invariance-to-history assumption}}: the time a unit under treatment does not affect the outcome (i.e., the time exposed does not matter, just whether exposed or not). This presents causal effect of early or late adoption on the outcome.
  \end{itemize}
\item
  And all the assumptions in listed in the \protect\hyperlink{multiple-periods-and-variation-in-treatment-timing}{Multiple periods and variation in treatment timing}
\item
  Auxiliary assumptions:

  \begin{itemize}
  \item
    Constant treatment effects across units
  \item
    Constant treatment effect over time
  \item
    Random sampling
  \item
    Effect Additivity
  \end{itemize}
\end{itemize}

Remedies for staggered DiD \citep{baker2022much}:

\begin{itemize}
\item
  Each treated cohort is compared to appropriate controls (not-yet-treated, never-treated)

  \begin{itemize}
  \item
    \citep{goodman2021difference}
  \item
    \citep{callaway2021difference} consistent for average ATT. more complicated but also more flexible than \citep{sun2021estimating}

    \begin{itemize}
    \tightlist
    \item
      \citep{sun2021estimating} (a special case of \citep{callaway2021difference})
    \end{itemize}
  \item
    \citep{de2020two}
  \item
    \citep{borusyak2021revisiting}
  \end{itemize}
\item
  \protect\hyperlink{stacked-did}{Stacked DID} (biased but simple):

  \begin{itemize}
  \item
    \citep{gormley2011growing}
  \item
    \citep{cengiz2019effect}
  \item
    \citep{deshpande2019screened}
  \end{itemize}
\end{itemize}

\hypertarget{stacked-did}{%
\subsection{Stacked DID}\label{stacked-did}}

Notations following \href{https://scholarworks.iu.edu/dspace/bitstream/handle/2022/26875/2021-10-22_wim_wing_did_slides.pdf?sequence=1\&isAllowed=y}{these slides}

\[
Y_{it} = \beta_{FE} D_{it} + A_i + B_t + \epsilon_{it}
\]

where

\begin{itemize}
\item
  \(A_i\) is the group fixed effects
\item
  \(B_t\) is the period fixed effects
\end{itemize}

Steps

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Choose Event Window
\item
  Enumerate Sub-experiments
\item
  Define Inclusion Criteria
\item
  Stack Data
\item
  Specify Estimating Equation
\end{enumerate}

\textbf{Event Window}

Let

\begin{itemize}
\item
  \(\kappa_a\) be the length of the pre-event window
\item
  \(\kappa_b\) be the length of the post-event window
\end{itemize}

By setting a common event window for the analysis, we essentially exclude all those events that do not meet this criteria.

\textbf{Sub-experiments}

Let \(T_1\) be the earliest period in the dataset

\(T_T\) be the last period in the dataset

Then, the collection of all policy adoption periods that are under our event window is

\[
\Omega_A = \{ A_i |T_1 + \kappa_a \le A_i \le T_T - \kappa_b\}
\]

where these events exist

\begin{itemize}
\item
  at least \(\kappa_a\) periods after the earliest period
\item
  at least \(\kappa_b\) periods before the last period
\end{itemize}

Let \(d = 1, \dots, D\) be the index column of the sub-experiments in \(\Omega_A\)

and \(\omega_d\) be the event date of the d-th sub-experiment (e.g., \(\omega_1\) = adoption date of the 1st experiment)

\textbf{Inclusion Criteria}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Valid treated Units

  \begin{itemize}
  \item
    Within sub-experiment \(d\), all treated units have the same adoption date
  \item
    This makes sure a unit can only serve as a treated unit in only 1 sub-experiment
  \end{itemize}
\item
  Clean controls

  \begin{itemize}
  \item
    Only units satisfying \(A_i >\omega_d + \kappa_b\) are included as controls in sub-experiment d
  \item
    This ensures controls are only

    \begin{itemize}
    \item
      never treated units
    \item
      units that are treated in far future
    \end{itemize}
  \item
    But a unit can be control unit in multiple sub-experiments (need to correct SE)
  \end{itemize}
\item
  Valid Time Periods

  \begin{itemize}
  \item
    All observations within sub-experiment d are from time periods within the sub-experiment's event window
  \item
    This ensures in sub-experiment d, only observations satisfying \(\omega_d - \kappa_a \le t \le \omega_d + \kappa_b\) are included
  \end{itemize}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(did)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(fixest)}

\FunctionTok{data}\NormalTok{(base\_stagg)}

\CommentTok{\# first make the stacked datasets}
\CommentTok{\# get the treatment cohorts}
\NormalTok{cohorts }\OtherTok{\textless{}{-}}\NormalTok{ base\_stagg }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{select}\NormalTok{(year\_treated) }\SpecialCharTok{\%\textgreater{}\%}
    \CommentTok{\# exclude never{-}treated group}
    \FunctionTok{filter}\NormalTok{(year\_treated }\SpecialCharTok{!=} \DecValTok{10000}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{unique}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{pull}\NormalTok{()}

\CommentTok{\# make formula to create the sub{-}datasets}
\NormalTok{getdata }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(j, window) \{}
    \CommentTok{\#keep what we need}
\NormalTok{    base\_stagg }\SpecialCharTok{\%\textgreater{}\%}
        \CommentTok{\# keep treated units and all units not treated within {-}5 to 5}
        \CommentTok{\# keep treated units and all units not treated within {-}window to window}
        \FunctionTok{filter}\NormalTok{(year\_treated }\SpecialCharTok{==}\NormalTok{ j }\SpecialCharTok{|}\NormalTok{ year\_treated }\SpecialCharTok{\textgreater{}}\NormalTok{ j }\SpecialCharTok{+}\NormalTok{ window) }\SpecialCharTok{\%\textgreater{}\%}
        \CommentTok{\# keep just year {-}window to window}
        \FunctionTok{filter}\NormalTok{(year }\SpecialCharTok{\textgreater{}=}\NormalTok{ j }\SpecialCharTok{{-}}\NormalTok{ window }\SpecialCharTok{\&}\NormalTok{ year }\SpecialCharTok{\textless{}=}\NormalTok{ j }\SpecialCharTok{+}\NormalTok{ window) }\SpecialCharTok{\%\textgreater{}\%}
        \CommentTok{\# create an indicator for the dataset}
        \FunctionTok{mutate}\NormalTok{(}\AttributeTok{df =}\NormalTok{ j)}
\NormalTok{\}}

\CommentTok{\# get data stacked}
\NormalTok{stacked\_data }\OtherTok{\textless{}{-}} \FunctionTok{map\_df}\NormalTok{(cohorts, }\SpecialCharTok{\textasciitilde{}} \FunctionTok{getdata}\NormalTok{(., }\AttributeTok{window =} \DecValTok{5}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{rel\_year =} \FunctionTok{if\_else}\NormalTok{(df }\SpecialCharTok{==}\NormalTok{ year\_treated, time\_to\_treatment, }\ConstantTok{NA\_real\_}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    fastDummies}\SpecialCharTok{::}\FunctionTok{dummy\_cols}\NormalTok{(}\StringTok{"rel\_year"}\NormalTok{, }\AttributeTok{ignore\_na =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{starts\_with}\NormalTok{(}\StringTok{"rel\_year\_"}\NormalTok{), }\SpecialCharTok{\textasciitilde{}} \FunctionTok{replace\_na}\NormalTok{(., }\DecValTok{0}\NormalTok{)))}

\CommentTok{\# get stacked value}
\NormalTok{stacked }\OtherTok{\textless{}{-}}
    \FunctionTok{feols}\NormalTok{(}
\NormalTok{        y }\SpecialCharTok{\textasciitilde{}} \StringTok{\textasciigrave{}}\AttributeTok{rel\_year\_{-}5}\StringTok{\textasciigrave{}} \SpecialCharTok{+} \StringTok{\textasciigrave{}}\AttributeTok{rel\_year\_{-}4}\StringTok{\textasciigrave{}} \SpecialCharTok{+} \StringTok{\textasciigrave{}}\AttributeTok{rel\_year\_{-}3}\StringTok{\textasciigrave{}} \SpecialCharTok{+}
            \StringTok{\textasciigrave{}}\AttributeTok{rel\_year\_{-}2}\StringTok{\textasciigrave{}} \SpecialCharTok{+}\NormalTok{ rel\_year\_0 }\SpecialCharTok{+}\NormalTok{ rel\_year\_1 }\SpecialCharTok{+}\NormalTok{ rel\_year\_2 }\SpecialCharTok{+}\NormalTok{ rel\_year\_3 }\SpecialCharTok{+}
\NormalTok{            rel\_year\_4 }\SpecialCharTok{+}\NormalTok{ rel\_year\_5 }\SpecialCharTok{|}
\NormalTok{            id }\SpecialCharTok{\^{}}\NormalTok{ df }\SpecialCharTok{+}\NormalTok{ year }\SpecialCharTok{\^{}}\NormalTok{ df,}
        \AttributeTok{data =}\NormalTok{ stacked\_data}
\NormalTok{    )}\SpecialCharTok{$}\NormalTok{coefficients}

\NormalTok{stacked\_se }\OtherTok{=} \FunctionTok{feols}\NormalTok{(}
\NormalTok{    y }\SpecialCharTok{\textasciitilde{}} \StringTok{\textasciigrave{}}\AttributeTok{rel\_year\_{-}5}\StringTok{\textasciigrave{}} \SpecialCharTok{+} \StringTok{\textasciigrave{}}\AttributeTok{rel\_year\_{-}4}\StringTok{\textasciigrave{}} \SpecialCharTok{+} \StringTok{\textasciigrave{}}\AttributeTok{rel\_year\_{-}3}\StringTok{\textasciigrave{}} \SpecialCharTok{+}
        \StringTok{\textasciigrave{}}\AttributeTok{rel\_year\_{-}2}\StringTok{\textasciigrave{}} \SpecialCharTok{+}\NormalTok{ rel\_year\_0 }\SpecialCharTok{+}\NormalTok{ rel\_year\_1 }\SpecialCharTok{+}\NormalTok{ rel\_year\_2 }\SpecialCharTok{+}\NormalTok{ rel\_year\_3 }\SpecialCharTok{+}
\NormalTok{        rel\_year\_4 }\SpecialCharTok{+}\NormalTok{ rel\_year\_5 }\SpecialCharTok{|}
\NormalTok{        id }\SpecialCharTok{\^{}}\NormalTok{ df }\SpecialCharTok{+}\NormalTok{ year }\SpecialCharTok{\^{}}\NormalTok{ df,}
    \AttributeTok{data =}\NormalTok{ stacked\_data}
\NormalTok{)}\SpecialCharTok{$}\NormalTok{se}

\CommentTok{\# add in 0 for omitted {-}1}
\NormalTok{stacked }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(stacked[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{], }\DecValTok{0}\NormalTok{, stacked[}\DecValTok{5}\SpecialCharTok{:}\DecValTok{10}\NormalTok{])}
\NormalTok{stacked\_se }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(stacked\_se[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{], }\DecValTok{0}\NormalTok{, stacked\_se[}\DecValTok{5}\SpecialCharTok{:}\DecValTok{10}\NormalTok{])}


\NormalTok{cs\_out }\OtherTok{\textless{}{-}} \FunctionTok{att\_gt}\NormalTok{(}
    \AttributeTok{yname =} \StringTok{"y"}\NormalTok{,}
    \AttributeTok{data =}\NormalTok{ base\_stagg,}
    \AttributeTok{gname =} \StringTok{"year\_treated"}\NormalTok{,}
    \AttributeTok{idname =} \StringTok{"id"}\NormalTok{,}
    \CommentTok{\# xformla = "\textasciitilde{}x1",}
    \AttributeTok{tname =} \StringTok{"year"}
\NormalTok{)}
\NormalTok{cs }\OtherTok{\textless{}{-}}
    \FunctionTok{aggte}\NormalTok{(}
\NormalTok{        cs\_out,}
        \AttributeTok{type =} \StringTok{"dynamic"}\NormalTok{,}
        \AttributeTok{min\_e =} \SpecialCharTok{{-}}\DecValTok{5}\NormalTok{,}
        \AttributeTok{max\_e =} \DecValTok{5}\NormalTok{,}
        \AttributeTok{bstrap =} \ConstantTok{FALSE}\NormalTok{,}
        \AttributeTok{cband =} \ConstantTok{FALSE}
\NormalTok{    )}



\NormalTok{res\_sa20 }\OtherTok{=} \FunctionTok{feols}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}} \FunctionTok{sunab}\NormalTok{(year\_treated, year) }\SpecialCharTok{|}
\NormalTok{                     id }\SpecialCharTok{+}\NormalTok{ year, base\_stagg)}
\NormalTok{sa }\OtherTok{=} \FunctionTok{tidy}\NormalTok{(res\_sa20)[}\DecValTok{5}\SpecialCharTok{:}\DecValTok{14}\NormalTok{, ] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{pull}\NormalTok{(estimate)}
\NormalTok{sa }\OtherTok{=} \FunctionTok{c}\NormalTok{(sa[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{], }\DecValTok{0}\NormalTok{, sa[}\DecValTok{5}\SpecialCharTok{:}\DecValTok{10}\NormalTok{])}

\NormalTok{sa\_se }\OtherTok{=} \FunctionTok{tidy}\NormalTok{(res\_sa20)[}\DecValTok{6}\SpecialCharTok{:}\DecValTok{15}\NormalTok{, ] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{pull}\NormalTok{(std.error)}
\NormalTok{sa\_se }\OtherTok{=} \FunctionTok{c}\NormalTok{(sa\_se[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{], }\DecValTok{0}\NormalTok{, sa\_se[}\DecValTok{5}\SpecialCharTok{:}\DecValTok{10}\NormalTok{])}

\NormalTok{compare\_df\_est }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}
    \AttributeTok{period =} \SpecialCharTok{{-}}\DecValTok{5}\SpecialCharTok{:}\DecValTok{5}\NormalTok{,}
    \AttributeTok{cs =}\NormalTok{ cs}\SpecialCharTok{$}\NormalTok{att.egt,}
    \AttributeTok{sa =}\NormalTok{ sa,}
    \AttributeTok{stacked =}\NormalTok{ stacked}
\NormalTok{)}

\NormalTok{compare\_df\_se }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}
    \AttributeTok{period =} \SpecialCharTok{{-}}\DecValTok{5}\SpecialCharTok{:}\DecValTok{5}\NormalTok{,}
    \AttributeTok{cs =}\NormalTok{ cs}\SpecialCharTok{$}\NormalTok{se.egt,}
    \AttributeTok{sa =}\NormalTok{ sa\_se,}
    \AttributeTok{stacked =}\NormalTok{ stacked\_se}
\NormalTok{)}

\NormalTok{compare\_df\_longer }\OtherTok{\textless{}{-}}\NormalTok{ compare\_df\_est }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{pivot\_longer}\NormalTok{(}\SpecialCharTok{!}\NormalTok{period, }\AttributeTok{names\_to =} \StringTok{"estimator"}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{"est"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    
    \FunctionTok{full\_join}\NormalTok{(compare\_df\_se }\SpecialCharTok{\%\textgreater{}\%} 
                  \FunctionTok{pivot\_longer}\NormalTok{(}\SpecialCharTok{!}\NormalTok{period, }\AttributeTok{names\_to =} \StringTok{"estimator"}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{"se"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
    
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{upper =}\NormalTok{ est }\SpecialCharTok{+}  \FloatTok{1.96} \SpecialCharTok{*}\NormalTok{ se,}
           \AttributeTok{lower =}\NormalTok{ est }\SpecialCharTok{{-}} \FloatTok{1.96} \SpecialCharTok{*}\NormalTok{ se)}


\FunctionTok{ggplot}\NormalTok{(compare\_df\_longer) }\SpecialCharTok{+}
    \FunctionTok{geom\_ribbon}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}
        \AttributeTok{x =}\NormalTok{ period,}
        \AttributeTok{ymin =}\NormalTok{ lower,}
        \AttributeTok{ymax =}\NormalTok{ upper,}
        \AttributeTok{group =}\NormalTok{ estimator}
\NormalTok{    )) }\SpecialCharTok{+}
    \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}
        \AttributeTok{x =}\NormalTok{ period,}
        \AttributeTok{y =}\NormalTok{ est,}
        \AttributeTok{group =}\NormalTok{ estimator,}
        \AttributeTok{col =}\NormalTok{ estimator}
\NormalTok{    ),}
    \AttributeTok{linewidth =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+} 
\NormalTok{    causalverse}\SpecialCharTok{::}\FunctionTok{ama\_theme}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-10-1} \end{center}

\textbf{Stack Data}

Estimating Equation

\[
Y_{itd} = \beta_0 + \beta_1 T_{id} + \beta_2 P_{td} + \beta_3 (T_{id} \times P_{td}) + \epsilon_{itd}
\]

where

\begin{itemize}
\item
  \(T_{id}\) = 1 if unit \(i\) is treated in sub-experiment \(d\), 0 if control
\item
  \(P_{td}\) = 1 if it's the period after the treatment in sub-experiment \(d\)
\end{itemize}

Equivalently,

\[
Y_{itd} = \beta_3 (T_{id} \times P_{td}) + \theta_{id} + \gamma_{td} + \epsilon_{itd}
\]

\(\beta_3\) averages all the time-varying effects into a single number (can't see the time-varying effects)

\textbf{Stacked Event Study}

Let \(YSE_{td} = t - \omega_d\) be the ``time since event'' variable in sub-experiment \(d\)

Then, \(YSE_{td} = -\kappa_a, \dots, 0, \dots, \kappa_b\) in every sub-experiment

In each sub-experiment, we can fit

\[
Y_{it}^d = \sum_{j = -\kappa_a}^{\kappa_b} \beta_j^d \times 1(TSE_{td} = j) + \sum_{m = -\kappa_a}^{\kappa_b} \delta_j^d (T_{id} \times 1 (TSE_{td} = j)) + \theta_i^d + \epsilon_{it}^d
\]

\begin{itemize}
\tightlist
\item
  Different set of event study coefficients in each sub-experiment
\end{itemize}

\[
Y_{itd} = \sum_{j = -\kappa_a}^{\kappa_b} \beta_j \times 1(TSE_{td} = j) + \sum_{m = -\kappa_a}^{\kappa_b} \delta_j (T_{id} \times 1 (TSE_{td} = j)) + \theta_{id} + \epsilon_{itd}
\]

\textbf{Clustering}

\begin{itemize}
\item
  Clustered at the unit x sub-experiment level \citep{cengiz2019effect}
\item
  Clustered at the unit level \citep{deshpande2019screened}
\end{itemize}

\hypertarget{goodman-bacon-decomposition}{%
\subsection{Goodman-Bacon Decomposition}\label{goodman-bacon-decomposition}}

Paper: \citep{goodman2021difference}

For an excellent explanation slides by the author, \href{https://www.stata.com/meeting/chicago19/slides/chicago19_Goodman-Bacon.pdf}{see}

Takeaways:

\begin{itemize}
\item
  A pairwise DID (\(\tau\)) gets more weight if the change is close to the middle of the study window
\item
  A pairwise DID (\(\tau\)) gets more weight if it includes more observations.
\end{itemize}

Code from \texttt{bacondecomp} vignette

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(bacondecomp)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{data}\NormalTok{(}\StringTok{"castle"}\NormalTok{)}
\NormalTok{castle }\OtherTok{\textless{}{-}}\NormalTok{ bacondecomp}\SpecialCharTok{::}\NormalTok{castle }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\StringTok{"l\_homicide"}\NormalTok{, }\StringTok{"post"}\NormalTok{, }\StringTok{"state"}\NormalTok{, }\StringTok{"year"}\NormalTok{)}
\FunctionTok{head}\NormalTok{(castle)}
\CommentTok{\#\textgreater{}   l\_homicide post   state year}
\CommentTok{\#\textgreater{} 1   2.027356    0 Alabama 2000}
\CommentTok{\#\textgreater{} 2   2.164867    0 Alabama 2001}
\CommentTok{\#\textgreater{} 3   1.936334    0 Alabama 2002}
\CommentTok{\#\textgreater{} 4   1.919567    0 Alabama 2003}
\CommentTok{\#\textgreater{} 5   1.749841    0 Alabama 2004}
\CommentTok{\#\textgreater{} 6   2.130440    0 Alabama 2005}


\NormalTok{df\_bacon }\OtherTok{\textless{}{-}} \FunctionTok{bacon}\NormalTok{(}
\NormalTok{    l\_homicide }\SpecialCharTok{\textasciitilde{}}\NormalTok{ post,}
    \AttributeTok{data =}\NormalTok{ castle,}
    \AttributeTok{id\_var =} \StringTok{"state"}\NormalTok{,}
    \AttributeTok{time\_var =} \StringTok{"year"}
\NormalTok{)}
\CommentTok{\#\textgreater{}                       type  weight  avg\_est}
\CommentTok{\#\textgreater{} 1 Earlier vs Later Treated 0.05976 {-}0.00554}
\CommentTok{\#\textgreater{} 2 Later vs Earlier Treated 0.03190  0.07032}
\CommentTok{\#\textgreater{} 3     Treated vs Untreated 0.90834  0.08796}

\CommentTok{\# weighted average of the decomposition}
\FunctionTok{sum}\NormalTok{(df\_bacon}\SpecialCharTok{$}\NormalTok{estimate }\SpecialCharTok{*}\NormalTok{ df\_bacon}\SpecialCharTok{$}\NormalTok{weight)}
\CommentTok{\#\textgreater{} [1] 0.08181162}
\end{Highlighting}
\end{Shaded}

Two-way Fixed effect estimate

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(broom)}
\NormalTok{fit\_tw }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(l\_homicide }\SpecialCharTok{\textasciitilde{}}\NormalTok{ post }\SpecialCharTok{+} \FunctionTok{factor}\NormalTok{(state) }\SpecialCharTok{+} \FunctionTok{factor}\NormalTok{(year),}
             \AttributeTok{data =}\NormalTok{ bacondecomp}\SpecialCharTok{::}\NormalTok{castle)}
\FunctionTok{head}\NormalTok{(}\FunctionTok{tidy}\NormalTok{(fit\_tw))}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 5}
\CommentTok{\#\textgreater{}   term                    estimate std.error statistic   p.value}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}                      \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 (Intercept)               1.95      0.0624    31.2   2.84e{-}118}
\CommentTok{\#\textgreater{} 2 post                      0.0818    0.0317     2.58  1.02e{-}  2}
\CommentTok{\#\textgreater{} 3 factor(state)Alaska      {-}0.373     0.0797    {-}4.68  3.77e{-}  6}
\CommentTok{\#\textgreater{} 4 factor(state)Arizona      0.0158    0.0797     0.198 8.43e{-}  1}
\CommentTok{\#\textgreater{} 5 factor(state)Arkansas    {-}0.118     0.0810    {-}1.46  1.44e{-}  1}
\CommentTok{\#\textgreater{} 6 factor(state)California  {-}0.108     0.0810    {-}1.34  1.82e{-}  1}
\end{Highlighting}
\end{Shaded}

Hence, naive TWFE fixed effect equals the weighted average of the Bacon decomposition (= 0.08).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}

\FunctionTok{ggplot}\NormalTok{(df\_bacon) }\SpecialCharTok{+}
    \FunctionTok{aes}\NormalTok{(}
        \AttributeTok{x =}\NormalTok{ weight,}
        \AttributeTok{y =}\NormalTok{ estimate,}
        \CommentTok{\# shape = factor(type),}
        \AttributeTok{color =}\NormalTok{ type}
\NormalTok{    ) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Weight"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Estimate"}\NormalTok{, }\AttributeTok{shape =} \StringTok{"Type"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
\NormalTok{    causalverse}\SpecialCharTok{::}\FunctionTok{ama\_theme}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-13-1} \end{center}

With time-varying controls that can identify variation within-treatment timing group, the''early vs.~late'' and ``late vs.~early'' estimates collapse to just one estimate (i.e., both treated).

\hypertarget{did-with-in-and-out-treatment-condition}{%
\subsection{DID with in and out treatment condition}\label{did-with-in-and-out-treatment-condition}}

\hypertarget{panel-match}{%
\subsubsection{Panel Match}\label{panel-match}}

\citet{imai2021use}

This case generalizes the staggered adoption setting, allowing units to vary in treatment over time. For \(N\) units across \(T\) time periods (with potentially unbalanced panels), let \(X_{it}\) represent treatment and \(Y_{it}\) the outcome for unit \(i\) at time \(t\). We use the two-way linear fixed effects model:

\[
Y_{it} = \alpha_i + \gamma_t + \beta X_{it} + \epsilon_{it}
\]

for \(i = 1, \dots, N\) and \(t = 1, \dots, T\). Here, \(\alpha_i\) and \(\gamma_t\) are unit and time fixed effects. They capture time-invariant unit-specific and unit-invariant time-specific unobserved confounders, respectively. We can express these as \(\alpha_i = h(\mathbf{U}_i)\) and \(\gamma_t = f(\mathbf{V}_t)\), with \(\mathbf{U}_i\) and \(\mathbf{V}_t\) being the confounders. The model doesn't assume a specific form for \(h(.)\) and \(f(.)\), but that they're additive and separable given binary treatment.

The least squares estimate of \(\beta\) leverages the covariance in outcome and treatment \citep[p.~406]{imai2021use}. Specifically, it uses the within-unit and within-time variations. Many researchers prefer the two fixed effects (2FE) estimator because it adjusts for both types of unobserved confounders without specific functional-form assumptions, but this is wrong \citep{imai2019should}. We do need functional-form assumption (i.e., linearity assumption) for the 2FE to work \citep[p.~406]{imai2021use}

\begin{itemize}
\item
  \textbf{Two-Way Matching Estimator}:

  \begin{itemize}
  \item
    It can lead to mismatches; units with the same treatment status get matched when estimating counterfactual outcomes.
  \item
    Observations need to be matched with opposite treatment status for correct causal effects estimation.
  \item
    Mismatches can cause attenuation bias.
  \item
    The 2FE estimator adjusts for this bias using the factor \(K\), which represents the net proportion of proper matches between observations with opposite treatment status.
  \end{itemize}
\item
  \textbf{Weighting in 2FE}:

  \begin{itemize}
  \item
    Observation \((i,t)\) is weighted based on how often it acts as a control unit.
  \item
    The weighted 2FE estimator still has mismatches, but fewer than the standard 2FE estimator.
  \item
    Adjustments are made based on observations that neither belong to the same unit nor the same time period as the matched observation.
  \item
    This means there are challenges in adjusting for unit-specific and time-specific unobserved confounders under the two-way fixed effect framework.
  \end{itemize}
\item
  \textbf{Equivalence \& Assumptions}:

  \begin{itemize}
  \item
    Equivalence between the 2FE estimator and the DID estimator is dependent on the linearity assumption.
  \item
    The multi-period DiD estimator is described as an average of two-time-period, two-group DiD estimators applied during changes from control to treatment.
  \end{itemize}
\item
  \textbf{Comparison with DiD}:

  \begin{itemize}
  \item
    In simple settings (two time periods, treatment given to one group in the second period), the standard nonparametric DiD estimator equals the 2FE estimator.
  \item
    This doesn't hold in multi-period DiD designs where units change treatment status multiple times at different intervals.
  \item
    Contrary to popular belief, the unweighted 2FE estimator isn't generally equivalent to the multi-period DiD estimator.
  \item
    While the multi-period DiD can be equivalent to the weighted 2FE, some control observations may have negative regression weights.
  \end{itemize}
\item
  \textbf{Conclusion}:

  \begin{itemize}
  \tightlist
  \item
    Justifying the 2FE estimator as the DID estimator isn't warranted without imposing the linearity assumption.
  \end{itemize}
\end{itemize}

\textbf{Application \citep{imai2021matching}}

\begin{itemize}
\item
  \textbf{Matching Methods}:

  \begin{itemize}
  \item
    Enhance the validity of causal inference.
  \item
    Reduce model dependence and provide intuitive diagnostics \citep{ho2007matching}
  \item
    Rarely utilized in analyzing time series cross-sectional data.
  \item
    The proposed matching estimators are more robust than the standard two-way fixed effects estimator, which can be biased if mis-specified
  \item
    Better than synthetic controls (e.g., \citep{xu2017generalized}) because it needs less data to achieve good performance and and adapt the the context of unit switching treatment status multiple times.
  \end{itemize}
\item
  Notes:

  \begin{itemize}
  \tightlist
  \item
    Potential carryover effects (treatment may have a long-term effect), leading to post-treatment bias.
  \end{itemize}
\item
  \textbf{Proposed Approach}:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \item
    Treated observations are matched with control observations from other units in the same time period with the same treatment history up to a specified number of lags.
  \item
    Standard matching and weighting techniques are employed to further refine the matched set.
  \item
    Apply a DiD estimator to adjust for time trend.
  \item
    The goal is to have treated and matched control observations with similar covariate values.
  \end{enumerate}
\item
  \textbf{Assessment}:

  \begin{itemize}
  \tightlist
  \item
    The quality of matches is evaluated through covariate balancing.
  \end{itemize}
\item
  \textbf{Estimation}:

  \begin{itemize}
  \tightlist
  \item
    Both short-term and long-term average treatment effects on the treated (ATT) are estimated.
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(PanelMatch)}
\end{Highlighting}
\end{Shaded}

\textbf{Treatment Variation plot}

\begin{itemize}
\item
  Visualize the variation of the treatment across space and time
\item
  Aids in discerning whether the treatment fluctuates adequately over time and units or if the variation is primarily clustered in a subset of data.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{DisplayTreatment}\NormalTok{(}
    \AttributeTok{unit.id =} \StringTok{"wbcode2"}\NormalTok{,}
    \AttributeTok{time.id =} \StringTok{"year"}\NormalTok{,}
    \AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"year"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"Country Code"}\NormalTok{,}
    \AttributeTok{treatment =} \StringTok{"dem"}\NormalTok{,}
    
    \AttributeTok{hide.x.tick.label =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{hide.y.tick.label =} \ConstantTok{TRUE}\NormalTok{, }
    \CommentTok{\# dense.plot = TRUE,}
    \AttributeTok{data =}\NormalTok{ dem}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-15-1} \end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Select \(F\) (i.e., the number of leads - time periods after treatment). Driven by what authors are interested in estimating:
\end{enumerate}

\begin{itemize}
\item
  \(F = 0\) is the contemporaneous effect (short-term effect)
\item
  \(F = n\) is the the treatment effect on the outcome two time periods after the treatment. (cumulative or long-term effect)
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Select \(L\) (number of lags to adjust).
\end{enumerate}

\begin{itemize}
\item
  Driven by the identification assumption.
\item
  Balances bias-variance tradeoff.
\item
  Higher \(L\) values increase credibility but reduce efficiency by limiting potential matches.
\end{itemize}

\textbf{Model assumption}:

\begin{itemize}
\item
  No spillover effect assumed.
\item
  Carryover effect allowed up to \(L\) periods.
\item
  Potential outcome for a unit depends neither on others' treatment status nor on its past treatment after \(L\) periods.
\end{itemize}

After defining causal quantity with parameters \(L\) and \(F\).

\begin{itemize}
\tightlist
\item
  Focus on the average treatment effect of treatment status change.
\item
  \(\delta(F,L)\) is the average causal effect of treatment change (ATT), \(F\) periods post-treatment, considering treatment history up to \(L\) periods.
\item
  Causal quantity considers potential future treatment reversals, meaning treatment could revert to control before outcome measurement.
\end{itemize}

Also possible to estimate the average treatment effect of treatment reversal on the reversed (ART).

Choose \(L,F\) based on specific needs.

\begin{itemize}
\item
  A large \(L\) value:

  \begin{itemize}
  \item
    Increases the credibility of the limited carryover effect assumption.
  \item
    Allows more past treatments (up to \(tL\)) to influence the outcome \(Y_{i,t+F}\).
  \item
    Might reduce the number of matches and lead to less precise estimates.
  \end{itemize}
\item
  Selecting an appropriate number of lags

  \begin{itemize}
  \item
    Researchers should base this choice on substantive knowledge.
  \item
    Sensitivity of empirical results to this choice should be examined.
  \end{itemize}
\item
  The choice of \(F\) should be:

  \begin{itemize}
  \item
    Substantively motivated.
  \item
    Decides whether the interest lies in short-term or long-term causal effects.
  \item
    A large \(F\) value can complicate causal effect interpretation, especially if many units switch treatment status during the \(F\) lead time period.
  \end{itemize}
\end{itemize}

\textbf{Identification Assumption}

\begin{itemize}
\item
  Parallel trend assumption conditioned on treatment, outcome (excluding immediate lag), and covariate histories.
\item
  Doesn't require strong unconfoundedness assumption.
\item
  Cannot account for unobserved time-varying confounders.
\item
  Essential to examine outcome time trends.

  \begin{itemize}
  \tightlist
  \item
    Check if they're parallel between treated and matched control units using pre-treatment data
  \end{itemize}
\item
  \textbf{Constructing the Matched Sets}:

  \begin{itemize}
  \item
    For each treated observation, create matched control units with identical treatment history from \(tL\) to \(t1\).
  \item
    Matching based on treatment history helps control for carryover effects.
  \item
    Past treatments often act as major confounders, but this method can correct for it.
  \item
    Exact matching on time period adjusts for time-specific unobserved confounders.
  \item
    Unlike staggered adoption methods, units can change treatment status multiple times.
  \item
    Matched set allows treatment switching in and out of treatment
  \end{itemize}
\item
  \textbf{Refining the Matched Sets}:

  \begin{itemize}
  \item
    Initially, matched sets adjust only for treatment history.
  \item
    Parallel trend assumption requires adjustments for other confounders like past outcomes and covariates.
  \item
    Matching methods:

    \begin{itemize}
    \item
      Match each treated observation with up to \(J\) control units.
    \item
      Distance measures like Mahalanobis distance or propensity score can be used.
    \item
      Match based on estimated propensity score, considering pretreatment covariates.
    \item
      Refined matched set selects most similar control units based on observed confounders.
    \end{itemize}
  \item
    Weighting methods:

    \begin{itemize}
    \item
      Assign weight to each control unit in a matched set.
    \item
      Weights prioritize more similar units.
    \item
      Inverse propensity score weighting method can be applied.
    \item
      Weighting is a more generalized method than matching.
    \end{itemize}
  \end{itemize}
\end{itemize}

\textbf{The Difference-in-Differences Estimator}:

\begin{itemize}
\item
  Using refined matched sets, the ATT (Average Treatment Effect on the Treated) of policy change is estimated.
\item
  For each treated observation, estimate the counterfactual outcome using the weighted average of control units in the refined set.
\item
  The DiD estimate of the ATT is computed for each treated observation, then averaged across all such observations.
\item
  For noncontemporaneous treatment effects where \(F > 0\):

  \begin{itemize}
  \item
    The ATT doesn't specify future treatment sequence.
  \item
    Matched control units might have units receiving treatment between time \(t\) and \(t + F\).
  \item
    Some treated units could return to control conditions between these times.
  \end{itemize}
\end{itemize}

\textbf{Checking Covariate Balance}:

\begin{itemize}
\item
  The proposed methodology offers the advantage of checking covariate balance between treated and matched control observations.
\item
  This check helps to see if treated and matched control observations are comparable with respect to observed confounders.
\item
  Once matched sets are refined, covariate balance examination becomes straightforward.
\item
  Examine the mean difference of each covariate between a treated observation and its matched controls for each pretreatment time period.
\item
  Standardize this difference using the standard deviation of each covariate across all treated observations in the dataset.
\item
  Aggregate this covariate balance measure across all treated observations for each covariate and pretreatment time period.
\item
  Examine balance for lagged outcome variables over multiple pretreatment periods and time-varying covariates.

  \begin{itemize}
  \tightlist
  \item
    This helps evaluate the validity of the parallel trend assumption underlying the proposed DiD estimator.
  \end{itemize}
\end{itemize}

\textbf{Relations with Linear Fixed Effects Regression Estimators}:

\begin{itemize}
\item
  The standard DiD estimator is equivalent to the linear two-way fixed effects regression estimator when:

  \begin{itemize}
  \item
    Only two time periods exist.
  \item
    Treatment is given to some units exclusively in the second period.
  \end{itemize}
\item
  This equivalence doesn't extend to multiperiod DiD designs, where:

  \begin{itemize}
  \item
    More than two time periods are considered.
  \item
    Units might receive treatment multiple times.
  \end{itemize}
\item
  Despite this, many researchers relate the use of the two-way fixed effects estimator to the DiD design.
\end{itemize}

\textbf{Standard Error Calculation}:

\begin{itemize}
\item
  Approach:

  \begin{itemize}
  \item
    Condition on the weights implied by the matching process.
  \item
    These weights denote how often an observation is utilized in matching \citep{imbens2015causal}
  \end{itemize}
\item
  Context:

  \begin{itemize}
  \item
    Analogous to the conditional variance seen in regression models.
  \item
    Resulting standard errors don't factor in uncertainties around the matching procedure.
  \item
    They can be viewed as a measure of uncertainty conditional upon the matching process \citep{ho2007matching}.
  \end{itemize}
\end{itemize}

\textbf{Key Findings}:

\begin{itemize}
\item
  Even in conditions favoring OLS, the proposed matching estimator displayed higher robustness to omitted relevant lags than the linear regression model with fixed effects.
\item
  The robustness offered by matching came at a cost - reduced statistical power.
\item
  This emphasizes the classic statistical tradeoff between bias (where matching has an advantage) and variance (where regression models might be more efficient).
\end{itemize}

\textbf{Data Requirements}

\begin{itemize}
\item
  The treatment variable is binary:

  \begin{itemize}
  \item
    0 signifies ``assignment'' to control.
  \item
    1 signifies assignment to treatment.
  \end{itemize}
\item
  Variables identifying units in the data must be: Numeric or integer.
\item
  Variables identifying time periods should be: Consecutive numeric/integer data.
\item
  Data format requirement: Must be provided as a standard \texttt{data.frame} object.
\end{itemize}

Basic functions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Utilize treatment histories to create matching sets of treated and control units.
\item
  Refine these matched sets by determining weights for each control unit in the set.

  \begin{itemize}
  \tightlist
  \item
    Units with higher weights have a larger influence during estimations.
  \end{itemize}
\end{enumerate}

\textbf{Matching on Treatment History}:

\begin{itemize}
\item
  Goal is to match units transitioning from untreated to treated status with control units that have similar past treatment histories.
\item
  Setting the Quantity of Interest (\texttt{qoi\ =})

  \begin{itemize}
  \item
    \texttt{att} average treatment effect on treated units
  \item
    \texttt{atc} average treatment effect of treatment on the control units
  \item
    \texttt{art} average effect of treatment reversal for units that experience treatment reversal
  \item
    \texttt{ate} average treatment effect
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(PanelMatch)}
\CommentTok{\# All examples follow the package\textquotesingle{}s vignette}
\CommentTok{\# Create the matched sets}
\NormalTok{PM.results.none }\OtherTok{\textless{}{-}}
    \FunctionTok{PanelMatch}\NormalTok{(}
        \AttributeTok{lag =} \DecValTok{4}\NormalTok{,}
        \AttributeTok{time.id =} \StringTok{"year"}\NormalTok{,}
        \AttributeTok{unit.id =} \StringTok{"wbcode2"}\NormalTok{,}
        \AttributeTok{treatment =} \StringTok{"dem"}\NormalTok{,}
        \AttributeTok{refinement.method =} \StringTok{"none"}\NormalTok{,}
        \AttributeTok{data =}\NormalTok{ dem,}
        \AttributeTok{match.missing =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{size.match =} \DecValTok{5}\NormalTok{,}
        \AttributeTok{qoi =} \StringTok{"att"}\NormalTok{,}
        \AttributeTok{outcome.var =} \StringTok{"y"}\NormalTok{,}
        \AttributeTok{lead =} \DecValTok{0}\SpecialCharTok{:}\DecValTok{4}\NormalTok{,}
        \AttributeTok{forbid.treatment.reversal =} \ConstantTok{FALSE}\NormalTok{,}
        \AttributeTok{use.diagonal.variance.matrix =} \ConstantTok{TRUE}
\NormalTok{    )}

\CommentTok{\# visualize the treated unit and matched controls}
\FunctionTok{DisplayTreatment}\NormalTok{(}
    \AttributeTok{unit.id =} \StringTok{"wbcode2"}\NormalTok{,}
    \AttributeTok{time.id =} \StringTok{"year"}\NormalTok{,}
    \AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"year"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"Country Code"}\NormalTok{,}
    \AttributeTok{treatment =} \StringTok{"dem"}\NormalTok{,}
    \AttributeTok{data =}\NormalTok{ dem,}
    \AttributeTok{matched.set =}\NormalTok{ PM.results.none}\SpecialCharTok{$}\NormalTok{att[}\DecValTok{1}\NormalTok{],}
    \CommentTok{\# highlight the particular set}
    \AttributeTok{show.set.only =} \ConstantTok{TRUE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-16-1} \end{center}

Control units and the treated unit have identical treatment histories over the lag window (1988-1991)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{DisplayTreatment}\NormalTok{(}
    \AttributeTok{unit.id =} \StringTok{"wbcode2"}\NormalTok{,}
    \AttributeTok{time.id =} \StringTok{"year"}\NormalTok{,}
    \AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"year"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"Country Code"}\NormalTok{,}
    \AttributeTok{treatment =} \StringTok{"dem"}\NormalTok{,}
    \AttributeTok{data =}\NormalTok{ dem,}
    \AttributeTok{matched.set =}\NormalTok{ PM.results.none}\SpecialCharTok{$}\NormalTok{att[}\DecValTok{2}\NormalTok{],}
    \CommentTok{\# highlight the particular set}
    \AttributeTok{show.set.only =} \ConstantTok{TRUE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-17-1} \end{center}

This set is more limited than the first one, but we can still see that we have exact past histories.

\begin{itemize}
\item
  \textbf{Refining Matched Sets}

  \begin{itemize}
  \item
    Refinement involves assigning weights to control units.
  \item
    Users must:

    \begin{enumerate}
    \def\labelenumi{\arabic{enumi}.}
    \item
      Specify a method for calculating unit similarity/distance.
    \item
      Choose variables for similarity/distance calculations.
    \end{enumerate}
  \end{itemize}
\item
  \textbf{Select a Refinement Method}

  \begin{itemize}
  \item
    Users determine the refinement method via the \textbf{\texttt{refinement.method}} argument.
  \item
    Options include:

    \begin{itemize}
    \item
      \texttt{mahalanobis}
    \item
      \texttt{ps.match}
    \item
      \texttt{CBPS.match}
    \item
      \texttt{ps.weight}
    \item
      \texttt{CBPS.weight}
    \item
      \texttt{ps.msm.weight}
    \item
      \texttt{CBPS.msm.weight}
    \item
      \texttt{none}
    \end{itemize}
  \item
    Methods with ``match'' in the name and Mahalanobis will assign equal weights to similar control units.
  \item
    ``Weighting'' methods give higher weights to control units more similar to treated units.
  \end{itemize}
\item
  \textbf{Variable Selection}

  \begin{itemize}
  \item
    Users need to define which covariates will be used through the \textbf{\texttt{covs.formula}} argument, a one-sided formula object.
  \item
    Variables on the right side of the formula are used for calculations.
  \item
    ``Lagged'' versions of variables can be included using the format: \textbf{\texttt{I(lag(name.of.var,\ 0:n))}}.
  \end{itemize}
\item
  \textbf{Understanding \texttt{PanelMatch} and \texttt{matched.set} objects}

  \begin{itemize}
  \item
    The \textbf{\texttt{PanelMatch} function} returns a \textbf{\texttt{PanelMatch} object}.
  \item
    The most crucial element within the \texttt{PanelMatch} object is the \textbf{matched.set object}.
  \item
    Within the \texttt{PanelMatch} object, the matched.set object will have names like att, art, or atc.
  \item
    If \textbf{\texttt{qoi\ =\ ate}}, there will be two matched.set objects: att and atc.
  \end{itemize}
\item
  \textbf{Matched.set Object Details}

  \begin{itemize}
  \item
    matched.set is a named list with added attributes.
  \item
    Attributes include:

    \begin{itemize}
    \item
      Lag
    \item
      Names of treatment
    \item
      Unit and time variables
    \end{itemize}
  \item
    Each list entry represents a matched set of treated and control units.
  \item
    Naming follows a structure: \textbf{\texttt{{[}id\ variable{]}.{[}time\ variable{]}}}.
  \item
    Each list element is a vector of control unit ids that match the treated unit mentioned in the element name.
  \item
    Since it's a matching method, weights are only given to the \textbf{\texttt{size.match}} most similar control units based on distance calculations.
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# PanelMatch without any refinement}
\NormalTok{PM.results.none }\OtherTok{\textless{}{-}}
    \FunctionTok{PanelMatch}\NormalTok{(}
        \AttributeTok{lag =} \DecValTok{4}\NormalTok{,}
        \AttributeTok{time.id =} \StringTok{"year"}\NormalTok{,}
        \AttributeTok{unit.id =} \StringTok{"wbcode2"}\NormalTok{,}
        \AttributeTok{treatment =} \StringTok{"dem"}\NormalTok{,}
        \AttributeTok{refinement.method =} \StringTok{"none"}\NormalTok{,}
        \AttributeTok{data =}\NormalTok{ dem,}
        \AttributeTok{match.missing =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{size.match =} \DecValTok{5}\NormalTok{,}
        \AttributeTok{qoi =} \StringTok{"att"}\NormalTok{,}
        \AttributeTok{outcome.var =} \StringTok{"y"}\NormalTok{,}
        \AttributeTok{lead =} \DecValTok{0}\SpecialCharTok{:}\DecValTok{4}\NormalTok{,}
        \AttributeTok{forbid.treatment.reversal =} \ConstantTok{FALSE}\NormalTok{,}
        \AttributeTok{use.diagonal.variance.matrix =} \ConstantTok{TRUE}
\NormalTok{    )}

\CommentTok{\# Extract the matched.set object}
\NormalTok{msets.none }\OtherTok{\textless{}{-}}\NormalTok{ PM.results.none}\SpecialCharTok{$}\NormalTok{att}

\CommentTok{\# PanelMatch with refinement}
\NormalTok{PM.results.maha }\OtherTok{\textless{}{-}}
    \FunctionTok{PanelMatch}\NormalTok{(}
        \AttributeTok{lag =} \DecValTok{4}\NormalTok{,}
        \AttributeTok{time.id =} \StringTok{"year"}\NormalTok{,}
        \AttributeTok{unit.id =} \StringTok{"wbcode2"}\NormalTok{,}
        \AttributeTok{treatment =} \StringTok{"dem"}\NormalTok{,}
        \AttributeTok{refinement.method =} \StringTok{"mahalanobis"}\NormalTok{, }\CommentTok{\# use Mahalanobis distance}
        \AttributeTok{data =}\NormalTok{ dem,}
        \AttributeTok{match.missing =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{covs.formula =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ tradewb,}
        \AttributeTok{size.match =} \DecValTok{5}\NormalTok{,}
        \AttributeTok{qoi =} \StringTok{"att"}\NormalTok{ ,}
        \AttributeTok{outcome.var =} \StringTok{"y"}\NormalTok{,}
        \AttributeTok{lead =} \DecValTok{0}\SpecialCharTok{:}\DecValTok{4}\NormalTok{,}
        \AttributeTok{forbid.treatment.reversal =} \ConstantTok{FALSE}\NormalTok{,}
        \AttributeTok{use.diagonal.variance.matrix =} \ConstantTok{TRUE}
\NormalTok{    )}
\NormalTok{msets.maha }\OtherTok{\textless{}{-}}\NormalTok{ PM.results.maha}\SpecialCharTok{$}\NormalTok{att}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# these 2 should be identical because weights are not shown}
\NormalTok{msets.none }\SpecialCharTok{|\textgreater{}} \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{}   wbcode2 year matched.set.size}
\CommentTok{\#\textgreater{} 1       4 1992               74}
\CommentTok{\#\textgreater{} 2       4 1997                2}
\CommentTok{\#\textgreater{} 3       6 1973               63}
\CommentTok{\#\textgreater{} 4       6 1983               73}
\CommentTok{\#\textgreater{} 5       7 1991               81}
\CommentTok{\#\textgreater{} 6       7 1998                1}
\NormalTok{msets.maha }\SpecialCharTok{|\textgreater{}} \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{}   wbcode2 year matched.set.size}
\CommentTok{\#\textgreater{} 1       4 1992               74}
\CommentTok{\#\textgreater{} 2       4 1997                2}
\CommentTok{\#\textgreater{} 3       6 1973               63}
\CommentTok{\#\textgreater{} 4       6 1983               73}
\CommentTok{\#\textgreater{} 5       7 1991               81}
\CommentTok{\#\textgreater{} 6       7 1998                1}
\CommentTok{\# summary(msets.none)}
\CommentTok{\# summary(msets.maha)}
\end{Highlighting}
\end{Shaded}

\textbf{Visualizing Matched Sets with the plot method}

\begin{itemize}
\item
  Users can visualize the distribution of the matched set sizes.
\item
  A red line, by default, indicates the count of matched sets where treated units had no matching control units (i.e., empty matched sets).
\item
  Plot adjustments can be made using \textbf{\texttt{graphics::plot}}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(msets.none)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-20-1} \end{center}

\textbf{Comparing Methods of Refinement}

\begin{itemize}
\item
  Users are encouraged to:

  \begin{itemize}
  \item
    Use substantive knowledge for experimentation and evaluation.
  \item
    Consider the following when configuring \texttt{PanelMatch}:

    \begin{enumerate}
    \def\labelenumi{\arabic{enumi}.}
    \item
      The number of matched sets.
    \item
      The number of controls matched to each treated unit.
    \item
      Achieving covariate balance.
    \end{enumerate}
  \item
    \textbf{Note}: Large numbers of small matched sets can lead to larger standard errors during the estimation stage.
  \item
    Covariates that aren't well balanced can lead to undesirable comparisons between treated and control units.
  \item
    Aspects to consider include:

    \begin{itemize}
    \item
      Refinement method.
    \item
      Variables for weight calculation.
    \item
      Size of the lag window.
    \item
      Procedures for addressing missing data (refer to \textbf{\texttt{match.missing}} and \textbf{\texttt{listwise.delete}} arguments).
    \item
      Maximum size of matched sets (for matching methods).
    \end{itemize}
  \end{itemize}
\item
  \textbf{Supportive Features:}

  \begin{itemize}
  \item
    \textbf{\texttt{print}}, \textbf{\texttt{plot}}, and \textbf{\texttt{summary}} methods assist in understanding matched sets and their sizes.
  \item
    \textbf{\texttt{get\_covariate\_balance}} helps evaluate covariate balance:

    \begin{itemize}
    \tightlist
    \item
      Lower values in the covariate balance calculations are preferred.
    \end{itemize}
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PM.results.none }\OtherTok{\textless{}{-}}
    \FunctionTok{PanelMatch}\NormalTok{(}
        \AttributeTok{lag =} \DecValTok{4}\NormalTok{,}
        \AttributeTok{time.id =} \StringTok{"year"}\NormalTok{,}
        \AttributeTok{unit.id =} \StringTok{"wbcode2"}\NormalTok{,}
        \AttributeTok{treatment =} \StringTok{"dem"}\NormalTok{,}
        \AttributeTok{refinement.method =} \StringTok{"none"}\NormalTok{,}
        \AttributeTok{data =}\NormalTok{ dem,}
        \AttributeTok{match.missing =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{size.match =} \DecValTok{5}\NormalTok{,}
        \AttributeTok{qoi =} \StringTok{"att"}\NormalTok{,}
        \AttributeTok{outcome.var =} \StringTok{"y"}\NormalTok{,}
        \AttributeTok{lead =} \DecValTok{0}\SpecialCharTok{:}\DecValTok{4}\NormalTok{,}
        \AttributeTok{forbid.treatment.reversal =} \ConstantTok{FALSE}\NormalTok{,}
        \AttributeTok{use.diagonal.variance.matrix =} \ConstantTok{TRUE}
\NormalTok{    )}
\NormalTok{PM.results.maha }\OtherTok{\textless{}{-}}
    \FunctionTok{PanelMatch}\NormalTok{(}
        \AttributeTok{lag =} \DecValTok{4}\NormalTok{,}
        \AttributeTok{time.id =} \StringTok{"year"}\NormalTok{,}
        \AttributeTok{unit.id =} \StringTok{"wbcode2"}\NormalTok{,}
        \AttributeTok{treatment =} \StringTok{"dem"}\NormalTok{,}
        \AttributeTok{refinement.method =} \StringTok{"mahalanobis"}\NormalTok{,}
        \AttributeTok{data =}\NormalTok{ dem,}
        \AttributeTok{match.missing =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{covs.formula =} \SpecialCharTok{\textasciitilde{}} \FunctionTok{I}\NormalTok{(}\FunctionTok{lag}\NormalTok{(tradewb, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{)) }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(}\FunctionTok{lag}\NormalTok{(y, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{)),}
        \AttributeTok{size.match =} \DecValTok{5}\NormalTok{,}
        \AttributeTok{qoi =} \StringTok{"att"}\NormalTok{,}
        \AttributeTok{outcome.var =} \StringTok{"y"}\NormalTok{,}
        \AttributeTok{lead =} \DecValTok{0}\SpecialCharTok{:}\DecValTok{4}\NormalTok{,}
        \AttributeTok{forbid.treatment.reversal =} \ConstantTok{FALSE}\NormalTok{,}
        \AttributeTok{use.diagonal.variance.matrix =} \ConstantTok{TRUE}
\NormalTok{    )}

\CommentTok{\# listwise deletion used for missing data}
\NormalTok{PM.results.listwise }\OtherTok{\textless{}{-}}
    \FunctionTok{PanelMatch}\NormalTok{(}
        \AttributeTok{lag =} \DecValTok{4}\NormalTok{,}
        \AttributeTok{time.id =} \StringTok{"year"}\NormalTok{,}
        \AttributeTok{unit.id =} \StringTok{"wbcode2"}\NormalTok{,}
        \AttributeTok{treatment =} \StringTok{"dem"}\NormalTok{,}
        \AttributeTok{refinement.method =} \StringTok{"mahalanobis"}\NormalTok{,}
        \AttributeTok{data =}\NormalTok{ dem,}
        \AttributeTok{match.missing =} \ConstantTok{FALSE}\NormalTok{,}
        \AttributeTok{listwise.delete =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{covs.formula =} \SpecialCharTok{\textasciitilde{}} \FunctionTok{I}\NormalTok{(}\FunctionTok{lag}\NormalTok{(tradewb, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{)) }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(}\FunctionTok{lag}\NormalTok{(y, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{)),}
        \AttributeTok{size.match =} \DecValTok{5}\NormalTok{,}
        \AttributeTok{qoi =} \StringTok{"att"}\NormalTok{,}
        \AttributeTok{outcome.var =} \StringTok{"y"}\NormalTok{,}
        \AttributeTok{lead =} \DecValTok{0}\SpecialCharTok{:}\DecValTok{4}\NormalTok{,}
        \AttributeTok{forbid.treatment.reversal =} \ConstantTok{FALSE}\NormalTok{,}
        \AttributeTok{use.diagonal.variance.matrix =} \ConstantTok{TRUE}
\NormalTok{    )}

\CommentTok{\# propensity score based weighting method}
\NormalTok{PM.results.ps.weight }\OtherTok{\textless{}{-}}
    \FunctionTok{PanelMatch}\NormalTok{(}
        \AttributeTok{lag =} \DecValTok{4}\NormalTok{,}
        \AttributeTok{time.id =} \StringTok{"year"}\NormalTok{,}
        \AttributeTok{unit.id =} \StringTok{"wbcode2"}\NormalTok{,}
        \AttributeTok{treatment =} \StringTok{"dem"}\NormalTok{,}
        \AttributeTok{refinement.method =} \StringTok{"ps.weight"}\NormalTok{,}
        \AttributeTok{data =}\NormalTok{ dem,}
        \AttributeTok{match.missing =} \ConstantTok{FALSE}\NormalTok{,}
        \AttributeTok{listwise.delete =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{covs.formula =} \SpecialCharTok{\textasciitilde{}} \FunctionTok{I}\NormalTok{(}\FunctionTok{lag}\NormalTok{(tradewb, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{)) }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(}\FunctionTok{lag}\NormalTok{(y, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{)),}
        \AttributeTok{size.match =} \DecValTok{5}\NormalTok{,}
        \AttributeTok{qoi =} \StringTok{"att"}\NormalTok{,}
        \AttributeTok{outcome.var =} \StringTok{"y"}\NormalTok{,}
        \AttributeTok{lead =} \DecValTok{0}\SpecialCharTok{:}\DecValTok{4}\NormalTok{,}
        \AttributeTok{forbid.treatment.reversal =} \ConstantTok{FALSE}
\NormalTok{    )}

\FunctionTok{get\_covariate\_balance}\NormalTok{(}
\NormalTok{    PM.results.none}\SpecialCharTok{$}\NormalTok{att,}
    \AttributeTok{data =}\NormalTok{ dem,}
    \AttributeTok{covariates =} \FunctionTok{c}\NormalTok{(}\StringTok{"tradewb"}\NormalTok{, }\StringTok{"y"}\NormalTok{),}
    \AttributeTok{plot =} \ConstantTok{FALSE}
\NormalTok{)}
\CommentTok{\#\textgreater{}         tradewb            y}
\CommentTok{\#\textgreater{} t\_4 {-}0.07245466  0.291871990}
\CommentTok{\#\textgreater{} t\_3 {-}0.20930129  0.208654876}
\CommentTok{\#\textgreater{} t\_2 {-}0.24425207  0.107736647}
\CommentTok{\#\textgreater{} t\_1 {-}0.10806125 {-}0.004950238}

\FunctionTok{get\_covariate\_balance}\NormalTok{(}
\NormalTok{    PM.results.maha}\SpecialCharTok{$}\NormalTok{att,}
    \AttributeTok{data =}\NormalTok{ dem,}
    \AttributeTok{covariates =} \FunctionTok{c}\NormalTok{(}\StringTok{"tradewb"}\NormalTok{, }\StringTok{"y"}\NormalTok{),}
    \AttributeTok{plot =} \ConstantTok{FALSE}
\NormalTok{)}
\CommentTok{\#\textgreater{}         tradewb          y}
\CommentTok{\#\textgreater{} t\_4  0.04558637 0.09701606}
\CommentTok{\#\textgreater{} t\_3 {-}0.03312750 0.10844046}
\CommentTok{\#\textgreater{} t\_2 {-}0.01396793 0.08890753}
\CommentTok{\#\textgreater{} t\_1  0.10474894 0.06618865}


\FunctionTok{get\_covariate\_balance}\NormalTok{(}
\NormalTok{    PM.results.listwise}\SpecialCharTok{$}\NormalTok{att,}
    \AttributeTok{data =}\NormalTok{ dem,}
    \AttributeTok{covariates =} \FunctionTok{c}\NormalTok{(}\StringTok{"tradewb"}\NormalTok{, }\StringTok{"y"}\NormalTok{),}
    \AttributeTok{plot =} \ConstantTok{FALSE}
\NormalTok{)}
\CommentTok{\#\textgreater{}         tradewb          y}
\CommentTok{\#\textgreater{} t\_4  0.05634922 0.05223623}
\CommentTok{\#\textgreater{} t\_3 {-}0.01104797 0.05217896}
\CommentTok{\#\textgreater{} t\_2  0.01411473 0.03094133}
\CommentTok{\#\textgreater{} t\_1  0.06850180 0.02092209}

\FunctionTok{get\_covariate\_balance}\NormalTok{(}
\NormalTok{    PM.results.ps.weight}\SpecialCharTok{$}\NormalTok{att,}
    \AttributeTok{data =}\NormalTok{ dem,}
    \AttributeTok{covariates =} \FunctionTok{c}\NormalTok{(}\StringTok{"tradewb"}\NormalTok{, }\StringTok{"y"}\NormalTok{),}
    \AttributeTok{plot =} \ConstantTok{FALSE}
\NormalTok{)}
\CommentTok{\#\textgreater{}         tradewb          y}
\CommentTok{\#\textgreater{} t\_4 0.014362590 0.04035905}
\CommentTok{\#\textgreater{} t\_3 0.005529734 0.04188731}
\CommentTok{\#\textgreater{} t\_2 0.009410044 0.04195008}
\CommentTok{\#\textgreater{} t\_1 0.027907540 0.03975173}
\end{Highlighting}
\end{Shaded}

\textbf{get\_covariate\_balance Function Options:}

\begin{itemize}
\item
  Allows for the generation of plots displaying covariate balance using \textbf{\texttt{plot\ =\ TRUE}}.
\item
  Plots can be customized using arguments typically used with the base R \textbf{\texttt{plot}} method.
\item
  Option to set \textbf{\texttt{use.equal.weights\ =\ TRUE}} for:

  \begin{itemize}
  \item
    Obtaining the balance of unrefined sets.
  \item
    Facilitating understanding of the refinement's impact.
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Use equal weights}
\FunctionTok{get\_covariate\_balance}\NormalTok{(}
\NormalTok{    PM.results.ps.weight}\SpecialCharTok{$}\NormalTok{att,}
    \AttributeTok{data =}\NormalTok{ dem,}
    \AttributeTok{use.equal.weights =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{covariates =} \FunctionTok{c}\NormalTok{(}\StringTok{"tradewb"}\NormalTok{, }\StringTok{"y"}\NormalTok{),}
    \AttributeTok{plot =} \ConstantTok{TRUE}\NormalTok{,}
    \CommentTok{\# visualize by setting plot to TRUE}
    \AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-22-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Compare covariate balance to refined sets}
\CommentTok{\# See large improvement in balance}
\FunctionTok{get\_covariate\_balance}\NormalTok{(}
\NormalTok{    PM.results.ps.weight}\SpecialCharTok{$}\NormalTok{att,}
    \AttributeTok{data =}\NormalTok{ dem,}
    \AttributeTok{covariates =} \FunctionTok{c}\NormalTok{(}\StringTok{"tradewb"}\NormalTok{, }\StringTok{"y"}\NormalTok{),}
    \AttributeTok{plot =} \ConstantTok{TRUE}\NormalTok{,}
    \CommentTok{\# visualize by setting plot to TRUE}
    \AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-22-2} \end{center}

\begin{Shaded}
\begin{Highlighting}[]


\FunctionTok{balance\_scatter}\NormalTok{(}
    \AttributeTok{matched\_set\_list =} \FunctionTok{list}\NormalTok{(PM.results.maha}\SpecialCharTok{$}\NormalTok{att,}
\NormalTok{                            PM.results.ps.weight}\SpecialCharTok{$}\NormalTok{att),}
    \AttributeTok{data =}\NormalTok{ dem,}
    \AttributeTok{covariates =} \FunctionTok{c}\NormalTok{(}\StringTok{"y"}\NormalTok{, }\StringTok{"tradewb"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-22-3} \end{center}

\textbf{\texttt{PanelEstimate}}

\begin{itemize}
\item
  \textbf{Standard Error Calculation Methods}

  \begin{itemize}
  \item
    There are different methods available:

    \begin{itemize}
    \item
      \textbf{Bootstrap} (default method with 1000 iterations).
    \item
      \textbf{Conditional}: Assumes independence across units, but not time.
    \item
      \textbf{Unconditional}: Doesn't make assumptions of independence across units or time.
    \end{itemize}
  \item
    For \textbf{\texttt{qoi}} values set to \texttt{att}, \texttt{art}, or \texttt{atc} \citep{imai2021matching}:

    \begin{itemize}
    \tightlist
    \item
      You can use analytical methods for calculating standard errors, which include both ``conditional'' and ``unconditional'' methods.
    \end{itemize}
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PE.results }\OtherTok{\textless{}{-}} \FunctionTok{PanelEstimate}\NormalTok{(}
    \AttributeTok{sets              =}\NormalTok{ PM.results.ps.weight,}
    \AttributeTok{data              =}\NormalTok{ dem,}
    \AttributeTok{se.method         =} \StringTok{"bootstrap"}\NormalTok{,}
    \AttributeTok{number.iterations =} \DecValTok{1000}\NormalTok{,}
    \AttributeTok{confidence.level  =}\NormalTok{ .}\DecValTok{95}
\NormalTok{)}

\CommentTok{\# point estimates}
\NormalTok{PE.results[[}\StringTok{"estimates"}\NormalTok{]]}
\CommentTok{\#\textgreater{}       t+0       t+1       t+2       t+3       t+4 }
\CommentTok{\#\textgreater{} 0.2609565 0.9630847 1.2851017 1.7370930 1.4871846}

\CommentTok{\# standard errors}
\NormalTok{PE.results[[}\StringTok{"standard.error"}\NormalTok{]]}
\CommentTok{\#\textgreater{}       t+0       t+1       t+2       t+3       t+4 }
\CommentTok{\#\textgreater{} 0.6399349 1.0304938 1.3825265 1.7625951 2.1672629}


\CommentTok{\# use conditional method}
\NormalTok{PE.results }\OtherTok{\textless{}{-}} \FunctionTok{PanelEstimate}\NormalTok{(}
    \AttributeTok{sets             =}\NormalTok{ PM.results.ps.weight,}
    \AttributeTok{data             =}\NormalTok{ dem,}
    \AttributeTok{se.method        =} \StringTok{"conditional"}\NormalTok{,}
    \AttributeTok{confidence.level =}\NormalTok{ .}\DecValTok{95}
\NormalTok{)}

\CommentTok{\# point estimates}
\NormalTok{PE.results[[}\StringTok{"estimates"}\NormalTok{]]}
\CommentTok{\#\textgreater{}       t+0       t+1       t+2       t+3       t+4 }
\CommentTok{\#\textgreater{} 0.2609565 0.9630847 1.2851017 1.7370930 1.4871846}

\CommentTok{\# standard errors}
\NormalTok{PE.results[[}\StringTok{"standard.error"}\NormalTok{]]}
\CommentTok{\#\textgreater{}       t+0       t+1       t+2       t+3       t+4 }
\CommentTok{\#\textgreater{} 0.4844805 0.8170604 1.1171942 1.4116879 1.7172143}

\FunctionTok{summary}\NormalTok{(PE.results)}
\CommentTok{\#\textgreater{} Weighted Difference{-}in{-}Differences with Propensity Score}
\CommentTok{\#\textgreater{} Matches created with 4 lags}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Standard errors computed with conditional  method}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Estimate of Average Treatment Effect on the Treated (ATT) by Period:}
\CommentTok{\#\textgreater{} $summary}
\CommentTok{\#\textgreater{}      estimate std.error       2.5\%    97.5\%}
\CommentTok{\#\textgreater{} t+0 0.2609565 0.4844805 {-}0.6886078 1.210521}
\CommentTok{\#\textgreater{} t+1 0.9630847 0.8170604 {-}0.6383243 2.564494}
\CommentTok{\#\textgreater{} t+2 1.2851017 1.1171942 {-}0.9045586 3.474762}
\CommentTok{\#\textgreater{} t+3 1.7370930 1.4116879 {-}1.0297644 4.503950}
\CommentTok{\#\textgreater{} t+4 1.4871846 1.7172143 {-}1.8784937 4.852863}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $lag}
\CommentTok{\#\textgreater{} [1] 4}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $qoi}
\CommentTok{\#\textgreater{} [1] "att"}

\FunctionTok{plot}\NormalTok{(PE.results)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-23-1} \end{center}

\textbf{Moderating Variables}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# moderating variable}
\NormalTok{dem}\SpecialCharTok{$}\NormalTok{moderator }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{dem}\SpecialCharTok{$}\NormalTok{moderator }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(dem}\SpecialCharTok{$}\NormalTok{wbcode2 }\SpecialCharTok{\textgreater{}} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{)}

\NormalTok{PM.results }\OtherTok{\textless{}{-}}
    \FunctionTok{PanelMatch}\NormalTok{(}
        \AttributeTok{lag                          =} \DecValTok{4}\NormalTok{,}
        \AttributeTok{time.id                      =} \StringTok{"year"}\NormalTok{,}
        \AttributeTok{unit.id                      =} \StringTok{"wbcode2"}\NormalTok{,}
        \AttributeTok{treatment                    =} \StringTok{"dem"}\NormalTok{,}
        \AttributeTok{refinement.method            =} \StringTok{"mahalanobis"}\NormalTok{,}
        \AttributeTok{data                         =}\NormalTok{ dem,}
        \AttributeTok{match.missing                =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{covs.formula                 =} \SpecialCharTok{\textasciitilde{}} \FunctionTok{I}\NormalTok{(}\FunctionTok{lag}\NormalTok{(tradewb, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{)) }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(}\FunctionTok{lag}\NormalTok{(y, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{)),}
        \AttributeTok{size.match                   =} \DecValTok{5}\NormalTok{,}
        \AttributeTok{qoi                          =} \StringTok{"att"}\NormalTok{,}
        \AttributeTok{outcome.var                  =} \StringTok{"y"}\NormalTok{,}
        \AttributeTok{lead                         =} \DecValTok{0}\SpecialCharTok{:}\DecValTok{4}\NormalTok{,}
        \AttributeTok{forbid.treatment.reversal    =} \ConstantTok{FALSE}\NormalTok{,}
        \AttributeTok{use.diagonal.variance.matrix =} \ConstantTok{TRUE}
\NormalTok{    )}
\NormalTok{PE.results }\OtherTok{\textless{}{-}}
    \FunctionTok{PanelEstimate}\NormalTok{(}\AttributeTok{sets      =}\NormalTok{ PM.results,}
                  \AttributeTok{data      =}\NormalTok{ dem,}
                  \AttributeTok{moderator =} \StringTok{"moderator"}\NormalTok{)}

\CommentTok{\# Each element in the list corresponds to a level in the moderator}
\FunctionTok{plot}\NormalTok{(PE.results[[}\DecValTok{1}\NormalTok{]])}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-24-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\FunctionTok{plot}\NormalTok{(PE.results[[}\DecValTok{2}\NormalTok{]])}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-24-2} \end{center}

To write up for journal submission, you can follow the following report:

In this study, closely aligned with the research by \citep{acemoglu2019democracy}, two key effects of democracy on economic growth are estimated: the impact of democratization and that of authoritarian reversal. The treatment variable, \(X_{it}\), is defined to be one if country \(i\) is democratic in year \(t\), and zero otherwise.

The Average Treatment Effect for the Treated (ATT) under democratization is formulated as follows:

\[
\begin{aligned}
\delta(F, L) &= \mathbb{E} \left\{ Y_{i, t + F} (X_{it} = 1, X_{i, t - 1} = 0, \{X_{i,t-l}\}_{l=2}^L) \right. \\
&\left. - Y_{i, t + F} (X_{it} = 0, X_{i, t - 1} = 0, \{X_{i,t-l}\}_{l=2}^L) | X_{it} = 1, X_{i, t - 1} = 0 \right\}
\end{aligned}
\]

In this framework, the treated observations are countries that transition from an authoritarian regime \(X_{it-1} = 0\) to a democratic one \(X_{it} = 1\). The variable \(F\) represents the number of leads, denoting the time periods following the treatment, and \(L\) signifies the number of lags, indicating the time periods preceding the treatment.

The ATT under authoritarian reversal is given by:

\[
\begin{aligned}
&\mathbb{E} \left[ Y_{i, t + F} (X_{it} = 0, X_{i, t - 1} = 1, \{ X_{i, t - l}\}_{l=2}^L ) \right. \\
&\left. - Y_{i, t + F} (X_{it} = 1, X_{it-1} = 1, \{X_{i, t - l} \}_{l=2}^L ) | X_{it} = 0, X_{i, t - 1} = 1 \right]
\end{aligned}
\]

The ATT is calculated conditioning on 4 years of lags (\(L = 4\)) and up to 4 years following the policy change \(F = 1, 2, 3, 4\). Matched sets for each treated observation are constructed based on its treatment history, with the number of matched control units generally decreasing when considering a 4-year treatment history as compared to a 1-year history.

To enhance the quality of matched sets, methods such as Mahalanobis distance matching, propensity score matching, and propensity score weighting are utilized. These approaches enable us to evaluate the effectiveness of each refinement method. In the process of matching, we employ both up-to-five and up-to-ten matching to investigate how sensitive our empirical results are to the maximum number of allowed matches. For more information on the refinement process, please see the Web Appendix

\begin{quote}
The Mahalanobis distance is expressed through a specific formula. We aim to pair each treated unit with a maximum of \(J\) control units, permitting replacement, denoted as \(| \mathcal{M}_{it} \le J|\). The average Mahalanobis distance between a treated and each control unit over time is computed as:

\[ S_{it} (i') = \frac{1}{L} \sum_{l = 1}^L \sqrt{(\mathbf{V}_{i, t - l} - \mathbf{V}_{i', t -l})^T \mathbf{\Sigma}_{i, t - l}^{-1} (\mathbf{V}_{i, t - l} - \mathbf{V}_{i', t -l})} \]

For a matched control unit \(i' \in \mathcal{M}_{it}\), \(\mathbf{V}_{it'}\) represents the time-varying covariates to adjust for, and \(\mathbf{\Sigma}_{it'}\) is the sample covariance matrix for \(\mathbf{V}_{it'}\). Essentially, we calculate a standardized distance using time-varying covariates and average this across different time intervals.

In the context of propensity score matching, we employ a logistic regression model with balanced covariates to derive the propensity score. Defined as the conditional likelihood of treatment given pre-treatment covariates \citep{rosenbaum1983central}, the propensity score is estimated by first creating a data subset comprised of all treated and their matched control units from the same year. This logistic regression model is then fitted as follows:

\[ \begin{aligned} & e_{it} (\{\mathbf{U}_{i, t - l} \}^L_{l = 1}) \\ &= Pr(X_{it} = 1| \mathbf{U}_{i, t -1}, \ldots, \mathbf{U}_{i, t - L}) \\ &= \frac{1}{1 = \exp(- \sum_{l = 1}^L \beta_l^T \mathbf{U}_{i, t - l})} \end{aligned} \]

where \(\mathbf{U}_{it'} = (X_{it'}, \mathbf{V}_{it'}^T)^T\). Given this model, the estimated propensity score for all treated and matched control units is then computed. This enables the adjustment for lagged covariates via matching on the calculated propensity score, resulting in the following distance measure:

\[ S_{it} (i') = | \text{logit} \{ \hat{e}_{it} (\{ \mathbf{U}_{i, t - l}\}^L_{l = 1})\} - \text{logit} \{ \hat{e}_{i't}( \{ \mathbf{U}_{i', t - l} \}^L_{l = 1})\} | \]

Here, \(\hat{e}_{i't} (\{ \mathbf{U}_{i, t - l}\}^L_{l = 1})\) represents the estimated propensity score for each matched control unit \(i' \in \mathcal{M}_{it}\).

Once the distance measure \(S_{it} (i')\) has been determined for all control units in the original matched set, we fine-tune this set by selecting up to \(J\) closest control units, which meet a researcher-defined caliper constraint \(C\). All other control units receive zero weight. This results in a refined matched set for each treated unit \((i, t)\):

\[ \mathcal{M}_{it}^* = \{i' : i' \in \mathcal{M}_{it}, S_{it} (i') < C, S_{it} \le S_{it}^{(J)}\} \]

\(S_{it}^{(J)}\) is the \(J\)th smallest distance among the control units in the original set \(\mathcal{M}_{it}\).

For further refinement using weighting, a weight is assigned to each control unit \(i'\) in a matched set corresponding to a treated unit \((i, t)\), with greater weight accorded to more similar units. We utilize inverse propensity score weighting, based on the propensity score model mentioned earlier:

\[ w_{it}^{i'} \propto \frac{\hat{e}_{i't} (\{ \mathbf{U}_{i, t-l} \}^L_{l = 1} )}{1 - \hat{e}_{i't} (\{ \mathbf{U}_{i, t-l} \}^L_{l = 1} )} \]

In this model, \(\sum_{i' \in \mathcal{M}_{it}} w_{it}^{i'} = 1\) and \(w_{it}^{i'} = 0\) for \(i' \notin \mathcal{M}_{it}\). The model is fitted to the complete sample of treated and matched control units.
\end{quote}

\begin{quote}
Checking Covariate Balance A distinct advantage of the proposed methodology over regression methods is the ability it offers researchers to inspect the covariate balance between treated and matched control observations. This facilitates the evaluation of whether treated and matched control observations are comparable regarding observed confounders. To investigate the mean difference of each covariate (e.g., \(V_{it'j}\), representing the \(j\)-th variable in \(\mathbf{V}_{it'}\)) between the treated observation and its matched control observation at each pre-treatment time period (i.e., \(t' < t\)), we further standardize this difference. For any given pretreatment time period, we adjust by the standard deviation of each covariate across all treated observations in the dataset. Thus, the mean difference is quantified in terms of standard deviation units. Formally, for each treated observation \((i,t)\) where \(D_{it} = 1\), we define the covariate balance for variable \(j\) at the pretreatment time period \(t - l\) as: \begin{equation}
B_{it}(j, l) = \frac{V_{i, t- l,j}- \sum_{i' \in \mathcal{M}_{it}}w_{it}^{i'}V_{i', t-l,j}}{\sqrt{\frac{1}{N_1 - 1} \sum_{i'=1}^N \sum_{t' = L+1}^{T-F}D_{i't'}(V_{i', t'-l, j} - \bar{V}_{t' - l, j})^2}}
\label{eq:covbalance}
\end{equation} where \(N_1 = \sum_{i'= 1}^N \sum_{t' = L+1}^{T-F} D_{i't'}\) denotes the total number of treated observations and \(\bar{V}_{t-l,j} = \sum_{i=1}^N D_{i,t-l,j}/N\). We then aggregate this covariate balance measure across all treated observations for each covariate and pre-treatment time period: \begin{equation}
\bar{B}(j, l) = \frac{1}{N_1} \sum_{i=1}^N \sum_{t = L+ 1}^{T-F}D_{it} B_{it}(j,l)
\label{eq:aggbalance}
\end{equation} Lastly, we evaluate the balance of lagged outcome variables over several pre-treatment periods and that of time-varying covariates. This examination aids in assessing the validity of the parallel trend assumption integral to the DiD estimator justification.
\end{quote}

In Figure \ref{fig:balancescatter}, we demonstrate the enhancement of covariate balance thank to the refinement of matched sets. Each scatter plot contrasts the absolute standardized mean difference, as detailed in Equation \eqref{eq:aggbalance}, before (horizontal axis) and after (vertical axis) this refinement. Points below the 45-degree line indicate an improved standardized mean balance for certain time-varying covariates post-refinement. The majority of variables benefit from this refinement process. Notably, the propensity score weighting (bottom panel) shows the most significant improvement, whereas Mahalanobis matching (top panel) yields a more modest improvement.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(PanelMatch)}
\FunctionTok{library}\NormalTok{(causalverse)}

\NormalTok{runPanelMatch }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(method, lag, }\AttributeTok{size.match=}\ConstantTok{NULL}\NormalTok{, }\AttributeTok{qoi=}\StringTok{"att"}\NormalTok{) \{}
    
    \CommentTok{\# Default parameters for PanelMatch}
\NormalTok{    common.args }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
        \AttributeTok{lag =}\NormalTok{ lag,}
        \AttributeTok{time.id =} \StringTok{"year"}\NormalTok{,}
        \AttributeTok{unit.id =} \StringTok{"wbcode2"}\NormalTok{,}
        \AttributeTok{treatment =} \StringTok{"dem"}\NormalTok{,}
        \AttributeTok{data =}\NormalTok{ dem,}
        \AttributeTok{covs.formula =} \SpecialCharTok{\textasciitilde{}} \FunctionTok{I}\NormalTok{(}\FunctionTok{lag}\NormalTok{(tradewb, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{)) }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(}\FunctionTok{lag}\NormalTok{(y, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{)),}
        \AttributeTok{qoi =}\NormalTok{ qoi,}
        \AttributeTok{outcome.var =} \StringTok{"y"}\NormalTok{,}
        \AttributeTok{lead =} \DecValTok{0}\SpecialCharTok{:}\DecValTok{4}\NormalTok{,}
        \AttributeTok{forbid.treatment.reversal =} \ConstantTok{FALSE}\NormalTok{,}
        \AttributeTok{size.match =}\NormalTok{ size.match  }\CommentTok{\# setting size.match here for all methods}
\NormalTok{    )}
    
    \ControlFlowTok{if}\NormalTok{(method }\SpecialCharTok{==} \StringTok{"mahalanobis"}\NormalTok{) \{}
\NormalTok{        common.args}\SpecialCharTok{$}\NormalTok{refinement.method }\OtherTok{\textless{}{-}} \StringTok{"mahalanobis"}
\NormalTok{        common.args}\SpecialCharTok{$}\NormalTok{match.missing }\OtherTok{\textless{}{-}} \ConstantTok{TRUE}
\NormalTok{        common.args}\SpecialCharTok{$}\NormalTok{use.diagonal.variance.matrix }\OtherTok{\textless{}{-}} \ConstantTok{TRUE}
\NormalTok{    \} }\ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{(method }\SpecialCharTok{==} \StringTok{"ps.match"}\NormalTok{) \{}
\NormalTok{        common.args}\SpecialCharTok{$}\NormalTok{refinement.method }\OtherTok{\textless{}{-}} \StringTok{"ps.match"}
\NormalTok{        common.args}\SpecialCharTok{$}\NormalTok{match.missing }\OtherTok{\textless{}{-}} \ConstantTok{FALSE}
\NormalTok{        common.args}\SpecialCharTok{$}\NormalTok{listwise.delete }\OtherTok{\textless{}{-}} \ConstantTok{TRUE}
\NormalTok{    \} }\ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{(method }\SpecialCharTok{==} \StringTok{"ps.weight"}\NormalTok{) \{}
\NormalTok{        common.args}\SpecialCharTok{$}\NormalTok{refinement.method }\OtherTok{\textless{}{-}} \StringTok{"ps.weight"}
\NormalTok{        common.args}\SpecialCharTok{$}\NormalTok{match.missing }\OtherTok{\textless{}{-}} \ConstantTok{FALSE}
\NormalTok{        common.args}\SpecialCharTok{$}\NormalTok{listwise.delete }\OtherTok{\textless{}{-}} \ConstantTok{TRUE}
\NormalTok{    \}}
    
    \FunctionTok{return}\NormalTok{(}\FunctionTok{do.call}\NormalTok{(PanelMatch, common.args))}
\NormalTok{\}}

\NormalTok{methods }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"mahalanobis"}\NormalTok{, }\StringTok{"ps.match"}\NormalTok{, }\StringTok{"ps.weight"}\NormalTok{)}
\NormalTok{lags }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{)}
\NormalTok{sizes }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

You can either do it sequentailly

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res\_pm }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}

\ControlFlowTok{for}\NormalTok{(method }\ControlFlowTok{in}\NormalTok{ methods) \{}
    \ControlFlowTok{for}\NormalTok{(lag }\ControlFlowTok{in}\NormalTok{ lags) \{}
        \ControlFlowTok{for}\NormalTok{(size }\ControlFlowTok{in}\NormalTok{ sizes) \{}
\NormalTok{            name }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(method, }\StringTok{"."}\NormalTok{, lag, }\StringTok{"lag."}\NormalTok{, size, }\StringTok{"m"}\NormalTok{)}
\NormalTok{            res\_pm[[name]] }\OtherTok{\textless{}{-}} \FunctionTok{runPanelMatch}\NormalTok{(method, lag, size)}
\NormalTok{        \}}
\NormalTok{    \}}
\NormalTok{\}}

\CommentTok{\# Now, you can access res\_pm using res\_pm[["mahalanobis.1lag.5m"]] etc.}

\CommentTok{\# for treatment reversal}
\NormalTok{res\_pm\_rev }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}

\ControlFlowTok{for}\NormalTok{(method }\ControlFlowTok{in}\NormalTok{ methods) \{}
    \ControlFlowTok{for}\NormalTok{(lag }\ControlFlowTok{in}\NormalTok{ lags) \{}
        \ControlFlowTok{for}\NormalTok{(size }\ControlFlowTok{in}\NormalTok{ sizes) \{}
\NormalTok{            name }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(method, }\StringTok{"."}\NormalTok{, lag, }\StringTok{"lag."}\NormalTok{, size, }\StringTok{"m"}\NormalTok{)}
\NormalTok{            res\_pm\_rev[[name]] }\OtherTok{\textless{}{-}} \FunctionTok{runPanelMatch}\NormalTok{(method, lag, size, }\AttributeTok{qoi =} \StringTok{"art"}\NormalTok{)}
\NormalTok{        \}}
\NormalTok{    \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

or in parallel

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(foreach)}
\FunctionTok{library}\NormalTok{(doParallel)}
\FunctionTok{registerDoParallel}\NormalTok{(}\AttributeTok{cores =} \DecValTok{4}\NormalTok{)}
\CommentTok{\# Initialize an empty list to store results}
\NormalTok{res\_pm }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}

\CommentTok{\# Replace nested for{-}loops with foreach}
\NormalTok{results }\OtherTok{\textless{}{-}}
  \FunctionTok{foreach}\NormalTok{(}
    \AttributeTok{method =}\NormalTok{ methods,}
    \AttributeTok{.combine =} \StringTok{\textquotesingle{}c\textquotesingle{}}\NormalTok{,}
    \AttributeTok{.multicombine =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{.packages =} \FunctionTok{c}\NormalTok{(}\StringTok{"PanelMatch"}\NormalTok{, }\StringTok{"causalverse"}\NormalTok{)}
\NormalTok{  ) }\SpecialCharTok{\%dopar\%}\NormalTok{ \{}
\NormalTok{    tmp }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}
    \ControlFlowTok{for}\NormalTok{ (lag }\ControlFlowTok{in}\NormalTok{ lags) \{}
      \ControlFlowTok{for}\NormalTok{ (size }\ControlFlowTok{in}\NormalTok{ sizes) \{}
\NormalTok{        name }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(method, }\StringTok{"."}\NormalTok{, lag, }\StringTok{"lag."}\NormalTok{, size, }\StringTok{"m"}\NormalTok{)}
\NormalTok{        tmp[[name]] }\OtherTok{\textless{}{-}} \FunctionTok{runPanelMatch}\NormalTok{(method, lag, size)}
\NormalTok{      \}}
\NormalTok{    \}}
\NormalTok{    tmp}
\NormalTok{  \}}

\CommentTok{\# Collate results}
\ControlFlowTok{for}\NormalTok{ (name }\ControlFlowTok{in} \FunctionTok{names}\NormalTok{(results)) \{}
\NormalTok{  res\_pm[[name]] }\OtherTok{\textless{}{-}}\NormalTok{ results[[name]]}
\NormalTok{\}}

\CommentTok{\# Treatment reversal}
\CommentTok{\# Initialize an empty list to store results}
\NormalTok{res\_pm\_rev }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}

\CommentTok{\# Replace nested for{-}loops with foreach}
\NormalTok{results\_rev }\OtherTok{\textless{}{-}}
  \FunctionTok{foreach}\NormalTok{(}
    \AttributeTok{method =}\NormalTok{ methods,}
    \AttributeTok{.combine =} \StringTok{\textquotesingle{}c\textquotesingle{}}\NormalTok{,}
    \AttributeTok{.multicombine =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{.packages =} \FunctionTok{c}\NormalTok{(}\StringTok{"PanelMatch"}\NormalTok{, }\StringTok{"causalverse"}\NormalTok{)}
\NormalTok{  ) }\SpecialCharTok{\%dopar\%}\NormalTok{ \{}
\NormalTok{    tmp }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}
    \ControlFlowTok{for}\NormalTok{ (lag }\ControlFlowTok{in}\NormalTok{ lags) \{}
      \ControlFlowTok{for}\NormalTok{ (size }\ControlFlowTok{in}\NormalTok{ sizes) \{}
\NormalTok{        name }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(method, }\StringTok{"."}\NormalTok{, lag, }\StringTok{"lag."}\NormalTok{, size, }\StringTok{"m"}\NormalTok{)}
\NormalTok{        tmp[[name]] }\OtherTok{\textless{}{-}}
          \FunctionTok{runPanelMatch}\NormalTok{(method, lag, size, }\AttributeTok{qoi =} \StringTok{"art"}\NormalTok{)}
\NormalTok{      \}}
\NormalTok{    \}}
\NormalTok{    tmp}
\NormalTok{  \}}

\CommentTok{\# Collate results}
\ControlFlowTok{for}\NormalTok{ (name }\ControlFlowTok{in} \FunctionTok{names}\NormalTok{(results\_rev)) \{}
\NormalTok{  res\_pm\_rev[[name]] }\OtherTok{\textless{}{-}}\NormalTok{ results\_rev[[name]]}
\NormalTok{\}}


\FunctionTok{stopImplicitCluster}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(gridExtra)}

\CommentTok{\# Updated plotting function}
\NormalTok{create\_balance\_plot }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(method, lag, sizes, res\_pm, dem) \{}
\NormalTok{    matched\_set\_lists }\OtherTok{\textless{}{-}} \FunctionTok{lapply}\NormalTok{(sizes, }\ControlFlowTok{function}\NormalTok{(size) \{}
\NormalTok{        res\_pm[[}\FunctionTok{paste0}\NormalTok{(method, }\StringTok{"."}\NormalTok{, lag, }\StringTok{"lag."}\NormalTok{, size, }\StringTok{"m"}\NormalTok{)]]}\SpecialCharTok{$}\NormalTok{att}
\NormalTok{    \})}
    
    \FunctionTok{return}\NormalTok{(}
        \FunctionTok{balance\_scatter\_custom}\NormalTok{(}
            \AttributeTok{matched\_set\_list =}\NormalTok{ matched\_set\_lists,}
            \AttributeTok{legend.title =} \StringTok{"Possible Matches"}\NormalTok{,}
            \AttributeTok{set.names =} \FunctionTok{as.character}\NormalTok{(sizes),}
            \AttributeTok{legend.position =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\FloatTok{0.8}\NormalTok{),}
            
            \CommentTok{\# for compiled plot, you don\textquotesingle{}t need x,y, or main labs}
            \AttributeTok{x.axis.label =} \StringTok{""}\NormalTok{,}
            \AttributeTok{y.axis.label =} \StringTok{""}\NormalTok{,}
            \AttributeTok{main =} \StringTok{""}\NormalTok{,}
            \AttributeTok{data =}\NormalTok{ dem,}
            \AttributeTok{dot.size =} \DecValTok{5}\NormalTok{,}
            \CommentTok{\# show.legend = F,}
            \AttributeTok{them\_use =}\NormalTok{ causalverse}\SpecialCharTok{::}\FunctionTok{ama\_theme}\NormalTok{(}\AttributeTok{base\_size =} \DecValTok{32}\NormalTok{),}
            \AttributeTok{covariates =} \FunctionTok{c}\NormalTok{(}\StringTok{"y"}\NormalTok{, }\StringTok{"tradewb"}\NormalTok{)}
\NormalTok{        )}
\NormalTok{    )}
\NormalTok{\}}

\NormalTok{plots }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}

\ControlFlowTok{for}\NormalTok{ (method }\ControlFlowTok{in}\NormalTok{ methods) \{}
    \ControlFlowTok{for}\NormalTok{ (lag }\ControlFlowTok{in}\NormalTok{ lags) \{}
\NormalTok{        plots[[}\FunctionTok{paste0}\NormalTok{(method, }\StringTok{"."}\NormalTok{, lag, }\StringTok{"lag"}\NormalTok{)]] }\OtherTok{\textless{}{-}}
            \FunctionTok{create\_balance\_plot}\NormalTok{(method, lag, sizes, res\_pm, dem)}
\NormalTok{    \}}
\NormalTok{\}}

\CommentTok{\# \# Arranging plots in a 3x2 grid}
\CommentTok{\# grid.arrange(plots[["mahalanobis.1lag"]],}
\CommentTok{\#              plots[["mahalanobis.4lag"]],}
\CommentTok{\#              plots[["ps.match.1lag"]],}
\CommentTok{\#              plots[["ps.match.4lag"]],}
\CommentTok{\#              plots[["ps.weight.1lag"]],}
\CommentTok{\#              plots[["ps.weight.4lag"]],}
\CommentTok{\#              ncol=2, nrow=3)}


\CommentTok{\# Standardized Mean Difference of Covariates}
\FunctionTok{library}\NormalTok{(gridExtra)}
\FunctionTok{library}\NormalTok{(grid)}

\CommentTok{\# Create column and row labels using textGrob}
\NormalTok{col\_labels }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"1{-}year Lag"}\NormalTok{, }\StringTok{"4{-}year Lag"}\NormalTok{)}
\NormalTok{row\_labels }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Maha Matching"}\NormalTok{, }\StringTok{"PS Matching"}\NormalTok{, }\StringTok{"PS Weigthing"}\NormalTok{)}

\NormalTok{major.axes.fontsize }\OtherTok{=} \DecValTok{40}
\NormalTok{minor.axes.fontsize }\OtherTok{=} \DecValTok{30}

\FunctionTok{png}\NormalTok{(}
    \FunctionTok{file.path}\NormalTok{(}\FunctionTok{getwd}\NormalTok{(), }\StringTok{"images"}\NormalTok{, }\StringTok{"did\_balance\_scatter.png"}\NormalTok{),}
    \AttributeTok{width =} \DecValTok{1200}\NormalTok{,}
    \AttributeTok{height =} \DecValTok{1000}
\NormalTok{)}

\CommentTok{\# Create a list{-}of{-}lists, where each inner list represents a row}
\NormalTok{grid\_list }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
    \FunctionTok{list}\NormalTok{(}
        \FunctionTok{nullGrob}\NormalTok{(),}
        \FunctionTok{textGrob}\NormalTok{(col\_labels[}\DecValTok{1}\NormalTok{], }\AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ minor.axes.fontsize)),}
        \FunctionTok{textGrob}\NormalTok{(col\_labels[}\DecValTok{2}\NormalTok{], }\AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ minor.axes.fontsize))}
\NormalTok{    ),}
    
    \FunctionTok{list}\NormalTok{(}\FunctionTok{textGrob}\NormalTok{(}
\NormalTok{        row\_labels[}\DecValTok{1}\NormalTok{],}
        \AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ minor.axes.fontsize),}
        \AttributeTok{rot =} \DecValTok{90}
\NormalTok{    ), plots[[}\StringTok{"mahalanobis.1lag"}\NormalTok{]], plots[[}\StringTok{"mahalanobis.4lag"}\NormalTok{]]),}
    
    \FunctionTok{list}\NormalTok{(}\FunctionTok{textGrob}\NormalTok{(}
\NormalTok{        row\_labels[}\DecValTok{2}\NormalTok{],}
        \AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ minor.axes.fontsize),}
        \AttributeTok{rot =} \DecValTok{90}
\NormalTok{    ), plots[[}\StringTok{"ps.match.1lag"}\NormalTok{]], plots[[}\StringTok{"ps.match.4lag"}\NormalTok{]]),}
    
    \FunctionTok{list}\NormalTok{(}\FunctionTok{textGrob}\NormalTok{(}
\NormalTok{        row\_labels[}\DecValTok{3}\NormalTok{],}
        \AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ minor.axes.fontsize),}
        \AttributeTok{rot =} \DecValTok{90}
\NormalTok{    ), plots[[}\StringTok{"ps.weight.1lag"}\NormalTok{]], plots[[}\StringTok{"ps.weight.4lag"}\NormalTok{]])}
\NormalTok{)}

\CommentTok{\# "Flatten" the list{-}of{-}lists into a single list of grobs}
\NormalTok{grobs }\OtherTok{\textless{}{-}} \FunctionTok{do.call}\NormalTok{(c, grid\_list)}

\FunctionTok{grid.arrange}\NormalTok{(}
    \AttributeTok{grobs =}\NormalTok{ grobs,}
    \AttributeTok{ncol =} \DecValTok{3}\NormalTok{,}
    \AttributeTok{nrow =} \DecValTok{4}\NormalTok{,}
    \AttributeTok{widths =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.15}\NormalTok{, }\FloatTok{0.42}\NormalTok{, }\FloatTok{0.42}\NormalTok{),}
    \AttributeTok{heights =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.15}\NormalTok{, }\FloatTok{0.28}\NormalTok{, }\FloatTok{0.28}\NormalTok{, }\FloatTok{0.28}\NormalTok{)}
\NormalTok{)}

\FunctionTok{grid.text}\NormalTok{(}
    \StringTok{"Before Refinement"}\NormalTok{,}
    \AttributeTok{x =} \FloatTok{0.5}\NormalTok{,}
    \AttributeTok{y =} \FloatTok{0.03}\NormalTok{,}
    \AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ major.axes.fontsize)}
\NormalTok{)}
\FunctionTok{grid.text}\NormalTok{(}
    \StringTok{"After Refinement"}\NormalTok{,}
    \AttributeTok{x =} \FloatTok{0.03}\NormalTok{,}
    \AttributeTok{y =} \FloatTok{0.5}\NormalTok{,}
    \AttributeTok{rot =} \DecValTok{90}\NormalTok{,}
    \AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ major.axes.fontsize)}
\NormalTok{)}
\FunctionTok{dev.off}\NormalTok{()}
\CommentTok{\#\textgreater{} pdf }
\CommentTok{\#\textgreater{}   2}
\end{Highlighting}
\end{Shaded}

Note: Scatter plots display the standardized mean difference of each covariate \(j\) and lag year \(l\) as defined in Equation \eqref{eq:aggbalance} before (x-axis) and after (y-axis) matched set refinement. Each plot includes varying numbers of possible matches for each matching method. Rows represent different matching/weighting methods, while columns indicate adjustments for various lag lengths.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Step 1: Define configurations}
\NormalTok{configurations }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
    \FunctionTok{list}\NormalTok{(}\AttributeTok{refinement.method =} \StringTok{"none"}\NormalTok{, }\AttributeTok{qoi =} \StringTok{"att"}\NormalTok{),}
    \FunctionTok{list}\NormalTok{(}\AttributeTok{refinement.method =} \StringTok{"none"}\NormalTok{, }\AttributeTok{qoi =} \StringTok{"art"}\NormalTok{),}
    \FunctionTok{list}\NormalTok{(}\AttributeTok{refinement.method =} \StringTok{"mahalanobis"}\NormalTok{, }\AttributeTok{qoi =} \StringTok{"att"}\NormalTok{),}
    \FunctionTok{list}\NormalTok{(}\AttributeTok{refinement.method =} \StringTok{"mahalanobis"}\NormalTok{, }\AttributeTok{qoi =} \StringTok{"art"}\NormalTok{),}
    \FunctionTok{list}\NormalTok{(}\AttributeTok{refinement.method =} \StringTok{"ps.match"}\NormalTok{, }\AttributeTok{qoi =} \StringTok{"att"}\NormalTok{),}
    \FunctionTok{list}\NormalTok{(}\AttributeTok{refinement.method =} \StringTok{"ps.match"}\NormalTok{, }\AttributeTok{qoi =} \StringTok{"art"}\NormalTok{),}
    \FunctionTok{list}\NormalTok{(}\AttributeTok{refinement.method =} \StringTok{"ps.weight"}\NormalTok{, }\AttributeTok{qoi =} \StringTok{"att"}\NormalTok{),}
    \FunctionTok{list}\NormalTok{(}\AttributeTok{refinement.method =} \StringTok{"ps.weight"}\NormalTok{, }\AttributeTok{qoi =} \StringTok{"art"}\NormalTok{)}
\NormalTok{)}

\CommentTok{\# Step 2: Use lapply or loop to generate results}
\NormalTok{results }\OtherTok{\textless{}{-}} \FunctionTok{lapply}\NormalTok{(configurations, }\ControlFlowTok{function}\NormalTok{(config) \{}
    \FunctionTok{PanelMatch}\NormalTok{(}
        \AttributeTok{lag                       =} \DecValTok{4}\NormalTok{,}
        \AttributeTok{time.id                   =} \StringTok{"year"}\NormalTok{,}
        \AttributeTok{unit.id                   =} \StringTok{"wbcode2"}\NormalTok{,}
        \AttributeTok{treatment                 =} \StringTok{"dem"}\NormalTok{,}
        \AttributeTok{data                      =}\NormalTok{ dem,}
        \AttributeTok{match.missing             =} \ConstantTok{FALSE}\NormalTok{,}
        \AttributeTok{listwise.delete           =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{size.match                =} \DecValTok{5}\NormalTok{,}
        \AttributeTok{outcome.var               =} \StringTok{"y"}\NormalTok{,}
        \AttributeTok{lead                      =} \DecValTok{0}\SpecialCharTok{:}\DecValTok{4}\NormalTok{,}
        \AttributeTok{forbid.treatment.reversal =} \ConstantTok{FALSE}\NormalTok{,}
        \AttributeTok{refinement.method         =}\NormalTok{ config}\SpecialCharTok{$}\NormalTok{refinement.method,}
        \AttributeTok{covs.formula              =} \SpecialCharTok{\textasciitilde{}} \FunctionTok{I}\NormalTok{(}\FunctionTok{lag}\NormalTok{(tradewb, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{)) }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(}\FunctionTok{lag}\NormalTok{(y, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{)),}
        \AttributeTok{qoi                       =}\NormalTok{ config}\SpecialCharTok{$}\NormalTok{qoi}
\NormalTok{    )}
\NormalTok{\})}

\CommentTok{\# Step 3: Get covariate balance and plot}
\NormalTok{plots }\OtherTok{\textless{}{-}} \FunctionTok{mapply}\NormalTok{(}\ControlFlowTok{function}\NormalTok{(result, config) \{}
\NormalTok{    df }\OtherTok{\textless{}{-}} \FunctionTok{get\_covariate\_balance}\NormalTok{(}
        \ControlFlowTok{if}\NormalTok{ (config}\SpecialCharTok{$}\NormalTok{qoi }\SpecialCharTok{==} \StringTok{"att"}\NormalTok{)}
\NormalTok{            result}\SpecialCharTok{$}\NormalTok{att}
        \ControlFlowTok{else}
\NormalTok{            result}\SpecialCharTok{$}\NormalTok{art,}
        \AttributeTok{data =}\NormalTok{ dem,}
        \AttributeTok{covariates =} \FunctionTok{c}\NormalTok{(}\StringTok{"tradewb"}\NormalTok{, }\StringTok{"y"}\NormalTok{),}
        \AttributeTok{plot =}\NormalTok{ F}
\NormalTok{    )}
\NormalTok{    causalverse}\SpecialCharTok{::}\FunctionTok{plot\_covariate\_balance\_pretrend}\NormalTok{(df, }\AttributeTok{main =} \StringTok{""}\NormalTok{, }\AttributeTok{show\_legend =}\NormalTok{ F)}
\NormalTok{\}, results, configurations, }\AttributeTok{SIMPLIFY =} \ConstantTok{FALSE}\NormalTok{)}

\CommentTok{\# Set names for plots}
\FunctionTok{names}\NormalTok{(plots) }\OtherTok{\textless{}{-}} \FunctionTok{sapply}\NormalTok{(configurations, }\ControlFlowTok{function}\NormalTok{(config) \{}
    \FunctionTok{paste}\NormalTok{(config}\SpecialCharTok{$}\NormalTok{qoi, config}\SpecialCharTok{$}\NormalTok{refinement.method, }\AttributeTok{sep =} \StringTok{"."}\NormalTok{)}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

To export

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(gridExtra)}
\FunctionTok{library}\NormalTok{(grid)}

\CommentTok{\# Column and row labels}
\NormalTok{col\_labels }\OtherTok{\textless{}{-}}
    \FunctionTok{c}\NormalTok{(}\StringTok{"None"}\NormalTok{,}
      \StringTok{"Mahalanobis"}\NormalTok{,}
      \StringTok{"Propensity Score Matching"}\NormalTok{,}
      \StringTok{"Propensity Score Weighting"}\NormalTok{)}
\NormalTok{row\_labels }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"ATT"}\NormalTok{, }\StringTok{"ART"}\NormalTok{)}

\CommentTok{\# Specify your desired fontsize for labels}
\NormalTok{minor.axes.fontsize }\OtherTok{\textless{}{-}} \DecValTok{16}
\NormalTok{major.axes.fontsize }\OtherTok{\textless{}{-}} \DecValTok{20}

\FunctionTok{png}\NormalTok{(}\FunctionTok{file.path}\NormalTok{(}\FunctionTok{getwd}\NormalTok{(), }\StringTok{"images"}\NormalTok{, }\StringTok{"p\_covariate\_balance.png"}\NormalTok{), }\AttributeTok{width=}\DecValTok{1200}\NormalTok{, }\AttributeTok{height=}\DecValTok{1000}\NormalTok{)}

\CommentTok{\# Create a list{-}of{-}lists, where each inner list represents a row}
\NormalTok{grid\_list }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
    \FunctionTok{list}\NormalTok{(}
        \FunctionTok{nullGrob}\NormalTok{(),}
        \FunctionTok{textGrob}\NormalTok{(col\_labels[}\DecValTok{1}\NormalTok{], }\AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ minor.axes.fontsize)),}
        \FunctionTok{textGrob}\NormalTok{(col\_labels[}\DecValTok{2}\NormalTok{], }\AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ minor.axes.fontsize)),}
        \FunctionTok{textGrob}\NormalTok{(col\_labels[}\DecValTok{3}\NormalTok{], }\AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ minor.axes.fontsize)),}
        \FunctionTok{textGrob}\NormalTok{(col\_labels[}\DecValTok{4}\NormalTok{], }\AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ minor.axes.fontsize))}
\NormalTok{    ),}
    
    \FunctionTok{list}\NormalTok{(}
        \FunctionTok{textGrob}\NormalTok{(}
\NormalTok{            row\_labels[}\DecValTok{1}\NormalTok{],}
            \AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ minor.axes.fontsize),}
            \AttributeTok{rot =} \DecValTok{90}
\NormalTok{        ),}
\NormalTok{        plots}\SpecialCharTok{$}\NormalTok{att.none,}
\NormalTok{        plots}\SpecialCharTok{$}\NormalTok{att.mahalanobis,}
\NormalTok{        plots}\SpecialCharTok{$}\NormalTok{att.ps.match,}
\NormalTok{        plots}\SpecialCharTok{$}\NormalTok{att.ps.weight}
\NormalTok{    ),}
    
    \FunctionTok{list}\NormalTok{(}
        \FunctionTok{textGrob}\NormalTok{(}
\NormalTok{            row\_labels[}\DecValTok{2}\NormalTok{],}
            \AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ minor.axes.fontsize),}
            \AttributeTok{rot =} \DecValTok{90}
\NormalTok{        ),}
\NormalTok{        plots}\SpecialCharTok{$}\NormalTok{art.none,}
\NormalTok{        plots}\SpecialCharTok{$}\NormalTok{art.mahalanobis,}
\NormalTok{        plots}\SpecialCharTok{$}\NormalTok{art.ps.match,}
\NormalTok{        plots}\SpecialCharTok{$}\NormalTok{art.ps.weight}
\NormalTok{    )}
\NormalTok{)}

\CommentTok{\# "Flatten" the list{-}of{-}lists into a single list of grobs}
\NormalTok{grobs }\OtherTok{\textless{}{-}} \FunctionTok{do.call}\NormalTok{(c, grid\_list)}

\CommentTok{\# Arrange your plots with text labels}
\FunctionTok{grid.arrange}\NormalTok{(}
    \AttributeTok{grobs   =}\NormalTok{ grobs,}
    \AttributeTok{ncol    =} \DecValTok{5}\NormalTok{,}
    \AttributeTok{nrow    =} \DecValTok{3}\NormalTok{,}
    \AttributeTok{widths  =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.225}\NormalTok{, }\FloatTok{0.225}\NormalTok{, }\FloatTok{0.225}\NormalTok{, }\FloatTok{0.225}\NormalTok{),}
    \AttributeTok{heights =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.45}\NormalTok{, }\FloatTok{0.45}\NormalTok{)}
\NormalTok{)}

\CommentTok{\# Add main x and y axis titles}
\FunctionTok{grid.text}\NormalTok{(}
    \StringTok{"Refinement Methods"}\NormalTok{,}
    \AttributeTok{x  =} \FloatTok{0.5}\NormalTok{,}
    \AttributeTok{y  =} \FloatTok{0.01}\NormalTok{,}
    \AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ major.axes.fontsize)}
\NormalTok{)}
\FunctionTok{grid.text}\NormalTok{(}
    \StringTok{"Quantities of Interest"}\NormalTok{,}
    \AttributeTok{x   =} \FloatTok{0.02}\NormalTok{,}
    \AttributeTok{y   =} \FloatTok{0.5}\NormalTok{,}
    \AttributeTok{rot =} \DecValTok{90}\NormalTok{,}
    \AttributeTok{gp  =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ major.axes.fontsize)}
\NormalTok{)}

\FunctionTok{dev.off}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(knitr)}
\FunctionTok{include\_graphics}\NormalTok{(}\FunctionTok{file.path}\NormalTok{(}\FunctionTok{getwd}\NormalTok{(), }\StringTok{"images"}\NormalTok{, }\StringTok{"p\_covariate\_balance.png"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{images/p_covariate_balance} \end{center}

Note: Each graph displays the standardized mean difference, as outlined in Equation \eqref{eq:aggbalance}, plotted on the vertical axis across a pre-treatment duration of four years represented on the horizontal axis. The leftmost column illustrates the balance prior to refinement, while the subsequent three columns depict the covariate balance post the application of distinct refinement techniques. Each individual line signifies the balance of a specific variable during the pre-treatment phase.The red line is tradewb and blue line is the lagged outcome variable.

In Figure \ref{fig:balancepretreat}, we observe a marked improvement in covariate balance due to the implemented matching procedures during the pre-treatment period. Our analysis prioritizes methods that adjust for time-varying covariates over a span of four years preceding the treatment initiation. The two rows delineate the standardized mean balance for both treatment modalities, with individual lines representing the balance for each covariate.

Across all scenarios, the refinement attributed to matched sets significantly enhances balance. Notably, using propensity score weighting considerably mitigates imbalances in confounders. While some degree of imbalance remains evident in the Mahalanobis distance and propensity score matching techniques, the standardized mean difference for the lagged outcome remains stable throughout the pre-treatment phase. This consistency lends credence to the validity of the proposed DiD estimator.

\textbf{Estimation Results}

We now detail the estimated ATTs derived from the matching techniques. Figure below offers visual representations of the impacts of treatment initiation (upper panel) and treatment reversal (lower panel) on the outcome variable for a duration of 5 years post-transition, specifically, (F = 0, 1, \ldots, 4). Across the five methods (columns), it becomes evident that the point estimates of effects associated with treatment initiation consistently approximate zero over the 5-year window. In contrast, the estimated outcomes of treatment reversal are notably negative and maintain statistical significance through all refinement techniques during the initial year of transition and the 1 to 4 years that follow, provided treatment reversal is permissible. These effects are notably pronounced, pointing to an estimated reduction of roughly X\% in the outcome variable.

Collectively, these findings indicate that the transition into the treated state from its absence doesn't invariably lead to a heightened outcome. Instead, the transition from the treated state back to its absence exerts a considerable negative effect on the outcome variable in both the short and intermediate terms. Hence, the positive effect of the treatment (if we were to use traditional DiD) is actually driven by the negative effect of treatment reversal.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# sequential}
\CommentTok{\# Step 1: Apply PanelEstimate function}

\CommentTok{\# Initialize an empty list to store results}
\NormalTok{res\_est }\OtherTok{\textless{}{-}} \FunctionTok{vector}\NormalTok{(}\StringTok{"list"}\NormalTok{, }\FunctionTok{length}\NormalTok{(res\_pm))}

\CommentTok{\# Iterate over each element in res\_pm}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(res\_pm)) \{}
\NormalTok{  res\_est[[i]] }\OtherTok{\textless{}{-}} \FunctionTok{PanelEstimate}\NormalTok{(}
\NormalTok{    res\_pm[[i]],}
    \AttributeTok{data =}\NormalTok{ dem,}
    \AttributeTok{se.method =} \StringTok{"bootstrap"}\NormalTok{,}
    \AttributeTok{number.iterations =} \DecValTok{1000}\NormalTok{,}
    \AttributeTok{confidence.level =}\NormalTok{ .}\DecValTok{95}
\NormalTok{  )}
  \CommentTok{\# Transfer the name of the current element to the res\_est list}
  \FunctionTok{names}\NormalTok{(res\_est)[i] }\OtherTok{\textless{}{-}} \FunctionTok{names}\NormalTok{(res\_pm)[i]}
\NormalTok{\}}

\CommentTok{\# Step 2: Apply plot\_PanelEstimate function}

\CommentTok{\# Initialize an empty list to store plot results}
\NormalTok{res\_est\_plot }\OtherTok{\textless{}{-}} \FunctionTok{vector}\NormalTok{(}\StringTok{"list"}\NormalTok{, }\FunctionTok{length}\NormalTok{(res\_est))}

\CommentTok{\# Iterate over each element in res\_est}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(res\_est)) \{}
\NormalTok{    res\_est\_plot[[i]] }\OtherTok{\textless{}{-}}
        \FunctionTok{plot\_PanelEstimate}\NormalTok{(res\_est[[i]],}
                           \AttributeTok{main =} \StringTok{""}\NormalTok{,}
                           \AttributeTok{theme\_use =}\NormalTok{ causalverse}\SpecialCharTok{::}\FunctionTok{ama\_theme}\NormalTok{(}\AttributeTok{base\_size =} \DecValTok{14}\NormalTok{))}
    \CommentTok{\# Transfer the name of the current element to the res\_est\_plot list}
    \FunctionTok{names}\NormalTok{(res\_est\_plot)[i] }\OtherTok{\textless{}{-}} \FunctionTok{names}\NormalTok{(res\_est)[i]}
\NormalTok{\}}

\CommentTok{\# check results}
\CommentTok{\# res\_est\_plot$mahalanobis.1lag.5m}


\CommentTok{\# Step 1: Apply PanelEstimate function for res\_pm\_rev}

\CommentTok{\# Initialize an empty list to store results}
\NormalTok{res\_est\_rev }\OtherTok{\textless{}{-}} \FunctionTok{vector}\NormalTok{(}\StringTok{"list"}\NormalTok{, }\FunctionTok{length}\NormalTok{(res\_pm\_rev))}

\CommentTok{\# Iterate over each element in res\_pm\_rev}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(res\_pm\_rev)) \{}
\NormalTok{  res\_est\_rev[[i]] }\OtherTok{\textless{}{-}} \FunctionTok{PanelEstimate}\NormalTok{(}
\NormalTok{    res\_pm\_rev[[i]],}
    \AttributeTok{data =}\NormalTok{ dem,}
    \AttributeTok{se.method =} \StringTok{"bootstrap"}\NormalTok{,}
    \AttributeTok{number.iterations =} \DecValTok{1000}\NormalTok{,}
    \AttributeTok{confidence.level =}\NormalTok{ .}\DecValTok{95}
\NormalTok{  )}
  \CommentTok{\# Transfer the name of the current element to the res\_est\_rev list}
  \FunctionTok{names}\NormalTok{(res\_est\_rev)[i] }\OtherTok{\textless{}{-}} \FunctionTok{names}\NormalTok{(res\_pm\_rev)[i]}
\NormalTok{\}}

\CommentTok{\# Step 2: Apply plot\_PanelEstimate function for res\_est\_rev}

\CommentTok{\# Initialize an empty list to store plot results}
\NormalTok{res\_est\_plot\_rev }\OtherTok{\textless{}{-}} \FunctionTok{vector}\NormalTok{(}\StringTok{"list"}\NormalTok{, }\FunctionTok{length}\NormalTok{(res\_est\_rev))}

\CommentTok{\# Iterate over each element in res\_est\_rev}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(res\_est\_rev)) \{}
\NormalTok{    res\_est\_plot\_rev[[i]] }\OtherTok{\textless{}{-}}
        \FunctionTok{plot\_PanelEstimate}\NormalTok{(res\_est\_rev[[i]],}
                           \AttributeTok{main =} \StringTok{""}\NormalTok{,}
                           \AttributeTok{theme\_use =}\NormalTok{ causalverse}\SpecialCharTok{::}\FunctionTok{ama\_theme}\NormalTok{(}\AttributeTok{base\_size =} \DecValTok{14}\NormalTok{))}
  \CommentTok{\# Transfer the name of the current element to the res\_est\_plot\_rev list}
  \FunctionTok{names}\NormalTok{(res\_est\_plot\_rev)[i] }\OtherTok{\textless{}{-}} \FunctionTok{names}\NormalTok{(res\_est\_rev)[i]}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# parallel}
\FunctionTok{library}\NormalTok{(doParallel)}
\FunctionTok{library}\NormalTok{(foreach)}

\CommentTok{\# Detect the number of cores to use for parallel processing}
\NormalTok{num\_cores }\OtherTok{\textless{}{-}} \DecValTok{4}

\CommentTok{\# Register the parallel backend}
\NormalTok{cl }\OtherTok{\textless{}{-}} \FunctionTok{makeCluster}\NormalTok{(num\_cores)}
\FunctionTok{registerDoParallel}\NormalTok{(cl)}

\CommentTok{\# Step 1: Apply PanelEstimate function in parallel}
\NormalTok{res\_est }\OtherTok{\textless{}{-}}
    \FunctionTok{foreach}\NormalTok{(}\AttributeTok{i =} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(res\_pm), }\AttributeTok{.packages =} \StringTok{"PanelMatch"}\NormalTok{) }\SpecialCharTok{\%dopar\%}\NormalTok{ \{}
        \FunctionTok{PanelEstimate}\NormalTok{(}
\NormalTok{            res\_pm[[i]],}
            \AttributeTok{data =}\NormalTok{ dem,}
            \AttributeTok{se.method =} \StringTok{"bootstrap"}\NormalTok{,}
            \AttributeTok{number.iterations =} \DecValTok{1000}\NormalTok{,}
            \AttributeTok{confidence.level =}\NormalTok{ .}\DecValTok{95}
\NormalTok{        )}
\NormalTok{    \}}

\CommentTok{\# Transfer names from res\_pm to res\_est}
\FunctionTok{names}\NormalTok{(res\_est) }\OtherTok{\textless{}{-}} \FunctionTok{names}\NormalTok{(res\_pm)}

\CommentTok{\# Step 2: Apply plot\_PanelEstimate function in parallel}
\NormalTok{res\_est\_plot }\OtherTok{\textless{}{-}}
    \FunctionTok{foreach}\NormalTok{(}
        \AttributeTok{i =} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(res\_est),}
        \AttributeTok{.packages =} \FunctionTok{c}\NormalTok{(}\StringTok{"PanelMatch"}\NormalTok{, }\StringTok{"causalverse"}\NormalTok{, }\StringTok{"ggplot2"}\NormalTok{)}
\NormalTok{    ) }\SpecialCharTok{\%dopar\%}\NormalTok{ \{}
        \FunctionTok{plot\_PanelEstimate}\NormalTok{(res\_est[[i]],}
                           \AttributeTok{main =} \StringTok{""}\NormalTok{,}
                           \AttributeTok{theme\_use =}\NormalTok{ causalverse}\SpecialCharTok{::}\FunctionTok{ama\_theme}\NormalTok{(}\AttributeTok{base\_size =} \DecValTok{10}\NormalTok{))}
\NormalTok{    \}}

\CommentTok{\# Transfer names from res\_est to res\_est\_plot}
\FunctionTok{names}\NormalTok{(res\_est\_plot) }\OtherTok{\textless{}{-}} \FunctionTok{names}\NormalTok{(res\_est)}



\CommentTok{\# Step 1: Apply PanelEstimate function for res\_pm\_rev in parallel}
\NormalTok{res\_est\_rev }\OtherTok{\textless{}{-}}
    \FunctionTok{foreach}\NormalTok{(}\AttributeTok{i =} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(res\_pm\_rev), }\AttributeTok{.packages =} \StringTok{"PanelMatch"}\NormalTok{) }\SpecialCharTok{\%dopar\%}\NormalTok{ \{}
        \FunctionTok{PanelEstimate}\NormalTok{(}
\NormalTok{            res\_pm\_rev[[i]],}
            \AttributeTok{data =}\NormalTok{ dem,}
            \AttributeTok{se.method =} \StringTok{"bootstrap"}\NormalTok{,}
            \AttributeTok{number.iterations =} \DecValTok{1000}\NormalTok{,}
            \AttributeTok{confidence.level =}\NormalTok{ .}\DecValTok{95}
\NormalTok{        )}
\NormalTok{    \}}

\CommentTok{\# Transfer names from res\_pm\_rev to res\_est\_rev}
\FunctionTok{names}\NormalTok{(res\_est\_rev) }\OtherTok{\textless{}{-}} \FunctionTok{names}\NormalTok{(res\_pm\_rev)}

\CommentTok{\# Step 2: Apply plot\_PanelEstimate function for res\_est\_rev in parallel}
\NormalTok{res\_est\_plot\_rev }\OtherTok{\textless{}{-}}
    \FunctionTok{foreach}\NormalTok{(}
        \AttributeTok{i =} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(res\_est\_rev),}
        \AttributeTok{.packages =} \FunctionTok{c}\NormalTok{(}\StringTok{"PanelMatch"}\NormalTok{, }\StringTok{"causalverse"}\NormalTok{, }\StringTok{"ggplot2"}\NormalTok{)}
\NormalTok{    ) }\SpecialCharTok{\%dopar\%}\NormalTok{ \{}
        \FunctionTok{plot\_PanelEstimate}\NormalTok{(res\_est\_rev[[i]],}
                           \AttributeTok{main =} \StringTok{""}\NormalTok{,}
                           \AttributeTok{theme\_use =}\NormalTok{ causalverse}\SpecialCharTok{::}\FunctionTok{ama\_theme}\NormalTok{(}\AttributeTok{base\_size =} \DecValTok{10}\NormalTok{))}
\NormalTok{    \}}

\CommentTok{\# Transfer names from res\_est\_rev to res\_est\_plot\_rev}
\FunctionTok{names}\NormalTok{(res\_est\_plot\_rev) }\OtherTok{\textless{}{-}} \FunctionTok{names}\NormalTok{(res\_est\_rev)}

\CommentTok{\# Stop the cluster}
\FunctionTok{stopCluster}\NormalTok{(cl)}
\end{Highlighting}
\end{Shaded}

To export

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(gridExtra)}
\FunctionTok{library}\NormalTok{(grid)}

\CommentTok{\# Column and row labels}
\NormalTok{col\_labels }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Mahalanobis 5m"}\NormalTok{, }
                \StringTok{"Mahalanobis 10m"}\NormalTok{, }
                \StringTok{"PS Matching 5m"}\NormalTok{, }
                \StringTok{"PS Matching 10m"}\NormalTok{, }
                \StringTok{"PS Weighting 5m"}\NormalTok{)}

\NormalTok{row\_labels }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"ATT"}\NormalTok{, }\StringTok{"ART"}\NormalTok{)}

\CommentTok{\# Specify your desired fontsize for labels}
\NormalTok{minor.axes.fontsize }\OtherTok{\textless{}{-}} \DecValTok{16}
\NormalTok{major.axes.fontsize }\OtherTok{\textless{}{-}} \DecValTok{20}

\FunctionTok{png}\NormalTok{(}\FunctionTok{file.path}\NormalTok{(}\FunctionTok{getwd}\NormalTok{(), }\StringTok{"images"}\NormalTok{, }\StringTok{"p\_did\_est\_in\_n\_out.png"}\NormalTok{), }\AttributeTok{width=}\DecValTok{1200}\NormalTok{, }\AttributeTok{height=}\DecValTok{1000}\NormalTok{)}

\CommentTok{\# Create a list{-}of{-}lists, where each inner list represents a row}
\NormalTok{grid\_list }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
  \FunctionTok{list}\NormalTok{(}
    \FunctionTok{nullGrob}\NormalTok{(),}
    \FunctionTok{textGrob}\NormalTok{(col\_labels[}\DecValTok{1}\NormalTok{], }\AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ minor.axes.fontsize)),}
    \FunctionTok{textGrob}\NormalTok{(col\_labels[}\DecValTok{2}\NormalTok{], }\AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ minor.axes.fontsize)),}
    \FunctionTok{textGrob}\NormalTok{(col\_labels[}\DecValTok{3}\NormalTok{], }\AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ minor.axes.fontsize)),}
    \FunctionTok{textGrob}\NormalTok{(col\_labels[}\DecValTok{4}\NormalTok{], }\AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ minor.axes.fontsize)),}
    \FunctionTok{textGrob}\NormalTok{(col\_labels[}\DecValTok{5}\NormalTok{], }\AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ minor.axes.fontsize))}
\NormalTok{  ),}
  
  \FunctionTok{list}\NormalTok{(}
    \FunctionTok{textGrob}\NormalTok{(row\_labels[}\DecValTok{1}\NormalTok{], }\AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ minor.axes.fontsize), }\AttributeTok{rot =} \DecValTok{90}\NormalTok{),}
\NormalTok{    res\_est\_plot}\SpecialCharTok{$}\NormalTok{mahalanobis}\FloatTok{.1}\NormalTok{lag}\FloatTok{.5}\NormalTok{m,}
\NormalTok{    res\_est\_plot}\SpecialCharTok{$}\NormalTok{mahalanobis}\FloatTok{.1}\NormalTok{lag}\FloatTok{.10}\NormalTok{m,}
\NormalTok{    res\_est\_plot}\SpecialCharTok{$}\NormalTok{ps.match}\FloatTok{.1}\NormalTok{lag}\FloatTok{.5}\NormalTok{m,}
\NormalTok{    res\_est\_plot}\SpecialCharTok{$}\NormalTok{ps.match}\FloatTok{.1}\NormalTok{lag}\FloatTok{.10}\NormalTok{m,}
\NormalTok{    res\_est\_plot}\SpecialCharTok{$}\NormalTok{ps.weight}\FloatTok{.1}\NormalTok{lag}\FloatTok{.5}\NormalTok{m}
\NormalTok{  ),}
  
  \FunctionTok{list}\NormalTok{(}
    \FunctionTok{textGrob}\NormalTok{(row\_labels[}\DecValTok{2}\NormalTok{], }\AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ minor.axes.fontsize), }\AttributeTok{rot =} \DecValTok{90}\NormalTok{),}
\NormalTok{    res\_est\_plot\_rev}\SpecialCharTok{$}\NormalTok{mahalanobis}\FloatTok{.1}\NormalTok{lag}\FloatTok{.5}\NormalTok{m,}
\NormalTok{    res\_est\_plot\_rev}\SpecialCharTok{$}\NormalTok{mahalanobis}\FloatTok{.1}\NormalTok{lag}\FloatTok{.10}\NormalTok{m,}
\NormalTok{    res\_est\_plot\_rev}\SpecialCharTok{$}\NormalTok{ps.match}\FloatTok{.1}\NormalTok{lag}\FloatTok{.5}\NormalTok{m,}
\NormalTok{    res\_est\_plot\_rev}\SpecialCharTok{$}\NormalTok{ps.match}\FloatTok{.1}\NormalTok{lag}\FloatTok{.10}\NormalTok{m,}
\NormalTok{    res\_est\_plot\_rev}\SpecialCharTok{$}\NormalTok{ps.weight}\FloatTok{.1}\NormalTok{lag}\FloatTok{.5}\NormalTok{m}
\NormalTok{  )}
\NormalTok{)}

\CommentTok{\# "Flatten" the list{-}of{-}lists into a single list of grobs}
\NormalTok{grobs }\OtherTok{\textless{}{-}} \FunctionTok{do.call}\NormalTok{(c, grid\_list)}

\CommentTok{\# Arrange your plots with text labels}
\FunctionTok{grid.arrange}\NormalTok{(}
  \AttributeTok{grobs   =}\NormalTok{ grobs,}
  \AttributeTok{ncol    =} \DecValTok{6}\NormalTok{,}
  \AttributeTok{nrow    =} \DecValTok{3}\NormalTok{,}
  \AttributeTok{widths  =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.18}\NormalTok{, }\FloatTok{0.18}\NormalTok{, }\FloatTok{0.18}\NormalTok{, }\FloatTok{0.18}\NormalTok{, }\FloatTok{0.18}\NormalTok{),}
  \AttributeTok{heights =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.45}\NormalTok{, }\FloatTok{0.45}\NormalTok{)}
\NormalTok{)}

\CommentTok{\# Add main x and y axis titles}
\FunctionTok{grid.text}\NormalTok{(}
  \StringTok{"Methods"}\NormalTok{,}
  \AttributeTok{x  =} \FloatTok{0.5}\NormalTok{,}
  \AttributeTok{y  =} \FloatTok{0.02}\NormalTok{,}
  \AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ major.axes.fontsize)}
\NormalTok{)}
\FunctionTok{grid.text}\NormalTok{(}
  \StringTok{""}\NormalTok{,}
  \AttributeTok{x   =} \FloatTok{0.02}\NormalTok{,}
  \AttributeTok{y   =} \FloatTok{0.5}\NormalTok{,}
  \AttributeTok{rot =} \DecValTok{90}\NormalTok{,}
  \AttributeTok{gp  =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ major.axes.fontsize)}
\NormalTok{)}

\FunctionTok{dev.off}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(knitr)}
\FunctionTok{include\_graphics}\NormalTok{(}\FunctionTok{file.path}\NormalTok{(}\FunctionTok{getwd}\NormalTok{(), }\StringTok{"images"}\NormalTok{, }\StringTok{"p\_did\_est\_in\_n\_out.png"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{counterfactual-estimators}{%
\subsubsection{Counterfactual Estimators}\label{counterfactual-estimators}}

\begin{itemize}
\tightlist
\item
  Also known as \textbf{imputation approach} \citep{liu2022practical}
\item
  This class of estimator consider observation treatment as missing data. Models are built using data from the control units to impute conterfactuals for the treated observations.
\item
  It's called counterfactual estimators because they predict outcomes as if the treated observations had not received the treatment.
\item
  Advantages:

  \begin{itemize}
  \tightlist
  \item
    Avoids negative weights and biases by not using treated observations for modeling and applying uniform weights.
  \item
    Supports various models, including those that may relax strict exogeneity assumptions.
  \end{itemize}
\item
  Methods including

  \begin{itemize}
  \tightlist
  \item
    Fixed-effects conterfactual estimator (FEct) (DiD is a special case):

    \begin{itemize}
    \tightlist
    \item
      Based on the \protect\hyperlink{two-way-fixed-effects}{Two-way Fixed-effects}, where assumes linear additive functional form of unobservables based on unit and time FEs. But FEct fixes the improper weighting of TWFE by comparing within each matched pair (where each pair is the treated observation and its predicted counterfactual that is the weighted sum of all untreated observations).
    \end{itemize}
  \item
    Interactive Fixed Effects conterfactual estimator (IFEct) \citep[\citet{xu2017generalized}]{gobillon2016regional}:

    \begin{itemize}
    \tightlist
    \item
      When we suspect unobserved time-varying confounder, FEct fails. Instead, IFEct uses the factor-augmented models to relax the strict exogeneity assumption where the effects of unobservables can be decomposed to unit FE + time FE + unit x time FE.
    \item
      Generalized Synthetic Controls are a subset of IFEct when treatments don't revert.
    \end{itemize}
  \item
    \protect\hyperlink{matrix-completion}{Matrix completion} (MC) \citep{athey2021matrix}:

    \begin{itemize}
    \tightlist
    \item
      Generalization of factor-augmented models. Different from IFEct which uses hard impute, MC uses soft impute to regularize the singular values when decomposing the residual matrix.
    \item
      Only when latent factors (of unobservables) are strong and sparse, IFEct outperforms MC.
    \end{itemize}
  \item
    {[}Synthetic Controls{]} (case studies)
  \end{itemize}
\end{itemize}

\textbf{Identifying Assumptions}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Function Form}: Additive separability of observables, unobservables, and idiosyncratic error term.

  \begin{itemize}
  \tightlist
  \item
    Hence, these models are scale dependent \citep{athey2006identification} (e.g., log-transform outcome can invadiate this assumption).
  \end{itemize}
\item
  \textbf{Strict Exogeneity}: Conditional on observables and unobservables, potential outcomes are independent of treatment assignment (i.e., baseline quasi-randomization)

  \begin{itemize}
  \tightlist
  \item
    In DiD, where unobservables = unit + time FEs, this assumption is the parallel trends assumption
  \end{itemize}
\item
  \textbf{Low-dimensional Decomposition (Feasibility Assumption)}: Unobservable effects can be decomposed in low-dimension.

  \begin{itemize}
  \tightlist
  \item
    For the case that \(U_{it} = f_t \times \lambda_i\) where \(f_t\) = common time trend (time FE), and \(\lambda_i\) = unit heterogeneity (unit FE). If \(U_{it} = f_t \times \lambda_i\) , DiD can satisfy this assumption. But this assumption is weaker than that of DID, and allows us to control for unobservables based on data.
  \end{itemize}
\end{enumerate}

\textbf{Estimation Procedure}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Using all control observations, estimate the functions of both observable and unobservable variables (relying on Assumptions 1 and 3).
\item
  Predict the counterfactual outcomes for each treated unit using the obtained functions.
\item
  Calculate the difference in treatment effect for each treated individual.
\item
  By averaging over all treated individuals, you can obtain the Average Treatment Effect on the Treated (ATT).
\end{enumerate}

Notes:

\begin{itemize}
\tightlist
\item
  Use jackknife when number of treated units is small \citep[p.166]{liu2022practical}.
\end{itemize}

\hypertarget{imputation-method}{%
\paragraph{Imputation Method}\label{imputation-method}}

\citet{liu2022practical} can also account for treatment reversals and heterogeneous treatment effects.

Other imputation estimators include

\begin{itemize}
\item
  {[}@gardner2022two and @borusyak2021revisiting{]}
\item
  \citet{RePEc:arx:papers:2301.11358}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(fect)}

\NormalTok{PanelMatch}\SpecialCharTok{::}\NormalTok{dem}

\NormalTok{model.fect }\OtherTok{\textless{}{-}}
    \FunctionTok{fect}\NormalTok{(}
        \AttributeTok{Y =} \StringTok{"y"}\NormalTok{,}
        \AttributeTok{D =} \StringTok{"dem"}\NormalTok{,}
        \AttributeTok{X =} \StringTok{"tradewb"}\NormalTok{,}
        \AttributeTok{data =} \FunctionTok{na.omit}\NormalTok{(PanelMatch}\SpecialCharTok{::}\NormalTok{dem),}
        \AttributeTok{method =} \StringTok{"fe"}\NormalTok{,}
        \AttributeTok{index =} \FunctionTok{c}\NormalTok{(}\StringTok{"wbcode2"}\NormalTok{, }\StringTok{"year"}\NormalTok{),}
        \AttributeTok{se =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{parallel =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{seed =} \DecValTok{1234}\NormalTok{,}
        \CommentTok{\# twfe}
        \AttributeTok{force =} \StringTok{"two{-}way"}
\NormalTok{    )}
\FunctionTok{print}\NormalTok{(model.fect}\SpecialCharTok{$}\NormalTok{est.avg)}

\FunctionTok{plot}\NormalTok{(model.fect)}

\FunctionTok{plot}\NormalTok{(model.fect, }\AttributeTok{stats =} \StringTok{"F.p"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

F-test \(H_0\): residual averages in the pre-treatment periods = 0

To see treatment reversal effects

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(model.fect, }\AttributeTok{stats =} \StringTok{"F.p"}\NormalTok{, }\AttributeTok{type =} \StringTok{\textquotesingle{}exit\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{placebo-test}{%
\paragraph{Placebo Test}\label{placebo-test}}

By selecting a part of the data and excluding observations within a specified range to improve the model fitting, we then evaluate whether the estimated Average Treatment Effect (ATT) within this range significantly differs from zero. This approach helps us analyze the periods before treatment.

If this test fails, either the functional form or strict exogeneity assumption is problematic.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{out.fect.p }\OtherTok{\textless{}{-}}
    \FunctionTok{fect}\NormalTok{(}
        \AttributeTok{Y =} \StringTok{"y"}\NormalTok{,}
        \AttributeTok{D =} \StringTok{"dem"}\NormalTok{,}
        \AttributeTok{X =} \StringTok{"tradewb"}\NormalTok{,}
        \AttributeTok{data =} \FunctionTok{na.omit}\NormalTok{(PanelMatch}\SpecialCharTok{::}\NormalTok{dem),}
        \AttributeTok{method =} \StringTok{"fe"}\NormalTok{,}
        \AttributeTok{index =} \FunctionTok{c}\NormalTok{(}\StringTok{"wbcode2"}\NormalTok{, }\StringTok{"year"}\NormalTok{),}
        \AttributeTok{se =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{placeboTest =} \ConstantTok{TRUE}\NormalTok{,}
        \CommentTok{\# using 3 periods}
        \AttributeTok{placebo.period =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{    )}
\FunctionTok{plot}\NormalTok{(out.fect.p, }\AttributeTok{proportion =} \FloatTok{0.1}\NormalTok{, }\AttributeTok{stats =} \StringTok{"placebo.p"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{no-carryover-effects-test}{%
\paragraph{(No) Carryover Effects Test}\label{no-carryover-effects-test}}

The placebo test can be adapted to assess carryover effects by masking several post-treatment periods instead of pre-treatment ones. If no carryover effects are present, the average prediction error should approximate zero. For the carryover test, set \texttt{carryoverTest\ =\ TRUE}. Specify a post-treatment period range in carryover.period to exclude observations for model fitting, then evaluate if the estimated ATT significantly deviates from zero.

Even if we have carryover effects, in most cases of the staggered adoption setting, researchers are interested in the cumulative effects, or aggregated treatment effects, so it's okay.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{out.fect.c }\OtherTok{\textless{}{-}}
    \FunctionTok{fect}\NormalTok{(}
        \AttributeTok{Y =} \StringTok{"y"}\NormalTok{,}
        \AttributeTok{D =} \StringTok{"dem"}\NormalTok{,}
        \AttributeTok{X =} \StringTok{"tradewb"}\NormalTok{,}
        \AttributeTok{data =} \FunctionTok{na.omit}\NormalTok{(PanelMatch}\SpecialCharTok{::}\NormalTok{dem),}
        \AttributeTok{method =} \StringTok{"fe"}\NormalTok{,}
        \AttributeTok{index =} \FunctionTok{c}\NormalTok{(}\StringTok{"wbcode2"}\NormalTok{, }\StringTok{"year"}\NormalTok{),}
        \AttributeTok{se =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{carryoverTest =} \ConstantTok{TRUE}\NormalTok{,}
        \CommentTok{\# how many periods of carryover}
        \AttributeTok{carryover.period =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{    )}
\FunctionTok{plot}\NormalTok{(out.fect.c,  }\AttributeTok{stats =} \StringTok{"carryover.p"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We have evidence of carryover effects.

\hypertarget{matrix-completion-1}{%
\subsubsection{Matrix Completion}\label{matrix-completion-1}}

Applications in marketing:

\begin{itemize}
\tightlist
\item
  \citet{bronnenberg2020consumer}
\end{itemize}

To estimate average causal effects in panel data with units exposed to treatment intermittently, two literatures are pivotal:

\begin{itemize}
\item
  \textbf{Unconfoundedness} \citep{imbens2015causal}: Imputes missing potential control outcomes for treated units using observed outcomes from similar control units in previous periods.
\item
  \textbf{Synthetic Control} \citep{abadie2010synthetic}: Imputes missing control outcomes for treated units using weighted averages from control units, matching lagged outcomes between treated and control units.
\end{itemize}

Both exploit missing potential outcomes under different assumptions:

\begin{itemize}
\item
  Unconfoundedness assumes time patterns are stable across units.
\item
  Synthetic control assumes unit patterns are stable over time.
\end{itemize}

Once regularization is applied, both approaches are applicable in similar settings \citep{athey2021matrix}.

\textbf{Matrix Completion} method, nesting both, is based on matrix factorization, focusing on imputing missing matrix elements assuming:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Complete matrix = low-rank matrix + noise.
\item
  Missingness is completely at random.
\end{enumerate}

It's distinguished by not imposing factorization restrictions but utilizing regularization to define the estimator, particularly effective with the nuclear norm as a regularizer for complex missing patterns \citep{athey2021matrix}.

Contributions of \citet{athey2021matrix} matrix completion include:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Recognizing structured missing patterns allowing time correlation, enabling staggered adoption.
\item
  Modifying estimators for unregularized unit and time fixed effects.
\item
  Performing well across various \(T\) and \(N\) sizes, unlike unconfoundedness and synthetic control, which falter when \(T >> N\) or \(N >> T\), respectively.
\end{enumerate}

Identifying Assumptions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  SUTVA: Potential outcomes indexed only by the unit's contemporaneous treatment.
\item
  No dynamic effects (it's okay under staggered adoption, it gives a different interpretation of estimand).
\end{enumerate}

Setup:

\begin{itemize}
\tightlist
\item
  \(Y_{it}(0)\) and \(Y_{it}(1)\) represent potential outcomes of \(Y_{it}\).
\item
  \(W_{it}\) is a binary treatment indicator.
\end{itemize}

Aim to estimate the average effect for the treated:

\[
\tau = \frac{\sum_{(i,t): W_{it} = 1}[Y_{it}(1) - Y_{it}(0)]}{\sum_{i,t}W_{it}}
\]

We observe all relevant values for \(Y_{it}(1)\)

We want to impute missing entries in the \(Y(0)\) matrix for treated units with \(W_{it} = 1\).

Define \(\mathcal{M}\) as the set of pairs of indices \((i,t)\), where \(i \in N\) and \(t \in T\), corresponding to missing entries with \(W_{it} = 1\); \(\mathcal{O}\) as the set of pairs of indices corresponding to observed entries in \(Y(0)\) with \(W_{it} = 0\).

Data is conceptualized as two \(N \times T\) matrices, one incomplete and one complete:

\[
Y = \begin{pmatrix}
Y_{11} & Y_{12} & ? & \cdots & Y_{1T} \\
? & ? & Y_{23} & \cdots & ? \\
Y_{31} & ? & Y_{33} & \cdots & ? \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
Y_{N1} & ? & Y_{N3} & \cdots & ?
\end{pmatrix},
\]

and

\[
W = \begin{pmatrix}
0 & 0 & 1 & \cdots & 0 \\
1 & 1 & 0 & \cdots & 1 \\
0 & 1 & 0 & \cdots & 1 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 1 & 0 & \cdots & 1
\end{pmatrix},
\]

where

\[
W_{it} =
\begin{cases}
1 & \text{if } (i,t) \in \mathcal{M}, \\
0 & \text{if } (i,t) \in \mathcal{O},
\end{cases}
\]

is an indicator for the event that the corresponding component of \(Y\), that is \(Y_{it}\), is missing.

Patterns of missing data in \(\mathbf{Y}\):

\begin{itemize}
\item
  Block (treatment) structure with 2 special cases

  \begin{itemize}
  \item
    Single-treated-period block structure \citep{imbens2015causal}
  \item
    Single-treated-unit block structure \citep{abadie2010synthetic}
  \end{itemize}
\item
  Staggered Adoption
\end{itemize}

Shape of matrix \(\mathbf{Y}\):

\begin{itemize}
\item
  Thin (\(N >> T\))
\item
  Fat (\(T >> N\))
\item
  Square (\(N \approx T\))
\end{itemize}

Combinations of patterns of missingness and shape create different literatures:

\begin{itemize}
\item
  Horizontal Regression = Thin matrix + single-treated-period block (focusing on cross-section correlation patterns)
\item
  Vertical Regression = Fat matrix + single-treated-unit block (focusing on time-series correlation patterns)
\item
  TWFE = Square matrix
\end{itemize}

To combine, we can exploit both stable patterns over time, and across units (e.g., TWFE, interactive FEs or matrix completion).

For the same factor model

\[
\mathbf{Y = UV}^T + \mathbf{\epsilon}
\]

where \(\mathbf{U}\) is \(N \times R\) and \(\mathbf{V}\) is \(T\times R\)

The interactive FE literature focuses on a fixed number of factors \(R\) in \(\mathbf{U, V}\), while matrix completion focuses on impute \(\mathbf{Y}\) using some forms regularization (e.g., nuclear norm).

\begin{itemize}
\tightlist
\item
  We can also estimate the number of factors \(R\) \citep[\citet{moon2015linear}]{bai2002determining}
\end{itemize}

To use the nuclear norm minimization estimator, we must add a penalty term to regularize the objective function. However, before doing so, we need to explicitly estimate the time (\(\lambda_t\)) and unit (\(\mu_i\)) fixed effects implicitly embedded in the missing data matrix to reduce the bias of the regularization term.

\href{https://bookdown.org/stanfordgsbsilab/ml-ci-tutorial/matrix-completion-methods.html}{Specifically},

\[
Y_{it}  =L_{it} + \sum_{p = 1}^P \sum_{q= 1}^Q X_{ip} H_{pq}Z_{qt} + \mu_i + \lambda_t + V_{it} \beta + \epsilon_{it}
\]

where

\begin{itemize}
\item
  \(X_{ip}\) is a matrix of \(p\) variables for unit \(i\)
\item
  \(Z_{qt}\) is a matrix of \(q\) variables for time \(t\)
\item
  \(V_{it}\) is a matrix of time-varying variables.
\end{itemize}

Lasso-type \(l_1\) norm (\(||H|| = \sum_{p = 1}^p \sum_{q = 1}^Q |H_{pq}|\)) is used to shrink \(H \to 0\)

There are several options to regularize \(L\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Frobenius (i.e., Ridge): not informative since it imputes missing values as 0.
\item
  Nuclear Norm (i.e., Lasso): computationally feasible (using SOFT-IMPUTE algorithm \citep{Mazumder2010SpectralRA}).
\item
  Rank (i.e., Subset selection): not computationally feasible
\end{enumerate}

This method allows to

\begin{itemize}
\item
  use more covariates
\item
  leverage data from treated units (can be used when treatment effect is constant and pattern of missing is not complex).
\item
  have autocorrelated errors
\item
  have weighted loss function (i.e., take into account the probability of outcomes for a unit being missing)
\end{itemize}

\hypertarget{gardner2022two-and-borusyak2021revisiting}{%
\subsection{\texorpdfstring{\citet{gardner2022two} and \citet{borusyak2021revisiting}}{@gardner2022two and @borusyak2021revisiting}}\label{gardner2022two-and-borusyak2021revisiting}}

\begin{itemize}
\item
  Estimate the time and unit fixed effects separately
\item
  Known as the imputation method \citep{borusyak2021revisiting} or two-stage DiD \citep{gardner2022two}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# remotes::install\_github("kylebutts/did2s")}
\FunctionTok{library}\NormalTok{(did2s)}
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(fixest)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{data}\NormalTok{(base\_stagg)}


\NormalTok{est }\OtherTok{\textless{}{-}} \FunctionTok{did2s}\NormalTok{(}
    \AttributeTok{data =}\NormalTok{ base\_stagg }\SpecialCharTok{|\textgreater{}} \FunctionTok{mutate}\NormalTok{(}\AttributeTok{treat =} \FunctionTok{if\_else}\NormalTok{(time\_to\_treatment }\SpecialCharTok{\textgreater{}=} \DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)),}
    \AttributeTok{yname =} \StringTok{"y"}\NormalTok{,}
    \AttributeTok{first\_stage =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ x1 }\SpecialCharTok{|}\NormalTok{ id }\SpecialCharTok{+}\NormalTok{ year,}
    \AttributeTok{second\_stage =} \SpecialCharTok{\textasciitilde{}} \FunctionTok{i}\NormalTok{(time\_to\_treatment, }\AttributeTok{ref =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{,}\SpecialCharTok{{-}}\DecValTok{1000}\NormalTok{)),}
    \AttributeTok{treatment =} \StringTok{"treat"}\NormalTok{ ,}
    \AttributeTok{cluster\_var =} \StringTok{"id"}
\NormalTok{)}

\NormalTok{fixest}\SpecialCharTok{::}\FunctionTok{esttable}\NormalTok{(est)}
\CommentTok{\#\textgreater{}                                       est}
\CommentTok{\#\textgreater{} Dependent Var.:                         y}
\CommentTok{\#\textgreater{}                                          }
\CommentTok{\#\textgreater{} time\_to\_treatment = {-}9  0.3518** (0.1332)}
\CommentTok{\#\textgreater{} time\_to\_treatment = {-}8  {-}0.3130* (0.1213)}
\CommentTok{\#\textgreater{} time\_to\_treatment = {-}7    0.0894 (0.2367)}
\CommentTok{\#\textgreater{} time\_to\_treatment = {-}6    0.0312 (0.2176)}
\CommentTok{\#\textgreater{} time\_to\_treatment = {-}5   {-}0.2079 (0.1519)}
\CommentTok{\#\textgreater{} time\_to\_treatment = {-}4   {-}0.1152 (0.1438)}
\CommentTok{\#\textgreater{} time\_to\_treatment = {-}3   {-}0.0127 (0.1483)}
\CommentTok{\#\textgreater{} time\_to\_treatment = {-}2    0.1503 (0.1440)}
\CommentTok{\#\textgreater{} time\_to\_treatment = 0  {-}5.139*** (0.3680)}
\CommentTok{\#\textgreater{} time\_to\_treatment = 1  {-}3.480*** (0.3784)}
\CommentTok{\#\textgreater{} time\_to\_treatment = 2  {-}2.021*** (0.3055)}
\CommentTok{\#\textgreater{} time\_to\_treatment = 3   {-}0.6965. (0.3947)}
\CommentTok{\#\textgreater{} time\_to\_treatment = 4    1.070** (0.3501)}
\CommentTok{\#\textgreater{} time\_to\_treatment = 5   2.173*** (0.4456)}
\CommentTok{\#\textgreater{} time\_to\_treatment = 6   4.449*** (0.3680)}
\CommentTok{\#\textgreater{} time\_to\_treatment = 7   4.864*** (0.3698)}
\CommentTok{\#\textgreater{} time\_to\_treatment = 8   6.187*** (0.2702)}
\CommentTok{\#\textgreater{} \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}
\CommentTok{\#\textgreater{} S.E. type                          Custom}
\CommentTok{\#\textgreater{} Observations                          950}
\CommentTok{\#\textgreater{} R2                                0.62486}
\CommentTok{\#\textgreater{} Adj. R2                           0.61843}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes: 0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}

\NormalTok{fixest}\SpecialCharTok{::}\FunctionTok{iplot}\NormalTok{(}
\NormalTok{    est,}
    \AttributeTok{main =} \StringTok{"Event study"}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"Time to treatment"}\NormalTok{,}
    \AttributeTok{ref.line =} \SpecialCharTok{{-}}\DecValTok{1}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-39-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\FunctionTok{coefplot}\NormalTok{(est)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-39-2} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mult\_est }\OtherTok{\textless{}{-}}\NormalTok{ did2s}\SpecialCharTok{::}\FunctionTok{event\_study}\NormalTok{(}
    \AttributeTok{data =}\NormalTok{ fixest}\SpecialCharTok{::}\NormalTok{base\_stagg }\SpecialCharTok{|\textgreater{}}
\NormalTok{        dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{year\_treated =}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{if\_else}\NormalTok{(year\_treated }\SpecialCharTok{==} \DecValTok{10000}\NormalTok{, }\DecValTok{0}\NormalTok{, year\_treated)),}
    \AttributeTok{gname =} \StringTok{"year\_treated"}\NormalTok{,}
    \AttributeTok{idname =} \StringTok{"id"}\NormalTok{,}
    \AttributeTok{tname =} \StringTok{"year"}\NormalTok{,}
    \AttributeTok{yname =} \StringTok{"y"}\NormalTok{,}
    \AttributeTok{estimator =} \StringTok{"all"}
\NormalTok{)}
\CommentTok{\#\textgreater{} Error in purrr::map(., function(y) \{ : i In index: 1.}
\CommentTok{\#\textgreater{} i With name: y.}
\CommentTok{\#\textgreater{} Caused by error in \textasciigrave{}.subset2()\textasciigrave{}:}
\CommentTok{\#\textgreater{} ! no such index at level 1}
\NormalTok{did2s}\SpecialCharTok{::}\FunctionTok{plot\_event\_study}\NormalTok{(mult\_est)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-40-1} \end{center}

\citet{borusyak2021revisiting} \texttt{didimputation}

This version is currently not working

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(didimputation)}
\FunctionTok{library}\NormalTok{(fixest)}
\FunctionTok{data}\NormalTok{(}\StringTok{"base\_stagg"}\NormalTok{)}

\FunctionTok{did\_imputation}\NormalTok{(}
    \AttributeTok{data =}\NormalTok{ base\_stagg,}
    \AttributeTok{yname =} \StringTok{"y"}\NormalTok{,}
    \AttributeTok{gname =} \StringTok{"year\_treated"}\NormalTok{,}
    \AttributeTok{tname =} \StringTok{"year"}\NormalTok{,}
    \AttributeTok{idname =} \StringTok{"id"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{de2020two}{%
\subsection{\texorpdfstring{\citet{de2020two}}{@de2020two}}\label{de2020two}}

use \texttt{twowayfeweights} from \href{https://github.com/shuo-zhang-ucsb/twowayfeweights}{GitHub} \citep{de2020two}

\begin{itemize}
\item
  Average instant treatment effect of changes in the treatment

  \begin{itemize}
  \tightlist
  \item
    This relaxes the no-carryover-effect assumption.
  \end{itemize}
\item
  Drawbacks:

  \begin{itemize}
  \tightlist
  \item
    Cannot observe treatment effects that manifest over time.
  \end{itemize}
\end{itemize}

There still isn't a good package for this estimator.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# remotes::install\_github("shuo{-}zhang{-}ucsb/did\_multiplegt") }
\FunctionTok{library}\NormalTok{(DIDmultiplegt)}
\FunctionTok{library}\NormalTok{(fixest)}
\FunctionTok{library}\NormalTok{(tidyverse)}

\FunctionTok{data}\NormalTok{(}\StringTok{"base\_stagg"}\NormalTok{)}

\NormalTok{res }\OtherTok{\textless{}{-}}
    \FunctionTok{did\_multiplegt}\NormalTok{(}
        \AttributeTok{df =}\NormalTok{ base\_stagg }\SpecialCharTok{|\textgreater{}}
\NormalTok{            dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{treatment =}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{if\_else}\NormalTok{(time\_to\_treatment }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)),}
        \AttributeTok{Y        =} \StringTok{"y"}\NormalTok{,}
        \AttributeTok{G        =} \StringTok{"year\_treated"}\NormalTok{,}
        \AttributeTok{T        =} \StringTok{"year"}\NormalTok{,}
        \AttributeTok{D        =} \StringTok{"treatment"}\NormalTok{,}
        \AttributeTok{controls =} \StringTok{"x1"}\NormalTok{,}
        \CommentTok{\# brep     = 20, \# getting SE will take forever}
        \AttributeTok{placebo  =} \DecValTok{5}\NormalTok{,}
        \AttributeTok{dynamic  =} \DecValTok{5}\NormalTok{, }
        \AttributeTok{average\_effect =} \StringTok{"simple"}
\NormalTok{    )}

\FunctionTok{head}\NormalTok{(res)}
\CommentTok{\#\textgreater{} $effect}
\CommentTok{\#\textgreater{} treatment }
\CommentTok{\#\textgreater{} {-}5.214207 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $N\_effect}
\CommentTok{\#\textgreater{} [1] 675}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $N\_switchers\_effect}
\CommentTok{\#\textgreater{} [1] 45}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $dynamic\_1}
\CommentTok{\#\textgreater{} [1] {-}3.63556}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $N\_dynamic\_1}
\CommentTok{\#\textgreater{} [1] 580}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $N\_switchers\_effect\_1}
\CommentTok{\#\textgreater{} [1] 40}
\end{Highlighting}
\end{Shaded}

I don't recommend the \texttt{TwoWayFEWeights} since it only gives the aggregated average treatment effect over all post-treatment periods, but not for each period.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(TwoWayFEWeights)}

\NormalTok{res }\OtherTok{\textless{}{-}} \FunctionTok{twowayfeweights}\NormalTok{(}
    \AttributeTok{data =}\NormalTok{ base\_stagg }\SpecialCharTok{|\textgreater{}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{treatment =}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{if\_else}\NormalTok{(time\_to\_treatment }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)),}
    \AttributeTok{Y =} \StringTok{"y"}\NormalTok{,}
    \AttributeTok{G =} \StringTok{"year\_treated"}\NormalTok{,}
    \AttributeTok{T =} \StringTok{"year"}\NormalTok{,}
    \AttributeTok{D =} \StringTok{"treatment"}\NormalTok{, }
    \AttributeTok{summary\_measures =}\NormalTok{ T}
\NormalTok{)}

\FunctionTok{print}\NormalTok{(res)}
\CommentTok{\#\textgreater{} Under the common trends assumption, beta estimates a weighted sum of 45 ATTs.}
\CommentTok{\#\textgreater{} 41 ATTs receive a positive weight, and 4 receive a negative weight.}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-} }
\CommentTok{\#\textgreater{} Treat. var: treatment    ATTs     weights }
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-} }
\CommentTok{\#\textgreater{} Positive weights           41       1.0238 }
\CommentTok{\#\textgreater{} Negative weights            4      {-}0.0238 }
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-} }
\CommentTok{\#\textgreater{} Total                      45            1 }
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Summary Measures:}
\CommentTok{\#\textgreater{}   TWFE Coefficient (\_fe): {-}3.4676}
\CommentTok{\#\textgreater{}   min () compatible with \_fe and \_TR = 0: 4.8357}
\CommentTok{\#\textgreater{}   min () compatible with \_fe and \_TR of a different sign: 36.1549}
\CommentTok{\#\textgreater{}   Reference: Corollary 1, de Chaisemartin, C and D\textquotesingle{}Haultfoeuille, X (2020a)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} The development of this package was funded by the European Union (ERC, REALLYCREDIBLE,GA N. 101043899).}
\end{Highlighting}
\end{Shaded}

\hypertarget{callaway2021difference}{%
\subsection{\texorpdfstring{\citet{callaway2021difference}}{@callaway2021difference}}\label{callaway2021difference}}

\begin{itemize}
\item
  \texttt{staggered} \href{https://github.com/jonathandroth/staggered}{package}
\item
  Group-time average treatment effect
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(staggered) }
\FunctionTok{library}\NormalTok{(fixest)}
\FunctionTok{data}\NormalTok{(}\StringTok{"base\_stagg"}\NormalTok{)}

\CommentTok{\# simple weighted average}
\FunctionTok{staggered}\NormalTok{(}
    \AttributeTok{df =}\NormalTok{ base\_stagg,}
    \AttributeTok{i =} \StringTok{"id"}\NormalTok{,}
    \AttributeTok{t =} \StringTok{"year"}\NormalTok{,}
    \AttributeTok{g =} \StringTok{"year\_treated"}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"y"}\NormalTok{,}
    \AttributeTok{estimand =} \StringTok{"simple"}
\NormalTok{)}
\CommentTok{\#\textgreater{}     estimate        se se\_neyman}
\CommentTok{\#\textgreater{} 1 {-}0.7110941 0.2211943 0.2214245}

\CommentTok{\# cohort weighted average}
\FunctionTok{staggered}\NormalTok{(}
    \AttributeTok{df =}\NormalTok{ base\_stagg,}
    \AttributeTok{i =} \StringTok{"id"}\NormalTok{,}
    \AttributeTok{t =} \StringTok{"year"}\NormalTok{,}
    \AttributeTok{g =} \StringTok{"year\_treated"}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"y"}\NormalTok{,}
    \AttributeTok{estimand =} \StringTok{"cohort"}
\NormalTok{)}
\CommentTok{\#\textgreater{}    estimate        se se\_neyman}
\CommentTok{\#\textgreater{} 1 {-}2.724242 0.2701093 0.2701745}

\CommentTok{\# calendar weighted average}
\FunctionTok{staggered}\NormalTok{(}
    \AttributeTok{df =}\NormalTok{ base\_stagg,}
    \AttributeTok{i =} \StringTok{"id"}\NormalTok{,}
    \AttributeTok{t =} \StringTok{"year"}\NormalTok{,}
    \AttributeTok{g =} \StringTok{"year\_treated"}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"y"}\NormalTok{,}
    \AttributeTok{estimand =} \StringTok{"calendar"}
\NormalTok{)}
\CommentTok{\#\textgreater{}     estimate        se se\_neyman}
\CommentTok{\#\textgreater{} 1 {-}0.5861831 0.1768297 0.1770729}

\NormalTok{res }\OtherTok{\textless{}{-}} \FunctionTok{staggered}\NormalTok{(}
    \AttributeTok{df =}\NormalTok{ base\_stagg,}
    \AttributeTok{i =} \StringTok{"id"}\NormalTok{,}
    \AttributeTok{t =} \StringTok{"year"}\NormalTok{,}
    \AttributeTok{g =} \StringTok{"year\_treated"}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"y"}\NormalTok{,}
    \AttributeTok{estimand =} \StringTok{"eventstudy"}\NormalTok{, }
    \AttributeTok{eventTime =} \SpecialCharTok{{-}}\DecValTok{9}\SpecialCharTok{:}\DecValTok{8}
\NormalTok{)}
\FunctionTok{head}\NormalTok{(res)}
\CommentTok{\#\textgreater{}      estimate        se se\_neyman eventTime}
\CommentTok{\#\textgreater{} 1  0.20418779 0.1045821 0.1045821        {-}9}
\CommentTok{\#\textgreater{} 2 {-}0.06215104 0.1669703 0.1670886        {-}8}
\CommentTok{\#\textgreater{} 3  0.02744671 0.1413273 0.1420377        {-}7}
\CommentTok{\#\textgreater{} 4 {-}0.02131747 0.2203695 0.2206338        {-}6}
\CommentTok{\#\textgreater{} 5 {-}0.30690897 0.2015697 0.2036412        {-}5}
\CommentTok{\#\textgreater{} 6  0.05594029 0.1908101 0.1921745        {-}4}


\FunctionTok{ggplot}\NormalTok{(}
\NormalTok{    res }\SpecialCharTok{|\textgreater{}} \FunctionTok{mutate}\NormalTok{(}
        \AttributeTok{ymin\_ptwise =}\NormalTok{ estimate }\SpecialCharTok{+} \FloatTok{1.96} \SpecialCharTok{*}\NormalTok{ se,}
        \AttributeTok{ymax\_ptwise =}\NormalTok{ estimate }\SpecialCharTok{{-}} \FloatTok{1.96} \SpecialCharTok{*}\NormalTok{ se}
\NormalTok{    ),}
    \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ eventTime, }\AttributeTok{y =}\NormalTok{ estimate)}
\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_pointrange}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{ymin =}\NormalTok{ ymin\_ptwise, }\AttributeTok{ymax =}\NormalTok{ ymax\_ptwise)) }\SpecialCharTok{+}
    \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{xlab}\NormalTok{(}\StringTok{"Event Time"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{ylab}\NormalTok{(}\StringTok{"Estimate"}\NormalTok{) }\SpecialCharTok{+}
\NormalTok{    causalverse}\SpecialCharTok{::}\FunctionTok{ama\_theme}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-44-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Callaway and Sant\textquotesingle{}Anna estimator for the simple weighted average}
\FunctionTok{staggered\_cs}\NormalTok{(}
    \AttributeTok{df =}\NormalTok{ base\_stagg,}
    \AttributeTok{i =} \StringTok{"id"}\NormalTok{,}
    \AttributeTok{t =} \StringTok{"year"}\NormalTok{,}
    \AttributeTok{g =} \StringTok{"year\_treated"}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"y"}\NormalTok{,}
    \AttributeTok{estimand =} \StringTok{"simple"}
\NormalTok{)}
\CommentTok{\#\textgreater{}     estimate        se se\_neyman}
\CommentTok{\#\textgreater{} 1 {-}0.7994889 0.4484987 0.4486122}

\CommentTok{\# Sun and Abraham estimator for the simple weighted average}
\FunctionTok{staggered\_sa}\NormalTok{(}
    \AttributeTok{df =}\NormalTok{ base\_stagg,}
    \AttributeTok{i =} \StringTok{"id"}\NormalTok{,}
    \AttributeTok{t =} \StringTok{"year"}\NormalTok{,}
    \AttributeTok{g =} \StringTok{"year\_treated"}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"y"}\NormalTok{,}
    \AttributeTok{estimand =} \StringTok{"simple"}
\NormalTok{)}
\CommentTok{\#\textgreater{}     estimate        se se\_neyman}
\CommentTok{\#\textgreater{} 1 {-}0.7551901 0.4407818 0.4409525}
\end{Highlighting}
\end{Shaded}

Fisher's Randomization Test (i.e., permutation test)

\(H_0\): \(TE = 0\)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{staggered}\NormalTok{(}
    \AttributeTok{df =}\NormalTok{ base\_stagg,}
    \AttributeTok{i =} \StringTok{"id"}\NormalTok{,}
    \AttributeTok{t =} \StringTok{"year"}\NormalTok{,}
    \AttributeTok{g =} \StringTok{"year\_treated"}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"y"}\NormalTok{,}
    \AttributeTok{estimand =} \StringTok{"simple"}\NormalTok{,}
    \AttributeTok{compute\_fisher =}\NormalTok{ T,}
    \AttributeTok{num\_fisher\_permutations =} \DecValTok{100}
\NormalTok{)}
\CommentTok{\#\textgreater{}     estimate        se se\_neyman fisher\_pval fisher\_pval\_se\_neyman}
\CommentTok{\#\textgreater{} 1 {-}0.7110941 0.2211943 0.2214245           0                     0}
\CommentTok{\#\textgreater{}   num\_fisher\_permutations}
\CommentTok{\#\textgreater{} 1                     100}
\end{Highlighting}
\end{Shaded}

\hypertarget{sun2021estimating}{%
\subsection{\texorpdfstring{\citet{sun2021estimating}}{@sun2021estimating}}\label{sun2021estimating}}

This paper utilizes the Cohort Average Treatment Effects on the Treated (CATT), which measures the cohort-specific average difference in outcomes relative to those never treated, offering a more detailed analysis than \citet{goodman2021difference}. In scenarios lacking a never-treated group, this method designates the last cohort to be treated as the control group.

Parameter of interest is the cohort-specific ATT \(l\) periods from int ital treatment period \(e\)

\[
CATT = E[Y_{i, e + I} - Y_{i, e + I}^\infty|E_i = e]
\]

This paper uses an \textbf{interaction-weighted estimator} in a panel data setting, where the original paper \citet{gibbons2018broken} used the same idea in a cross-sectional setting.

\begin{itemize}
\item
  \citet{callaway2021difference} explores group-time average treatment effects, employing cohorts that have not yet been treated as controls, and permits conditioning on time-varying covariates.
\item
  \citet{athey2022design} examines the treatment effect in relation to the counterfactual outcome of the always-treated group, diverging from the conventional focus on the never-treated.
\item
  \citet{borusyak2021revisiting} presumes a uniform treatment effect across cohorts, effectively simplifying CATT to ATT.
\end{itemize}

Identifying Assumptions for dynamic TWFE:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Parallel Trends}: Baseline outcomes follow parallel trends across cohorts before treatment.

  \begin{itemize}
  \tightlist
  \item
    This gives us all CATT (including own, included bins, and excluded bins)
  \end{itemize}
\item
  \textbf{No Anticipatory Behavior}: There is no effect of the treatment during pre-treatment periods, indicating that outcomes are not influenced by the anticipation of treatment.
\item
  \textbf{Treatment Effect Homogeneity}: The treatment effect is consistent across cohorts for each relative period. Each adoption cohort should have the same path of treatment effects. In other words, the trajectory of each treatment cohort is similar. Compare to other designs:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    \citet{athey2022design} assume heterogeneity of treatment effects vary over adoption cohorts, but not over time.
  \item
    \citet{borusyak2021revisiting} assume heterogeneity of treatment effects vary over time, but not over adoption cohorts.
  \item
    \citet{callaway2021difference} assume heterogeneity of treatment effects vary over time and across cohorts.
  \item
    \citet{de2023two} assume heterogeneity of treatment effects vary across groups and over time.
  \item
    \citet{goodman2021difference} assume heterogeneity either ``vary across units but not over time'' or ``vary over time but not across units''.
  \item
    \citet{sun2021estimating} allows for treatment effect heterogeneity across units and time.
  \end{enumerate}
\end{enumerate}

Sources of Heterogeneous Treatment Effects

\begin{itemize}
\item
  Adoption cohorts can differ based on certain covariates. Similarly, composition of units within each adoption cohort is different.
\item
  The response to treatment varies among cohorts if units self-select their initial treatment timing based on anticipated treatment effects. However, this self-selection is still compatible with the parallel trends assumption. This is true if units choose based on an evaluation of baseline outcomes - that is, if baseline outcomes are similar (following parallel trends), then we might not see selection into treatment based on the evaluation of the baseline outcome.
\item
  Treatment effects can vary across cohorts due to calendar time-varying effects, such as changes in economic conditions.
\end{itemize}

Notes:

\begin{itemize}
\item
  If you do TWFE, you actually have to drop 2 terms to avoid multicollinearity:

  \begin{itemize}
  \item
    Period right before treatment (this one was known before this paper)
  \item
    Drop or bin or trim a distant lag period (this one was clarified by the paper). The reason is before of the multicollinearity in the linear relationship between TWFE and the relative period indicators.
  \end{itemize}
\item
  Contamination of the treatment effect estimates from excluded periods is a type of ``normalization''. To avoid this, we have to assume that all pre-treatment periods have the same CATT.

  \begin{itemize}
  \tightlist
  \item
    \citet{sun2021estimating} estimation method gives reasonable weights to CATT (i..e, weights that sum to 1, and are non negative). They estimate the weighted average of CATT where the weights are shares of cohorts that experience at least \(l\) periods after to treatment, normalized by the size of total periods \(g\).
  \end{itemize}
\item
  Aggregation of CATT is similar to that of \citet{callaway2021difference}
\end{itemize}

\textbf{Application}

can use \texttt{fixest} in r with \texttt{sunab} function

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(fixest)}
\FunctionTok{data}\NormalTok{(}\StringTok{"base\_stagg"}\NormalTok{)}
\NormalTok{res\_sa20 }\OtherTok{=} \FunctionTok{feols}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x1 }\SpecialCharTok{+} \FunctionTok{sunab}\NormalTok{(year\_treated, year) }\SpecialCharTok{|}\NormalTok{ id }\SpecialCharTok{+}\NormalTok{ year, base\_stagg)}
\FunctionTok{iplot}\NormalTok{(res\_sa20)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-47-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\FunctionTok{summary}\NormalTok{(res\_sa20, }\AttributeTok{agg =} \StringTok{"att"}\NormalTok{)}
\CommentTok{\#\textgreater{} OLS estimation, Dep. Var.: y}
\CommentTok{\#\textgreater{} Observations: 950 }
\CommentTok{\#\textgreater{} Fixed{-}effects: id: 95,  year: 10}
\CommentTok{\#\textgreater{} Standard{-}errors: Clustered (id) }
\CommentTok{\#\textgreater{}      Estimate Std. Error  t value  Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} x1   0.994678   0.018378 54.12293 \textless{} 2.2e{-}16 ***}
\CommentTok{\#\textgreater{} ATT {-}1.133749   0.205070 {-}5.52858 2.882e{-}07 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} RMSE: 0.921817     Adj. R2: 0.887984}
\CommentTok{\#\textgreater{}                  Within R2: 0.876406}


\FunctionTok{summary}\NormalTok{(res\_sa20, }\AttributeTok{agg =} \FunctionTok{c}\NormalTok{(}\StringTok{"att"} \OtherTok{=} \StringTok{"year::[\^{}{-}]"}\NormalTok{)) }
\CommentTok{\#\textgreater{} OLS estimation, Dep. Var.: y}
\CommentTok{\#\textgreater{} Observations: 950 }
\CommentTok{\#\textgreater{} Fixed{-}effects: id: 95,  year: 10}
\CommentTok{\#\textgreater{} Standard{-}errors: Clustered (id) }
\CommentTok{\#\textgreater{}                      Estimate Std. Error   t value   Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} x1                   0.994678   0.018378 54.122928  \textless{} 2.2e{-}16 ***}
\CommentTok{\#\textgreater{} year::{-}9:cohort::10  0.351766   0.359073  0.979649 3.2977e{-}01    }
\CommentTok{\#\textgreater{} year::{-}8:cohort::9   0.033914   0.471437  0.071937 9.4281e{-}01    }
\CommentTok{\#\textgreater{} year::{-}8:cohort::10 {-}0.191932   0.352896 {-}0.543876 5.8781e{-}01    }
\CommentTok{\#\textgreater{} year::{-}7:cohort::8  {-}0.589387   0.736910 {-}0.799809 4.2584e{-}01    }
\CommentTok{\#\textgreater{} year::{-}7:cohort::9   0.872995   0.493427  1.769249 8.0096e{-}02 .  }
\CommentTok{\#\textgreater{} year::{-}7:cohort::10  0.019512   0.603411  0.032336 9.7427e{-}01    }
\CommentTok{\#\textgreater{} year::{-}6:cohort::7  {-}0.042147   0.865736 {-}0.048683 9.6127e{-}01    }
\CommentTok{\#\textgreater{} year::{-}6:cohort::8  {-}0.657571   0.573257 {-}1.147078 2.5426e{-}01    }
\CommentTok{\#\textgreater{} year::{-}6:cohort::9   0.877743   0.533331  1.645775 1.0315e{-}01    }
\CommentTok{\#\textgreater{} year::{-}6:cohort::10 {-}0.403635   0.347412 {-}1.161832 2.4825e{-}01    }
\CommentTok{\#\textgreater{} year::{-}5:cohort::6  {-}0.658034   0.913407 {-}0.720418 4.7306e{-}01    }
\CommentTok{\#\textgreater{} year::{-}5:cohort::7  {-}0.316974   0.697939 {-}0.454158 6.5076e{-}01    }
\CommentTok{\#\textgreater{} year::{-}5:cohort::8  {-}0.238213   0.469744 {-}0.507113 6.1326e{-}01    }
\CommentTok{\#\textgreater{} year::{-}5:cohort::9   0.301477   0.604201  0.498968 6.1897e{-}01    }
\CommentTok{\#\textgreater{} year::{-}5:cohort::10 {-}0.564801   0.463214 {-}1.219308 2.2578e{-}01    }
\CommentTok{\#\textgreater{} year::{-}4:cohort::5  {-}0.983453   0.634492 {-}1.549984 1.2451e{-}01    }
\CommentTok{\#\textgreater{} year::{-}4:cohort::6   0.360407   0.858316  0.419900 6.7552e{-}01    }
\CommentTok{\#\textgreater{} year::{-}4:cohort::7  {-}0.430610   0.661356 {-}0.651102 5.1657e{-}01    }
\CommentTok{\#\textgreater{} year::{-}4:cohort::8  {-}0.895195   0.374901 {-}2.387816 1.8949e{-}02 *  }
\CommentTok{\#\textgreater{} year::{-}4:cohort::9  {-}0.392478   0.439547 {-}0.892914 3.7418e{-}01    }
\CommentTok{\#\textgreater{} year::{-}4:cohort::10  0.519001   0.597880  0.868069 3.8757e{-}01    }
\CommentTok{\#\textgreater{} year::{-}3:cohort::4   0.591288   0.680169  0.869324 3.8688e{-}01    }
\CommentTok{\#\textgreater{} year::{-}3:cohort::5  {-}1.000650   0.971741 {-}1.029749 3.0577e{-}01    }
\CommentTok{\#\textgreater{} year::{-}3:cohort::6   0.072188   0.652641  0.110609 9.1216e{-}01    }
\CommentTok{\#\textgreater{} year::{-}3:cohort::7  {-}0.836820   0.804275 {-}1.040465 3.0079e{-}01    }
\CommentTok{\#\textgreater{} year::{-}3:cohort::8  {-}0.783148   0.701312 {-}1.116691 2.6697e{-}01    }
\CommentTok{\#\textgreater{} year::{-}3:cohort::9   0.811285   0.564470  1.437251 1.5397e{-}01    }
\CommentTok{\#\textgreater{} year::{-}3:cohort::10  0.527203   0.320051  1.647250 1.0285e{-}01    }
\CommentTok{\#\textgreater{} year::{-}2:cohort::3   0.036941   0.673771  0.054828 9.5639e{-}01    }
\CommentTok{\#\textgreater{} year::{-}2:cohort::4   0.832250   0.859544  0.968246 3.3541e{-}01    }
\CommentTok{\#\textgreater{} year::{-}2:cohort::5  {-}1.574086   0.525563 {-}2.995051 3.5076e{-}03 ** }
\CommentTok{\#\textgreater{} year::{-}2:cohort::6   0.311758   0.832095  0.374666 7.0875e{-}01    }
\CommentTok{\#\textgreater{} year::{-}2:cohort::7  {-}0.558631   0.871993 {-}0.640638 5.2332e{-}01    }
\CommentTok{\#\textgreater{} year::{-}2:cohort::8   0.429591   0.305270  1.407250 1.6265e{-}01    }
\CommentTok{\#\textgreater{} year::{-}2:cohort::9   1.201899   0.819186  1.467188 1.4566e{-}01    }
\CommentTok{\#\textgreater{} year::{-}2:cohort::10 {-}0.002429   0.682087 {-}0.003562 9.9717e{-}01    }
\CommentTok{\#\textgreater{} att                 {-}1.133749   0.205070 {-}5.528584 2.8820e{-}07 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} RMSE: 0.921817     Adj. R2: 0.887984}
\CommentTok{\#\textgreater{}                  Within R2: 0.876406}

\CommentTok{\# alternatively}
\FunctionTok{summary}\NormalTok{(res\_sa20, }\AttributeTok{agg =} \FunctionTok{c}\NormalTok{(}\StringTok{"att"} \OtherTok{=} \StringTok{"year::[012345678]"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} 
    \FunctionTok{etable}\NormalTok{(}\AttributeTok{digits =} \DecValTok{2}\NormalTok{)}
\CommentTok{\#\textgreater{}                         summary(res\_..}
\CommentTok{\#\textgreater{} Dependent Var.:                      y}
\CommentTok{\#\textgreater{}                                       }
\CommentTok{\#\textgreater{} x1                      0.99*** (0.02)}
\CommentTok{\#\textgreater{} year = {-}9 x cohort = 10    0.35 (0.36)}
\CommentTok{\#\textgreater{} year = {-}8 x cohort = 9     0.03 (0.47)}
\CommentTok{\#\textgreater{} year = {-}8 x cohort = 10   {-}0.19 (0.35)}
\CommentTok{\#\textgreater{} year = {-}7 x cohort = 8    {-}0.59 (0.74)}
\CommentTok{\#\textgreater{} year = {-}7 x cohort = 9    0.87. (0.49)}
\CommentTok{\#\textgreater{} year = {-}7 x cohort = 10    0.02 (0.60)}
\CommentTok{\#\textgreater{} year = {-}6 x cohort = 7    {-}0.04 (0.87)}
\CommentTok{\#\textgreater{} year = {-}6 x cohort = 8    {-}0.66 (0.57)}
\CommentTok{\#\textgreater{} year = {-}6 x cohort = 9     0.88 (0.53)}
\CommentTok{\#\textgreater{} year = {-}6 x cohort = 10   {-}0.40 (0.35)}
\CommentTok{\#\textgreater{} year = {-}5 x cohort = 6    {-}0.66 (0.91)}
\CommentTok{\#\textgreater{} year = {-}5 x cohort = 7    {-}0.32 (0.70)}
\CommentTok{\#\textgreater{} year = {-}5 x cohort = 8    {-}0.24 (0.47)}
\CommentTok{\#\textgreater{} year = {-}5 x cohort = 9     0.30 (0.60)}
\CommentTok{\#\textgreater{} year = {-}5 x cohort = 10   {-}0.56 (0.46)}
\CommentTok{\#\textgreater{} year = {-}4 x cohort = 5    {-}0.98 (0.63)}
\CommentTok{\#\textgreater{} year = {-}4 x cohort = 6     0.36 (0.86)}
\CommentTok{\#\textgreater{} year = {-}4 x cohort = 7    {-}0.43 (0.66)}
\CommentTok{\#\textgreater{} year = {-}4 x cohort = 8   {-}0.90* (0.37)}
\CommentTok{\#\textgreater{} year = {-}4 x cohort = 9    {-}0.39 (0.44)}
\CommentTok{\#\textgreater{} year = {-}4 x cohort = 10    0.52 (0.60)}
\CommentTok{\#\textgreater{} year = {-}3 x cohort = 4     0.59 (0.68)}
\CommentTok{\#\textgreater{} year = {-}3 x cohort = 5     {-}1.0 (0.97)}
\CommentTok{\#\textgreater{} year = {-}3 x cohort = 6     0.07 (0.65)}
\CommentTok{\#\textgreater{} year = {-}3 x cohort = 7    {-}0.84 (0.80)}
\CommentTok{\#\textgreater{} year = {-}3 x cohort = 8    {-}0.78 (0.70)}
\CommentTok{\#\textgreater{} year = {-}3 x cohort = 9     0.81 (0.56)}
\CommentTok{\#\textgreater{} year = {-}3 x cohort = 10    0.53 (0.32)}
\CommentTok{\#\textgreater{} year = {-}2 x cohort = 3     0.04 (0.67)}
\CommentTok{\#\textgreater{} year = {-}2 x cohort = 4     0.83 (0.86)}
\CommentTok{\#\textgreater{} year = {-}2 x cohort = 5   {-}1.6** (0.53)}
\CommentTok{\#\textgreater{} year = {-}2 x cohort = 6     0.31 (0.83)}
\CommentTok{\#\textgreater{} year = {-}2 x cohort = 7    {-}0.56 (0.87)}
\CommentTok{\#\textgreater{} year = {-}2 x cohort = 8     0.43 (0.31)}
\CommentTok{\#\textgreater{} year = {-}2 x cohort = 9      1.2 (0.82)}
\CommentTok{\#\textgreater{} year = {-}2 x cohort = 10  {-}0.002 (0.68)}
\CommentTok{\#\textgreater{} att                     {-}1.1*** (0.21)}
\CommentTok{\#\textgreater{} Fixed{-}Effects:          {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} id                                 Yes}
\CommentTok{\#\textgreater{} year                               Yes}
\CommentTok{\#\textgreater{} \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_}
\CommentTok{\#\textgreater{} S.E.: Clustered                 by: id}
\CommentTok{\#\textgreater{} Observations                       950}
\CommentTok{\#\textgreater{} R2                             0.90982}
\CommentTok{\#\textgreater{} Within R2                      0.87641}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes: 0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

Using the same syntax as \texttt{fixest}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# devtools::install\_github("kylebutts/fwlplot")}
\FunctionTok{library}\NormalTok{(fwlplot)}
\FunctionTok{fwl\_plot}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x1, }\AttributeTok{data =}\NormalTok{ base\_stagg)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/plot residuals-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\FunctionTok{fwl\_plot}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x1 }\SpecialCharTok{|}\NormalTok{ id }\SpecialCharTok{+}\NormalTok{ year, }\AttributeTok{data =}\NormalTok{ base\_stagg, }\AttributeTok{n\_sample =} \DecValTok{100}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/plot residuals-2} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\FunctionTok{fwl\_plot}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x1 }\SpecialCharTok{|}\NormalTok{ id }\SpecialCharTok{+}\NormalTok{ year, }\AttributeTok{data =}\NormalTok{ base\_stagg, }\AttributeTok{n\_sample =} \DecValTok{100}\NormalTok{, }\AttributeTok{fsplit =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ treated)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/plot residuals-3} \end{center}

\hypertarget{wooldridge2022simple}{%
\subsection{\texorpdfstring{\citet{wooldridge2022simple}}{@wooldridge2022simple}}\label{wooldridge2022simple}}

use \href{https://grantmcdermott.com/etwfe/}{etwfe}(Extended two-way Fixed Effects) \citep{wooldridge2022simple}

\hypertarget{doubly-robust-did}{%
\subsection{Doubly Robust DiD}\label{doubly-robust-did}}

Also known as the locally efficient doubly robust DiD \citep{sant2020doubly}

\href{https://psantanna.com/DRDID/index.html}{Code example by the authors}

The package (not method) is rather limited application:

\begin{itemize}
\item
  Use OLS (cannot handle \texttt{glm})
\item
  Canonical DiD only (cannot handle DDD).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(DRDID)}
\FunctionTok{data}\NormalTok{(}\StringTok{"nsw\_long"}\NormalTok{)}
\NormalTok{eval\_lalonde\_cps }\OtherTok{\textless{}{-}}
    \FunctionTok{subset}\NormalTok{(nsw\_long, nsw\_long}\SpecialCharTok{$}\NormalTok{treated }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{|}\NormalTok{ nsw\_long}\SpecialCharTok{$}\NormalTok{sample }\SpecialCharTok{==} \DecValTok{2}\NormalTok{)}
\FunctionTok{head}\NormalTok{(eval\_lalonde\_cps)}
\CommentTok{\#\textgreater{}   id year treated age educ black married nodegree dwincl      re74 hisp}
\CommentTok{\#\textgreater{} 1  1 1975      NA  42   16     0       1        0     NA     0.000    0}
\CommentTok{\#\textgreater{} 2  1 1978      NA  42   16     0       1        0     NA     0.000    0}
\CommentTok{\#\textgreater{} 3  2 1975      NA  20   13     0       0        0     NA  2366.794    0}
\CommentTok{\#\textgreater{} 4  2 1978      NA  20   13     0       0        0     NA  2366.794    0}
\CommentTok{\#\textgreater{} 5  3 1975      NA  37   12     0       1        0     NA 25862.322    0}
\CommentTok{\#\textgreater{} 6  3 1978      NA  37   12     0       1        0     NA 25862.322    0}
\CommentTok{\#\textgreater{}   early\_ra sample experimental         re}
\CommentTok{\#\textgreater{} 1       NA      2            0     0.0000}
\CommentTok{\#\textgreater{} 2       NA      2            0   100.4854}
\CommentTok{\#\textgreater{} 3       NA      2            0  3317.4678}
\CommentTok{\#\textgreater{} 4       NA      2            0  4793.7451}
\CommentTok{\#\textgreater{} 5       NA      2            0 22781.8555}
\CommentTok{\#\textgreater{} 6       NA      2            0 25564.6699}


\CommentTok{\# locally efficient doubly robust DiD Estimators for the ATT}
\NormalTok{out }\OtherTok{\textless{}{-}}
    \FunctionTok{drdid}\NormalTok{(}
        \AttributeTok{yname =} \StringTok{"re"}\NormalTok{,}
        \AttributeTok{tname =} \StringTok{"year"}\NormalTok{,}
        \AttributeTok{idname =} \StringTok{"id"}\NormalTok{,}
        \AttributeTok{dname =} \StringTok{"experimental"}\NormalTok{,}
        \AttributeTok{xformla =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ educ }\SpecialCharTok{+}\NormalTok{ black }\SpecialCharTok{+}\NormalTok{ married }\SpecialCharTok{+}\NormalTok{ nodegree }\SpecialCharTok{+}\NormalTok{ hisp }\SpecialCharTok{+}\NormalTok{ re74,}
        \AttributeTok{data =}\NormalTok{ eval\_lalonde\_cps,}
        \AttributeTok{panel =} \ConstantTok{TRUE}
\NormalTok{    )}
\FunctionTok{summary}\NormalTok{(out)}
\CommentTok{\#\textgreater{}  Call:}
\CommentTok{\#\textgreater{} drdid(yname = "re", tname = "year", idname = "id", dname = "experimental", }
\CommentTok{\#\textgreater{}     xformla = \textasciitilde{}age + educ + black + married + nodegree + hisp + }
\CommentTok{\#\textgreater{}         re74, data = eval\_lalonde\_cps, panel = TRUE)}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}  Further improved locally efficient DR DID estimator for the ATT:}
\CommentTok{\#\textgreater{}  }
\CommentTok{\#\textgreater{}    ATT     Std. Error  t value    Pr(\textgreater{}|t|)  [95\% Conf. Interval] }
\CommentTok{\#\textgreater{} {-}901.2703   393.6247   {-}2.2897     0.022    {-}1672.7747  {-}129.766 }
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}  Estimator based on panel data.}
\CommentTok{\#\textgreater{}  Outcome regression est. method: weighted least squares.}
\CommentTok{\#\textgreater{}  Propensity score est. method: inverse prob. tilting.}
\CommentTok{\#\textgreater{}  Analytical standard error.}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}  See Sant\textquotesingle{}Anna and Zhao (2020) for details.}



\CommentTok{\# Improved locally efficient doubly robust DiD estimator }
\CommentTok{\# for the ATT, with panel data}
\CommentTok{\# drdid\_imp\_panel()}

\CommentTok{\# Locally efficient doubly robust DiD estimator for the ATT, }
\CommentTok{\# with panel data}
\CommentTok{\# drdid\_panel()}

\CommentTok{\# Locally efficient doubly robust DiD estimator for the ATT, }
\CommentTok{\# with repeated cross{-}section data}
\CommentTok{\# drdid\_rc()}

\CommentTok{\# Improved locally efficient doubly robust DiD estimator for the ATT, }
\CommentTok{\# with repeated cross{-}section data}
\CommentTok{\# drdid\_imp\_rc()}
\end{Highlighting}
\end{Shaded}

\hypertarget{augmentedforward-did}{%
\subsection{Augmented/Forward DID}\label{augmentedforward-did}}

\begin{itemize}
\tightlist
\item
  DID Methods for Limited Pre-Treatment Periods:
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1235}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3412}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5294}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Method}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Scenario}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Approach}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Augmented DID}

\citep{li2023augmented} & Treatment outcome is outside the range of control units & Constructs the treatment counterfactual using a scaled average of control units \\
\textbf{Forward DID}

\citep{li2024frontiers} & Treatment outcome is within the range of control units & Uses a forward selection algorithm to choose relevant control units before applying DID \\
\end{longtable}

\hypertarget{multiple-treatments}{%
\section{Multiple Treatments}\label{multiple-treatments}}

When you have 2 treatments in a setting, you should always try to model both of them under one regression to see whether they are significantly different.

\begin{itemize}
\tightlist
\item
  Never use one treated groups as control for the other, and run separate regression.
\item
  Could check this \href{https://stats.stackexchange.com/questions/474533/difference-in-difference-with-two-treatment-groups-and-one-control-group-classi}{answer}
\end{itemize}

\[
\begin{aligned}
Y_{it} &= \alpha + \gamma_1 Treat1_{i} + \gamma_2 Treat2_{i} + \lambda Post_t  \\
&+ \delta_1(Treat1_i \times Post_t) + \delta_2(Treat2_i \times Post_t) + \epsilon_{it}
\end{aligned}
\]

\citep{fricke2017identification}

\citep{de2023two} \href{https://www.youtube.com/watch?v=UHeJoc27qEM\&ab_channel=TaylorWright}{video} \href{https://drive.google.com/file/d/156Fu73avBvvV_H64wePm7eW04V0jEG3K/view}{code}

\hypertarget{mediation-under-did}{%
\section{Mediation Under DiD}\label{mediation-under-did}}

Check this \href{https://stats.stackexchange.com/questions/261218/difference-in-difference-model-with-mediators-estimating-the-effect-of-differen}{post}

\hypertarget{assumptions-1}{%
\section{Assumptions}\label{assumptions-1}}

\begin{itemize}
\item
  \textbf{Parallel Trends}: Difference between the treatment and control groups remain constant if there were no treatment.

  \begin{itemize}
  \item
    should be used in cases where

    \begin{itemize}
    \item
      you observe before and after an event
    \item
      you have treatment and control groups
    \end{itemize}
  \item
    not in cases where

    \begin{itemize}
    \item
      treatment is not random
    \item
      confounders.
    \end{itemize}
  \item
    To support we use

    \begin{itemize}
    \item
      \protect\hyperlink{placebo-test}{Placebo test}
    \item
      \protect\hyperlink{prior-parallel-trends-test}{Prior Parallel Trends Test}
    \end{itemize}
  \end{itemize}
\item
  \textbf{Linear additive effects} (of group/unit specific and time-specific):

  \begin{itemize}
  \item
    If they are not additively interact, we have to use the weighted 2FE estimator \citep{imai2021use}
  \item
    Typically seen in the \protect\hyperlink{staggered-dif-n-dif}{Staggered Dif-n-dif}
  \end{itemize}
\item
  No anticipation: There is no causal effect of the treatment before its implementation.
\end{itemize}

\textbf{Possible issues}

\begin{itemize}
\item
  Estimate dependent on functional form:

  \begin{itemize}
  \tightlist
  \item
    When the size of the response depends (nonlinearly) on the size of the intervention, we might want to look at the the difference in the group with high intensity vs.~low.
  \end{itemize}
\item
  Selection on (time--varying) unobservables

  \begin{itemize}
  \tightlist
  \item
    Can use the overall sensitivity of coefficient estimates to hidden bias using \protect\hyperlink{rosenbaum-bounds}{Rosenbaum Bounds}
  \end{itemize}
\item
  Long-term effects

  \begin{itemize}
  \tightlist
  \item
    Parallel trends are more likely to be observed over shorter period (window of observation)
  \end{itemize}
\item
  Heterogeneous effects

  \begin{itemize}
  \tightlist
  \item
    Different intensity (e.g., doses) for different groups.
  \end{itemize}
\item
  Ashenfelter dip \citep{ashenfelter1985} (job training program participant are more likely to experience an earning drop prior enrolling in these programs)

  \begin{itemize}
  \tightlist
  \item
    Participants are systemically different from nonparticipants before the treatment, leading to the question of permanent or transitory changes.
  \item
    A fix to this transient endogeneity is to calculate long-run differences (exclude a number of periods symmetrically around the adoption/ implementation date). If we see a sustained impact, then we have strong evidence for the causal impact of a policy. \citep{proserpio2017} \citep{heckman1999c} \citep{jepsen2014} \citep{li2011}
  \end{itemize}
\item
  Response to event might not be immediate (can't be observed right away in the dependent variable)

  \begin{itemize}
  \tightlist
  \item
    Using lagged dependent variable \(Y_{it-1}\) might be more appropriate \citep{blundell1998initial}
  \end{itemize}
\item
  Other factors that affect the difference in trends between the two groups (i.e., treatment and control) will bias your estimation.
\item
  Correlated observations within a group or time
\item
  Incidental parameters problems \citep{lancaster2000incidental}: it's always better to use individual and time fixed effect.
\item
  When examining the effects of variation in treatment timing, we have to be careful because negative weights (per group) can be negative if there is a heterogeneity in the treatment effects over time. Example: {[}\citet{athey2022design}{]}\citep{borusyak2021revisiting}\citep{goodman2021difference}. In this case you should use new estimands proposed by \protect\hyperlink{callaway2021difference}{@callaway2021difference}\citep{de2020two}, in the \texttt{did} package. If you expect lags and leads, see \citep{sun2021estimating}
\item
  \citep{gibbons2018broken} caution when we suspect the treatment effect and treatment variance vary across groups
\end{itemize}

\hypertarget{prior-parallel-trends-test}{%
\subsection{Prior Parallel Trends Test}\label{prior-parallel-trends-test}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Plot the average outcomes over time for both treatment and control group before and after the treatment in time.
\item
  Statistical test for difference in trends (\textbf{using data from before the treatment period})
\end{enumerate}

\[
Y = \alpha_g + \beta_1 T + \beta_2 T\times G + \epsilon
\]

where

\begin{itemize}
\item
  \(Y\) = the outcome variable
\item
  \(\alpha_g\) = group fixed effects
\item
  \(T\) = time (e.g., specific year, or month)
\item
  \(\beta_2\) = different time trends for each group
\end{itemize}

Hence, if \(\beta_2 =0\) provides evidence that there are no differences in the trend for the two groups prior the time treatment.

You can also use different functional forms (e..g, polynomial or nonlinear).

If \(\beta_2 \neq 0\) statistically, possible reasons can be:

\begin{itemize}
\item
  Statistical significance can be driven by large sample
\item
  Or the trends are so consistent, and just one period deviation can throw off the trends. Hence, statistical statistical significance.
\end{itemize}

Technically, we can still salvage the research by including time fixed effects, instead of just the before-and-after time fixed effect (actually, most researchers do this mechanically anyway nowadays). However, a side effect can be that the time fixed effects can also absorb some part your treatment effect as well, especially in cases where the treatment effects vary with time (i.e., stronger or weaker over time) \citep{wolfers2003business}.

Debate:

\begin{itemize}
\item
  \citep{kahn2020promise} argue that DiD will be more plausible when the treatment and control groups are similar not only in \textbf{trends}, but also in \textbf{levels}. Because when we observe dissimilar in levels prior to the treatment, why is it okay to think that this will not affect future trends?

  \begin{itemize}
  \item
    Show a plot of the dependent variable's time series for treated and control groups and also a similar plot with matched sample. \citep{ryan2019now} show evidence of matched DiD did well in the setting of non-parallel trends (at least in health care setting).
  \item
    In the case that we don't have similar levels ex ante between treatment and control groups, functional form assumptions matter and we need justification for our choice.
  \end{itemize}
\item
  Pre-trend statistical tests: \citep{roth2022pretest} provides evidence that these test are usually under powered.

  \begin{itemize}
  \tightlist
  \item
    See \href{https://github.com/jonathandroth/PretrendsPower}{PretrendsPower} and \href{https://github.com/jonathandroth/pretrends}{pretrends} packages for correcting this.
  \end{itemize}
\item
  Parallel trends assumption is specific to both the transformation and units of the outcome \citep{roth2023parallel}

  \begin{itemize}
  \tightlist
  \item
    See falsification test (\(H_0\): parallel trends is insensitive to functional form).
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(fixest)}
\NormalTok{od }\OtherTok{\textless{}{-}}\NormalTok{ causaldata}\SpecialCharTok{::}\NormalTok{organ\_donations }\SpecialCharTok{\%\textgreater{}\%}
    \CommentTok{\# Use only pre{-}treatment data}
    \FunctionTok{filter}\NormalTok{(Quarter\_Num }\SpecialCharTok{\textless{}=} \DecValTok{3}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
    \CommentTok{\# Treatment variable}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{California =}\NormalTok{ State }\SpecialCharTok{==} \StringTok{\textquotesingle{}California\textquotesingle{}}\NormalTok{)}

\CommentTok{\# use my package}
\NormalTok{causalverse}\SpecialCharTok{::}\FunctionTok{plot\_par\_trends}\NormalTok{(}
    \AttributeTok{data =}\NormalTok{ od,}
    \AttributeTok{metrics\_and\_names =} \FunctionTok{list}\NormalTok{(}\StringTok{"Rate"} \OtherTok{=} \StringTok{"Rate"}\NormalTok{),}
    \AttributeTok{treatment\_status\_var =} \StringTok{"California"}\NormalTok{,}
    \AttributeTok{time\_var =} \FunctionTok{list}\NormalTok{(}\AttributeTok{Quarter\_Num =} \StringTok{"Time"}\NormalTok{),}
    \AttributeTok{display\_CI =}\NormalTok{ F}
\NormalTok{)}
\CommentTok{\#\textgreater{} [[1]]}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-49-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# do it manually}
\CommentTok{\# always good but plot the dependent out}
\NormalTok{od }\SpecialCharTok{|\textgreater{}}
    \CommentTok{\# group by treatment status and time}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{group\_by}\NormalTok{(California, Quarter) }\SpecialCharTok{|\textgreater{}}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{summarize\_all}\NormalTok{(mean) }\SpecialCharTok{|\textgreater{}}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
    \CommentTok{\# view()}
    
\NormalTok{    ggplot2}\SpecialCharTok{::}\FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Quarter\_Num, }\AttributeTok{y =}\NormalTok{ Rate, }\AttributeTok{color =}\NormalTok{ California)) }\SpecialCharTok{+}
\NormalTok{    ggplot2}\SpecialCharTok{::}\FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
\NormalTok{    causalverse}\SpecialCharTok{::}\FunctionTok{ama\_theme}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-49-2} \end{center}

\begin{Shaded}
\begin{Highlighting}[]


\CommentTok{\# but it\textquotesingle{}s also important to use statistical test}
\NormalTok{prior\_trend }\OtherTok{\textless{}{-}}\NormalTok{ fixest}\SpecialCharTok{::}\FunctionTok{feols}\NormalTok{(Rate }\SpecialCharTok{\textasciitilde{}} \FunctionTok{i}\NormalTok{(Quarter\_Num, California) }\SpecialCharTok{|}\NormalTok{ State }\SpecialCharTok{+}\NormalTok{ Quarter,}
               \AttributeTok{data =}\NormalTok{ od)}

\NormalTok{fixest}\SpecialCharTok{::}\FunctionTok{coefplot}\NormalTok{(prior\_trend, }\AttributeTok{grid =}\NormalTok{ F)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-49-3} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fixest}\SpecialCharTok{::}\FunctionTok{iplot}\NormalTok{(prior\_trend, }\AttributeTok{grid =}\NormalTok{ F)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-49-4} \end{center}

This is alarming since one of the periods is significantly different from 0, which means that our parallel trends assumption is not plausible.

In cases where you might have violations of parallel trends assumption, check \citep{rambachan2023more}

\begin{itemize}
\item
  Impose restrictions on how different the post-treatment violations of parallel trends can be from the pre-trends.
\item
  Partial identification of causal parameter
\item
  Sensitivity analysis
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# https://github.com/asheshrambachan/HonestDiD}
\CommentTok{\# remotes::install\_github("asheshrambachan/HonestDiD")}
\CommentTok{\# library(HonestDiD)}
\end{Highlighting}
\end{Shaded}

Alternatively, \citet{ban2022generalized} propose a method that with an information set (i.e., pre-treatment covariates), and an assumption on the selection bias in the post-treatment period (i.e., lies within the convex hull of all selection biases), they can still identify a set of ATT, and with stricter assumption on selection bias from the policymakers perspective, they can also have a point estimate.

Alternatively, we can use the \texttt{pretrends} package to examine our assumptions \citep{roth2022pretest}

\hypertarget{placebo-test-1}{%
\subsection{Placebo Test}\label{placebo-test-1}}

Procedure:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Sample data only in the period before the treatment in time.
\item
  Consider different fake cutoff in time, either

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    Try the whole sequence in time
  \item
    Generate random treatment period, and use \textbf{randomization inference} to account for sampling distribution of the fake effect.
  \end{enumerate}
\item
  Estimate the DiD model but with the post-time = 1 with the fake cutoff
\item
  A significant DiD coefficient means that you violate the parallel trends! You have a big problem.
\end{enumerate}

Alternatively,

\begin{itemize}
\tightlist
\item
  When data have multiple control groups, drop the treated group, and assign another control group as a ``fake'' treated group. But even if it fails (i.e., you find a significant DiD effect) among the control groups, it can still be fine. However, this method is used under \protect\hyperlink{synthetic-control}{Synthetic Control}
\end{itemize}

\href{https://theeffectbook.net/ch-DifferenceinDifference.html}{Code by theeffectbook.net}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(fixest)}

\NormalTok{od }\OtherTok{\textless{}{-}}\NormalTok{ causaldata}\SpecialCharTok{::}\NormalTok{organ\_donations }\SpecialCharTok{\%\textgreater{}\%}
    \CommentTok{\# Use only pre{-}treatment data}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(Quarter\_Num }\SpecialCharTok{\textless{}=} \DecValTok{3}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    
    \CommentTok{\# Create fake treatment variables}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}
        \AttributeTok{FakeTreat1 =}\NormalTok{ State }\SpecialCharTok{==} \StringTok{\textquotesingle{}California\textquotesingle{}} \SpecialCharTok{\&}
\NormalTok{            Quarter }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Q12011\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Q22011\textquotesingle{}}\NormalTok{),}
        \AttributeTok{FakeTreat2 =}\NormalTok{ State }\SpecialCharTok{==} \StringTok{\textquotesingle{}California\textquotesingle{}} \SpecialCharTok{\&}
\NormalTok{            Quarter }\SpecialCharTok{==} \StringTok{\textquotesingle{}Q22011\textquotesingle{}}
\NormalTok{    )}


\NormalTok{clfe1 }\OtherTok{\textless{}{-}}\NormalTok{ fixest}\SpecialCharTok{::}\FunctionTok{feols}\NormalTok{(Rate }\SpecialCharTok{\textasciitilde{}}\NormalTok{ FakeTreat1 }\SpecialCharTok{|}\NormalTok{ State }\SpecialCharTok{+}\NormalTok{ Quarter,}
               \AttributeTok{data =}\NormalTok{ od)}
\NormalTok{clfe2 }\OtherTok{\textless{}{-}}\NormalTok{ fixest}\SpecialCharTok{::}\FunctionTok{feols}\NormalTok{(Rate }\SpecialCharTok{\textasciitilde{}}\NormalTok{ FakeTreat2 }\SpecialCharTok{|}\NormalTok{ State }\SpecialCharTok{+}\NormalTok{ Quarter,}
               \AttributeTok{data =}\NormalTok{ od)}

\NormalTok{fixest}\SpecialCharTok{::}\FunctionTok{etable}\NormalTok{(clfe1,clfe2)}
\CommentTok{\#\textgreater{}                           clfe1            clfe2}
\CommentTok{\#\textgreater{} Dependent Var.:            Rate             Rate}
\CommentTok{\#\textgreater{}                                                 }
\CommentTok{\#\textgreater{} FakeTreat1TRUE  0.0061 (0.0051)                 }
\CommentTok{\#\textgreater{} FakeTreat2TRUE                  {-}0.0017 (0.0028)}
\CommentTok{\#\textgreater{} Fixed{-}Effects:  {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} State                       Yes              Yes}
\CommentTok{\#\textgreater{} Quarter                     Yes              Yes}
\CommentTok{\#\textgreater{} \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}
\CommentTok{\#\textgreater{} S.E.: Clustered       by: State        by: State}
\CommentTok{\#\textgreater{} Observations                 81               81}
\CommentTok{\#\textgreater{} R2                      0.99377          0.99376}
\CommentTok{\#\textgreater{} Within R2               0.00192          0.00015}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes: 0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

We would like the ``supposed'' DiD to be insignificant.

\hypertarget{assumption-violations}{%
\subsection{Assumption Violations}\label{assumption-violations}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Endogenous Timing
\end{enumerate}

If the timing of units can be influenced by strategic decisions in a DID analysis, an instrumental variable approach with a control function can be used to control for endogeneity in timing.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Questionable Counterfactuals
\end{enumerate}

In situations where the control units may not serve as a reliable counterfactual for the treated units, matching methods such as propensity score matching or generalized random forest can be utilized. Additional methods can be found in \protect\hyperlink{matching-methods}{Matching Methods}.

\hypertarget{robustness-checks}{%
\subsection{Robustness Checks}\label{robustness-checks}}

\begin{itemize}
\item
  Placebo DiD (if the DiD estimate \(\neq 0\), parallel trend is violated, and original DiD is biased):

  \begin{itemize}
  \item
    Group: Use fake treatment groups: A population that was \textbf{not} affect by the treatment
  \item
    Time: Redo the DiD analysis for period before the treatment (expected treatment effect is 0) (e.g., for previous year or period).
  \end{itemize}
\item
  Possible alternative control group: Expected results should be similar
\item
  Try different windows (further away from the treatment point, other factors can creep in and nullify your effect).
\item
  Treatment Reversal (what if we don't see the treatment event)
\item
  Higher-order polynomial time trend (to relax linearity assumption)
\item
  Test whether other dependent variables that should not be affected by the event are indeed unaffected.

  \begin{itemize}
  \tightlist
  \item
    Use the same control and treatment period (DiD \(\neq0\), there is a problem)
  \end{itemize}
\item
  The \textbf{triple-difference strategy} involves examining the interaction between the \textbf{treatment variable} and \textbf{the probability of being affected by the program}, and the group-level participation rate. The identification assumption is that there are no differential trends between high and low participation groups in early versus late implementing countries.
\end{itemize}

\hypertarget{changes-in-changes}{%
\chapter{Changes-in-Changes}\label{changes-in-changes}}

\begin{itemize}
\item
  \textbf{Introduction}

  \begin{itemize}
  \tightlist
  \item
    The Changes-in-Changes (CiC) estimator, introduced by \citet{athey2006identification}, is an alternative to the Difference-in-Differences (DiD) strategy.
  \item
    Unlike traditional DiD, which estimates the Average Treatment Effect on the Treated (ATT), CiC focuses on the Quantile Treatment Effect on the Treated (QTT).
  \item
    QTT captures the difference between potential outcome distributions for treated units at a specific quantile.
  \item
    \textbf{Beyond Averages:} Policymakers often look beyond average program impacts, considering how benefits are distributed across different groups.

    \begin{itemize}
    \tightlist
    \item
      \textbf{Job Training Example:} Two programs with the same negative average impact may be treated differently: one benefiting high earners might be rejected, while one benefiting low earners could be approved.
    \item
      \textbf{Traditional Methods' Limitations:} Methods like linear regression, which assume uniform effects, fail to capture important distributional differences.
    \item
      \textbf{QTEs' Advantage:} QTE methods are tailored for analyzing how treatment effects vary across different segments of a population.
    \end{itemize}
  \item
    \textbf{QTE vs.~ATE:} While QTEs provide detailed insights into distributional impacts, they also allow for the recovery of ATEs. However, ATEs are usually identified under weaker assumptions, making QTEs more suitable for exploring the shape of treatment effects rather than just their central tendency.
  \end{itemize}
\item
  \textbf{Key Concepts}

  \begin{itemize}
  \tightlist
  \item
    \textbf{Quantile Treatment Effect on the Treated (QTT):} Difference in quantiles of treated units' potential outcome distributions.
  \item
    \textbf{Rank Preservation:} Assumes each unit's rank remains constant across potential outcome distributions---this is a strong assumption.
  \item
    \textbf{Counterfactual Distribution:} Estimation focuses on determining this distribution for the treated units in period 1.
  \end{itemize}
\item
  \textbf{Estimating QTT}

  \begin{itemize}
  \tightlist
  \item
    CiC uses four distributions from a 2x2 DiD design:

    \begin{enumerate}
    \def\labelenumi{\arabic{enumi}.}
    \tightlist
    \item
      \(F_{Y(0),00}\): CDF of \(Y(0)\) for control units in period 0.
    \item
      \(F_{Y(0),10}\): CDF of \(Y(0)\) for treatment units in period 0.
    \item
      \(F_{Y(0),01}\): CDF of \(Y(0)\) for control units in period 1.
    \item
      \(F_{Y(1),11}\): CDF of \(Y(1)\) for treatment units in period 1.
    \end{enumerate}
  \item
    QTT is defined as the difference between the inverses of \(F_{Y(1),11}\) and the counterfactual distribution \(F_{Y(0),11}\) at quantile \(q\):
  \end{itemize}

  \[
    \Delta_\theta^{QTT} = F_{Y(1), 11}^{-1} (\theta) - F_{Y (0), 11}^{-1} (\theta)
    \]
\item
  \textbf{Estimation Process}

  \begin{itemize}
  \tightlist
  \item
    \textbf{Counterfactual CDF:}
  \end{itemize}

  \[
    \hat{F}_{Y(0),11}(y) = F_{y,01}\left(F^{-1}_{y,00}\left(F_{y,10}(y)\right)\right)
    \]

  \begin{itemize}
  \tightlist
  \item
    \textbf{Equivalent Expression:}
  \end{itemize}

  \[
    \hat{F}^{-1}_{Y(0),11}(\theta) = F^{-1}_{y,01}\left(F_{y,00}\left(F^{-1}_{y,10}(\theta)\right)\right)
    \]

  \begin{itemize}
  \tightlist
  \item
    \textbf{Treatment Effect Estimate:}
  \end{itemize}

  \[
    \hat{\Delta}^{CIC}_{\theta} = F^{-1}_{Y(1),11}(\theta) - \hat{F}^{-1}_{Y(0),11}(\theta)
    \]

  \begin{itemize}
  \tightlist
  \item
    \textbf{Equivalently:}
  \end{itemize}

  \(\Delta^{CIC}_{\theta}\) is the difference between two QTE estimates:

  \[
    \Delta^{CIC}_{\theta} = \Delta^{QTE}_{\theta,1} - \Delta^{QTE}_{\theta',0}
    \]

  where:

  \begin{itemize}
  \tightlist
  \item
    \(\Delta^{QTT}_{\theta,1}\) = change over time in \(y\) at quantile \(\theta\) for \(D = 1\) group.
  \item
    \(\Delta^{QTU}_{\theta',0}\) = change over time in \(y\) at quantile \(\theta'\) for \(D = 0\) group, where \(q'\) is the quantile in the \(D = 0, T = 0\) distribution corresponding to the value of \(y\) associated with quantile \(\theta\) in the \(D = 1, T = 0\) distribution.
  \end{itemize}
\item
  \textbf{Marketing Example}

  \begin{itemize}
  \tightlist
  \item
    Suppose a company implements a new online marketing strategy aimed at improving customer retention rates.
  \item
    \textbf{QTT:} The goal is to estimate the effect of the strategy on customer retention rates at different quantiles (e.g., median retention rate).
  \item
    \textbf{Rank Preservation:} Assumes customers' rank in retention distribution remains the same, regardless of the strategy---this assumption is strong and should be carefully considered.
  \item
    \textbf{Counterfactual:} CiC helps estimate how retention rates would have changed without the new strategy by comparing it with a control group.
  \end{itemize}
\item
  \textbf{References}

  \begin{itemize}
  \tightlist
  \item
    \citet{athey2006identification}
  \item
    \citet{frolich2013unconditional}: IV-based
  \item
    \citet{callaway2019quantile}: panel data
  \item
    \citet{huber2022direct}
  \end{itemize}
\item
  \textbf{Additional Resources}

  \begin{itemize}
  \tightlist
  \item
    Code examples available in \href{https://sites.google.com/site/blaisemelly/home/computer-programs/cic_stata}{Stata}.
  \end{itemize}
\end{itemize}

\hypertarget{application-7}{%
\section{Application}\label{application-7}}

\hypertarget{ecic-package}{%
\subsection{ECIC package}\label{ecic-package}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ecic)}
\FunctionTok{data}\NormalTok{(dat, }\AttributeTok{package =} \StringTok{"ecic"}\NormalTok{)}
\NormalTok{mod }\OtherTok{=}
  \FunctionTok{ecic}\NormalTok{(}
    \AttributeTok{yvar  =}\NormalTok{ lemp,         }\CommentTok{\# dependent variable}
    \AttributeTok{gvar  =}\NormalTok{ first.treat,  }\CommentTok{\# group indicator}
    \AttributeTok{tvar  =}\NormalTok{ year,         }\CommentTok{\# time indicator}
    \AttributeTok{ivar  =}\NormalTok{ countyreal,   }\CommentTok{\# unit ID}
    \AttributeTok{dat   =}\NormalTok{ dat,          }\CommentTok{\# dataset}
    \AttributeTok{boot  =} \StringTok{"weighted"}\NormalTok{,   }\CommentTok{\# bootstrap proceduce ("no", "normal", or "weighted")}
    \AttributeTok{nReps =} \DecValTok{3}            \CommentTok{\# number of bootstrap runs}
\NormalTok{    )}
\NormalTok{mod\_res }\OtherTok{\textless{}{-}} \FunctionTok{summary}\NormalTok{(mod)}
\NormalTok{mod\_res}
\CommentTok{\#\textgreater{}   perc    coefs          se}
\CommentTok{\#\textgreater{} 1  0.1 1.206140 0.021351711}
\CommentTok{\#\textgreater{} 2  0.2 1.316599 0.009225026}
\CommentTok{\#\textgreater{} 3  0.3 1.449963 0.001859468}
\CommentTok{\#\textgreater{} 4  0.4 1.583415 0.015296156}
\CommentTok{\#\textgreater{} 5  0.5 1.739932 0.011240454}
\CommentTok{\#\textgreater{} 6  0.6 1.915558 0.013060348}
\CommentTok{\#\textgreater{} 7  0.7 2.114966 0.014482208}
\CommentTok{\#\textgreater{} 8  0.8 2.363105 0.005173865}
\CommentTok{\#\textgreater{} 9  0.9 2.779202 0.020831180}

\FunctionTok{ecic\_plot}\NormalTok{(mod\_res)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{27-changes-in-changes_files/figure-latex/unnamed-chunk-1-1} \end{center}

\hypertarget{qte-package}{%
\subsection{QTE package}\label{qte-package}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(qte)}
\FunctionTok{data}\NormalTok{(lalonde)}

\CommentTok{\# randomized setting}
\CommentTok{\# qte is identical to qtet}
\NormalTok{jt.rand }\OtherTok{\textless{}{-}}
    \FunctionTok{ci.qtet}\NormalTok{(}
\NormalTok{        re78 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ treat,}
        \AttributeTok{data =}\NormalTok{ lalonde.exp,}
        \AttributeTok{iters =} \DecValTok{10}
\NormalTok{    )}
\FunctionTok{summary}\NormalTok{(jt.rand)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Quantile Treatment Effect:}
\CommentTok{\#\textgreater{}      }
\CommentTok{\#\textgreater{} tau  QTE Std. Error}
\CommentTok{\#\textgreater{} 0.05    0.00    0.00}
\CommentTok{\#\textgreater{} 0.1     0.00    0.00}
\CommentTok{\#\textgreater{} 0.15    0.00    0.00}
\CommentTok{\#\textgreater{} 0.2     0.00  179.39}
\CommentTok{\#\textgreater{} 0.25  338.65  310.66}
\CommentTok{\#\textgreater{} 0.3   846.40  403.44}
\CommentTok{\#\textgreater{} 0.35 1451.51  550.40}
\CommentTok{\#\textgreater{} 0.4  1177.72  832.39}
\CommentTok{\#\textgreater{} 0.45 1396.08  958.97}
\CommentTok{\#\textgreater{} 0.5  1123.55  923.72}
\CommentTok{\#\textgreater{} 0.55 1181.54  746.56}
\CommentTok{\#\textgreater{} 0.6  1466.51  657.59}
\CommentTok{\#\textgreater{} 0.65 2115.04  561.46}
\CommentTok{\#\textgreater{} 0.7  1795.12  463.82}
\CommentTok{\#\textgreater{} 0.75 2347.49  824.58}
\CommentTok{\#\textgreater{} 0.8  2278.12 1111.52}
\CommentTok{\#\textgreater{} 0.85 2178.28  934.05}
\CommentTok{\#\textgreater{} 0.9  3239.60 1313.89}
\CommentTok{\#\textgreater{} 0.95 3979.62 1440.05}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Average Treatment Effect:    1794.34}
\CommentTok{\#\textgreater{}   Std. Error:        545.04}
\FunctionTok{ggqte}\NormalTok{(jt.rand)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{27-changes-in-changes_files/figure-latex/unnamed-chunk-2-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# conditional independence assumption (CIA)}
\NormalTok{jt.cia }\OtherTok{\textless{}{-}} \FunctionTok{ci.qte}\NormalTok{(}
\NormalTok{    re78 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ treat,}
    \AttributeTok{xformla =}  \SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ education,}
    \AttributeTok{data =}\NormalTok{ lalonde.psid,}
    \AttributeTok{iters =} \DecValTok{10}
\NormalTok{)}
\FunctionTok{summary}\NormalTok{(jt.cia)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Quantile Treatment Effect:}
\CommentTok{\#\textgreater{}      }
\CommentTok{\#\textgreater{} tau  QTE Std. Error}
\CommentTok{\#\textgreater{} 0.05      0.00        0.00}
\CommentTok{\#\textgreater{} 0.1       0.00       69.16}
\CommentTok{\#\textgreater{} 0.15  {-}4433.18     1216.49}
\CommentTok{\#\textgreater{} 0.2   {-}8219.15      573.30}
\CommentTok{\#\textgreater{} 0.25 {-}10435.74      800.14}
\CommentTok{\#\textgreater{} 0.3  {-}12232.03     1306.61}
\CommentTok{\#\textgreater{} 0.35 {-}12428.30     1789.50}
\CommentTok{\#\textgreater{} 0.4  {-}14195.24     1626.30}
\CommentTok{\#\textgreater{} 0.45 {-}14248.66     2166.41}
\CommentTok{\#\textgreater{} 0.5  {-}15538.67     2505.12}
\CommentTok{\#\textgreater{} 0.55 {-}16550.71     2576.07}
\CommentTok{\#\textgreater{} 0.6  {-}15595.02     2724.52}
\CommentTok{\#\textgreater{} 0.65 {-}15827.52     3396.82}
\CommentTok{\#\textgreater{} 0.7  {-}16090.32     3480.56}
\CommentTok{\#\textgreater{} 0.75 {-}16091.49     3148.62}
\CommentTok{\#\textgreater{} 0.8  {-}17864.76     3657.43}
\CommentTok{\#\textgreater{} 0.85 {-}16756.71     3178.53}
\CommentTok{\#\textgreater{} 0.9  {-}17914.99     2856.43}
\CommentTok{\#\textgreater{} 0.95 {-}23646.22     3061.80}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Average Treatment Effect:    {-}13435.40}
\CommentTok{\#\textgreater{}   Std. Error:        1637.45}
\FunctionTok{ggqte}\NormalTok{(jt.cia)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{27-changes-in-changes_files/figure-latex/unnamed-chunk-3-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{jt.ciat }\OtherTok{\textless{}{-}} \FunctionTok{ci.qtet}\NormalTok{(}
\NormalTok{    re78 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ treat,}
    \AttributeTok{xformla =}  \SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ education,}
    \AttributeTok{data =}\NormalTok{ lalonde.psid,}
    \AttributeTok{iters =} \DecValTok{10}
\NormalTok{)}
\FunctionTok{summary}\NormalTok{(jt.ciat)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Quantile Treatment Effect:}
\CommentTok{\#\textgreater{}      }
\CommentTok{\#\textgreater{} tau  QTE Std. Error}
\CommentTok{\#\textgreater{} 0.05      0.00        0.00}
\CommentTok{\#\textgreater{} 0.1   {-}1018.15      547.30}
\CommentTok{\#\textgreater{} 0.15  {-}3251.00     1196.05}
\CommentTok{\#\textgreater{} 0.2   {-}7240.86      546.85}
\CommentTok{\#\textgreater{} 0.25  {-}8379.94      301.65}
\CommentTok{\#\textgreater{} 0.3   {-}8758.82      439.07}
\CommentTok{\#\textgreater{} 0.35  {-}9897.44      539.91}
\CommentTok{\#\textgreater{} 0.4  {-}10239.57      654.74}
\CommentTok{\#\textgreater{} 0.45 {-}10751.39      836.43}
\CommentTok{\#\textgreater{} 0.5  {-}10570.14      425.39}
\CommentTok{\#\textgreater{} 0.55 {-}11348.96      753.11}
\CommentTok{\#\textgreater{} 0.6  {-}11550.84      982.00}
\CommentTok{\#\textgreater{} 0.65 {-}12203.56     1203.44}
\CommentTok{\#\textgreater{} 0.7  {-}13277.72     1141.45}
\CommentTok{\#\textgreater{} 0.75 {-}14011.74      940.89}
\CommentTok{\#\textgreater{} 0.8  {-}14373.95      980.16}
\CommentTok{\#\textgreater{} 0.85 {-}14499.18     1385.55}
\CommentTok{\#\textgreater{} 0.9  {-}15008.63     1974.96}
\CommentTok{\#\textgreater{} 0.95 {-}15954.05     1701.02}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Average Treatment Effect:    4266.19}
\CommentTok{\#\textgreater{}   Std. Error:        352.88}
\FunctionTok{ggqte}\NormalTok{(jt.ciat)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{27-changes-in-changes_files/figure-latex/unnamed-chunk-3-2} \end{center}

\begin{itemize}
\item
  \textbf{QTE} compares quantiles of the entire population under treatment and control, whereas \textbf{QTET} compares quantiles within the treated group itself. This difference means that QTE reflects the overall population-level impact, while QTET focuses on the treated group's specific impact.
\item
  \textbf{CIA} enables identification of both QTE and QTET, but since QTET is conditional on treatment, it might reflect different effects than QTE, especially when the treatment effect is heterogeneous across different subpopulations. For example, the QTE could show a more generalized effect across all individuals, while the QTET may reveal stronger or weaker effects for the subgroup that actually received the treatment.
\end{itemize}

These are DID-like models

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  With the distributional difference-in-differences assumption \citep[\citet{callaway2019quantile}]{fan2012partial}, which is an extension of the parallel trends assumption, we can estimate QTET.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# distributional DiD assumption}
\NormalTok{jt.pqtet }\OtherTok{\textless{}{-}} \FunctionTok{panel.qtet}\NormalTok{(}
\NormalTok{    re }\SpecialCharTok{\textasciitilde{}}\NormalTok{ treat,}
    \AttributeTok{t =} \DecValTok{1978}\NormalTok{,}
    \AttributeTok{tmin1 =} \DecValTok{1975}\NormalTok{,}
    \AttributeTok{tmin2 =} \DecValTok{1974}\NormalTok{,}
    \AttributeTok{tname =} \StringTok{"year"}\NormalTok{,}
    \AttributeTok{idname =} \StringTok{"id"}\NormalTok{,}
    \AttributeTok{data =}\NormalTok{ lalonde.psid.panel,}
    \AttributeTok{iters =} \DecValTok{10}
\NormalTok{)}
\FunctionTok{summary}\NormalTok{(jt.pqtet)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Quantile Treatment Effect:}
\CommentTok{\#\textgreater{}      }
\CommentTok{\#\textgreater{} tau  QTE Std. Error}
\CommentTok{\#\textgreater{} 0.05  4779.21     1511.78}
\CommentTok{\#\textgreater{} 0.1   1987.35      648.15}
\CommentTok{\#\textgreater{} 0.15   842.95     3420.35}
\CommentTok{\#\textgreater{} 0.2  {-}7366.04     3887.64}
\CommentTok{\#\textgreater{} 0.25 {-}8449.96     1188.96}
\CommentTok{\#\textgreater{} 0.3  {-}7992.15     1183.48}
\CommentTok{\#\textgreater{} 0.35 {-}7429.21     1235.74}
\CommentTok{\#\textgreater{} 0.4  {-}6597.37     1301.13}
\CommentTok{\#\textgreater{} 0.45 {-}5519.45     1327.31}
\CommentTok{\#\textgreater{} 0.5  {-}4702.88     1245.44}
\CommentTok{\#\textgreater{} 0.55 {-}3904.52     1077.21}
\CommentTok{\#\textgreater{} 0.6  {-}2741.80     1199.48}
\CommentTok{\#\textgreater{} 0.65 {-}1507.31     1268.90}
\CommentTok{\#\textgreater{} 0.7   {-}771.12     1260.84}
\CommentTok{\#\textgreater{} 0.75   707.81     1241.03}
\CommentTok{\#\textgreater{} 0.8    580.00      893.78}
\CommentTok{\#\textgreater{} 0.85   821.75     1491.33}
\CommentTok{\#\textgreater{} 0.9   {-}250.77     1904.54}
\CommentTok{\#\textgreater{} 0.95 {-}1874.54     3129.77}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Average Treatment Effect:    2326.51}
\CommentTok{\#\textgreater{}   Std. Error:        544.91}
\FunctionTok{ggqte}\NormalTok{(jt.pqtet)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{27-changes-in-changes_files/figure-latex/unnamed-chunk-4-1} \end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  With 2 periods, the distributional DiD assumption can partially identify QTET with bounds \citep{fan2012partial}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res\_bound }\OtherTok{\textless{}{-}}
    \FunctionTok{bounds}\NormalTok{(}
\NormalTok{        re }\SpecialCharTok{\textasciitilde{}}\NormalTok{ treat,}
        \AttributeTok{t =} \DecValTok{1978}\NormalTok{,}
        \AttributeTok{tmin1 =} \DecValTok{1975}\NormalTok{,}
        \AttributeTok{data =}\NormalTok{ lalonde.psid.panel,}
        \AttributeTok{idname =} \StringTok{"id"}\NormalTok{,}
        \AttributeTok{tname =} \StringTok{"year"}
\NormalTok{    )}
\FunctionTok{summary}\NormalTok{(res\_bound)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Bounds on the Quantile Treatment Effect on the Treated:}
\CommentTok{\#\textgreater{}      }
\CommentTok{\#\textgreater{} tau  Lower Bound Upper Bound}
\CommentTok{\#\textgreater{}         tau  Lower Bound Upper Bound}
\CommentTok{\#\textgreater{}        0.05       {-}51.72           0}
\CommentTok{\#\textgreater{}         0.1     {-}1220.84           0}
\CommentTok{\#\textgreater{}        0.15      {-}1881.9           0}
\CommentTok{\#\textgreater{}         0.2     {-}2601.32           0}
\CommentTok{\#\textgreater{}        0.25     {-}2916.38      485.23}
\CommentTok{\#\textgreater{}         0.3     {-}3080.16      943.05}
\CommentTok{\#\textgreater{}        0.35     {-}3327.89     1505.98}
\CommentTok{\#\textgreater{}         0.4     {-}3240.59     2133.59}
\CommentTok{\#\textgreater{}        0.45     {-}2982.51     2616.84}
\CommentTok{\#\textgreater{}         0.5     {-}3108.01      2566.2}
\CommentTok{\#\textgreater{}        0.55     {-}3342.66     2672.82}
\CommentTok{\#\textgreater{}         0.6      {-}3491.4      3065.7}
\CommentTok{\#\textgreater{}        0.65     {-}3739.74     3349.74}
\CommentTok{\#\textgreater{}         0.7     {-}4647.82     2992.03}
\CommentTok{\#\textgreater{}        0.75     {-}4826.78     3219.32}
\CommentTok{\#\textgreater{}         0.8      {-}5801.7     2702.33}
\CommentTok{\#\textgreater{}        0.85     {-}6588.61     2499.41}
\CommentTok{\#\textgreater{}         0.9     {-}8953.84     2020.84}
\CommentTok{\#\textgreater{}        0.95    {-}14283.61      397.04}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Average Treatment Effect on the Treated: 2326.51}
\FunctionTok{plot}\NormalTok{(res\_bound)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{27-changes-in-changes_files/figure-latex/unnamed-chunk-5-1} \end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  With a restrictive assumption that difference in the quantiles of the distribution of potential outcomes for the treated and untreated groups be the same for all values of quantiles, we can have the mean DiD model
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{jt.mdid }\OtherTok{\textless{}{-}} \FunctionTok{ddid2}\NormalTok{(}
\NormalTok{    re }\SpecialCharTok{\textasciitilde{}}\NormalTok{ treat,}
    \AttributeTok{t =} \DecValTok{1978}\NormalTok{,}
    \AttributeTok{tmin1 =} \DecValTok{1975}\NormalTok{,}
    \AttributeTok{tname =} \StringTok{"year"}\NormalTok{,}
    \AttributeTok{idname =} \StringTok{"id"}\NormalTok{,}
    \AttributeTok{data =}\NormalTok{ lalonde.psid.panel,}
    \AttributeTok{iters =} \DecValTok{10}
\NormalTok{)}
\FunctionTok{summary}\NormalTok{(jt.mdid)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Quantile Treatment Effect:}
\CommentTok{\#\textgreater{}      }
\CommentTok{\#\textgreater{} tau  QTE Std. Error}
\CommentTok{\#\textgreater{} 0.05 10616.61      551.36}
\CommentTok{\#\textgreater{} 0.1   5019.83      323.38}
\CommentTok{\#\textgreater{} 0.15  2388.12      229.69}
\CommentTok{\#\textgreater{} 0.2   1033.23      226.29}
\CommentTok{\#\textgreater{} 0.25   485.23      302.16}
\CommentTok{\#\textgreater{} 0.3    943.05      227.36}
\CommentTok{\#\textgreater{} 0.35   931.45      562.40}
\CommentTok{\#\textgreater{} 0.4    945.35      925.57}
\CommentTok{\#\textgreater{} 0.45  1205.88      820.33}
\CommentTok{\#\textgreater{} 0.5   1362.11      618.99}
\CommentTok{\#\textgreater{} 0.55  1279.05      615.60}
\CommentTok{\#\textgreater{} 0.6   1618.13      742.07}
\CommentTok{\#\textgreater{} 0.65  1834.30      961.33}
\CommentTok{\#\textgreater{} 0.7   1326.06     1078.10}
\CommentTok{\#\textgreater{} 0.75  1586.35      913.81}
\CommentTok{\#\textgreater{} 0.8   1256.09      796.22}
\CommentTok{\#\textgreater{} 0.85   723.10     1304.06}
\CommentTok{\#\textgreater{} 0.9    251.36     1904.49}
\CommentTok{\#\textgreater{} 0.95 {-}1509.92     3034.49}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Average Treatment Effect:    2326.51}
\CommentTok{\#\textgreater{}   Std. Error:        712.66}
\FunctionTok{plot}\NormalTok{(jt.mdid)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{27-changes-in-changes_files/figure-latex/unnamed-chunk-6-1} \end{center}

On top of the distributional DiD assumption, we need \textbf{copula stability} assumption (i.e., If, before the treatment, the units with the highest outcomes were improving the most, we would expect to see them improving the most in the current period too.) for these models:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3232}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3434}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{QDiD}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{CiC}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Treatment of Time and Group} & Symmetric & Asymmetric \\
\textbf{QTET Computation} & Not inherently scale-invariant & Outcome Variable Scale-Invariant \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{jt.qdid }\OtherTok{\textless{}{-}} \FunctionTok{QDiD}\NormalTok{(}
\NormalTok{    re }\SpecialCharTok{\textasciitilde{}}\NormalTok{ treat,}
    \AttributeTok{t =} \DecValTok{1978}\NormalTok{,}
    \AttributeTok{tmin1 =} \DecValTok{1975}\NormalTok{,}
    \AttributeTok{tname =} \StringTok{"year"}\NormalTok{,}
    \AttributeTok{idname =} \StringTok{"id"}\NormalTok{,}
    \AttributeTok{data =}\NormalTok{ lalonde.psid.panel,}
    \AttributeTok{iters =} \DecValTok{10}\NormalTok{,}
    \AttributeTok{panel =}\NormalTok{ T}
\NormalTok{)}

\NormalTok{jt.cic }\OtherTok{\textless{}{-}} \FunctionTok{CiC}\NormalTok{(}
\NormalTok{    re }\SpecialCharTok{\textasciitilde{}}\NormalTok{ treat,}
    \AttributeTok{t =} \DecValTok{1978}\NormalTok{,}
    \AttributeTok{tmin1 =} \DecValTok{1975}\NormalTok{,}
    \AttributeTok{tname =} \StringTok{"year"}\NormalTok{,}
    \AttributeTok{idname =} \StringTok{"id"}\NormalTok{,}
    \AttributeTok{data =}\NormalTok{ lalonde.psid.panel,}
    \AttributeTok{iters =} \DecValTok{10}\NormalTok{,}
    \AttributeTok{panel =}\NormalTok{ T}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{synthetic-control}{%
\chapter{Synthetic Control}\label{synthetic-control}}

Examples in marketing:

\begin{itemize}
\tightlist
\item
  \citep{tirunillai2017}: offline TV ad on Online Chatter
\item
  \citep{wang2019mobile}: mobile hailing technology adoption on drivers' hourly earnings
\item
  \citep{guo2020let}: payment disclosure laws effect on physician prescription behavior using Timing of the Massachusetts open payment law as the exogenous shock
\item
  \citep{adalja2023gmo}: mandatory GMO labels had no impact on consumer demand (Using Vermont as a mandatory state)
\end{itemize}

\textbf{Notes}

\begin{itemize}
\item
  The SC method provides asymptotically normal estimators for various linear panel data models, given sufficiently large pre-treatment periods, making it a natural alternative to the \protect\hyperlink{difference-in-differences}{Difference-in-differences} model \citep{arkhangelsky2023large}.
\item
  SCM is \textbf{superior} than \protect\hyperlink{matching-methods}{Matching Methods} because it not only matches on covariates (i.e., pre-treatment variables), but also outcomes.
\item
  For a review of the method, see \citep{abadie2021using}
\item
  SCMs can also be used under the Bayesian framework (\protect\hyperlink{bayesian-synthetic-control}{Bayesian Synthetic Control}) where we do not have to impose any restrictive priori \citep{kim2020bayesian}
\item
  Different from \protect\hyperlink{matching-methods}{Matching Methods} because SCMs match on the pre-treatment outcomes in each period while \protect\hyperlink{matching-methods}{Matching Methods} match on the number of covariates.
\item
  A data driven procedure to construct more comparable control groups (i.e., black box).
\item
  To do causal inference with control and treatment group using \protect\hyperlink{matching-methods}{Matching Methods}, you typically have to have similar covariates in the control and the treated groups. However, if you don't methods like \protect\hyperlink{propensity-scores}{Propensity Scores} and DID can perform rather poorly (i.e., large bias).
\end{itemize}

Advantages over \protect\hyperlink{difference-in-differences}{Difference-in-differences}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Maximization of the observable similarity between control and treatment (maybe also unobservables)
\item
  Can also be used in cases where no untreated case with similar on matching dimensions with treated cases
\item
  Objective selection of controls.
\end{enumerate}

Advantages over linear regression

\begin{itemize}
\item
  Regression weights for the estimator will be outside of {[}0,1{]} (because regression allows extrapolation), and it will not be sparse (i.e., can be less than 0).
\item
  No extrapolation under SCMs
\item
  Explicitly state the fit (i.e., the weight)
\item
  Can be estimated without the post-treatment outcomes for the control group (can't p-hack)
\end{itemize}

Advantages:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  From the selection criteria, researchers can understand the relative importance of each candidate
\item
  Post-intervention outcomes are not used in synthetic. Hence, you can't retro-fit.
\item
  Observable similarity between control and treatment cases is maximized
\end{enumerate}

Disadvantages:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  It's hard to argue for the weights you use to create the ``synthetic control''
\end{enumerate}

SCM is recommended when

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Social events to evaluate large-scale program or policy
\item
  Only one treated case with several control candidates.
\end{enumerate}

\textbf{Assumptions}

\begin{itemize}
\item
  Donor subject is a good match for the synthetic control (i.e., gap between the dependent of the donor subject and that of the synthetic control should be 0 before treatment)
\item
  Only the treated subject undergoes the treatment and not any of the subjects in the donor pool.
\item
  No other changes to the subjects during the whole window.
\item
  The counterfactual outcome of the treatment group can be imputed in a \textbf{linear combination} of control groups.
\end{itemize}

\textbf{Identification}: The exclusion restriction is met conditional on the pre-treatment outcomes.

\texttt{Synth} provides an algorithm that finds weighted combination of the comparison units where the weights are chosen such that it best resembles the values of predictors of the outcome variable for the affected units before the intervention

Setting (notation followed professor \href{https://conference.nber.org/confer/2021/SI2021/ML/AbadieSlides.pdf}{Alberto Abadie})

\begin{itemize}
\item
  \(J + 1\) units in periods \(1, \dots, T\)
\item
  The first unit is the treated one during \(T_0 + 1, \dots, T\)
\item
  \(J\) units are called a donor pool
\item
  \(Y_{it}^I\) is the outcome for unit \(i\) if it's exposed to the treatment during \(T_0 + 1 , \dots T\)
\item
  \(Y_{it}^N\) is the outcome for unit \(i\) if it's not exposed to the treatment
\end{itemize}

We try to estimate the effect of treatment on the treated unit

\[
\tau_{1t} = Y_{1t}^I - Y_{1t}^N
\]

where we observe the first treated unit already \(Y_{1t}^I = Y_{1t}\)

To construct the synthetic control unit, we have to find appropriate weight for each donor in the donor pool by finding \(\mathbf{W} = (w_2, \dots, w_{J=1})'\) where

\begin{itemize}
\item
  \(w_j \ge 0\) for \(j = 2, \dots, J+1\)
\item
  \(w_2 + \dots + w_{J+1} = 1\)
\end{itemize}

The ``appropriate'' vector \(\mathbf{W}\) here is constrained to

\[
\min||\mathbf{X}_1 - \mathbf{X}_0 \mathbf{W}||
\]

where

\begin{itemize}
\item
  \(\mathbf{X}_1\) is the \(k \times 1\) vector of pre-treatment characteristics for the treated unit
\item
  \(\mathbf{X}_0\) is the \(k \times J\) matrix of pre-treatment characteristics for the untreated units
\end{itemize}

For simplicity, researchers usually use

\[
\begin{aligned}
&\min||\mathbf{X}_1 - \mathbf{X}_0 \mathbf{W}|| \\
&= (\sum_{h=1}^k v_h(X_{h1}- w_2 X-{h2} - \dots - w_{J+1} X_{hJ +1})^{1/2}
\end{aligned}
\]

where

\begin{itemize}
\tightlist
\item
  \(v_1, \dots, v_k\) is a vector positive constants that represent the predictive power of the \(k\) predictors on \(Y_{1t}^N\) (i.e., the potential outcome of the treated without treatment) and it can be chosen either explicitly by the researcher or by data-driven methods
\end{itemize}

For penalized synthetic control \citep{abadie2021penalized}, the minimization problem becomes

\[
\min_{\mathbf{W}} ||\mathbf{X}_1 - \sum_{j=2}^{J + 1}W_j \mathbf{X}_j ||^2 + \lambda \sum_{j=2}^{J+1} W_j ||\mathbf{X}_1 - \mathbf{X}_j||^2
\]

where

\begin{itemize}
\item
  \(W_j \ge 0\) and \(\sum_{j=2}^{J+1} W_j = 1\)
\item
  \(\lambda >0\) balances over-fitting of the treated and minimize the sum of pairwise distances

  \begin{itemize}
  \item
    \(\lambda \to 0\): pure synthetic control (i.e solution for the unpenalized estimator)
  \item
    \(\lambda \to \infty\): nearest neighbor matching
  \end{itemize}
\end{itemize}

Advantages:

\begin{itemize}
\item
  For \(\lambda >0\), you have unique and sparse solution
\item
  Reduces the interpolation bias when averaging dissimilar units
\item
  Penalized SC never uses dissimilar units
\end{itemize}

Then the synthetic control estimator is

\[
\hat{\tau}_{1t} = Y_{1t} - \sum_{j=2}^{J+1} w_j^* Y_{jt}
\]

where \(Y_{jt}\) is the outcome for unit \(j\) at time \(t\)

Consideration

Under the factor model \citep{abadie2010synthetic}

\[
Y_{it}^N = \mathbf{\theta}_t \mathbf{Z}_i + \mathbf{\lambda}_t \mathbf{\mu}_i + \epsilon_{it}
\]

where

\begin{itemize}
\item
  \(Z_i\) = observables
\item
  \(\mu_i\) = unobservables
\item
  \(\epsilon_{it}\) = unit-level transitory shock (i.e., random noise)
\end{itemize}

with assumptions of \(\mathbf{W}^*\) such that

\[
\begin{aligned}
\sum_{j=2}^{J+1} w_j^* \mathbf{Z}_j  &= \mathbf{Z}_1 \\
&\dots \\
\sum_{j=2}^{J+1} w_j^* Y_{j1} &= Y_{11} \\
\sum_{j=2}^{J+1} w_j^* Y_{jT_0} &= Y_{1T_0}
\end{aligned}
\]

Basically, we assume that the synthetic control is a good counterfactual when the treated unit is not exposed to the treatment.

Then,

\begin{itemize}
\item
  the bias bound depends on close fit, which is controlled by the ratio between \(\epsilon_{it}\) (transitory shock) and \(T_0\) (the number of pre-treatment periods). In other words, you should have good fit for \(Y_{1t}\) for pre-treatment period (i.e., \(T_0\) should be large while small variance in \(\epsilon_{it}\))
\item
  When you have poor fit, you have to use bias correction version of the synthetic control. See \citep[\citet{abadie2021using}, \citet{ben2020varying}]{arkhangelsky2019synthetic}
\item
  Overfitting can be the result of small \(T_0\) (the number of pre-treatment periods), large \(J\) (the number of units in the donor pool), and large \(\epsilon_{it}\) (noise)

  \begin{itemize}
  \tightlist
  \item
    Mitigation: put only similar units (to the treated one) in the donor pool
  \end{itemize}
\end{itemize}

To make inference, we have to create a permutation distribution (by iteratively reassigning the treatment to the units in the donor pool and estimate the placebo effects in each iteration). We say there is an effect of the treatment when the magnitude of value of the treatment effect on the treated unit is extreme relative to the permutation distribution.

It's recommended to use one-sided inference. And the permutation distribution is superior to the p-values alone (because sampling-based inference is hard under SCMs either because of undefined sampling mechanism or the sample is the population).

For benchmark (permutation) distribution (e.g., uniform), see \citep{firpo2018synthetic}

\hypertarget{applications-1}{%
\section{Applications}\label{applications-1}}

\hypertarget{example-1-1}{%
\subsection{Example 1}\label{example-1-1}}

by \href{https://rpubs.com/danilofreire/synth}{Danilo Freire}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# install.packages("Synth")}
\CommentTok{\# install.packages("gsynth")}
\FunctionTok{library}\NormalTok{(}\StringTok{"Synth"}\NormalTok{)}
\FunctionTok{library}\NormalTok{(}\StringTok{"gsynth"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

simulate data for 10 states and 30 years. State A receives the treatment \texttt{T\ =\ 20} after year 15.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{year         }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{30}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{state        }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(LETTERS[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{], }\AttributeTok{each =} \DecValTok{30}\NormalTok{)}
\NormalTok{X1           }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{300}\NormalTok{, }\AttributeTok{mean =} \DecValTok{2}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{), }\DecValTok{2}\NormalTok{)}
\NormalTok{X2           }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(}\FunctionTok{rbinom}\NormalTok{(}\DecValTok{300}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{300}\NormalTok{), }\DecValTok{2}\NormalTok{)}
\NormalTok{Y            }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(}\DecValTok{1} \SpecialCharTok{+} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ X1 }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{300}\NormalTok{), }\DecValTok{2}\NormalTok{)}
\NormalTok{df           }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(Y, X1, X2, state, year))}
\NormalTok{df}\SpecialCharTok{$}\NormalTok{Y         }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{as.character}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{Y))}
\NormalTok{df}\SpecialCharTok{$}\NormalTok{X1        }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{as.character}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{X1))}
\NormalTok{df}\SpecialCharTok{$}\NormalTok{X2        }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{as.character}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{X2))}
\NormalTok{df}\SpecialCharTok{$}\NormalTok{year      }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{as.character}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{year))}
\NormalTok{df}\SpecialCharTok{$}\NormalTok{state.num }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{, }\AttributeTok{each =} \DecValTok{30}\NormalTok{)}
\NormalTok{df}\SpecialCharTok{$}\NormalTok{state     }\OtherTok{\textless{}{-}} \FunctionTok{as.character}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{state)}
\NormalTok{df}\SpecialCharTok{$}\StringTok{\textasciigrave{}}\AttributeTok{T}\StringTok{\textasciigrave{}}       \OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{state }\SpecialCharTok{==} \StringTok{"A"} \SpecialCharTok{\&}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{year }\SpecialCharTok{\textgreater{}=} \DecValTok{15}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{df}\SpecialCharTok{$}\NormalTok{Y         }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{state }\SpecialCharTok{==} \StringTok{"A"} \SpecialCharTok{\&}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{year }\SpecialCharTok{\textgreater{}=} \DecValTok{15}\NormalTok{, }
\NormalTok{                       df}\SpecialCharTok{$}\NormalTok{Y }\SpecialCharTok{+} \DecValTok{20}\NormalTok{, df}\SpecialCharTok{$}\NormalTok{Y)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(df)}
\CommentTok{\#\textgreater{} \textquotesingle{}data.frame\textquotesingle{}:    300 obs. of  7 variables:}
\CommentTok{\#\textgreater{}  $ Y        : num  2.29 4.51 2.07 8.87 4.37 1.32 8 7.49 6.98 3.72 ...}
\CommentTok{\#\textgreater{}  $ X1       : num  1.37 2.18 1.16 3.6 2.33 1.18 2.49 2.74 2.58 1.69 ...}
\CommentTok{\#\textgreater{}  $ X2       : num  1.96 0.4 {-}0.75 {-}0.56 {-}0.45 1.06 0.51 {-}2.1 0 0.54 ...}
\CommentTok{\#\textgreater{}  $ state    : chr  "A" "A" "A" "A" ...}
\CommentTok{\#\textgreater{}  $ year     : num  1 2 3 4 5 6 7 8 9 10 ...}
\CommentTok{\#\textgreater{}  $ state.num: int  1 1 1 1 1 1 1 1 1 1 ...}
\CommentTok{\#\textgreater{}  $ T        : num  0 0 0 0 0 0 0 0 0 0 ...}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataprep.out }\OtherTok{\textless{}{-}}
    \FunctionTok{dataprep}\NormalTok{(}
\NormalTok{        df,}
        \AttributeTok{predictors            =} \FunctionTok{c}\NormalTok{(}\StringTok{"X1"}\NormalTok{, }\StringTok{"X2"}\NormalTok{),}
        \AttributeTok{dependent             =} \StringTok{"Y"}\NormalTok{,}
        \AttributeTok{unit.variable         =} \StringTok{"state.num"}\NormalTok{,}
        \AttributeTok{time.variable         =} \StringTok{"year"}\NormalTok{,}
        \AttributeTok{unit.names.variable   =} \StringTok{"state"}\NormalTok{,}
        \AttributeTok{treatment.identifier  =} \DecValTok{1}\NormalTok{,}
        \AttributeTok{controls.identifier   =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\SpecialCharTok{:}\DecValTok{10}\NormalTok{),}
        \AttributeTok{time.predictors.prior =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{14}\NormalTok{),}
        \AttributeTok{time.optimize.ssr     =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{14}\NormalTok{),}
        \AttributeTok{time.plot             =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{30}\NormalTok{)}
\NormalTok{    )}


\NormalTok{synth.out }\OtherTok{\textless{}{-}} \FunctionTok{synth}\NormalTok{(dataprep.out)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} X1, X0, Z1, Z0 all come directly from dataprep object.}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} **************** }
\CommentTok{\#\textgreater{}  searching for synthetic control unit  }
\CommentTok{\#\textgreater{}  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} **************** }
\CommentTok{\#\textgreater{} **************** }
\CommentTok{\#\textgreater{} **************** }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} MSPE (LOSS V): 9.831789 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} solution.v:}
\CommentTok{\#\textgreater{}  0.3888387 0.6111613 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} solution.w:}
\CommentTok{\#\textgreater{}  0.1115941 0.1832781 0.1027237 0.312091 0.06096758 0.03509706 0.05893735 0.05746256 0.07784853}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(synth.tables   }\OtherTok{\textless{}{-}} \FunctionTok{synth.tab}\NormalTok{(}
        \AttributeTok{dataprep.res =}\NormalTok{ dataprep.out,}
        \AttributeTok{synth.res    =}\NormalTok{ synth.out)}
\NormalTok{      )}
\CommentTok{\#\textgreater{} $tab.pred}
\CommentTok{\#\textgreater{}    Treated Synthetic Sample Mean}
\CommentTok{\#\textgreater{} X1   2.028     2.028       2.017}
\CommentTok{\#\textgreater{} X2   0.513     0.513       0.394}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $tab.v}
\CommentTok{\#\textgreater{}    v.weights}
\CommentTok{\#\textgreater{} X1 0.389    }
\CommentTok{\#\textgreater{} X2 0.611    }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $tab.w}
\CommentTok{\#\textgreater{}    w.weights unit.names unit.numbers}
\CommentTok{\#\textgreater{} 2      0.112          B            2}
\CommentTok{\#\textgreater{} 3      0.183          C            3}
\CommentTok{\#\textgreater{} 4      0.103          D            4}
\CommentTok{\#\textgreater{} 5      0.312          E            5}
\CommentTok{\#\textgreater{} 6      0.061          F            6}
\CommentTok{\#\textgreater{} 7      0.035          G            7}
\CommentTok{\#\textgreater{} 8      0.059          H            8}
\CommentTok{\#\textgreater{} 9      0.057          I            9}
\CommentTok{\#\textgreater{} 10     0.078          J           10}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $tab.loss}
\CommentTok{\#\textgreater{}            Loss W   Loss V}
\CommentTok{\#\textgreater{} [1,] 9.761708e{-}12 9.831789}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{path.plot}\NormalTok{(}\AttributeTok{synth.res    =}\NormalTok{ synth.out,}
          \AttributeTok{dataprep.res =}\NormalTok{ dataprep.out,}
          \AttributeTok{Ylab         =} \FunctionTok{c}\NormalTok{(}\StringTok{"Y"}\NormalTok{),}
          \AttributeTok{Xlab         =} \FunctionTok{c}\NormalTok{(}\StringTok{"Year"}\NormalTok{),}
          \AttributeTok{Legend       =} \FunctionTok{c}\NormalTok{(}\StringTok{"State A"}\NormalTok{,}\StringTok{"Synthetic State A"}\NormalTok{),}
          \AttributeTok{Legend.position =} \FunctionTok{c}\NormalTok{(}\StringTok{"topleft"}\NormalTok{)}
\NormalTok{)}

\FunctionTok{abline}\NormalTok{(}\AttributeTok{v   =} \DecValTok{15}\NormalTok{,}
       \AttributeTok{lty =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{28-synthetic-control_files/figure-latex/unnamed-chunk-6-1} \end{center}

Gaps plot:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gaps.plot}\NormalTok{(}\AttributeTok{synth.res    =}\NormalTok{ synth.out,}
          \AttributeTok{dataprep.res =}\NormalTok{ dataprep.out,}
          \AttributeTok{Ylab         =} \FunctionTok{c}\NormalTok{(}\StringTok{"Gap"}\NormalTok{),}
          \AttributeTok{Xlab         =} \FunctionTok{c}\NormalTok{(}\StringTok{"Year"}\NormalTok{),}
          \AttributeTok{Ylim         =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{30}\NormalTok{, }\DecValTok{30}\NormalTok{),}
          \AttributeTok{Main         =} \StringTok{""}
\NormalTok{)}

\FunctionTok{abline}\NormalTok{(}\AttributeTok{v   =} \DecValTok{15}\NormalTok{,}
       \AttributeTok{lty =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{28-synthetic-control_files/figure-latex/unnamed-chunk-7-1} \end{center}

Alternatively, \texttt{gsynth} provides options to estimate iterative fixed effects, and handle multiple treated units at tat time.

Here, we use two=way fixed effects and bootstrapped standard errors

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gsynth.out }\OtherTok{\textless{}{-}} \FunctionTok{gsynth}\NormalTok{(}
\NormalTok{  Y }\SpecialCharTok{\textasciitilde{}} \StringTok{\textasciigrave{}}\AttributeTok{T}\StringTok{\textasciigrave{}} \SpecialCharTok{+}\NormalTok{ X1 }\SpecialCharTok{+}\NormalTok{ X2,}
  \AttributeTok{data =}\NormalTok{ df,}
  \AttributeTok{index =} \FunctionTok{c}\NormalTok{(}\StringTok{"state"}\NormalTok{, }\StringTok{"year"}\NormalTok{),}
  \AttributeTok{force =} \StringTok{"two{-}way"}\NormalTok{,}
  \AttributeTok{CV =} \ConstantTok{TRUE}\NormalTok{,}
  \AttributeTok{r =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{),}
  \AttributeTok{se =} \ConstantTok{TRUE}\NormalTok{,}
  \AttributeTok{inference =} \StringTok{"parametric"}\NormalTok{,}
  \AttributeTok{nboots =} \DecValTok{1000}\NormalTok{,}
  \AttributeTok{parallel =}\NormalTok{ F }\CommentTok{\# TRUE}
\NormalTok{)}
\CommentTok{\#\textgreater{} Cross{-}validating ... }
\CommentTok{\#\textgreater{}  r = 0; sigma2 = 1.13533; IC = 0.95632; PC = 0.96713; MSPE = 1.65502}
\CommentTok{\#\textgreater{}  r = 1; sigma2 = 0.96885; IC = 1.54420; PC = 4.30644; MSPE = 1.33375}
\CommentTok{\#\textgreater{}  r = 2; sigma2 = 0.81855; IC = 2.08062; PC = 6.58556; MSPE = 1.27341*}
\CommentTok{\#\textgreater{}  r = 3; sigma2 = 0.71670; IC = 2.61125; PC = 8.35187; MSPE = 1.79319}
\CommentTok{\#\textgreater{}  r = 4; sigma2 = 0.62823; IC = 3.10156; PC = 9.59221; MSPE = 2.02301}
\CommentTok{\#\textgreater{}  r = 5; sigma2 = 0.55497; IC = 3.55814; PC = 10.48406; MSPE = 2.79596}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  r* = 2}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\NormalTok{Simulating errors .............}
\NormalTok{Bootstrapping ...}
\CommentTok{\#\textgreater{} ..........}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(gsynth.out)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{28-synthetic-control_files/figure-latex/unnamed-chunk-9-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(gsynth.out, }\AttributeTok{type =} \StringTok{"counterfactual"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{28-synthetic-control_files/figure-latex/unnamed-chunk-10-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(gsynth.out, }\AttributeTok{type =} \StringTok{"counterfactual"}\NormalTok{, }\AttributeTok{raw =} \StringTok{"all"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{28-synthetic-control_files/figure-latex/unnamed-chunk-11-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# shows estimations for the control cases}
\end{Highlighting}
\end{Shaded}

\hypertarget{example-2-1}{%
\subsection{Example 2}\label{example-2-1}}

by \href{https://towardsdatascience.com/causal-inference-using-synthetic-control-the-ultimate-guide-a622ad5cf827}{Leihua Ye}

\begin{Shaded}
\begin{Highlighting}[]

\FunctionTok{library}\NormalTok{(Synth)}
\FunctionTok{data}\NormalTok{(}\StringTok{"basque"}\NormalTok{)}
\FunctionTok{dim}\NormalTok{(basque) }\CommentTok{\#774*17}
\CommentTok{\#\textgreater{} [1] 774  17}
\FunctionTok{head}\NormalTok{(basque)}
\CommentTok{\#\textgreater{}   regionno     regionname year   gdpcap sec.agriculture sec.energy sec.industry}
\CommentTok{\#\textgreater{} 1        1 Spain (Espana) 1955 2.354542              NA         NA           NA}
\CommentTok{\#\textgreater{} 2        1 Spain (Espana) 1956 2.480149              NA         NA           NA}
\CommentTok{\#\textgreater{} 3        1 Spain (Espana) 1957 2.603613              NA         NA           NA}
\CommentTok{\#\textgreater{} 4        1 Spain (Espana) 1958 2.637104              NA         NA           NA}
\CommentTok{\#\textgreater{} 5        1 Spain (Espana) 1959 2.669880              NA         NA           NA}
\CommentTok{\#\textgreater{} 6        1 Spain (Espana) 1960 2.869966              NA         NA           NA}
\CommentTok{\#\textgreater{}   sec.construction sec.services.venta sec.services.nonventa school.illit}
\CommentTok{\#\textgreater{} 1               NA                 NA                    NA           NA}
\CommentTok{\#\textgreater{} 2               NA                 NA                    NA           NA}
\CommentTok{\#\textgreater{} 3               NA                 NA                    NA           NA}
\CommentTok{\#\textgreater{} 4               NA                 NA                    NA           NA}
\CommentTok{\#\textgreater{} 5               NA                 NA                    NA           NA}
\CommentTok{\#\textgreater{} 6               NA                 NA                    NA           NA}
\CommentTok{\#\textgreater{}   school.prim school.med school.high school.post.high popdens invest}
\CommentTok{\#\textgreater{} 1          NA         NA          NA               NA      NA     NA}
\CommentTok{\#\textgreater{} 2          NA         NA          NA               NA      NA     NA}
\CommentTok{\#\textgreater{} 3          NA         NA          NA               NA      NA     NA}
\CommentTok{\#\textgreater{} 4          NA         NA          NA               NA      NA     NA}
\CommentTok{\#\textgreater{} 5          NA         NA          NA               NA      NA     NA}
\CommentTok{\#\textgreater{} 6          NA         NA          NA               NA      NA     NA}
\end{Highlighting}
\end{Shaded}

transform data to be used in \texttt{synth()}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataprep.out }\OtherTok{\textless{}{-}} \FunctionTok{dataprep}\NormalTok{(}
    \AttributeTok{foo =}\NormalTok{ basque,}
    \AttributeTok{predictors =} \FunctionTok{c}\NormalTok{(}
        \StringTok{"school.illit"}\NormalTok{,}
        \StringTok{"school.prim"}\NormalTok{,}
        \StringTok{"school.med"}\NormalTok{,}
        \StringTok{"school.high"}\NormalTok{,}
        \StringTok{"school.post.high"}\NormalTok{,}
        \StringTok{"invest"}
\NormalTok{    ),}
    \AttributeTok{predictors.op =}  \StringTok{"mean"}\NormalTok{,}
    \CommentTok{\# the operator}
    \AttributeTok{time.predictors.prior =} \DecValTok{1964}\SpecialCharTok{:}\DecValTok{1969}\NormalTok{,}
    \CommentTok{\#the entire time frame from the \#beginning to the end}
    \AttributeTok{special.predictors =} \FunctionTok{list}\NormalTok{(}
        \FunctionTok{list}\NormalTok{(}\StringTok{"gdpcap"}\NormalTok{, }\DecValTok{1960}\SpecialCharTok{:}\DecValTok{1969}\NormalTok{,  }\StringTok{"mean"}\NormalTok{),}
        \FunctionTok{list}\NormalTok{(}\StringTok{"sec.agriculture"}\NormalTok{, }\FunctionTok{seq}\NormalTok{(}\DecValTok{1961}\NormalTok{, }\DecValTok{1969}\NormalTok{, }\DecValTok{2}\NormalTok{), }\StringTok{"mean"}\NormalTok{),}
        \FunctionTok{list}\NormalTok{(}\StringTok{"sec.energy"}\NormalTok{, }\FunctionTok{seq}\NormalTok{(}\DecValTok{1961}\NormalTok{, }\DecValTok{1969}\NormalTok{, }\DecValTok{2}\NormalTok{), }\StringTok{"mean"}\NormalTok{),}
        \FunctionTok{list}\NormalTok{(}\StringTok{"sec.industry"}\NormalTok{, }\FunctionTok{seq}\NormalTok{(}\DecValTok{1961}\NormalTok{, }\DecValTok{1969}\NormalTok{, }\DecValTok{2}\NormalTok{), }\StringTok{"mean"}\NormalTok{),}
        \FunctionTok{list}\NormalTok{(}\StringTok{"sec.construction"}\NormalTok{, }\FunctionTok{seq}\NormalTok{(}\DecValTok{1961}\NormalTok{, }\DecValTok{1969}\NormalTok{, }\DecValTok{2}\NormalTok{), }\StringTok{"mean"}\NormalTok{),}
        \FunctionTok{list}\NormalTok{(}\StringTok{"sec.services.venta"}\NormalTok{, }\FunctionTok{seq}\NormalTok{(}\DecValTok{1961}\NormalTok{, }\DecValTok{1969}\NormalTok{, }\DecValTok{2}\NormalTok{), }\StringTok{"mean"}\NormalTok{),}
        \FunctionTok{list}\NormalTok{(}\StringTok{"sec.services.nonventa"}\NormalTok{, }\FunctionTok{seq}\NormalTok{(}\DecValTok{1961}\NormalTok{, }\DecValTok{1969}\NormalTok{, }\DecValTok{2}\NormalTok{), }\StringTok{"mean"}\NormalTok{),}
        \FunctionTok{list}\NormalTok{(}\StringTok{"popdens"}\NormalTok{, }\DecValTok{1969}\NormalTok{,  }\StringTok{"mean"}\NormalTok{)}
\NormalTok{    ),}
    \AttributeTok{dependent =}  \StringTok{"gdpcap"}\NormalTok{,}
    \CommentTok{\# dv}
    \AttributeTok{unit.variable =}  \StringTok{"regionno"}\NormalTok{,}
    \CommentTok{\#identifying unit numbers}
    \AttributeTok{unit.names.variable =}  \StringTok{"regionname"}\NormalTok{,}
    \CommentTok{\#identifying unit names}
    \AttributeTok{time.variable =}  \StringTok{"year"}\NormalTok{,}
    \CommentTok{\#time{-}periods}
    \AttributeTok{treatment.identifier =} \DecValTok{17}\NormalTok{,}
    \CommentTok{\#the treated case}
    \AttributeTok{controls.identifier =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\SpecialCharTok{:}\DecValTok{16}\NormalTok{, }\DecValTok{18}\NormalTok{),}
    \CommentTok{\#the control cases; all others \#except number 17}
    \AttributeTok{time.optimize.ssr =} \DecValTok{1960}\SpecialCharTok{:}\DecValTok{1969}\NormalTok{,}
    \CommentTok{\#the time{-}period over which to optimize}
    \AttributeTok{time.plot =} \DecValTok{1955}\SpecialCharTok{:}\DecValTok{1997}
\NormalTok{) }\CommentTok{\#the entire time period before/after the treatment}
\end{Highlighting}
\end{Shaded}

where

\begin{itemize}
\item
  \(X_1\) = the control case before the treatment
\item
  \(X_0\) = the control cases after the treatment
\item
  \(Z_1\): the treatment case before the treatment
\item
  \(Z_0\): the treatment case after the treatment
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{synth.out }\OtherTok{=} \FunctionTok{synth}\NormalTok{(}\AttributeTok{data.prep.obj =}\NormalTok{ dataprep.out, }\AttributeTok{method =} \StringTok{"BFGS"}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} X1, X0, Z1, Z0 all come directly from dataprep object.}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} **************** }
\CommentTok{\#\textgreater{}  searching for synthetic control unit  }
\CommentTok{\#\textgreater{}  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} **************** }
\CommentTok{\#\textgreater{} **************** }
\CommentTok{\#\textgreater{} **************** }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} MSPE (LOSS V): 0.008864606 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} solution.v:}
\CommentTok{\#\textgreater{}  0.02773094 1.194e{-}07 1.60609e{-}05 0.0007163836 1.486e{-}07 0.002423908 0.0587055 0.2651997 0.02851006 0.291276 0.007994382 0.004053188 0.009398579 0.303975 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} solution.w:}
\CommentTok{\#\textgreater{}  2.53e{-}08 4.63e{-}08 6.44e{-}08 2.81e{-}08 3.37e{-}08 4.844e{-}07 4.2e{-}08 4.69e{-}08 0.8508145 9.75e{-}08 3.2e{-}08 5.54e{-}08 0.1491843 4.86e{-}08 9.89e{-}08 1.162e{-}07}
\end{Highlighting}
\end{Shaded}

Calculate the difference between the real basque region and the synthetic control

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gaps }\OtherTok{=}\NormalTok{ dataprep.out}\SpecialCharTok{$}\NormalTok{Y1plot }\SpecialCharTok{{-}}\NormalTok{ (dataprep.out}\SpecialCharTok{$}\NormalTok{Y0plot }
                                     \SpecialCharTok{\%*\%}\NormalTok{ synth.out}\SpecialCharTok{$}\NormalTok{solution.w)}
\NormalTok{gaps[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{]}
\CommentTok{\#\textgreater{}       1955       1956       1957 }
\CommentTok{\#\textgreater{} 0.15023473 0.09168035 0.03716475}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{synth.tables }\OtherTok{=} \FunctionTok{synth.tab}\NormalTok{(}\AttributeTok{dataprep.res =}\NormalTok{ dataprep.out,}
                         \AttributeTok{synth.res =}\NormalTok{ synth.out)}
\FunctionTok{names}\NormalTok{(synth.tables)}
\CommentTok{\#\textgreater{} [1] "tab.pred" "tab.v"    "tab.w"    "tab.loss"}
\NormalTok{synth.tables}\SpecialCharTok{$}\NormalTok{tab.pred[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{13}\NormalTok{,]}
\CommentTok{\#\textgreater{}                                          Treated Synthetic Sample Mean}
\CommentTok{\#\textgreater{} school.illit                              39.888   256.337     170.786}
\CommentTok{\#\textgreater{} school.prim                             1031.742  2730.104    1127.186}
\CommentTok{\#\textgreater{} school.med                                90.359   223.340      76.260}
\CommentTok{\#\textgreater{} school.high                               25.728    63.437      24.235}
\CommentTok{\#\textgreater{} school.post.high                          13.480    36.153      13.478}
\CommentTok{\#\textgreater{} invest                                    24.647    21.583      21.424}
\CommentTok{\#\textgreater{} special.gdpcap.1960.1969                   5.285     5.271       3.581}
\CommentTok{\#\textgreater{} special.sec.agriculture.1961.1969          6.844     6.179      21.353}
\CommentTok{\#\textgreater{} special.sec.energy.1961.1969               4.106     2.760       5.310}
\CommentTok{\#\textgreater{} special.sec.industry.1961.1969            45.082    37.636      22.425}
\CommentTok{\#\textgreater{} special.sec.construction.1961.1969         6.150     6.952       7.276}
\CommentTok{\#\textgreater{} special.sec.services.venta.1961.1969      33.754    41.104      36.528}
\CommentTok{\#\textgreater{} special.sec.services.nonventa.1961.1969    4.072     5.371       7.111}
\end{Highlighting}
\end{Shaded}

Relative importance of each unit

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{synth.tables}\SpecialCharTok{$}\NormalTok{tab.w[}\DecValTok{8}\SpecialCharTok{:}\DecValTok{14}\NormalTok{, ]}
\CommentTok{\#\textgreater{}    w.weights            unit.names unit.numbers}
\CommentTok{\#\textgreater{} 9      0.000    Castilla{-}La Mancha            9}
\CommentTok{\#\textgreater{} 10     0.851              Cataluna           10}
\CommentTok{\#\textgreater{} 11     0.000  Comunidad Valenciana           11}
\CommentTok{\#\textgreater{} 12     0.000           Extremadura           12}
\CommentTok{\#\textgreater{} 13     0.000               Galicia           13}
\CommentTok{\#\textgreater{} 14     0.149 Madrid (Comunidad De)           14}
\CommentTok{\#\textgreater{} 15     0.000    Murcia (Region de)           15}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# plot the changes before and after the treatment }
\FunctionTok{path.plot}\NormalTok{(}
    \AttributeTok{synth.res =}\NormalTok{ synth.out,}
    \AttributeTok{dataprep.res =}\NormalTok{ dataprep.out,}
    \AttributeTok{Ylab =} \StringTok{"real per{-}capita gdp (1986 USD, thousand)"}\NormalTok{,}
    \AttributeTok{Xlab =} \StringTok{"year"}\NormalTok{,}
    \AttributeTok{Ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{12}\NormalTok{),}
    \AttributeTok{Legend =} \FunctionTok{c}\NormalTok{(}\StringTok{"Basque country"}\NormalTok{,}
               \StringTok{"synthetic Basque country"}\NormalTok{),}
    \AttributeTok{Legend.position =} \StringTok{"bottomright"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{28-synthetic-control_files/figure-latex/unnamed-chunk-18-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gaps.plot}\NormalTok{(}
    \AttributeTok{synth.res =}\NormalTok{ synth.out,}
    \AttributeTok{dataprep.res =}\NormalTok{ dataprep.out,}
    \AttributeTok{Ylab =}  \StringTok{"gap in real per {-} capita GDP (1986 USD, thousand)"}\NormalTok{,}
    \AttributeTok{Xlab =}  \StringTok{"year"}\NormalTok{,}
    \AttributeTok{Ylim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{1.5}\NormalTok{, }\FloatTok{1.5}\NormalTok{),}
    \AttributeTok{Main =} \ConstantTok{NA}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{28-synthetic-control_files/figure-latex/unnamed-chunk-19-1} \end{center}

Doubly Robust Difference-in-Differences

Example from \texttt{DRDID} package

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(DRDID)}
\FunctionTok{data}\NormalTok{(nsw\_long)}
\CommentTok{\# Form the Lalonde sample with CPS comparison group}
\NormalTok{eval\_lalonde\_cps }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(nsw\_long, nsw\_long}\SpecialCharTok{$}\NormalTok{treated }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{|} 
\NormalTok{                               nsw\_long}\SpecialCharTok{$}\NormalTok{sample }\SpecialCharTok{==} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Estimate Average Treatment Effect on Treated using Improved Locally Efficient Doubly Robust DID estimator

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{out }\OtherTok{\textless{}{-}}
    \FunctionTok{drdid}\NormalTok{(}
        \AttributeTok{yname =} \StringTok{"re"}\NormalTok{,}
        \AttributeTok{tname =} \StringTok{"year"}\NormalTok{,}
        \AttributeTok{idname =} \StringTok{"id"}\NormalTok{,}
        \AttributeTok{dname =} \StringTok{"experimental"}\NormalTok{,}
        \AttributeTok{xformla =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ educ }\SpecialCharTok{+}\NormalTok{ black }\SpecialCharTok{+}\NormalTok{ married }\SpecialCharTok{+}\NormalTok{ nodegree }\SpecialCharTok{+}\NormalTok{ hisp }\SpecialCharTok{+}\NormalTok{ re74,}
        \AttributeTok{data =}\NormalTok{ eval\_lalonde\_cps,}
        \AttributeTok{panel =} \ConstantTok{TRUE}
\NormalTok{    )}
\FunctionTok{summary}\NormalTok{(out)}
\CommentTok{\#\textgreater{}  Call:}
\CommentTok{\#\textgreater{} drdid(yname = "re", tname = "year", idname = "id", dname = "experimental", }
\CommentTok{\#\textgreater{}     xformla = \textasciitilde{}age + educ + black + married + nodegree + hisp + }
\CommentTok{\#\textgreater{}         re74, data = eval\_lalonde\_cps, panel = TRUE)}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}  Further improved locally efficient DR DID estimator for the ATT:}
\CommentTok{\#\textgreater{}  }
\CommentTok{\#\textgreater{}    ATT     Std. Error  t value    Pr(\textgreater{}|t|)  [95\% Conf. Interval] }
\CommentTok{\#\textgreater{} {-}901.2703   393.6247   {-}2.2897     0.022    {-}1672.7747  {-}129.766 }
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}  Estimator based on panel data.}
\CommentTok{\#\textgreater{}  Outcome regression est. method: weighted least squares.}
\CommentTok{\#\textgreater{}  Propensity score est. method: inverse prob. tilting.}
\CommentTok{\#\textgreater{}  Analytical standard error.}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}  See Sant\textquotesingle{}Anna and Zhao (2020) for details.}
\end{Highlighting}
\end{Shaded}

\hypertarget{example-3-1}{%
\subsection{Example 3}\label{example-3-1}}

by \texttt{Synth} package's authors

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(Synth)}
\FunctionTok{data}\NormalTok{(}\StringTok{"basque"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\texttt{synth()} requires

\begin{itemize}
\item
  \(X_1\) vector of treatment predictors
\item
  \(X_0\) matrix of same variables for control group
\item
  \(Z_1\) vector of outcome variable for treatment group
\item
  \(Z_0\) matrix of outcome variable for control group
\end{itemize}

use \texttt{dataprep()} to prepare data in the format that can be used throughout the \texttt{Synth} package

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataprep.out }\OtherTok{\textless{}{-}} \FunctionTok{dataprep}\NormalTok{(}
    \AttributeTok{foo =}\NormalTok{ basque,}
    \AttributeTok{predictors =} \FunctionTok{c}\NormalTok{(}
        \StringTok{"school.illit"}\NormalTok{,}
        \StringTok{"school.prim"}\NormalTok{,}
        \StringTok{"school.med"}\NormalTok{,}
        \StringTok{"school.high"}\NormalTok{,}
        \StringTok{"school.post.high"}\NormalTok{,}
        \StringTok{"invest"}
\NormalTok{    ),}
    \AttributeTok{predictors.op =} \StringTok{"mean"}\NormalTok{,}
    \AttributeTok{time.predictors.prior =} \DecValTok{1964}\SpecialCharTok{:}\DecValTok{1969}\NormalTok{,}
    \AttributeTok{special.predictors =} \FunctionTok{list}\NormalTok{(}
        \FunctionTok{list}\NormalTok{(}\StringTok{"gdpcap"}\NormalTok{, }\DecValTok{1960}\SpecialCharTok{:}\DecValTok{1969}\NormalTok{ , }\StringTok{"mean"}\NormalTok{),}
        \FunctionTok{list}\NormalTok{(}\StringTok{"sec.agriculture"}\NormalTok{, }\FunctionTok{seq}\NormalTok{(}\DecValTok{1961}\NormalTok{, }\DecValTok{1969}\NormalTok{, }\DecValTok{2}\NormalTok{), }\StringTok{"mean"}\NormalTok{),}
        \FunctionTok{list}\NormalTok{(}\StringTok{"sec.energy"}\NormalTok{, }\FunctionTok{seq}\NormalTok{(}\DecValTok{1961}\NormalTok{, }\DecValTok{1969}\NormalTok{, }\DecValTok{2}\NormalTok{), }\StringTok{"mean"}\NormalTok{),}
        \FunctionTok{list}\NormalTok{(}\StringTok{"sec.industry"}\NormalTok{, }\FunctionTok{seq}\NormalTok{(}\DecValTok{1961}\NormalTok{, }\DecValTok{1969}\NormalTok{, }\DecValTok{2}\NormalTok{), }\StringTok{"mean"}\NormalTok{),}
        \FunctionTok{list}\NormalTok{(}\StringTok{"sec.construction"}\NormalTok{, }\FunctionTok{seq}\NormalTok{(}\DecValTok{1961}\NormalTok{, }\DecValTok{1969}\NormalTok{, }\DecValTok{2}\NormalTok{), }\StringTok{"mean"}\NormalTok{),}
        \FunctionTok{list}\NormalTok{(}\StringTok{"sec.services.venta"}\NormalTok{, }\FunctionTok{seq}\NormalTok{(}\DecValTok{1961}\NormalTok{, }\DecValTok{1969}\NormalTok{, }\DecValTok{2}\NormalTok{), }\StringTok{"mean"}\NormalTok{),}
        \FunctionTok{list}\NormalTok{(}\StringTok{"sec.services.nonventa"}\NormalTok{, }\FunctionTok{seq}\NormalTok{(}\DecValTok{1961}\NormalTok{, }\DecValTok{1969}\NormalTok{, }\DecValTok{2}\NormalTok{), }\StringTok{"mean"}\NormalTok{),}
        \FunctionTok{list}\NormalTok{(}\StringTok{"popdens"}\NormalTok{, }\DecValTok{1969}\NormalTok{, }\StringTok{"mean"}\NormalTok{)}
\NormalTok{    ),}
    \AttributeTok{dependent =} \StringTok{"gdpcap"}\NormalTok{,}
    \AttributeTok{unit.variable =} \StringTok{"regionno"}\NormalTok{,}
    \AttributeTok{unit.names.variable =} \StringTok{"regionname"}\NormalTok{,}
    \AttributeTok{time.variable =} \StringTok{"year"}\NormalTok{,}
    \AttributeTok{treatment.identifier =} \DecValTok{17}\NormalTok{,}
    \AttributeTok{controls.identifier =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\SpecialCharTok{:}\DecValTok{16}\NormalTok{, }\DecValTok{18}\NormalTok{),}
    \AttributeTok{time.optimize.ssr =} \DecValTok{1960}\SpecialCharTok{:}\DecValTok{1969}\NormalTok{,}
    \AttributeTok{time.plot =} \DecValTok{1955}\SpecialCharTok{:}\DecValTok{1997}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

find optimal weights that identifies the synthetic control for the treatment group

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{synth.out }\OtherTok{\textless{}{-}} \FunctionTok{synth}\NormalTok{(}\AttributeTok{data.prep.obj =}\NormalTok{ dataprep.out, }\AttributeTok{method =} \StringTok{"BFGS"}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} X1, X0, Z1, Z0 all come directly from dataprep object.}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} **************** }
\CommentTok{\#\textgreater{}  searching for synthetic control unit  }
\CommentTok{\#\textgreater{}  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} **************** }
\CommentTok{\#\textgreater{} **************** }
\CommentTok{\#\textgreater{} **************** }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} MSPE (LOSS V): 0.008864606 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} solution.v:}
\CommentTok{\#\textgreater{}  0.02773094 1.194e{-}07 1.60609e{-}05 0.0007163836 1.486e{-}07 0.002423908 0.0587055 0.2651997 0.02851006 0.291276 0.007994382 0.004053188 0.009398579 0.303975 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} solution.w:}
\CommentTok{\#\textgreater{}  2.53e{-}08 4.63e{-}08 6.44e{-}08 2.81e{-}08 3.37e{-}08 4.844e{-}07 4.2e{-}08 4.69e{-}08 0.8508145 9.75e{-}08 3.2e{-}08 5.54e{-}08 0.1491843 4.86e{-}08 9.89e{-}08 1.162e{-}07}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gaps }\OtherTok{\textless{}{-}}\NormalTok{ dataprep.out}\SpecialCharTok{$}\NormalTok{Y1plot }\SpecialCharTok{{-}}\NormalTok{ (dataprep.out}\SpecialCharTok{$}\NormalTok{Y0plot }\SpecialCharTok{\%*\%}\NormalTok{ synth.out}\SpecialCharTok{$}\NormalTok{solution.w)}
\NormalTok{gaps[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{]}
\CommentTok{\#\textgreater{}       1955       1956       1957 }
\CommentTok{\#\textgreater{} 0.15023473 0.09168035 0.03716475}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{synth.tables }\OtherTok{\textless{}{-}}
    \FunctionTok{synth.tab}\NormalTok{(}\AttributeTok{dataprep.res =}\NormalTok{ dataprep.out, }\AttributeTok{synth.res =}\NormalTok{ synth.out)}
\FunctionTok{names}\NormalTok{(synth.tables) }\CommentTok{\# you can pick tables to see }
\CommentTok{\#\textgreater{} [1] "tab.pred" "tab.v"    "tab.w"    "tab.loss"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{path.plot}\NormalTok{(}
    \AttributeTok{synth.res =}\NormalTok{ synth.out,}
    \AttributeTok{dataprep.res =}\NormalTok{ dataprep.out,}
    \AttributeTok{Ylab =} \StringTok{"real per{-}capita GDP (1986 USD, thousand)"}\NormalTok{,}
    \AttributeTok{Xlab =} \StringTok{"year"}\NormalTok{,}
    \AttributeTok{Ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{12}\NormalTok{),}
    \AttributeTok{Legend =} \FunctionTok{c}\NormalTok{(}\StringTok{"Basque country"}\NormalTok{,}
               \StringTok{"synthetic Basque country"}\NormalTok{),}
    \AttributeTok{Legend.position =} \StringTok{"bottomright"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{28-synthetic-control_files/figure-latex/unnamed-chunk-27-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gaps.plot}\NormalTok{(}
    \AttributeTok{synth.res =}\NormalTok{ synth.out,}
    \AttributeTok{dataprep.res =}\NormalTok{ dataprep.out,}
    \AttributeTok{Ylab =} \StringTok{"gap in real per{-}capita GDP (1986 USD, thousand)"}\NormalTok{,}
    \AttributeTok{Xlab =} \StringTok{"year"}\NormalTok{,}
    \AttributeTok{Ylim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{1.5}\NormalTok{, }\FloatTok{1.5}\NormalTok{),}
    \AttributeTok{Main =} \ConstantTok{NA}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{28-synthetic-control_files/figure-latex/unnamed-chunk-28-1} \end{center}

You could also run placebo tests

\hypertarget{example-4-1}{%
\subsection{Example 4}\label{example-4-1}}

by \href{https://cran.r-project.org/web/packages/microsynth/vignettes/introduction.html}{Michael Robbins and Steven Davenport} who are authors of \texttt{MicroSynth} with the following improvements:

\begin{itemize}
\item
  Standardization \texttt{use.survey\ =\ TRUE} and permutation ( \texttt{perm\ =\ 250} and \texttt{jack\ =\ TRUE} ) for placebo tests
\item
  Omnibus statistic (set to \texttt{omnibus.var} ) for multiple outcome variables
\item
  incorporate multiple follow-up periods \texttt{end.post}
\end{itemize}

Notes:

\begin{itemize}
\item
  Both predictors and outcome will be used to match units before intervention

  \begin{itemize}
  \item
    Outcome variable has to be \textbf{time-variant}
  \item
    Predictors are \textbf{time-invariant}
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# right now the package is not availabe for R version 4.2}
\FunctionTok{library}\NormalTok{(microsynth)}
\FunctionTok{data}\NormalTok{(}\StringTok{"seattledmi"}\NormalTok{)}


\NormalTok{cov.var }\OtherTok{\textless{}{-}}
    \FunctionTok{c}\NormalTok{(}
        \StringTok{"TotalPop"}\NormalTok{,}
        \StringTok{"BLACK"}\NormalTok{,}
        \StringTok{"HISPANIC"}\NormalTok{,}
        \StringTok{"Males\_1521"}\NormalTok{,}
        \StringTok{"HOUSEHOLDS"}\NormalTok{,}
        \StringTok{"FAMILYHOUS"}\NormalTok{,}
        \StringTok{"FEMALE\_HOU"}\NormalTok{,}
        \StringTok{"RENTER\_HOU"}\NormalTok{,}
        \StringTok{"VACANT\_HOU"}
\NormalTok{    )}
\NormalTok{match.out }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"i\_felony"}\NormalTok{, }\StringTok{"i\_misdemea"}\NormalTok{, }\StringTok{"i\_drugs"}\NormalTok{, }\StringTok{"any\_crime"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sea1 }\OtherTok{\textless{}{-}} \FunctionTok{microsynth}\NormalTok{(}
\NormalTok{    seattledmi,}
    \AttributeTok{idvar       =} \StringTok{"ID"}\NormalTok{,}
    \AttributeTok{timevar     =} \StringTok{"time"}\NormalTok{,}
    \AttributeTok{intvar      =} \StringTok{"Intervention"}\NormalTok{,}
    \AttributeTok{start.pre   =} \DecValTok{1}\NormalTok{,}
    \AttributeTok{end.pre     =} \DecValTok{12}\NormalTok{,}
    \AttributeTok{end.post    =} \DecValTok{16}\NormalTok{,}
    \AttributeTok{match.out   =}\NormalTok{ match.out, }\CommentTok{\# outcome variable will be matched on exactly}
    \AttributeTok{match.covar =}\NormalTok{ cov.var, }\CommentTok{\# specify covariates will be matched on exactly}
    \AttributeTok{result.var  =}\NormalTok{ match.out, }\CommentTok{\# used to report results}
    \AttributeTok{omnibus.var =}\NormalTok{ match.out, }\CommentTok{\# feature in the omnibus p{-}value}
    \AttributeTok{test        =} \StringTok{"lower"}\NormalTok{,}
    \AttributeTok{n.cores     =} \FunctionTok{min}\NormalTok{(parallel}\SpecialCharTok{::}\FunctionTok{detectCores}\NormalTok{(), }\DecValTok{2}\NormalTok{)}
\NormalTok{)}
\NormalTok{sea1}
\FunctionTok{summary}\NormalTok{(sea1)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot\_microsynth}\NormalTok{(sea1)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sea2 }\OtherTok{\textless{}{-}} \FunctionTok{microsynth}\NormalTok{(}
\NormalTok{    seattledmi,}
    \AttributeTok{idvar =} \StringTok{"ID"}\NormalTok{,}
    \AttributeTok{timevar =} \StringTok{"time"}\NormalTok{,}
    \AttributeTok{intvar =} \StringTok{"Intervention"}\NormalTok{,}
    \AttributeTok{start.pre =} \DecValTok{1}\NormalTok{,}
    \AttributeTok{end.pre =} \DecValTok{12}\NormalTok{,}
    \AttributeTok{end.post =} \FunctionTok{c}\NormalTok{(}\DecValTok{14}\NormalTok{, }\DecValTok{16}\NormalTok{),}
    \AttributeTok{match.out =}\NormalTok{ match.out,}
    \AttributeTok{match.covar =}\NormalTok{ cov.var,}
    \AttributeTok{result.var =}\NormalTok{ match.out,}
    \AttributeTok{omnibus.var =}\NormalTok{ match.out,}
    \AttributeTok{test =} \StringTok{"lower"}\NormalTok{,}
    \AttributeTok{perm =} \DecValTok{250}\NormalTok{,}
    \AttributeTok{jack =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{n.cores =} \FunctionTok{min}\NormalTok{(parallel}\SpecialCharTok{::}\FunctionTok{detectCores}\NormalTok{(), }\DecValTok{2}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{augmented-synthetic-control-method}{%
\section{Augmented Synthetic Control Method}\label{augmented-synthetic-control-method}}

package: \texttt{augsynth} \citep{ben2021augmented}

\hypertarget{synthetic-control-with-staggered-adoption}{%
\section{Synthetic Control with Staggered Adoption}\label{synthetic-control-with-staggered-adoption}}

references: \url{https://ebenmichael.github.io/assets/research/jamboree.pdf} \citep{ben2022synthetic} package: \texttt{augsynth}

\hypertarget{bayesian-synthetic-control}{%
\section{Bayesian Synthetic Control}\label{bayesian-synthetic-control}}

\citet{kim2020bayesian}

\citet{pang2022bayesian}

\hypertarget{generalized-synthetic-control}{%
\section{Generalized Synthetic Control}\label{generalized-synthetic-control}}

reference: \citep{xu2017generalized}

\begin{itemize}
\tightlist
\item
  Bootstrap procedure here is biased \citep{li2023statistical}. Hence, we need to follow \citet{li2023statistical} in terms of SEs estimation.
\end{itemize}

\hypertarget{other-advances}{%
\section{Other Advances}\label{other-advances}}

\citet{sun2023using} Using Multiple Outcomes to Improve SCM

\begin{itemize}
\tightlist
\item
  \textbf{Common Weights Across Outcomes}: This paper proposes using a single set of synthetic control weights across multiple outcomes, rather than estimating separate weights for each outcome.
\end{itemize}

\begin{itemize}
\item
  \textbf{Reduced Bias with Low-Rank Factor Model}: By balancing a vector or an index of outcomes, this approach yields lower bias bounds under a low-rank factor model, with further improvements as the number of outcomes increases.
\item
  \textbf{Evidence}: re-analysis of the Flint water crisis's impact on educational outcome.
\end{itemize}

\hypertarget{event-studies}{%
\chapter{Event Studies}\label{event-studies}}

The earliest paper that used event study was \citep{dolley1933characteristics}

\citep{campbell1998econometrics} introduced this method, which based on the efficient markets theory by \citep{fama1970efficient}

Review:

\begin{itemize}
\item
  \citep{mcwilliams1997event}: in management
\item
  \citep{sorescu2017}: in marketing
\end{itemize}

Previous marketing studies:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Firm-initiated activities
\end{enumerate}

\begin{itemize}
\item
  \citep{horsky1987does}: name change
\item
  \citep{chaney1991impact} new product announcements
\item
  \citep{agrawal1995economic}: celebrity endorsement
\item
  \citep{lane1995stock}: brand extensions
\item
  \citep{houston2000buyer}: joint venture
\item
  \citep{geyskens2002market}: Internet channel (for newspapers)
\item
  \citep{cornwell2005relationship}: sponsorship announcements
\item
  \citep{elberse2007power}: casting announcements
\item
  \citep{sorescu2007some}: M\&A
\item
  \citep{sood2009innovations}: innovation payoff
\item
  \citep{wiles2009worth}: product placements in movies
\item
  \citep{joshi2009movie}: movie releases
\item
  \citep{wiles2010stock}: Regulatory Reports of Deceptive Advertising
\item
  \citep{boyd2010chief}: new CMO appointments
\item
  \citep{karniouchina2011marketing}: product placement
\item
  \citep{wiles2012effect}: Brand Acquisition and Disposal
\item
  \citep{kalaignanam2013corporate}: corporate brand name change
\item
  \citep{raassens2012market}: new product development outsourcing
\item
  \citep{mazodier2013sponsorship}: sports announcements
\item
  \citep{borah2014make}: make, buy or ally for innovations
\item
  \citep{homburg2014firm}: channel expansions
\item
  \citep{fang2015timing}: Co-development agreements
\item
  \citep{wu2015sleeping}: horizontal collaboration in new product development
\item
  \citep{fama1969adjustment}: stock split
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Non-firm-initiated activities
\end{enumerate}

\begin{itemize}
\item
  \citep{sorescu2003}: FDA approvals
\item
  \citep{pandey2005relationship}: diversity elite list
\item
  \citep{balasubramanian2005impact}: high-quality achievements
\item
  \citep{tellis2007}: quality reviews by Walter Mossberg
\item
  \citep{fornell2006customer}: customer satisfaction
\item
  \citep{gielens2008dancing}: Walmart's entry into the UK market
\item
  \citep{boyd2008market}: indirect ties
\item
  \citep{rao2008fruits}: FDA approvals
\item
  \citep{ittner2009commentary}: customer satisfaction
\item
  \citep{tipton2009regulatory}: Deceptive advertising
\item
  \citep{chen2009does}: product recalls
\item
  \citep{jacobson2009financial}: satisfaction score release
\item
  \citep{karniouchina2009impact}: Mad money with Jim Cramer
\item
  \citep{wiles2010stock}: deceptive advertising
\item
  \citep{chen2012third}: third-party movie reviews
\item
  \citep{xiong2013asymmetric}: positive and negative news
\item
  \citep{gao2015should}: product recall
\item
  \citep{malhotra2011evaluating}: data breach
\item
  \citep{bhagat1998shareholder}: litigation
\end{itemize}

Potential avenues:

\begin{itemize}
\item
  Ad campaigns
\item
  Market entry
\item
  product failure/recalls
\item
  Patents
\end{itemize}

Pros:

\begin{itemize}
\item
  Better than accounting based measures (e.g., profits) because managers can manipulate profits \citep{benston1985validity}
\item
  Easy to do
\end{itemize}

Fun fact:

\begin{itemize}
\tightlist
\item
  \citep{dubow2006measuring} came up with a way to gauge how `clean' a market is. They based their measure on how much prices seemed to move in a way that suggested insider knowledge, before the release of important regulatory announcements that could affect the stock prices. Such price shifts might suggest that insider trading was occurring. Essentially, they were watching for any unusual price changes before the day of the announcement.
\end{itemize}

Events can be

\begin{itemize}
\item
  Internal (e.g., stock repurchase)
\item
  External (e.g., macroeconomic variables)
\end{itemize}

\textbf{Assumptions}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Efficient market theory
\item
  Shareholders are the most important group among stakeholders
\item
  The event sharply affects share price
\item
  Expected return is calculated appropriately
\end{enumerate}

\textbf{Steps}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Event Identification: (e.g., dividends, M\&A, stock buyback, laws or regulation, privatization vs.~nationalization, celebrity endorsements, name changes, or brand extensions etc. To see the list of events in US and international, see WRDS \textbf{S\&P Capital IQ Key Developments}). Events must affect either cash flows or on the discount rate of firms \citep[p.~191]{sorescu2017}

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    Estimation window: Normal return expected return (\(T_0 \to T_1\)) (sometimes include days before to capture leakages).

    \begin{itemize}
    \item
      Recommendation by \citep{johnston2007review} is to use 250 days before the event (and 45-day between the estimation window and the event window).

      \begin{itemize}
      \item
        \citep{wiles2012effect} used an 90-trading-day estimation window ending 6 days before the event (this is consistent with the finance literature).
      \item
        \citep{gielens2008dancing} 260 to 10 days before or 300 to 46 days before
      \item
        \citep{tirunillai2012does} estimation window of 255 days and ends 46 days before the event.
      \end{itemize}
    \item
      Similarly, \citep{mcwilliams1997event} and \citep{fornell2006customer} 255 days ending 46 days before the event date
    \item
      \citep[p.~194]{sorescu2017} suggest 100 days before the event date
    \item
      Leakage: try to cover as broad news sources as possible (LexisNexis, Factiva, and RavenPack).
    \end{itemize}
  \item
    Event window: contain the event date (\(T_1 \to T_2\)) (have to argue for the event window and can't do it empirically)

    \begin{itemize}
    \item
      One day: \citep{balasubramanian2005impact, boyd2010chief, fornell2006customer}
    \item
      Two days: \citep{raassens2012market, sood2009innovations}
    \item
      Up to 10 days: \citep{cornwell2005relationship, kalaignanam2013corporate, sorescu2007some}
    \end{itemize}
  \item
    Post Event window: \(T_2 \to T_3\)
  \end{enumerate}
\item
  Normal vs.~Abnormal returns
\end{enumerate}

\[
\epsilon_{it}^* = \frac{P_{it} - E(P_{it})}{P_{it-1}} = R_{it} - E(R_{it}|X_t)
\]

where

\begin{itemize}
\item
  \(\epsilon_{it}^*\) = abnormal return
\item
  \(R_{it}\) = realized (actual) return
\item
  \(P\) = dividend-adjusted price of the stock
\item
  \(E(R_{it}|X_t)\) normal expected return
\end{itemize}

There are several model to calculate the expected return

A. \protect\hyperlink{statistical-models}{Statistical Models}: assumes jointly multivariate normal and iid over time (need distributional assumptions for valid finite-sample estimation) rather robust (hence, recommended)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \protect\hyperlink{constant-mean-return-model}{Constant Mean Return Model}
\item
  \protect\hyperlink{market-model}{Market Model}
\item
  Adjusted Market Return Model
\item
  Factor Model
\end{enumerate}

B. \protect\hyperlink{economic-model}{Economic Model} (strong assumption regarding investor behavior)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \protect\hyperlink{capital-asset-pricing-model-capm}{Capital Asset Pricing Model (CAPM)}
\item
  \protect\hyperlink{arbitrage-pricing-theory-apt}{Arbitrage Pricing Theory (APT)}
\end{enumerate}

\hypertarget{other-issues}{%
\section{Other Issues}\label{other-issues}}

\hypertarget{event-studies-in-marketing}{%
\subsection{Event Studies in marketing}\label{event-studies-in-marketing}}

\citep{skiera2017should} What should be the dependent variable in marketing-related event studies?

\begin{itemize}
\item
  Based on valuation theory, Shareholder value = the value of the operating business + non-operating asset - debt \citep{schulze2012linking}

  \begin{itemize}
  \tightlist
  \item
    Many marketing events only affect the operating business value, but not non-operating assets and debt
  \end{itemize}
\item
  Ignoring the differences in firm-specific leverage effects has dual effects:

  \begin{itemize}
  \item
    inflates the impact of observation pertaining to firms with large debt
  \item
    deflates those pertaining to firms with large non-operating asset.
  \end{itemize}
\item
  It's recommended that marketing papers should report both \(CAR^{OB}\) and \(CAR^{SHV}\) and argue for whichever one more appropriate.
\item
  Up until this paper, only two previous event studies control for financial structure: \citep{gielens2008dancing} \citep{chaney1991impact}
\end{itemize}

Definitions:

\begin{itemize}
\item
  Cumulative abnormal percentage return on shareholder value (\(CAR^{SHV}\))

  \begin{itemize}
  \tightlist
  \item
    Shareholder value refers to a firm's market capitalization = share price x \# of shares.
  \end{itemize}
\item
  Cumulative abnormal percentage return on the value of the operating business (\(CAR^{OB}\))

  \begin{itemize}
  \item
    \(CAR^{OB} = CAR^{SHV}/\text{leverage effect}_{before}\)
  \item
    Leverage effect = Operating business value / Shareholder value (LE describes how a 1\% change in operating business translates into a percentage change in shareholder value).
  \item
    Value of operating business = shareholder value - non-operating assets + debt
  \item
    Leverage effect \(\neq\) leverage ratio, where leverage ratio is debt / firm size

    \begin{itemize}
    \item
      debt = long-term + short-term debt; long-term debt
    \item
      firm size = book value of equity; market cap; total assets; debt + equity
    \end{itemize}
  \end{itemize}
\item
  Operating assets are those used by firm in their core business operations (e..g, property, plant, equipment, natural resources, intangible asset)
\item
  Non--operating assets (redundant assets), do not play a role in a firm's operations, but still generate some form of return (e.g., excess cash , marketable securities - commercial papers, market instruments)
\end{itemize}

Marketing events usually influence the value of a firm's operating assets (more specifically intangible assets). Then, changes in the value of the operating business can impact shareholder value.

\begin{itemize}
\item
  Three rare instances where marketing events can affect non-operating assets and debt

  \begin{itemize}
  \item
    \citep{hall2004determinants}: excess pre-orderings can influence short-term debt
  \item
    \citep{berger1997managerial} Firing CMO increase debt as the manager's tenure is negatively associated with the firm's debt
  \item
    \citep{bhaduri2002determinants} production of unique products.
  \end{itemize}
\end{itemize}

A marketing-related event can either influence

\begin{itemize}
\item
  value components of a firm's value (= firm's operating business, non-operating assets and its debt)
\item
  only the operating business.
\end{itemize}

Replication of the leverage effect

\[
\begin{aligned}
\text{leverage effect} &= \frac{\text{operating business}}{\text{shareholder value}} \\
&= \frac{\text{(shareholder value - non-operating assets + debt)}}{\text{shareholder value}} \\
&= \frac{prcc_f \times csho - ivst + dd1 + dltt + pstk}{prcc_f \times csho}
\end{aligned}
\]

Compustat Data Item

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.1944}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.4583}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Label
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Variable
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{prcc\_f} & Share price \\
\texttt{csho} & Common shares outstanding \\
\texttt{ivst} & short-term investments

(Non-operating assets) \\
\texttt{dd1} & long-term debt due in one year \\
\texttt{dltt} & long-term debt \\
\texttt{pstk} & preferred stock \\
\end{longtable}

Since WRDS no longer maintains the S\&P 500 list as of the time of this writing, I can't replicate the list used in \citep{skiera2017should} paper.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{df\_leverage\_effect }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"data/leverage\_effect.csv.gz"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
    
    \CommentTok{\# get active firms only}
    \FunctionTok{filter}\NormalTok{(costat }\SpecialCharTok{==} \StringTok{"A"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
    
    \CommentTok{\# drop missing values}
    \FunctionTok{drop\_na}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
    
    \CommentTok{\# create the leverage effect variable}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{le =}\NormalTok{ (prcc\_f }\SpecialCharTok{*}\NormalTok{ csho }\SpecialCharTok{{-}}\NormalTok{ ivst }\SpecialCharTok{+}\NormalTok{ dd1 }\SpecialCharTok{+}\NormalTok{ dltt }\SpecialCharTok{+}\NormalTok{ pstk)}\SpecialCharTok{/}\NormalTok{ (prcc\_f }\SpecialCharTok{*}\NormalTok{ csho)) }\SpecialCharTok{\%\textgreater{}\%} 
    
    \CommentTok{\# get shareholder value}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{shv =}\NormalTok{ prcc\_f }\SpecialCharTok{*}\NormalTok{ csho) }\SpecialCharTok{\%\textgreater{}\%} 
    
    \CommentTok{\# remove Infinity value for leverage effect (i.e., shareholder value = 0)}
    \FunctionTok{filter\_all}\NormalTok{(}\FunctionTok{all\_vars}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.infinite}\NormalTok{(.))) }\SpecialCharTok{\%\textgreater{}\%} 
    
    \CommentTok{\# positive values only }
    \FunctionTok{filter\_all}\NormalTok{(}\FunctionTok{all\_vars}\NormalTok{(. }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
    
    \CommentTok{\# get the within coefficient of variation}
    \FunctionTok{group\_by}\NormalTok{(gvkey) }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{within\_var\_mean\_le =} \FunctionTok{mean}\NormalTok{(le),}
           \AttributeTok{within\_var\_sd\_le =} \FunctionTok{sd}\NormalTok{(le)) }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{ungroup}\NormalTok{()}


\CommentTok{\# get the mean and standard deviation}
\FunctionTok{mean}\NormalTok{(df\_leverage\_effect}\SpecialCharTok{$}\NormalTok{le)}
\CommentTok{\#\textgreater{} [1] 150.1087}
\FunctionTok{max}\NormalTok{(df\_leverage\_effect}\SpecialCharTok{$}\NormalTok{le)}
\CommentTok{\#\textgreater{} [1] 183629.6}
\FunctionTok{hist}\NormalTok{(df\_leverage\_effect}\SpecialCharTok{$}\NormalTok{le)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{29-event-study_files/figure-latex/unnamed-chunk-1-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# coefficient of variation }
\FunctionTok{sd}\NormalTok{(df\_leverage\_effect}\SpecialCharTok{$}\NormalTok{le) }\SpecialCharTok{/} \FunctionTok{mean}\NormalTok{(df\_leverage\_effect}\SpecialCharTok{$}\NormalTok{le) }\SpecialCharTok{*} \DecValTok{100}
\CommentTok{\#\textgreater{} [1] 2749.084}

\CommentTok{\# Within{-}firm variation (similar to fig 3a)}
\NormalTok{df\_leverage\_effect }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{group\_by}\NormalTok{(gvkey) }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(within\_var\_mean\_le, within\_var\_sd\_le) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{cv =}\NormalTok{ within\_var\_sd\_le}\SpecialCharTok{/}\NormalTok{ within\_var\_mean\_le) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(cv) }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{pull}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{hist}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{29-event-study_files/figure-latex/unnamed-chunk-1-2} \end{center}

\hypertarget{economic-significance}{%
\subsection{Economic significance}\label{economic-significance}}

Total wealth gain (loss) from the event

\[
\Delta W_t = CAR_t \times MKTVAL_0
\]

where

\begin{itemize}
\item
  \(\Delta W_t\) = gain (loss)
\item
  \(CAR_t\) = cumulative residuals to date \(t\)
\item
  \(MKTVAL_0\) market value of the firm before the event window
\end{itemize}

\hypertarget{statistical-power}{%
\subsection{Statistical Power}\label{statistical-power}}

increases with

\begin{itemize}
\item
  more firms
\item
  less days in the event window (avoiding potential contamination from confounds)
\end{itemize}

\hypertarget{testing}{%
\section{Testing}\label{testing}}

\hypertarget{parametric-test}{%
\subsection{Parametric Test}\label{parametric-test}}

\citep{brown1985using} provide evidence that even in the presence of non-normality, the parametric tests still perform well. Since the proportion of positive and negative abnormal returns tends to be equal in the sample (of at least 5 securities). The excess returns will coverage to normality as the sample size increases. Hence, parametric test is advocated than non-parametric one.

Low power to detect significance \citep{kothari1997measuring}

\begin{itemize}
\tightlist
\item
  Power = f(sample, size, the actual size of abnormal returns, the variance of abnormal returns across firms)
\end{itemize}

\hypertarget{t-test}{%
\subsubsection{T-test}\label{t-test}}

Applying CLT

\[
\begin{aligned}
t_{CAR} &= \frac{\bar{CAR_{it}}}{\sigma (CAR_{it})/\sqrt{n}} \\
t_{BHAR} &= \frac{\bar{BHAR_{it}}}{\sigma (BHAR_{it})/\sqrt{n}}
\end{aligned}
\]

Assume

\begin{itemize}
\item
  Abnormal returns are normally distributed
\item
  Var(abnormal returns) are equal across firms
\item
  No cross-correlation in abnormal returns.
\end{itemize}

Hence, it will be misspecified if you suspected

\begin{itemize}
\item
  Heteroskedasticity
\item
  Cross-sectional dependence
\item
  Technically, abnormal returns could follow non-normal distribution (but because of the design of abnormal returns calculation, it typically forces the distribution to be normal)
\end{itemize}

To address these concerns, \protect\hyperlink{patell-standardized-residual-psr}{Patell Standardized Residual (PSR)} can sometimes help.

\hypertarget{patell-standardized-residual-psr}{%
\subsubsection{Patell Standardized Residual (PSR)}\label{patell-standardized-residual-psr}}

\citep{patell1976corporate}

\begin{itemize}
\tightlist
\item
  Since market model uses observations outside the event window, abnormal returns contain prediction errors on top of the true residuals , and should be standardized:
\end{itemize}

\[
AR_{it} = \frac{\hat{u}_{it}}{s_i \sqrt{C_{it}}}
\]

where

\begin{itemize}
\item
  \(\hat{u}_{it}\) = estimated residual
\item
  \(s_i\) = standard deviation estimate of residuals (from the estimation period)
\item
  \(C_{it}\) = a correction to account for the prediction's increased variation outside of the estimation period \citep{strong1992}
\end{itemize}

\[
C_{it} = 1 + \frac{1}{T} + \frac{(R_{mt} - \bar{R}_m)^2}{\sum_t (R_{mt} - \bar{R}_m)^2}
\]

where

\begin{itemize}
\item
  \(T\) = number of observations (from estimation period)
\item
  \(R_{mt}\) = average rate of return of all stocks trading the the stock market at time \(t\)
\item
  \(\bar{R}_m = \frac{1}{T} \sum_{t=1}^T R_{mt}\)
\end{itemize}

\hypertarget{non-parametric-test}{%
\subsection{Non-parametric Test}\label{non-parametric-test}}

\begin{itemize}
\item
  No assumptions about return distribution
\item
  Sign Test (assumes symmetry in returns)

  \begin{itemize}
  \tightlist
  \item
    \texttt{binom.test()}
  \end{itemize}
\item
  Wilcoxon Signed-Rank Test (allows for non-symmetry in returns)

  \begin{itemize}
  \tightlist
  \item
    Use \texttt{wilcox.test(sample)}
  \end{itemize}
\item
  Gen Sign Test
\item
  Corrado Rank Test
\end{itemize}

\hypertarget{sample}{%
\section{Sample}\label{sample}}

\begin{itemize}
\item
  Sample can be relative small

  \begin{itemize}
  \item
    \citep{wiles2012effect} 572 acquisition announcements, 308 disposal announcements
  \item
    Can range from 71 \citep{markovitch2008findings} to 3552 \citep{borah2014make}
  \end{itemize}
\end{itemize}

\hypertarget{confounders}{%
\subsection{Confounders}\label{confounders}}

\begin{itemize}
\tightlist
\item
  Avoid confounding events: earnings announcements, key executive changes, unexpected stock buybacks, changes in dividends within the two-trading day window surrounding the event, mergers and acquisitions, spin-offers, stock splits, management changes, joint ventures, unexpected dividend, IPO, debt defaults, dividend cancellations \citep{mcwilliams1997event}
\end{itemize}

According to \citep{fornell2006customer}, need to control:

\begin{itemize}
\item
  one-day event period = day when Wall Street Journal publish ACSI announcement.
\item
  5 days before and after event to rule out other news (PR Newswires, Dow Jones, Business Wires)

  \begin{itemize}
  \item
    M\&A, Spin-offs, stock splits
  \item
    CEO or CFO changes,
  \item
    Layoffs, restructurings, earnings announcements, lawsuits
  \item
    Capital IQ - Key Developments: covers almost all important events so you don't have to search on news.
  \end{itemize}
\end{itemize}

\citep{sorescu2017} examine confounding events in the short-term windows:

\begin{itemize}
\item
  From RavenPack, 3982 US publicly traded firms, with all the press releases (2000-2013)
\item
  3-day window around event dates
\item
  The difference between a sample with full observations and a sample without confounded events is negligible (non-significant).
\item
  Conclusion: \textbf{excluding confounded observations may be unnecessary for short-term event studies.}

  \begin{itemize}
  \item
    Biases can stem from researchers pick and choose events to exclude
  \item
    As time progresses, more and more events you need to exclude which can be infeasible.
  \end{itemize}
\end{itemize}

To further illustrate this point, let's do a quick simulation exercise

In this example, we will explore three types of events:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Focal events
\item
  Correlated events (i.e., events correlated with the focal events; the presence of correlated events can follow the presence of the focal event)
\item
  Uncorrelated events (i.e., events with dates that might randomly coincide with the focal events, but are not correlated with them).
\end{enumerate}

We have the ability to control the strength of correlation between focal and correlated events in this study, as well as the number of unrelated events we wish to examine.

Let's examine the implications of including and excluding correlated and uncorrelated events on the estimates of our focal events.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load required libraries}
\FunctionTok{library}\NormalTok{(dplyr)}
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(tidyr)}
\FunctionTok{library}\NormalTok{(tidyverse)}

\CommentTok{\# Parameters}
\NormalTok{n                  }\OtherTok{\textless{}{-}} \DecValTok{100000}         \CommentTok{\# Number of observations}
\NormalTok{n\_focal            }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(n }\SpecialCharTok{*} \FloatTok{0.2}\NormalTok{) }\CommentTok{\# Number of focal events}
\NormalTok{overlap\_correlated }\OtherTok{\textless{}{-}} \FloatTok{0.5}            \CommentTok{\# Overlapping percentage between focal and correlated events}

\CommentTok{\# Function to compute mean and confidence interval}
\NormalTok{mean\_ci }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{}
\NormalTok{    m }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(x)}
\NormalTok{    ci }\OtherTok{\textless{}{-}} \FunctionTok{qt}\NormalTok{(}\FloatTok{0.975}\NormalTok{, }\FunctionTok{length}\NormalTok{(x)}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{) }\SpecialCharTok{*} \FunctionTok{sd}\NormalTok{(x) }\SpecialCharTok{/} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{length}\NormalTok{(x)) }\CommentTok{\# 95\% confidence interval}
    \FunctionTok{list}\NormalTok{(}\AttributeTok{mean =}\NormalTok{ m, }\AttributeTok{lower =}\NormalTok{ m }\SpecialCharTok{{-}}\NormalTok{ ci, }\AttributeTok{upper =}\NormalTok{ m }\SpecialCharTok{+}\NormalTok{ ci)}
\NormalTok{\}}

\CommentTok{\# Simulate data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{date       =} \FunctionTok{seq.Date}\NormalTok{(}\AttributeTok{from =} \FunctionTok{as.Date}\NormalTok{(}\StringTok{"2010{-}01{-}01"}\NormalTok{), }\AttributeTok{by =} \StringTok{"day"}\NormalTok{, }\AttributeTok{length.out =}\NormalTok{ n), }\CommentTok{\# Date sequence}
    \AttributeTok{focal      =} \FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, n),}
    \AttributeTok{correlated =} \FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, n),}
    \AttributeTok{ab\_ret     =} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{)}


\CommentTok{\# Define focal events}
\NormalTok{focal\_idx }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n, n\_focal)}
\NormalTok{data}\SpecialCharTok{$}\NormalTok{focal[focal\_idx] }\OtherTok{\textless{}{-}} \DecValTok{1}

\NormalTok{true\_effect }\OtherTok{\textless{}{-}} \FloatTok{0.25}

\CommentTok{\# Adjust the ab\_ret for the focal events to have a mean of true\_effect}
\NormalTok{data}\SpecialCharTok{$}\NormalTok{ab\_ret[focal\_idx] }\OtherTok{\textless{}{-}}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{ab\_ret[focal\_idx] }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{ab\_ret[focal\_idx]) }\SpecialCharTok{+}\NormalTok{ true\_effect}



\CommentTok{\# Determine the number of correlated events that overlap with focal and those that don\textquotesingle{}t}
\NormalTok{n\_correlated\_overlap }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(}\FunctionTok{length}\NormalTok{(focal\_idx) }\SpecialCharTok{*}\NormalTok{ overlap\_correlated)}
\NormalTok{n\_correlated\_non\_overlap }\OtherTok{\textless{}{-}}\NormalTok{ n\_correlated\_overlap}

\CommentTok{\# Sample the overlapping correlated events from the focal indices}
\NormalTok{correlated\_idx }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(focal\_idx, }\AttributeTok{size =}\NormalTok{ n\_correlated\_overlap)}

\CommentTok{\# Get the remaining indices that are not part of focal}
\NormalTok{remaining\_idx }\OtherTok{\textless{}{-}} \FunctionTok{setdiff}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n, focal\_idx)}

\CommentTok{\# Check to ensure that we\textquotesingle{}re not attempting to sample more than the available remaining indices}
\ControlFlowTok{if}\NormalTok{ (}\FunctionTok{length}\NormalTok{(remaining\_idx) }\SpecialCharTok{\textless{}}\NormalTok{ n\_correlated\_non\_overlap) \{}
    \FunctionTok{stop}\NormalTok{(}\StringTok{"Not enough remaining indices for non{-}overlapping correlated events"}\NormalTok{)}
\NormalTok{\}}

\CommentTok{\# Sample the non{-}overlapping correlated events from the remaining indices}
\NormalTok{correlated\_non\_focal\_idx }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(remaining\_idx, }\AttributeTok{size =}\NormalTok{ n\_correlated\_non\_overlap)}

\CommentTok{\# Combine the two to get all correlated indices}
\NormalTok{all\_correlated\_idx }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(correlated\_idx, correlated\_non\_focal\_idx)}

\CommentTok{\# Set the correlated events in the data}
\NormalTok{data}\SpecialCharTok{$}\NormalTok{correlated[all\_correlated\_idx] }\OtherTok{\textless{}{-}} \DecValTok{1}


\CommentTok{\# Inflate the effect for correlated events to have a mean of }
\NormalTok{correlated\_non\_focal\_idx }\OtherTok{\textless{}{-}} \FunctionTok{setdiff}\NormalTok{(all\_correlated\_idx, focal\_idx) }\CommentTok{\# Fixing the selection of non{-}focal correlated events}
\NormalTok{data}\SpecialCharTok{$}\NormalTok{ab\_ret[correlated\_non\_focal\_idx] }\OtherTok{\textless{}{-}}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{ab\_ret[correlated\_non\_focal\_idx] }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{ab\_ret[correlated\_non\_focal\_idx]) }\SpecialCharTok{+} \DecValTok{1}


\CommentTok{\# Define the numbers of uncorrelated events for each scenario}
\NormalTok{num\_uncorrelated }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{40}\NormalTok{)}

\CommentTok{\# Define uncorrelated events}
\ControlFlowTok{for}\NormalTok{ (num }\ControlFlowTok{in}\NormalTok{ num\_uncorrelated) \{}
    \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{num) \{}
\NormalTok{        data[}\FunctionTok{paste0}\NormalTok{(}\StringTok{"uncorrelated\_"}\NormalTok{, i)] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{        uncorrelated\_idx }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n, }\FunctionTok{round}\NormalTok{(n }\SpecialCharTok{*} \FloatTok{0.1}\NormalTok{))}
\NormalTok{        data[uncorrelated\_idx, }\FunctionTok{paste0}\NormalTok{(}\StringTok{"uncorrelated\_"}\NormalTok{, i)] }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{    \}}
\NormalTok{\}}


\CommentTok{\# Define uncorrelated columns and scenarios}
\NormalTok{unc\_cols }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\StringTok{"uncorrelated\_"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\NormalTok{num\_uncorrelated)}
\NormalTok{results }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{Scenario =} \FunctionTok{c}\NormalTok{(}\StringTok{"Include Correlated"}\NormalTok{, }\StringTok{"Correlated Effects"}\NormalTok{, }\StringTok{"Exclude Correlated"}\NormalTok{, }\StringTok{"Exclude Correlated and All Uncorrelated"}\NormalTok{),}
    \AttributeTok{MeanEffect =} \FunctionTok{c}\NormalTok{(}
        \FunctionTok{mean\_ci}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{ab\_ret[data}\SpecialCharTok{$}\NormalTok{focal }\SpecialCharTok{==} \DecValTok{1}\NormalTok{])}\SpecialCharTok{$}\NormalTok{mean,}
        \FunctionTok{mean\_ci}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{ab\_ret[data}\SpecialCharTok{$}\NormalTok{focal }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{|}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{correlated }\SpecialCharTok{==} \DecValTok{1}\NormalTok{])}\SpecialCharTok{$}\NormalTok{mean,}
        \FunctionTok{mean\_ci}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{ab\_ret[data}\SpecialCharTok{$}\NormalTok{focal }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{correlated }\SpecialCharTok{==} \DecValTok{0}\NormalTok{])}\SpecialCharTok{$}\NormalTok{mean,}
        \FunctionTok{mean\_ci}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{ab\_ret[data}\SpecialCharTok{$}\NormalTok{focal }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{correlated }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\&} \FunctionTok{rowSums}\NormalTok{(data[, }\FunctionTok{paste0}\NormalTok{(}\StringTok{"uncorrelated\_"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\NormalTok{num\_uncorrelated)]) }\SpecialCharTok{==} \DecValTok{0}\NormalTok{])}\SpecialCharTok{$}\NormalTok{mean}
\NormalTok{    ),}
    \AttributeTok{LowerCI =} \FunctionTok{c}\NormalTok{(}
        \FunctionTok{mean\_ci}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{ab\_ret[data}\SpecialCharTok{$}\NormalTok{focal }\SpecialCharTok{==} \DecValTok{1}\NormalTok{])}\SpecialCharTok{$}\NormalTok{lower,}
        \FunctionTok{mean\_ci}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{ab\_ret[data}\SpecialCharTok{$}\NormalTok{focal }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{|}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{correlated }\SpecialCharTok{==} \DecValTok{1}\NormalTok{])}\SpecialCharTok{$}\NormalTok{lower,}
        \FunctionTok{mean\_ci}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{ab\_ret[data}\SpecialCharTok{$}\NormalTok{focal }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{correlated }\SpecialCharTok{==} \DecValTok{0}\NormalTok{])}\SpecialCharTok{$}\NormalTok{lower,}
        \FunctionTok{mean\_ci}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{ab\_ret[data}\SpecialCharTok{$}\NormalTok{focal }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{correlated }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\&} \FunctionTok{rowSums}\NormalTok{(data[, }\FunctionTok{paste0}\NormalTok{(}\StringTok{"uncorrelated\_"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\NormalTok{num\_uncorrelated)]) }\SpecialCharTok{==} \DecValTok{0}\NormalTok{])}\SpecialCharTok{$}\NormalTok{lower}
\NormalTok{    ),}
    \AttributeTok{UpperCI =} \FunctionTok{c}\NormalTok{(}
        \FunctionTok{mean\_ci}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{ab\_ret[data}\SpecialCharTok{$}\NormalTok{focal }\SpecialCharTok{==} \DecValTok{1}\NormalTok{])}\SpecialCharTok{$}\NormalTok{upper,}
        \FunctionTok{mean\_ci}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{ab\_ret[data}\SpecialCharTok{$}\NormalTok{focal }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{|}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{correlated }\SpecialCharTok{==} \DecValTok{1}\NormalTok{])}\SpecialCharTok{$}\NormalTok{upper,}
        \FunctionTok{mean\_ci}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{ab\_ret[data}\SpecialCharTok{$}\NormalTok{focal }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{correlated }\SpecialCharTok{==} \DecValTok{0}\NormalTok{])}\SpecialCharTok{$}\NormalTok{upper,}
        \FunctionTok{mean\_ci}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{ab\_ret[data}\SpecialCharTok{$}\NormalTok{focal }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{correlated }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\&} \FunctionTok{rowSums}\NormalTok{(data[, }\FunctionTok{paste0}\NormalTok{(}\StringTok{"uncorrelated\_"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\NormalTok{num\_uncorrelated)]) }\SpecialCharTok{==} \DecValTok{0}\NormalTok{])}\SpecialCharTok{$}\NormalTok{upper}
\NormalTok{    )}
\NormalTok{)}

\CommentTok{\# Add the scenarios for excluding 5, 10, 20, and 50 uncorrelated}
\ControlFlowTok{for}\NormalTok{ (num }\ControlFlowTok{in}\NormalTok{ num\_uncorrelated) \{}
\NormalTok{    unc\_cols }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\StringTok{"uncorrelated\_"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\NormalTok{num)}
\NormalTok{    results }\OtherTok{\textless{}{-}}\NormalTok{ results }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{add\_row}\NormalTok{(}
            \AttributeTok{Scenario =} \FunctionTok{paste}\NormalTok{(}\StringTok{"Exclude"}\NormalTok{, num, }\StringTok{"Uncorrelated"}\NormalTok{),}
            \AttributeTok{MeanEffect =} \FunctionTok{mean\_ci}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{ab\_ret[data}\SpecialCharTok{$}\NormalTok{focal }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{correlated }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\&} \FunctionTok{rowSums}\NormalTok{(data[, unc\_cols]) }\SpecialCharTok{==} \DecValTok{0}\NormalTok{])}\SpecialCharTok{$}\NormalTok{mean,}
            \AttributeTok{LowerCI =} \FunctionTok{mean\_ci}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{ab\_ret[data}\SpecialCharTok{$}\NormalTok{focal }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{correlated }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\&} \FunctionTok{rowSums}\NormalTok{(data[, unc\_cols]) }\SpecialCharTok{==} \DecValTok{0}\NormalTok{])}\SpecialCharTok{$}\NormalTok{lower,}
            \AttributeTok{UpperCI =} \FunctionTok{mean\_ci}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{ab\_ret[data}\SpecialCharTok{$}\NormalTok{focal }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{correlated }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\&} \FunctionTok{rowSums}\NormalTok{(data[, unc\_cols]) }\SpecialCharTok{==} \DecValTok{0}\NormalTok{])}\SpecialCharTok{$}\NormalTok{upper}
\NormalTok{        )}
\NormalTok{\}}


\FunctionTok{ggplot}\NormalTok{(results,}
       \FunctionTok{aes}\NormalTok{(}
           \AttributeTok{x =} \FunctionTok{factor}\NormalTok{(Scenario, }\AttributeTok{levels =}\NormalTok{ Scenario),}
           \AttributeTok{y =}\NormalTok{ MeanEffect,}
           \AttributeTok{ymin =}\NormalTok{ LowerCI,}
           \AttributeTok{ymax =}\NormalTok{ UpperCI}
\NormalTok{       )) }\SpecialCharTok{+}
    \FunctionTok{geom\_pointrange}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{coord\_flip}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{ylab}\NormalTok{(}\StringTok{"Mean Effect"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{xlab}\NormalTok{(}\StringTok{"Scenario"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Mean Effect of Focal Events under Different Scenarios"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =}\NormalTok{ true\_effect,}
               \AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{,}
               \AttributeTok{color =} \StringTok{"red"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{29-event-study_files/figure-latex/unnamed-chunk-2-1} \end{center}

As depicted in the plot, the inclusion of correlated events demonstrates minimal impact on the estimation of our focal events. Conversely, excluding these correlated events can diminish our statistical power. This is true in cases of pronounced correlation.

However, the consequences of excluding unrelated events are notably more significant. It becomes evident that by omitting around 40 unrelated events from our study, we lose the ability to accurately identify the true effects of the focal events. In reality and within research, we often rely on the Key Developments database, excluding over 150 events, a practice that can substantially impair our capacity to ascertain the authentic impact of the focal events.

This little experiment really drives home the point -- you better have a darn good reason to exclude an event from your study (make it super convincing)!

\hypertarget{biases}{%
\section{Biases}\label{biases}}

\begin{itemize}
\item
  Different closing time obscure estimation of the abnormal returns, check \citep{campbell1998econometrics}
\item
  Upward bias in aggregating CAR + transaction prices (bid and ask)
\item
  Cross-sectional dependence in the returns bias the standard deviation estimates downward, which inflates the test statistics when events share common dates \citep{mackinlay1997event}. Hence, \citep{jaffe1974special} \protect\hyperlink{calendar-time-portfolio-abnormal-returns-ctars}{Calendar-time Portfolio Abnormal Returns (CTARs)} should be used to correct for this bias.
\item
  \citep{wiles2012effect}: For events confined to relatively few industries, cross-sectional dependence in the returns can bias the SD estimate downward, inflating the associated test statistics'' (p.~47). To control for potential cross-sectional correlation in the abnormal returns, you can use time-series standard deviation test statistic \citep{brown1980measuring}
\item
  Sample selection bias (self-selection of firms into the event treatment) similar to omitted variable bias where the omitted variable is the private info that leads a firm to take the action.

  \begin{itemize}
  \item
    See \protect\hyperlink{endogenous-sample-selection}{Endogenous Sample Selection} for more methods to correct this bias.
  \item
    Use Heckman model \citep{acharya1993value}

    \begin{itemize}
    \item
      But hard to find an instrument that meets the exclusion requirements (and strong, because weak instruments can lead to multicollinearity in the second equation)
    \item
      Can estimate the private information unknown to investors (which is Mills ratio \(\lambda\) itself). Testing \(\lambda\) significance is to see whether private info can explain outcomes (e.g., magnitude of the CARs to the announcement).
    \item
      Examples: \citep{chen2009does} \citep{wiles2012effect} \citep{fang2015timing}
    \end{itemize}
  \item
    Counterfactual observations

    \begin{itemize}
    \item
      Propensity score matching:

      \begin{itemize}
      \item
        Finance: \citep[\citet{doan2021does}]{iskandar2013valuation} \citep{masulis2011venture}
      \item
        Marketing: \citep{warren2017how} \citep{borah2014make} \citep{cao2013wedded}
      \end{itemize}
    \item
      Switching regression: comparison between 2 specific outcomes (also account for selection on unobservables - using instruments) \citep{cao2013wedded}
    \end{itemize}
  \end{itemize}
\end{itemize}

\hypertarget{long-run-event-studies}{%
\section{Long-run event studies}\label{long-run-event-studies}}

\begin{itemize}
\item
  Usually make an assumption that the distribution of the abnormal returns to these events has a mean of 0 \citep[p.~192]{sorescu2017}. And \citep{sorescu2017} provide evidence that for all events they examine the results from samples with and without confounding events do not differ.
\item
  Long-horizon event studies face challenges due to systematic errors over time and sensitivity to model choice.
\item
  Two main approaches are used to measure long-term abnormal stock returns

  \begin{itemize}
  \item
    \protect\hyperlink{buy-and-hold-abnormal-returns-bhar}{Buy and Hold Abnormal Returns (BHAR)}
  \item
    \protect\hyperlink{long-term-cumulative-abnormal-returns-lcars}{Long-term Cumulative Abnormal Returns (LCARs)}
  \item
    \protect\hyperlink{calendar-time-portfolio-abnormal-returns-ctars}{Calendar-time Portfolio Abnormal Returns (CTARs)} (Jensen's Alpha): manages cross-sectional dependence better and is less sensitive to (asset pricing) model misspecification
  \end{itemize}
\item
  Two types:

  \begin{itemize}
  \item
    Unexpected changes in firm specific variables (typically not announced, may not be immediately visible to all investors, impact on firm value is not straightforward): customer satisfaction scores effect on firm value \citep{jacobson2009financial} or unexpected changes in marketing expenditures \citep{kim2011stock} to determine mispricing.
  \item
    Complex consequences (investors take time to learn and incorporate info): acquisition depends on integration \citep{sorescu2007some}
  \end{itemize}
\item
  12 - 60 months event window: \citep{loughran1995new} \citep{brav1997myth}
\item
  Example: \citep{dutta2018robust}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(crseEventStudy)}

\CommentTok{\# example by the package\textquotesingle{}s author}
\FunctionTok{data}\NormalTok{(demo\_returns)}
\NormalTok{SAR }\OtherTok{\textless{}{-}}
    \FunctionTok{sar}\NormalTok{(}\AttributeTok{event =}\NormalTok{ demo\_returns}\SpecialCharTok{$}\NormalTok{EON,}
        \AttributeTok{control =}\NormalTok{ demo\_returns}\SpecialCharTok{$}\NormalTok{RWE,}
        \AttributeTok{logret =} \ConstantTok{FALSE}\NormalTok{)}
\FunctionTok{mean}\NormalTok{(SAR)}
\CommentTok{\#\textgreater{} [1] 0.006870196}
\end{Highlighting}
\end{Shaded}

\hypertarget{buy-and-hold-abnormal-returns-bhar}{%
\subsection{Buy and Hold Abnormal Returns (BHAR)}\label{buy-and-hold-abnormal-returns-bhar}}

\begin{itemize}
\tightlist
\item
  Classic references: \citep{loughran1995new} \citep{barber1997firm} \citep{lyon1999improved}
\end{itemize}

Use a portfolio of stocks that are close matches of the current firm over the same period as benchmark, and see the difference between the firm return and that of the portfolio.

\begin{itemize}
\tightlist
\item
  More technical note is that it measures returns from buying stocks in event-experiencing firms and shorting stocks in similar non-event firms within the same time.
\item
  Because of high cross-sectional correlations, BHARs' t-stat can be inflated, but its rank order is not affected \citep{markovitch2008findings, sorescu2007some}
\end{itemize}

To construct the portfolio, use similar

\begin{itemize}
\tightlist
\item
  size
\item
  book-to-market
\item
  momentum
\end{itemize}

Matching Procedure \citep{barber1997firm}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Each year from July to June, all common stocks in the CRSP database are categorized into ten groups (deciles) based on their market capitalization from the previous June.
\item
  Within these deciles, firms are further sorted into five groups (quintiles) based on their book-to-market ratios as of December of the previous year or earlier, considering possible delays in financial statement reporting.
\item
  Benchmark portfolios are designed to exclude firms with specific events but include all firms that can be classified into the characteristic-based portfolios.
\end{enumerate}

Similarly, \citet{wiles2010stock} uses the following matching procedure:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  All firms in the same two-digit SIC code with market values of 50\% to 150\% of the focal firms are selected
\item
  From this list, the 10 firms with the most comparable book-to-market ratios are chosen to serve as the matched portfolio (the matched portfolio can have less than 10 firms).
\end{enumerate}

Calculations:

\[
AR_{it} = R_{it} - E(R_{it}|X_t)
\]

Cumulative Abnormal Return (CAR):

\[
CAR_{it} = \sum_{t=1}^T (R_{it} - E(R_{it}))
\]

Buy-and-Hold Abnormal Return (BHAR)

\[
BHAR_{t = 1}^T = \Pi_{t=1}^T(1 + R_{it}) - \Pi_{t = 1}^T (1 + E(R_{it}))
\]

where as CAR is the arithmetic sum, BHAR is the geometric sum.

\begin{itemize}
\tightlist
\item
  In short-term event studies, differences between CAR and BHAR are often minimal. However, in long-term studies, this difference could significantly skew results. \citep{barber1997firm} shows that while BHAR is usually slightly lower than annual CAR, but it dramatically surpasses CAR when annual BHAR exceeds 28\%.
\end{itemize}

To calculate the long-run return (\(\Pi_{t=1}^T (1 + E(R_{it}))\)) of the benchmark portfolio, we can:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{With annual rebalance}: In each period, each portfolio is re-balanced and then compound mean stock returns in a portfolio over a given period:
\end{enumerate}

\[
\Pi_{t = 1}^T (1 + E(R_{it})) = \Pi_{t}^T (1 + \sum_{i = s}^{n_t}w_{it} R_{it})
\]

where \(n_t\) is the number of firms in period \(t\), and \(w_{it}\) is (1) \(1/n_t\) or (2) value-weight of firm \(i\) in period \(t\).

To avoid favoring recent events, in cross-sectional event studies, researchers usually treat all events equally when studying their impact on the stock market over time. This approach helps identify any abnormal changes in stock prices, especially when dealing with a series of unplanned events.

Potential problems:

\begin{itemize}
\item
  Solution first: Form benchmark portfolios that will never change constituent firms \citep{mitchell2000managerial}, because of these problems:

  \begin{itemize}
  \item
    Newly public companies often perform worse than a balanced market index \citep{ritter1991long}, and this, over time, might distort long-term return expectations due to the inclusion of these new companies (a phenomenon called ``new listing bias'' identified by \citet{barber1997firm}).
  \item
    Regularly rebalancing an equal-weight portfolio can lead to overestimated long-term returns and potentially skew buy-and-hold abnormal returns (BHARs) negatively due to constant selling of winning stocks and buying of underperformers (i.e., ``rebalancing bias'' \citep{barber1997firm}).
  \item
    Value-weight portfolios, which favor larger market cap stocks, can be viewed as an active investment strategy that keeps buying winning stocks and selling underperformers. Over time, this approach tends to positively distort BHARs.
  \end{itemize}
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Without annual rebalance}: Compounding the returns of the securities comprising the portfolio, followed by calculating the average across all securities
\end{enumerate}

\[
\Pi_{t = s}^{T} (1 + E(R_{it})) = \sum_{i=s}^{n_t} (w_{is} \Pi_{t=1}^T (1 + R_{it}))
\]

where \(t\) is the investment period, \(R_{it}\) is the return on security \(i\), \(n_i\) is the number of securities, \(w_{it}\) is either \(1/n_s\) or value-weight factor of security \(i\) at initial period \(s\). This portfolio's profits come from a simple investment where all the included stocks are given equal importance, or weighted according to their market value, as they were in a specific past period (period s). This means that it doesn't consider any stocks that were listed after this period, nor does it adjust the portfolio each month. However, one problem with this method is that the value assigned to each stock, based on its market size, needs to be corrected. This is to make sure that recent stocks don't end up having too much influence.

Fortunately, on \href{https://wrds-www.wharton.upenn.edu/pages/get-data/event-study-wrds/long-run-event-study-upload-you-own-events/}{WRDS}, it will give you all types of BHAR (2x2) (equal-weighted vs.~value-weighted and with annual rebalance and without annual rebalance)

\begin{itemize}
\tightlist
\item
  ``MINWIN'' is the smallest number of months a company trades after an event to be included in the study.
\end{itemize}

\begin{itemize}
\item
  ``MAXWIN'' is the most months that the study considers in its calculations.

  \begin{itemize}
  \tightlist
  \item
    Companies aren't excluded if they have less than MAXWIN months, unless they also have fewer than MINWIN months.
  \end{itemize}
\item
  The term ``MONTH'' signifies chosen months (typically 12, 24, or 36) used to work out BHAR.

  \begin{itemize}
  \tightlist
  \item
    If monthly returns are missing during the set period, matching portfolio returns fill in the gaps.
  \end{itemize}
\end{itemize}

\hypertarget{long-term-cumulative-abnormal-returns-lcars}{%
\subsection{Long-term Cumulative Abnormal Returns (LCARs)}\label{long-term-cumulative-abnormal-returns-lcars}}

Formula for LCARs during the \((1,T)\) postevent horizon \citep{sorescu2007some}

\[
LCAR_{pT} = \sum_{t = 1}^{t = T} (R_{it} - R_{pt})
\]

where \(R_{it}\) is the rate of return of stock \(i\) in month \(t\)

\(R_{pt}\) is the rate of return on the counterfactual portfolio in month \(t\)

\hypertarget{calendar-time-portfolio-abnormal-returns-ctars}{%
\subsection{Calendar-time Portfolio Abnormal Returns (CTARs)}\label{calendar-time-portfolio-abnormal-returns-ctars}}

This section follows strictly the procedure in \citep{wiles2010stock}

A portfolio for every day in calendar time (including all securities which experience an event that time).

For each portfolio, the securities and their returns are equally weighted

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  For all portfolios, the average abnormal return are calculated as
\end{enumerate}

\[
AAR_{Pt} = \frac{\sum_{i=1}^S AR_i}{S}
\]

where

\begin{itemize}
\tightlist
\item
  \(S\) is the number of securities in portfolio \(P\)
\item
  \(AR_i\) is the abnormal return for the stock \(i\) in the portfolio
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  For every portfolio \(P\), a time series estimate of \(\sigma(AAR_{Pt})\) is calculated for the preceding \(k\) days, assuming that the \(AAR_{Pt}\) are independent over time.
\item
  Each portfolio's average abnormal return is standardized
\end{enumerate}

\[
SAAR_{Pt} = \frac{AAR_{Pt}}{SD(AAR_{Pt})}
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Average standardized residual across all portfolio's in calendar time
\end{enumerate}

\[
ASAAR = \frac{1}{n}\sum_{i=1}^{255} SAAR_{Pt} \times D_t
\]

where

\begin{itemize}
\item
  \(D_t = 1\) when there is at least one security in portfolio \(t\)
\item
  \(D_t = 0\) when there are no security in portfolio \(t\)
\item
  \(n\) is the number of days in which the portfolio have at least one security \(n = \sum_{i = 1}^{255}D_t\)
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  The cumulative average standardized average abnormal returns is
\end{enumerate}

\[
CASSAR_{S_1, S_2} = \sum_{i=S_1}^{S_2} ASAAR
\]

If the ASAAR are independent over time, then standard deviation for the above estimate is \(\sqrt{S_2 - S_1 + 1}\)

then, the test statistics is

\[
t = \frac{CASAAR_{S_1,S_2}}{\sqrt{S_2 - S_1 + 1}}
\]

Limitations

\begin{itemize}
\item
  Cannot examine individual stock difference, can only see the difference at the portfolio level.

  \begin{itemize}
  \tightlist
  \item
    One can construct multiple portfolios (based on the metrics of interest) so that firms in the same portfolio shares that same characteristics. Then, one can compare the intercepts in each portfolio.
  \end{itemize}
\item
  Low power \citep{loughran2000uniformly}, type II error is likely.
\end{itemize}

\hypertarget{aggregation}{%
\section{Aggregation}\label{aggregation}}

\hypertarget{over-time}{%
\subsection{Over Time}\label{over-time}}

We calculate the cumulative abnormal (CAR) for the event windows

\(H_0\): Standardized cumulative abnormal return for stock \(i\) is 0 (no effect of events on stock performance)

\(H_1\): SCAR is not 0 (there is an effect of events on stock performance)

\hypertarget{across-firms-over-time}{%
\subsection{Across Firms + Over Time}\label{across-firms-over-time}}

Additional assumptions: Abnormal returns of different socks are uncorrelated (rather strong), but it's very valid if event windows for different stocks do not overlap. If the windows for different overlap, follow \citep{bernard1987cross} and \citep[\citet{schipper1983effects}]{schipper1983evidence}

\(H_0\): The mean of the abnormal returns across all firms is 0 (no effect)

\(H_1\): The mean of the abnormal returns across all firms is different form 0 (there is an effect)

Parametric (empirically either one works fine) (assume abnormal returns is normally distributed) :

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Aggregate the CAR of all stocks (Use this if the true abnormal variance is greater for stocks with higher variance)
\item
  Aggregate the SCAR of all stocks (Use this if the true abnormal return is constant across all stocks)
\end{enumerate}

Non-parametric (no parametric assumptions):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Sign test:

  \begin{itemize}
  \item
    Assume both the abnormal returns and CAR to be independent across stocks
  \item
    Assume 50\% with positive abnormal returns and 50\% with negative abnormal return
  \item
    The null will be that there is a positive abnormal return correlated with the event (if you want the alternative to be there is a negative relationship)
  \item
    With skewed distribution (likely in daily stock data), the size test is not trustworthy. Hence, rank test might be better
  \end{itemize}
\item
  Rank test

  \begin{itemize}
  \tightlist
  \item
    Null: there is no abnormal return during the event window
  \end{itemize}
\end{enumerate}

\hypertarget{heterogeneity-in-the-event-effect}{%
\section{Heterogeneity in the event effect}\label{heterogeneity-in-the-event-effect}}

\[
y = X \theta + \eta
\]

where

\begin{itemize}
\item
  \(y\) = CAR
\item
  \(X\) = Characteristics that lead to heterogeneity in the event effect (i.e., abnormal returns) (e.g., firm or event specific)
\item
  \(\eta\) = error term
\end{itemize}

Note:

\begin{itemize}
\tightlist
\item
  In cases with selection bias (firm characteristics and investor anticipation of the event: larger firms might enjoy great positive effect of an event, and investors endogenously anticipate this effect and overvalue the stock), we have to use the White's \(t\)-statistics to have the lower bounds of the true significance of the estimates.
\item
  This technique should be employed even if the average CAR is not significantly different from 0, especially when the CAR variance is high \citep{boyd2010chief}
\end{itemize}

\hypertarget{common-variables-in-marketing}{%
\subsection{Common variables in marketing}\label{common-variables-in-marketing}}

\citep{sorescu2017} Table 4

\begin{itemize}
\item
  Firm size is negatively correlated with abnormal return in finance \citep{sorescu2017}, but mixed results in marketing.
\item
  \# of event occurrences
\item
  R\&D expenditure
\item
  Advertising expense
\item
  Marketing investment (SG\&A)
\item
  Industry concentration (HHI, \# of competitors)
\item
  Financial leverage
\item
  Market share
\item
  Market size (total sales volume within the firm's SIC code)
\item
  marketing capability
\item
  Book to market value
\item
  ROA
\item
  Free cash flow
\item
  Sales growth
\item
  Firm age
\end{itemize}

\hypertarget{expected-return-calculation}{%
\section{Expected Return Calculation}\label{expected-return-calculation}}

\hypertarget{statistical-models}{%
\subsection{Statistical Models}\label{statistical-models}}

\begin{itemize}
\item
  based on statistical assumptions about the behavior of returns (e..g, multivariate normality)
\item
  we only need to assume stable distributions \citep{owen1983class}
\end{itemize}

\hypertarget{constant-mean-return-model}{%
\subsubsection{Constant Mean Return Model}\label{constant-mean-return-model}}

The expected normal return is the mean of the real returns

\[
Ra_{it} = R_{it} - \bar{R}_i
\]

Assumption:

\begin{itemize}
\tightlist
\item
  returns revert to its mean (very questionable)
\end{itemize}

The basic mean returns model generally delivers similar findings to more complex models since the variance of abnormal returns is not decreased considerably \citep{brown1985using}

\hypertarget{market-model}{%
\subsubsection{Market Model}\label{market-model}}

\[
R_{it} = \alpha_i + \beta R_{mt} + \epsilon_{it}
\]

where

\begin{itemize}
\item
  \(R_{it}\) = stock return \(i\) in period \(t\)
\item
  \(R_{mt}\) = market return
\item
  \(\epsilon_{it}\) = zero mean (\(E(e_{it}) = 0\)) error term with its own variance \(\sigma^2\)
\end{itemize}

Notes:

\begin{itemize}
\item
  People typically use S\&P 500, CRSP value-weighed or equal-weighted index as the market portfolio.
\item
  When \(\beta =0\), the \protect\hyperlink{market-model}{Market Model} is the \protect\hyperlink{constant-mean-return-model}{Constant Mean Return Model}
\item
  better fit of the market-model, the less variance in abnormal return, and the more easy to detect the event's effect
\item
  recommend generalized method of moments to be robust against auto-correlation and heteroskedasticity
\end{itemize}

\hypertarget{fama-french-model}{%
\subsubsection{Fama-French Model}\label{fama-french-model}}

Please note that there is a difference between between just taking the return versus taking the excess return as the dependent variable.

The correct way is to use the excess return for firm and for market \citep[p.~1917]{fama2010luck}.

\begin{itemize}
\tightlist
\item
  \(\alpha_i\) ``is the average return left unexplained by the benchmark model'' (i.e., abnormal return)
\end{itemize}

\hypertarget{ff3}{%
\paragraph{FF3}\label{ff3}}

\citep{fama1993common}

\[
\begin{aligned}
E(R_{it}|X_t) - r_{ft} = \alpha_i &+ \beta_{1i} (E(R_{mt}|X_t )- r_{ft}) \\
&+ b_{2i} SML_t + b_{3i} HML_t
\end{aligned}
\]

where

\begin{itemize}
\item
  \(r_{ft}\) risk-free rate (e.g., 3-month Treasury bill)
\item
  \(R_{mt}\) is the market-rate (e.g., S\&P 500)
\item
  SML: returns on small (size) portfolio minus returns on big portfolio
\item
  HML: returns on high (B/M) portfolio minus returns on low portfolio.
\end{itemize}

\hypertarget{ff4}{%
\paragraph{FF4}\label{ff4}}

\citep[p.~195]{sorescu2017} suggest the use of \protect\hyperlink{market-model}{Market Model} in marketing for short-term window and \protect\hyperlink{fama-french-model}{Fama-French Model} for the long-term window (the statistical properties of this model have not been examined the the daily setting).

\citep{carhart1997persistence}

\[
\begin{aligned}
E(R_{it}|X_t) - r_{ft} = \alpha_i &+ \beta_{1i} (E(R_{mt}|X_t )- r_{ft}) \\
&+ b_{2i} SML_t + b_{3i} HML_t + b_{4i} UMD_t
\end{aligned}
\]

where

\begin{itemize}
\tightlist
\item
  \(UMD_t\) is the momentum factor (difference between high and low prior return stock portfolios) in day \(t\).
\end{itemize}

\hypertarget{economic-model}{%
\subsection{Economic Model}\label{economic-model}}

The only difference between CAPM and APT is that APT has multiple factors (including factors beyond the focal company)

Economic models put limits on a statistical model that come from assumed behavior that is derived from theory.

\hypertarget{capital-asset-pricing-model-capm}{%
\subsubsection{Capital Asset Pricing Model (CAPM)}\label{capital-asset-pricing-model-capm}}

\[
E(R_i) = R_f + \beta_i (E(R_m) - R_f)
\]

where

\begin{itemize}
\item
  \(E(R_i)\) = expected firm return
\item
  \(R_f\) = risk free rate
\item
  \(E(R_m - R_f)\) = market risk premium
\item
  \(\beta_i\) = firm sensitivity
\end{itemize}

\hypertarget{arbitrage-pricing-theory-apt}{%
\subsubsection{Arbitrage Pricing Theory (APT)}\label{arbitrage-pricing-theory-apt}}

\[
R = R_f + \Lambda f + \epsilon
\]

where

\begin{itemize}
\item
  \(\epsilon \sim N(0, \Psi)\)
\item
  \(\Lambda\) = factor loadings
\item
  \(f \sim N(\mu, \Omega)\) = general factor model

  \begin{itemize}
  \item
    \(\mu\) = expected risk premium vector
  \item
    \(\Omega\) = factor covariance matrix
  \end{itemize}
\end{itemize}

\hypertarget{application-8}{%
\section{Application}\label{application-8}}

Packages:

\begin{itemize}
\item
  \texttt{eventstudies}
\item
  \texttt{erer}
\item
  \texttt{EventStudy}
\item
  \texttt{AbnormalReturns}
\item
  \href{https://www.eventstudytools.com/}{Event Study Tools}
\item
  \href{https://irudnyts.github.io/estudy2/}{estudy2}
\item
  \texttt{PerformanceAnalytics}
\end{itemize}

In practice, people usually sort portfolio because they are not sure whether the FF model is specified correctly.

Steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Sort all returns in CRSP into 10 deciles based on size.
\item
  In each decile, sort returns into 10 decides based on BM
\item
  Get the average return of the 100 portfolios for each period (i.e., expected returns of stocks given decile - characteristics)
\item
  For each stock in the event study: Compare the return of the stock to the corresponding portfolio based on size and BM.
\end{enumerate}

Notes:

\begin{itemize}
\item
  Sorting produces outcomes that are often more conservative (e.g., FF abnormal returns can be greater than those that used sorting).
\item
  If the results change when we do B/M first then size or vice versa, then the results are not robust (this extends to more than just two characteristics - e.g., momentum).
\end{itemize}

Examples:

Forestry:

\begin{itemize}
\item
  \citep{mei2008} M\&A on financial performance (forest product)
\item
  \citep{sun2011effects} litigation on firm values
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(erer)}

\CommentTok{\# example by the package\textquotesingle{}s author}
\FunctionTok{data}\NormalTok{(daEsa)}
\NormalTok{hh }\OtherTok{\textless{}{-}} \FunctionTok{evReturn}\NormalTok{(}
    \AttributeTok{y =}\NormalTok{ daEsa,       }\CommentTok{\# dataset}
    \AttributeTok{firm =} \StringTok{"wpp"}\NormalTok{,    }\CommentTok{\# firm name}
    \AttributeTok{y.date =} \StringTok{"date"}\NormalTok{, }\CommentTok{\# date in y }
    \AttributeTok{index =} \StringTok{"sp500"}\NormalTok{, }\CommentTok{\# index}
    \AttributeTok{est.win =} \DecValTok{250}\NormalTok{,   }\CommentTok{\# estimation window wedith in days}
    \AttributeTok{digits =} \DecValTok{3}\NormalTok{, }
    \AttributeTok{event.date =} \DecValTok{19990505}\NormalTok{, }\CommentTok{\# firm event dates }
    \AttributeTok{event.win =} \DecValTok{5}          \CommentTok{\# one{-}side event window wdith in days (default = 3, where 3 before + 1 event date + 3 days after = 7 days)}
\NormalTok{)}
\NormalTok{hh; }\FunctionTok{plot}\NormalTok{(hh)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} === Regression coefficients by firm =========}
\CommentTok{\#\textgreater{}   N firm event.date alpha.c alpha.e alpha.t alpha.p alpha.s beta.c beta.e}
\CommentTok{\#\textgreater{} 1 1  wpp   19990505  {-}0.135   0.170  {-}0.795   0.428          0.665  0.123}
\CommentTok{\#\textgreater{}   beta.t beta.p beta.s}
\CommentTok{\#\textgreater{} 1  5.419  0.000    ***}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} === Abnormal returns by date ================}
\CommentTok{\#\textgreater{}    day Ait.wpp    HNt}
\CommentTok{\#\textgreater{} 1   {-}5   4.564  4.564}
\CommentTok{\#\textgreater{} 2   {-}4   0.534  5.098}
\CommentTok{\#\textgreater{} 3   {-}3  {-}1.707  3.391}
\CommentTok{\#\textgreater{} 4   {-}2   2.582  5.973}
\CommentTok{\#\textgreater{} 5   {-}1  {-}0.942  5.031}
\CommentTok{\#\textgreater{} 6    0  {-}3.247  1.784}
\CommentTok{\#\textgreater{} 7    1  {-}0.646  1.138}
\CommentTok{\#\textgreater{} 8    2  {-}2.071 {-}0.933}
\CommentTok{\#\textgreater{} 9    3   0.368 {-}0.565}
\CommentTok{\#\textgreater{} 10   4   4.141  3.576}
\CommentTok{\#\textgreater{} 11   5   0.861  4.437}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} === Average abnormal returns across firms ===}
\CommentTok{\#\textgreater{}      name estimate error t.value p.value sig}
\CommentTok{\#\textgreater{} 1 CiT.wpp    4.437 8.888   0.499   0.618    }
\CommentTok{\#\textgreater{} 2     GNT    4.437 8.888   0.499   0.618}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{29-event-study_files/figure-latex/unnamed-chunk-4-1} \end{center}

Example by \href{https://lamfo-unb.github.io/2017/08/17/Teste-de-Eventos-en/}{Ana Julia Akaishi Padula, Pedro Albuquerque (posted on LAMFO)}

Example in \texttt{AbnormalReturns} package

\hypertarget{eventus}{%
\subsection{Eventus}\label{eventus}}

2 types of output:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \begin{itemize}
  \item
    Using different estimation methods (e.g., market model to calendar-time approach)
  \item
    Does not include event-specific returns. Hence, no regression later to determine variables that can affect abnormal stock returns.
  \end{itemize}
\item
  \protect\hyperlink{cross-sectional-analysis-of-eventus}{Cross-sectional Analysis of Eventus}: Event-specific abnormal returns (using monthly or data data) for cross-sectional analysis (under \textbf{Cross-Sectional Analysis} section)

  \begin{itemize}
  \tightlist
  \item
    Since it has the stock-specific abnormal returns, we can do regression on CARs later. But it only gives market-adjusted model. However, according to \citep{sorescu2017}, they advocate for the use of market-adjusted model for the short-term only, and reserve the FF4 for the longer-term event studies using monthly daily.
  \end{itemize}
\end{enumerate}

\hypertarget{basic-event-study}{%
\subsubsection{Basic Event Study}\label{basic-event-study}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Input a text file contains a firm identifier (e.g., PERMNO, CUSIP) and the event date
\item
  Choose market indices: equally weighted and the value weighted index (i.e., weighted by their market capitalization). And check Fama-French and Carhart factors.
\item
  Estimation options

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    Estimation period: \texttt{ESTLEN\ =\ 100} is the convention so that the estimation is not impacted by outliers.
  \item
    Use ``autodate'' options: the first trading after the event date is used if the event falls on a weekend or holiday
  \end{enumerate}
\item
  Abnormal returns window: depends on the specific event
\item
  Choose test: either parametric (including \protect\hyperlink{patell-standardized-residual-psr}{Patell Standardized Residual (PSR)}) or non-parametric
\end{enumerate}

\hypertarget{cross-sectional-analysis-of-eventus}{%
\subsubsection{Cross-sectional Analysis of Eventus}\label{cross-sectional-analysis-of-eventus}}

Similar to the \href{Average\%20abnormal\%20returns\%20across\%20a\%20sample}{Basic Event Study}, but now you can have event-specific abnormal returns.

\hypertarget{evenstudies}{%
\subsection{Evenstudies}\label{evenstudies}}

This package does not use the Fama-French model, only the market models.

This example is by the author of the package

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(eventstudies)}
\CommentTok{\# firm and date data}
\FunctionTok{data}\NormalTok{(}\StringTok{"SplitDates"}\NormalTok{)}
\FunctionTok{head}\NormalTok{(SplitDates)}

\CommentTok{\# stock price data }
\FunctionTok{data}\NormalTok{(}\StringTok{"StockPriceReturns"}\NormalTok{)}
\FunctionTok{head}\NormalTok{(StockPriceReturns)}
\FunctionTok{class}\NormalTok{(StockPriceReturns)}

\NormalTok{es }\OtherTok{\textless{}{-}}
    \FunctionTok{eventstudy}\NormalTok{(}
        \AttributeTok{firm.returns =}\NormalTok{ StockPriceReturns,}
        \AttributeTok{event.list =}\NormalTok{ SplitDates,}
        \AttributeTok{event.window =} \DecValTok{5}\NormalTok{,}
        \AttributeTok{type =} \StringTok{"None"}\NormalTok{,}
        \AttributeTok{to.remap =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{remap =} \StringTok{"cumsum"}\NormalTok{,}
        \AttributeTok{inference =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{inference.strategy =} \StringTok{"bootstrap"}
\NormalTok{    )}

\FunctionTok{plot}\NormalTok{(es)}
\end{Highlighting}
\end{Shaded}

\hypertarget{eventstudy}{%
\subsection{EventStudy}\label{eventstudy}}

You have to pay for the API key. (It's \$10/month).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(EventStudy)}
\end{Highlighting}
\end{Shaded}

\href{https://cran.rstudio.com/web/packages/EventStudy/vignettes/get_started.html}{Example} by the authors of the package

Data Prep

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyquant)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(readr)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"Quandl"}\NormalTok{)}
\FunctionTok{library}\NormalTok{(}\StringTok{"quantmod"}\NormalTok{)}
\FunctionTok{Quandl.auth}\NormalTok{(}\StringTok{"LDqWhYXzVd2omw4zipN2"}\NormalTok{)}
\NormalTok{TWTR }\OtherTok{\textless{}{-}} \FunctionTok{Quandl}\NormalTok{(}\StringTok{"NSE/OIL"}\NormalTok{,}\AttributeTok{type =}\StringTok{"xts"}\NormalTok{)}
\FunctionTok{candleChart}\NormalTok{(TWTR)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{29-event-study_files/figure-latex/unnamed-chunk-8-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{addSMA}\NormalTok{(}\AttributeTok{col=}\StringTok{"red"}\NormalTok{) }\CommentTok{\#Adding a Simple Moving Average}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{29-event-study_files/figure-latex/unnamed-chunk-8-2} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{addEMA}\NormalTok{() }\CommentTok{\#Adding an Exponential Moving Average}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{29-event-study_files/figure-latex/unnamed-chunk-8-3} \end{center}

Reference market in Germany is DAX

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Index Data}
\CommentTok{\# indexName \textless{}{-} c("DAX")}

\NormalTok{indexData }\OtherTok{\textless{}{-}} \FunctionTok{tq\_get}\NormalTok{(}\StringTok{"\^{}GDAXI"}\NormalTok{, }\AttributeTok{from =} \StringTok{"2014{-}05{-}01"}\NormalTok{, }\AttributeTok{to =} \StringTok{"2015{-}12{-}31"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{date =} \FunctionTok{format}\NormalTok{(date, }\StringTok{"\%d.\%m.\%Y"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{symbol =} \StringTok{"DAX"}\NormalTok{)}

\FunctionTok{head}\NormalTok{(indexData)}
\end{Highlighting}
\end{Shaded}

Create files

\begin{itemize}
\tightlist
\item
  \texttt{01\_RequestFile.csv}
\item
  \texttt{02\_FirmData.csv}
\item
  \texttt{03\_MarketData.csv}
\end{itemize}

Calculating abnormal returns

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# get \& set parameters for abnormal return Event Study}
\CommentTok{\# we use a garch model and csv as return}
\CommentTok{\# Attention: fitting a GARCH(1, 1) model is compute intensive}
\NormalTok{esaParams }\OtherTok{\textless{}{-}}\NormalTok{ EventStudy}\SpecialCharTok{::}\NormalTok{ARCApplicationInput}\SpecialCharTok{$}\FunctionTok{new}\NormalTok{()}
\NormalTok{esaParams}\SpecialCharTok{$}\FunctionTok{setResultFileType}\NormalTok{(}\StringTok{"csv"}\NormalTok{)}
\NormalTok{esaParams}\SpecialCharTok{$}\FunctionTok{setBenchmarkModel}\NormalTok{(}\StringTok{"garch"}\NormalTok{)}


\NormalTok{dataFiles }\OtherTok{\textless{}{-}}
    \FunctionTok{c}\NormalTok{(}
        \StringTok{"request\_file"} \OtherTok{=} \FunctionTok{file.path}\NormalTok{(}\FunctionTok{getwd}\NormalTok{(), }\StringTok{"data"}\NormalTok{, }\StringTok{"EventStudy"}\NormalTok{, }\StringTok{"01\_requestFile.csv"}\NormalTok{),}
        \StringTok{"firm\_data"}    \OtherTok{=} \FunctionTok{file.path}\NormalTok{(}\FunctionTok{getwd}\NormalTok{(), }\StringTok{"data"}\NormalTok{, }\StringTok{"EventStudy"}\NormalTok{, }\StringTok{"02\_firmDataPrice.csv"}\NormalTok{),}
        \StringTok{"market\_data"}  \OtherTok{=} \FunctionTok{file.path}\NormalTok{(}\FunctionTok{getwd}\NormalTok{(), }\StringTok{"data"}\NormalTok{, }\StringTok{"EventStudy"}\NormalTok{, }\StringTok{"03\_marketDataPrice.csv"}\NormalTok{)}
\NormalTok{    )}

\CommentTok{\# check data files, you can do it also in our R6 class}
\NormalTok{EventStudy}\SpecialCharTok{::}\FunctionTok{checkFiles}\NormalTok{(dataFiles)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{arEventStudy }\OtherTok{\textless{}{-}}\NormalTok{ estSetup}\SpecialCharTok{$}\FunctionTok{performEventStudy}\NormalTok{(}\AttributeTok{estParams     =}\NormalTok{ esaParams, }
                                      \AttributeTok{dataFiles     =}\NormalTok{ dataFiles, }
                                      \AttributeTok{downloadFiles =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(EventStudy)}

\NormalTok{apiUrl }\OtherTok{\textless{}{-}} \StringTok{"https://api.eventstudytools.com"}
\FunctionTok{Sys.setenv}\NormalTok{(}\AttributeTok{EventStudyapiKey =} \StringTok{""}\NormalTok{)}

\CommentTok{\# The URL is already set by default}
\FunctionTok{options}\NormalTok{(}\AttributeTok{EventStudy.URL =}\NormalTok{ apiUrl)}
\FunctionTok{options}\NormalTok{(}\AttributeTok{EventStudy.KEY =} \FunctionTok{Sys.getenv}\NormalTok{(}\StringTok{"EventStudyapiKey"}\NormalTok{))}

\CommentTok{\# use EventStudy estAPIKey function}
\FunctionTok{estAPIKey}\NormalTok{(}\FunctionTok{Sys.getenv}\NormalTok{(}\StringTok{"EventStudyapiKey"}\NormalTok{))}

\CommentTok{\# initialize object}
\NormalTok{estSetup }\OtherTok{\textless{}{-}}\NormalTok{ EventStudyAPI}\SpecialCharTok{$}\FunctionTok{new}\NormalTok{()}
\NormalTok{estSetup}\SpecialCharTok{$}\FunctionTok{authentication}\NormalTok{(}\AttributeTok{apiKey =} \FunctionTok{Sys.getenv}\NormalTok{(}\StringTok{"EventStudyapiKey"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{instrumental-variables}{%
\chapter{Instrumental Variables}\label{instrumental-variables}}

Similar to \protect\hyperlink{experimental-design}{RCT}, we try to introduce randomization (random assignment to treatment) to our treatment variable by using only variation in the instrument.

Logic of using an instrument:

\begin{itemize}
\item
  Use only exogenous variation to see the variation in treatment (try to exclude all endogenous variation in the treatment)
\item
  Use only exogenous variation to see the variation in outcome (try to exclude all endogenous variation in the outcome)
\item
  See the relationship between treatment and outcome in terms of residual variations that are exogenous to omitted variables.
\end{itemize}

\textbf{Notes}:

\begin{itemize}
\item
  Instruments can be used to remove attenuation bias in errors-in-variables.
\item
  Be careful with the F-test and standard errors when you do 2SLS by hand (you need to correct them).
\item
  Repeated use of related IVs across different studies can collectively invalidate these instruments, primarily through the violation of the exclusion restriction \citep{gallen2020broken}. One needs to test for invalid instruments (Hausman-like test).

  \begin{itemize}
  \tightlist
  \item
    \citet{mellon2023rain} shows the widespread use of weather as an instrument in social sciences (289 studies linking weather to 195 variables) demonstrates significant exclusion violations that can overturn many IV results.
  \end{itemize}
\item
  For {[}Zero-valued Outcomes{]}, we can't directly interpret the treatment coefficient of log-transformed outcome regression as percentage change \citep{chen2023logs}. We have to distinguish the treatment effect on the intensive (outcome: 10 to 11) vs.~extensive margins (outcome: 0 to 1), and we can't readily interpret the treatment coefficient of log-transformed outcome regression as percentage change. To have percentage change interpretation, we can either do:

  \begin{itemize}
  \item
    Proportional LATE: estimate \(\theta_{ATE\%}\) for those who are compliers under the instrument. To estimate proportional LATE,

    \begin{enumerate}
    \def\labelenumi{\arabic{enumi}.}
    \item
      Regress \(Y_i = \beta D_i + X_i + \epsilon_i\) using 2SLS with an instrument on \(D_i\), where \(\beta\) is interpreted as the LATE in levels of the control group's mean for compliers.
    \item
      Get estimate of the control complier mean by regressing with same 2SLS regression \citep{abadie2002instrumental} where the final outcome is \(-(D_i - 1)Y_i\) , we refer to the new new estimated effect of \(D_i\) as \(\beta_{cc}\)
    \item
      The \(\theta_{ATE \%}\) for compliers that are induced by the instrument is \(\hat{\beta}/\hat{\beta}_{cc}\), where it can be interpreted directly as the percentage change for compliers that are induced by the instrument under treatment as compared to under control.
    \item
      SE can be obtained by non-parametric bootstrap.
    \item
      For specific case that the instrument is binary, \(\theta\) of the intensive margin for compliers can be directly obtained by Poisson IV regression (\texttt{ivpoisson} in Stata).
    \end{enumerate}
  \item
    \citet{lee2009training} bounds: we can get bounds for the average treatment effect in logs for compliers who have positive outcome regardless of treatment status (i.e., intensive-margin effect). This requires a monotonicity assumption for compliers where they should still have positive outcome regardless of treatment status.
  \end{itemize}
\end{itemize}

\textbf{Notes} on First-stage:

\begin{itemize}
\item
  Always use the OLS regression in the first stage (regardless of the type of endogenous variables - e.g., continuous or discreet) (suggested by \citep{angrist2009mostly}. Estimates of IV can still be consistent regardless of the form of the endogenous variables (discreet vs.~continuous).

  \begin{itemize}
  \tightlist
  \item
    Alternatively, we could use ``biprobit'' model, but this is applicable only in cases where you have both dependent and endogenous variables to be binary.
  \end{itemize}
\item
  If you still want to continue and use logit or probit models for the first stage when you have binary variables, you have a ``\href{https://www.statalist.org/forums/forum/general-stata-discussion/general/1379449-two-step-iv-method-with-binary-dependent-variable}{forbidden regression}'' (also \href{https://stats.stackexchange.com/questions/125830/consistency-of-2sls-with-binary-endogenous-variable}{1}, \href{https://stats.stackexchange.com/questions/94063/probit-two-stage-least-squares-2sls/94392\#94392}{2}) (i.e., an incorrect extension of 2SLS to a nonlinear case).
\end{itemize}

There are several ways to understand this problem:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Identification strategy}: The identification strategy in instrumental variables analysis relies on the fact that the instrumental variable affects the outcome variable only through its effect on the endogenous variable. However, when the endogenous variable is binary, the relationship between the instrumental variable and the endogenous variable is not continuous. This means that the instrumental variable can only affect the endogenous variable in discrete jumps, rather than through a continuous change. As a result, the identification of the causal effect of the endogenous variable on the outcome variable may not be possible with probit or logit regression in the first stage.
\item
  \textbf{Model assumptions}: Both models assume that the error term has a specific distribution (normal or logistic), and that the probability of the binary outcome is a function of the linear combination of the regressors.

  When the endogenous variable is binary, however, the distribution of the error term is not specified, as there is no continuous relationship between the endogenous variable and the outcome variable. This means that the assumptions of the probit and logit models may not hold, and the resulting estimates may not be reliable or interpretable.
\item
  \textbf{Issue of weak instruments}: When the instrument is weak, the variance of the inverse Mills ratio (which is used to correct for endogeneity in instrumental variables analysis) can be very large. In the case of binary endogenous variables, the inverse Mills ratio cannot be consistently estimated using probit or logit regression, and this can lead to biased and inconsistent estimates of the causal effect of the endogenous variable on the outcome variable.
\end{enumerate}

Problems with weak instruments \citep{bound1995problems}:

\begin{itemize}
\item
  Weak instrumental variables can produce (finite-sample) \textbf{biased} and \textbf{inconsistent} estimates of the causal effect of an endogenous variable on an outcome variable (even in the presence of large sample size)
\item
  In a finite sample, instrumental variables (IV) estimates can be biased in the same direction as ordinary least squares (OLS) estimates. Additionally, the bias of IV estimates approaches that of OLS estimates as the correlation (R2) between the instruments and the endogenous explanatory variable approaches zero. This means that when the correlation between the instruments and the endogenous variable is weak, the bias of the IV estimates can be similar to that of the OLS estimates.
\item
  Weak instruments are problematic because they do not have enough variation to fully capture the variation in the endogenous variable, leading to measurement error and other sources of noise in the estimates.
\item
  Using weak instruments can produce large standard errors and low t-ratio. And when the feedback (reverse causality) is strong, the bias in IV is even greater than that of OLS \citep{nelson1988distribution}.
\item
  Using lagged dependent variables as instruments for current values depends on serial correlations, typically low \citep{nelson1988distribution}.
\item
  Using multiple covariates to artificially increase the first-stage \(R^2\) does not solve this weak instrument problem \citep{nelson1988distribution}.
\item
  Solutions:

  \begin{itemize}
  \item
    use of multiple instruments
  \item
    use of instrumental variables with higher correlation
  \item
    use of alternative estimation methods such as limited information maximum likelihood (LIML) or two-stage least squares (2SLS) with heteroscedasticity-robust standard errors.
  \end{itemize}
\end{itemize}

Instrument Validity:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Random assignment (\protect\hyperlink{exogeneity-assumption}{Exogeneity Assumption}).
\item
  Any effect of the instrument on the outcome must be through the endogenous variable (\protect\hyperlink{relevance-assumption}{Relevance Assumption}).
\end{enumerate}

\hypertarget{framework}{%
\section{Framework}\label{framework}}

\begin{itemize}
\item
  \(D_i \sim Bern\) Dummy Treatment
\item
  \(Y_{0i}, Y_{1i}\) potential outcomes
\item
  \(Y_i = Y_{0i} + (Y_{1i} - Y_{0i}) D_i\) observed outcome
\item
  \(Z_i \perp Y_{0i}, Y_{1i}\) Instrumental variables (and also correlate with \(D_i\))
\end{itemize}

Under constant-effects and linear (\(Y_{1i} - Y_{0i}\) are the same for everyone)

\[ \begin{aligned} Y_{0i} &= \alpha + \eta_i \\ Y_{1i} - Y_{0i} &= \rho \\ Y_i &= Y_{0i} + D_i (Y_{1i} - Y_{0i}) \\ &= \alpha + \eta_i  + D_i \rho \\ &= \alpha + \rho D_i + \eta_i \end{aligned} \]

where

\begin{itemize}
\item
  \(\eta_i\) is individual differences
\item
  \(\rho\) is the difference between treated outcome and untreated outcome. Here we assume they are constant for everyone
\end{itemize}

However, we have a problem with OLS because \(D_i\) is correlated with \(\eta_i\) for each unit

But \(Z_i\) can come to the rescue, the causal estimate can be written as

\[ \begin{aligned} \rho &= \frac{Cov( Y_i, Z_i)}{Cov(D_i, Z_i)} \\ &= \frac{Cov(Y_i, Z_i) / V(Z_i) }{Cov( D_i, Z_i) / V(Z_i)} = \frac{Reduced form}{First-stage} \\ &= \frac{E[Y_i |Z_i = 1] - E[Y_i | Z_i = 0]}{E[D_i |Z_i = 1] - E[D_i | Z_i = 0 ]} \end{aligned} \]

Under heterogeneous treatment effect (\(Y_{1i} - Y_{0i}\) are different for everyone) with LATE framework

\(Y_i(d,z)\) denotes the potential outcome for unit \(i\) with treatment \(D_i = d\) and instrument \(Z_i = z\)

Observed treatment status

\[ D_i = D_{0i} + Z_i (D_{1i} - D_{0i}) \]

where

\begin{itemize}
\item
  \(D_{1i}\) is treatment status of unit \(i\) when \(z_i = 1\)
\item
  \(D_{0i}\) is treatment status of unit \(i\) when \(z_i = 0\)
\item
  \(D_{1i} - D_{0i}\) is the causal effect of \(Z_i\) on \(D_i\)
\end{itemize}

\textbf{Assumptions}

\begin{itemize}
\item
  \ul{\textbf{\emph{Independence}}}: The instrument is randomly assigned (i.e., independent of potential outcomes and potential treatments)

  \begin{itemize}
  \item
    \([\{Y_i(d,z); \forall d, z \}, D_{1i}, D_{0i} ] \Pi Z_i\)
  \item
    This assumption let the first-stage equation be the average causal effect of \(Z_i\) on \(D_i\)
  \end{itemize}

  \[ \begin{aligned} E[D_i |Z_i = 1] - E[D_i | Z_i = 0] &= E[D_{1i} |Z_i = 1] - E[D_{0i} |Z_i = 0] \\ &= E[D_{1i} - D_{0i}] \end{aligned} \]

  \begin{itemize}
  \tightlist
  \item
    This assumption also is sufficient for a causal interpretation of the reduced form, where we see the effect of the instrument on the outcome.
  \end{itemize}
\end{itemize}

\[ E[Y_i |Z_i = 1 ] - E[Y_i|Z_i = 0] = E[Y_i (D_{1i}, Z_i = 1) - Y_i (D_{0i} , Z_i = 0)] \]

\begin{itemize}
\item
  \ul{\textbf{\emph{Exclusion}}} (i.e., existence of instruments \citep{imbens1994identification}

  \begin{itemize}
  \tightlist
  \item
    The treatment \(D_i\) fully mediates the effect of \(Z_i\) on \(Y_i\)
  \end{itemize}

  \[ Y_{1i} = Y_i (1,1) = Y_i (1,0) \\  Y_{0i} = Y_i (0,1) = Y_i (0, 0) \]

  \begin{itemize}
  \tightlist
  \item
    With this assumption, the observed outcome \(Y_i\) can be thought of as (assume \(Y_{1i}, Y_{0i}\) already satisfy the independence assumption)
  \end{itemize}

  \[ \begin{aligned} Y_i &= Y_i (0, Z_i) + [Y_i (1 , Z_i) - Y_i (0, Z_i)] D_i \\ &= Y_{0i} + (Y_{1i} - Y_{0i} ) D_i \end{aligned} \]

  \begin{itemize}
  \tightlist
  \item
    This assumption let us go from reduced-form causal effects to treatment effects \citep{angrist1995two}
  \end{itemize}
\item
  \ul{\textbf{\emph{Monotonicity}}}: \(D_{1i} > D_{0i} \forall i\)

  \begin{itemize}
  \item
    With this assumption, we have \(E[D_{1i} - D_{0i} ] = P[D_{1i} > D_{0i}]\)
  \item
    This assumption lets us assume that there is a first stage, in which we examine the proportion of the population that \(D_i\) is driven by \(Z_i\)
  \item
    This assumption is used to solve to problem of the shifts between participation status back to non-participation status.

    \begin{itemize}
    \item
      Alternatively, one can solve the same problem by assuming constant (homogeneous) treatment effect \citep{imbens1994identification}, but this is rather restrictive.
    \item
      A third solution is the assumption that there exists a value of the instrument, where the probability of participation conditional on that value is 0 \citep[\citet{angrist1991sources}]{heckman1990varieties}.
    \end{itemize}
  \end{itemize}
\end{itemize}

With these three assumptions, we have the LATE theorem \citep[4.4.1]{angrist2009mostly}

\[ \frac{E[Y_i | Z_i = 1] - E[Y_i | Z_i = 0]}{E[D_i |Z_i = 1] - E[D_i |Z_i = 0]} = E[Y_{1i} - Y_{0i} | D_{1i} > D_{0i}] \]

LATE assumptions allow us to go back to the types of subjects we have in \protect\hyperlink{causal-inference}{Causal Inference}

\begin{itemize}
\item
  Switchers:

  \begin{itemize}
  \tightlist
  \item
    Compliers: \(D_{1i} > D_{0i}\)
  \end{itemize}
\item
  Non-switchers:

  \begin{itemize}
  \item
    Always-takers: \(D_{1i} = D_{0i} = 1\)
  \item
    Never-takers: \(D_{1i} = D_{0i} = 0\)
  \end{itemize}
\end{itemize}

\protect\hyperlink{instrumental-variables}{Instrumental Variables} can't say anything about non-switchers because treatment status \(D_i\) has no effects on them (similar to fixed effects models).

When all groups are the same, we come back to the constant-effects world.

Treatment effects on the treated is a weighted average of always-takers and compliers.

In the special case of IV in randomized trials, we have a compliance problem (when compliance is voluntary), where those in the treated will not always take the treatment (i.e., might be selection bias).

\begin{itemize}
\item
  \textbf{Intention-to-treat analysis} is valid, but contaminated by non-compliance
\item
  IV in this case (\(Z_i\) = random assignment to the treatment; \(D_i\) = whether the unit actually received/took the treatment) can solve this problem.
\item
  Under certain assumptions (i.e., SUTVA, random assignment, exclusion restriction, no defiers, and monotinicity), this analysis can give causal interpreation of LATE because it's the average causal effect for the compliers only.

  \begin{itemize}
  \tightlist
  \item
    Without these assumptions, it's a ratio of intention-to-treat.
  \end{itemize}
\item
  Without always-takers in this case, LATE = Treatment effects on the treated
\end{itemize}

See proof \citet{bloom1984accounting} and examples \citet{bloom1997benefits} and \citet{sherman1984minneapolis}

\[ \frac{E[Y_i |Z_i = 1] - E[Y_i |Z_i = 0]}{E[D_i |Z_i = 1]} = \frac{\text{Intention-to-treat effect}}{\text{Compliance rate}} \\ = E[Y_{1i} - Y_{0i} |D_i = 1] \]

\hypertarget{estimation-1}{%
\section{Estimation}\label{estimation-1}}

\hypertarget{sls-estimation}{%
\subsection{2SLS Estimation}\label{sls-estimation}}

A special case of \protect\hyperlink{iv-gmm}{IV-GMM}

Examples by authors of \texttt{fixest} package

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(fixest)}
\NormalTok{base }\OtherTok{=}\NormalTok{ iris}
\FunctionTok{names}\NormalTok{(base) }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"y"}\NormalTok{, }\StringTok{"x1"}\NormalTok{, }\StringTok{"x\_endo\_1"}\NormalTok{, }\StringTok{"x\_inst\_1"}\NormalTok{, }\StringTok{"fe"}\NormalTok{)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\NormalTok{base}\SpecialCharTok{$}\NormalTok{x\_inst\_2 }\OtherTok{=} \FloatTok{0.2} \SpecialCharTok{*}\NormalTok{ base}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{+} \FloatTok{0.2} \SpecialCharTok{*}\NormalTok{ base}\SpecialCharTok{$}\NormalTok{x\_endo\_1 }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{150}\NormalTok{, }\AttributeTok{sd =} \FloatTok{0.5}\NormalTok{)}
\NormalTok{base}\SpecialCharTok{$}\NormalTok{x\_endo\_2 }\OtherTok{=} \FloatTok{0.2} \SpecialCharTok{*}\NormalTok{ base}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{{-}} \FloatTok{0.2} \SpecialCharTok{*}\NormalTok{ base}\SpecialCharTok{$}\NormalTok{x\_inst\_1 }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{150}\NormalTok{, }\AttributeTok{sd =} \FloatTok{0.5}\NormalTok{)}

\CommentTok{\# est\_iv = feols(y \textasciitilde{} x1 | x\_endo\_1  \textasciitilde{} x\_inst\_1 , base)}
\NormalTok{est\_iv }\OtherTok{=} \FunctionTok{feols}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x1 }\SpecialCharTok{|}\NormalTok{ x\_endo\_1 }\SpecialCharTok{+}\NormalTok{ x\_endo\_2 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x\_inst\_1 }\SpecialCharTok{+}\NormalTok{ x\_inst\_2, base)}
\NormalTok{est\_iv}
\CommentTok{\#\textgreater{} TSLS estimation {-} Dep. Var.: y}
\CommentTok{\#\textgreater{}                   Endo.    : x\_endo\_1, x\_endo\_2}
\CommentTok{\#\textgreater{}                   Instr.   : x\_inst\_1, x\_inst\_2}
\CommentTok{\#\textgreater{} Second stage: Dep. Var.: y}
\CommentTok{\#\textgreater{} Observations: 150}
\CommentTok{\#\textgreater{} Standard{-}errors: IID }
\CommentTok{\#\textgreater{}              Estimate Std. Error  t value   Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)  1.831380   0.411435  4.45121 1.6844e{-}05 ***}
\CommentTok{\#\textgreater{} fit\_x\_endo\_1 0.444982   0.022086 20.14744  \textless{} 2.2e{-}16 ***}
\CommentTok{\#\textgreater{} fit\_x\_endo\_2 0.639916   0.307376  2.08186 3.9100e{-}02 *  }
\CommentTok{\#\textgreater{} x1           0.565095   0.084715  6.67051 4.9180e{-}10 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} RMSE: 0.398842   Adj. R2: 0.761653}
\CommentTok{\#\textgreater{} F{-}test (1st stage), x\_endo\_1: stat = 903.2    , p \textless{} 2.2e{-}16 , on 2 and 146 DoF.}
\CommentTok{\#\textgreater{} F{-}test (1st stage), x\_endo\_2: stat =   3.25828, p = 0.041268, on 2 and 146 DoF.}
\CommentTok{\#\textgreater{}                   Wu{-}Hausman: stat =   6.79183, p = 0.001518, on 2 and 144 DoF.}
\end{Highlighting}
\end{Shaded}

Default statistics

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  F-test first-stage (weak instrument test)
\item
  Wu-Hausman endogeneity test
\item
  Over-identifying restriction (Sargan) J-test
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{fitstat}\NormalTok{(}
\NormalTok{    est\_iv,}
    \AttributeTok{type =} \FunctionTok{c}\NormalTok{(}
        \StringTok{"n"}\NormalTok{, }\StringTok{"ll"}\NormalTok{, }\StringTok{"aic"}\NormalTok{, }\StringTok{"bic"}\NormalTok{, }\StringTok{"rmse"}\NormalTok{, }\CommentTok{\# ll means log{-}likelihood}
        
        \StringTok{"my"}\NormalTok{, }\CommentTok{\# mean dependent var}

        \StringTok{"g"}\NormalTok{, }\CommentTok{\# degrees of freedom used to compute the t{-}test}

        \StringTok{"r2"}\NormalTok{, }\StringTok{"ar2"}\NormalTok{, }\StringTok{"wr2"}\NormalTok{, }\StringTok{"awr2"}\NormalTok{, }\StringTok{"pr2"}\NormalTok{, }\StringTok{"apr2"}\NormalTok{, }\StringTok{"wpr2"}\NormalTok{, }\StringTok{"awpr2"}\NormalTok{,}

        \StringTok{"theta"}\NormalTok{, }\CommentTok{\# over{-}dispersion parameter in Negative Binomial models}

        \StringTok{"f"}\NormalTok{, }\StringTok{"wf"}\NormalTok{, }\CommentTok{\# F{-}tests of nullity of the coefficients}

        \StringTok{"wald"}\NormalTok{, }\CommentTok{\# Wald test of joint nullity of the coefficients}

        \StringTok{"ivf"}\NormalTok{,}
        
        \StringTok{"ivf1"}\NormalTok{,}

        \StringTok{"ivf2"}\NormalTok{,}

        \StringTok{"ivfall"}\NormalTok{,}
        
        \StringTok{"ivwald"}\NormalTok{, }\StringTok{"ivwald1"}\NormalTok{, }\StringTok{"ivwald2"}\NormalTok{, }\StringTok{"ivwaldall"}\NormalTok{,}

        \StringTok{"cd"}
        
        \CommentTok{\# "kpr"}
        
        
\NormalTok{        ),}
    \AttributeTok{cluster =} \StringTok{\textquotesingle{}fe\textquotesingle{}}
\NormalTok{)}
\CommentTok{\#\textgreater{}                 Observations: 150}
\CommentTok{\#\textgreater{}               Log{-}Likelihood: {-}75.0}
\CommentTok{\#\textgreater{}                          AIC: 157.9}
\CommentTok{\#\textgreater{}                          BIC: 170.0}
\CommentTok{\#\textgreater{}                         RMSE: 0.398842}
\CommentTok{\#\textgreater{}               Dep. Var. mean: 5.84333}
\CommentTok{\#\textgreater{}                            G: 3}
\CommentTok{\#\textgreater{}                           R2: 0.766452}
\CommentTok{\#\textgreater{}                      Adj. R2: 0.761653}
\CommentTok{\#\textgreater{}                    Within R2: NA}
\CommentTok{\#\textgreater{}                         awr2: NA}
\CommentTok{\#\textgreater{}                    Pseudo R2: 0.592684}
\CommentTok{\#\textgreater{}               Adj. Pseudo R2: 0.576383}
\CommentTok{\#\textgreater{}             Within Pseudo R2: NA}
\CommentTok{\#\textgreater{}                        awpr2: NA}
\CommentTok{\#\textgreater{}              Over{-}dispersion: NA}
\CommentTok{\#\textgreater{}                       F{-}test: stat =       1.80769, p = 0.375558, on 3 and 2 DoF.}
\CommentTok{\#\textgreater{}           F{-}test (projected): NA}
\CommentTok{\#\textgreater{}         Wald (joint nullity): stat = 539,363.2    , p \textless{} 2.2e{-}16 , on 3 and 146 DoF, VCOV: Clustered (fe).}
\CommentTok{\#\textgreater{} F{-}test (1st stage), x\_endo\_1: stat =     903.2    , p \textless{} 2.2e{-}16 , on 2 and 146 DoF.}
\CommentTok{\#\textgreater{} F{-}test (1st stage), x\_endo\_2: stat =       3.25828, p = 0.041268, on 2 and 146 DoF.}
\CommentTok{\#\textgreater{}           F{-}test (2nd stage): stat =     194.2    , p \textless{} 2.2e{-}16 , on 2 and 146 DoF.}
\CommentTok{\#\textgreater{}             F{-}test (IV only): stat =     194.2    , p \textless{} 2.2e{-}16 , on 2 and 146 DoF.}
\CommentTok{\#\textgreater{} Wald (1st stage), x\_endo\_1  : stat =   1,482.6    , p \textless{} 2.2e{-}16 , on 2 and 146 DoF, VCOV: Clustered (fe).}
\CommentTok{\#\textgreater{} Wald (1st stage), x\_endo\_2  : stat =       2.22157, p = 0.112092, on 2 and 146 DoF, VCOV: Clustered (fe).}
\CommentTok{\#\textgreater{}             Wald (2nd stage): stat = 539,363.2    , p \textless{} 2.2e{-}16 , on 2 and 146 DoF, VCOV: Clustered (fe).}
\CommentTok{\#\textgreater{}               Wald (IV only): stat = 539,363.2    , p \textless{} 2.2e{-}16 , on 2 and 146 DoF, VCOV: Clustered (fe).}
\CommentTok{\#\textgreater{}                 Cragg{-}Donald: 3.11162}
\end{Highlighting}
\end{Shaded}

To set default printing

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# always add second{-}stage Wald test}
\FunctionTok{setFixest\_print}\NormalTok{(}\AttributeTok{fitstat =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ . }\SpecialCharTok{+}\NormalTok{ ivwald2)}
\NormalTok{est\_iv}
\end{Highlighting}
\end{Shaded}

To see results from different stages

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# first{-}stage}
\FunctionTok{summary}\NormalTok{(est\_iv, }\AttributeTok{stage =} \DecValTok{1}\NormalTok{)}

\CommentTok{\# second{-}stage}
\FunctionTok{summary}\NormalTok{(est\_iv, }\AttributeTok{stage =} \DecValTok{2}\NormalTok{)}

\CommentTok{\# both stages}
\FunctionTok{etable}\NormalTok{(}\FunctionTok{summary}\NormalTok{(est\_iv, }\AttributeTok{stage =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{), }\AttributeTok{fitstat =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ . }\SpecialCharTok{+}\NormalTok{ ivfall }\SpecialCharTok{+}\NormalTok{ ivwaldall.p)}
\FunctionTok{etable}\NormalTok{(}\FunctionTok{summary}\NormalTok{(est\_iv, }\AttributeTok{stage =} \DecValTok{2}\SpecialCharTok{:}\DecValTok{1}\NormalTok{), }\AttributeTok{fitstat =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ . }\SpecialCharTok{+}\NormalTok{ ivfall }\SpecialCharTok{+}\NormalTok{ ivwaldall.p)}
\CommentTok{\# .p means p{-}value, not statistic}
\CommentTok{\# \textasciigrave{}all\textasciigrave{} means IV only}
\end{Highlighting}
\end{Shaded}

\hypertarget{iv-gmm}{%
\subsection{IV-GMM}\label{iv-gmm}}

This is a more general framework.

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{sls-estimation}{2SLS Estimation} is a special case of \protect\hyperlink{iv-gmm}{IV-GMM} estimator
\end{itemize}

\[
Y = X \beta + u, u \sim (0, \Omega)
\]

where

\begin{itemize}
\tightlist
\item
  \(X\) is a matrix of endogenous variables (\(N\times k\))
\end{itemize}

We will use a matrix of instruments \(X\) where it has \(N \times l\) dimensions (where \(l \ge k\))

Then, we can have a set of \(l\) moments:

\[
g_i (\beta) = Z_i' u_i = Z_i' (Y_i - X_i \beta)
\]

where

\begin{itemize}
\tightlist
\item
  \(i \in (1,N)\)
\end{itemize}

Each \(l\) moment equation is a sample moment, which can be estimated by averaging over \(N\)

\[
\bar{g}(\beta) = \frac{1}{N} \sum_{i = 1}^N Z_i (Y_i - X_i \beta) = \frac{1}{N} Z'u
\]

GMM then estimate \(\beta\) so that \(\bar{g}(\hat{\beta}_{GMM}) = 0\)

When \(l = k\) there is a unique solution to this system of equations (and equivalent to the IV estimator)

\[
\hat{\beta}_{IV} = (Z'X)^{-1}Z'Y
\]

When \(l > k\), we have a set of \(k\) instruments

\[
\hat{X} = Z(Z'Z)^{-1} Z' X = P_ZX
\]

then we can use the 2SLS estimator

\[
\begin{aligned}
\hat{\beta}_{2SLS} &= (\hat{X}'X)^{-1} \hat{X}' Y \\
&= (X'P_Z X)^{-1}X' P_Z Y
\end{aligned}
\]

Differences between 2SLS and IV-GMM:

\begin{itemize}
\item
  In the 2SLS method, when there are more instruments available than what is actually needed for the estimation, to address this, a matrix is created that only includes the necessary instruments, which simplifies the calculation.
\item
  The IV-GMM method uses all the available instruments, but applies a weighting system to prioritize the instruments that are most relevant. This approach is useful when there are more instruments than necessary, which can make the calculation more complex. The IV-GMM method uses a criterion function to weight the estimates and improve their accuracy.
\item
  \textbf{In short, always use IV-GMM when you have overid problems}
\end{itemize}

GMM estimator minimizes

\[
J (\hat{\beta}_{GMM} ) = N \bar{g}(\hat{\beta}_{GMM})' W \bar{g} (\hat{\beta}_{GMM})
\]

where \(W\) is a symmetric weighting matrix \(l \times l\)

For an overid equation, solving the set of FOCs for the IV-GMM estimator, we should have

\[
\hat{\beta}_{GMM} = (X'ZWZ' X)^{-1} X'ZWZ'Y
\]

which is identical for all \(W\) matrices. The optimal \(W = S^{-1}\) \citep{hansen1982large} where \(S\) is the covariance matrix of the moment conditions to produce the most efficient estimator:

\[
S = E[Z'uu'Z] = \lim_{N \to \infty} N^{-1}[Z' \Omega Z]
\]

With a consistent estimator of \(S\) from the 2SLS residuals, the feasible IV-GMM estimator can be defined as

\[
\hat{\beta}_{FEGMM} = (X'Z \hat{S}^{-1} Z' X)^{-1} X'Z \hat{S}^{-1} Z'Y
\]

In cases where \(\Omega\) (i.e., the vcov of the error process \(u\)) satisfy all classical assumptions

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  IID
\item
  \(S = \sigma^2_u I_N\)
\item
  The optimal weighting matrix is proportional to the identity matrix
\end{enumerate}

Then, IV-GMM estimator is the standard IV (or 2SLS) estimator.

For IV-GMM, you also have an additional test of overid restrictions: GMM distance (also known as Hayashi C statistic)

To account for clustering, one can use code provided by this \href{https://www.r-bloggers.com/2014/04/iv-estimates-via-gmm-with-clustering-in-r/}{blog}

\hypertarget{inference-2}{%
\section{Inference}\label{inference-2}}

Under just-identified instrument variable model, we have

\[
Y = \beta X + u
\]

where \(corr(u, Z) = 0\) (relevant assumption) and \(corr(Z,X) \neq 0\) (exogenous assumption)

The t-ratio approach to construct the 95 CIs is

\[
\hat{\beta} \pm 1.96 \sqrt{\hat{V}_N(\hat{\beta})}
\]

But this is wrong, and has been long recognized by those who understand the ``weak instruments'' problem \citep[\citet{dufour1997some}]{staiger1997instrumental}

To test the null hypothesis of \(\beta = \beta_0\) \citep{lee2022valid} \[ \frac{(\hat{\beta} - \beta_0)^2}{\hat{V}_N(\hat{\beta})} = \hat{t}^2 = \hat{t}^2_{AR} \times \frac{1}{1 - \hat{\rho} \frac{\hat{t}_{AR}}{\hat{f}} + \frac{\hat{t}^2_{AR}}{\hat{f}^2}} \] where \(\hat{t}_{AR}^2 \sim \chi^2(1)\) (even with weak instruments) \citep{anderson1949estimation}

\[
\hat{t}_{AR} = \frac{\hat{\pi}(\hat{\beta} - \beta_0)}{\sqrt{\hat{V}_N (\hat{\pi} (\hat{\beta} - \beta_0))}} \sim N(0,1)
\]

where

\begin{itemize}
\item
  \(\hat{f} = \frac{\hat{\pi}}{\sqrt{\hat{V}_N(\hat{\pi})}}\sim N\)
\item
  \(\hat{\pi}\) = 1st-stage coefficient
\item
  \(\hat{\rho} = COV(Zv, Zu)\) = correlation between the 1st-stage residual and an estimate of \(u\)
\end{itemize}

Even in large samples, \(\hat{t}^2 \neq \hat{t}^2_{AR}\) because the right-hand term does not have a degenerate distribution. Thus, the normal t critical values wouldn't work.

The t-ratios does not match that of standard normal, but it matches the proposed density by \citet{staiger1997instrumental} and \citet{stock2005testing} .

The deviation between \(\hat{t}^2 , \hat{t}^2_{AR}\) depends on

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(\pi\) (i.e., correlation between the instrument and the endogenous variable)
\item
  \(E(F)\) (i.e., strength of the first-stage)
\item
  Magnitude of \(|\rho|\) (i.e., degree of endogeneity)
\end{enumerate}

Hence, we can think of several scenarios:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Worst case}: Very weak first stage (\(\pi = 0\)) and high degree of endogeneity (\(|\rho |= 1\)).
\end{enumerate}

\begin{itemize}
\item
  The interval \(\hat{\beta} \pm 1.96 \times SE\) does not contain the true parameter \(\beta\).
\item
  A 5 percent significance test under these conditions will incorrectly reject the null hypothesis (\(\beta = \beta_0\)) 100\% of the time.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Best case}: No endogeneity (\(\rho =0\)) or very large \(\hat{f}\) (very strong first-stage)
\end{enumerate}

\begin{itemize}
\tightlist
\item
  The interval \(\hat{\beta} \pm 1.96 \times SD\) accurately contains \(\beta\) at least 95\% of the time.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Intermediate case}: The performance of the interval lies between the two extremes.
\end{enumerate}

\textbf{Solutions}: To have valid inference of \(\hat{\beta} \pm 1.96 \times SE\) using t-ratio (\(\hat{t}^2 \approx \hat{t}^2_{AR}\)), we can either

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Assume our problem away

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Assume \(E(F) > 142.6\) \citep{lee2022valid} (Not much of an assumption since we can observe first-stage F-stat empirically).
  \item
    Assume \(|\rho| < 0.565\) \citep[\citet{lee2022valid}]{angrist2024one}, but this defeats our motivation to use IV in the first place because we think there is a strong endogeneity bias, that's why we are trying to correct for it (circular argument).
  \end{enumerate}
\item
  Deal with it head on

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    \protect\hyperlink{ar-approach}{AR approach} \citep{anderson1949estimation}
  \item
    \protect\hyperlink{tf-procedure}{tF Procedure} \citep{lee2022valid}
  \item
    \protect\hyperlink{ak-approach}{AK approach} \citep{angrist2023one}
  \end{enumerate}
\end{enumerate}

\textbf{Common Practices \& Challenges}:

\begin{itemize}
\item
  The t-ratio test is preferred by many researchers but has its pitfalls:

  \begin{itemize}
  \tightlist
  \item
    Known to over-reject (equivalently, under-cover confidence intervals), especially with weak instruments \citep[\citet{nelson1990distribution}, \citet{bound1995problems}, \citet{dufour1997some}]{lee2022valid}.
  \end{itemize}
\item
  To address this:

  \begin{itemize}
  \item
    The first-stage F-statistic is used as an indicator of weak instruments.
  \item
    \citep[\citet{stock2005testing}]{staiger1997instrumental} provided a framework to understand and correct these \textbf{distortions}.
  \end{itemize}
\end{itemize}

\textbf{Misinterpretations}:

\begin{itemize}
\item
  Common errors in application:

  \begin{itemize}
  \item
    Using a rule-of-thumb F-stat threshold of 10 instead of referring to \citet{stock2005testing}.
  \item
    Mislabeling intervals such as \(\hat{\beta} \pm 1.96 \times \hat{se}(\hat{\beta})\) as 95\% confidence intervals (when passed the \(F>10\) rule of thumb). \citet{staiger1997instrumental} clarified that such intervals actually represent 85\% confidence when using \(F > 16.38\) from \citet{stock2005testing}
  \end{itemize}
\item
  Pretesting for weak instruments might exacerbate over-rejection of the t-ratio test mentioned above \citep{hall1996judging}.
\item
  Selective model specification (i.e., dropping certain specification) based on F-statistics also leads to significant \textbf{distortions} \citep{andrews2019weak}.
\end{itemize}

\hypertarget{ar-approach}{%
\subsection{AR approach}\label{ar-approach}}

\textbf{Validity of Anderson-Rubin Test} (notated as AR) \citep{anderson1949estimation}:

\begin{itemize}
\item
  Gives accurate results even under non-normal and homoskedastic errors \citep{staiger1997instrumental}.
\item
  Maintains validity across diverse error structures \citep{stock2000gmm}.
\item
  Minimizes type II error among several alternative tests, in cases of:

  \begin{itemize}
  \item
    Homoskedastic errors \citep[\citet{moreira2009tests}]{andrews2006optimal}.
  \item
    Generalized for heteroskedastic, clustered, and autocorrelated errors \citep{moreira2019optimal}.
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ivDiag)}

\CommentTok{\# AR test (robust to weak instruments)}
\CommentTok{\# example by the package\textquotesingle{}s authors}
\NormalTok{ivDiag}\SpecialCharTok{::}\FunctionTok{AR\_test}\NormalTok{(}
    \AttributeTok{data =}\NormalTok{ rueda,}
    \AttributeTok{Y =} \StringTok{"e\_vote\_buying"}\NormalTok{,}
    \CommentTok{\# treatment}
    \AttributeTok{D =} \StringTok{"lm\_pob\_mesa"}\NormalTok{,}
    \CommentTok{\# instruments}
    \AttributeTok{Z =} \StringTok{"lz\_pob\_mesa\_f"}\NormalTok{,}
    \AttributeTok{controls =} \FunctionTok{c}\NormalTok{(}\StringTok{"lpopulation"}\NormalTok{, }\StringTok{"lpotencial"}\NormalTok{),}
    \AttributeTok{cl =} \StringTok{"muni\_code"}\NormalTok{,}
    \AttributeTok{CI =} \ConstantTok{FALSE}
\NormalTok{)}
\CommentTok{\#\textgreater{} $Fstat}
\CommentTok{\#\textgreater{}         F       df1       df2         p }
\CommentTok{\#\textgreater{}   50.5097    1.0000 4350.0000    0.0000}

\NormalTok{g }\OtherTok{\textless{}{-}}\NormalTok{ ivDiag}\SpecialCharTok{::}\FunctionTok{ivDiag}\NormalTok{(}
    \AttributeTok{data =}\NormalTok{ rueda,}
    \AttributeTok{Y =} \StringTok{"e\_vote\_buying"}\NormalTok{,}
    \AttributeTok{D =} \StringTok{"lm\_pob\_mesa"}\NormalTok{,}
    \AttributeTok{Z =} \StringTok{"lz\_pob\_mesa\_f"}\NormalTok{,}
    \AttributeTok{controls =} \FunctionTok{c}\NormalTok{(}\StringTok{"lpopulation"}\NormalTok{, }\StringTok{"lpotencial"}\NormalTok{),}
    \AttributeTok{cl =} \StringTok{"muni\_code"}\NormalTok{,}
    \AttributeTok{cores =} \DecValTok{4}\NormalTok{,}
    \AttributeTok{bootstrap =} \ConstantTok{FALSE}
\NormalTok{)}
\NormalTok{g}\SpecialCharTok{$}\NormalTok{AR}
\CommentTok{\#\textgreater{} $Fstat}
\CommentTok{\#\textgreater{}         F       df1       df2         p }
\CommentTok{\#\textgreater{}   50.5097    1.0000 4350.0000    0.0000 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $ci.print}
\CommentTok{\#\textgreater{} [1] "[{-}1.2545, {-}0.7156]"}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $ci}
\CommentTok{\#\textgreater{} [1] {-}1.2545169 {-}0.7155854}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $bounded}
\CommentTok{\#\textgreater{} [1] TRUE}
\NormalTok{ivDiag}\SpecialCharTok{::}\FunctionTok{plot\_coef}\NormalTok{(g)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{30-instrumental_var_files/figure-latex/unnamed-chunk-5-1} \end{center}

\hypertarget{tf-procedure}{%
\subsection{tF Procedure}\label{tf-procedure}}

\citet{lee2022valid} propose a new method that is aligned better with traditional econometric training than AR, where it is called the tF procedure. It incorporates both the 1st-stage F-stat and the 2SLS \(t\)-value. This method is applicable to single instrumental variable (i.e., just-identified model), including

\begin{itemize}
\item
  Randomized trials with imperfect compliance \citep{imbens1994identification}.
\item
  Fuzzy \protect\hyperlink{regression-discontinuity}{Regression Discontinuity} designs \citep{lee2010regression}.
\item
  Fuzzy regression kink designs \citep{card2015inference}.
\end{itemize}

See \citet{andrews2019weak} for a comparison between \protect\hyperlink{ar-approach}{AR approach} and \protect\hyperlink{tf-procedure}{tF Procedure}.

\textbf{tF Procedure}:

\begin{itemize}
\item
  Adjusts the t-ratio based on the first-stage F-statistic.
\item
  Rather than a fixed pretesting threshold, it applies an adjustment factor to 2SLS standard errors.
\item
  Adjustment factors are provided for 95\% and 99\% confidence levels.
\end{itemize}

\textbf{Advantages of the tF Procedure}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Smooth Adjustment}:

  \begin{itemize}
  \item
    Gives usable finite confidence intervals for smaller F statistic values.
  \item
    95\% confidence is applicable for \(F > 3.84\), aligning with AR's bounded 95\% confidence intervals.
  \end{itemize}
\item
  \textbf{Clear Confidence Levels}:

  \begin{itemize}
  \item
    These levels incorporate effects of basing inference on the first-stage F.
  \item
    Mirrors AR or other zero distortion procedures.
  \end{itemize}
\item
  \textbf{Robustness}:

  \begin{itemize}
  \item
    Robust against common error structures (e.g., heteroskedasticity or clustering and/or autocorrelated errors).
  \item
    No further adjustments are necessary as long as robust variance estimators are consistently used (same robust variance estimator used for the 1st-stage as for the IV estimate).
  \end{itemize}
\item
  \textbf{Comparison to AR}:

  \begin{itemize}
  \tightlist
  \item
    Surprisingly, with \(F > 3.84\), AR's expected interval length is infinite, while tF's is finite (i.e., better).
  \end{itemize}
\item
  \textbf{Applicability}:

  \begin{itemize}
  \item
    The tF adjustment can re-evaluate published studies if the first-stage F-statistic is available.
  \item
    Original data access is not needed.
  \end{itemize}
\end{enumerate}

\textbf{Impacts in Applied Research}:

\begin{itemize}
\item
  \citet{lee2022valid} examined recent single-instrument specification studies from the American Economic Review (AER).
\item
  Observations:

  \begin{itemize}
  \item
    For at least 25\% of the studied specifications, using tF increased confidence interval lengths by:

    \begin{itemize}
    \item
      49\% (5\% significance level).
    \item
      136\% (1\% significance level).
    \end{itemize}
  \item
    For specifications with \(F > 10\) and \(t > 1.96\), about 25\% became statistically insignificant at the 5\% level when adjusted using tF.
  \item
    Conclusion: tF adjustments could greatly influence inferences in research employing t-ratio inferences.
  \end{itemize}
\end{itemize}

Notation

\begin{itemize}
\tightlist
\item
  \(Y = X \beta + W \gamma + u\)
\item
  \(X = Z \pi + W \xi + \nu\)
\end{itemize}

where

\begin{itemize}
\tightlist
\item
  \(W\): Additional covariates, possibly including an intercept term.
\item
  \(X\): variable of interest
\item
  \(Z\): instruments
\end{itemize}

Key Statistics:

\begin{itemize}
\item
  \(t\)-ratio for the instrumental variable estimator: \(\hat{t} = \frac{\hat{\beta} - \beta_0}{\sqrt{\hat{V}_N (\hat{\beta})}}\)
\item
  \(t\)-ratio for the first-stage coefficient: \(\hat{f} = \frac{\hat{\pi}}{\sqrt{\hat{V}_N (\hat{\pi})}}\)
\item
  \(\hat{F} = \hat{f}^2\)
\end{itemize}

where

\begin{itemize}
\tightlist
\item
  \(\hat{\beta}\): Instrumental variable estimator.
\item
  \(\hat{V}_N (\hat{\beta})\): Estimated variance of \(\hat{\beta}\), possibly robust to deal with non-iid errors.
\item
  \(\hat{t}\): \(t\)-ratio under the null hypothesis.
\item
  \(\hat{f}\): \(t\)-ratio under the null hypothesis of \(\pi=0\).
\end{itemize}

Traditional \(t\) Inference:

\begin{itemize}
\tightlist
\item
  In large samples, \(\hat{t}^2 \to^d t^2\)
\item
  Standard normal critical values are \(\pm 1.96\) for 5\% significance level testing.
\end{itemize}

Distortions in Inference in the case of IV:

\begin{itemize}
\tightlist
\item
  Use of a standard normal can lead to distorted inferences even in large samples.

  \begin{itemize}
  \tightlist
  \item
    Despite large samples, t-distribution might not be normal.
  \end{itemize}
\item
  But magnitude of this distortion can be quantified.

  \begin{itemize}
  \tightlist
  \item
    \citet{stock2005testing} provides a formula for Wald test statistics using 2SLS.
  \item
    \(t^2\) formula allows for quantification of inference distortions.
  \item
    In the just-identified case with one endogenous regressor \(t^2 = f + t_{AR} + \rho f t_{AR}\) \citep{stock2005testing}

    \begin{itemize}
    \tightlist
    \item
      \(\hat{f} \to^d f\) and \(\bar{f} = \frac{\pi}{\sqrt{\frac{1}{N} AV(\hat{\pi})}}\) and \(AV(\hat{\pi})\) is the asymptotic variance of \(\hat{\pi}\)
    \item
      \(t_{AR}\) is a standard normal with \(AR = t^2_{AR}\)
    \item
      \(\rho\) (degree of endogeneity) is the correlation of \(Zu\) and \(Z \nu\) (when data are homoskedastic, \(\rho\) is the correlation between \(u\) and \(\nu\))
    \end{itemize}
  \end{itemize}
\end{itemize}

Implications of \(t^2\) formula:

\begin{itemize}
\tightlist
\item
  Varies rejection rates depending on \(\rho\) value.

  \begin{itemize}
  \tightlist
  \item
    \(\rho \in (0,0.5]\) (low) the t-ratio rejects at a probability below the nominal \(0.05\) rate
  \item
    \(\rho = 0.8\) (high) the rejection rate can be \(0.13\)
  \end{itemize}
\item
  In short, incorrect test size when relying solely on \(t^2\) (based on traditional econometric understanding)
\end{itemize}

To correct for this, one can

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Estimate the usually 2SLS standard errors
\item
  Multiply the SE by the adjustment factor based on the observed first-stage \(\hat{F}\) stat
\item
  One can go back to the traditional hypothesis by using either the t-ratio of confidence intervals
\end{enumerate}

\citet{lee2022valid} call this adjusted SE as ``0.05 tF SE''.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ivDiag)}
\NormalTok{g }\OtherTok{\textless{}{-}}\NormalTok{ ivDiag}\SpecialCharTok{::}\FunctionTok{ivDiag}\NormalTok{(}
    \AttributeTok{data =}\NormalTok{ rueda,}
    \AttributeTok{Y =} \StringTok{"e\_vote\_buying"}\NormalTok{,}
    \AttributeTok{D =} \StringTok{"lm\_pob\_mesa"}\NormalTok{,}
    \AttributeTok{Z =} \StringTok{"lz\_pob\_mesa\_f"}\NormalTok{,}
    \AttributeTok{controls =} \FunctionTok{c}\NormalTok{(}\StringTok{"lpopulation"}\NormalTok{, }\StringTok{"lpotencial"}\NormalTok{),}
    \AttributeTok{cl =} \StringTok{"muni\_code"}\NormalTok{,}
    \AttributeTok{cores =} \DecValTok{4}\NormalTok{,}
    \AttributeTok{bootstrap =} \ConstantTok{FALSE}
\NormalTok{)}
\NormalTok{g}\SpecialCharTok{$}\NormalTok{tF}
\CommentTok{\#\textgreater{}         F        cF      Coef        SE         t    CI2.5\%   CI97.5\%   p{-}value }
\CommentTok{\#\textgreater{} 8598.3264    1.9600   {-}0.9835    0.1540   {-}6.3872   {-}1.2853   {-}0.6817    0.0000}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# example in fixest package}
\FunctionTok{library}\NormalTok{(fixest)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{base }\OtherTok{=}\NormalTok{ iris}
\FunctionTok{names}\NormalTok{(base) }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"y"}\NormalTok{, }\StringTok{"x1"}\NormalTok{, }\StringTok{"x\_endo\_1"}\NormalTok{, }\StringTok{"x\_inst\_1"}\NormalTok{, }\StringTok{"fe"}\NormalTok{)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\NormalTok{base}\SpecialCharTok{$}\NormalTok{x\_inst\_2 }\OtherTok{=} \FloatTok{0.2} \SpecialCharTok{*}\NormalTok{ base}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{+} \FloatTok{0.2} \SpecialCharTok{*}\NormalTok{ base}\SpecialCharTok{$}\NormalTok{x\_endo\_1 }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{150}\NormalTok{, }\AttributeTok{sd =} \FloatTok{0.5}\NormalTok{)}
\NormalTok{base}\SpecialCharTok{$}\NormalTok{x\_endo\_2 }\OtherTok{=} \FloatTok{0.2} \SpecialCharTok{*}\NormalTok{ base}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{{-}} \FloatTok{0.2} \SpecialCharTok{*}\NormalTok{ base}\SpecialCharTok{$}\NormalTok{x\_inst\_1 }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{150}\NormalTok{, }\AttributeTok{sd =} \FloatTok{0.5}\NormalTok{)}

\NormalTok{est\_iv }\OtherTok{=} \FunctionTok{feols}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x1 }\SpecialCharTok{|}\NormalTok{ x\_endo\_1 }\SpecialCharTok{+}\NormalTok{ x\_endo\_2 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x\_inst\_1 }\SpecialCharTok{+}\NormalTok{ x\_inst\_2, base)}
\NormalTok{est\_iv}
\CommentTok{\#\textgreater{} TSLS estimation {-} Dep. Var.: y}
\CommentTok{\#\textgreater{}                   Endo.    : x\_endo\_1, x\_endo\_2}
\CommentTok{\#\textgreater{}                   Instr.   : x\_inst\_1, x\_inst\_2}
\CommentTok{\#\textgreater{} Second stage: Dep. Var.: y}
\CommentTok{\#\textgreater{} Observations: 150}
\CommentTok{\#\textgreater{} Standard{-}errors: IID }
\CommentTok{\#\textgreater{}              Estimate Std. Error  t value   Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)  1.831380   0.411435  4.45121 1.6844e{-}05 ***}
\CommentTok{\#\textgreater{} fit\_x\_endo\_1 0.444982   0.022086 20.14744  \textless{} 2.2e{-}16 ***}
\CommentTok{\#\textgreater{} fit\_x\_endo\_2 0.639916   0.307376  2.08186 3.9100e{-}02 *  }
\CommentTok{\#\textgreater{} x1           0.565095   0.084715  6.67051 4.9180e{-}10 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} RMSE: 0.398842   Adj. R2: 0.761653}
\CommentTok{\#\textgreater{} F{-}test (1st stage), x\_endo\_1: stat = 903.2    , p \textless{} 2.2e{-}16 , on 2 and 146 DoF.}
\CommentTok{\#\textgreater{} F{-}test (1st stage), x\_endo\_2: stat =   3.25828, p = 0.041268, on 2 and 146 DoF.}
\CommentTok{\#\textgreater{}                   Wu{-}Hausman: stat =   6.79183, p = 0.001518, on 2 and 144 DoF.}

\NormalTok{res\_est\_iv }\OtherTok{\textless{}{-}}\NormalTok{ est\_iv}\SpecialCharTok{$}\NormalTok{coeftable }\SpecialCharTok{|\textgreater{}} 
    \FunctionTok{rownames\_to\_column}\NormalTok{()}


\NormalTok{coef\_of\_interest }\OtherTok{\textless{}{-}}
\NormalTok{    res\_est\_iv[res\_est\_iv}\SpecialCharTok{$}\NormalTok{rowname }\SpecialCharTok{==} \StringTok{"fit\_x\_endo\_1"}\NormalTok{, }\StringTok{"Estimate"}\NormalTok{]}
\NormalTok{se\_of\_interest }\OtherTok{\textless{}{-}}
\NormalTok{    res\_est\_iv[res\_est\_iv}\SpecialCharTok{$}\NormalTok{rowname }\SpecialCharTok{==} \StringTok{"fit\_x\_endo\_1"}\NormalTok{, }\StringTok{"Std. Error"}\NormalTok{]}
\NormalTok{fstat\_1st }\OtherTok{\textless{}{-}} \FunctionTok{fitstat}\NormalTok{(est\_iv, }\AttributeTok{type =} \StringTok{"ivf1"}\NormalTok{)[[}\DecValTok{1}\NormalTok{]]}\SpecialCharTok{$}\NormalTok{stat}

\CommentTok{\# To get the correct SE based on 1st{-}stage F{-}stat (This result is similar without adjustment since F is large)}
\CommentTok{\# the results are the new CIS and p.value}
\FunctionTok{tF}\NormalTok{(}\AttributeTok{coef =}\NormalTok{ coef\_of\_interest, }\AttributeTok{se =}\NormalTok{ se\_of\_interest, }\AttributeTok{Fstat =}\NormalTok{ fstat\_1st) }\SpecialCharTok{|\textgreater{}} 
\NormalTok{    causalverse}\SpecialCharTok{::}\FunctionTok{nice\_tab}\NormalTok{(}\DecValTok{5}\NormalTok{)}
\CommentTok{\#\textgreater{}          F   cF    Coef      SE        t  CI2.5. CI97.5. p.value}
\CommentTok{\#\textgreater{} 1 903.1628 1.96 0.44498 0.02209 20.14744 0.40169 0.48827       0}

\CommentTok{\# We can try to see a different 1st{-}stage F{-}stat and how it changes the results}
\FunctionTok{tF}\NormalTok{(}\AttributeTok{coef =}\NormalTok{ coef\_of\_interest, }\AttributeTok{se =}\NormalTok{ se\_of\_interest, }\AttributeTok{Fstat =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
\NormalTok{    causalverse}\SpecialCharTok{::}\FunctionTok{nice\_tab}\NormalTok{(}\DecValTok{5}\NormalTok{)}
\CommentTok{\#\textgreater{}   F    cF    Coef      SE        t  CI2.5. CI97.5. p.value}
\CommentTok{\#\textgreater{} 1 2 18.66 0.44498 0.02209 20.14744 0.03285 0.85711 0.03432}
\end{Highlighting}
\end{Shaded}

\hypertarget{ak-approach}{%
\subsection{AK approach}\label{ak-approach}}

\citep{angrist2023one}

\hypertarget{testing-assumptions}{%
\section{Testing Assumptions}\label{testing-assumptions}}

\[
Y = \beta_1 X_1 + \beta_2 X_2 + \epsilon
\]

where

\begin{itemize}
\item
  \(X_1\) are exogenous variables
\item
  \(X_2\) are endogenous variables
\item
  \(Z\) are instrumental variables
\end{itemize}

If \(Z\) satisfies the relevance condition, it means \(Cov(Z, X_2) \neq 0\)

This is important because we need this to be able to estimate \(\beta_2\) where

\[
\beta_2 = \frac{Cov(Z,Y)}{Cov(Z, X_2)}
\]

If \(Z\) satisfies the exogeneity condition, \(E[Z\epsilon]=0\), this can achieve by

\begin{itemize}
\item
  \(Z\) having no direct effect on \(Y\) except through \(X_2\)
\item
  In the presence of omitted variable, \(Z\) is uncorrelated with this variable.
\end{itemize}

If we just want to know the effect of \(Z\) on \(Y\) (\textbf{reduced form}) where the coefficient of \(Z\) is

\[
\rho = \frac{Cov(Y, Z)}{Var(Z)}
\]

and this effect is only through \(X_2\) (by the exclusion restriction assumption).

We can also consistently estimate the effect of \(Z\) on \(X\) (\textbf{first stage}) where the the coefficient of \(X_2\) is

\[
\pi = \frac{Cov(X_2, Z)}{Var(Z)}
\]

and the IV estimate is

\[
\beta_2 = \frac{Cov(Y,Z)}{Cov(X_2, Z)} = \frac{\rho}{\pi}
\]

\hypertarget{relevance-assumption}{%
\subsection{Relevance Assumption}\label{relevance-assumption}}

\begin{itemize}
\item
  \textbf{Weak instruments}: can explain little variation in the endogenous regressor

  \begin{itemize}
  \tightlist
  \item
    Coefficient estimate of the endogenous variable will be inaccurate.
  \item
    For cases where weak instruments are unavoidable, \citet{moreira2003conditional} proposes the conditional likelihood ratio test for robust inference. This test is considered approximately optimal for weak instrument scenarios \citep{andrews2008efficient, andrews2008exactly}.
  \end{itemize}
\item
  Rule of thumb:

  \begin{itemize}
  \item
    Compute F-statistic in the first-stage, where it should be greater than 10. But this is discouraged now by \citet{lee2022valid}
  \item
    use \texttt{linearHypothesis()} to see only instrument coefficients.
  \end{itemize}
\end{itemize}

\textbf{First-Stage F-Test}

In the context of a two-stage least squares (2SLS) setup where you are estimating the equation:

\[
Y = X \beta + \epsilon
\]

and \(X\) is endogenous, you typically estimate a first-stage regression of:

\[
X = Z \pi + u
\]

where Z is the instrument.

The first-stage F-test evaluates the joint significance of the instruments in this first stage:

\[
F = \frac{(SSR_r - SSR_{ur})/q}{SSR_{ur}/ (n - k - 1)}
\]

where:

\begin{itemize}
\item
  \(SSR_r\) is the sum of squared residuals from the restricted model (no instruments, just the constant).
\item
  \(SSR_{ur}\) is the sum of squared residuals from the unrestricted model (with instruments).
\item
  \(q\) is the number of instruments excluded from the main equation.
\item
  \(n\) is the number of observations.
\item
  \(k\) is the number of explanatory variables excluding the instruments.
\end{itemize}

\textbf{Cragg-Donald Test}

The Cragg-Donald statistic is essentially the same as the Wald statistic of the joint significance of the instruments in the first stage, and it's used specifically when you have multiple endogenous regressors. It's calculated as:

\[
CD = n \times (R_{ur}^2 - R_r^2)
\]

where:

\begin{itemize}
\item
  \(R_{ur}^2\) and \(R_r^2\) are the R-squared values from the unrestricted and restricted models respectively.
\item
  \(n\) is the number of observations.
\end{itemize}

For one endogenous variable, the Cragg-Donald test results should align closely with those from Stock and Yogo. The Anderson canonical correlation test, a likelihood ratio test, also works under similar conditions, contrasting with Cragg-Donald's Wald statistic approach. Both are valid with one endogenous variable and at least one instrument.

\textbf{Stock-Yogo Weak IV Test}

The Stock-Yogo test does not directly compute a statistic like the F-test or Cragg-Donald, but rather uses pre-computed critical values to assess the strength of instruments. It often uses the eigenvalues derived from the concentration matrix:

\[
S = \frac{1}{n} (Z' X) (X'Z)
\]

where \(Z\) is the matrix of instruments and \(X\) is the matrix of endogenous regressors.

Stock and Yogo provide critical values for different scenarios (bias, size distortion) for a given number of instruments and endogenous regressors, based on the smallest eigenvalue of \(S\). The test compares these eigenvalues against critical values that correspond to thresholds of permissible bias or size distortion in a 2SLS estimator.

\begin{itemize}
\tightlist
\item
  \textbf{Critical Values and Test Conditions}: The critical values derived by Stock and Yogo depend on the level of acceptable bias, the number of endogenous regressors, and the number of instruments. For example, with a 5\% maximum acceptable bias, one endogenous variable, and three instruments, the critical value for a sufficient first stage F-statistic is 13.91. Note that this framework requires at least two overidentifying degree of freedom.
\end{itemize}

\textbf{Comparison}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.0887}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2110}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2324}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.4679}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Test}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Description}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Focus}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Usage}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{First-Stage F-Test} & Evaluates the joint significance of instruments in the first stage. & Predictive power of instruments for the endogenous variable. & Simplest and most direct test, widely used especially with a single endogenous variable. Rule of thumb: F \textless{} 10 suggests weak instruments. \\
\textbf{Cragg-Donald Test} & Wald statistic for joint significance of instruments. & Joint strength of multiple instruments with multiple endogenous variables. & More appropriate in complex IV setups with multiple endogenous variables. Compares statistic against critical values for assessing instrument strength. \\
\textbf{Stock-Yogo Weak IV Test} & Compares test statistic to pre-determined critical values. & Minimizing size distortions and bias from weak instruments. & Theoretical evaluation of instrument strength, ensuring the reliability of 2SLS estimates against specific thresholds of bias or size distortion. \\
\end{longtable}

All the mentioned tests (Stock Yogo, Cragg-Donald, Anderson canonical correlation test) assume errors are independently and identically distributed. If this assumption is violated, the Kleinbergen-Paap test is robust against violations of the iid assumption and can be applied even with a single endogenous variable and instrument, provided the model is properly identified \citep{baum2021ivreg2h}.

\hypertarget{weak-instrument-tests}{%
\subsubsection{Weak Instrument Tests}\label{weak-instrument-tests}}

\hypertarget{cragg-donald}{%
\subsubsection{Cragg-Donald}\label{cragg-donald}}

\citep{cragg1993testing}

Similar to the first-stage F-statistic

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(cragg)}
\FunctionTok{library}\NormalTok{(AER) }\CommentTok{\# for dataaset}
\FunctionTok{data}\NormalTok{(}\StringTok{"WeakInstrument"}\NormalTok{)}

\FunctionTok{cragg\_donald}\NormalTok{(}
    \CommentTok{\# control variables}
    \AttributeTok{X =} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }
    \CommentTok{\# endogeneous variables}
    \AttributeTok{D =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }
    \CommentTok{\# instrument variables }
    \AttributeTok{Z =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ z, }
    \AttributeTok{data =}\NormalTok{ WeakInstrument}
\NormalTok{)}
\CommentTok{\#\textgreater{} Cragg{-}Donald test for weak instruments:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}      Data:                        WeakInstrument }
\CommentTok{\#\textgreater{}      Controls:                    \textasciitilde{}1 }
\CommentTok{\#\textgreater{}      Treatments:                  \textasciitilde{}x }
\CommentTok{\#\textgreater{}      Instruments:                 \textasciitilde{}z }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}      Cragg{-}Donald Statistic:        4.566136 }
\CommentTok{\#\textgreater{}      Df:                                 198}
\end{Highlighting}
\end{Shaded}

Large CD statistic implies that the instruments are strong, but not in our case here. But to judge it against some critical value, we have to look at \protect\hyperlink{stock-yogo}{Stock-Yogo}

\hypertarget{stock-yogo}{%
\subsubsection{Stock-Yogo}\label{stock-yogo}}

\citet{stock2002testing} set the critical values such that the bias is less then 10\% (default)

\(H_0:\) Instruments are weak

\(H_1:\) Instruments are not weak

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(cragg)}
\FunctionTok{library}\NormalTok{(AER) }\CommentTok{\# for dataaset}
\FunctionTok{data}\NormalTok{(}\StringTok{"WeakInstrument"}\NormalTok{)}
\FunctionTok{stock\_yogo\_test}\NormalTok{(}
    \CommentTok{\# control variables}
    \AttributeTok{X =} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{,}
    \CommentTok{\# endogeneous variables}
    \AttributeTok{D =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ x,}
    \CommentTok{\# instrument variables}
    \AttributeTok{Z =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ z,}
    \AttributeTok{size\_bias =} \StringTok{"bias"}\NormalTok{,}
    \AttributeTok{data =}\NormalTok{ WeakInstrument}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The CD statistic should be bigger than the set critical value to be considered strong instruments.

\hypertarget{anderson-rubin}{%
\subsubsection{Anderson-Rubin}\label{anderson-rubin}}

\hypertarget{stock-wright}{%
\subsubsection{Stock-Wright}\label{stock-wright}}

\hypertarget{exogeneity-assumption}{%
\subsection{Exogeneity Assumption}\label{exogeneity-assumption}}

The local average treatment effect (LATE) is defined as:

\[
\text{LATE} = \frac{\text{reduced form}}{\text{first stage}} = \frac{\rho}{\phi} 
\]

This implies that the reduced form (\(\rho\)) is the product of the first stage (\(\phi\)) and LATE:

\[
\rho = \phi \times \text{LATE}
\]

Thus, if the first stage (\(\phi\)) is 0, the reduced form (\(\rho\)) should also be 0.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load necessary libraries}
\FunctionTok{library}\NormalTok{(shiny)}
\FunctionTok{library}\NormalTok{(AER)  }\CommentTok{\# for ivreg}
\FunctionTok{library}\NormalTok{(ggplot2)  }\CommentTok{\# for visualization}
\FunctionTok{library}\NormalTok{(dplyr)  }\CommentTok{\# for data manipulation}

\CommentTok{\# Function to simulate the dataset}
\NormalTok{simulate\_iv\_data }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(n, beta, phi, direct\_effect) \{}
\NormalTok{  Z }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{  epsilon\_x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{  epsilon\_y }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{  X }\OtherTok{\textless{}{-}}\NormalTok{ phi }\SpecialCharTok{*}\NormalTok{ Z }\SpecialCharTok{+}\NormalTok{ epsilon\_x}
\NormalTok{  Y }\OtherTok{\textless{}{-}}\NormalTok{ beta }\SpecialCharTok{*}\NormalTok{ X }\SpecialCharTok{+}\NormalTok{ direct\_effect }\SpecialCharTok{*}\NormalTok{ Z }\SpecialCharTok{+}\NormalTok{ epsilon\_y}
\NormalTok{  data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{Y =}\NormalTok{ Y, }\AttributeTok{X =}\NormalTok{ X, }\AttributeTok{Z =}\NormalTok{ Z)}
  \FunctionTok{return}\NormalTok{(data)}
\NormalTok{\}}

\CommentTok{\# Function to run the simulations and calculate the effects}
\NormalTok{run\_simulation }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(n, beta, phi, direct\_effect) \{}
  \CommentTok{\# Simulate the data}
\NormalTok{  simulated\_data }\OtherTok{\textless{}{-}} \FunctionTok{simulate\_iv\_data}\NormalTok{(n, beta, phi, direct\_effect)}
  
  \CommentTok{\# Estimate first{-}stage effect (phi)}
\NormalTok{  first\_stage }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(X }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Z, }\AttributeTok{data =}\NormalTok{ simulated\_data)}
\NormalTok{  phi }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(first\_stage)[}\StringTok{"Z"}\NormalTok{]}
\NormalTok{  phi\_ci }\OtherTok{\textless{}{-}} \FunctionTok{confint}\NormalTok{(first\_stage)[}\StringTok{"Z"}\NormalTok{, ]}
  
  \CommentTok{\# Estimate reduced{-}form effect (rho)}
\NormalTok{  reduced\_form }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Z, }\AttributeTok{data =}\NormalTok{ simulated\_data)}
\NormalTok{  rho }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(reduced\_form)[}\StringTok{"Z"}\NormalTok{]}
\NormalTok{  rho\_ci }\OtherTok{\textless{}{-}} \FunctionTok{confint}\NormalTok{(reduced\_form)[}\StringTok{"Z"}\NormalTok{, ]}
  
  \CommentTok{\# Estimate LATE using IV regression}
\NormalTok{  iv\_model }\OtherTok{\textless{}{-}} \FunctionTok{ivreg}\NormalTok{(Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X }\SpecialCharTok{|}\NormalTok{ Z, }\AttributeTok{data =}\NormalTok{ simulated\_data)}
\NormalTok{  iv\_late }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(iv\_model)[}\StringTok{"X"}\NormalTok{]}
\NormalTok{  iv\_late\_ci }\OtherTok{\textless{}{-}} \FunctionTok{confint}\NormalTok{(iv\_model)[}\StringTok{"X"}\NormalTok{, ]}
  
  \CommentTok{\# Calculate LATE as the ratio of reduced{-}form and first{-}stage coefficients}
\NormalTok{  calculated\_late }\OtherTok{\textless{}{-}}\NormalTok{ rho }\SpecialCharTok{/}\NormalTok{ phi}
\NormalTok{  calculated\_late\_se }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(}
\NormalTok{    (rho\_ci[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{{-}}\NormalTok{ rho)}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{/}\NormalTok{ phi}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+}\NormalTok{ (rho }\SpecialCharTok{*}\NormalTok{ (phi\_ci[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{{-}}\NormalTok{ phi) }\SpecialCharTok{/}\NormalTok{ phi}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2}
\NormalTok{  )}
\NormalTok{  calculated\_late\_ci }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(calculated\_late }\SpecialCharTok{{-}} \FloatTok{1.96} \SpecialCharTok{*}\NormalTok{ calculated\_late\_se, }
\NormalTok{                          calculated\_late }\SpecialCharTok{+} \FloatTok{1.96} \SpecialCharTok{*}\NormalTok{ calculated\_late\_se)}
  
  \CommentTok{\# Return a list of results}
  \FunctionTok{list}\NormalTok{(}\AttributeTok{phi =}\NormalTok{ phi, }
       \AttributeTok{phi\_ci =}\NormalTok{ phi\_ci,}
       \AttributeTok{rho =}\NormalTok{ rho, }
       \AttributeTok{rho\_ci =}\NormalTok{ rho\_ci,}
       \AttributeTok{direct\_effect =}\NormalTok{ direct\_effect,}
       \AttributeTok{direct\_effect\_ci =} \FunctionTok{c}\NormalTok{(direct\_effect, direct\_effect),  }\CommentTok{\# Placeholder for direct effect CI}
       \AttributeTok{iv\_late =}\NormalTok{ iv\_late, }
       \AttributeTok{iv\_late\_ci =}\NormalTok{ iv\_late\_ci,}
       \AttributeTok{calculated\_late =}\NormalTok{ calculated\_late, }
       \AttributeTok{calculated\_late\_ci =}\NormalTok{ calculated\_late\_ci,}
       \AttributeTok{true\_effect =}\NormalTok{ beta,}
       \AttributeTok{true\_effect\_ci =} \FunctionTok{c}\NormalTok{(beta, beta))  }\CommentTok{\# Placeholder for true effect CI}
\NormalTok{\}}

\CommentTok{\# Define UI for the sliders}
\NormalTok{ui }\OtherTok{\textless{}{-}} \FunctionTok{fluidPage}\NormalTok{(}
  \FunctionTok{titlePanel}\NormalTok{(}\StringTok{"IV Model Simulation"}\NormalTok{),}
  \FunctionTok{sidebarLayout}\NormalTok{(}
    \FunctionTok{sidebarPanel}\NormalTok{(}
      \FunctionTok{sliderInput}\NormalTok{(}\StringTok{"beta"}\NormalTok{, }\StringTok{"True Effect of X on Y (beta):"}\NormalTok{, }\AttributeTok{min =} \DecValTok{0}\NormalTok{, }\AttributeTok{max =} \FloatTok{1.0}\NormalTok{, }\AttributeTok{value =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{step =} \FloatTok{0.1}\NormalTok{),}
      \FunctionTok{sliderInput}\NormalTok{(}\StringTok{"phi"}\NormalTok{, }\StringTok{"First Stage Effect (phi):"}\NormalTok{, }\AttributeTok{min =} \DecValTok{0}\NormalTok{, }\AttributeTok{max =} \FloatTok{1.0}\NormalTok{, }\AttributeTok{value =} \FloatTok{0.7}\NormalTok{, }\AttributeTok{step =} \FloatTok{0.1}\NormalTok{),}
      \FunctionTok{sliderInput}\NormalTok{(}\StringTok{"direct\_effect"}\NormalTok{, }\StringTok{"Direct Effect of Z on Y:"}\NormalTok{, }\AttributeTok{min =} \SpecialCharTok{{-}}\FloatTok{0.5}\NormalTok{, }\AttributeTok{max =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{value =} \DecValTok{0}\NormalTok{, }\AttributeTok{step =} \FloatTok{0.1}\NormalTok{)}
\NormalTok{    ),}
    \FunctionTok{mainPanel}\NormalTok{(}
      \FunctionTok{plotOutput}\NormalTok{(}\StringTok{"dotPlot"}\NormalTok{)}
\NormalTok{    )}
\NormalTok{  )}
\NormalTok{)}

\CommentTok{\# Define server logic to run the simulation and generate the plot}
\NormalTok{server }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(input, output) \{}
\NormalTok{  output}\SpecialCharTok{$}\NormalTok{dotPlot }\OtherTok{\textless{}{-}} \FunctionTok{renderPlot}\NormalTok{(\{}
    \CommentTok{\# Run simulation}
\NormalTok{    results }\OtherTok{\textless{}{-}} \FunctionTok{run\_simulation}\NormalTok{(}\AttributeTok{n =} \DecValTok{1000}\NormalTok{, }\AttributeTok{beta =}\NormalTok{ input}\SpecialCharTok{$}\NormalTok{beta, }\AttributeTok{phi =}\NormalTok{ input}\SpecialCharTok{$}\NormalTok{phi, }\AttributeTok{direct\_effect =}\NormalTok{ input}\SpecialCharTok{$}\NormalTok{direct\_effect)}
    
    \CommentTok{\# Prepare data for plotting}
\NormalTok{    plot\_data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
      \AttributeTok{Effect =} \FunctionTok{c}\NormalTok{(}\StringTok{"First Stage (phi)"}\NormalTok{, }\StringTok{"Reduced Form (rho)"}\NormalTok{, }\StringTok{"Direct Effect"}\NormalTok{, }\StringTok{"LATE (Ratio)"}\NormalTok{, }\StringTok{"LATE (IV)"}\NormalTok{, }\StringTok{"True Effect"}\NormalTok{),}
      \AttributeTok{Value =} \FunctionTok{c}\NormalTok{(results}\SpecialCharTok{$}\NormalTok{phi, results}\SpecialCharTok{$}\NormalTok{rho, results}\SpecialCharTok{$}\NormalTok{direct\_effect, results}\SpecialCharTok{$}\NormalTok{calculated\_late, results}\SpecialCharTok{$}\NormalTok{iv\_late, results}\SpecialCharTok{$}\NormalTok{true\_effect),}
      \AttributeTok{CI\_Lower =} \FunctionTok{c}\NormalTok{(results}\SpecialCharTok{$}\NormalTok{phi\_ci[}\DecValTok{1}\NormalTok{], results}\SpecialCharTok{$}\NormalTok{rho\_ci[}\DecValTok{1}\NormalTok{], results}\SpecialCharTok{$}\NormalTok{direct\_effect\_ci[}\DecValTok{1}\NormalTok{], results}\SpecialCharTok{$}\NormalTok{calculated\_late\_ci[}\DecValTok{1}\NormalTok{], results}\SpecialCharTok{$}\NormalTok{iv\_late\_ci[}\DecValTok{1}\NormalTok{], results}\SpecialCharTok{$}\NormalTok{true\_effect\_ci[}\DecValTok{1}\NormalTok{]),}
      \AttributeTok{CI\_Upper =} \FunctionTok{c}\NormalTok{(results}\SpecialCharTok{$}\NormalTok{phi\_ci[}\DecValTok{2}\NormalTok{], results}\SpecialCharTok{$}\NormalTok{rho\_ci[}\DecValTok{2}\NormalTok{], results}\SpecialCharTok{$}\NormalTok{direct\_effect\_ci[}\DecValTok{2}\NormalTok{], results}\SpecialCharTok{$}\NormalTok{calculated\_late\_ci[}\DecValTok{2}\NormalTok{], results}\SpecialCharTok{$}\NormalTok{iv\_late\_ci[}\DecValTok{2}\NormalTok{], results}\SpecialCharTok{$}\NormalTok{true\_effect\_ci[}\DecValTok{2}\NormalTok{])}
\NormalTok{    )}
    
    \CommentTok{\# Create dot plot with confidence intervals}
    \FunctionTok{ggplot}\NormalTok{(plot\_data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Effect, }\AttributeTok{y =}\NormalTok{ Value)) }\SpecialCharTok{+}
      \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{size =} \DecValTok{3}\NormalTok{) }\SpecialCharTok{+}
      \FunctionTok{geom\_errorbar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{ymin =}\NormalTok{ CI\_Lower, }\AttributeTok{ymax =}\NormalTok{ CI\_Upper), }\AttributeTok{width =} \FloatTok{0.2}\NormalTok{) }\SpecialCharTok{+}
      \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"IV Model Effects"}\NormalTok{,}
           \AttributeTok{y =} \StringTok{"Coefficient Value"}\NormalTok{) }\SpecialCharTok{+}
      \FunctionTok{coord\_cartesian}\NormalTok{(}\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)) }\SpecialCharTok{+}  \CommentTok{\# Limits the y{-}axis to {-}1 to 1 but allows CI beyond}
      \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
      \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.text.x =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{angle =} \DecValTok{45}\NormalTok{, }\AttributeTok{hjust =} \DecValTok{1}\NormalTok{))}
\NormalTok{  \})}
\NormalTok{\}}

\CommentTok{\# Run the application }
\FunctionTok{shinyApp}\NormalTok{(}\AttributeTok{ui =}\NormalTok{ ui, }\AttributeTok{server =}\NormalTok{ server)}
\end{Highlighting}
\end{Shaded}

A statistically significant reduced form estimate without a corresponding first stage indicates an issue, suggesting an alternative channel linking instruments to outcomes or a direct effect of the IV on the outcome.

\begin{itemize}
\tightlist
\item
  \textbf{No Direct Effect}: When the direct effect is 0 and the first stage is 0, the reduced form is 0.

  \begin{itemize}
  \tightlist
  \item
    Note: Extremely rare cases with multiple additional paths that perfectly cancel each other out can also produce this result, but testing for all possible paths is impractical.
  \end{itemize}
\item
  \textbf{With Direct Effect}: When there is a direct effect of the IV on the outcome, the reduced form can be significantly different from 0, even if the first stage is 0.

  \begin{itemize}
  \tightlist
  \item
    This violates the exogeneity assumption, as the IV should only affect the outcome through the treatment variable.
  \end{itemize}
\end{itemize}

To test the validity of the exogeneity assumption, we can use a sanity test:

\begin{itemize}
\tightlist
\item
  Identify groups for which the effects of instruments on the treatment variable are small and not significantly different from 0. The reduced form estimate for these groups should also be 0. These ``no-first-stage samples'' provide evidence of whether the exogeneity assumption is violated.
\end{itemize}

\hypertarget{overid-tests}{%
\subsubsection{Overid Tests}\label{overid-tests}}

\begin{itemize}
\item
  Wald test and Hausman test for exogeneity of \(X\) assuming \(Z\) is exogenous

  \begin{itemize}
  \tightlist
  \item
    People might prefer Wald test over Hausman test.
  \end{itemize}
\item
  Sargan (for 2SLS) is a simpler version of Hansen's J test (for IV-GMM)
\item
  Modified J test (i.e., Regularized jacknife IV): can handle weak instruments and small sample size \citep{carrasco2022testing} (also proposed a regularized F-test to test relevance assumption that is robust to heteroskedasticity).
\item
  New advances: endogeneity robust inference in finite sample and sensitivity analysis of inference \citep{kiviet2020testing}
\end{itemize}

These tests that can provide evidence fo the validity of the over-identifying restrictions is not sufficient or necessary for the validity of the moment conditions (i.e., this assumption cannot be tested). \citep{deaton2010instruments, parente2012cautionary}

\begin{itemize}
\item
  The over-identifying restriction can still be valid even when the instruments are correlated with the error terms, but then in this case, what you're estimating is no longer your parameters of interest.
\item
  Rejection of the over-identifying restrictions can also be the result of \textbf{parameter heterogeneity} \citep{angrist2000interpretation}
\end{itemize}

Why overid tests hold no value/info?

\begin{itemize}
\item
  Overidentifying restrictions are valid irrespective of the instruments' validity

  \begin{itemize}
  \tightlist
  \item
    Whenever instruments have the same motivation and are on the same scale, the estimated parameter of interests will be very close \citep[p.~316]{parente2012cautionary}
  \end{itemize}
\item
  Overidentifying restriction are invalid when each instrument is valid

  \begin{itemize}
  \tightlist
  \item
    When the effect of your parameter of interest is heterogeneous (e.g., you have two groups with two different true effects), your first instrument can be correlated with your variable of interest only for the first group and your second interments can be correlated with your variable of interest only for the second group (i.e., each instrument is valid), and if you use each instrument, you can still identify the parameter of interest. However, if you use both of them, what you estimate is a mixture of the two groups. Hence, the overidentifying restriction will be invalid (because no single parameters can make the errors of the model orthogonal to both instruments). The result may seem confusing at first because if each subset of overidentifying restrictions is valid, the full set should also be valid. However, this interpretation is flawed because the residual's orthogonality to the instruments depends on the chosen set of instruments, and therefore the set of restrictions tested when using two sets of instruments together is not the same as the union of the sets of restrictions tested when using each set of instruments separately \citep[p.~316]{parente2012cautionary}
  \end{itemize}
\end{itemize}

These tests (of overidentifying restrictions) should be used to check whether different instruments identify the same parameters of interest, not to check their validity

\citep{hausman1983specification, parente2012cautionary}

\hypertarget{wald-test-1}{%
\paragraph{Wald Test}\label{wald-test-1}}

Assuming that \(Z\) is exogenous (a valid instrument), we want to know whether \(X_2\) is exogenous

1st stage:

\[
X_2 = \hat{\alpha} Z + \hat{\epsilon}
\]

2nd stage:

\[
Y = \delta_0 X_1 + \delta_1 X_2 + \delta_2 \hat{\epsilon} + u
\]

where

\begin{itemize}
\tightlist
\item
  \(\hat{\epsilon}\) is the residuals from the 1st stage
\end{itemize}

The Wald test of exogeneity assumes

\[
H_0: \delta_2 = 0 \\
H_1: \delta_2 \neq 0
\]

If you have more than one endogenous variable with more than one instrument, \(\delta_2\) is a vector of all residuals from all the first-stage equations. And the null hypothesis is that they are jointly equal 0.

If you reject this hypothesis, it means that \(X_2\) is \textbf{not endogenous}. Hence, for this test, we do not want to reject the null hypothesis.

If the test is not sacrificially significant, we might just don't have enough information to reject the null.

When you have a valid instrument \(Z\), whether \(X_2\) is endogenous or exogenous, your coefficient estimates of \(X_2\) should still be consistent. But if \(X_2\) is exogenous, then 2SLS will be inefficient (i.e., larger standard errors).

Intuition:

\(\hat{\epsilon}\) is the supposed endogenous part of \(X_2\), When we regress \(Y\) on \(\hat{\epsilon}\) and observe that its coefficient is not different from 0. It means that the exogenous part of \(X_2\) can explain well the impact on \(Y\), and there is no endogenous part.

\hypertarget{hausmans-test}{%
\paragraph{Hausman's Test}\label{hausmans-test}}

Similar to \protect\hyperlink{sec-wald-test-logistic}{Wald Test} and identical to \protect\hyperlink{sec-wald-test-logistic}{Wald Test} when we have homoskedasticity (i.e., homogeneity of variances). Because of this assumption, it's used less often than \protect\hyperlink{sec-wald-test-logistic}{Wald Test}

\hypertarget{hansens-j}{%
\paragraph{Hansen's J}\label{hansens-j}}

\begin{itemize}
\item
  \citep{hansen1982large}
\item
  J-test (over-identifying restrictions test): test whether \textbf{additional} instruments are exogenous

  \begin{itemize}
  \tightlist
  \item
    Can only be applied in cases where you have more instruments than endogenous variables

    \begin{itemize}
    \tightlist
    \item
      \(dim(Z) > dim(X_2)\)
    \end{itemize}
  \item
    Assume at least one instrument within \(Z\) is exogenous
  \end{itemize}
\end{itemize}

Procedure IV-GMM:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Obtain the residuals of the 2SLS estimation
\item
  Regress the residuals on all instruments and exogenous variables.
\item
  Test the joint hypothesis that all coefficients of the residuals across instruments are 0 (i.e., this is true when instruments are exogenous).

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    Compute \(J = mF\) where \(m\) is the number of instruments, and \(F\) is your equation \(F\) statistic (can you use \texttt{linearHypothesis()} again).
  \item
    If your exogeneity assumption is true, then \(J \sim \chi^2_{m-k}\) where \(k\) is the number of endogenous variables.
  \end{enumerate}
\item
  If you reject this hypothesis, it can be that

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    The first sets of instruments are invalid
  \item
    The second sets of instruments are invalid
  \item
    Both sets of instruments are invalid
  \end{enumerate}
\end{enumerate}

\textbf{Note}: This test is only true when your residuals are homoskedastic.

For a heteroskedasticity-robust \(J\)-statistic, see \citep{carrasco2022testing, li2022testing}

\hypertarget{sargan-test}{%
\paragraph{Sargan Test}\label{sargan-test}}

\citep{sargan1958estimation}

Similar to \protect\hyperlink{hansens-j}{Hansen's J}, but it assumes homoskedasticity

\begin{itemize}
\item
  Have to be careful when sample is not collected exogenously. As such, when you have choice-based sampling design, the sampling weights have to be considered to have consistent estimates. However, even if we apply sampling weights, the tests are not suitable because the iid assumption off errors are already violated. Hence, the test is invalid in this case \citep{pitt2011overidentification}.
\item
  If one has heteroskedasticity in its design, the Sargan test is invalid \citep{pitt2011overidentification}
\end{itemize}

\hypertarget{negative-r2}{%
\section{\texorpdfstring{Negative \(R^2\)}{Negative R\^{}2}}\label{negative-r2}}

It's okay to have negative \(R^2\) in the 2nd stage. We care more about consistent coefficient estimates.

\(R^2\) has no statistical meaning in instrumental variable regression or 2 or 3SLS

\[
R^2 = \frac{MSS}{TSS}
\]

where

\begin{itemize}
\tightlist
\item
  MSS = model sum of squares (TSS- RSS)
\item
  TSS = total sum of squares (\(\sum(y - \bar{y})^2\))
\item
  RSS = residual sum of squares (\(\sum (y - Xb)^2\))
\end{itemize}

If \(TSS > RSS\), then we have negative RSS and negative \(R^2\). Since the predicted values of the endogenous variables are different from the endogenous variables themselves, the error that is used to calculate RSS can be different from the error in the second stage, and RSS in the second stage can be less than TSS.

\hypertarget{treatment-intensity}{%
\section{Treatment Intensity}\label{treatment-intensity}}

Two-Stage Least Squares (TSLS) can be used to estimate the average causal effect of variable treatment intensity, and it ``identifies a weighted average of per-unit treatment effects along the length of a causal response function'' \citep[p.~431]{angrist1995two}. For example

\begin{itemize}
\item
  Drug dosage
\item
  Hours of exam prep on score \citep{powers1984effects}
\item
  Cigarette smoking on birth weights \citep{permutt1989simultaneous}
\item
  Years of education
\item
  Class size on test score \citep{angrist1999using}
\item
  Sibship size on earning \citep{lavy2006new}
\item
  Social Media Adoption
\end{itemize}

The \textbf{average causal effect} here refers to the conditional expectation of the difference in outcomes between the treated and what would have happened in the counterfactual world.

Notes:

\begin{itemize}
\tightlist
\item
  We do not need a linearity assumption of the relationships between the dependent variable, treatment intensities, and instruments.
\end{itemize}

Example

In their original paper, \citet{angrist1995two} take the example of schooling effect on earnings where they have quarters of birth as the instrumental variable.

For each additional year of schooling, there can be an increase in earnings, and each additional year can be heterogeneous (both in the sense that grade 9th to grade 10th is qualitatively different and one can change to a different school).

\[
Y = \gamma_0 + \gamma_1 X_1 + \rho S + \epsilon
\]

where

\begin{itemize}
\item
  \(S\) is years of schooling (i.e., endogenous regressor)
\item
  \(\rho\) is the return to a year of schooling
\item
  \(X_1\) is a matrix of exogenous covariates
\end{itemize}

Schooling can also be related to the exogenous variable \(X_1\)

\[
S = \delta_0 + X_1 \delta_1 + X_2 \delta_2 + \eta
\]

where

\begin{itemize}
\item
  \(X_2\) is an exogenous instrument
\item
  \(\delta_2\) is the coefficient of the instrument
\end{itemize}

by using only the fitted value in the second, the TSLS can give a consistent estimate of the effect of schooling on earning

\[
Y = \gamma_0 + X_1 \gamma-1 + \rho \hat{S} + \nu
\]

To give \(\rho\) a causal interpretation,

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  We first have to have the SUTVA (stable unit treatment value assumption), where the potential outcomes of the same person with different years of schooling are independent.
\item
  When \(\rho\) has a probability limit equal to a weighted average of \(E[Y_j - Y_{j-1}] \forall j\)
\end{enumerate}

Even though the first bullet point is not trivial, most of the time we don't have to defend much about it in a research article, the second bullet point is the harder one to argue and only apply to certain cases.

\hypertarget{control-function}{%
\section{Control Function}\label{control-function}}

Also known as \textbf{two-stage residual inclusion}

Resources:

\begin{itemize}
\item
  Binary outcome and binary endogenous variable application \citep{tchetgen2014note}

  \begin{itemize}
  \item
    In rare events: we use a logistic model in the 2nd stage
  \item
    In non-rare events: use risk ratio regression in the 2nd stage
  \end{itemize}
\item
  Application in marketing for consumer choice model \citep{petrin2010control}
\end{itemize}

Notes

\begin{itemize}
\tightlist
\item
  This approach is better suited for models with nonadditive errors (e.g., discrete choice models), or binary endogenous model, binary response variable, etc.
\end{itemize}

\[
Y = g(X) + U \\
X = \pi(Z) + V \\
E(U |Z,V) = E(U|V) \\
E(V|Z) = 0
\]

Under control function approach,

\[
E(Y|Z,V) = g(X) + E(U|Z,V) \\
= g(X) + E(U|V) \\
= g(X) + h(V)
\]

where \(h(V)\) is the control function that models the endogeneity

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Linear in parameters
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Linear Endogenous Variables:

  \begin{itemize}
  \tightlist
  \item
    The control function function approach is identical to the usual 2SLS estimator
  \end{itemize}
\item
  Nonlinear Endogenous Variables:

  \begin{itemize}
  \tightlist
  \item
    The control function is different from the 2SLS estimator
  \end{itemize}
\item
  Nonlinear in parameters:

  \begin{itemize}
  \tightlist
  \item
    The CF function is superior than the 2SLS estimator
  \end{itemize}
\end{enumerate}

\hypertarget{simulation}{%
\subsection{Simulation}\label{simulation}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(fixest)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(modelsummary)}

\CommentTok{\# Set the seed for reproducibility}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{n }\OtherTok{=} \DecValTok{10000}
\CommentTok{\# Generate the exogenous variable from a normal distribution}
\NormalTok{exogenous }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =} \DecValTok{5}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)}

\CommentTok{\# Generate the omitted variable as a function of the exogenous variable}
\NormalTok{omitted }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =} \DecValTok{2}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)}

\CommentTok{\# Generate the endogenous variable as a function of the omitted variable and the exogenous variable}
\NormalTok{endogenous }\OtherTok{\textless{}{-}} \DecValTok{5} \SpecialCharTok{*}\NormalTok{ omitted }\SpecialCharTok{+} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ exogenous }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)}

\CommentTok{\# nonlinear endogenous variable}
\NormalTok{endogenous\_nonlinear }\OtherTok{\textless{}{-}} \DecValTok{5} \SpecialCharTok{*}\NormalTok{ omitted}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ exogenous }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)}

\NormalTok{unrelated }\OtherTok{\textless{}{-}} \FunctionTok{rexp}\NormalTok{(n, }\AttributeTok{rate =} \DecValTok{1}\NormalTok{)}

\CommentTok{\# Generate the response variable as a function of the endogenous variable and the omitted variable}
\NormalTok{response }\OtherTok{\textless{}{-}} \DecValTok{4} \SpecialCharTok{+}  \DecValTok{3} \SpecialCharTok{*}\NormalTok{ endogenous }\SpecialCharTok{+} \DecValTok{6} \SpecialCharTok{*}\NormalTok{ omitted }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)}

\NormalTok{response\_nonlinear }\OtherTok{\textless{}{-}} \DecValTok{4} \SpecialCharTok{+}  \DecValTok{3} \SpecialCharTok{*}\NormalTok{ endogenous\_nonlinear }\SpecialCharTok{+} \DecValTok{6} \SpecialCharTok{*}\NormalTok{ omitted }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)}

\NormalTok{response\_nonlinear\_para }\OtherTok{\textless{}{-}} \DecValTok{4} \SpecialCharTok{+}  \DecValTok{3} \SpecialCharTok{*}\NormalTok{ endogenous }\SpecialCharTok{\^{}} \DecValTok{2} \SpecialCharTok{+} \DecValTok{6} \SpecialCharTok{*}\NormalTok{ omitted }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)}


\CommentTok{\# Combine the variables into a data frame}
\NormalTok{my\_data }\OtherTok{\textless{}{-}}
    \FunctionTok{data.frame}\NormalTok{(}
\NormalTok{        exogenous,}
\NormalTok{        omitted,}
\NormalTok{        endogenous,}
\NormalTok{        response,}
\NormalTok{        unrelated,}
\NormalTok{        response,}
\NormalTok{        response\_nonlinear,}
\NormalTok{        response\_nonlinear\_para}
\NormalTok{    )}

\CommentTok{\# View the first few rows of the data frame}
\CommentTok{\# head(my\_data)}

\NormalTok{wo\_omitted }\OtherTok{\textless{}{-}} \FunctionTok{feols}\NormalTok{(response }\SpecialCharTok{\textasciitilde{}}\NormalTok{ endogenous }\SpecialCharTok{+} \FunctionTok{sw0}\NormalTok{(unrelated), }\AttributeTok{data =}\NormalTok{ my\_data)}
\NormalTok{w\_omitted  }\OtherTok{\textless{}{-}} \FunctionTok{feols}\NormalTok{(response }\SpecialCharTok{\textasciitilde{}}\NormalTok{ endogenous }\SpecialCharTok{+}\NormalTok{ omitted }\SpecialCharTok{+}\NormalTok{ unrelated, }\AttributeTok{data =}\NormalTok{ my\_data)}


\CommentTok{\# ivreg::ivreg(response \textasciitilde{} endogenous + unrelated | exogenous, data = my\_data)}
\NormalTok{iv }\OtherTok{\textless{}{-}} \FunctionTok{feols}\NormalTok{(response }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+} \FunctionTok{sw0}\NormalTok{(unrelated) }\SpecialCharTok{|}\NormalTok{ endogenous }\SpecialCharTok{\textasciitilde{}}\NormalTok{ exogenous, }\AttributeTok{data =}\NormalTok{ my\_data)}

\FunctionTok{etable}\NormalTok{(}
\NormalTok{    wo\_omitted,}
\NormalTok{    w\_omitted,}
\NormalTok{    iv, }
    \AttributeTok{digits =} \DecValTok{2}
    \CommentTok{\# vcov = list("each", "iid", "hetero")}
\NormalTok{)}
\CommentTok{\#\textgreater{}                   wo\_omitted.1   wo\_omitted.2      w\_omitted           iv.1}
\CommentTok{\#\textgreater{} Dependent Var.:       response       response       response       response}
\CommentTok{\#\textgreater{}                                                                            }
\CommentTok{\#\textgreater{} Constant        {-}3.9*** (0.10) {-}4.0*** (0.10)  4.0*** (0.05) 15.7*** (0.59)}
\CommentTok{\#\textgreater{} endogenous      4.0*** (0.005) 4.0*** (0.005) 3.0*** (0.004)  3.0*** (0.03)}
\CommentTok{\#\textgreater{} unrelated                         0.03 (0.03)  0.002 (0.010)               }
\CommentTok{\#\textgreater{} omitted                                        6.0*** (0.02)               }
\CommentTok{\#\textgreater{} \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_}
\CommentTok{\#\textgreater{} S.E. type                  IID            IID            IID            IID}
\CommentTok{\#\textgreater{} Observations            10,000         10,000         10,000         10,000}
\CommentTok{\#\textgreater{} R2                     0.98566        0.98567        0.99803        0.92608}
\CommentTok{\#\textgreater{} Adj. R2                0.98566        0.98566        0.99803        0.92607}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}                           iv.2}
\CommentTok{\#\textgreater{} Dependent Var.:       response}
\CommentTok{\#\textgreater{}                               }
\CommentTok{\#\textgreater{} Constant        15.6*** (0.59)}
\CommentTok{\#\textgreater{} endogenous       3.0*** (0.03)}
\CommentTok{\#\textgreater{} unrelated         0.10. (0.06)}
\CommentTok{\#\textgreater{} omitted                       }
\CommentTok{\#\textgreater{} \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_}
\CommentTok{\#\textgreater{} S.E. type                  IID}
\CommentTok{\#\textgreater{} Observations            10,000}
\CommentTok{\#\textgreater{} R2                     0.92610}
\CommentTok{\#\textgreater{} Adj. R2                0.92608}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes: 0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

Linear in parameter and linear in endogenous variable

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# manual}
\CommentTok{\# 2SLS}
\NormalTok{first\_stage }\OtherTok{=} \FunctionTok{lm}\NormalTok{(endogenous }\SpecialCharTok{\textasciitilde{}}\NormalTok{ exogenous, }\AttributeTok{data =}\NormalTok{ my\_data)}
\NormalTok{new\_data }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(my\_data, }\AttributeTok{new\_endogenous =} \FunctionTok{predict}\NormalTok{(first\_stage, my\_data))}
\NormalTok{second\_stage }\OtherTok{=} \FunctionTok{lm}\NormalTok{(response }\SpecialCharTok{\textasciitilde{}}\NormalTok{ new\_endogenous, }\AttributeTok{data =}\NormalTok{ new\_data)}
\FunctionTok{summary}\NormalTok{(second\_stage)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = response \textasciitilde{} new\_endogenous, data = new\_data)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}     Min      1Q  Median      3Q     Max }
\CommentTok{\#\textgreater{} {-}77.683 {-}14.374  {-}0.107  14.289  78.274 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}                Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)     15.6743     2.0819   7.529 5.57e{-}14 ***}
\CommentTok{\#\textgreater{} new\_endogenous   3.0142     0.1039  29.025  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 21.26 on 9998 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.07771,    Adjusted R{-}squared:  0.07762 }
\CommentTok{\#\textgreater{} F{-}statistic: 842.4 on 1 and 9998 DF,  p{-}value: \textless{} 2.2e{-}16}

\NormalTok{new\_data\_cf }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(my\_data, }\AttributeTok{residual =} \FunctionTok{resid}\NormalTok{(first\_stage))}
\NormalTok{second\_stage\_cf }\OtherTok{=} \FunctionTok{lm}\NormalTok{(response }\SpecialCharTok{\textasciitilde{}}\NormalTok{ endogenous }\SpecialCharTok{+}\NormalTok{ residual, }\AttributeTok{data =}\NormalTok{ new\_data\_cf)}
\FunctionTok{summary}\NormalTok{(second\_stage\_cf)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = response \textasciitilde{} endogenous + residual, data = new\_data\_cf)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}    Min     1Q Median     3Q    Max }
\CommentTok{\#\textgreater{} {-}5.360 {-}1.016  0.003  1.023  5.201 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}              Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept) 15.674265   0.149350   105.0   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} endogenous   3.014202   0.007450   404.6   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} residual     1.140920   0.008027   142.1   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 1.525 on 9997 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.9953, Adjusted R{-}squared:  0.9953 }
\CommentTok{\#\textgreater{} F{-}statistic: 1.048e+06 on 2 and 9997 DF,  p{-}value: \textless{} 2.2e{-}16}

\FunctionTok{modelsummary}\NormalTok{(}\FunctionTok{list}\NormalTok{(second\_stage, second\_stage\_cf))}
\end{Highlighting}
\end{Shaded}

\begin{table}
\centering
\begin{tabular}[t]{lcc}
\toprule
  & (1) & (2)\\
\midrule
(Intercept) & \num{15.674} & \num{15.674}\\
 & (\num{2.082}) & (\num{0.149})\\
new\_endogenous & \num{3.014} & \\
 & (\num{0.104}) & \\
endogenous &  & \num{3.014}\\
 &  & (\num{0.007})\\
residual &  & \num{1.141}\\
 &  & (\num{0.008})\\
\midrule
Num.Obs. & \num{10000} & \num{10000}\\
R2 & \num{0.078} & \num{0.995}\\
R2 Adj. & \num{0.078} & \num{0.995}\\
AIC & \num{89520.9} & \num{36826.8}\\
BIC & \num{89542.5} & \num{36855.6}\\
Log.Lik. & \num{-44757.438} & \num{-18409.377}\\
F & \num{842.424} & \num{1048263.304}\\
RMSE & \num{21.26} & \num{1.53}\\
\bottomrule
\end{tabular}
\end{table}

Nonlinear in endogenous variable

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 2SLS}
\NormalTok{first\_stage }\OtherTok{=} \FunctionTok{lm}\NormalTok{(endogenous\_nonlinear }\SpecialCharTok{\textasciitilde{}}\NormalTok{ exogenous, }\AttributeTok{data =}\NormalTok{ my\_data)}

\NormalTok{new\_data }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(my\_data, }\AttributeTok{new\_endogenous\_nonlinear =} \FunctionTok{predict}\NormalTok{(first\_stage, my\_data))}
\NormalTok{second\_stage }\OtherTok{=} \FunctionTok{lm}\NormalTok{(response\_nonlinear }\SpecialCharTok{\textasciitilde{}}\NormalTok{ new\_endogenous\_nonlinear, }\AttributeTok{data =}\NormalTok{ new\_data)}
\FunctionTok{summary}\NormalTok{(second\_stage)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = response\_nonlinear \textasciitilde{} new\_endogenous\_nonlinear, data = new\_data)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}    Min     1Q Median     3Q    Max }
\CommentTok{\#\textgreater{} {-}94.43 {-}52.10 {-}15.29  36.50 446.08 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}                          Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)               15.3390    11.8175   1.298    0.194    }
\CommentTok{\#\textgreater{} new\_endogenous\_nonlinear   3.0174     0.3376   8.938   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 69.51 on 9998 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.007927,   Adjusted R{-}squared:  0.007828 }
\CommentTok{\#\textgreater{} F{-}statistic: 79.89 on 1 and 9998 DF,  p{-}value: \textless{} 2.2e{-}16}

\NormalTok{new\_data\_cf }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(my\_data, }\AttributeTok{residual =} \FunctionTok{resid}\NormalTok{(first\_stage))}
\NormalTok{second\_stage\_cf }\OtherTok{=} \FunctionTok{lm}\NormalTok{(response\_nonlinear }\SpecialCharTok{\textasciitilde{}}\NormalTok{ endogenous\_nonlinear }\SpecialCharTok{+}\NormalTok{ residual, }\AttributeTok{data =}\NormalTok{ new\_data\_cf)}
\FunctionTok{summary}\NormalTok{(second\_stage\_cf)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = response\_nonlinear \textasciitilde{} endogenous\_nonlinear + residual, }
\CommentTok{\#\textgreater{}     data = new\_data\_cf)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}      Min       1Q   Median       3Q      Max }
\CommentTok{\#\textgreater{} {-}17.5437  {-}0.8348   0.4614   1.4424   4.8154 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}                      Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)          15.33904    0.38459   39.88   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} endogenous\_nonlinear  3.01737    0.01099  274.64   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} residual              0.24919    0.01104   22.58   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 2.262 on 9997 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.9989, Adjusted R{-}squared:  0.9989 }
\CommentTok{\#\textgreater{} F{-}statistic: 4.753e+06 on 2 and 9997 DF,  p{-}value: \textless{} 2.2e{-}16}

\FunctionTok{modelsummary}\NormalTok{(}\FunctionTok{list}\NormalTok{(second\_stage, second\_stage\_cf))}
\end{Highlighting}
\end{Shaded}

\begin{table}
\centering
\begin{tabular}[t]{lcc}
\toprule
  & (1) & (2)\\
\midrule
(Intercept) & \num{15.339} & \num{15.339}\\
 & (\num{11.817}) & (\num{0.385})\\
new\_endogenous\_nonlinear & \num{3.017} & \\
 & (\num{0.338}) & \\
endogenous\_nonlinear &  & \num{3.017}\\
 &  & \vphantom{1} (\num{0.011})\\
residual &  & \num{0.249}\\
 &  & (\num{0.011})\\
\midrule
Num.Obs. & \num{10000} & \num{10000}\\
R2 & \num{0.008} & \num{0.999}\\
R2 Adj. & \num{0.008} & \num{0.999}\\
AIC & \num{113211.6} & \num{44709.6}\\
BIC & \num{113233.2} & \num{44738.4}\\
Log.Lik. & \num{-56602.782} & \num{-22350.801}\\
F & \num{79.887} & \num{4752573.052}\\
RMSE & \num{69.50} & \num{2.26}\\
\bottomrule
\end{tabular}
\end{table}

Nonlinear in parameters

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 2SLS}
\NormalTok{first\_stage }\OtherTok{=} \FunctionTok{lm}\NormalTok{(endogenous }\SpecialCharTok{\textasciitilde{}}\NormalTok{ exogenous, }\AttributeTok{data =}\NormalTok{ my\_data)}

\NormalTok{new\_data }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(my\_data, }\AttributeTok{new\_endogenous =} \FunctionTok{predict}\NormalTok{(first\_stage, my\_data))}
\NormalTok{second\_stage }\OtherTok{=} \FunctionTok{lm}\NormalTok{(response\_nonlinear\_para }\SpecialCharTok{\textasciitilde{}}\NormalTok{ new\_endogenous, }\AttributeTok{data =}\NormalTok{ new\_data)}
\FunctionTok{summary}\NormalTok{(second\_stage)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = response\_nonlinear\_para \textasciitilde{} new\_endogenous, data = new\_data)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}     Min      1Q  Median      3Q     Max }
\CommentTok{\#\textgreater{} {-}1536.5  {-}452.4   {-}80.7   368.4  3780.9 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}                 Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)    {-}1089.943     61.706  {-}17.66   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} new\_endogenous   119.829      3.078   38.93   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 630.2 on 9998 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.1316, Adjusted R{-}squared:  0.1316 }
\CommentTok{\#\textgreater{} F{-}statistic:  1516 on 1 and 9998 DF,  p{-}value: \textless{} 2.2e{-}16}

\NormalTok{new\_data\_cf }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(my\_data, }\AttributeTok{residual =} \FunctionTok{resid}\NormalTok{(first\_stage))}
\NormalTok{second\_stage\_cf }\OtherTok{=} \FunctionTok{lm}\NormalTok{(response\_nonlinear\_para }\SpecialCharTok{\textasciitilde{}}\NormalTok{ endogenous\_nonlinear }\SpecialCharTok{+}\NormalTok{ residual, }\AttributeTok{data =}\NormalTok{ new\_data\_cf)}
\FunctionTok{summary}\NormalTok{(second\_stage\_cf)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = response\_nonlinear\_para \textasciitilde{} endogenous\_nonlinear + }
\CommentTok{\#\textgreater{}     residual, data = new\_data\_cf)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}     Min      1Q  Median      3Q     Max }
\CommentTok{\#\textgreater{} {-}961.00 {-}139.32  {-}16.02  135.57 1403.62 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}                      Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)          678.1593     9.9177   68.38   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} endogenous\_nonlinear  17.7884     0.2759   64.46   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} residual              52.5016     1.1552   45.45   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 231.9 on 9997 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.8824, Adjusted R{-}squared:  0.8824 }
\CommentTok{\#\textgreater{} F{-}statistic: 3.751e+04 on 2 and 9997 DF,  p{-}value: \textless{} 2.2e{-}16}

\FunctionTok{modelsummary}\NormalTok{(}\FunctionTok{list}\NormalTok{(second\_stage, second\_stage\_cf))}
\end{Highlighting}
\end{Shaded}

\begin{table}
\centering
\begin{tabular}[t]{lcc}
\toprule
  & (1) & (2)\\
\midrule
(Intercept) & \num{-1089.943} & \num{678.159}\\
 & (\num{61.706}) & (\num{9.918})\\
new\_endogenous & \num{119.829} & \\
 & (\num{3.078}) & \\
endogenous\_nonlinear &  & \num{17.788}\\
 &  & (\num{0.276})\\
residual &  & \num{52.502}\\
 &  & (\num{1.155})\\
\midrule
Num.Obs. & \num{10000} & \num{10000}\\
R2 & \num{0.132} & \num{0.882}\\
R2 Adj. & \num{0.132} & \num{0.882}\\
AIC & \num{157302.4} & \num{137311.3}\\
BIC & \num{157324.1} & \num{137340.1}\\
Log.Lik. & \num{-78648.225} & \num{-68651.628}\\
F & \num{1515.642} & \num{37505.777}\\
RMSE & \num{630.10} & \num{231.88}\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{new-advances}{%
\section{New Advances}\label{new-advances}}

\begin{itemize}
\tightlist
\item
  Combine ML and IV \citep{singh2020machine}
\end{itemize}

\hypertarget{matching-methods}{%
\chapter{Matching Methods}\label{matching-methods}}

Matching is a process that aims to close back doors - potential sources of bias - by constructing comparison groups that are similar according to a set of matching variables. This helps to ensure that any observed differences in outcomes between the treatment and comparison groups can be more confidently attributed to the treatment itself, rather than other factors that may differ between the groups.

Matching and DiD can use pre-treatment outcomes to correct for selection bias. From real world data and simulation, \citep{chabe2015analysis} found that matching generally underestimates the average causal effect and gets closer to the true effect with more number of pre-treatment outcomes. When selection bias is symmetric around the treatment date, DID is still consistent when implemented symmetrically (i.e., the same number of period before and after treatment). In cases where selection bias is asymmetric, the MC simulations show that Symmetric DID still performs better than Matching.

Matching is useful, but not a general solution to causal problems \citep{smith2005does}

\textbf{Assumption}: Observables can identify the selection into the treatment and control groups

\textbf{Identification}: The exclusion restriction can be met conditional on the observables

\textbf{Motivation}

Effect of college quality on earnings

\begin{itemize}
\tightlist
\item
  They ultimately estimate the treatment effect on the treated of attending a top (high ACT) versus bottom (low ACT) quartile college
\end{itemize}

\textbf{Example}

\citet{aaronson2007teachers}

Do teachers qualifications (causally) affect student test scores?

Step 1:

\[
Y_{ijt} = \delta_0 + Y_{ij(t-1)} \delta_1 + X_{it} \delta_2 + Z_{jt} \delta_3 + \epsilon_{ijt}
\]

There can always be another variable

Any observable sorting is imperfect

Step 2:

\[
Y_{ijst} = \alpha_0 + Y_{ij(t-1)}\alpha_1 + X_{it} \alpha_2 + Z_{jt} \alpha_3 + \gamma_s + u_{isjt}
\]

\begin{itemize}
\item
  \(\delta_3 >0\)
\item
  \(\delta_3 > \alpha_3\)
\item
  \(\gamma_s\) = school fixed effect
\end{itemize}

Sorting is less within school. Hence, we can introduce the school fixed effect

Step 3:

Find schools that look like they are putting students in class randomly (or as good as random) + we run step 2

\[
\begin{aligned}
Y_{isjt} = Y_{isj(t-1)} \lambda &+ X_{it} \alpha_1 +Z_{jt} \alpha_{21} \\
&+ (Z_{jt} \times D_i)\alpha_{22}+ \gamma_5 + u_{isjt}
\end{aligned}
\]

\begin{itemize}
\item
  \(D_{it}\) is an element of \(X_{it}\)
\item
  \(Z_{it}\) = teacher experience
\end{itemize}

\[
D_{it}=
\begin{cases}
1 & \text{ if high poverty} \\
0 & \text{otherwise}
\end{cases}
\]

\(H_0:\) \(\alpha_{22} = 0\) test for effect heterogeneity whether the effect of teacher experience (\(Z_{jt}\)) is different

\begin{itemize}
\item
  For low poverty is \(\alpha_{21}\)
\item
  For high poverty effect is \(\alpha_{21} + \alpha_{22}\)
\end{itemize}

Matching is \textbf{selection on observables} and only works if you have good observables.

Sufficient identification assumption under Selection on observable/ back-door criterion (based on Bernard Koch's \href{https://www.youtube.com/watch?v=v9uf9rDYEMg\&ab_channel=SummerInstituteinComputationalSocialScience}{presentation})

\begin{itemize}
\item
  Strong conditional ignorability

  \begin{itemize}
  \item
    \(Y(0),Y(1) \perp T|X\)
  \item
    No hidden confounders
  \end{itemize}
\item
  Overlap

  \begin{itemize}
  \item
    \(\forall x \in X, t \in \{0, 1\}: p (T = t | X = x> 0\)
  \item
    All treatments have non-zero probability of being observed
  \end{itemize}
\item
  SUTVA/ Consistency

  \begin{itemize}
  \tightlist
  \item
    Treatment and outcomes of different subjects are independent
  \end{itemize}
\end{itemize}

Relative to \protect\hyperlink{ordinary-least-squares}{OLS}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Matching makes the \textbf{common support} explicit (and changes default from ``ignore'' to ``enforce'')
\item
  Relaxes linear function form. Thus, less parametric.
\end{enumerate}

It also helps if you have high ratio of controls to treatments.

For detail summary \citep{stuart2010matching}

Matching is defined as ``any method that aims to equate (or''balance'') the distribution of covariates in the treated and control groups.'' \citep[pp.~1]{stuart2010matching}

Equivalently, matching is a selection on observables identifications strategy.

\textbf{If you think your OLS estimate is biased, a matching estimate (almost surely) is too.}

Unconditionally, consider

\[
\begin{aligned}
E(Y_i^T | T) - E(Y_i^C |C) &+ E(Y_i^C | T) - E(Y_i^C | T) \\
= E(Y_i^T - Y_i^C | T) &+ [E(Y_i^C | T) - E(Y_i^C |C)] \\
= E(Y_i^T - Y_i^C | T) &+ \text{selection bias}
\end{aligned}
\]

where \(E(Y_i^T - Y_i^C | T)\) is the causal inference that we want to know.

Randomization eliminates the selection bias.

If we don't have randomization, then \(E(Y_i^C | T) \neq E(Y_i^C |C)\)

Matching tries to do selection on observables \(E(Y_i^C | X, T) = E(Y_i^C|X, C)\)

\protect\hyperlink{propensity-scores}{Propensity Scores} basically do \(E(Y_i^C| P(X) , T) = E(Y_i^C | P(X), C)\)

\textbf{Matching standard errors will exceed OLS standard errors}

The treatment should have larger predictive power than the control because you use treatment to pick control (not control to pick treatment).

The average treatment effect (ATE) is

\[
\frac{1}{N_T} \sum_{i=1}^{N_T} (Y_i^T - \frac{1}{N_{C_T}} \sum_{i=1}^{N_{C_T}} Y_i^C)
\]

Since there is no closed-form solution for the standard error of the average treatment effect, we have to use bootstrapping to get standard error.

Professor Gary King advocates instead of using the word ``matching'', we should use ``\textbf{pruning}'' (i.e., deleting observations). It is a preprocessing step where it prunes nonmatches to make control variables less important in your analysis.

Without Matching

\begin{itemize}
\tightlist
\item
  \textbf{Imbalance data} leads to \textbf{model dependence} lead to a lot of \textbf{researcher discretion} leads to \textbf{bias}
\end{itemize}

With Matching

\begin{itemize}
\tightlist
\item
  We have balance data which essentially erase human discretion
\end{itemize}

\begin{longtable}[]{@{}lll@{}}
\caption{Table @ref(tab:Gary King - International Methods Colloquium talk 2015)}\tabularnewline
\toprule\noalign{}
Balance Covariates & Complete Randomization & Fully Exact \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
Balance Covariates & Complete Randomization & Fully Exact \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Observed & On average & Exact \\
Unobserved & On average & On average \\
\end{longtable}

Fully blocked is superior on

\begin{itemize}
\item
  imbalance
\item
  model dependence
\item
  power
\item
  efficiency
\item
  bias
\item
  research costs
\item
  robustness
\end{itemize}

Matching is used when

\begin{itemize}
\item
  Outcomes are not available to select subjects for follow-up
\item
  Outcomes are available to improve precision of the estimate (i.e., reduce bias)
\end{itemize}

Hence, we can only observe one outcome of a unit (either treated or control), we can think of this problem as missing data as well. Thus, this section is closely related to \protect\hyperlink{imputation-missing-data}{Imputation (Missing Data)}

In observational studies, we cannot randomize the treatment effect. Subjects select their own treatments, which could introduce selection bias (i.e., systematic differences between group differences that confound the effects of response variable differences).

Matching is used to

\begin{itemize}
\item
  reduce model dependence
\item
  diagnose balance in the dataset
\end{itemize}

Assumptions of matching:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  treatment assignment is independent of potential outcomes given the covariates

  \begin{itemize}
  \item
    \(T \perp (Y(0),Y(1))|X\)
  \item
    known as ignorability, or ignorable, no hidden bias, or unconfounded.
  \item
    You typically satisfy this assumption when unobserved covariates correlated with observed covariates.

    \begin{itemize}
    \tightlist
    \item
      But when unobserved covariates are unrelated to the observed covariates, you can use sensitivity analysis to check your result, or use ``design sensitivity'' \citep{heller2009split}
    \end{itemize}
  \end{itemize}
\item
  positive probability of receiving treatment for all X

  \begin{itemize}
  \tightlist
  \item
    \(0 < P(T=1|X)<1 \forall X\)
  \end{itemize}
\item
  Stable Unit Treatment value Assumption (SUTVA)

  \begin{itemize}
  \item
    Outcomes of A are not affected by treatment of B.

    \begin{itemize}
    \tightlist
    \item
      Very hard in cases where there is ``spillover'' effects (interactions between control and treatment). To combat, we need to reduce interactions.
    \end{itemize}
  \end{itemize}
\end{enumerate}

Generalization

\begin{itemize}
\item
  \(P_t\): treated population -\textgreater{} \(N_t\): random sample from treated
\item
  \(P_c\): control population -\textgreater{} \(N_c\): random sample from control
\item
  \(\mu_i\) = means ; \(\Sigma_i\) = variance covariance matrix of the \(p\) covariates in group i (\(i = t,c\))
\item
  \(X_j\) = \(p\) covariates of individual \(j\)
\item
  \(T_j\) = treatment assignment
\item
  \(Y_j\) = observed outcome
\item
  Assume: \(N_t < N_c\)
\item
  Treatment effect is \(\tau(x) = R_1(x) - R_0(x)\) where

  \begin{itemize}
  \item
    \(R_1(x) = E(Y(1)|X)\)
  \item
    \(R_0(x) = E(Y(0)|X)\)
  \end{itemize}
\item
  Assume: parallel trends hence \(\tau(x) = \tau \forall x\)

  \begin{itemize}
  \tightlist
  \item
    If the parallel trends are not assumed, an average effect can be estimated.
  \end{itemize}
\item
  Common estimands:

  \begin{itemize}
  \item
    Average effect of the treatment on the treated (\textbf{ATT}): effects on treatment group
  \item
    Average treatment effect (\textbf{ATE}): effect on both treatment and control
  \end{itemize}
\end{itemize}

Steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Define ``closeness'': decide distance measure to be used

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    Which variables to include:

    \begin{enumerate}
    \def\labelenumiii{\arabic{enumiii}.}
    \item
      Ignorability (no unobserved differences between treatment and control)

      \begin{enumerate}
      \def\labelenumiv{\arabic{enumiv}.}
      \item
        Since cost of including unrelated variables is small, you should include as many as possible (unless sample size/power doesn't allow you to because of increased variance)
      \item
        Do not include variables that were affected by the treatment.
      \item
        Note: if a matching variable (i.e., heavy drug users) is highly correlated to the outcome variable (i.e., heavy drinkers) , you will be better to exclude it in the matching set.
      \end{enumerate}
    \end{enumerate}
  \item
    Which distance measures: more below
  \end{enumerate}
\item
  Matching methods

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    Nearest neighbor matching

    \begin{enumerate}
    \def\labelenumiii{\arabic{enumiii}.}
    \item
      Simple (greedy) matching: performs poorly when there is competition for controls.
    \item
      Optimal matching: considers global distance measure
    \item
      Ratio matching: to combat increase bias and reduced variation when you have k:1 matching, one can use approximations by \citet{rubin1996matching}.
    \item
      With or without replacement: with replacement is typically better, but one needs to account for dependent in the matched sample when doing later analysis (can use frequency weights to combat).
    \end{enumerate}
  \item
    Subclassification, Full Matching and Weighting

    Nearest neighbor matching assign is 0 (control) or 1 (treated), while these methods use weights between 0 and 1.

    \begin{enumerate}
    \def\labelenumiii{\arabic{enumiii}.}
    \item
      Subclassification: distribution into multiple subclass (e.g., 5-10)
    \item
      Full matching: optimal ly minimize the average of the distances between each treated unit and each control unit within each matched set.
    \item
      Weighting adjustments: weighting technique uses propensity scores to estimate ATE. If the weights are extreme, the variance can be large not due to the underlying probabilities, but due to the estimation procure. To combat this, use (1) weight trimming, or (2) doubly -robust methods when propensity scores are used for weighing or matching.

      \begin{enumerate}
      \def\labelenumiv{\arabic{enumiv}.}
      \item
        Inverse probability of treatment weighting (IPTW) \(w_i = \frac{T_i}{\hat{e}_i} + \frac{1 - T_i}{1 - \hat{e}_i}\)
      \item
        Odds \(w_i = T_i + (1-T_i) \frac{\hat{e}_i}{1-\hat{e}_i}\)
      \item
        Kernel weighting (e.g., in economics) averages over multiple units in the control group.
      \end{enumerate}
    \end{enumerate}
  \item
    Assessing Common Support

    \begin{itemize}
    \tightlist
    \item
      common support means overlapping of the propensity score distributions in the treatment and control groups. Propensity score is used to discard control units from the common support. Alternatively, convex hull of the covariates in the multi-dimensional space.
    \end{itemize}
  \end{enumerate}
\item
  Assessing the quality of matched samples (Diagnose)

  \begin{itemize}
  \item
    Balance = similarity of the empirical distribution of the full set of covariates in the matched treated and control groups. Equivalently, treatment is unrelated to the covariates

    \begin{itemize}
    \tightlist
    \item
      \(\tilde{p}(X|T=1) = \tilde{p}(X|T=0)\) where \(\tilde{p}\) is the empirical distribution.
    \end{itemize}
  \item
    Numerical Diagnostics

    \begin{enumerate}
    \def\labelenumii{\arabic{enumii}.}
    \item
      standardized difference in means of each covariate (most common), also known as''standardized bias'', ``standardized difference in means''.
    \item
      standardized difference of means of the propensity score (should be \textless{} 0.25) \citep{rubin2001using}
    \item
      ratio of the variances of the propensity score in the treated and control groups (should be between 0.5 and 2). \citep{rubin2001using}
    \item
      For each covariate, the ratio fo the variance of the residuals orthogonal to the propensity score in the treated and control groups.

      Note: can't use hypothesis tests or p-values because of (1) in-sample property (not population), (2) conflation of changes in balance with changes in statistical power.
    \end{enumerate}
  \item
    Graphical Diagnostics

    \begin{itemize}
    \item
      QQ plots
    \item
      Empirical Distribution Plot
    \end{itemize}
  \end{itemize}
\item
  Estimate the treatment effect

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    After k:1

    \begin{enumerate}
    \def\labelenumiii{\arabic{enumiii}.}
    \tightlist
    \item
      Need to account for weights when use matching with replacement.
    \end{enumerate}
  \item
    After Subclassification and Full Matching

    \begin{enumerate}
    \def\labelenumiii{\arabic{enumiii}.}
    \item
      Weighting the subclass estimates by the number of treated units in each subclass for ATT
    \item
      Weighting by the overall number of individual in each subclass for ATE.
    \end{enumerate}
  \item
    Variance estimation: should incorporate uncertainties in both the matching procedure (step 3) and the estimation procedure (step 4)
  \end{enumerate}
\end{enumerate}

\textbf{Notes}:

\begin{itemize}
\item
  With missing data, use generalized boosted models, or multiple imputation \citep{qu2009propensity}
\item
  Violation of ignorable treatment assignment (i.e., unobservables affect treatment and outcome). control by

  \begin{itemize}
  \item
    measure pre-treatment measure of the outcome variable
  \item
    find the difference in outcomes between multiple control groups. If there is a significant difference, there is evidence for violation.
  \item
    find the range of correlations between unobservables and both treatment assignment and outcome to nullify the significant effect.
  \end{itemize}
\item
  Choosing between methods

  \begin{itemize}
  \item
    smallest standardized difference of mean across the largest number of covariates
  \item
    minimize the standardized difference of means of a few particularly prognostic covariates
  \item
    fest number of large standardized difference of means (\textgreater{} 0.25)
  \item
    \citep{diamond2013genetic} automates the process
  \end{itemize}
\item
  In practice

  \begin{itemize}
  \item
    If ATE, ask if there is enough overlap of the treated and control groups' propensity score to estimate ATE, if not use ATT instead
  \item
    If ATT, ask if there are controls across the full range of the treated group
  \end{itemize}
\item
  Choose matching method

  \begin{itemize}
  \item
    If ATE, use IPTW or full matching
  \item
    If ATT, and more controls than treated (at least 3 times), k:1 nearest neighbor without replacement
  \item
    If ATT, and few controls , use subclassification, full matching, and weighting by the odds
  \end{itemize}
\item
  Diagnostic

  \begin{itemize}
  \item
    If balance, use regression on matched samples
  \item
    If imbalance on few covariates, treat them with Mahalanobis
  \item
    If imbalance on many covariates, try k:1 matching with replacement
  \end{itemize}
\end{itemize}

Ways to define the distance \(D_{ij}\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Exact
\end{enumerate}

\[
D_{ij} = 
\begin{cases}
0, \text{ if } X_i = X_j, \\
\infty, \text{ if } X_i \neq X_j
\end{cases}
\]

An advanced is \protect\hyperlink{coarsened-exact-matching}{Coarsened Exact Matching}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Mahalanobis
\end{enumerate}

\[
D_{ij} = (X_i - X_j)'\Sigma^{-1} (X_i - X_j)
\]

where

\(\Sigma\) = variance covariance matrix of X in the

\begin{itemize}
\item
  control group if ATT is interested
\item
  polled treatment and control groups if ATE is interested
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Propensity score:
\end{enumerate}

\[
D_{ij} = |e_i - e_j|
\]

where \(e_k\) = the propensity score for individual k

An advanced is Prognosis score \citep{hansen2008prognostic}, but you have to know (i.e., specify) the relationship between the covariates and outcome.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Linear propensity score
\end{enumerate}

\[
D_{ij} = |logit(e_i) - logit(e_j)|
\]

The exact and Mahalanobis are not good in high dimensional or non normally distributed X's cases.

We can combine Mahalanobis matching with propensity score calipers \citep{rubin2000combining}

Other advanced methods for longitudinal settings

\begin{itemize}
\item
  marginal structural models \citep{robins2000marginal}
\item
  balanced risk set matching \citep{li2001balanced}
\end{itemize}

Most matching methods are based on (ex-post)

\begin{itemize}
\item
  propensity score
\item
  distance metric
\item
  covariates
\end{itemize}

Packages

\begin{itemize}
\item
  \texttt{cem} Coarsened exact matching
\item
  \texttt{Matching} Multivariate and propensity score matching with balance optimization
\item
  \texttt{MatchIt} Nonparametric preprocessing for parametric causal inference. Have nearest neighbor, Mahalanobis, caliper, exact, full, optimal, subclassification
\item
  \texttt{MatchingFrontier} optimize balance and sample size \citep{king2017balance}
\item
  \texttt{optmatch}optimal matching with variable ratio, optimal and full matching
\item
  \texttt{PSAgraphics} Propensity score graphics
\item
  \texttt{rbounds} sensitivity analysis with matched data, examine ignorable treatment assignment assumption
\item
  \texttt{twang} weighting and analysis of non-equivalent groups
\item
  \texttt{CBPS} covariate balancing propensity score. Can also be used in the longitudinal setting with marginal structural models.
\item
  \texttt{PanelMatch} based on \href{https://imai.fas.harvard.edu/research/files/tscs.pdf}{Imai, Kim, and Wang (2018)}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.6201}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.3799}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Matching
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Regression
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Not as sensitive to the functional form of the covariates & can estimate the effect of a continuous treatment \\
Easier to asses whether it's working

Easier to explain

allows a nice visualization of an evaluation & estimate the effect of all the variables (not just the treatment) \\
If you treatment is fairly rare, you may have a lot of control observations that are obviously no comparable & can estimate interactions of treatment with covariates \\
Less parametric & More parametric \\
Enforces common support (i.e., space where treatment and control have the same characteristics) & \\
\end{longtable}

However, the problem of \textbf{omitted variables} (i.e., those that affect both the outcome and whether observation was treated) - unobserved confounders is still present in matching methods.

Difference between matching and regression following Pischke's \href{https://econ.lse.ac.uk/staff/spischke/ec533/regression\%20vs\%20matching.pdf}{lecture}

Suppose we want to estimate the effect of treatment on the treated

\[
\begin{aligned}
\delta_{TOT} &= E[ Y_{1i} - Y_{0i} | D_i = 1 ] \\
&= E\{E[Y_{1i} | X_i, D_i = 1] \\
& - E[Y_{0i}|X_i, D_i = 1]|D_i = 1\} && \text{law of itereated expectations}
\end{aligned}
\]

Under conditional independence

\[
E[Y_{0i} |X_i , D_i = 0 ] = E[Y_{0i} | X_i, D_i = 1]
\]

then

\[
\begin{aligned}
\delta_{TOT} &= E \{ E[ Y_{1i} | X_i, D_i = 1] - E[ Y_{0i}|X_i, D_i = 0 ]|D_i = 1\} \\
&= E\{E[y_i | X_i, D_i = 1] - E[y_i |X_i, D_i = 0 ] | D_i = 1\} \\
&= E[\delta_X |D_i = 1]
\end{aligned}
\]

where \(\delta_X\) is an X-specific difference in means at covariate value \(X_i\)

When \(X_i\) is discrete, the matching estimand is

\[
\delta_M = \sum_x \delta_x P(X_i = x |D_i = 1)
\]

where \(P(X_i = x |D_i = 1)\) is the probability mass function for \(X_i\) given \(D_i = 1\)

According to Bayes rule,

\[
P(X_i = x | D_i = 1) = \frac{P(D_i = 1 | X_i = x) \times P(X_i = x)}{P(D_i = 1)}
\]

hence,

\[
\begin{aligned}
\delta_M &= \frac{\sum_x \delta_x P (D_i = 1 | X_i = x) P (X_i = x)}{\sum_x P(D_i = 1 |X_i = x)P(X_i = x)} \\
&= \sum_x \delta_x \frac{ P (D_i = 1 | X_i = x) P (X_i = x)}{\sum_x P(D_i = 1 |X_i = x)P(X_i = x)}
\end{aligned}
\]

On the other hand, suppose we have regression

\[
y_i = \sum_x d_{ix} \beta_x + \delta_R D_i + \epsilon_i
\]

where

\begin{itemize}
\item
  \(d_{ix}\) = dummy that indicates \(X_i = x\)
\item
  \(\beta_x\) = regression-effect for \(X_i = x\)
\item
  \(\delta_R\) = regression estimand where
\end{itemize}

\[
\begin{aligned}
\delta_R &= \frac{\sum_x \delta_x [P(D_i = 1 | X_i = x) (1 - P(D_i = 1 | X_i = x))]P(X_i = x)}{\sum_x [P(D_i = 1| X_i = x)(1 - P(D_i = 1 | X_i = x))]P(X_i = x)} \\
&= \sum_x \delta_x \frac{[P(D_i = 1 | X_i = x) (1 - P(D_i = 1 | X_i = x))]P(X_i = x)}{\sum_x [P(D_i = 1| X_i = x)(1 - P(D_i = 1 | X_i = x))]P(X_i = x)}
\end{aligned}
\]

the difference between the regression and matching estimand is the weights they use to combine the covariate specific treatment effect \(\delta_x\)

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.0253}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1715}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.4464}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3528}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
uses weights which depend on
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
interpretation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
makes sense because
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Matching & \(P(D_i = 1|X_i = x)\)

the fraction of treated observations in a covariate cell (i.e., or the mean of \(D_i\)) & This is larger in cells with many treated observations. & we want the effect of treatment on the treated \\
Regression & \(P(D_i = 1 |X_i = x)(1 - P(D_i = 1| X_i ))\)

the variance of \(D_i\) in the covariate cell & This weight is largest in cells where there are half treated and half untreated observations. (this is the reason why we want to treat our sample so it is balanced, before running regular regression model, as mentioned above). & these cells will produce the lowest variance estimates of \(\delta_x\). If all the \(\delta_x\) are the same, the most efficient estimand uses the lowest variance cells most heavily. \\
\end{longtable}

The goal of matching is to produce covariate balance (i.e., distributions of covariates in treatment and control groups are approximately similar as they would be in a successful randomized experiment).

\hypertarget{selection-on-observables}{%
\section{Selection on Observables}\label{selection-on-observables}}

\hypertarget{matchit}{%
\subsection{MatchIt}\label{matchit}}

Procedure typically involves (proposed by \href{https://cran.r-project.org/web/packages/MatchIt/vignettes/MatchIt.html}{Noah Freifer} using \texttt{MatchIt})

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  planning
\item
  matching
\item
  checking (balance)
\item
  estimating the treatment effect
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(MatchIt)}
\FunctionTok{data}\NormalTok{(}\StringTok{"lalonde"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

examine \texttt{treat} on \texttt{re78}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Planning
\end{enumerate}

\begin{itemize}
\item
  select type of effect to be estimated (e.g., mediation effect, conditional effect, marginal effect)
\item
  select the target population
\item
  select variables to match/balance \citep{austin2011optimal} \citep{vanderweele2019principles}
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Check Initial Imbalance
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# No matching; constructing a pre{-}match matchit object}
\NormalTok{m.out0 }\OtherTok{\textless{}{-}} \FunctionTok{matchit}\NormalTok{(}
    \FunctionTok{formula}\NormalTok{(treat }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ educ }\SpecialCharTok{+}\NormalTok{ race }
            \SpecialCharTok{+}\NormalTok{ married }\SpecialCharTok{+}\NormalTok{ nodegree }\SpecialCharTok{+}\NormalTok{ re74 }\SpecialCharTok{+}\NormalTok{ re75, }\AttributeTok{env =}\NormalTok{ lalonde),}
    \AttributeTok{data =} \FunctionTok{data.frame}\NormalTok{(lalonde),}
    \AttributeTok{method =} \ConstantTok{NULL}\NormalTok{,}
    \CommentTok{\# assess balance before matching}
    \AttributeTok{distance =} \StringTok{"glm"} \CommentTok{\# logistic regression}
\NormalTok{)}

\CommentTok{\# Checking balance prior to matching}
\FunctionTok{summary}\NormalTok{(m.out0)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Matching
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 1:1 NN PS matching w/o replacement}
\NormalTok{m.out1 }\OtherTok{\textless{}{-}} \FunctionTok{matchit}\NormalTok{(treat }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ educ,}
                  \AttributeTok{data =}\NormalTok{ lalonde,}
                  \AttributeTok{method =} \StringTok{"nearest"}\NormalTok{,}
                  \AttributeTok{distance =} \StringTok{"glm"}\NormalTok{)}
\NormalTok{m.out1}
\CommentTok{\#\textgreater{} A matchit object}
\CommentTok{\#\textgreater{}  {-} method: 1:1 nearest neighbor matching without replacement}
\CommentTok{\#\textgreater{}  {-} distance: Propensity score}
\CommentTok{\#\textgreater{}              {-} estimated with logistic regression}
\CommentTok{\#\textgreater{}  {-} number of obs.: 614 (original), 370 (matched)}
\CommentTok{\#\textgreater{}  {-} target estimand: ATT}
\CommentTok{\#\textgreater{}  {-} covariates: age, educ}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Check balance
\end{enumerate}

Sometimes you have to make trade-off between balance and sample size.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Checking balance after NN matching}
\FunctionTok{summary}\NormalTok{(m.out1, }\AttributeTok{un =} \ConstantTok{FALSE}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} matchit(formula = treat \textasciitilde{} age + educ, data = lalonde, method = "nearest", }
\CommentTok{\#\textgreater{}     distance = "glm")}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Summary of Balance for Matched Data:}
\CommentTok{\#\textgreater{}          Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean}
\CommentTok{\#\textgreater{} distance        0.3080        0.3077          0.0094     0.9963    0.0033}
\CommentTok{\#\textgreater{} age            25.8162       25.8649         {-}0.0068     1.0300    0.0050}
\CommentTok{\#\textgreater{} educ           10.3459       10.2865          0.0296     0.5886    0.0253}
\CommentTok{\#\textgreater{}          eCDF Max Std. Pair Dist.}
\CommentTok{\#\textgreater{} distance   0.0432          0.0146}
\CommentTok{\#\textgreater{} age        0.0162          0.0597}
\CommentTok{\#\textgreater{} educ       0.1189          0.8146}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Sample Sizes:}
\CommentTok{\#\textgreater{}           Control Treated}
\CommentTok{\#\textgreater{} All           429     185}
\CommentTok{\#\textgreater{} Matched       185     185}
\CommentTok{\#\textgreater{} Unmatched     244       0}
\CommentTok{\#\textgreater{} Discarded       0       0}

\CommentTok{\# examine visually}
\FunctionTok{plot}\NormalTok{(m.out1, }\AttributeTok{type =} \StringTok{"jitter"}\NormalTok{, }\AttributeTok{interactive =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{31-matching-methods_files/figure-latex/unnamed-chunk-4-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\FunctionTok{plot}\NormalTok{(}
\NormalTok{    m.out1,}
    \AttributeTok{type =} \StringTok{"qq"}\NormalTok{,}
    \AttributeTok{interactive =} \ConstantTok{FALSE}\NormalTok{,}
    \AttributeTok{which.xs =} \FunctionTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{31-matching-methods_files/figure-latex/unnamed-chunk-4-2} \end{center}

Try Full Match (i.e., every treated matches with one control, and every control with one treated).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Full matching on a probit PS}
\NormalTok{m.out2 }\OtherTok{\textless{}{-}} \FunctionTok{matchit}\NormalTok{(treat }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ educ, }
                  \AttributeTok{data =}\NormalTok{ lalonde,}
                  \AttributeTok{method =} \StringTok{"full"}\NormalTok{, }
                  \AttributeTok{distance =} \StringTok{"glm"}\NormalTok{, }
                  \AttributeTok{link =} \StringTok{"probit"}\NormalTok{)}
\NormalTok{m.out2}
\CommentTok{\#\textgreater{} A matchit object}
\CommentTok{\#\textgreater{}  {-} method: Optimal full matching}
\CommentTok{\#\textgreater{}  {-} distance: Propensity score}
\CommentTok{\#\textgreater{}              {-} estimated with probit regression}
\CommentTok{\#\textgreater{}  {-} number of obs.: 614 (original), 614 (matched)}
\CommentTok{\#\textgreater{}  {-} target estimand: ATT}
\CommentTok{\#\textgreater{}  {-} covariates: age, educ}
\end{Highlighting}
\end{Shaded}

Checking balance again

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Checking balance after full matching}
\FunctionTok{summary}\NormalTok{(m.out2, }\AttributeTok{un =} \ConstantTok{FALSE}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} matchit(formula = treat \textasciitilde{} age + educ, data = lalonde, method = "full", }
\CommentTok{\#\textgreater{}     distance = "glm", link = "probit")}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Summary of Balance for Matched Data:}
\CommentTok{\#\textgreater{}          Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean}
\CommentTok{\#\textgreater{} distance        0.3082        0.3081          0.0023     0.9815    0.0028}
\CommentTok{\#\textgreater{} age            25.8162       25.8035          0.0018     0.9825    0.0062}
\CommentTok{\#\textgreater{} educ           10.3459       10.2315          0.0569     0.4390    0.0481}
\CommentTok{\#\textgreater{}          eCDF Max Std. Pair Dist.}
\CommentTok{\#\textgreater{} distance   0.0270          0.0382}
\CommentTok{\#\textgreater{} age        0.0249          0.1110}
\CommentTok{\#\textgreater{} educ       0.1300          0.9805}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Sample Sizes:}
\CommentTok{\#\textgreater{}               Control Treated}
\CommentTok{\#\textgreater{} All            429.       185}
\CommentTok{\#\textgreater{} Matched (ESS)  145.23     185}
\CommentTok{\#\textgreater{} Matched        429.       185}
\CommentTok{\#\textgreater{} Unmatched        0.         0}
\CommentTok{\#\textgreater{} Discarded        0.         0}

\FunctionTok{plot}\NormalTok{(}\FunctionTok{summary}\NormalTok{(m.out2))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{31-matching-methods_files/figure-latex/unnamed-chunk-6-1} \end{center}

Exact Matching

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Full matching on a probit PS}
\NormalTok{m.out3 }\OtherTok{\textless{}{-}}
    \FunctionTok{matchit}\NormalTok{(}
\NormalTok{        treat }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ educ,}
        \AttributeTok{data =}\NormalTok{ lalonde,}
        \AttributeTok{method =} \StringTok{"exact"}
\NormalTok{    )}
\NormalTok{m.out3}
\CommentTok{\#\textgreater{} A matchit object}
\CommentTok{\#\textgreater{}  {-} method: Exact matching}
\CommentTok{\#\textgreater{}  {-} number of obs.: 614 (original), 332 (matched)}
\CommentTok{\#\textgreater{}  {-} target estimand: ATT}
\CommentTok{\#\textgreater{}  {-} covariates: age, educ}
\end{Highlighting}
\end{Shaded}

Subclassfication

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m.out4 }\OtherTok{\textless{}{-}} \FunctionTok{matchit}\NormalTok{(}
\NormalTok{    treat }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ educ, }
    \AttributeTok{data =}\NormalTok{ lalonde,}
    \AttributeTok{method =} \StringTok{"subclass"}
\NormalTok{)}
\NormalTok{m.out4}
\CommentTok{\#\textgreater{} A matchit object}
\CommentTok{\#\textgreater{}  {-} method: Subclassification (6 subclasses)}
\CommentTok{\#\textgreater{}  {-} distance: Propensity score}
\CommentTok{\#\textgreater{}              {-} estimated with logistic regression}
\CommentTok{\#\textgreater{}  {-} number of obs.: 614 (original), 614 (matched)}
\CommentTok{\#\textgreater{}  {-} target estimand: ATT}
\CommentTok{\#\textgreater{}  {-} covariates: age, educ}

\CommentTok{\# Or you can use in conjunction with "nearest"}
\NormalTok{m.out4 }\OtherTok{\textless{}{-}} \FunctionTok{matchit}\NormalTok{(}
\NormalTok{    treat }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ educ,}
    \AttributeTok{data =}\NormalTok{ lalonde,}
    \AttributeTok{method =} \StringTok{"nearest"}\NormalTok{,}
    \AttributeTok{option =} \StringTok{"subclass"}
\NormalTok{)}
\NormalTok{m.out4}
\CommentTok{\#\textgreater{} A matchit object}
\CommentTok{\#\textgreater{}  {-} method: 1:1 nearest neighbor matching without replacement}
\CommentTok{\#\textgreater{}  {-} distance: Propensity score}
\CommentTok{\#\textgreater{}              {-} estimated with logistic regression}
\CommentTok{\#\textgreater{}  {-} number of obs.: 614 (original), 370 (matched)}
\CommentTok{\#\textgreater{}  {-} target estimand: ATT}
\CommentTok{\#\textgreater{}  {-} covariates: age, educ}
\end{Highlighting}
\end{Shaded}

Optimal Matching

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m.out5 }\OtherTok{\textless{}{-}} \FunctionTok{matchit}\NormalTok{(}
\NormalTok{    treat }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ educ, }
    \AttributeTok{data =}\NormalTok{ lalonde,}
    \AttributeTok{method =} \StringTok{"optimal"}\NormalTok{,}
    \AttributeTok{ratio =} \DecValTok{2}
\NormalTok{)}
\NormalTok{m.out5}
\CommentTok{\#\textgreater{} A matchit object}
\CommentTok{\#\textgreater{}  {-} method: 2:1 optimal pair matching}
\CommentTok{\#\textgreater{}  {-} distance: Propensity score}
\CommentTok{\#\textgreater{}              {-} estimated with logistic regression}
\CommentTok{\#\textgreater{}  {-} number of obs.: 614 (original), 555 (matched)}
\CommentTok{\#\textgreater{}  {-} target estimand: ATT}
\CommentTok{\#\textgreater{}  {-} covariates: age, educ}
\end{Highlighting}
\end{Shaded}

Genetic Matching

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m.out6 }\OtherTok{\textless{}{-}} \FunctionTok{matchit}\NormalTok{(}
\NormalTok{    treat }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ educ, }
    \AttributeTok{data =}\NormalTok{ lalonde,}
    \AttributeTok{method =} \StringTok{"genetic"}
\NormalTok{)}
\NormalTok{m.out6}
\CommentTok{\#\textgreater{} A matchit object}
\CommentTok{\#\textgreater{}  {-} method: 1:1 genetic matching without replacement}
\CommentTok{\#\textgreater{}  {-} distance: Propensity score}
\CommentTok{\#\textgreater{}              {-} estimated with logistic regression}
\CommentTok{\#\textgreater{}  {-} number of obs.: 614 (original), 370 (matched)}
\CommentTok{\#\textgreater{}  {-} target estimand: ATT}
\CommentTok{\#\textgreater{}  {-} covariates: age, educ}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Estimating the Treatment Effect
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# get matched data}
\NormalTok{m.data1 }\OtherTok{\textless{}{-}} \FunctionTok{match.data}\NormalTok{(m.out1)}

\FunctionTok{head}\NormalTok{(m.data1)}
\CommentTok{\#\textgreater{}      treat age educ   race married nodegree re74 re75       re78  distance}
\CommentTok{\#\textgreater{} NSW1     1  37   11  black       1        1    0    0  9930.0460 0.2536942}
\CommentTok{\#\textgreater{} NSW2     1  22    9 hispan       0        1    0    0  3595.8940 0.3245468}
\CommentTok{\#\textgreater{} NSW3     1  30   12  black       0        0    0    0 24909.4500 0.2881139}
\CommentTok{\#\textgreater{} NSW4     1  27   11  black       0        1    0    0  7506.1460 0.3016672}
\CommentTok{\#\textgreater{} NSW5     1  33    8  black       0        1    0    0   289.7899 0.2683025}
\CommentTok{\#\textgreater{} NSW6     1  22    9  black       0        1    0    0  4056.4940 0.3245468}
\CommentTok{\#\textgreater{}      weights subclass}
\CommentTok{\#\textgreater{} NSW1       1        1}
\CommentTok{\#\textgreater{} NSW2       1       98}
\CommentTok{\#\textgreater{} NSW3       1      109}
\CommentTok{\#\textgreater{} NSW4       1      120}
\CommentTok{\#\textgreater{} NSW5       1      131}
\CommentTok{\#\textgreater{} NSW6       1      142}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"lmtest"}\NormalTok{) }\CommentTok{\#coeftest}
\FunctionTok{library}\NormalTok{(}\StringTok{"sandwich"}\NormalTok{) }\CommentTok{\#vcovCL}

\CommentTok{\# imbalance matched dataset}
\NormalTok{fit1 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(re78 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ treat }\SpecialCharTok{+}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ educ ,}
           \AttributeTok{data =}\NormalTok{ m.data1, }
           \AttributeTok{weights =}\NormalTok{ weights)}

\FunctionTok{coeftest}\NormalTok{(fit1, }\AttributeTok{vcov. =}\NormalTok{ vcovCL, }\AttributeTok{cluster =} \SpecialCharTok{\textasciitilde{}}\NormalTok{subclass)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} t test of coefficients:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}              Estimate Std. Error t value Pr(\textgreater{}|t|)   }
\CommentTok{\#\textgreater{} (Intercept)  {-}174.902   2445.013 {-}0.0715 0.943012   }
\CommentTok{\#\textgreater{} treat       {-}1139.085    780.399 {-}1.4596 0.145253   }
\CommentTok{\#\textgreater{} age           153.133     55.317  2.7683 0.005922 **}
\CommentTok{\#\textgreater{} educ          358.577    163.860  2.1883 0.029278 * }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

\texttt{treat} coefficient = estimated ATT

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# balance matched dataset }
\NormalTok{m.data2 }\OtherTok{\textless{}{-}} \FunctionTok{match.data}\NormalTok{(m.out2)}

\NormalTok{fit2 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(re78 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ treat }\SpecialCharTok{+}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ educ , }
           \AttributeTok{data =}\NormalTok{ m.data2, }\AttributeTok{weights =}\NormalTok{ weights)}

\FunctionTok{coeftest}\NormalTok{(fit2, }\AttributeTok{vcov. =}\NormalTok{ vcovCL, }\AttributeTok{cluster =} \SpecialCharTok{\textasciitilde{}}\NormalTok{subclass)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} t test of coefficients:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)  }
\CommentTok{\#\textgreater{} (Intercept) 2151.952   3141.152  0.6851  0.49355  }
\CommentTok{\#\textgreater{} treat       {-}725.184    703.297 {-}1.0311  0.30289  }
\CommentTok{\#\textgreater{} age          120.260     53.933  2.2298  0.02612 *}
\CommentTok{\#\textgreater{} educ         175.693    241.694  0.7269  0.46755  }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

When reporting, remember to mention

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  the matching specification (method, and additional options)
\item
  the distance measure (e.g., propensity score)
\item
  other methods, and rationale for the final chosen method.
\item
  balance statistics of the matched dataset.
\item
  number of matched, unmatched, discarded
\item
  estimation method for treatment effect.
\end{enumerate}

\hypertarget{designmatch}{%
\subsection{designmatch}\label{designmatch}}

This package includes

\begin{itemize}
\item
  \texttt{distmatch} optimal distance matching
\item
  \texttt{bmatch} optimal bipartile matching
\item
  \texttt{cardmatch} optimal cardinality matching
\item
  \texttt{profmatch} optimal profile matching
\item
  \texttt{nmatch} optimal nonbipartile matching
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(designmatch)}
\end{Highlighting}
\end{Shaded}

\hypertarget{matchingfrontier}{%
\subsection{MatchingFrontier}\label{matchingfrontier}}

As mentioned in \texttt{MatchIt}, you have to make trade-off (also known as bias-variance trade-off) between balance and sample size. An automated procedure to optimize this trade-off is implemented in \texttt{MatchingFrontier} \citep{king2017balance}, which solves this joint optimization problem.

Following \texttt{MatchingFrontier} \href{https://projects.iq.harvard.edu/files/frontier/files/using_matchingfrontier.pdf}{guide}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# library(devtools)}
\CommentTok{\# install\_github(\textquotesingle{}ChristopherLucas/MatchingFrontier\textquotesingle{})}
\FunctionTok{library}\NormalTok{(MatchingFrontier)}
\FunctionTok{data}\NormalTok{(}\StringTok{"lalonde"}\NormalTok{)}
\CommentTok{\# choose var to match on}
\NormalTok{match.on }\OtherTok{\textless{}{-}}
    \FunctionTok{colnames}\NormalTok{(lalonde)[}\SpecialCharTok{!}\NormalTok{(}\FunctionTok{colnames}\NormalTok{(lalonde) }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}re78\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}treat\textquotesingle{}}\NormalTok{))]}
\NormalTok{match.on}

\CommentTok{\# Mahanlanobis frontier (default)}
\NormalTok{mahal.frontier }\OtherTok{\textless{}{-}}
    \FunctionTok{makeFrontier}\NormalTok{(}
        \AttributeTok{dataset =}\NormalTok{ lalonde,}
        \AttributeTok{treatment =} \StringTok{"treat"}\NormalTok{,}
        \AttributeTok{match.on =}\NormalTok{ match.on}
\NormalTok{    )}
\NormalTok{mahal.frontier}

\CommentTok{\# L1 frontier}
\NormalTok{L1.frontier }\OtherTok{\textless{}{-}}
    \FunctionTok{makeFrontier}\NormalTok{(}
        \AttributeTok{dataset =}\NormalTok{ lalonde,}
        \AttributeTok{treatment =} \StringTok{\textquotesingle{}treat\textquotesingle{}}\NormalTok{,}
        \AttributeTok{match.on =}\NormalTok{ match.on,}
        \AttributeTok{QOI =} \StringTok{\textquotesingle{}SATT\textquotesingle{}}\NormalTok{,}
        \AttributeTok{metric =} \StringTok{\textquotesingle{}L1\textquotesingle{}}\NormalTok{,}
        \AttributeTok{ratio =} \StringTok{\textquotesingle{}fixed\textquotesingle{}}
\NormalTok{    )}
\NormalTok{L1.frontier}

\CommentTok{\# estimate effects along the frontier}

\CommentTok{\# Set base form}
\NormalTok{my.form }\OtherTok{\textless{}{-}}
    \FunctionTok{as.formula}\NormalTok{(re78 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ treat }\SpecialCharTok{+}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ black }\SpecialCharTok{+}\NormalTok{ education }
               \SpecialCharTok{+}\NormalTok{ hispanic }\SpecialCharTok{+}\NormalTok{ married }\SpecialCharTok{+}\NormalTok{ nodegree }\SpecialCharTok{+}\NormalTok{ re74 }\SpecialCharTok{+}\NormalTok{ re75)}

\CommentTok{\# Estimate effects for the mahalanobis frontier}
\NormalTok{mahal.estimates }\OtherTok{\textless{}{-}}
    \FunctionTok{estimateEffects}\NormalTok{(}
\NormalTok{        mahal.frontier,}
        \StringTok{\textquotesingle{}re78 \textasciitilde{} treat\textquotesingle{}}\NormalTok{,}
        \AttributeTok{mod.dependence.formula =}\NormalTok{ my.form,}
        \AttributeTok{continuous.vars =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}education\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}re74\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}re75\textquotesingle{}}\NormalTok{),}
        \AttributeTok{prop.estimated =}\NormalTok{ .}\DecValTok{1}\NormalTok{,}
        \AttributeTok{means.as.cutpoints =} \ConstantTok{TRUE}
\NormalTok{    )}

\CommentTok{\# Estimate effects for the L1 frontier}
\NormalTok{L1.estimates }\OtherTok{\textless{}{-}}
    \FunctionTok{estimateEffects}\NormalTok{(}
\NormalTok{        L1.frontier,}
        \StringTok{\textquotesingle{}re78 \textasciitilde{} treat\textquotesingle{}}\NormalTok{,}
        \AttributeTok{mod.dependence.formula =}\NormalTok{ my.form,}
        \AttributeTok{continuous.vars =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}education\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}re74\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}re75\textquotesingle{}}\NormalTok{),}
        \AttributeTok{prop.estimated =}\NormalTok{ .}\DecValTok{1}\NormalTok{,}
        \AttributeTok{means.as.cutpoints =} \ConstantTok{TRUE}
\NormalTok{    )}

\CommentTok{\# Plot covariates means }
\CommentTok{\# plotPrunedMeans()}


\CommentTok{\# Plot estimates (deprecated)}
\CommentTok{\# plotEstimates(}
\CommentTok{\#     L1.estimates,}
\CommentTok{\#     ylim = c({-}10000, 3000),}
\CommentTok{\#     cex.lab = 1.4,}
\CommentTok{\#     cex.axis = 1.4,}
\CommentTok{\#     panel.first = grid(NULL, NULL, lwd = 2,)}
\CommentTok{\# )}

\CommentTok{\# Plot estimates}
\FunctionTok{plotMeans}\NormalTok{(L1.frontier)}


\CommentTok{\# parallel plot}
\FunctionTok{parallelPlot}\NormalTok{(}
\NormalTok{    L1.frontier,}
    \AttributeTok{N =} \DecValTok{400}\NormalTok{,}
    \AttributeTok{variables =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}re74\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}re75\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{),}
    \AttributeTok{treated.col =} \StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{,}
    \AttributeTok{control.col =} \StringTok{\textquotesingle{}gray\textquotesingle{}}
\NormalTok{)}

\CommentTok{\# export matched dataset}
\CommentTok{\# take 400 units}
\NormalTok{matched.data }\OtherTok{\textless{}{-}} \FunctionTok{generateDataset}\NormalTok{(L1.frontier, }\AttributeTok{N =} \DecValTok{400}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\hypertarget{propensity-scores}{%
\subsection{Propensity Scores}\label{propensity-scores}}

Even though I mention the propensity scores matching method here, it is no longer recommended to use such method in research and publication \citep{king2019propensity} because it increases

\begin{itemize}
\item
  imbalance
\item
  inefficiency
\item
  model dependence: small changes in the model specification lead to big changes in model results
\item
  bias
\end{itemize}

\citep{abadie2016matching}note

\begin{itemize}
\item
  The initial estimation of the propensity score influences the large sample distribution of the estimators.
\item
  Adjustments are made to the large sample variances of these estimators for both ATE and ATT.

  \begin{itemize}
  \item
    The adjustment for the ATE estimator is either negative or zero, indicating greater efficiency when matching on an estimated propensity score versus the true score in large samples.
  \item
    For the ATET estimator, the sign of the adjustment depends on the data generating process. Neglecting the estimation error in the propensity score can lead to inaccurate confidence intervals for the ATT estimator, making them either too large or too small.
  \end{itemize}
\end{itemize}

PSM tries to accomplish complete randomization while other methods try to achieve fully blocked. Hence, you probably better off use any other methods.

Propensity is ``the probability of receiving the treatment given the observed covariates.'' \citep{rosenbaum1985bias}

Equivalently, it can to understood as the probability of being treated.

\[
e_i (X_i) = P(T_i = 1 | X_i)
\]

Estimation using

\begin{itemize}
\item
  logistic regression
\item
  Non parametric methods:

  \begin{itemize}
  \item
    boosted CART
  \item
    generalized boosted models (gbm)
  \end{itemize}
\end{itemize}

Steps by Gary King's \href{https://www.youtube.com/watch?v=rBv39pK1iEs\&ab_channel=MethodsColloquium}{slides}

\begin{itemize}
\item
  reduce k elements of X to scalar
\item
  \(\pi_i \equiv P(T_i = 1|X) = \frac{1}{1+e^{X_i \beta}}\)
\item
  Distance (\(X_c, X_t\)) = \(|\pi_c - \pi_t|\)
\item
  match each treated unit to the nearest control unit
\item
  control units: not reused; pruned if unused
\item
  prune matches if distances \textgreater{} caliper
\end{itemize}

In the best case scenario, you randomly prune, which increases imbalance

Other methods dominate because they try to match exactly hence

\begin{itemize}
\item
  \(X_c = X_t \to \pi_c = \pi_t\) (exact match leads to equal propensity scores) but
\item
  \(\pi_c = \pi_t \nrightarrow X_c = X_t\) (equal propensity scores do not necessarily lead to exact match)
\end{itemize}

Notes:

\begin{itemize}
\item
  Do not include/control for irrelevant covariates because it leads your PSM to be more random, hence more imbalance
\item
  Do not include for \citep{bhattacharya2007instrumental} instrumental variable in the predictor set of a propensity score matching estimator. More generally, using variables that do not control for potential confounders, even if they are predictive of the treatment, can result in biased estimates
\end{itemize}

What you left with after pruning is more important than what you start with then throw out.

Diagnostics:

\begin{itemize}
\item
  balance of the covariates
\item
  no need to concern about collinearity
\item
  can't use c-stat or stepwise because those model fit stat do not apply
\end{itemize}

Application

\begin{itemize}
\item
  Finance:

  \begin{itemize}
  \tightlist
  \item
    \citet{hirtle2020impact} examine the impact of bank supervision on risk, profitability, and growth, using a matched sample approach to show that increased supervisory attention leads to less risky loan portfolios and reduced volatility without compromising profitability or growth.
  \end{itemize}
\end{itemize}

\hypertarget{look-ahead-propensity-score-matching}{%
\subsubsection{Look Ahead Propensity Score Matching}\label{look-ahead-propensity-score-matching}}

\begin{itemize}
\tightlist
\item
  \citep{bapna2018monetizing}
\end{itemize}

\hypertarget{mahalanobis-distance}{%
\subsection{Mahalanobis Distance}\label{mahalanobis-distance}}

Approximates fully blocked experiment

Distance \((X_c,X_t)\) = \(\sqrt{(X_c - X_t)'S^{-1}(X_c - X_t)}\)

where \(S^{-1}\) standardize the distance

In application we use Euclidean distance.

Prune unused control units, and prune matches if distance \textgreater{} caliper

\hypertarget{coarsened-exact-matching}{%
\subsection{Coarsened Exact Matching}\label{coarsened-exact-matching}}

Steps from Gray King's \href{https://www.youtube.com/watch?v=rBv39pK1iEs\&ab_channel=MethodsColloquium}{slides} International Methods Colloquium talk 2015

\begin{itemize}
\item
  Temporarily coarsen \(X\)
\item
  Apply exact matching to the coarsened \(X, C(X)\)

  \begin{itemize}
  \item
    sort observation into strata, each with unique values of \(C(X)\)
  \item
    prune stratum with 0 treated or 0 control units
  \end{itemize}
\item
  Pass on original (uncoarsened) units except those pruned
\end{itemize}

Properties:

\begin{itemize}
\item
  Monotonic imbalance bounding (MIB) matching method

  \begin{itemize}
  \tightlist
  \item
    maximum imbalance between the treated and control chosen ex ante
  \end{itemize}
\item
  meets congruence principle
\item
  robust to measurement error
\item
  can be implemented with multiple imputation
\item
  works well for multi-category treatments
\end{itemize}

Assumptions:

\begin{itemize}
\tightlist
\item
  Ignorability (i.e., no omitted variable bias)
\end{itemize}

More detail in \citep{iacus2012causal}

Example by \href{https://cran.r-project.org/web/packages/cem/vignettes/cem.pdf}{package's authors}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(cem)}
\FunctionTok{data}\NormalTok{(LeLonde)}

\NormalTok{Le }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\FunctionTok{na.omit}\NormalTok{(LeLonde)) }\CommentTok{\# remove missing data}
\CommentTok{\# treated and control groups}
\NormalTok{tr }\OtherTok{\textless{}{-}} \FunctionTok{which}\NormalTok{(Le}\SpecialCharTok{$}\NormalTok{treated}\SpecialCharTok{==}\DecValTok{1}\NormalTok{)}
\NormalTok{ct }\OtherTok{\textless{}{-}} \FunctionTok{which}\NormalTok{(Le}\SpecialCharTok{$}\NormalTok{treated}\SpecialCharTok{==}\DecValTok{0}\NormalTok{)}
\NormalTok{ntr }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(tr)}
\NormalTok{nct }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(ct)}

\CommentTok{\# unadjusted, biased difference in means}
\FunctionTok{mean}\NormalTok{(Le}\SpecialCharTok{$}\NormalTok{re78[tr]) }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(Le}\SpecialCharTok{$}\NormalTok{re78[ct])}
\CommentTok{\#\textgreater{} [1] 759.0479}

\CommentTok{\# pre{-}treatment covariates}
\NormalTok{vars }\OtherTok{\textless{}{-}}
    \FunctionTok{c}\NormalTok{(}
        \StringTok{"age"}\NormalTok{,}
        \StringTok{"education"}\NormalTok{,}
        \StringTok{"black"}\NormalTok{,}
        \StringTok{"married"}\NormalTok{,}
        \StringTok{"nodegree"}\NormalTok{,}
        \StringTok{"re74"}\NormalTok{,}
        \StringTok{"re75"}\NormalTok{,}
        \StringTok{"hispanic"}\NormalTok{,}
        \StringTok{"u74"}\NormalTok{,}
        \StringTok{"u75"}\NormalTok{,}
        \StringTok{"q1"}
\NormalTok{    )}

\CommentTok{\# overall imbalance statistics}
\FunctionTok{imbalance}\NormalTok{(}\AttributeTok{group=}\NormalTok{Le}\SpecialCharTok{$}\NormalTok{treated, }\AttributeTok{data=}\NormalTok{Le[vars]) }\CommentTok{\# L1 = 0.902}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Multivariate Imbalance Measure: L1=0.902}
\CommentTok{\#\textgreater{} Percentage of local common support: LCS=5.8\%}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Univariate Imbalance Measures:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}               statistic   type           L1 min 25\%      50\%       75\%}
\CommentTok{\#\textgreater{} age        {-}0.252373042 (diff) 5.102041e{-}03   0   0   0.0000   {-}1.0000}
\CommentTok{\#\textgreater{} education   0.153634710 (diff) 8.463851e{-}02   1   0   1.0000    1.0000}
\CommentTok{\#\textgreater{} black      {-}0.010322734 (diff) 1.032273e{-}02   0   0   0.0000    0.0000}
\CommentTok{\#\textgreater{} married    {-}0.009551495 (diff) 9.551495e{-}03   0   0   0.0000    0.0000}
\CommentTok{\#\textgreater{} nodegree   {-}0.081217371 (diff) 8.121737e{-}02   0  {-}1   0.0000    0.0000}
\CommentTok{\#\textgreater{} re74      {-}18.160446880 (diff) 5.551115e{-}17   0   0 284.0715  806.3452}
\CommentTok{\#\textgreater{} re75      101.501761679 (diff) 5.551115e{-}17   0   0 485.6310 1238.4114}
\CommentTok{\#\textgreater{} hispanic   {-}0.010144756 (diff) 1.014476e{-}02   0   0   0.0000    0.0000}
\CommentTok{\#\textgreater{} u74        {-}0.045582186 (diff) 4.558219e{-}02   0   0   0.0000    0.0000}
\CommentTok{\#\textgreater{} u75        {-}0.065555292 (diff) 6.555529e{-}02   0   0   0.0000    0.0000}
\CommentTok{\#\textgreater{} q1          7.494021189 (Chi2) 1.067078e{-}01  NA  NA       NA        NA}
\CommentTok{\#\textgreater{}                  max}
\CommentTok{\#\textgreater{} age          {-}6.0000}
\CommentTok{\#\textgreater{} education     1.0000}
\CommentTok{\#\textgreater{} black         0.0000}
\CommentTok{\#\textgreater{} married       0.0000}
\CommentTok{\#\textgreater{} nodegree      0.0000}
\CommentTok{\#\textgreater{} re74      {-}2139.0195}
\CommentTok{\#\textgreater{} re75        490.3945}
\CommentTok{\#\textgreater{} hispanic      0.0000}
\CommentTok{\#\textgreater{} u74           0.0000}
\CommentTok{\#\textgreater{} u75           0.0000}
\CommentTok{\#\textgreater{} q1                NA}

\CommentTok{\# drop other variables that are not pre {-} treatmentt matching variables}
\NormalTok{todrop }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"treated"}\NormalTok{, }\StringTok{"re78"}\NormalTok{)}
\FunctionTok{imbalance}\NormalTok{(}\AttributeTok{group=}\NormalTok{Le}\SpecialCharTok{$}\NormalTok{treated, }\AttributeTok{data=}\NormalTok{Le, }\AttributeTok{drop=}\NormalTok{todrop)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Multivariate Imbalance Measure: L1=0.902}
\CommentTok{\#\textgreater{} Percentage of local common support: LCS=5.8\%}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Univariate Imbalance Measures:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}               statistic   type           L1 min 25\%      50\%       75\%}
\CommentTok{\#\textgreater{} age        {-}0.252373042 (diff) 5.102041e{-}03   0   0   0.0000   {-}1.0000}
\CommentTok{\#\textgreater{} education   0.153634710 (diff) 8.463851e{-}02   1   0   1.0000    1.0000}
\CommentTok{\#\textgreater{} black      {-}0.010322734 (diff) 1.032273e{-}02   0   0   0.0000    0.0000}
\CommentTok{\#\textgreater{} married    {-}0.009551495 (diff) 9.551495e{-}03   0   0   0.0000    0.0000}
\CommentTok{\#\textgreater{} nodegree   {-}0.081217371 (diff) 8.121737e{-}02   0  {-}1   0.0000    0.0000}
\CommentTok{\#\textgreater{} re74      {-}18.160446880 (diff) 5.551115e{-}17   0   0 284.0715  806.3452}
\CommentTok{\#\textgreater{} re75      101.501761679 (diff) 5.551115e{-}17   0   0 485.6310 1238.4114}
\CommentTok{\#\textgreater{} hispanic   {-}0.010144756 (diff) 1.014476e{-}02   0   0   0.0000    0.0000}
\CommentTok{\#\textgreater{} u74        {-}0.045582186 (diff) 4.558219e{-}02   0   0   0.0000    0.0000}
\CommentTok{\#\textgreater{} u75        {-}0.065555292 (diff) 6.555529e{-}02   0   0   0.0000    0.0000}
\CommentTok{\#\textgreater{} q1          7.494021189 (Chi2) 1.067078e{-}01  NA  NA       NA        NA}
\CommentTok{\#\textgreater{}                  max}
\CommentTok{\#\textgreater{} age          {-}6.0000}
\CommentTok{\#\textgreater{} education     1.0000}
\CommentTok{\#\textgreater{} black         0.0000}
\CommentTok{\#\textgreater{} married       0.0000}
\CommentTok{\#\textgreater{} nodegree      0.0000}
\CommentTok{\#\textgreater{} re74      {-}2139.0195}
\CommentTok{\#\textgreater{} re75        490.3945}
\CommentTok{\#\textgreater{} hispanic      0.0000}
\CommentTok{\#\textgreater{} u74           0.0000}
\CommentTok{\#\textgreater{} u75           0.0000}
\CommentTok{\#\textgreater{} q1                NA}
\end{Highlighting}
\end{Shaded}

automated coarsening

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mat }\OtherTok{\textless{}{-}}
    \FunctionTok{cem}\NormalTok{(}
        \AttributeTok{treatment =} \StringTok{"treated"}\NormalTok{,}
        \AttributeTok{data =}\NormalTok{ Le,}
        \AttributeTok{drop =} \StringTok{"re78"}\NormalTok{,}
        \AttributeTok{keep.all =} \ConstantTok{TRUE}
\NormalTok{    )}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Using \textquotesingle{}treated\textquotesingle{}=\textquotesingle{}1\textquotesingle{} as baseline group}
\NormalTok{mat}
\CommentTok{\#\textgreater{}            G0  G1}
\CommentTok{\#\textgreater{} All       392 258}
\CommentTok{\#\textgreater{} Matched    95  84}
\CommentTok{\#\textgreater{} Unmatched 297 174}

\CommentTok{\# mat$w}
\end{Highlighting}
\end{Shaded}

coarsening by explicit user choice

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# categorial variables}
\FunctionTok{levels}\NormalTok{(Le}\SpecialCharTok{$}\NormalTok{q1) }\CommentTok{\# grouping option}
\CommentTok{\#\textgreater{} [1] "agree"             "disagree"          "neutral"          }
\CommentTok{\#\textgreater{} [4] "no opinion"        "strongly agree"    "strongly disagree"}
\NormalTok{q1.grp }\OtherTok{\textless{}{-}}
    \FunctionTok{list}\NormalTok{(}
        \FunctionTok{c}\NormalTok{(}\StringTok{"strongly agree"}\NormalTok{, }\StringTok{"agree"}\NormalTok{),}
        \FunctionTok{c}\NormalTok{(}\StringTok{"neutral"}\NormalTok{, }\StringTok{"no opinion"}\NormalTok{),}
        \FunctionTok{c}\NormalTok{(}\StringTok{"strongly disagree"}\NormalTok{, }\StringTok{"disagree"}\NormalTok{)}
\NormalTok{    ) }\CommentTok{\# if you want ordered categories}

\CommentTok{\# continuous variables}
\FunctionTok{table}\NormalTok{(Le}\SpecialCharTok{$}\NormalTok{education)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   3   4   5   6   7   8   9  10  11  12  13  14  15 }
\CommentTok{\#\textgreater{}   1   5   4   6  12  55 106 146 173 113  19   9   1}
\NormalTok{educut }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{6.5}\NormalTok{, }\FloatTok{8.5}\NormalTok{, }\FloatTok{12.5}\NormalTok{, }\DecValTok{17}\NormalTok{)  }\CommentTok{\# use cutpoints}

\NormalTok{mat1 }\OtherTok{\textless{}{-}}
    \FunctionTok{cem}\NormalTok{(}
        \AttributeTok{treatment =} \StringTok{"treated"}\NormalTok{,}
        \AttributeTok{data =}\NormalTok{ Le,}
        \AttributeTok{drop =} \StringTok{"re78"}\NormalTok{,}
        \AttributeTok{cutpoints =} \FunctionTok{list}\NormalTok{(}\AttributeTok{education =}\NormalTok{ educut),}
        \AttributeTok{grouping =} \FunctionTok{list}\NormalTok{(}\AttributeTok{q1 =}\NormalTok{ q1.grp)}
\NormalTok{    )}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Using \textquotesingle{}treated\textquotesingle{}=\textquotesingle{}1\textquotesingle{} as baseline group}
\NormalTok{mat1}
\CommentTok{\#\textgreater{}            G0  G1}
\CommentTok{\#\textgreater{} All       392 258}
\CommentTok{\#\textgreater{} Matched   158 115}
\CommentTok{\#\textgreater{} Unmatched 234 143}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  Can also use progressive coarsening method to control the number of matches.
\item
  \texttt{cem} can also handle some missingness.
\end{itemize}

\hypertarget{genetic-matching}{%
\subsection{Genetic Matching}\label{genetic-matching}}

\begin{itemize}
\item
  GM uses iterative checking process of propensity scores, which combines propensity scores and Mahalanobis distance.

  \begin{itemize}
  \tightlist
  \item
    GenMatch \citep{diamond2013genetic}
  \end{itemize}
\item
  GM is arguably ``superior'' method than nearest neighbor or full matching in imbalanced data
\item
  Use a genetic search algorithm to find weights for each covariate such that we have optimal balance.
\item
  Implementation

  \begin{itemize}
  \item
    could use \emph{with replacement}
  \item
    balance can be based on

    \begin{itemize}
    \item
      paired \(t\)-tests (dichotomous variables)
    \item
      Kolmogorov-Smirnov (multinomial and continuous)
    \end{itemize}
  \end{itemize}
\end{itemize}

Packages

\texttt{Matching}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(Matching)}
\FunctionTok{data}\NormalTok{(lalonde)}
\FunctionTok{attach}\NormalTok{(lalonde)}

\CommentTok{\#The covariates we want to match on}
\NormalTok{X }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(age, educ, black, hisp, married, nodegr, u74, u75, re75, re74)}

\CommentTok{\#The covariates we want to obtain balance on}
\NormalTok{BalanceMat }\OtherTok{\textless{}{-}}
    \FunctionTok{cbind}\NormalTok{(age,}
\NormalTok{          educ,}
\NormalTok{          black,}
\NormalTok{          hisp,}
\NormalTok{          married,}
\NormalTok{          nodegr,}
\NormalTok{          u74,}
\NormalTok{          u75,}
\NormalTok{          re75,}
\NormalTok{          re74,}
          \FunctionTok{I}\NormalTok{(re74 }\SpecialCharTok{*}\NormalTok{ re75))}

\CommentTok{\#}
\CommentTok{\#Let\textquotesingle{}s call GenMatch() to find the optimal weight to give each}
\CommentTok{\#covariate in \textquotesingle{}X\textquotesingle{} so as we have achieved balance on the covariates in}
\CommentTok{\#\textquotesingle{}BalanceMat\textquotesingle{}. This is only an example so we want GenMatch to be quick}
\CommentTok{\#so the population size has been set to be only 16 via the \textquotesingle{}pop.size\textquotesingle{}}
\CommentTok{\#option. This is *WAY* too small for actual problems.}
\CommentTok{\#For details see http://sekhon.berkeley.edu/papers/MatchingJSS.pdf.}
\CommentTok{\#}
\NormalTok{genout }\OtherTok{\textless{}{-}}
    \FunctionTok{GenMatch}\NormalTok{(}
        \AttributeTok{Tr =}\NormalTok{ treat,}
        \AttributeTok{X =}\NormalTok{ X,}
        \AttributeTok{BalanceMatrix =}\NormalTok{ BalanceMat,}
        \AttributeTok{estimand =} \StringTok{"ATE"}\NormalTok{,}
        \AttributeTok{M =} \DecValTok{1}\NormalTok{,}
        \AttributeTok{pop.size =} \DecValTok{16}\NormalTok{,}
        \AttributeTok{max.generations =} \DecValTok{10}\NormalTok{,}
        \AttributeTok{wait.generations =} \DecValTok{1}
\NormalTok{    )}

\CommentTok{\#The outcome variable}
\NormalTok{Y}\OtherTok{=}\NormalTok{re78}\SpecialCharTok{/}\DecValTok{1000}

\CommentTok{\#}
\CommentTok{\# Now that GenMatch() has found the optimal weights, let\textquotesingle{}s estimate}
\CommentTok{\# our causal effect of interest using those weights}
\CommentTok{\#}
\NormalTok{mout }\OtherTok{\textless{}{-}}
    \FunctionTok{Match}\NormalTok{(}
        \AttributeTok{Y =}\NormalTok{ Y,}
        \AttributeTok{Tr =}\NormalTok{ treat,}
        \AttributeTok{X =}\NormalTok{ X,}
        \AttributeTok{estimand =} \StringTok{"ATE"}\NormalTok{,}
        \AttributeTok{Weight.matrix =}\NormalTok{ genout}
\NormalTok{    )}
\FunctionTok{summary}\NormalTok{(mout)}

\CommentTok{\#                        }
\CommentTok{\#Let\textquotesingle{}s determine if balance has actually been obtained on the variables of interest}
\CommentTok{\#                        }
\NormalTok{mb }\OtherTok{\textless{}{-}}
    \FunctionTok{MatchBalance}\NormalTok{(}
\NormalTok{        treat }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ educ }\SpecialCharTok{+}\NormalTok{ black }\SpecialCharTok{+}\NormalTok{ hisp }\SpecialCharTok{+}\NormalTok{ married }\SpecialCharTok{+}\NormalTok{ nodegr }
        \SpecialCharTok{+}\NormalTok{ u74 }\SpecialCharTok{+}\NormalTok{ u75 }\SpecialCharTok{+}\NormalTok{ re75 }\SpecialCharTok{+}\NormalTok{ re74 }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(re74 }\SpecialCharTok{*}\NormalTok{ re75),}
        \AttributeTok{match.out =}\NormalTok{ mout,}
        \AttributeTok{nboots =} \DecValTok{500}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\hypertarget{entropy-balancing}{%
\subsection{Entropy Balancing}\label{entropy-balancing}}

\citep{hainmueller2012entropy}

\begin{itemize}
\item
  Entropy balancing is a method for achieving covariate balance in observational studies with binary treatments.
\item
  It uses a maximum entropy reweighting scheme to ensure that treatment and control groups are balanced based on sample moments.
\item
  This method adjusts for inequalities in the covariate distributions, reducing dependence on the model used for estimating treatment effects.
\item
  Entropy balancing improves balance across all included covariate moments and removes the need for repetitive balance checking and iterative model searching.
\end{itemize}

\hypertarget{matching-for-high-dimensional-data}{%
\subsection{Matching for high-dimensional data}\label{matching-for-high-dimensional-data}}

One could reduce the number of dimensions using methods such as:

\begin{itemize}
\item
  Lasso \citep{gordon2019comparison}
\item
  Penalized logistic regression \citep{eckles2021bias}
\item
  PCA (Principal Component Analysis)
\item
  Locality Preserving Projections (LPP) \citep{li2016matching}
\item
  Random projection
\item
  Autoencoders \citep{ramachandra2018deep}
\end{itemize}

Additionally, one could jointly does dimension reduction while balancing the distributions of the control and treated groups \citep{yao2018representation}.

\hypertarget{matching-for-time-series-cross-section-data}{%
\subsection{Matching for time series-cross-section data}\label{matching-for-time-series-cross-section-data}}

Examples: \citep{scheve2012democracy} and \citep{acemoglu2019democracy}

Identification strategy:

\begin{itemize}
\item
  Within-unit over-time variation
\item
  within-time across-units variation
\end{itemize}

See \protect\hyperlink{did-with-in-and-out-treatment-condition}{DID with in and out treatment condition} for details of this method

\hypertarget{matching-for-multiple-treatments}{%
\subsection{Matching for multiple treatments}\label{matching-for-multiple-treatments}}

In cases where you have multiple treatment groups, and you want to do matching, it's important to have the same baseline (control) group. For more details, see

\begin{itemize}
\item
  \citep{mccaffrey2013tutorial}
\item
  \citep{lopez2017estimation}
\item
  \citep{zhao2021propensity}: also for continuous treatment
\end{itemize}

If you insist on using the \texttt{MatchIt} package, then see this \href{https://stats.stackexchange.com/questions/405019/matching-with-multiple-treatments}{answer}

\hypertarget{matching-for-multi-level-treatments}{%
\subsection{Matching for multi-level treatments}\label{matching-for-multi-level-treatments}}

See \citep{yang2016propensity}

Package in R \texttt{shuyang1987/multilevelMatching} on Github

\hypertarget{matching-for-repeated-treatments}{%
\subsection{Matching for repeated treatments}\label{matching-for-repeated-treatments}}

\url{https://cran.r-project.org/web/packages/twang/vignettes/iptw.pdf}

package in R \texttt{twang}

\hypertarget{selection-on-unobservables}{%
\section{Selection on Unobservables}\label{selection-on-unobservables}}

There are several ways one can deal with selection on unobservables:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \protect\hyperlink{rosenbaum-bounds}{Rosenbaum Bounds}
\item
  \protect\hyperlink{endogenous-sample-selection}{Endogenous Sample Selection} (i.e., Heckman-style correction): examine the \(\lambda\) term to see whether it's significant (sign of endogenous selection)
\item
  \protect\hyperlink{relative-correlation-restrictions}{Relative Correlation Restrictions}
\item
  \protect\hyperlink{coefficient-stability-bounds}{Coefficient-stability Bounds}
\end{enumerate}

\hypertarget{rosenbaum-bounds}{%
\subsection{Rosenbaum Bounds}\label{rosenbaum-bounds}}

Examples in marketing

\begin{itemize}
\item
  \citep{oestreicher2013content}: A range of 1.5 to 1.8 is important for the effect of the level of community participation of users on their willingness to pay for premium services.
\item
  \citep{sun2013ad}: A factor of 1.5 is essential for understanding the relationship between the launch of an ad revenue-sharing program and the popularity of content.
\item
  \citep{manchanda2015social}: A factor of 1.6 is required for the social dollar effect to be nullified.
\item
  \citep{sudhir2015peter}: A factor of 1.9 is needed for IT adoption to impact labor productivity, and 2.2 for IT adoption to affect floor productivity.
\item
  \citep{proserpio2017}: A factor of 2 is necessary for the firm's use of management responses to influence online reputation.
\item
  \citep{zhang2022makes}: A factor of 1.55 is critical for the acquisition of verified images to drive demand for Airbnb properties.
\item
  \citep{chae2023paywall}: A factor of 27 (not a typo) is significant in how paywall suspensions affect subsequent subscription decisions.
\end{itemize}

General

\begin{itemize}
\item
  \protect\hyperlink{matching-methods}{Matching Methods} are favored for estimating treatment effects in observational data, offering advantages over regression methods because

  \begin{itemize}
  \item
    It reduces reliance on functional form assumptions.
  \item
    Assumes all selection-influencing covariates are observable; estimates are unbiased if no unobserved confounders are missed.
  \end{itemize}
\item
  Concerns arise when potentially relevant covariates are unmeasured.

  \begin{itemize}
  \tightlist
  \item
    \href{examine\%20departure\%20from\%20assumption\%20of\%20free\%20hidden\%20bias\%20due\%20to\%20unobservables.}{Rosenbaum Bounds} assess the overall sensitivity of coefficient estimates to hidden bias \citep{rosenbaum2002overt} without having knowledge (e.g., direction) of the bias. Because the unboservables that cause hidden bias have to both affect selection into treatment by a factor of \(\Gamma\) and predictive of outcome, this method is also known as \textbf{worst case analyses} \citep{diprete2004assessing}.
  \end{itemize}
\item
  Can't provide precise bounds on estimates of treatment effects (see \protect\hyperlink{relative-correlation-restrictions}{Relative Correlation Restrictions})
\item
  Typically, we show both p-value and H-L point estimate for each level of gamma \(\Gamma\)
\end{itemize}

With random treatment assignment, we can use the non-parametric test (Wilcoxon signed rank test) to see if there is treatment effect.

Without random treatment assignment (i.e., observational data), we cannot use this test. With \protect\hyperlink{selection-on-observables}{Selection on Observables}, we can use this test if we believe there are no unmeasured confounders. And this is where \citet{rosenbaum2002attributing} can come in to talk about the believability of this notion.

In layman's terms, consider that the treatment assignment is based on a method where the odds of treatment for a unit and its control differ by a multiplier \(\Gamma\)

\begin{itemize}
\tightlist
\item
  For example, \(\Gamma = 1\) means that the odds of assignment are identical, indicating random treatment assignment.
\item
  Another example, \(\Gamma = 2\), in the same matched pair, one unit is twice as likely to receive the treatment (due to unobservables).
\item
  Since we can't know \(\Gamma\) with certainty, we run sensitivity analysis to see if the results change with different values of \(\Gamma\)
\item
  This bias is the product of an unobservable that influences both treatment selection and outcome by a factor \(\Gamma\) (omitted variable bias)
\end{itemize}

In technical terms,

\begin{itemize}
\tightlist
\item
  \textbf{Treatment Assignment and Probability}:

  \begin{itemize}
  \tightlist
  \item
    Consider unit \(j\) with a probability \(\pi_j\) of receiving the treatment, and unit \(i\) with \(\pi_i\).
  \item
    Ideally, after matching, if there's no hidden bias, we'd have \(\pi_i = \pi_j\).
  \item
    However, observing \(\pi_i \neq \pi_j\) raises questions about potential biases affecting our inference. This is evaluated using the odds ratio.
  \end{itemize}
\item
  \textbf{Odds Ratio and Hidden Bias}:

  \begin{itemize}
  \tightlist
  \item
    The odds of treatment for a unit \(j\) is defined as \(\frac{\pi_j}{1 - \pi_j}\).
  \item
    The odds ratio between two matched units \(i\) and \(j\) is constrained by \(\frac{1}{\Gamma} \le \frac{\pi_i / (1- \pi_i)}{\pi_j/ (1- \pi_j)} \le \Gamma\).

    \begin{itemize}
    \tightlist
    \item
      If \(\Gamma = 1\), it implies an absence of hidden bias.
    \item
      If \(\Gamma = 2\), the odds of receiving treatment could differ by up to a factor of 2 between the two units.
    \end{itemize}
  \end{itemize}
\item
  \textbf{Sensitivity Analysis Using Gamma}:

  \begin{itemize}
  \tightlist
  \item
    The value of \(\Gamma\) helps measure the potential departure from a bias-free study.
  \item
    Sensitivity analysis involves varying \(\Gamma\) to examine how inferences might change with the presence of hidden biases.
  \end{itemize}
\item
  \textbf{Incorporating Unobserved Covariates}:

  \begin{itemize}
  \tightlist
  \item
    Consider a scenario where unit \(i\) has observed covariates \(x_i\) and an unobserved covariate \(u_i\), that both affect the outcome.
  \item
    A logistic regression model could link the odds of assignment to these covariates: \(\log(\frac{\pi_i}{1 - \pi_i}) = \kappa x_i + \gamma u_i\), where \(\gamma\) represents the impact of the unobserved covariate.
  \end{itemize}
\item
  \textbf{Steps for Sensitivity Analysis} (We could create a table of different levels of \(\Gamma\) to assess how the magnitude of biases can affect our evidence of the treatment effect (estimate):

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    Select a range of values for \(\Gamma\) (e.g., \(1 \to 2\)).
  \item
    Assess how the p-value or the magnitude of the treatment effect \citep{hodges2011estimates} (for more details, see \citep{hollander2013nonparametric}) changes with varying \(\Gamma\) values.
  \item
    Employ specific randomization tests based on the type of outcome to establish bounds on inferences.

    \begin{itemize}
    \tightlist
    \item
      report the minimum value of \(\Gamma\) at which the treatment treat is nullified (i.e., become insignificant). And the literature's rules of thumb is that if \(\Gamma > 2\), then we have strong evidence for our treatment effect is robust to large biases \citep{proserpio2017online}
    \end{itemize}
  \end{enumerate}
\end{itemize}

Notes:

\begin{itemize}
\tightlist
\item
  If we have treatment assignment is clustered (e.g., within school, within state) we need to adjust the bounds for clustered treatment assignment \citep{hansen2014clustered} (similar to clustered standard errors).
\end{itemize}

Packages

\begin{itemize}
\item
  \texttt{rbounds} \citep{keele2010overview}
\item
  \texttt{sensitivitymv} \citep{rosenbaum2015two}
\end{itemize}

Since we typically assess our estimate sensitivity to unboservables after matching, we first do some matching.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(MatchIt)}
\FunctionTok{library}\NormalTok{(Matching)}
\FunctionTok{data}\NormalTok{(}\StringTok{"lalonde"}\NormalTok{)}

\NormalTok{matched }\OtherTok{\textless{}{-}}\NormalTok{ MatchIt}\SpecialCharTok{::}\FunctionTok{matchit}\NormalTok{(}
\NormalTok{    treat }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ educ,}
    \AttributeTok{data =}\NormalTok{ lalonde,}
    \AttributeTok{method =} \StringTok{"nearest"}
\NormalTok{)}
\FunctionTok{summary}\NormalTok{(matched)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} MatchIt::matchit(formula = treat \textasciitilde{} age + educ, data = lalonde, }
\CommentTok{\#\textgreater{}     method = "nearest")}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Summary of Balance for All Data:}
\CommentTok{\#\textgreater{}          Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean}
\CommentTok{\#\textgreater{} distance        0.4203        0.4125          0.1689     1.2900    0.0431}
\CommentTok{\#\textgreater{} age            25.8162       25.0538          0.1066     1.0278    0.0254}
\CommentTok{\#\textgreater{} educ           10.3459       10.0885          0.1281     1.5513    0.0287}
\CommentTok{\#\textgreater{}          eCDF Max}
\CommentTok{\#\textgreater{} distance   0.1251}
\CommentTok{\#\textgreater{} age        0.0652}
\CommentTok{\#\textgreater{} educ       0.1265}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Summary of Balance for Matched Data:}
\CommentTok{\#\textgreater{}          Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean}
\CommentTok{\#\textgreater{} distance        0.4203        0.4179          0.0520     1.1691    0.0105}
\CommentTok{\#\textgreater{} age            25.8162       25.5081          0.0431     1.1518    0.0148}
\CommentTok{\#\textgreater{} educ           10.3459       10.2811          0.0323     1.5138    0.0224}
\CommentTok{\#\textgreater{}          eCDF Max Std. Pair Dist.}
\CommentTok{\#\textgreater{} distance   0.0595          0.0598}
\CommentTok{\#\textgreater{} age        0.0486          0.5628}
\CommentTok{\#\textgreater{} educ       0.0757          0.3602}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Sample Sizes:}
\CommentTok{\#\textgreater{}           Control Treated}
\CommentTok{\#\textgreater{} All           260     185}
\CommentTok{\#\textgreater{} Matched       185     185}
\CommentTok{\#\textgreater{} Unmatched      75       0}
\CommentTok{\#\textgreater{} Discarded       0       0}
\NormalTok{matched\_data }\OtherTok{\textless{}{-}} \FunctionTok{match.data}\NormalTok{(matched)}

\NormalTok{treatment\_group }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(matched\_data, treat }\SpecialCharTok{==} \DecValTok{1}\NormalTok{)}
\NormalTok{control\_group }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(matched\_data, treat }\SpecialCharTok{==} \DecValTok{0}\NormalTok{)}


\FunctionTok{library}\NormalTok{(rbounds)}

\CommentTok{\# p{-}value sensitivity }
\NormalTok{psens\_res }\OtherTok{\textless{}{-}}
    \FunctionTok{psens}\NormalTok{(treatment\_group}\SpecialCharTok{$}\NormalTok{re78,}
\NormalTok{          control\_group}\SpecialCharTok{$}\NormalTok{re78,}
          \AttributeTok{Gamma =} \DecValTok{2}\NormalTok{,}
          \AttributeTok{GammaInc =}\NormalTok{ .}\DecValTok{1}\NormalTok{)}

\NormalTok{psens\_res}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Rosenbaum Sensitivity Test for Wilcoxon Signed Rank P{-}Value }
\CommentTok{\#\textgreater{}  }
\CommentTok{\#\textgreater{} Unconfounded estimate ....  0.0058 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Gamma Lower bound Upper bound}
\CommentTok{\#\textgreater{}    1.0      0.0058      0.0058}
\CommentTok{\#\textgreater{}    1.1      0.0011      0.0235}
\CommentTok{\#\textgreater{}    1.2      0.0002      0.0668}
\CommentTok{\#\textgreater{}    1.3      0.0000      0.1458}
\CommentTok{\#\textgreater{}    1.4      0.0000      0.2599}
\CommentTok{\#\textgreater{}    1.5      0.0000      0.3967}
\CommentTok{\#\textgreater{}    1.6      0.0000      0.5378}
\CommentTok{\#\textgreater{}    1.7      0.0000      0.6664}
\CommentTok{\#\textgreater{}    1.8      0.0000      0.7723}
\CommentTok{\#\textgreater{}    1.9      0.0000      0.8523}
\CommentTok{\#\textgreater{}    2.0      0.0000      0.9085}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Note: Gamma is Odds of Differential Assignment To}
\CommentTok{\#\textgreater{}  Treatment Due to Unobserved Factors }
\CommentTok{\#\textgreater{} }

\CommentTok{\# Hodges{-}Lehmann point estimate sensitivity}
\CommentTok{\# median difference between treatment and control}
\NormalTok{hlsens\_res }\OtherTok{\textless{}{-}}
    \FunctionTok{hlsens}\NormalTok{(treatment\_group}\SpecialCharTok{$}\NormalTok{re78,}
\NormalTok{           control\_group}\SpecialCharTok{$}\NormalTok{re78,}
           \AttributeTok{Gamma =} \DecValTok{2}\NormalTok{,}
           \AttributeTok{GammaInc =}\NormalTok{ .}\DecValTok{1}\NormalTok{)}
\NormalTok{hlsens\_res}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Rosenbaum Sensitivity Test for Hodges{-}Lehmann Point Estimate }
\CommentTok{\#\textgreater{}  }
\CommentTok{\#\textgreater{} Unconfounded estimate ....  1745.843 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Gamma Lower bound Upper bound}
\CommentTok{\#\textgreater{}    1.0 1745.800000      1745.8}
\CommentTok{\#\textgreater{}    1.1 1139.100000      1865.6}
\CommentTok{\#\textgreater{}    1.2  830.840000      2160.9}
\CommentTok{\#\textgreater{}    1.3  533.740000      2462.4}
\CommentTok{\#\textgreater{}    1.4  259.940000      2793.8}
\CommentTok{\#\textgreater{}    1.5   {-}0.056912      3059.3}
\CommentTok{\#\textgreater{}    1.6 {-}144.960000      3297.8}
\CommentTok{\#\textgreater{}    1.7 {-}380.560000      3535.7}
\CommentTok{\#\textgreater{}    1.8 {-}554.360000      3751.0}
\CommentTok{\#\textgreater{}    1.9 {-}716.360000      4012.1}
\CommentTok{\#\textgreater{}    2.0 {-}918.760000      4224.3}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Note: Gamma is Odds of Differential Assignment To}
\CommentTok{\#\textgreater{}  Treatment Due to Unobserved Factors }
\CommentTok{\#\textgreater{} }
\end{Highlighting}
\end{Shaded}

For multiple control group matching

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(Matching)}
\FunctionTok{library}\NormalTok{(MatchIt)}

\NormalTok{n\_ratio }\OtherTok{\textless{}{-}} \DecValTok{2}
\NormalTok{matched }\OtherTok{\textless{}{-}}\NormalTok{ MatchIt}\SpecialCharTok{::}\FunctionTok{matchit}\NormalTok{(treat }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ educ ,}
                   \AttributeTok{method =} \StringTok{"nearest"}\NormalTok{, }\AttributeTok{ratio =}\NormalTok{ n\_ratio)}
\FunctionTok{summary}\NormalTok{(matched)}
\NormalTok{matched\_data }\OtherTok{\textless{}{-}} \FunctionTok{match.data}\NormalTok{(matched)}

\NormalTok{mcontrol\_res }\OtherTok{\textless{}{-}}\NormalTok{ rbounds}\SpecialCharTok{::}\FunctionTok{mcontrol}\NormalTok{(}
    \AttributeTok{y          =}\NormalTok{ matched\_data}\SpecialCharTok{$}\NormalTok{re78,}
    \AttributeTok{grp.id     =}\NormalTok{ matched\_data}\SpecialCharTok{$}\NormalTok{subclass,}
    \AttributeTok{treat.id   =}\NormalTok{ matched\_data}\SpecialCharTok{$}\NormalTok{treat,}
    \AttributeTok{group.size =}\NormalTok{ n\_ratio }\SpecialCharTok{+} \DecValTok{1}\NormalTok{,}
    \AttributeTok{Gamma      =} \FloatTok{2.5}\NormalTok{,}
    \AttributeTok{GammaInc   =}\NormalTok{ .}\DecValTok{1}
\NormalTok{)}

\NormalTok{mcontrol\_res}
\end{Highlighting}
\end{Shaded}

\texttt{sensitivitymw} is faster than \texttt{sensitivitymw}. But \texttt{sensitivitymw} can match where matched sets can have differing numbers of controls \citep{rosenbaum2015two}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(sensitivitymv)}
\FunctionTok{data}\NormalTok{(lead150)}
\FunctionTok{head}\NormalTok{(lead150)}
\CommentTok{\#\textgreater{}      [,1] [,2] [,3] [,4] [,5] [,6]}
\CommentTok{\#\textgreater{} [1,] 1.40 1.23 2.24 0.96 1.90 1.14}
\CommentTok{\#\textgreater{} [2,] 0.63 0.99 0.87 1.90 0.67 1.40}
\CommentTok{\#\textgreater{} [3,] 1.98 0.82 0.66 0.58 1.00 1.30}
\CommentTok{\#\textgreater{} [4,] 1.45 0.53 1.43 1.70 0.85 1.50}
\CommentTok{\#\textgreater{} [5,] 1.60 1.70 0.63 1.05 1.08 0.92}
\CommentTok{\#\textgreater{} [6,] 1.13 0.31 0.71 1.10 0.86 1.14}
\FunctionTok{senmv}\NormalTok{(lead150,}\AttributeTok{gamma=}\DecValTok{2}\NormalTok{,}\AttributeTok{trim=}\DecValTok{2}\NormalTok{)}
\CommentTok{\#\textgreater{} $pval}
\CommentTok{\#\textgreater{} [1] 0.02665519}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $deviate}
\CommentTok{\#\textgreater{} [1] 1.932398}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $statistic}
\CommentTok{\#\textgreater{} [1] 27.97564}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $expectation}
\CommentTok{\#\textgreater{} [1] 18.0064}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $variance}
\CommentTok{\#\textgreater{} [1] 26.61524}

\FunctionTok{library}\NormalTok{(sensitivitymw)}
\FunctionTok{senmw}\NormalTok{(lead150,}\AttributeTok{gamma=}\DecValTok{2}\NormalTok{,}\AttributeTok{trim=}\DecValTok{2}\NormalTok{)}
\CommentTok{\#\textgreater{} $pval}
\CommentTok{\#\textgreater{} [1] 0.02665519}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $deviate}
\CommentTok{\#\textgreater{} [1] 1.932398}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $statistic}
\CommentTok{\#\textgreater{} [1] 27.97564}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $expectation}
\CommentTok{\#\textgreater{} [1] 18.0064}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $variance}
\CommentTok{\#\textgreater{} [1] 26.61524}
\end{Highlighting}
\end{Shaded}

\hypertarget{relative-correlation-restrictions}{%
\subsection{Relative Correlation Restrictions}\label{relative-correlation-restrictions}}

Examples in marketing

\begin{itemize}
\item
  \citep{manchanda2015social}: 3.23 for social dollar effect to be nullified
\item
  \citep{chae2023paywall}: 6.69 (i.e., how much stronger the selection on unobservables has to be compared to the selection on observables to negate the result) for paywall suspensions affect subsequent subscription decisions
\item
  \citep{sun2013ad}
\end{itemize}

General

\begin{itemize}
\item
  Proposed by \citet{altonji2005selection}
\item
  Generalized by \citet{krauth2016bounding}
\item
  Estimate bounds of the treatment effects due to unobserved selection.
\end{itemize}

\[
Y_i = X_i \beta  + C_i \gamma + \epsilon_i
\]

where

\begin{itemize}
\item
  \(\beta\) is the effect of interest
\item
  \(C_i\) is the control variable
\item
  Using OLS, \(cor(X_i, \epsilon_i) = 0\)
\end{itemize}

Under RCR analysis, we assume

\[
cor(X_i, \epsilon_i) = \lambda cor(X_i, C_i \gamma)
\]

where \(\lambda \in (\lambda_l, \lambda_h)\)

Choice of \(\lambda\)

\begin{itemize}
\item
  Strong assumption of no omitted variable bias (small
\item
  If \(\lambda = 0\), then \(cor(X_i, \epsilon_i) = 0\)
\item
  If \(\lambda = 1\), then \(cor(X_i, \epsilon_i) = cor(X_i, C_i \gamma)\)
\item
  We typically examine \(\lambda \in (0, 1)\)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# remotes::install\_github("bvkrauth/rcr/r/rcrbounds")}
\FunctionTok{library}\NormalTok{(rcrbounds)}
\CommentTok{\# rcrbounds::install\_rcrpy()}
\FunctionTok{data}\NormalTok{(}\StringTok{"ChickWeight"}\NormalTok{)}

\NormalTok{rcr\_res }\OtherTok{\textless{}{-}}
\NormalTok{    rcrbounds}\SpecialCharTok{::}\FunctionTok{rcr}\NormalTok{(weight }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Time }\SpecialCharTok{|}
\NormalTok{                       Diet, ChickWeight, }\AttributeTok{rc\_range =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{))}
\NormalTok{rcr\_res}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} rcrbounds::rcr(formula = weight \textasciitilde{} Time | Diet, data = ChickWeight, }
\CommentTok{\#\textgreater{}     rc\_range = c(0, 10))}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}     rcInf effectInf       rc0   effectL   effectH }
\CommentTok{\#\textgreater{} 34.676505 71.989336 34.741955  7.447713  8.750492}
\FunctionTok{summary}\NormalTok{(rcr\_res)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} rcrbounds::rcr(formula = weight \textasciitilde{} Time | Diet, data = ChickWeight, }
\CommentTok{\#\textgreater{}     rc\_range = c(0, 10))}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}            Estimate  Std. Error    t value      Pr(\textgreater{}|t|)}
\CommentTok{\#\textgreater{} rcInf     34.676505  50.1295005  0.6917385  4.891016e{-}01}
\CommentTok{\#\textgreater{} effectInf 71.989336 112.5711682  0.6395007  5.224973e{-}01}
\CommentTok{\#\textgreater{} rc0       34.741955  58.7169195  0.5916856  5.540611e{-}01}
\CommentTok{\#\textgreater{} effectL    7.447713   2.4276246  3.0679014  2.155677e{-}03}
\CommentTok{\#\textgreater{} effectH    8.750492   0.2607671 33.5567355 7.180405e{-}247}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} conservative confidence interval:}
\CommentTok{\#\textgreater{}          2.5  \%  97.5  \%}
\CommentTok{\#\textgreater{} effect 2.689656 9.261586}

\CommentTok{\# hypothesis test for the coefficient}
\NormalTok{rcrbounds}\SpecialCharTok{::}\FunctionTok{effect\_test}\NormalTok{(rcr\_res, }\AttributeTok{h0 =} \DecValTok{0}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 0.001234233}
\FunctionTok{plot}\NormalTok{(rcr\_res)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{31-matching-methods_files/figure-latex/unnamed-chunk-23-1} \end{center}

\hypertarget{coefficient-stability-bounds}{%
\subsection{Coefficient-stability Bounds}\label{coefficient-stability-bounds}}

\begin{itemize}
\tightlist
\item
  Developed by \citet{oster2019unobservable}
\item
  Assess robustness to omitted variable bias by observing:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \item
    Changes in the coefficient of interest
  \item
    Shifts in model \(R^2\)
  \end{enumerate}
\item
  Refer \citet{masten2022effect} for reverse sign problem.
\end{itemize}

\hypertarget{interrupted-time-series}{%
\chapter{Interrupted Time Series}\label{interrupted-time-series}}

\begin{itemize}
\item
  Regression Discontinuity in Time
\item
  Control for

  \begin{itemize}
  \item
    Seasonable trends
  \item
    Concurrent events
  \end{itemize}
\item
  Pros \citep{penfold2013use}

  \begin{itemize}
  \tightlist
  \item
    control for long-term trends
  \end{itemize}
\item
  Cons

  \begin{itemize}
  \item
    Min of 8 data points before and 8 after an intervention
  \item
    Multiple events hard to distinguish
  \end{itemize}
\end{itemize}

Notes:

\begin{itemize}
\tightlist
\item
  For subgroup analysis (heterogeneity in effect size), see \citep{harper2017did}
\item
  To interpret with control variables, see \citep{bottomley2019analysing}
\end{itemize}

Interrupted time series should be used when

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  longitudinal data (outcome over time - observations before and after the intervention)
\item
  full population was affected at one specific point in time (or can be stacked based on intervention)
\end{enumerate}

In each ITS framework, there can be 4 possible scenarios of outcome after an intervention

\begin{itemize}
\item
  No effects
\item
  Immediate effect
\item
  Sustained (long-term) effect (smooth)
\item
  Both immediate and sustained effect
\end{itemize}

\[
Y = \beta_0 + \beta_1 T + \beta_2 D + \beta_3 P + \epsilon
\]

where

\begin{itemize}
\item
  \(Y\) is the outcome variable

  \begin{itemize}
  \tightlist
  \item
    \(\beta_0\) is the baseline level of the outcome
  \end{itemize}
\item
  \(T\) is the time variable (e.g., days, weeks, etc.) passed from the start of the observation period

  \begin{itemize}
  \tightlist
  \item
    \(\beta_1\) is the slope of the line before the intervention
  \end{itemize}
\item
  \(D\) is the treatment variable where \(1\) is after the intervention and \(0\) is before the intervention.

  \begin{itemize}
  \tightlist
  \item
    \(\beta_2\) is the \textbf{immediate effect} after the intervention
  \end{itemize}
\item
  \(P\) is the time variable indicating time passed since the intervention (before the intervention, the value is set to 0) (to examine the sustained effect).

  \begin{itemize}
  \tightlist
  \item
    \(\beta_3\) is the \textbf{sustained effect} = difference between the slope of the line prior to the intervention and the slope of the line subsequent to the intervention
  \end{itemize}
\end{itemize}

\textbf{Example}

Create a fictitious dataset where we know the true data generating process

\[
Outcome = 10 \times time + 20 \times treatment + 25 \times timesincetreatment + noise
\]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# number of days}
\NormalTok{n }\OtherTok{=} \DecValTok{365}


\CommentTok{\# intervention at day}
\NormalTok{interven }\OtherTok{=} \DecValTok{200}

\CommentTok{\# time index from 1 to 365}
\NormalTok{time }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n)}

\CommentTok{\# treatment variable: before internvation = day 1 to 200, }
\CommentTok{\# after intervention = day 201 to 365}
\NormalTok{treatment }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, interven), }\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, n }\SpecialCharTok{{-}}\NormalTok{ interven))}

\CommentTok{\# time since treatment}
\NormalTok{timesincetreat }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, interven), }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(n }\SpecialCharTok{{-}}\NormalTok{ interven)))}

\CommentTok{\# outcome}
\NormalTok{outcome }\OtherTok{=} \DecValTok{10} \SpecialCharTok{+} \DecValTok{15} \SpecialCharTok{*}\NormalTok{ time }\SpecialCharTok{+} \DecValTok{20} \SpecialCharTok{*}\NormalTok{ treatment }\SpecialCharTok{+} 
    \DecValTok{25} \SpecialCharTok{*}\NormalTok{ timesincetreat }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)}

\NormalTok{df }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(outcome, time, treatment, timesincetreat)}

\FunctionTok{head}\NormalTok{(df, }\DecValTok{10}\NormalTok{)}
\CommentTok{\#\textgreater{}      outcome time treatment timesincetreat}
\CommentTok{\#\textgreater{} 1   24.79426    1         0              0}
\CommentTok{\#\textgreater{} 2   41.23378    2         0              0}
\CommentTok{\#\textgreater{} 3   54.72249    3         0              0}
\CommentTok{\#\textgreater{} 4   71.08714    4         0              0}
\CommentTok{\#\textgreater{} 5   84.40310    5         0              0}
\CommentTok{\#\textgreater{} 6   99.60984    6         0              0}
\CommentTok{\#\textgreater{} 7  114.91295    7         0              0}
\CommentTok{\#\textgreater{} 8  130.64154    8         0              0}
\CommentTok{\#\textgreater{} 9  142.81387    9         0              0}
\CommentTok{\#\textgreater{} 10 160.91194   10         0              0}
\end{Highlighting}
\end{Shaded}

Visualize

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{time, df}\SpecialCharTok{$}\NormalTok{outcome)}

\CommentTok{\# intervention date}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{v =}\NormalTok{ interven, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{)}

\CommentTok{\# regression line}
\NormalTok{ts }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(outcome }\SpecialCharTok{\textasciitilde{}}\NormalTok{ time }\SpecialCharTok{+}\NormalTok{ treatment }\SpecialCharTok{+}\NormalTok{ timesincetreat, }\AttributeTok{data =}\NormalTok{ df)}
\FunctionTok{lines}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{time, ts}\SpecialCharTok{$}\NormalTok{fitted.values, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{32-interrupted-time-series_files/figure-latex/unnamed-chunk-2-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(ts)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = outcome \textasciitilde{} time + treatment + timesincetreat, data = df)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}      Min       1Q   Median       3Q      Max }
\CommentTok{\#\textgreater{} {-}3.07701 {-}0.68627 {-}0.05883  0.66136  2.83087 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}                 Estimate Std. Error  t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)    10.026599   0.139242    72.01   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} time           14.999387   0.001201 12485.28   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} treatment      20.232649   0.206490    97.98   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} timesincetreat 24.999649   0.002003 12478.57   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 0.9809 on 361 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:      1,  Adjusted R{-}squared:      1 }
\CommentTok{\#\textgreater{} F{-}statistic: 9.909e+08 on 3 and 361 DF,  p{-}value: \textless{} 2.2e{-}16}
\end{Highlighting}
\end{Shaded}

Interpretation

\begin{itemize}
\item
  Time coefficient shows before-intervention outcome trend. Positive and significant, indicating a rising trend. Every day adds 15 points.
\item
  The treatment coefficient shows the \textbf{immediate} increase in outcome. \textbf{Immediate effect} is positive and significant, increasing outcome by 20 points.
\item
  The time since treatment coefficient reflects a change in trend subsequent to the intervention. The \textbf{sustained effect} is positive and statistically significant, showing that the outcome increases by 25 points per day after the intervention.
\end{itemize}

See \citet{lee2014graphical} for suggestions

Plot of counterfactual

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# treatment prediction}
\NormalTok{pred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(ts, df)}

\CommentTok{\# counterfactual dataset}
\NormalTok{new\_df }\OtherTok{\textless{}{-}}
    \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(}
        \AttributeTok{time =}\NormalTok{ time,}
        \CommentTok{\# treatment = 0 means counterfactual}
        \AttributeTok{treatment =} \FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, n),}
        \CommentTok{\# time since treatment = 0 means counterfactual}
        \AttributeTok{timesincetreat =} \FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{)}
\NormalTok{    ))}

\CommentTok{\# counterfactual predictions}
\NormalTok{pred\_cf }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(ts, new\_df)}

\CommentTok{\# plot}
\FunctionTok{plot}\NormalTok{(}
\NormalTok{    outcome,}
    \AttributeTok{col =} \FunctionTok{gray}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\FloatTok{0.2}\NormalTok{),}
    \AttributeTok{pch =} \DecValTok{19}\NormalTok{,}
    \AttributeTok{xlim  =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{365}\NormalTok{),}
    \AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{10000}\NormalTok{),}
    \AttributeTok{xlab =} \StringTok{"xlab"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"ylab"}
\NormalTok{)}

\CommentTok{\# regression line before treatment}
\FunctionTok{lines}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{interven), pred[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{interven], }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{3}\NormalTok{)}

\CommentTok{\# regression line after treatment}
\FunctionTok{lines}\NormalTok{(}\FunctionTok{rep}\NormalTok{((interven }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{n), pred[(interven }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{n], }
      \AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{3}\NormalTok{)}

\CommentTok{\# regression line after treatment (counterfactual)}
\FunctionTok{lines}\NormalTok{(}
    \FunctionTok{rep}\NormalTok{(interven}\SpecialCharTok{:}\NormalTok{n),}
\NormalTok{    pred\_cf[(interven)}\SpecialCharTok{:}\NormalTok{n],}
    \AttributeTok{col =} \StringTok{"yellow"}\NormalTok{,}
    \AttributeTok{lwd =} \DecValTok{3}\NormalTok{,}
    \AttributeTok{lty =} \DecValTok{5}
\NormalTok{)}

\FunctionTok{abline}\NormalTok{(}\AttributeTok{v =}\NormalTok{ interven, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{32-interrupted-time-series_files/figure-latex/unnamed-chunk-4-1} \end{center}

Possible threats to the validity of interrupted time series analysis \citep{baicker2019testing}

\begin{itemize}
\item
  Delayed effects \citep{rodgers2005did} (may have to make assess some time after the intervention - do not assess the immediate dates).
\item
  Other confounding events \citep[\citet{linden2017comprehensive}]{linden2016using}
\item
  Intervention is introduced but later withdrawn \citep{linden2015conducting}
\item
  \protect\hyperlink{autocorrelation}{Autocorrelation} (for every time series data): might cause underestimation in the standard errors (i.e., overestimating the statistical significance of the treatment effect)
\item
  Regression to the mean: after a the short-term shock to the outcome, individuals can revert back to their initial states.
\item
  Selection bias: only certain individuals are affected by the treatment (could use a \protect\hyperlink{multiple-groups}{Multiple Groups}).
\end{itemize}

\hypertarget{autocorrelation}{%
\section{Autocorrelation}\label{autocorrelation}}

Assess autocorrelation from residual

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# simple regression on time }
\NormalTok{simple\_ts }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(outcome }\SpecialCharTok{\textasciitilde{}}\NormalTok{ time, }\AttributeTok{data =}\NormalTok{ df)}

\FunctionTok{plot}\NormalTok{(}\FunctionTok{resid}\NormalTok{(simple\_ts))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{32-interrupted-time-series_files/figure-latex/unnamed-chunk-5-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# alternatively}
\FunctionTok{acf}\NormalTok{(}\FunctionTok{resid}\NormalTok{(simple\_ts))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{32-interrupted-time-series_files/figure-latex/unnamed-chunk-5-2} \end{center}

This is not the best example since I created this dataset. But when residuals do have autocorrelation, you should not see any patterns (i.e., points should be randomly distributed on the plot)

To formally test for autocorrelation, we can use the Durbin-Watson test

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lmtest}\SpecialCharTok{::}\FunctionTok{dwtest}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{outcome }\SpecialCharTok{\textasciitilde{}}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{time)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Durbin{-}Watson test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  df$outcome \textasciitilde{} df$time}
\CommentTok{\#\textgreater{} DW = 0.00037525, p{-}value \textless{} 2.2e{-}16}
\CommentTok{\#\textgreater{} alternative hypothesis: true autocorrelation is greater than 0}
\end{Highlighting}
\end{Shaded}

From the p-value, we know that there is autocorrelation in the time series

A solution to this problem is to use more advanced time series analysis (e.g., ARIMA - coming up in the book) to adjust for seasonality and other dependency.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{forecast}\SpecialCharTok{::}\FunctionTok{auto.arima}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{outcome, }\AttributeTok{xreg =} \FunctionTok{as.matrix}\NormalTok{(df[,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]))}
\CommentTok{\#\textgreater{} Series: df$outcome }
\CommentTok{\#\textgreater{} Regression with ARIMA(0,0,0) errors }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}       intercept     time  treatment  timesincetreat}
\CommentTok{\#\textgreater{}         10.0266  14.9994    20.2326         24.9996}
\CommentTok{\#\textgreater{} s.e.     0.1385   0.0012     0.2054          0.0020}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} sigma\^{}2 = 0.9622:  log likelihood = {-}508.86}
\CommentTok{\#\textgreater{} AIC=1027.72   AICc=1027.89   BIC=1047.22}
\end{Highlighting}
\end{Shaded}

\hypertarget{multiple-groups}{%
\section{Multiple Groups}\label{multiple-groups}}

When you suspect that you might have confounding events or selection bias, you can add a control group that did not experience the treatment (very much similar to \protect\hyperlink{difference-in-differences}{Difference-in-differences})

The model then becomes

\[
\begin{aligned}
Y = \beta_0 &+ \beta_1 time+ \beta_2 treatment +\beta_3 \times timesincetreat \\
&+\beta_4 group + \beta_5 group \times time + \beta_6 group \times treatment \\
&+ \beta_7 group \times timesincetreat
\end{aligned}
\]

where

\begin{itemize}
\item
  Group = 1 when the observation is under treatment and 0 under control
\item
  \(\beta_4\) = baseline difference between the treatment and control group
\item
  \(\beta_5\) = slope difference between the treatment and control group before treatment
\item
  \(\beta_6\) = baseline difference between the treatment and control group associated with the treatment.
\item
  \(\beta_7\) = difference between the sustained effect of the treatment and control group after the treatment.
\end{itemize}

\hypertarget{part-c.-other-concerns}{%
\part*{C. OTHER CONCERNS}\label{part-c.-other-concerns}}
\addcontentsline{toc}{part}{C. OTHER CONCERNS}

\hypertarget{endogeneity}{%
\chapter{Endogeneity}\label{endogeneity}}

Refresher

A general model framework

\[
\mathbf{Y = X \beta + \epsilon}
\]

where

\begin{itemize}
\item
  \(\mathbf{Y} = n \times 1\)
\item
  \(\mathbf{X} = n \times k\)
\item
  \(\beta = k \times 1\)
\item
  \(\epsilon = n \times 1\)
\end{itemize}

Then, OLS estimates of coefficients are

\[
\begin{aligned}
\hat{\beta}_{OLS} &= (\mathbf{X}'\mathbf{X})^{-1}(\mathbf{X}'\mathbf{Y}) \\
&= (\mathbf{X}'\mathbf{X})^{-1}(\mathbf{X}'(\mathbf{X \beta + \epsilon})) \\
&= (\mathbf{X}'\mathbf{X})^{-1} (\mathbf{X}'\mathbf{X}) \beta + (\mathbf{X}'\mathbf{X})^{-1} (\mathbf{X}'\mathbf{\epsilon}) \\
\hat{\beta}_{OLS} & \to \beta + (\mathbf{X}'\mathbf{X})^{-1} (\mathbf{X}'\mathbf{\epsilon})
\end{aligned}
\]

To have unbiased estimates, we have to get rid of the second part \((\mathbf{X}'\mathbf{X})^{-1} (\mathbf{X}'\mathbf{\epsilon})\)

There are 2 conditions to achieve unbiased estimates:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(E(\epsilon |X) = 0\) (This is easy, putting an intercept can solve this issue)
\item
  \(Cov(\mathbf{X}, \epsilon) = 0\) (This is the hard part)
\end{enumerate}

We only care about omitted variable

Usually, the problem will stem Omitted Variables Bias, but we only care about omitted variable bias when

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Omitted variables correlate with the variables we care about (\(X\)). If OMV does not correlate with \(X\), we don't care, and random assignment makes this correlation goes to 0)
\item
  Omitted variables correlates with outcome/ dependent variable
\end{enumerate}

There are more types of endogeneity listed below.

Types of endogeneity (See \citet{hill2021endogeneity} for a review in management):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \protect\hyperlink{endogenous-treatment}{Endogenous Treatment}
\end{enumerate}

\begin{itemize}
\item
  Omitted Variables Bias

  \begin{itemize}
  \tightlist
  \item
    Motivation
  \item
    Ability/talent
  \item
    Self-selection
  \end{itemize}
\item
  Feedback Effect (\protect\hyperlink{simultaneity}{Simultaneity}): also known as bidirectionality
\item
  Reverse Causality: Subtle difference from \protect\hyperlink{simultaneity}{Simultaneity}: Technically, two variables affect each other sequentially, but in a big enough time frame, (e.g., monthly, or yearly), our coefficient will be biased just like simultaneity.
\item
  \protect\hyperlink{measurement-error}{Measurement Error}
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \protect\hyperlink{endogenous-sample-selection}{Endogenous Sample Selection}
\end{enumerate}

To deal with this problem, we have a toolbox (that has been mentioned in previous chapter \ref{causal-inference})

Using control variables in regression is a ``selection on observables'' identification strategy.

In other words, if you believe you have an omitted variable, and you can measure it, including it in the regression model solves your problem. These uninterested variables are called control variables in your model.

However, this is rarely the case (because the problem is we don't have their measurements). Hence, we need more elaborate methods:

\begin{itemize}
\item
  \protect\hyperlink{endogenous-treatment}{Endogenous Treatment}
\item
  \protect\hyperlink{endogenous-sample-selection}{Endogenous Sample Selection}
\end{itemize}

Before we get to methods that deal with bias arises from omitted variables, we consider cases where we do have measurements of a variable, but there is measurement error (bias).

\hypertarget{endogenous-treatment}{%
\section{Endogenous Treatment}\label{endogenous-treatment}}

\hypertarget{measurement-error}{%
\subsection{Measurement Error}\label{measurement-error}}

\begin{itemize}
\item
  Data error can stem from

  \begin{itemize}
  \item
    Coding errors
  \item
    Reporting errors
  \end{itemize}
\end{itemize}

Two forms of measurement error:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Random (stochastic) (indeterminate error) (\protect\hyperlink{classical-measurement-errors}{Classical Measurement Errors}): noise or measurement errors do not show up in a consistent or predictable way.
\item
  Systematic (determinate error) (\protect\hyperlink{non-classical-measurement-errors}{Non-classical Measurement Errors}): When measurement error is consistent and predictable across observations.

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Instrument errors (e.g., faulty scale) -\textgreater{} calibration or adjustment
  \item
    Method errors (e.g., sampling errors) -\textgreater{} better method development + study design
  \item
    Human errors (e.g., judgement)
  \end{enumerate}
\end{enumerate}

Usually the systematic measurement error is a bigger issue because it introduces ``bias'' into our estimates, while random error introduces noise into our estimates

\begin{itemize}
\tightlist
\item
  Noise -\textgreater{} regression estimate to 0
\item
  Bias -\textgreater{} can pull estimate to upward or downward.
\end{itemize}

\hypertarget{classical-measurement-errors}{%
\subsubsection{Classical Measurement Errors}\label{classical-measurement-errors}}

\hypertarget{right-hand-side}{%
\paragraph{Right-hand side}\label{right-hand-side}}

\begin{itemize}
\tightlist
\item
  Right-hand side measurement error: When the measurement is in the covariates, then we have the endogeneity problem.
\end{itemize}

Say you know the true model is

\[
Y_i = \beta_0 + \beta_1 X_i + u_i
\]

But you don't observe \(X_i\), but you observe

\[
\tilde{X}_i = X_i + e_i
\]

which is known as classical measurement errors where we \textbf{assume} \(e_i\) is uncorrelated with \(X_i\) (i.e., \(E(X_i e_i) = 0\))

Then, when you estimate your observed variables, you have (substitute \(X_i\) with \(\tilde{X}_i - e_i\) ):

\[
\begin{aligned}
Y_i &= \beta_0 + \beta_1 (\tilde{X}_i - e_i)+ u_i \\
&= \beta_0 + \beta_1 \tilde{X}_i + u_i - \beta_1 e_i \\
&= \beta_0 + \beta_1 \tilde{X}_i + v_i
\end{aligned}
\]

In words, the measurement error in \(X_i\) is now a part of the error term in the regression equation \(v_i\). Hence, we have an endogeneity bias.

Endogeneity arises when

\[
\begin{aligned}
E(\tilde{X}_i v_i) &= E((X_i + e_i )(u_i - \beta_1 e_i)) \\
&= -\beta_1 Var(e_i) \neq 0
\end{aligned}
\]

Since \(\tilde{X}_i\) and \(e_i\) are positively correlated, then it leads to

\begin{itemize}
\item
  a negative bias in \(\hat{\beta}_1\) if the true \(\beta_1\) is positive
\item
  a positive bias if \(\beta_1\) is negative
\end{itemize}

In other words, measurement errors cause \textbf{attenuation bias}, which inter turn pushes the coefficient towards 0

As \(Var(e_i)\) increases or \(\frac{Var(e_i)}{Var(\tilde{X})} \to 1\) then \(e_i\) is a random (noise) and \(\beta_1 \to 0\) (random variable \(\tilde{X}\) should not have any relation to \(Y_i\))

Technical note:

The size of the bias in the OLS-estimator is

\[
\hat{\beta}_{OLS} = \frac{ cov(\tilde{X}, Y)}{var(\tilde{X})} = \frac{cov(X + e, \beta X + u)}{var(X + e)}
\]

then

\[
plim \hat{\beta}_{OLS} = \beta \frac{\sigma^2_X}{\sigma^2_X + \sigma^2_e} = \beta \lambda
\]

where \(\lambda\) is \textbf{reliability} or signal-to-total variance ratio or attenuation factor

Reliability affect the extent to which measurement error attenuates \(\hat{\beta}\). The attenuation bias is

\[
\hat{\beta}_{OLS} - \beta = -(1-\lambda)\beta
\]

Thus, \(\hat{\beta}_{OLS} < \beta\) (unless \(\lambda = 1\), in which case we don't even have measurement error).

Note:

\textbf{Data transformation worsen (magnify) the measurement error}

\[
y= \beta x + \gamma x^2 + \epsilon
\]

then, the attenuation factor for \(\hat{\gamma}\) is the square of the attenuation factor for \(\hat{\beta}\) (i.e., \(\lambda_{\hat{\gamma}} = \lambda_{\hat{\beta}}^2\))

\textbf{Adding covariates increases attenuation bias}

To fix classical measurement error problem, we can

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Find estimates of either \(\sigma^2_X, \sigma^2_\epsilon\) or \(\lambda\) from validation studies, or survey data.
\item
  \protect\hyperlink{endogenous-treatment}{Endogenous Treatment} Use instrument \(Z\) correlated with \(X\) but uncorrelated with \(\epsilon\)
\item
  Abandon your project
\end{enumerate}

\hypertarget{left-hand-side}{%
\paragraph{Left-hand side}\label{left-hand-side}}

When the measurement is in the outcome variable, econometricians or causal scientists do not care because they still have an unbiased estimate of the coefficients (the zero conditional mean assumption is not violated, hence we don't have endogeneity). However, statisticians might care because it might inflate our uncertainty in the coefficient estimates (i.e., higher standard errors).

\[
\tilde{Y} = Y + v
\]

then the model you estimate is

\[
\tilde{Y} = \beta X + u + v
\]

Since \(v\) is uncorrelated with \(X\), then \(\hat{\beta}\) is consistently estimated by OLS

If we have measurement error in \(Y_i\), it will pass through \(\beta_1\) and go to \(u_i\)

\hypertarget{non-classical-measurement-errors}{%
\subsubsection{Non-classical Measurement Errors}\label{non-classical-measurement-errors}}

Relaxing the assumption that \(X\) and \(\epsilon\) are uncorrelated

Recall the true model we have true estimate is

\[
\hat{\beta} = \frac{cov(X + \epsilon, \beta X + u)}{var(X + \epsilon)}
\]

then without the above assumption, we have

\[
\begin{aligned}
plim \hat{\beta} &= \frac{\beta (\sigma^2_X + \sigma_{X \epsilon})}{\sigma^2_X + \sigma^2_\epsilon + 2 \sigma_{X \epsilon}} \\
&= (1 - \frac{\sigma^2_{\epsilon} + \sigma_{X \epsilon}}{\sigma^2_X + \sigma^2_\epsilon + 2 \sigma_{X \epsilon}}) \beta \\
&= (1 - b_{\epsilon \tilde{X}}) \beta
\end{aligned}
\]

where \(b_{\epsilon \tilde{X}}\) is the covariance between \(\tilde{X}\) and \(\epsilon\) (also the regression coefficient of a regression of \(\epsilon\) on \(\tilde{X}\))

Hence, the \protect\hyperlink{classical-measurement-errors}{Classical Measurement Errors} is just a special case of \protect\hyperlink{non-classical-measurement-errors}{Non-classical Measurement Errors} where \(b_{\epsilon \tilde{X}} = 1 - \lambda\)

So when \(\sigma_{X \epsilon} = 0\) (\protect\hyperlink{classical-measurement-errors}{Classical Measurement Errors}), increasing this covariance \(b_{\epsilon \tilde{X}}\) increases the covariance increases the attenuation factor if more than half of the variance in \(\tilde{X}\) is measurement error, and decreases the attenuation factor otherwise. This is also known as \textbf{mean reverting measurement error} \citep[\citet{bound2001measurement}]{bound1989measurement}

A general framework for both right-hand side and left-hand side measurement error is \citep{bound2001measurement}:

consider the true model

\[
\mathbf{Y = X \beta + \epsilon}
\]

then

\[
\begin{aligned}
\hat{\beta} &= \mathbf{(\tilde{X}' \tilde{X})^{-1}\tilde{X} \tilde{Y}} \\
&= \mathbf{(\tilde{X}' \tilde{X})^{-1} \tilde{X}' (\tilde{X} \beta - U \beta + v + \epsilon )} \\
&= \mathbf{\beta + (\tilde{X}' \tilde{X})^{-1} \tilde{X}' (-U \beta + v + \epsilon)} \\
plim \hat{\beta} &= \beta + plim (\tilde{X}' \tilde{X})^{-1} \tilde{X}' ( -U\beta + v) \\
&= \beta + plim (\tilde{X}' \tilde{X})^{-1} \tilde{X}' W 
\left[
\begin{array}
{c}
- \beta \\
1
\end{array}
\right]
\end{aligned}
\]

Since we collect the measurement errors in a matrix \(W = [U|v]\), then

\[
( -U\beta + v) = W 
\left[
\begin{array}
{c}
- \beta \\
1
\end{array}
\right]
\]

Hence, in general, biases in the coefficients \(\beta\) are regression coefficients from regressing the measurement errors on the mis-measured \(\tilde{X}\)

Notes:

\begin{itemize}
\item
  \protect\hyperlink{instrumental-variable}{Instrumental Variable} can help fix this problem
\item
  There can also be measurement error in dummy variables and you can still use \protect\hyperlink{instrumental-variable}{Instrumental Variable} to fix it.
\end{itemize}

\hypertarget{solution-to-measurement-errors}{%
\subsubsection{Solution to Measurement Errors}\label{solution-to-measurement-errors}}

\hypertarget{correlation}{%
\paragraph{Correlation}\label{correlation}}

\[
\begin{aligned}
P(\rho | data) &= \frac{P(data|\rho)P(\rho)}{P(data)} \\
\text{Posterior Probability} &\propto \text{Likelihood} \times \text{Prior Probability}
\end{aligned}
\] where

\begin{itemize}
\tightlist
\item
  \(\rho\) is a correlation coefficient
\item
  \(P(data|\rho)\) is the likelihood function evaluated at \(\rho\)
\item
  \(P(\rho)\) prior probability
\item
  \(P(data)\) is the normalizing constant
\end{itemize}

With sample correlation coefficient \(r\):

\[
r = \frac{S_{xy}}{\sqrt{S_{xx}S_{yy}}}
\] Then the posterior density approximation of \(\rho\) is \citep[pp.3]{schisterman2003estimation}

\[
P(\rho| x, y)  \propto P(\rho) \frac{(1- \rho^2)^{(n-1)/2}}{(1- \rho \times r)^{n - (3/2)}}
\]

where

\begin{itemize}
\tightlist
\item
  \(\rho = \tanh \xi\) where \(\xi \sim N(z, 1/n)\)
\item
  \(r = \tanh z\)
\end{itemize}

Then the posterior density follow a normal distribution where

\textbf{Mean}

\[
\mu_{posterior} = \sigma^2_{posterior} \times (n_{prior} \times \tanh^{-1} r_{prior}+ n_{likelihood} \times \tanh^{-1} r_{likelihood})
\]

\textbf{variance}

\[
\sigma^2_{posterior} = \frac{1}{n_{prior} + n_{Likelihood}}
\]

To simplify the integration process, we choose prior that is

\[
P(\rho) \propto (1 - \rho^2)^c
\] where

\begin{itemize}
\tightlist
\item
  \(c\) is the weight the prior will have in estimation (i.e., \(c = 0\) if no prior info, hence \(P(\rho) \propto 1\))
\end{itemize}

Example:

Current study: \(r_{xy} = 0.5, n = 200\)

Previous study: \(r_{xy} = 0.2765, (n=50205)\)

Combining two, we have the posterior following a normal distribution with the \textbf{variance} of

\[
\sigma^2_{posterior} =  \frac{1}{n_{prior} + n_{Likelihood}} = \frac{1}{200 + 50205} = 0.0000198393
\]

\textbf{Mean}

\[
\begin{aligned}
\mu_{Posterior} &= \sigma^2_{Posterior}  \times (n_{prior} \times \tanh^{-1} r_{prior}+ n_{likelihood} \times \tanh^{-1} r_{likelihood}) \\
&= 0.0000198393 \times (50205 \times \tanh^{-1} 0.2765 + 200 \times \tanh^{-1}0.5 )\\
&= 0.2849415
\end{aligned}
\]

Hence, \(Posterior \sim N(0.691, 0.0009)\), which means the correlation coefficient is \(\tanh(0.691) = 0.598\) and 95\% CI is

\[
\mu_{posterior} \pm 1.96 \times \sqrt{\sigma^2_{Posterior}} = 0.2849415 \pm 1.96 \times (0.0000198393)^{1/2} = (0.2762115, 0.2936714)
\]

Hence, the interval for posterior \(\rho\) is \((0.2693952, 0.2855105)\)

If future authors suspect that they have

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Large sampling variation
\item
  Measurement error in either measures in the correlation, which attenuates the relationship between the two variables
\end{enumerate}

Applying this Bayesian correction can give them a better estimate of the correlation between the two.

To implement this calculation in R, see below

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n\_new              }\OtherTok{\textless{}{-}} \DecValTok{200}
\NormalTok{r\_new              }\OtherTok{\textless{}{-}} \FloatTok{0.5}
\NormalTok{alpha              }\OtherTok{\textless{}{-}} \FloatTok{0.05}

\NormalTok{update\_correlation }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(n\_new, r\_new, alpha) \{}
\NormalTok{    n\_meta             }\OtherTok{\textless{}{-}} \DecValTok{50205}
\NormalTok{    r\_meta             }\OtherTok{\textless{}{-}} \FloatTok{0.2765}
    
    \CommentTok{\# Variance}
\NormalTok{    var\_xi         }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{/}\NormalTok{ (n\_new }\SpecialCharTok{+}\NormalTok{ n\_meta)}
    \FunctionTok{format}\NormalTok{(var\_xi, }\AttributeTok{scientific =} \ConstantTok{FALSE}\NormalTok{)}
    
    \CommentTok{\# mean}
\NormalTok{    mu\_xi          }\OtherTok{\textless{}{-}}\NormalTok{ var\_xi }\SpecialCharTok{*}\NormalTok{ (n\_meta }\SpecialCharTok{*} \FunctionTok{atanh}\NormalTok{(r\_meta) }\SpecialCharTok{+}\NormalTok{ n\_new }\SpecialCharTok{*}\NormalTok{ (}\FunctionTok{atanh}\NormalTok{(r\_new)))}
    \FunctionTok{format}\NormalTok{(mu\_xi, }\AttributeTok{scientific  =} \ConstantTok{FALSE}\NormalTok{)}
    
    \CommentTok{\# confidence interval}
\NormalTok{    upper\_xi       }\OtherTok{\textless{}{-}}\NormalTok{ mu\_xi }\SpecialCharTok{+} \FunctionTok{qnorm}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alpha }\SpecialCharTok{/} \DecValTok{2}\NormalTok{) }\SpecialCharTok{*} \FunctionTok{sqrt}\NormalTok{(var\_xi)}
\NormalTok{    lower\_xi       }\OtherTok{\textless{}{-}}\NormalTok{ mu\_xi }\SpecialCharTok{{-}} \FunctionTok{qnorm}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alpha }\SpecialCharTok{/} \DecValTok{2}\NormalTok{) }\SpecialCharTok{*} \FunctionTok{sqrt}\NormalTok{(var\_xi)}
    
    \CommentTok{\# rho}
\NormalTok{    mean\_rho       }\OtherTok{\textless{}{-}} \FunctionTok{tanh}\NormalTok{(mu\_xi)}
\NormalTok{    upper\_rho      }\OtherTok{\textless{}{-}} \FunctionTok{tanh}\NormalTok{(upper\_xi)}
\NormalTok{    lower\_rho      }\OtherTok{\textless{}{-}} \FunctionTok{tanh}\NormalTok{(lower\_xi)}
    
    \CommentTok{\# return a list}
    \FunctionTok{return}\NormalTok{(}
        \FunctionTok{list}\NormalTok{(}
            \StringTok{"mu\_xi"} \OtherTok{=}\NormalTok{ mu\_xi,}
            \StringTok{"var\_xi"} \OtherTok{=}\NormalTok{ var\_xi,}
            \StringTok{"upper\_xi"} \OtherTok{=}\NormalTok{ upper\_xi,}
            \StringTok{"lower\_xi"} \OtherTok{=}\NormalTok{ lower\_xi,}
            \StringTok{"mean\_rho"} \OtherTok{=}\NormalTok{ mean\_rho,}
            \StringTok{"upper\_rho"} \OtherTok{=}\NormalTok{ upper\_rho,}
            \StringTok{"lower\_rho"} \OtherTok{=}\NormalTok{ lower\_rho}
\NormalTok{        )}
\NormalTok{    )}
\NormalTok{\}}




\CommentTok{\# Old confidence interval}
\NormalTok{r\_new }\SpecialCharTok{+} \FunctionTok{qnorm}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alpha }\SpecialCharTok{/} \DecValTok{2}\NormalTok{) }\SpecialCharTok{*} \FunctionTok{sqrt}\NormalTok{(}\DecValTok{1}\SpecialCharTok{/}\NormalTok{n\_new)}
\CommentTok{\#\textgreater{} [1] 0.6385904}
\NormalTok{r\_new }\SpecialCharTok{{-}} \FunctionTok{qnorm}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alpha }\SpecialCharTok{/} \DecValTok{2}\NormalTok{) }\SpecialCharTok{*} \FunctionTok{sqrt}\NormalTok{(}\DecValTok{1}\SpecialCharTok{/}\NormalTok{n\_new)}
\CommentTok{\#\textgreater{} [1] 0.3614096}

\NormalTok{testing }\OtherTok{=} \FunctionTok{update\_correlation}\NormalTok{(}\AttributeTok{n\_new =}\NormalTok{ n\_new, }\AttributeTok{r\_new =}\NormalTok{ r\_new, }\AttributeTok{alpha =}\NormalTok{ alpha)}

\CommentTok{\# Updated rho}
\NormalTok{testing}\SpecialCharTok{$}\NormalTok{mean\_rho}
\CommentTok{\#\textgreater{} [1] 0.2774723}

\CommentTok{\# Updated confidence interval}
\NormalTok{testing}\SpecialCharTok{$}\NormalTok{upper\_rho}
\CommentTok{\#\textgreater{} [1] 0.2855105}
\NormalTok{testing}\SpecialCharTok{$}\NormalTok{lower\_rho}
\CommentTok{\#\textgreater{} [1] 0.2693952}
\end{Highlighting}
\end{Shaded}

\hypertarget{simultaneity}{%
\subsection{Simultaneity}\label{simultaneity}}

\begin{itemize}
\item
  When independent variables (\(X\)'s) are jointly determined with the dependent variable \(Y\), typically through an equilibrium mechanism, violates the second condition for causality (i.e., temporal order).
\item
  Examples: quantity and price by demand and supply, investment and productivity, sales and advertisement
\end{itemize}

General Simultaneous (Structural) Equations

\[
\begin{aligned}
Y_i &= \beta_0 + \beta_1 X_i + u_i \\
X_i &= \alpha_0 + \alpha_1 Y_i + v_i
\end{aligned}
\]

Hence, the solutions are

\[
\begin{aligned}
Y_i &= \frac{\beta_0 + \beta_1 \alpha_0}{1 - \alpha_1 \beta_1} + \frac{\beta_1 v_i + u_i}{1 - \alpha_1 \beta_1} \\
X_i &= \frac{\alpha_0 + \alpha_1 \beta_0}{1 - \alpha_1 \beta_1} + \frac{v_i + \alpha_1 u_i}{1 - \alpha_1 \beta_1}
\end{aligned}
\]

If we run only one regression, we will have biased estimators (because of \textbf{simultaneity bias}):

\[
\begin{aligned}
Cov(X_i, u_i) &= Cov(\frac{v_i + \alpha_1 u_i}{1 - \alpha_1 \beta_1}, u_i) \\
&= \frac{\alpha_1}{1- \alpha_1 \beta_1} Var(u_i)
\end{aligned}
\]

In an even more general model

\[
\begin{cases}
Y_i = \beta_0 + \beta_1 X_i + \beta_2 T_i + u_i \\
X_i = \alpha_0 + \alpha_1 Y_i + \alpha_2 Z_i + v_i
\end{cases}
\]

where

\begin{itemize}
\item
  \(X_i, Y_i\) are \textbf{endogenous} variables determined within the system
\item
  \(T_i, Z_i\) are \textbf{exogenous} variables
\end{itemize}

Then, the reduced form of the model is

\[
\begin{cases}
\begin{aligned}
Y_i &= \frac{\beta_0 + \beta_1 \alpha_0}{1 - \alpha_1 \beta_1} + \frac{\beta_1 \alpha_2}{1 - \alpha_1 \beta_1} Z_i + \frac{\beta_2}{1 - \alpha_1 \beta_1} T_i + \tilde{u}_i \\
&= B_0 + B_1 Z_i + B_2 T_i + \tilde{u}_i
\end{aligned}
\\
\begin{aligned}
X_i &= \frac{\alpha_0 + \alpha_1 \beta_0}{1 - \alpha_1 \beta_1} + \frac{\alpha_2}{1 - \alpha_1 \beta_1} Z_i + \frac{\alpha_1\beta_2}{1 - \alpha_1 \beta_1} T_i + \tilde{v}_i \\
&= A_0 + A_1 Z_i + A_2 T_i + \tilde{v}_i
\end{aligned}
\end{cases}
\]

Then, now we can get consistent estimates of the reduced form parameters

And to get the original parameter estimates

\[
\begin{aligned}
\frac{B_1}{A_1} &= \beta_1 \\
B_2 (1 - \frac{B_1 A_2}{A_1B_2}) &= \beta_2 \\
\frac{A_2}{B_2} &= \alpha_1 \\
A_1 (1 - \frac{B_1 A_2}{A_1 B_2}) &= \alpha_2
\end{aligned}
\]

Rules for Identification

\textbf{Order Condition} (necessary but not sufficient)

\[
K - k \ge m - 1
\]

where

\begin{itemize}
\item
  \(M\) = number of endogenous variables in the model
\item
  K = number of exogenous variables int he model
\item
  \(m\) = number of endogenous variables in a given
\item
  \(k\) = is the number of exogenous variables in a given equation
\end{itemize}

This is actually the general framework for instrumental variables

\hypertarget{endogenous-treatment-solutions}{%
\subsection{Endogenous Treatment Solutions}\label{endogenous-treatment-solutions}}

Using the OLS estimates as a reference point

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(AER)}
\FunctionTok{library}\NormalTok{(REndo)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{421}\NormalTok{)}
\FunctionTok{data}\NormalTok{(}\StringTok{"CASchools"}\NormalTok{)}
\NormalTok{school }\OtherTok{\textless{}{-}}\NormalTok{ CASchools}
\NormalTok{school}\SpecialCharTok{$}\NormalTok{stratio }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(CASchools, students }\SpecialCharTok{/}\NormalTok{ teachers)}
\NormalTok{m1.ols }\OtherTok{\textless{}{-}}
    \FunctionTok{lm}\NormalTok{(read }\SpecialCharTok{\textasciitilde{}}\NormalTok{ stratio }\SpecialCharTok{+}\NormalTok{ english }\SpecialCharTok{+}\NormalTok{ lunch }
       \SpecialCharTok{+}\NormalTok{ grades }\SpecialCharTok{+}\NormalTok{ income }\SpecialCharTok{+}\NormalTok{ calworks }\SpecialCharTok{+}\NormalTok{ county,}
       \AttributeTok{data =}\NormalTok{ school)}
\FunctionTok{summary}\NormalTok{(m1.ols)}\SpecialCharTok{$}\NormalTok{coefficients[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{7}\NormalTok{,]}
\CommentTok{\#\textgreater{}                 Estimate Std. Error     t value      Pr(\textgreater{}|t|)}
\CommentTok{\#\textgreater{} (Intercept) 683.45305948 9.56214469  71.4748711 3.011667e{-}218}
\CommentTok{\#\textgreater{} stratio      {-}0.30035544 0.25797023  {-}1.1643027  2.450536e{-}01}
\CommentTok{\#\textgreater{} english      {-}0.20550107 0.03765408  {-}5.4576041  8.871666e{-}08}
\CommentTok{\#\textgreater{} lunch        {-}0.38684059 0.03700982 {-}10.4523759  1.427370e{-}22}
\CommentTok{\#\textgreater{} gradesKK{-}08  {-}1.91291321 1.35865394  {-}1.4079474  1.599886e{-}01}
\CommentTok{\#\textgreater{} income        0.71615378 0.09832843   7.2832829  1.986712e{-}12}
\CommentTok{\#\textgreater{} calworks     {-}0.05273312 0.06154758  {-}0.8567863  3.921191e{-}01}
\end{Highlighting}
\end{Shaded}

\hypertarget{instrumental-variable}{%
\subsubsection{Instrumental Variable}\label{instrumental-variable}}

{[}A3a{]} requires \(\epsilon_i\) to be uncorrelated with \(\mathbf{x}_i\)

Assume \protect\hyperlink{a1-linearity}{A1} , \protect\hyperlink{a2-full-rank}{A2}, \protect\hyperlink{a5-data-generation-random-sampling}{A5}

\[
plim(\hat{\beta}_{OLS}) = \beta + [E(\mathbf{x_i'x_i})]^{-1}E(\mathbf{x_i'}\epsilon_i)
\]

{[}A3a{]} is the weakest assumption needed for OLS to be \textbf{consistent}

\protect\hyperlink{a3-exogeneity-of-independent-variables}{A3} fails when \(x_{ik}\) is correlated with \(\epsilon_i\)

\begin{itemize}
\tightlist
\item
  Omitted Variables Bias: \(\epsilon_i\) includes any other factors that may influence the dependent variable (linearly)
\item
  \protect\hyperlink{simultaneity}{Simultaneity} Demand and prices are simultaneously determined.
\item
  \protect\hyperlink{endogenous-sample-selection}{Endogenous Sample Selection} we did not have iid sample
\item
  \protect\hyperlink{measurement-error}{Measurement Error}
\end{itemize}

\textbf{Note}

\begin{itemize}
\tightlist
\item
  Omitted Variable: an omitted variable is a variable, omitted from the model (but is in the \(\epsilon_i\)) and unobserved has predictive power towards the outcome.
\item
  Omitted Variable Bias: is the bias (and inconsistency when looking at large sample properties) of the OLS estimator when the omitted variable.
\item
  We cam have both positive and negative selection bias (it depends on what our story is)
\end{itemize}

The \textbf{structural equation} is used to emphasize that we are interested understanding a \textbf{causal relationship}

\[
y_{i1} = \beta_0 + \mathbf{z}_i1 \beta_1 + y_{i2}\beta_2 +  \epsilon_i
\]

where

\begin{itemize}
\tightlist
\item
  \(y_{it}\) is the outcome variable (inherently correlated with \(\epsilon_i\))
\item
  \(y_{i2}\) is the endogenous covariate (presumed to be correlated with \(\epsilon_i\))
\item
  \(\beta_1\) represents the causal effect of \(y_{i2}\) on \(y_{i1}\)
\item
  \(\mathbf{z}_{i1}\) is exogenous controls (uncorrelated with \(\epsilon_i\)) (\(E(z_{1i}'\epsilon_i) = 0\))
\end{itemize}

OLS is an inconsistent estimator of the causal effect \(\beta_2\)

If there was no endogeneity

\begin{itemize}
\tightlist
\item
  \(E(y_{i2}'\epsilon_i) = 0\)
\item
  the exogenous variation in \(y_{i2}\) is what identifies the causal effect
\end{itemize}

If there is endogeneity

\begin{itemize}
\tightlist
\item
  Any wiggle in \(y_{i2}\) will shift simultaneously with \(\epsilon_i\)
\end{itemize}

\[
plim(\hat{\beta}_{OLS}) = \beta + [E(\mathbf{x'_ix_i})]^{-1}E(\mathbf{x'_i}\epsilon_i)
\]

where

\begin{itemize}
\tightlist
\item
  \(\beta\) is the causal effect
\item
  \([E(\mathbf{x'_ix_i})]^{-1}E(\mathbf{x'_i}\epsilon_i)\) is the endogenous effect
\end{itemize}

Hence \(\hat{\beta}_{OLS}\) can be either more positive and negative than the true causal effect.

Motivation for \textbf{Two Stage Least Squares (2SLS)}

\[
y_{i1}=\beta_0 + \mathbf{z}_{i1}\beta_1 + y_{i2}\beta_2 + \epsilon_i
\]

We want to understand how movement in \(y_{i2}\) effects movement in \(y_{i1}\), but whenever we move \(y_{i2}\), \(\epsilon_i\) also moves.

\textbf{Solution}\\
We need a way to move \(y_{i2}\) independently of \(\epsilon_i\), then we can analyze the response in \(y_{i1}\) as a causal effect

\begin{itemize}
\item
  Find an \textbf{instrumental variable(s)} \(z_{i2}\)

  \begin{itemize}
  \tightlist
  \item
    Instrument \textbf{Relevance}: when** \(z_{i2}\) moves then \(y_{i2}\) also moves
  \item
    Instrument \textbf{Exogeneity}: when \(z_{i2}\) moves then \(\epsilon_i\) does not move.
  \end{itemize}
\item
  \(z_{i2}\) is the \textbf{exogenous variation that identifies} the causal effect \(\beta_2\)
\end{itemize}

Finding an Instrumental variable:

\begin{itemize}
\tightlist
\item
  Random Assignment: + Effect of class size on educational outcomes: instrument is initial random
\item
  Relation's Choice + Effect of Education on Fertility: instrument is parent's educational level
\item
  Eligibility + Trade-off between IRA and 401K retirement savings: instrument is 401k eligibility
\end{itemize}

\textbf{Example}

Return to College

\begin{itemize}
\item
  education is correlated with ability - endogenous
\item
  \textbf{Near 4year} as an instrument

  \begin{itemize}
  \tightlist
  \item
    Instrument Relevance: when \textbf{near} moves then education also moves
  \item
    Instrument Exogeneity: when \textbf{near} moves then \(\epsilon_i\) does not move.
  \end{itemize}
\item
  Other potential instruments; near a 2-year college. Parent's Education. Owning Library Card
\end{itemize}

\[
y_{i1}=\beta_0 + \mathbf{z}_{i1}\beta_1 + y_{i2}\beta_2 + \epsilon_i
\]

First Stage (Reduced Form) Equation:

\[
y_{i2} = \pi_0 + \mathbf{z_{i1}\pi_1} + \mathbf{z_{i2}\pi_2} + v_i
\]

where

\begin{itemize}
\tightlist
\item
  \(\pi_0 + \mathbf{z_{i1}\pi_1} + \mathbf{z_{i2}\pi_2}\) is exogenous variation \(v_i\) is endogenous variation
\end{itemize}

This is called a \textbf{reduced form equation}

\begin{itemize}
\item
  Not interested in the causal interpretation of \(\pi_1\) or \(\pi_2\)
\item
  A linear projection of \(z_{i1}\) and \(z_{i2}\) on \(y_{i2}\) (simple correlations)
\item
  The projections \(\pi_1\) and \(\pi_2\) guarantee that \(E(z_{i1}'v_i)=0\) and \(E(z_{i2}'v_i)=0\)
\end{itemize}

Instrumental variable \(z_{i2}\)

\begin{itemize}
\tightlist
\item
  \textbf{Instrument Relevance}: \(\pi_2 \neq 0\)
\item
  \textbf{Instrument Exogeneity}: \(E(\mathbf{z_{i2}\epsilon_i})=0\)
\end{itemize}

Moving only the exogenous part of \(y_i2\) is moving

\[
\tilde{y}_{i2} = \pi_0 + \mathbf{z_{i1}\pi_1 + z_{i2}\pi_2}
\]

\textbf{two Stage Least Squares (2SLS)}

\[
y_{i1} = \beta_0 +\mathbf{z_{i1}\beta_1}+ y_{i2}\beta_2 + \epsilon_i
\]

\[
y_{i2} = \pi_0 + \mathbf{z_{i2}\pi_2} + \mathbf{v_i}
\]

Equivalently,

\begin{equation}
\begin{split}
y_{i1} = \beta_0 + \mathbf{z_{i1}}\beta_1 + \tilde{y}_{i2}\beta_2 + u_i
\end{split}
\label{eq:2SLS}
\end{equation}

where

\begin{itemize}
\tightlist
\item
  \(\tilde{y}_{i2} =\pi_0 + \mathbf{z_{i2}\pi_2}\)
\item
  \(u_i = v_i \beta_2+ \epsilon_i\)
\end{itemize}

The \eqref{eq:2SLS} holds for \protect\hyperlink{a1-linearity}{A1}, \protect\hyperlink{a5-data-generation-random-sampling}{A5}

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{a2-full-rank}{A2} holds if the instrument is relevant \(\pi_2 \neq 0\) + \(y_{i1} = \beta_0 + \mathbf{z_{i1}\beta_1 + (\pi_0 + z_{i1}\pi_1 + z_{i2}\pi_2)}\beta_2 + u_i\)
\item
  {[}A3a{]} holds if the instrument is exogenous \(E(\mathbf{z}_{i2}\epsilon_i)=0\)
\end{itemize}

\[
\begin{aligned}
E(\tilde{y}_{i2}'u_i) &= E((\pi_0 + \mathbf{z_{i1}\pi_1+z_{i2}})(v_i\beta_2 + \epsilon_i)) \\
&= E((\pi_0 + \mathbf{z_{i1}\pi_1+z_{i2}})( \epsilon_i)) \\
&= E(\epsilon_i)\pi_0 + E(\epsilon_iz_{i1})\pi_1 + E(\epsilon_iz_{i2}) \\
&=0 
\end{aligned}
\]

Hence, \eqref{eq:2SLS} is consistent

The 2SLS Estimator\\
1. Estimate the first stage using \protect\hyperlink{ordinary-least-squares}{OLS}

\[
y_{i2} = \pi_0 + \mathbf{z_{i2}\pi_2} + \mathbf{v_i}
\]

and obtained estimated value \(\hat{y}_{i2}\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Estimate the altered equation using \protect\hyperlink{ordinary-least-squares}{OLS}
\end{enumerate}

\[
y_{i1} = \beta_0 +\mathbf{z_{i1}\beta_1}+ \hat{y}_{i2}\beta_2 + \epsilon_i
\]

\textbf{Properties of the 2SLS Estimator}

\begin{itemize}
\tightlist
\item
  Under \protect\hyperlink{a1-linearity}{A1}, \protect\hyperlink{a2-full-rank}{A2}, {[}A3a{]} (for \(z_{i1}\)), \protect\hyperlink{a5-data-generation-random-sampling}{A5} and if the instrument satisfies the following two conditions, + \textbf{Instrument Relevance}: \(\pi_2 \neq 0\) + \textbf{Instrument Exogeneity}: \(E(\mathbf{z}_{i2}'\epsilon_i) = 0\) then the 2SLS estimator is consistent
\item
  Can handle more than one endogenous variable and more than one instrumental variable
\end{itemize}

\[
\begin{aligned}
y_{i1} &= \beta_0 + z_{i1}\beta_1 + y_{i2}\beta_2 + y_{i3}\beta_3 + \epsilon_i \\
y_{i2} &= \pi_0 + z_{i1}\pi_1 + z_{i2}\pi_2 + z_{i3}\pi_3 + z_{i4}\pi_4 + v_{i2} \\
y_{i3} &= \gamma_0 + z_{i1}\gamma_1 + z_{i2}\gamma_2 + z_{i3}\gamma_3 + z_{i4}\gamma_4 + v_{i3}
\end{aligned}
\]

\begin{verbatim}
    + **IV estimator**: one endogenous variable with a single instrument 
    + **2SLS estimator**: one endogenous variable with multiple instruments 
    + **GMM estimator**: multiple endogenous variables with multiple instruments
    
\end{verbatim}

\begin{itemize}
\item
  Standard errors produced in the second step are not correct

  \begin{itemize}
  \tightlist
  \item
    Because we do not know \(\tilde{y}\) perfectly and need to estimate it in the firs step, we are introducing additional variation
  \item
    We did not have this problem with \protect\hyperlink{feasible-generalized-least-squares}{FGLS} because ``the first stage was orthogonal to the second stage.'' This is generally not true for most multi-step procedure.\\
  \item
    If \protect\hyperlink{a4-homoskedasticity}{A4} does not hold, need to report robust standard errors.
  \end{itemize}
\item
  2SLS is less efficient than OLS and will always have larger standard errors.\\

  \begin{itemize}
  \tightlist
  \item
    First, \(Var(u_i) = Var(v_i\beta_2 + \epsilon_i) > Var(\epsilon_i)\)\\
  \item
    Second, \(\hat{y}_{i2}\) is generally highly collinear with \(\mathbf{z}_{i1}\)
  \end{itemize}
\item
  The number of instruments need to be at least as many or more the number of endogenous variables.
\end{itemize}

\textbf{Note}

\begin{itemize}
\tightlist
\item
  2SLS can be combined with \protect\hyperlink{feasible-generalized-least-squares}{FGLS} to make the estimator more efficient: You have the same first-stage, and in the second-stage, instead of using OLS, you can use FLGS with the weight matrix \(\hat{w}\)
\item
  Generalized Method of Moments can be more efficient than 2SLS.
\item
  In the second-stage of 2SLS, you can also use \protect\hyperlink{maximum-likelihood-estimator}{MLE}, but then you are making assumption on the distribution of the outcome variable, the endogenous variable, and their relationship (joint distribution).
\end{itemize}

\hypertarget{testing-assumptions-1}{%
\paragraph{Testing Assumptions}\label{testing-assumptions-1}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \protect\hyperlink{endogeneity-test}{Endogeneity Test}: Is \(y_{i2}\) truly endogenous (i.e., can we just use OLS instead of 2SLS)?
\item
  \protect\hyperlink{exogeneity}{Exogeneity} (Cannot always test and when you can it might not be informative)
\item
  \protect\hyperlink{relevancy}{Relevancy} (need to avoid ``weak instruments'')
\end{enumerate}

\hypertarget{endogeneity-test}{%
\subparagraph{Endogeneity Test}\label{endogeneity-test}}

\begin{itemize}
\item
  2SLS is generally so inefficient that we may prefer OLS if there is not much endogeneity
\item
  Biased but inefficient vs efficient but biased
\item
  Want a sense of ``how endogenous'' \(y_{i2}\) is

  \begin{itemize}
  \tightlist
  \item
    if ``very'' endogenous - should use 2SLS
  \item
    if not ``very'' endogenous - perhaps prefer OLS
  \end{itemize}
\end{itemize}

\textbf{Invalid} Test of Endogeneity: \(y_{i2}\) is endogenous if it is correlated with \(\epsilon_i\),

\[
\epsilon_i = \gamma_0 + y_{i2}\gamma_1 + error_i
\]

where \(\gamma_1 \neq 0\) implies that there is endogeneity

\begin{itemize}
\tightlist
\item
  \(\epsilon_i\) is not observed, but using the residuals
\end{itemize}

\[
e_i = \gamma_0 + y_{i2}\gamma_1 + error_i
\]

is \textbf{NOT} a valid test of endogeneity + The OLS residual, e is mechanically uncorrelated with \(y_{i2}\) (by FOC for OLS) + In every situation, \(\gamma_1\) will be essentially 0 and you will never be able to reject the null of no endogeneity

\textbf{Valid} test of endogeneity

\begin{itemize}
\tightlist
\item
  If \(y_{i2}\) is not endogenous then \(\epsilon_i\) and v are uncorrelated
\end{itemize}

\[
\begin{aligned}
y_{i1} &= \beta_0 + \mathbf{z}_{i1}\beta_1 + y_{i2}\beta_2 + \epsilon_i \\
y_{i2} &= \pi_0 + \mathbf{z}_{i1}\pi_1 + z_{i2}\pi_2 + v_i
\end{aligned}
\]

\textbf{Variable Addition test}: include the first stage residuals as an additional variable,

\[
y_{i1} = \beta_0 + \mathbf{z}_{i1}\beta_1 + y_{i2}\beta_2 + \hat{v}_i \theta + error_i
\]

Then the usual \(t\)-test of significance is a valid test to evaluate the following hypothesis. \textbf{note} this test requires your instrument to be valid instrument.

\[
\begin{aligned}
&H_0: \theta = 0 & \text{  (not endogenous)} \\
&H_1: \theta \neq 0 & \text{  (endogenous)}
\end{aligned}
\]

\hypertarget{exogeneity}{%
\subparagraph{Exogeneity}\label{exogeneity}}

Why exogeneity matter?

\[
E(\mathbf{z}_{i2}'\epsilon_i) = 0
\]

\begin{itemize}
\tightlist
\item
  If {[}A3a{]} fails - 2SLS is also inconsistent
\item
  If instrument is not exogenous, then we need to find a new one.
\item
  Similar to \protect\hyperlink{endogeneity-test}{Endogeneity Test}, when there is a single instrument
\end{itemize}

\[
\begin{aligned}
e_i &= \gamma_0 + \mathbf{z}_{i2}\gamma_1 + error_i \\
H_0: \gamma_1 &= 0
\end{aligned}
\]

is \textbf{NOT} a valid test of endogeneity

\begin{itemize}
\tightlist
\item
  the OLS residual, e is mechanically uncorrelated with \(z_{i2}\): \(\hat{\gamma}_1\) will be essentially 0 and you will never be able to determine if the instrument is endogenous.
\end{itemize}

\textbf{Solution}

Testing Instrumental Exogeneity in an Over-identified Model

\begin{itemize}
\item
  When there is more than one exogenous instrument (per endogenous variable), we can test for instrument exogeneity.

  \begin{itemize}
  \item
    When we have multiple instruments, the model is said to be over-identified.
  \item
    Could estimate the same model several ways (i.e., can identify/ estimate \(\beta_1\) more than one way)
  \end{itemize}
\item
  Idea behind the test: if the controls and instruments are truly exogenous then OLS estimation of the following regression,
\end{itemize}

\[
\epsilon_i = \gamma_0 + \mathbf{z}_{i1}\gamma_1 + \mathbf{z}_{i2}\gamma_2 + error_i
\]

should have a very low \(R^2\)

\begin{itemize}
\tightlist
\item
  if the model is \textbf{just identified} (one instrument per endogenous variable) then the \(R^2 = 0\)
\end{itemize}

Steps:

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\item
  Estimate the structural equation by 2SLS (using all available instruments) and obtain the residuals e
\item
  Regress e on all controls and instruments and obtain the \(R^2\)
\item
  Under the null hypothesis (all IV's are uncorrelated), \(nR^2 \sim \chi^2(q)\), where q is the number of instrumental variables minus the number of endogenous variables

  \begin{itemize}
  \tightlist
  \item
    if the model is just identified (one instrument per endogenous variable) then q = 0, and the distribution under the null collapses.
  \end{itemize}
\end{enumerate}

low p-value means you reject the null of exogenous instruments. Hence you would like to have high p-value in this test.

\textbf{Pitfalls for the Overid test}

\begin{itemize}
\item
  the overid test is essentially compiling the following information.

  \begin{itemize}
  \tightlist
  \item
    Conditional on first instrument being exogenous is the other instrument exogenous?
  \item
    Conditional on the other instrument being exogenous, is the first instrument exogenous?
  \end{itemize}
\item
  If all instruments are endogenous than neither test will be valid
\item
  really only useful if one instrument is thought to be truly exogenous (randomly assigned). even f you do reject the null, the test does not tell you which instrument is exogenous and which is endogenous.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.7500}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Result
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Implication
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
reject the null & you can be pretty sure there is an endogenous instrument, but don't know which one. \\
fail to reject & could be either (1) they are both exogenous, (2) they are both endogenous. \\
\end{longtable}

\hypertarget{relevancy}{%
\subparagraph{Relevancy}\label{relevancy}}

Why Relevance matter?

\[
\pi_2 \neq 0 
\]

\begin{itemize}
\item
  used to show \protect\hyperlink{a2-full-rank}{A2} holds

  \begin{itemize}
  \item
    If \(\pi_2 = 0\) (instrument is not relevant) then \protect\hyperlink{a2-full-rank}{A2} fails - perfect multicollinearity
  \item
    If \(\pi_2\) is close to 0 (\textbf{weak instrument}) then there is near perfect multicollinearity - 2SLS is highly inefficient (Large standard errors).
  \end{itemize}
\item
  A weak instrument will exacerbate any inconsistency due to an instrument being (even slightly) endogenous.

  \begin{itemize}
  \tightlist
  \item
    In the simple case with no controls and a single endogenous variable and single instrumental variable,
  \end{itemize}
\end{itemize}

\[
plim(\hat{\beta}_{2_{2SLS}}) = \beta_2 + \frac{E(z_{i2}\epsilon_i)}{E(z_{i2}y_{i2})}
\]

\textbf{Testing Weak Instruments}

\begin{itemize}
\item
  can use \(t\)-test (or \(F\)-test for over-identified models) in the first stage to determine if there is a weak instrument problem.
\item
  \citep[\citet{stock2005asymptotic}]{stock2002testing}: a statistical rejection of the null hypothesis in the first stage at the 5\% (or even 1\%) level is not enough to insure the instrument is not weak

  \begin{itemize}
  \tightlist
  \item
    Rule of Thumb: need a \(F\)-stat of at least 10 (or a \(t\)-stat of at least 3.2) to reject the null hypothesis that the instrument is weak.
  \end{itemize}
\end{itemize}

\textbf{Summary of the 2SLS Estimator}

\[
\begin{aligned}
y_{i1} &=\beta_0 + \mathbf{z}_{i1}\beta_1 + y_{i2}\beta_2 + \epsilon_i \\
y_{i2} &= \pi_0 + \mathbf{z_{i1}\pi_1} + \mathbf{z_{i2}\pi_2} + v_i
\end{aligned}
\]

\begin{itemize}
\tightlist
\item
  when {[}A3a{]} does not hold
\end{itemize}

\[
E(y_{i2}'\epsilon_i) \neq 0
\]

\begin{itemize}
\item
  Then the OLS estimator is no longer unbiased or consistent.
\item
  If we have valid instruments \(\mathbf{z}_{i2}\)
\item
  \protect\hyperlink{relevancy}{Relevancy} (need to avoid ``weak instruments''): \(\pi_2 \neq 0\) Then the 2SLS estimator is consistent under \protect\hyperlink{a1-linearity}{A1}, \protect\hyperlink{a2-full-rank}{A2}, {[}A5a{]}, and the above two conditions.

  \begin{itemize}
  \item
    If \protect\hyperlink{a4-homoskedasticity}{A4} also holds, then the usual standard errors are valid.
  \item
    If \protect\hyperlink{a4-homoskedasticity}{A4} does not hold then use the robust standard errors.
  \end{itemize}
\end{itemize}

\[
\begin{aligned}
y_{i1} &= \beta_0 + \mathbf{z}_{i1}\beta_1 + y_{i2}\beta_2 + \epsilon_i \\
y_{i2} &= \pi_0 + \mathbf{z_{i1}\pi_1} + \mathbf{z_{i2}\pi_2} + v_i
\end{aligned}
\]

\begin{itemize}
\tightlist
\item
  When {[}A3a{]} does hold
\end{itemize}

\[
E(y_{i2}'\epsilon_i) = 0
\]

and we have valid instruments, then both the OLS and 2SLS estimators are consistent.

\begin{itemize}
\tightlist
\item
  The OLS estimator is always more efficient
\item
  can use the variable addition test to determine if 2SLS is need (A3a does hold) or if OLS is valid (A3a does not hold)
\end{itemize}

Sometimes we can test the assumption for instrument to be valid:

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{exogeneity}{Exogeneity} : Only table when there are more instruments than endogenous variables.
\item
  \protect\hyperlink{relevancy}{Relevancy} (need to avoid ``weak instruments''): Always testable, need the F-stat to be greater than 10 to rule out a weak instrument
\end{itemize}

Application

Expenditure as observed instrument

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m2}\FloatTok{.2}\NormalTok{sls }\OtherTok{\textless{}{-}}
    \FunctionTok{ivreg}\NormalTok{(}
\NormalTok{        read }\SpecialCharTok{\textasciitilde{}}\NormalTok{ stratio }\SpecialCharTok{+}\NormalTok{ english }\SpecialCharTok{+}\NormalTok{ lunch }
        \SpecialCharTok{+}\NormalTok{ grades }\SpecialCharTok{+}\NormalTok{ income }\SpecialCharTok{+}\NormalTok{ calworks }\SpecialCharTok{+}\NormalTok{ county }\SpecialCharTok{|}
            
\NormalTok{            expenditure }\SpecialCharTok{+}\NormalTok{ english }\SpecialCharTok{+}\NormalTok{ lunch }
        \SpecialCharTok{+}\NormalTok{ grades }\SpecialCharTok{+}\NormalTok{ income }\SpecialCharTok{+}\NormalTok{ calworks }\SpecialCharTok{+}\NormalTok{ county ,}
        \AttributeTok{data =}\NormalTok{ school}
\NormalTok{    )}
\FunctionTok{summary}\NormalTok{(m2}\FloatTok{.2}\NormalTok{sls)}\SpecialCharTok{$}\NormalTok{coefficients[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{7}\NormalTok{,]}
\CommentTok{\#\textgreater{}                 Estimate  Std. Error     t value      Pr(\textgreater{}|t|)}
\CommentTok{\#\textgreater{} (Intercept) 700.47891593 13.58064436  51.5792106 8.950497e{-}171}
\CommentTok{\#\textgreater{} stratio      {-}1.13674002  0.53533638  {-}2.1234126  3.438427e{-}02}
\CommentTok{\#\textgreater{} english      {-}0.21396934  0.03847833  {-}5.5607753  5.162571e{-}08}
\CommentTok{\#\textgreater{} lunch        {-}0.39384225  0.03773637 {-}10.4366757  1.621794e{-}22}
\CommentTok{\#\textgreater{} gradesKK{-}08  {-}1.89227865  1.37791820  {-}1.3732881  1.704966e{-}01}
\CommentTok{\#\textgreater{} income        0.62487986  0.11199008   5.5797785  4.668490e{-}08}
\CommentTok{\#\textgreater{} calworks     {-}0.04950501  0.06244410  {-}0.7927892  4.284101e{-}01}
\end{Highlighting}
\end{Shaded}

\hypertarget{checklist}{%
\paragraph{Checklist}\label{checklist}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Regress the dependent variable on the instrument (reduced form). Since under OLS, we have unbiased estimate, the coefficient estimate should be significant (make sure the sign makes sense)
\item
  Report F-stat on the excluded instruments. F-stat \textless{} 10 means you have a weak instrument \citep{stock2002survey}.
\item
  Present \(R^2\) before and after including the instrument \citep{rossi2014even}
\item
  For models with multiple instrument, present firs-t and second-stage result for each instrument separately. Overid test should be conducted (e.g., Sargan-Hansen J)
\item
  Hausman test between OLS and 2SLS (don't confuse this test for evidence that endogeneity is irrelevant - under invalid IV, the test is useless)
\item
  Compare the 2SLS with the limited information ML. If they are different, you have evidence for weak instruments.
\end{enumerate}

\hypertarget{good-instruments}{%
\subsubsection{Good Instruments}\label{good-instruments}}

\protect\hyperlink{exogeneity}{Exogeneity} and \protect\hyperlink{relevancy}{Relevancy} are necessary but not sufficient for IV to produce consistent estimates.

Without theory or possible explanation, you can always create a new variable that is correlated with \(X\) and uncorrelated with \(\epsilon\)

For example, we want to estimate the effect of price on quantity \citep[p.~960]{reiss2011structural}

\[
\begin{aligned}
Q &= \beta_1 P + \beta_2 X + \epsilon \\
P &= \pi_1 X + \eta
\end{aligned}
\]

where \(\epsilon\) and \(\eta\) are jointly determined, \(X \perp \epsilon, \eta\)

Without theory, we can just create a new variable \(Z = X + u\) where \(E(u) = 0; u \perp X, \epsilon, \eta\)

Then, \(Z\) satisfied both conditions:

\begin{itemize}
\item
  Relevancy: \(X\) correlates \(P\) \(\rightarrow\) \(Z\) correlates \(P\)
\item
  Exogeneity: \(u \perp \epsilon\) (random noise)
\end{itemize}

But obviously, it's not a valid instrument (intuitively). But theoretically, relevance and exogeneity are not sufficient to identify \(\beta\) because of unsatisfied rank condition for identification.

Moreover, the functional form of the instrument also plays a role when choosing a good instrument. Hence, we always need to check for the robustness of our instrument.

IV methods even with valid instruments can still have poor sampling properties (finite sample bias, large sampling errors) \citep{rossi2014even}

When you have a weak instrument, it's important to report it appropriately. This problem will be exacerbated if you have multiple instruments \citep{larcker2010use}.

\hypertarget{lagged-dependent-variable}{%
\paragraph{Lagged dependent variable}\label{lagged-dependent-variable}}

In time series data sets, we can use lagged dependent variable as an instrument because it is not influenced by current shocks. For example, \citet{chetty2014measuring} used lagged dependent variable in econ.

\hypertarget{lagged-explanatory-variable}{%
\paragraph{Lagged explanatory variable}\label{lagged-explanatory-variable}}

\begin{itemize}
\item
  Common practice in applied economics: Replace suspected simultaneously determined explanatory variable with its lagged value \citep[\citet{bellemare2017lagged}]{reed2015practice}.

  \begin{itemize}
  \item
    This practice does not avoid simultaneity bias.
  \item
    Estimates using this method are still inconsistent.
  \item
    Hypothesis testing becomes invalid under this approach.
  \item
    Lagging variables changes how endogeneity bias operates, adding a ``no dynamics among unobservables'' assumption to the ``selection on observables'' assumption.
  \end{itemize}
\item
  Key conditions for appropriate use \citep{bellemare2017lagged}:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Under unobserved confounding:}

    \begin{itemize}
    \tightlist
    \item
      No dynamics among unobservables.
    \item
      The lagged variable \(X\) is a stationary autoregressive process.
    \end{itemize}
  \item
    \textbf{Under no unobserved confounding:}

    \begin{itemize}
    \tightlist
    \item
      No reverse causality; the causal effect operates with a one-period lag (\(X_{t-1} \to Y\), \(X_t \not\to Y_t\)).
    \item
      Reverse causality is contemporaneous, with a one-period lag effect.
    \item
      Reverse causality is contemporaneous; no dynamics in \(Y\), but dynamics exist in \(X\) (\(X_{t-1} \to X\)).
    \end{itemize}
  \end{itemize}
\item
  \textbf{Alternative approach}: Use lagged values of the endogenous variable in IV estimation. However, IV estimation is only effective if \citep{reed2015practice}:

  \begin{itemize}
  \item
    Lagged values do not belong in the estimating equation.
  \item
    Lagged values are sufficiently correlated with the simultaneously determined explanatory variable.
  \item
    Lagged IVs help mitigate endogeneity if they only violate the independence assumption. However, if lagged IVs violate both the independence assumption and exclusion restriction, they may aggravate endogeneity \citep{wang2019lagged}.
  \end{itemize}
\end{itemize}

\hypertarget{internal-instrumental-variable}{%
\subsubsection{Internal instrumental variable}\label{internal-instrumental-variable}}

\begin{itemize}
\item
  (also known as \textbf{instrument free methods}). This section is based on Raluca Gui's \href{https://cran.r-project.org/web/packages/REndo/vignettes/REndo-introduction.pdf}{guide}
\item
  alternative to external instrumental variable approaches
\item
  All approaches here assume a \textbf{continuous dependent variable}
\end{itemize}

\hypertarget{non-hierarchical-data-cross-classified}{%
\paragraph{Non-hierarchical Data (Cross-classified)}\label{non-hierarchical-data-cross-classified}}

\[
Y_t = \beta_0 + \beta_1 P_t + \beta_2 X_t + \epsilon_t
\]

where

\begin{itemize}
\tightlist
\item
  \(t = 1, .., T\) (indexes either time or cross-sectional units)
\item
  \(Y_t\) is a \(k \times 1\) response variable
\item
  \(X_t\) is a \(k \times n\) exogenous regressor
\item
  \(P_t\) is a \(k \times 1\) continuous endogenous regressor
\item
  \(\epsilon_t\) is a structural error term with \(\mu_\epsilon =0\) and \(E(\epsilon^2) = \sigma^2\)
\item
  \(\beta\) are model parameters
\end{itemize}

The endogeneity problem arises from the correlation of \(P_t\) and \(\epsilon_t\):

\[
P_t = \gamma Z_t + v_t
\]

where

\begin{itemize}
\tightlist
\item
  \(Z_t\) is a \(l \times 1\) vector of internal instrumental variables
\item
  \(_t\) is a random error with \(\mu_{v_t}, E(v^2) = \sigma^2_v, E(\epsilon v) = \sigma_{\epsilon v}\)
\item
  \(Z_t\) is assumed to be stochastic with distribution \(G\)
\item
  \(_t\) is assumed to have density \(h()\)
\end{itemize}

\hypertarget{latent-instrumental-variable}{%
\subparagraph{Latent Instrumental Variable}\label{latent-instrumental-variable}}

\citep{ebbes2005solving}

assume \(Z_t\) (unobserved) to be uncorrelated with \(\epsilon_t\), which is similar to \protect\hyperlink{instrumental-variable}{Instrumental Variable}. Hence, \(Z_t\) and \(_t\) can't be identified without distributional assumptions

The distributions of \(Z_t\) and \(_t\) need to be specified such that:

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  endogeneity of \(P_t\) is corrected
\item
  the distribution of \(P_t\) is empirically close to the integral that expresses the amount of overlap of Z as it is shifted over  (= the convolution between \(Z_t\) and \(_t\)).
\end{enumerate}

When the density h() = Normal, then G cannot be normal because the parameters would not be identified \citep{ebbes2005solving} .

Hence,

\begin{itemize}
\tightlist
\item
  in the \protect\hyperlink{latent-instrumental-variable}{LIV} model the distribution of \(Z_t\) is discrete
\item
  in the \protect\hyperlink{higher-moments-method}{Higher Moments Method} and \protect\hyperlink{joint-estimation-using-copula}{Joint Estimation Using Copula} methods, the distribution of \(Z_t\) is taken to be skewed.
\end{itemize}

\(Z_t\) are assumed \textbf{unobserved, discrete and exogenous}, with

\begin{itemize}
\tightlist
\item
  an unknown number of groups m
\item
  \(\gamma\) is a vector of group means.
\end{itemize}

Identification of the parameters relies on the distributional assumptions of

\begin{itemize}
\tightlist
\item
  \(P_t\): a non-Gaussian distribution
\item
  \(Z_t\) discrete with \(m \ge 2\)
\end{itemize}

Note:

\begin{itemize}
\tightlist
\item
  If \(Z_t\) is continuous, the model is unidentified
\item
  If \(P_t \sim N\), you have inefficient estimates.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m3.liv }\OtherTok{\textless{}{-}} \FunctionTok{latentIV}\NormalTok{(read }\SpecialCharTok{\textasciitilde{}}\NormalTok{ stratio, }\AttributeTok{data =}\NormalTok{ school)}
\FunctionTok{summary}\NormalTok{(m3.liv)}\SpecialCharTok{$}\NormalTok{coefficients[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{7}\NormalTok{, ]}
\CommentTok{\#\textgreater{}                   Estimate    Std. Error       z{-}score     Pr(\textgreater{}|z|)}
\CommentTok{\#\textgreater{} (Intercept)   6.996014e+02  2.686186e+02  2.604441e+00 9.529597e{-}03}
\CommentTok{\#\textgreater{} stratio      {-}2.272673e+00  1.367757e+01 {-}1.661605e{-}01 8.681108e{-}01}
\CommentTok{\#\textgreater{} pi1          {-}4.896363e+01  5.526907e{-}08 {-}8.859139e+08 0.000000e+00}
\CommentTok{\#\textgreater{} pi2           1.963920e+01  9.225351e{-}02  2.128830e+02 0.000000e+00}
\CommentTok{\#\textgreater{} theta5       6.939432e{-}152 3.354672e{-}160  2.068587e+08 0.000000e+00}
\CommentTok{\#\textgreater{} theta6        3.787512e+02  4.249457e+01  8.912932e+00 1.541524e{-}17}
\CommentTok{\#\textgreater{} theta7       {-}1.227543e+00  4.885276e+01 {-}2.512741e{-}02 9.799653e{-}01}
\end{Highlighting}
\end{Shaded}

it will return a coefficient very different from the other methods since there is only one endogenous variable.

\hypertarget{joint-estimation-using-copula}{%
\subparagraph{Joint Estimation Using Copula}\label{joint-estimation-using-copula}}

assume \(Z_t\) (unobserved) to be uncorrelated with \(\epsilon_t\), which is similar to \protect\hyperlink{instrumental-variable}{Instrumental Variable}. Hence, \(Z_t\) and \(_t\) can't be identified without distributional assumptions

\citep{park2012handling} allows joint estimation of the continuous \(P_t\) and \(\epsilon_t\) using Gaussian copulas, where a copula is a function that maps several conditional distribution functions (CDF) into their joint CDF).

The underlying idea is that using information contained in the observed data, one selects marginal distributions for \(P_t\) and \(\epsilon_t\). Then, the copula model constructs a flexible multivariate joint distribution that allows a wide range of correlations between the two marginals.

The method allows both continuous and discrete \(P_t\).

In the special case of \textbf{one continuous} \(P_t\), estimation is based on MLE\\
Otherwise, based on Gaussian copulas, augmented OLS estimation is used.

\textbf{Assumptions}:

\begin{itemize}
\item
  skewed \(P_t\)
\item
  the recovery of the correct parameter estimates
\item
  \(\epsilon_t \sim\) normal marginal distribution. The marginal distribution of \(P_t\) is obtained using the \textbf{Epanechnikov kernel density estimator}\\
  \[
  \hat{h}_p = \frac{1}{T . b} \sum_{t=1}^TK(\frac{p - P_t}{b})
  \] where
\item
  \(P_t\) = endogenous variables
\item
  \(K(x) = 0.75(1-x^2)I(||x||\le 1)\)
\item
  \(b=0.9T^{-1/5}\times min(s, IQR/1.34)\)

  \begin{itemize}
  \tightlist
  \item
    IQR = interquartile range
  \item
    \(s\) = sample standard deviation
  \item
    \(T\) = n of time periods observed in the data
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 1.34 comes from this}
\FunctionTok{diff}\NormalTok{(}\FunctionTok{qnorm}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FloatTok{0.25}\NormalTok{, }\FloatTok{0.75}\NormalTok{)))}
\CommentTok{\#\textgreater{} [1] 1.34898}
\end{Highlighting}
\end{Shaded}

In augmented OLS and MLE, the inference procedure occurs in two stages:

(1): the empirical distribution of \(P_t\) is computed\\
(2) used in it constructing the likelihood function)\\
Hence, the standard errors would not be correct.

So we use the sampling distributions (from bootstrapping) to get standard errors and the variance-covariance matrix. Since the distribution of the bootstrapped parameters is highly skewed, we report the percentile confidence intervals is preferable.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{110}\NormalTok{)}
\NormalTok{m4.cc }\OtherTok{\textless{}{-}}
    \FunctionTok{copulaCorrection}\NormalTok{(}
\NormalTok{        read }\SpecialCharTok{\textasciitilde{}}\NormalTok{ stratio }\SpecialCharTok{+}\NormalTok{ english }\SpecialCharTok{+}\NormalTok{ lunch }\SpecialCharTok{+}\NormalTok{ calworks }\SpecialCharTok{+}
\NormalTok{            grades }\SpecialCharTok{+}\NormalTok{ income }\SpecialCharTok{+}\NormalTok{ county }\SpecialCharTok{|}
            \FunctionTok{continuous}\NormalTok{(stratio),}
        \AttributeTok{data =}\NormalTok{ school,}
        \AttributeTok{optimx.args =} \FunctionTok{list}\NormalTok{(}\AttributeTok{method =} \FunctionTok{c}\NormalTok{(}\StringTok{"Nelder{-}Mead"}\NormalTok{), }
                           \AttributeTok{itnmax =} \DecValTok{60000}\NormalTok{),}
        \AttributeTok{num.boots =} \DecValTok{2}\NormalTok{,}
        \AttributeTok{verbose =} \ConstantTok{FALSE}
\NormalTok{    )}
\FunctionTok{summary}\NormalTok{(m4.cc)}\SpecialCharTok{$}\NormalTok{coefficients[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{7}\NormalTok{,]}
\CommentTok{\#\textgreater{}             Point Estimate   Boots SE Lower Boots CI (95\%) Upper Boots CI (95\%)}
\CommentTok{\#\textgreater{} (Intercept)   683.06900891 2.80554212                   NA                   NA}
\CommentTok{\#\textgreater{} stratio        {-}0.32434608 0.02075999                   NA                   NA}
\CommentTok{\#\textgreater{} english        {-}0.21576110 0.01450666                   NA                   NA}
\CommentTok{\#\textgreater{} lunch          {-}0.37087664 0.01902052                   NA                   NA}
\CommentTok{\#\textgreater{} calworks       {-}0.05569058 0.02076781                   NA                   NA}
\CommentTok{\#\textgreater{} gradesKK{-}08    {-}1.92286128 0.25684614                   NA                   NA}
\CommentTok{\#\textgreater{} income          0.73595353 0.04725700                   NA                   NA}
\end{Highlighting}
\end{Shaded}

we run this model with only one endogenous continuous regressor (\texttt{stratio}). Sometimes, the code will not converge, in which case you can use different

\begin{itemize}
\tightlist
\item
  optimization algorithm
\item
  starting values
\item
  maximum number of iterations
\end{itemize}

\hypertarget{higher-moments-method}{%
\subparagraph{Higher Moments Method}\label{higher-moments-method}}

suggested by \citep{lewbel1997constructing} to identify \(\epsilon_t\) caused by \textbf{measurement error}.

Identification is achieved by using third moments of the data, with no restrictions on the distribution of \(\epsilon_t\)\\
The following instruments can be used with 2SLS estimation to obtain consistent estimates:

\[
\begin{aligned}
q_{1t} &=  (G_t - \bar{G}) \\
q_{2t} &=  (G_t - \bar{G})(P_t - \bar{P}) \\
q_{3t} &=   (G_t - \bar{G})(Y_t - \bar{Y})\\
q_{4t} &=  (Y_t - \bar{Y})(P_t - \bar{P}) \\
q_{5t} &=  (P_t - \bar{P})^2 \\
q_{6t} &=  (Y_t - \bar{Y})^2 \\
\end{aligned}
\]

where

\begin{itemize}
\tightlist
\item
  \(G_t = G(X_t)\) for any given function G that has finite third own and cross moments
\item
  \(X\) = exogenous variable
\end{itemize}

\(q_{5t}, q_{6t}\) can be used only when the measurement and \(\epsilon_t\) are symmetrically distributed. The rest of the instruments does not require any distributional assumptions for \(\epsilon_t\).

Since the regressors \(G(X) = X\) are included as instruments, \(G(X)\) can't be a linear function of X in \(q_{1t}\)

Since this method has very strong assumptions, \protect\hyperlink{higher-moments-method}{Higher Moments Method} should only be used in case of overidentification

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{111}\NormalTok{)}
\NormalTok{m5.hetEr }\OtherTok{\textless{}{-}}
    \FunctionTok{hetErrorsIV}\NormalTok{(}
\NormalTok{        read }\SpecialCharTok{\textasciitilde{}}\NormalTok{ stratio }\SpecialCharTok{+}\NormalTok{ english }\SpecialCharTok{+}\NormalTok{ lunch }\SpecialCharTok{+}\NormalTok{ calworks }\SpecialCharTok{+}\NormalTok{ income }\SpecialCharTok{+}
\NormalTok{            grades }\SpecialCharTok{+}\NormalTok{ county }\SpecialCharTok{|}
\NormalTok{            stratio }\SpecialCharTok{|} \FunctionTok{IIV}\NormalTok{(income, english),}
        \AttributeTok{data =}\NormalTok{ school}
\NormalTok{    )}
\FunctionTok{summary}\NormalTok{(m5.hetEr)}\SpecialCharTok{$}\NormalTok{coefficients[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{7}\NormalTok{,]}
\CommentTok{\#\textgreater{}                 Estimate  Std. Error    t value     Pr(\textgreater{}|t|)}
\CommentTok{\#\textgreater{} (Intercept) 662.78791557 27.90173069 23.7543657 2.380436e{-}76}
\CommentTok{\#\textgreater{} stratio       0.71480686  1.31077325  0.5453322 5.858545e{-}01}
\CommentTok{\#\textgreater{} english      {-}0.19522271  0.04057527 {-}4.8113717 2.188618e{-}06}
\CommentTok{\#\textgreater{} lunch        {-}0.37834232  0.03927793 {-}9.6324402 9.760809e{-}20}
\CommentTok{\#\textgreater{} calworks     {-}0.05665126  0.06302095 {-}0.8989273 3.692776e{-}01}
\CommentTok{\#\textgreater{} income        0.82693755  0.17236557  4.7975797 2.335271e{-}06}
\CommentTok{\#\textgreater{} gradesKK{-}08  {-}1.93795843  1.38723186 {-}1.3969968 1.632541e{-}01}
\end{Highlighting}
\end{Shaded}

recommend using this approach to create additional instruments to use with external ones for better efficiency.

\hypertarget{heteroskedastic-error-approach}{%
\subparagraph{Heteroskedastic Error Approach}\label{heteroskedastic-error-approach}}

\begin{itemize}
\tightlist
\item
  using means of variables that are uncorrelated with the product of heteroskedastic errors to identify structural parameters.
\item
  This method can be use either when you don't have external instruments or you want to use additional instruments to improve the efficiency of the IV estimator \citep{lewbel2012using}
\item
  The instruments are constructed as simple functions of data
\item
  Model's assumptions:
\end{itemize}

\[
\begin{aligned}
E(X \epsilon) &= 0 \\
E(X v ) &= 0 \\
cov(Z, \epsilon v) &= 0  \\
cov(Z, v^2) &\neq 0 \text{  (for identification)}
\end{aligned}
\]

Structural parameters are identified by 2SLS regression of Y on X and P, using X and {[}Z  E(Z){]} as instruments.

\[
\text{instrument's strength} \propto cov((Z-\bar{Z})v,v)
\]

where \(cov((Z-\bar{Z})v,v)\) is the degree of heteroskedasticity of  with respect to Z \citep{lewbel2012using}, which can be empirically tested.

If it is zero or close to zero (i.e.,the instrument is weak), you might have imprecise estimates, with large standard errors.

\begin{itemize}
\tightlist
\item
  Under homoskedasticity, the parameters of the model are unidentified.
\item
  Under heteroskedasticity related to at least some elements of X, the parameters of the model are identified.
\end{itemize}

\hypertarget{hierarchical-data}{%
\paragraph{Hierarchical Data}\label{hierarchical-data}}

Multiple independent assumptions involving various random components at different levels mean that any moderate correlation between some predictors and a random component or error term can result in a significant bias of the coefficients and of the variance components. \citep{kim2007multilevel} proposed a generalized method of moments which uses both, the between and within variations of the exogenous variables, but only assumes the within variation of the variables to be endogenous.

\textbf{Assumptions}

\begin{itemize}
\tightlist
\item
  the errors at each level \(\sim iid N\)
\item
  the slope variables are exogenous
\item
  the level-1 \(\epsilon \perp X, P\). If this is not the case, additional, external instruments are necessary
\end{itemize}

\textbf{Hierarchical Model}

\[
\begin{aligned}
Y_{cst} &= Z_{cst}^1 \beta_{cs}^1 + X_{cst}^1 \beta_1 + \epsilon_{cst}^1 \\
\beta^1_{cs} &= Z_{cs}^2 \beta_{c}^2 + X_{cst}^2 \beta_2 + \epsilon_{cst}^2 \\
\beta^2_{c} &= X^3_c \beta_3 + \epsilon_c^3
\end{aligned}
\]

Bias could stem from:

\begin{itemize}
\tightlist
\item
  errors at the higher two levels (\(\epsilon_c^3,\epsilon_{cst}^2\)) are correlated with some of the regressors
\item
  only third level errors (\(\epsilon_c^3\)) are correlated with some of the regressors
\end{itemize}

\citep{kim2007multilevel} proposed

\begin{itemize}
\tightlist
\item
  When all variables are assumed exogenous, the proposed estimator equals the random effects estimator
\item
  When all variables are assumed endogenous, it equals the fixed effects estimator
\item
  also use omitted variable test (based on the Hausman-test \citep{hausman1978specification} for panel data), which allows the comparison of a robust estimator and an estimator that is efficient under the null hypothesis of no omitted variables or the comparison of two robust estimators at different levels.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# function \textquotesingle{}cholmod\_factor\_ldetA\textquotesingle{} not provided by package \textquotesingle{}Matrix\textquotesingle{}}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{113}\NormalTok{)}
\NormalTok{school}\SpecialCharTok{$}\NormalTok{gr08 }\OtherTok{\textless{}{-}}\NormalTok{ school}\SpecialCharTok{$}\NormalTok{grades }\SpecialCharTok{==} \StringTok{"KK{-}06"}
\NormalTok{m7.multilevel }\OtherTok{\textless{}{-}}
    \FunctionTok{multilevelIV}\NormalTok{(read }\SpecialCharTok{\textasciitilde{}}\NormalTok{ stratio }\SpecialCharTok{+}\NormalTok{ english }\SpecialCharTok{+}\NormalTok{ lunch }\SpecialCharTok{+}\NormalTok{ income }\SpecialCharTok{+}\NormalTok{ gr08 }\SpecialCharTok{+}
\NormalTok{                     calworks }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ county) }\SpecialCharTok{|} \FunctionTok{endo}\NormalTok{(stratio),}
                 \AttributeTok{data =}\NormalTok{ school)}
\FunctionTok{summary}\NormalTok{(m7.multilevel)}\SpecialCharTok{$}\NormalTok{coefficients[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{7}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

Another example using simulated data

\begin{itemize}
\tightlist
\item
  level-1 regressors: \(X_{11}, X_{12}, X_{13}, X_{14}, X_{15}\), where \(X_{15}\) is correlated with the level-2 error (i.e., endogenous).\\
\item
  level-2 regressors: \(X_{21}, X_{22}, X_{23}, X_{24}\)\\
\item
  level-3 regressors: \(X_{31}, X_{32}, X_{33}\)
\end{itemize}

We estimate a three-level model with X15 assumed endogenous. Having a three-level hierarchy, \texttt{multilevelIV()} returns five estimators, from the most robust to omitted variables (FE\_L2), to the most efficient (REF) (i.e.~lowest mean squared error).

\begin{itemize}
\tightlist
\item
  The random effects estimator (REF) is efficient assuming no omitted variables
\item
  The fixed effects estimator (FE) is unbiased and asymptotically normal even in the presence of omitted variables.
\item
  Because of the efficiency, the random effects estimator is preferable if you think there is no omitted. variables
\item
  The robust estimator would be preferable if you think there is omitted variables.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# function \textquotesingle{}cholmod\_factor\_ldetA\textquotesingle{} not provided by package \textquotesingle{}Matrix\textquotesingle{}}
\FunctionTok{data}\NormalTok{(dataMultilevelIV)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{114}\NormalTok{)}
\NormalTok{formula1 }\OtherTok{\textless{}{-}}
\NormalTok{    y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X11 }\SpecialCharTok{+}\NormalTok{ X12 }\SpecialCharTok{+}\NormalTok{ X13 }\SpecialCharTok{+}\NormalTok{ X14 }\SpecialCharTok{+}\NormalTok{ X15 }\SpecialCharTok{+}\NormalTok{ X21 }\SpecialCharTok{+}\NormalTok{ X22 }\SpecialCharTok{+}\NormalTok{ X23 }\SpecialCharTok{+}\NormalTok{ X24 }\SpecialCharTok{+}
\NormalTok{    X31 }\SpecialCharTok{+}\NormalTok{ X32 }\SpecialCharTok{+}\NormalTok{ X33 }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ CID) }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ SID) }\SpecialCharTok{|} \FunctionTok{endo}\NormalTok{(X15)}
\NormalTok{m8.multilevel }\OtherTok{\textless{}{-}}
    \FunctionTok{multilevelIV}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ formula1, }\AttributeTok{data =}\NormalTok{ dataMultilevelIV)}
\FunctionTok{coef}\NormalTok{(m8.multilevel)}

\FunctionTok{summary}\NormalTok{(m8.multilevel, }\StringTok{"REF"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

True \(\beta_{X_{15}} =-1\). We can see that some estimators are bias because \(X_{15}\) is correlated with the level-two error, to which only FE\_L2 and GMM\_L2 are robust

To select the appropriate estimator, we use the omitted variable test.

In a three-level setting, we can have different estimator comparisons:

\begin{itemize}
\tightlist
\item
  Fixed effects vs.~random effects estimators: Test for omitted level-two and level-three omitted effects, simultaneously, one compares FE\_L2 to REF. But we will not know at which omitted variables exist.\\
\item
  Fixed effects vs.~GMM estimators: Once the existence of omitted effects is established but not sure at which level, we test for level-2 omitted effects by comparing FE\_L2 vs GMM\_L3. If you reject the null, the omitted variables are at level-2 The same is accomplished by testing FE\_L2 vs.~GMM\_L2, since the latter is consistent only if there are no omitted effects at level-2.\\
\item
  Fixed effects vs.~fixed effects estimators: We can test for omitted level-2 effects, while allowing for omitted level-3 effects by comparing FE\_L2 vs.~FE\_L3 since FE\_L2 is robust against both level-2 and level-3 omitted effects while FE\_L3 is only robust to level-3 omitted variables.
\end{itemize}

Summary, use the omitted variable test comparing \texttt{REF\ vs.\ FE\_L2} first.

\begin{itemize}
\item
  If the null hypothesis is rejected, then there are omitted variables either at level-2 or level-3
\item
  Next, test whether there are level-2 omitted effects, since testing for omitted level three effects relies on the assumption there are no level-two omitted effects. You can use any of these pair of comparisons:

  \begin{itemize}
  \tightlist
  \item
    \texttt{FE\_L2\ vs.\ FE\_L3}
  \item
    \texttt{FE\_L2\ vs.\ GMM\_L2}
  \end{itemize}
\item
  If no omitted variables at level-2 are found, test for omitted level-3 effects by comparing either

  \begin{itemize}
  \tightlist
  \item
    \texttt{FE\_L3} vs.~\texttt{GMM\_L3}
  \item
    \texttt{GMM\_L2} vs.~\texttt{GMM\_L3}
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(m8.multilevel, }\StringTok{"REF"}\NormalTok{)}
\CommentTok{\# compare REF with all the other estimators. Testing REF (the most efficient estimator) against FE\_L2 (the most robust estimator), equivalently we are testing simultaneously for level{-}2 and level{-}3 omitted effects. }
\end{Highlighting}
\end{Shaded}

Since the null hypothesis is rejected (p = 0.000139), there is bias in the random effects estimator.

To test for level-2 omitted effects (regardless of level-3 omitted effects), we compare FE\_L2 versus FE\_L3

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(m8.multilevel,}\StringTok{"FE\_L2"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The null hypothesis of no omitted level-2 effects is rejected (\(p = 3.92e  05\)). Hence, there are omitted effects at level-two. We should use FE\_L2 which is consistent with the underlying data that we generated (level-2 error correlated with \(X_15\), which leads to biased FE\_L3 coefficients.

The omitted variable test between FE\_L2 and GMM\_L2 should reject the null hypothesis of no omitted level-2 effects (p-value is 0).

If we assume an endogenous variable as exogenous, the RE and GMM estimators will be biased because of the wrong set of internal instrumental variables. To increase our confidence, we should compare the omitted variable tests when the variable is considered endogenous vs.~exogenous to get a sense whether the variable is truly endogenous.

\hypertarget{proxy-variables}{%
\subsubsection{Proxy Variables}\label{proxy-variables}}

\begin{itemize}
\item
  Can be in place of the omitted variable
\item
  will not be able to estimate the effect of the omitted variable
\item
  will be able to reduce some endogeneity caused bye the omitted variable
\item
  but it can have \protect\hyperlink{measurement-error}{Measurement Error}. Hence, you have to be extremely careful when using proxies.
\end{itemize}

Criteria for a proxy variable:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The proxy is correlated with the omitted variable.
\item
  Having the omitted variable in the regression will solve the problem of endogeneity
\item
  The variation of the omitted variable unexplained by the proxy is uncorrelated with all independent variables, including the proxy.
\end{enumerate}

IQ test can be a proxy for ability in the regression between wage explained education.

For the third requirement

\[
ability = \gamma_0 + \gamma_1 IQ + \epsilon
\]

where \(\epsilon\) is uncorrelated with education and IQ test.

\hypertarget{endogenous-sample-selection}{%
\section{Endogenous Sample Selection}\label{endogenous-sample-selection}}

\begin{itemize}
\item
  Also known as sample selection or self-selection problem or incidental truncation.
\item
  The omitted variable is how people were selected into the sample
\end{itemize}

Some disciplines consider nonresponse bias and selection bias as sample selection.

\begin{itemize}
\tightlist
\item
  When unobservable factors that affect who is in the sample are independent of unobservable factors that affect the outcome, the sample selection is not endogenous. Hence, the sample selection is ignorable and estimator that ignores sample selection is still consistent.
\item
  when the unobservable factors that affect who is included in the sample are correlated with the unobservable factors that affect the outcome, the sample selection is endogenous and not ignorable, because estimators that ignore endogenous sample selection are not consistent (we don't know which part of the observable outcome is related to the causal relationship and which part is due to different people were selected for the treatment and control groups).
\end{itemize}

Assumptions: - The unobservables that affect the treatment selection and the outcome are jointly distributed as bivariate normal.

\textbf{Notes}:

\begin{itemize}
\item
  If you don't have strong exclusion restriction, identification is driven by the assumed non linearity in the functional form (through inverse Mills ratio). E.g., the estimate depend on the bivariate normal distribution of the error structure:

  \begin{itemize}
  \tightlist
  \item
    With strong exclusion restriction for the covariate in the correction equation, the variation in this variable can help identify the control for selection
  \item
    With weak exclusion restriction, and the variable exists in both steps, it's the assumed error structure that identifies the control for selection \citep{heckman2004using}.
  \end{itemize}
\item
  In management, \citet{wolfolds2019misaccounting} found that papers should have valid exclusion conditions, because without these, simulations show that results using the Heckman method are less reliable than those obtained with OLS.
\item
  There are differences between Heckman Sample Selection vs.~Heckman-type correction
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2361}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2917}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4722}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Heckman Sample Selection Model}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Heckman-Type Corrections}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
When & Only observes one sample (treated), addressing selection bias directly. & Two samples are observed (treated and untreated), known as the control function approach. \\
Model & Probit & OLS (even for dummy endogenous variable) \\
Integration of 1st stage & Also include a term (called Inverse Mills ratio) besides the endogenous variable. & Decompose the endogenous variable to get the part that is uncorrelated with the error terms of the outcome equation. Either use the predicted endogenous variable directly or include the residual from the first-stage equation. \\
Advantages and Assumptions & Provides a direct test for endogeneity via the coefficient of the inverse Mills ratio but requires the assumption of joint normality of errors. & Does not require the assumption of joint normality, but can't test for endogeneity directly. \\
\end{longtable}

To deal with {[}Sample Selection{]}, we can

\begin{itemize}
\tightlist
\item
  Randomization: participants are randomly selected into treatment and control.
\item
  Instruments that determine the treatment status (i.e., treatment vs.~control) but not the outcome (\(Y\))
\item
  Functional form of the selection and outcome processes: originated from \citep{Heckman_1976}, later on generalize by \citep{amemiya1984tobit}
\end{itemize}

We have our main model

\[
\mathbf{y^* = xb + \epsilon}
\]

However, the pattern of missingness (i.e., censored) is related to the unobserved (latent) process:

\[
\mathbf{z^* = w \gamma + u}
\]

and

\[
z_i = 
\begin{cases}
1& \text{if } z_i^*>0 \\
0&\text{if } z_i^*\le0\\
\end{cases}
\]

Equivalently, \(z_i = 1\) (\(y_i\) is observed) when

\[
u_i \ge -w_i \gamma
\]

Hence, the probability of observed \(y_i\) is

\[
\begin{aligned}
P(u_i \ge -w_i \gamma) &= 1 - \Phi(-w_i \gamma) \\
&= \Phi(w_i \gamma) & \text{symmetry of the standard normal distribution}
\end{aligned}
\]

We will \textbf{assume}

\begin{itemize}
\tightlist
\item
  the error term of the selection \(\mathbf{u \sim N(0,I)}\)
\item
  \(Var(u_i) = 1\) for identification purposes
\end{itemize}

Visually, \(P(u_i \ge -w_i \gamma)\) is the shaded area.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{=} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\AttributeTok{length =} \DecValTok{200}\NormalTok{)}
\NormalTok{y }\OtherTok{=} \FunctionTok{dnorm}\NormalTok{(x, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(x,}
\NormalTok{     y,}
     \AttributeTok{type =} \StringTok{"l"}\NormalTok{,}
     \AttributeTok{main =} \FunctionTok{bquote}\NormalTok{(}\StringTok{"Probabibility distribution of"} \SpecialCharTok{\textasciitilde{}}\NormalTok{ u[i]))}
\NormalTok{x }\OtherTok{=} \FunctionTok{seq}\NormalTok{(}\FloatTok{0.3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\AttributeTok{length =} \DecValTok{100}\NormalTok{)}
\NormalTok{y }\OtherTok{=} \FunctionTok{dnorm}\NormalTok{(x, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)}
\FunctionTok{polygon}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FloatTok{0.3}\NormalTok{, x, }\DecValTok{3}\NormalTok{), }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, y, }\DecValTok{0}\NormalTok{), }\AttributeTok{col =} \StringTok{"gray"}\NormalTok{)}
\FunctionTok{text}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\FunctionTok{bquote}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ Phi }\SpecialCharTok{\textasciitilde{}}\NormalTok{ (}\SpecialCharTok{{-}}\NormalTok{w[i] }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gamma)))}
\FunctionTok{arrows}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\DecValTok{0}\NormalTok{, }\AttributeTok{length =}\NormalTok{ .}\DecValTok{15}\NormalTok{)}
\FunctionTok{text}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.12}\NormalTok{, }\FunctionTok{bquote}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{w[i] }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gamma))}
\FunctionTok{legend}\NormalTok{(}
    \StringTok{"topright"}\NormalTok{,}
    \StringTok{"Gray = Prob of Observed"}\NormalTok{,}
    \AttributeTok{pch =} \DecValTok{1}\NormalTok{,}
    \AttributeTok{title =} \StringTok{"legend"}\NormalTok{,}
    \AttributeTok{inset =}\NormalTok{ .}\DecValTok{02}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{33-endogeneity_files/figure-latex/unnamed-chunk-12-1} \end{center}

Hence in our observed model, we see

\begin{equation}
y_i = x_i\beta + \epsilon_i \text{when $z_i=1$}
\end{equation}

and the joint distribution of the selection model (\(u_i\)), and the observed equation (\(\epsilon_i\)) as

\[
\left[
\begin{array}
{c}
u \\
\epsilon \\
\end{array}
\right]
\sim^{iid}N
\left(
\left[
\begin{array}
{c}
0 \\
0 \\
\end{array}
\right],
\left[
\begin{array}
{cc}
1 & \rho \\
\rho & \sigma^2_{\epsilon} \\
\end{array}
\right]
\right)
\]

The relation between the observed and selection models:

\[
\begin{aligned}
E(y_i | y_i \text{ observed}) &= E(y_i| z^*>0) \\
&= E(y_i| -w_i \gamma) \\
&= \mathbf{x}_i \beta + E(\epsilon_i | u_i > -w_i \gamma) \\
&= \mathbf{x}_i \beta + \rho \sigma_\epsilon \frac{\phi(w_i \gamma)}{\Phi(w_i \gamma)}
\end{aligned}
\]

where \(\frac{\phi(w_i \gamma)}{\Phi(w_i \gamma)}\) is the Inverse Mills Ratio. and \(\rho \sigma_\epsilon \frac{\phi(w_i \gamma)}{\Phi(w_i \gamma)} \ge 0\)

A property of IMR: Its derivative is: \(IMR'(x) = -x IMR(x) - IMR(x)^2\)

Great visualization of special cases of correlation patterns among data and errors by professor \href{https://rlhick.people.wm.edu/stories/econ_407_notes_heckman.html}{Rob Hick}

Note:

\citep{bareinboim2014transportability} is an excellent summary of cases that we can still do causal inference in case of selection bias. I'll try to summarize their idea here:

Let \(X\) be an action, \(Y\) be an outcome, and S be a binary indicator of entry into the data pool where (\(S = 1 =\) in the sample, \(S = 0 =\) out of sample) and Q be the conditional distribution \(Q = P(y|x)\).

Usually we want to understand , but because of \(S\), we only have \(P(y, x|S = 1)\). Hence, we'd like to recover \(P(y|x)\) from \(P(y, x|S = 1)\)

\begin{itemize}
\tightlist
\item
  If both X and Y affect S, we can't unbiasedly estimate \(P(y|x)\)
\end{itemize}

In the case of Omitted variable bias (\(U\)) and sample selection bias (\(S\)), you have unblocked extraneous ``flow'' of information between X and \(Y\), which causes spurious correlation for \(X\) and \(Y\). Traditionally, we would recover \(Q\) by parametric assumption of

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  The data generating process (e.g., Heckman 2-step)
\item
  Type of data-generating model (e..g, treatment-dependent or outcome-dependent)
\item
  Selection's probability \(P(S = 1|P a_s)\) with non-parametrically based causal graphical models, the authors proposed more robust way to model misspecification regardless of the type of data-generating model, and do not require selection's probability. Hence, you can recover Q

  \begin{itemize}
  \tightlist
  \item
    Without external data
  \item
    With external data
  \item
    Causal effects with the Selection-backdoor criterion
  \end{itemize}
\end{enumerate}

\hypertarget{tobit-2}{%
\subsection{Tobit-2}\label{tobit-2}}

also known as Heckman's standard sample selection model\\
Assumption: joint normality of the errors

Data here is taken from \citet{mroz1984sensitivity}.

We want to estimate the log(wage) for married women, with education, experience, experience squared, and a dummy variable for living in a big city. But we can only observe the wage for women who are working, which means a lot of married women in 1975 who were out of the labor force are unaccounted for. Hence, an OLS estimate of the wage equation would be bias due to sample selection. Since we have data on non-participants (i.e., those who are not working for pay), we can correct for the selection process.

The Tobit-2 estimates are consistent

\hypertarget{example-1-2}{%
\subsubsection{Example 1}\label{example-1-2}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(sampleSelection)}
\FunctionTok{library}\NormalTok{(dplyr)}
\CommentTok{\# 1975 data on married womens pay and labor{-}force participation }
\CommentTok{\# from the Panel Study of Income Dynamics (PSID)}
\FunctionTok{data}\NormalTok{(}\StringTok{"Mroz87"}\NormalTok{) }
\FunctionTok{head}\NormalTok{(Mroz87)}
\CommentTok{\#\textgreater{}   lfp hours kids5 kids618 age educ   wage repwage hushrs husage huseduc huswage}
\CommentTok{\#\textgreater{} 1   1  1610     1       0  32   12 3.3540    2.65   2708     34      12  4.0288}
\CommentTok{\#\textgreater{} 2   1  1656     0       2  30   12 1.3889    2.65   2310     30       9  8.4416}
\CommentTok{\#\textgreater{} 3   1  1980     1       3  35   12 4.5455    4.04   3072     40      12  3.5807}
\CommentTok{\#\textgreater{} 4   1   456     0       3  34   12 1.0965    3.25   1920     53      10  3.5417}
\CommentTok{\#\textgreater{} 5   1  1568     1       2  31   14 4.5918    3.60   2000     32      12 10.0000}
\CommentTok{\#\textgreater{} 6   1  2032     0       0  54   12 4.7421    4.70   1040     57      11  6.7106}
\CommentTok{\#\textgreater{}   faminc    mtr motheduc fatheduc unem city exper  nwifeinc wifecoll huscoll}
\CommentTok{\#\textgreater{} 1  16310 0.7215       12        7  5.0    0    14 10.910060    FALSE   FALSE}
\CommentTok{\#\textgreater{} 2  21800 0.6615        7        7 11.0    1     5 19.499981    FALSE   FALSE}
\CommentTok{\#\textgreater{} 3  21040 0.6915       12        7  5.0    0    15 12.039910    FALSE   FALSE}
\CommentTok{\#\textgreater{} 4   7300 0.7815        7        7  5.0    0     6  6.799996    FALSE   FALSE}
\CommentTok{\#\textgreater{} 5  27300 0.6215       12       14  9.5    1     7 20.100058     TRUE   FALSE}
\CommentTok{\#\textgreater{} 6  19495 0.6915       14        7  7.5    1    33  9.859054    FALSE   FALSE}
\NormalTok{Mroz87 }\OtherTok{=}\NormalTok{ Mroz87 }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{kids =}\NormalTok{ kids5 }\SpecialCharTok{+}\NormalTok{ kids618)}

\FunctionTok{library}\NormalTok{(nnet)}
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(reshape2)}
\end{Highlighting}
\end{Shaded}

2-stage Heckman's model:

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  probit equation estimates the selection process (who is in the labor force?)
\item
  the results from 1st stage are used to construct a variable that captures the selection effect in the wage equation. This correction variable is called the \textbf{inverse Mills ratio}.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# OLS: log wage regression on LF participants only}
\NormalTok{ols1 }\OtherTok{=} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(wage) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ educ }\SpecialCharTok{+}\NormalTok{ exper }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(exper }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ city, }
          \AttributeTok{data =} \FunctionTok{subset}\NormalTok{(Mroz87, lfp }\SpecialCharTok{==} \DecValTok{1}\NormalTok{))}
\CommentTok{\# Heckman\textquotesingle{}s Two{-}step estimation with LFP selection equation}
\NormalTok{heck1 }\OtherTok{=} \FunctionTok{heckit}\NormalTok{(}
    \AttributeTok{selection =}\NormalTok{ lfp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(age }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ kids }\SpecialCharTok{+}\NormalTok{ huswage }\SpecialCharTok{+}\NormalTok{ educ,}
    \CommentTok{\# the selection process, l}
    \CommentTok{\# fp = 1 if the woman is participating in the labor force}
    \AttributeTok{outcome =} \FunctionTok{log}\NormalTok{(wage) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ educ }\SpecialCharTok{+}\NormalTok{ exper }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(exper }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ city,}
    \AttributeTok{data =}\NormalTok{ Mroz87}
\NormalTok{)}

\FunctionTok{summary}\NormalTok{(heck1}\SpecialCharTok{$}\NormalTok{probit)}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} Probit binary choice model/Maximum Likelihood estimation}
\CommentTok{\#\textgreater{} Newton{-}Raphson maximisation, 4 iterations}
\CommentTok{\#\textgreater{} Return code 1: gradient close to zero (gradtol)}
\CommentTok{\#\textgreater{} Log{-}Likelihood: {-}482.8212 }
\CommentTok{\#\textgreater{} Model: Y == \textquotesingle{}1\textquotesingle{} in contrary to \textquotesingle{}0\textquotesingle{}}
\CommentTok{\#\textgreater{} 753 observations (325 \textquotesingle{}negative\textquotesingle{} and 428 \textquotesingle{}positive\textquotesingle{}) and 6 free parameters (df = 747)}
\CommentTok{\#\textgreater{} Estimates:}
\CommentTok{\#\textgreater{}                  Estimate  Std. error t value   Pr(\textgreater{} t)    }
\CommentTok{\#\textgreater{} XS(Intercept) {-}4.18146681  1.40241567 {-}2.9816  0.002867 ** }
\CommentTok{\#\textgreater{} XSage          0.18608901  0.06517476  2.8552  0.004301 ** }
\CommentTok{\#\textgreater{} XSI(age\^{}2)    {-}0.00241491  0.00075857 {-}3.1835  0.001455 ** }
\CommentTok{\#\textgreater{} XSkids        {-}0.14955977  0.03825079 {-}3.9100 9.230e{-}05 ***}
\CommentTok{\#\textgreater{} XShuswage     {-}0.04303635  0.01220791 {-}3.5253  0.000423 ***}
\CommentTok{\#\textgreater{} XSeduc         0.12502818  0.02277645  5.4894 4.034e{-}08 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} Significance test:}
\CommentTok{\#\textgreater{} chi2(5) = 64.10407 (p=1.719042e{-}12)}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\FunctionTok{summary}\NormalTok{(heck1}\SpecialCharTok{$}\NormalTok{lm)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = YO \textasciitilde{} {-}1 + XO + imrData$IMR1, subset = YS == 1, weights = weightsNoNA)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}      Min       1Q   Median       3Q      Max }
\CommentTok{\#\textgreater{} {-}3.09494 {-}0.30953  0.05341  0.36530  2.34770 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}                 Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} XO(Intercept) {-}0.6143381  0.3768796  {-}1.630  0.10383    }
\CommentTok{\#\textgreater{} XOeduc         0.1092363  0.0197062   5.543 5.24e{-}08 ***}
\CommentTok{\#\textgreater{} XOexper        0.0419205  0.0136176   3.078  0.00222 ** }
\CommentTok{\#\textgreater{} XOI(exper\^{}2)  {-}0.0008226  0.0004059  {-}2.026  0.04335 *  }
\CommentTok{\#\textgreater{} XOcity         0.0510492  0.0692414   0.737  0.46137    }
\CommentTok{\#\textgreater{} imrData$IMR1   0.0551177  0.2111916   0.261  0.79423    }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 0.6674 on 422 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.7734, Adjusted R{-}squared:  0.7702 }
\CommentTok{\#\textgreater{} F{-}statistic:   240 on 6 and 422 DF,  p{-}value: \textless{} 2.2e{-}16}
\end{Highlighting}
\end{Shaded}

Use only variables that affect the selection process in the selection equation. Technically, the selection equation and the equation of interest could have the same set of regressors. But it is not recommended because we should only use variables (or at least one) in the selection equation that affect the selection process, but not the wage process (i.e., instruments). Here, variable \texttt{kids} fulfill that role: women with kids may be more likely to stay home, but working moms with kids would not have their wages change.

Alternatively,

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# ML estimation of selection model}
\NormalTok{ml1 }\OtherTok{=} \FunctionTok{selection}\NormalTok{(}
    \AttributeTok{selection =}\NormalTok{ lfp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(age }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ kids }\SpecialCharTok{+}\NormalTok{ huswage }\SpecialCharTok{+}\NormalTok{ educ,}
    \AttributeTok{outcome =} \FunctionTok{log}\NormalTok{(wage) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ educ }\SpecialCharTok{+}\NormalTok{ exper }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(exper }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ city,}
    \AttributeTok{data =}\NormalTok{ Mroz87}
\NormalTok{) }
\FunctionTok{summary}\NormalTok{(ml1)}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} Tobit 2 model (sample selection model)}
\CommentTok{\#\textgreater{} Maximum Likelihood estimation}
\CommentTok{\#\textgreater{} Newton{-}Raphson maximisation, 3 iterations}
\CommentTok{\#\textgreater{} Return code 8: successive function values within relative tolerance limit (reltol)}
\CommentTok{\#\textgreater{} Log{-}Likelihood: {-}914.0777 }
\CommentTok{\#\textgreater{} 753 observations (325 censored and 428 observed)}
\CommentTok{\#\textgreater{} 13 free parameters (df = 740)}
\CommentTok{\#\textgreater{} Probit selection equation:}
\CommentTok{\#\textgreater{}               Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept) {-}4.1484037  1.4109302  {-}2.940 0.003382 ** }
\CommentTok{\#\textgreater{} age          0.1842132  0.0658041   2.799 0.005253 ** }
\CommentTok{\#\textgreater{} I(age\^{}2)    {-}0.0023925  0.0007664  {-}3.122 0.001868 ** }
\CommentTok{\#\textgreater{} kids        {-}0.1488158  0.0384888  {-}3.866 0.000120 ***}
\CommentTok{\#\textgreater{} huswage     {-}0.0434253  0.0123229  {-}3.524 0.000451 ***}
\CommentTok{\#\textgreater{} educ         0.1255639  0.0229229   5.478 5.91e{-}08 ***}
\CommentTok{\#\textgreater{} Outcome equation:}
\CommentTok{\#\textgreater{}               Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept) {-}0.5814781  0.3052031  {-}1.905  0.05714 .  }
\CommentTok{\#\textgreater{} educ         0.1078481  0.0172998   6.234 7.63e{-}10 ***}
\CommentTok{\#\textgreater{} exper        0.0415752  0.0133269   3.120  0.00188 ** }
\CommentTok{\#\textgreater{} I(exper\^{}2)  {-}0.0008125  0.0003974  {-}2.044  0.04129 *  }
\CommentTok{\#\textgreater{} city         0.0522990  0.0682652   0.766  0.44385    }
\CommentTok{\#\textgreater{}    Error terms:}
\CommentTok{\#\textgreater{}       Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} sigma  0.66326    0.02309  28.729   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} rho    0.05048    0.23169   0.218    0.828    }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\# summary(ml1$twoStep)}
\end{Highlighting}
\end{Shaded}

Manual

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{myprob }\OtherTok{\textless{}{-}} \FunctionTok{probit}\NormalTok{(lfp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{( age}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{ ) }\SpecialCharTok{+}\NormalTok{ kids }\SpecialCharTok{+}\NormalTok{ huswage }\SpecialCharTok{+}\NormalTok{ educ, }
                 \CommentTok{\# x = TRUE, }
                 \CommentTok{\# iterlim = 30, }
                 \AttributeTok{data =}\NormalTok{ Mroz87)}
\FunctionTok{summary}\NormalTok{(myprob)}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} Probit binary choice model/Maximum Likelihood estimation}
\CommentTok{\#\textgreater{} Newton{-}Raphson maximisation, 4 iterations}
\CommentTok{\#\textgreater{} Return code 1: gradient close to zero (gradtol)}
\CommentTok{\#\textgreater{} Log{-}Likelihood: {-}482.8212 }
\CommentTok{\#\textgreater{} Model: Y == \textquotesingle{}1\textquotesingle{} in contrary to \textquotesingle{}0\textquotesingle{}}
\CommentTok{\#\textgreater{} 753 observations (325 \textquotesingle{}negative\textquotesingle{} and 428 \textquotesingle{}positive\textquotesingle{}) and 6 free parameters (df = 747)}
\CommentTok{\#\textgreater{} Estimates:}
\CommentTok{\#\textgreater{}                Estimate  Std. error t value   Pr(\textgreater{} t)    }
\CommentTok{\#\textgreater{} (Intercept) {-}4.18146681  1.40241567 {-}2.9816  0.002867 ** }
\CommentTok{\#\textgreater{} age          0.18608901  0.06517476  2.8552  0.004301 ** }
\CommentTok{\#\textgreater{} I(age\^{}2)    {-}0.00241491  0.00075857 {-}3.1835  0.001455 ** }
\CommentTok{\#\textgreater{} kids        {-}0.14955977  0.03825079 {-}3.9100 9.230e{-}05 ***}
\CommentTok{\#\textgreater{} huswage     {-}0.04303635  0.01220791 {-}3.5253  0.000423 ***}
\CommentTok{\#\textgreater{} educ         0.12502818  0.02277645  5.4894 4.034e{-}08 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} Significance test:}
\CommentTok{\#\textgreater{} chi2(5) = 64.10407 (p=1.719042e{-}12)}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}

\NormalTok{imr }\OtherTok{\textless{}{-}} \FunctionTok{invMillsRatio}\NormalTok{(myprob)}
\NormalTok{Mroz87}\SpecialCharTok{$}\NormalTok{IMR1 }\OtherTok{\textless{}{-}}\NormalTok{ imr}\SpecialCharTok{$}\NormalTok{IMR1}

\NormalTok{manually\_est }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(wage) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ educ }\SpecialCharTok{+}\NormalTok{ exper }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{( exper}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{ ) }\SpecialCharTok{+}\NormalTok{ city }\SpecialCharTok{+}\NormalTok{ IMR1,}
                   \AttributeTok{data =}\NormalTok{ Mroz87, }
                   \AttributeTok{subset =}\NormalTok{ (lfp }\SpecialCharTok{==} \DecValTok{1}\NormalTok{))}

\FunctionTok{summary}\NormalTok{(manually\_est)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = log(wage) \textasciitilde{} educ + exper + I(exper\^{}2) + city + IMR1, }
\CommentTok{\#\textgreater{}     data = Mroz87, subset = (lfp == 1))}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}      Min       1Q   Median       3Q      Max }
\CommentTok{\#\textgreater{} {-}3.09494 {-}0.30953  0.05341  0.36530  2.34770 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}               Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept) {-}0.6143381  0.3768796  {-}1.630  0.10383    }
\CommentTok{\#\textgreater{} educ         0.1092363  0.0197062   5.543 5.24e{-}08 ***}
\CommentTok{\#\textgreater{} exper        0.0419205  0.0136176   3.078  0.00222 ** }
\CommentTok{\#\textgreater{} I(exper\^{}2)  {-}0.0008226  0.0004059  {-}2.026  0.04335 *  }
\CommentTok{\#\textgreater{} city         0.0510492  0.0692414   0.737  0.46137    }
\CommentTok{\#\textgreater{} IMR1         0.0551177  0.2111916   0.261  0.79423    }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 0.6674 on 422 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.1582, Adjusted R{-}squared:  0.1482 }
\CommentTok{\#\textgreater{} F{-}statistic: 15.86 on 5 and 422 DF,  p{-}value: 2.505e{-}14}
\end{Highlighting}
\end{Shaded}

Similarly,

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{probit\_selection }\OtherTok{\textless{}{-}}
    \FunctionTok{glm}\NormalTok{(lfp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{( age}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{ ) }\SpecialCharTok{+}\NormalTok{ kids }\SpecialCharTok{+}\NormalTok{ huswage }\SpecialCharTok{+}\NormalTok{ educ,}
        \AttributeTok{data =}\NormalTok{ Mroz87,}
        \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{\textquotesingle{}probit\textquotesingle{}}\NormalTok{))}

\CommentTok{\# library(fixest)}
\CommentTok{\# probit\_selection \textless{}{-}}
\CommentTok{\#     fixest::feglm(lfp \textasciitilde{} age + I( age\^{}2 ) + kids + huswage + educ,}
\CommentTok{\#         data = Mroz87,}
\CommentTok{\#         family = binomial(link = \textquotesingle{}probit\textquotesingle{}))}

\NormalTok{probit\_lp }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\FunctionTok{predict}\NormalTok{(probit\_selection)}
\NormalTok{inv\_mills }\OtherTok{\textless{}{-}} \FunctionTok{dnorm}\NormalTok{(probit\_lp) }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{pnorm}\NormalTok{(probit\_lp))}
\NormalTok{Mroz87}\SpecialCharTok{$}\NormalTok{inv\_mills }\OtherTok{\textless{}{-}}\NormalTok{ inv\_mills}


\NormalTok{probit\_outcome }\OtherTok{\textless{}{-}}
    \FunctionTok{glm}\NormalTok{(}
        \FunctionTok{log}\NormalTok{(wage) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ educ }\SpecialCharTok{+}\NormalTok{ exper }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(exper }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ city }\SpecialCharTok{+}\NormalTok{ inv\_mills,}
        \AttributeTok{data =}\NormalTok{ Mroz87,}
        \AttributeTok{subset =}\NormalTok{ (lfp }\SpecialCharTok{==} \DecValTok{1}\NormalTok{)}
\NormalTok{    )}
\FunctionTok{summary}\NormalTok{(probit\_outcome)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} glm(formula = log(wage) \textasciitilde{} educ + exper + I(exper\^{}2) + city + }
\CommentTok{\#\textgreater{}     inv\_mills, data = Mroz87, subset = (lfp == 1))}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Deviance Residuals: }
\CommentTok{\#\textgreater{}      Min        1Q    Median        3Q       Max  }
\CommentTok{\#\textgreater{} {-}3.09494  {-}0.30953   0.05341   0.36530   2.34770  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}               Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept) {-}0.6143383  0.3768798  {-}1.630  0.10383    }
\CommentTok{\#\textgreater{} educ         0.1092363  0.0197062   5.543 5.24e{-}08 ***}
\CommentTok{\#\textgreater{} exper        0.0419205  0.0136176   3.078  0.00222 ** }
\CommentTok{\#\textgreater{} I(exper\^{}2)  {-}0.0008226  0.0004059  {-}2.026  0.04335 *  }
\CommentTok{\#\textgreater{} city         0.0510492  0.0692414   0.737  0.46137    }
\CommentTok{\#\textgreater{} inv\_mills    0.0551179  0.2111918   0.261  0.79423    }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} (Dispersion parameter for gaussian family taken to be 0.4454809)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Null deviance: 223.33  on 427  degrees of freedom}
\CommentTok{\#\textgreater{} Residual deviance: 187.99  on 422  degrees of freedom}
\CommentTok{\#\textgreater{} AIC: 876.49}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Fisher Scoring iterations: 2}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"stargazer"}\NormalTok{)}
\FunctionTok{library}\NormalTok{(}\StringTok{"Mediana"}\NormalTok{)}
\FunctionTok{library}\NormalTok{(}\StringTok{"plm"}\NormalTok{)}
\CommentTok{\# function to calculate corrected SEs for regression }
\NormalTok{cse }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(reg) \{}
\NormalTok{  rob }\OtherTok{=} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{diag}\NormalTok{(}\FunctionTok{vcovHC}\NormalTok{(reg, }\AttributeTok{type =} \StringTok{"HC1"}\NormalTok{)))}
  \FunctionTok{return}\NormalTok{(rob)}
\NormalTok{\}}

\CommentTok{\# stargazer table}
\FunctionTok{stargazer}\NormalTok{(}
    \CommentTok{\# ols1,}
\NormalTok{    heck1,}
\NormalTok{    ml1,}
    \CommentTok{\# manually\_est,}
    
    \AttributeTok{se =} \FunctionTok{list}\NormalTok{(}\FunctionTok{cse}\NormalTok{(ols1), }\ConstantTok{NULL}\NormalTok{, }\ConstantTok{NULL}\NormalTok{),}
    \AttributeTok{title =} \StringTok{"Married women\textquotesingle{}s wage regressions"}\NormalTok{,}
    \AttributeTok{type =} \StringTok{"text"}\NormalTok{,}
    \AttributeTok{df =} \ConstantTok{FALSE}\NormalTok{,}
    \AttributeTok{digits =} \DecValTok{4}\NormalTok{,}
    \AttributeTok{selection.equation =}\NormalTok{ T}
\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Married women\textquotesingle{}s wage regressions}
\CommentTok{\#\textgreater{} ===================================================}
\CommentTok{\#\textgreater{}                           Dependent variable:      }
\CommentTok{\#\textgreater{}                     {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}                                   lfp              }
\CommentTok{\#\textgreater{}                         Heckman        selection   }
\CommentTok{\#\textgreater{}                        selection                   }
\CommentTok{\#\textgreater{}                           (1)             (2)      }
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} age                    0.1861***       0.1842***   }
\CommentTok{\#\textgreater{}                                        (0.0658)    }
\CommentTok{\#\textgreater{}                                                    }
\CommentTok{\#\textgreater{} I(age2)                 {-}0.0024       {-}0.0024***   }
\CommentTok{\#\textgreater{}                                        (0.0008)    }
\CommentTok{\#\textgreater{}                                                    }
\CommentTok{\#\textgreater{} kids                  {-}0.1496***      {-}0.1488***   }
\CommentTok{\#\textgreater{}                                        (0.0385)    }
\CommentTok{\#\textgreater{}                                                    }
\CommentTok{\#\textgreater{} huswage                 {-}0.0430       {-}0.0434***   }
\CommentTok{\#\textgreater{}                                        (0.0123)    }
\CommentTok{\#\textgreater{}                                                    }
\CommentTok{\#\textgreater{} educ                    0.1250         0.1256***   }
\CommentTok{\#\textgreater{}                        (0.0130)        (0.0229)    }
\CommentTok{\#\textgreater{}                                                    }
\CommentTok{\#\textgreater{} Constant              {-}4.1815***      {-}4.1484***   }
\CommentTok{\#\textgreater{}                        (0.2032)        (1.4109)    }
\CommentTok{\#\textgreater{}                                                    }
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} Observations              753             753      }
\CommentTok{\#\textgreater{} R2                      0.1582                     }
\CommentTok{\#\textgreater{} Adjusted R2             0.1482                     }
\CommentTok{\#\textgreater{} Log Likelihood                         {-}914.0777   }
\CommentTok{\#\textgreater{} rho                     0.0830      0.0505 (0.2317)}
\CommentTok{\#\textgreater{} Inverse Mills Ratio 0.0551 (0.2099)                }
\CommentTok{\#\textgreater{} ===================================================}
\CommentTok{\#\textgreater{} Note:                   *p\textless{}0.1; **p\textless{}0.05; ***p\textless{}0.01}


\FunctionTok{stargazer}\NormalTok{(}
\NormalTok{    ols1,}
    \CommentTok{\# heck1,}
    \CommentTok{\# ml1,}
\NormalTok{    manually\_est,}
    
    \AttributeTok{se =} \FunctionTok{list}\NormalTok{(}\FunctionTok{cse}\NormalTok{(ols1), }\ConstantTok{NULL}\NormalTok{, }\ConstantTok{NULL}\NormalTok{),}
    \AttributeTok{title =} \StringTok{"Married women\textquotesingle{}s wage regressions"}\NormalTok{,}
    \AttributeTok{type =} \StringTok{"text"}\NormalTok{,}
    \AttributeTok{df =} \ConstantTok{FALSE}\NormalTok{,}
    \AttributeTok{digits =} \DecValTok{4}\NormalTok{,}
    \AttributeTok{selection.equation =}\NormalTok{ T}
\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Married women\textquotesingle{}s wage regressions}
\CommentTok{\#\textgreater{} ================================================}
\CommentTok{\#\textgreater{}                         Dependent variable:     }
\CommentTok{\#\textgreater{}                     {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}                              log(wage)          }
\CommentTok{\#\textgreater{}                          (1)            (2)     }
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} educ                  0.1057***      0.1092***  }
\CommentTok{\#\textgreater{}                        (0.0130)      (0.0197)   }
\CommentTok{\#\textgreater{}                                                 }
\CommentTok{\#\textgreater{} exper                 0.0411***      0.0419***  }
\CommentTok{\#\textgreater{}                        (0.0154)      (0.0136)   }
\CommentTok{\#\textgreater{}                                                 }
\CommentTok{\#\textgreater{} I(exper2)              {-}0.0008*      {-}0.0008**  }
\CommentTok{\#\textgreater{}                        (0.0004)      (0.0004)   }
\CommentTok{\#\textgreater{}                                                 }
\CommentTok{\#\textgreater{} city                    0.0542        0.0510    }
\CommentTok{\#\textgreater{}                        (0.0653)      (0.0692)   }
\CommentTok{\#\textgreater{}                                                 }
\CommentTok{\#\textgreater{} IMR1                                  0.0551    }
\CommentTok{\#\textgreater{}                                      (0.2112)   }
\CommentTok{\#\textgreater{}                                                 }
\CommentTok{\#\textgreater{} Constant              {-}0.5308***      {-}0.6143   }
\CommentTok{\#\textgreater{}                        (0.2032)      (0.3769)   }
\CommentTok{\#\textgreater{}                                                 }
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} Observations             428            428     }
\CommentTok{\#\textgreater{} R2                      0.1581        0.1582    }
\CommentTok{\#\textgreater{} Adjusted R2             0.1501        0.1482    }
\CommentTok{\#\textgreater{} Residual Std. Error     0.6667        0.6674    }
\CommentTok{\#\textgreater{} F Statistic           19.8561***    15.8635***  }
\CommentTok{\#\textgreater{} ================================================}
\CommentTok{\#\textgreater{} Note:                *p\textless{}0.1; **p\textless{}0.05; ***p\textless{}0.01}
\end{Highlighting}
\end{Shaded}

Rho is an estimate of the correlation of the errors between the selection and wage equations. In the lower panel, the estimated coefficient on the inverse Mills ratio is given for the Heckman model. The fact that it is not statistically different from zero is consistent with the idea that selection bias was not a serious problem in this case.

If the estimated coefficient of the inverse Mills ratio in the Heckman model is not statistically different from zero, then selection bias was not a serious problem.

\hypertarget{example-2-2}{%
\subsubsection{Example 2}\label{example-2-2}}

This code is from \href{https://cran.r-project.org/web/packages/sampleSelection/vignettes/selection.pdf}{R package sampleSelection}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{0}\NormalTok{)}
\FunctionTok{library}\NormalTok{(}\StringTok{"sampleSelection"}\NormalTok{)}
\FunctionTok{library}\NormalTok{(}\StringTok{"mvtnorm"}\NormalTok{)}
\CommentTok{\# bivariate normal disturbances}
\NormalTok{eps }\OtherTok{\textless{}{-}}
    \FunctionTok{rmvnorm}\NormalTok{(}\DecValTok{500}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{), }\FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{0.7}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{0.7}\NormalTok{, }\DecValTok{1}\NormalTok{), }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)) }

\CommentTok{\# uniformly distributed explanatory variable }
\CommentTok{\# (vectors of explanatory variables for the selection)}
\NormalTok{xs }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\DecValTok{500}\NormalTok{)}

\CommentTok{\# probit data generating process}
\NormalTok{ys }\OtherTok{\textless{}{-}}\NormalTok{ xs }\SpecialCharTok{+}\NormalTok{ eps[, }\DecValTok{1}\NormalTok{] }\SpecialCharTok{\textgreater{}} \DecValTok{0} 

\CommentTok{\# vectors of explanatory variables for outcome equation}
\NormalTok{xo }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\DecValTok{500}\NormalTok{) }
\NormalTok{yoX }\OtherTok{\textless{}{-}}\NormalTok{ xo }\SpecialCharTok{+}\NormalTok{ eps[, }\DecValTok{2}\NormalTok{] }\CommentTok{\# latent outcome}
\NormalTok{yo }\OtherTok{\textless{}{-}}\NormalTok{ yoX }\SpecialCharTok{*}\NormalTok{ (ys }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{) }\CommentTok{\# observable outcome}
\CommentTok{\# true intercepts = 0 and our true slopes = 1}
\CommentTok{\# xs and xo are independent. }
\CommentTok{\# Hence, exclusion restriction is fulfilled}
\FunctionTok{summary}\NormalTok{(}\FunctionTok{selection}\NormalTok{(ys }\SpecialCharTok{\textasciitilde{}}\NormalTok{ xs, yo }\SpecialCharTok{\textasciitilde{}}\NormalTok{ xo))}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} Tobit 2 model (sample selection model)}
\CommentTok{\#\textgreater{} Maximum Likelihood estimation}
\CommentTok{\#\textgreater{} Newton{-}Raphson maximisation, 5 iterations}
\CommentTok{\#\textgreater{} Return code 1: gradient close to zero (gradtol)}
\CommentTok{\#\textgreater{} Log{-}Likelihood: {-}712.3163 }
\CommentTok{\#\textgreater{} 500 observations (172 censored and 328 observed)}
\CommentTok{\#\textgreater{} 6 free parameters (df = 494)}
\CommentTok{\#\textgreater{} Probit selection equation:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)  {-}0.2228     0.1081  {-}2.061   0.0399 *  }
\CommentTok{\#\textgreater{} xs            1.3377     0.2014   6.642 8.18e{-}11 ***}
\CommentTok{\#\textgreater{} Outcome equation:}
\CommentTok{\#\textgreater{}               Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept) {-}0.0002265  0.1294178  {-}0.002    0.999    }
\CommentTok{\#\textgreater{} xo           0.7299070  0.1635925   4.462 1.01e{-}05 ***}
\CommentTok{\#\textgreater{}    Error terms:}
\CommentTok{\#\textgreater{}       Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} sigma   0.9190     0.0574  16.009  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} rho    {-}0.5392     0.1521  {-}3.544 0.000431 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\end{Highlighting}
\end{Shaded}

without the exclusion restriction, we generate yo using xs instead of xo.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{yoX }\OtherTok{\textless{}{-}}\NormalTok{ xs }\SpecialCharTok{+}\NormalTok{ eps[,}\DecValTok{2}\NormalTok{]}
\NormalTok{yo }\OtherTok{\textless{}{-}}\NormalTok{ yoX}\SpecialCharTok{*}\NormalTok{(ys }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(}\FunctionTok{selection}\NormalTok{(ys }\SpecialCharTok{\textasciitilde{}}\NormalTok{ xs, yo }\SpecialCharTok{\textasciitilde{}}\NormalTok{ xs))}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} Tobit 2 model (sample selection model)}
\CommentTok{\#\textgreater{} Maximum Likelihood estimation}
\CommentTok{\#\textgreater{} Newton{-}Raphson maximisation, 14 iterations}
\CommentTok{\#\textgreater{} Return code 8: successive function values within relative tolerance limit (reltol)}
\CommentTok{\#\textgreater{} Log{-}Likelihood: {-}712.8298 }
\CommentTok{\#\textgreater{} 500 observations (172 censored and 328 observed)}
\CommentTok{\#\textgreater{} 6 free parameters (df = 494)}
\CommentTok{\#\textgreater{} Probit selection equation:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)  {-}0.1984     0.1114  {-}1.781   0.0756 .  }
\CommentTok{\#\textgreater{} xs            1.2907     0.2085   6.191 1.25e{-}09 ***}
\CommentTok{\#\textgreater{} Outcome equation:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)   }
\CommentTok{\#\textgreater{} (Intercept)  {-}0.5499     0.5644  {-}0.974  0.33038   }
\CommentTok{\#\textgreater{} xs            1.3987     0.4482   3.120  0.00191 **}
\CommentTok{\#\textgreater{}    Error terms:}
\CommentTok{\#\textgreater{}       Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} sigma  0.85091    0.05352  15.899   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} rho   {-}0.13226    0.72684  {-}0.182    0.856    }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\end{Highlighting}
\end{Shaded}

We can see that our estimates are still unbiased but standard errors are substantially larger. The exclusion restriction (i.e., independent information about the selection process) has a certain identifying power that we desire. Hence, it's better to have different set of variable for the selection process from the interested equation. Without the exclusion restriction, we solely rely on the functional form identification.

\hypertarget{tobit-5}{%
\subsection{Tobit-5}\label{tobit-5}}

Also known as the switching regression model\\
Condition: There is at least one variable in X in the selection process not included in the observed process. Used when there are separate models for participants, and non-participants.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{0}\NormalTok{)}
\NormalTok{vc }\OtherTok{\textless{}{-}} \FunctionTok{diag}\NormalTok{(}\DecValTok{3}\NormalTok{)}
\NormalTok{vc[}\FunctionTok{lower.tri}\NormalTok{(vc)] }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{0.9}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.1}\NormalTok{)}
\NormalTok{vc[}\FunctionTok{upper.tri}\NormalTok{(vc)] }\OtherTok{\textless{}{-}}\NormalTok{ vc[}\FunctionTok{lower.tri}\NormalTok{(vc)]}

\CommentTok{\# 3 disturbance vectors by a 3{-}dimensional normal distribution}
\NormalTok{eps }\OtherTok{\textless{}{-}} \FunctionTok{rmvnorm}\NormalTok{(}\DecValTok{500}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{), vc) }
\NormalTok{xs }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\DecValTok{500}\NormalTok{) }\CommentTok{\# uniformly distributed on [0, 1]}
\NormalTok{ys }\OtherTok{\textless{}{-}}\NormalTok{ xs }\SpecialCharTok{+}\NormalTok{ eps[,}\DecValTok{1}\NormalTok{] }\SpecialCharTok{\textgreater{}} \DecValTok{0}
\NormalTok{xo1 }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\DecValTok{500}\NormalTok{) }\CommentTok{\# uniformly distributed on [0, 1]}
\NormalTok{yo1 }\OtherTok{\textless{}{-}}\NormalTok{ xo1 }\SpecialCharTok{+}\NormalTok{ eps[,}\DecValTok{2}\NormalTok{]}
\NormalTok{xo2 }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\DecValTok{500}\NormalTok{) }\CommentTok{\# uniformly distributed on [0, 1]}
\NormalTok{yo2 }\OtherTok{\textless{}{-}}\NormalTok{ xo2 }\SpecialCharTok{+}\NormalTok{ eps[,}\DecValTok{3}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

exclusion restriction is fulfilled when \(x\)'s are independent.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# one selection equation and a list of two outcome equations}
\FunctionTok{summary}\NormalTok{(}\FunctionTok{selection}\NormalTok{(ys}\SpecialCharTok{\textasciitilde{}}\NormalTok{xs, }\FunctionTok{list}\NormalTok{(yo1 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ xo1, yo2 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ xo2))) }
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} Tobit 5 model (switching regression model)}
\CommentTok{\#\textgreater{} Maximum Likelihood estimation}
\CommentTok{\#\textgreater{} Newton{-}Raphson maximisation, 11 iterations}
\CommentTok{\#\textgreater{} Return code 1: gradient close to zero (gradtol)}
\CommentTok{\#\textgreater{} Log{-}Likelihood: {-}895.8201 }
\CommentTok{\#\textgreater{} 500 observations: 172 selection 1 (FALSE) and 328 selection 2 (TRUE)}
\CommentTok{\#\textgreater{} 10 free parameters (df = 490)}
\CommentTok{\#\textgreater{} Probit selection equation:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)  {-}0.1550     0.1051  {-}1.474    0.141    }
\CommentTok{\#\textgreater{} xs            1.1408     0.1785   6.390 3.86e{-}10 ***}
\CommentTok{\#\textgreater{} Outcome equation 1:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)  0.02708    0.16395   0.165    0.869    }
\CommentTok{\#\textgreater{} xo1          0.83959    0.14968   5.609  3.4e{-}08 ***}
\CommentTok{\#\textgreater{} Outcome equation 2:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)   0.1583     0.1885   0.840    0.401    }
\CommentTok{\#\textgreater{} xo2           0.8375     0.1707   4.908 1.26e{-}06 ***}
\CommentTok{\#\textgreater{}    Error terms:}
\CommentTok{\#\textgreater{}        Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} sigma1  0.93191    0.09211  10.118   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} sigma2  0.90697    0.04434  20.455   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} rho1    0.88988    0.05353  16.623   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} rho2    0.17695    0.33139   0.534    0.594    }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\end{Highlighting}
\end{Shaded}

All the estimates are close to the true values.

Example of functional form misspecification

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{5}\NormalTok{)}
\NormalTok{eps }\OtherTok{\textless{}{-}} \FunctionTok{rmvnorm}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{3}\NormalTok{), vc)}
\NormalTok{eps }\OtherTok{\textless{}{-}}\NormalTok{ eps}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{{-}} \DecValTok{1} \CommentTok{\# subtract 1 in order to get the mean zero disturbances}

\CommentTok{\# interval [1, 0] to get an asymmetric distribution over observed choices}
\NormalTok{xs }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{) }
\NormalTok{ys }\OtherTok{\textless{}{-}}\NormalTok{ xs }\SpecialCharTok{+}\NormalTok{ eps[,}\DecValTok{1}\NormalTok{] }\SpecialCharTok{\textgreater{}} \DecValTok{0}
\NormalTok{xo1 }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\DecValTok{1000}\NormalTok{)}
\NormalTok{yo1 }\OtherTok{\textless{}{-}}\NormalTok{ xo1 }\SpecialCharTok{+}\NormalTok{ eps[,}\DecValTok{2}\NormalTok{]}
\NormalTok{xo2 }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\DecValTok{1000}\NormalTok{)}
\NormalTok{yo2 }\OtherTok{\textless{}{-}}\NormalTok{ xo2 }\SpecialCharTok{+}\NormalTok{ eps[,}\DecValTok{3}\NormalTok{]}
\FunctionTok{summary}\NormalTok{(}\FunctionTok{selection}\NormalTok{(ys}\SpecialCharTok{\textasciitilde{}}\NormalTok{xs, }\FunctionTok{list}\NormalTok{(yo1 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ xo1, yo2 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ xo2), }\AttributeTok{iterlim=}\DecValTok{20}\NormalTok{))}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} Tobit 5 model (switching regression model)}
\CommentTok{\#\textgreater{} Maximum Likelihood estimation}
\CommentTok{\#\textgreater{} Newton{-}Raphson maximisation, 4 iterations}
\CommentTok{\#\textgreater{} Return code 3: Last step could not find a value above the current.}
\CommentTok{\#\textgreater{} Boundary of parameter space?  }
\CommentTok{\#\textgreater{} Consider switching to a more robust optimisation method temporarily.}
\CommentTok{\#\textgreater{} Log{-}Likelihood: {-}1665.936 }
\CommentTok{\#\textgreater{} 1000 observations: 760 selection 1 (FALSE) and 240 selection 2 (TRUE)}
\CommentTok{\#\textgreater{} 10 free parameters (df = 990)}
\CommentTok{\#\textgreater{} Probit selection equation:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept) {-}0.53698    0.05808  {-}9.245  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} xs           0.31268    0.09395   3.328 0.000906 ***}
\CommentTok{\#\textgreater{} Outcome equation 1:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept) {-}0.70679    0.03573  {-}19.78   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} xo1          0.91603    0.05626   16.28   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} Outcome equation 2:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)  }
\CommentTok{\#\textgreater{} (Intercept)   0.1446        NaN     NaN      NaN  }
\CommentTok{\#\textgreater{} xo2           1.1196     0.5014   2.233   0.0258 *}
\CommentTok{\#\textgreater{}    Error terms:}
\CommentTok{\#\textgreater{}        Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} sigma1  0.67770    0.01760   38.50   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} sigma2  2.31432    0.07615   30.39   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} rho1   {-}0.97137        NaN     NaN      NaN    }
\CommentTok{\#\textgreater{} rho2    0.17039        NaN     NaN      NaN    }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\end{Highlighting}
\end{Shaded}

Although we still have an exclusion restriction (xo1 and xo2 are independent), we now have problems with the intercepts (i.e., they are statistically significantly different from the true values zero), and convergence problems.

If we don't have the exclusion restriction, we will have a larger variance of xs

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{6}\NormalTok{)}
\NormalTok{xs }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{ys }\OtherTok{\textless{}{-}}\NormalTok{ xs }\SpecialCharTok{+}\NormalTok{ eps[,}\DecValTok{1}\NormalTok{] }\SpecialCharTok{\textgreater{}} \DecValTok{0}
\NormalTok{yo1 }\OtherTok{\textless{}{-}}\NormalTok{ xs }\SpecialCharTok{+}\NormalTok{ eps[,}\DecValTok{2}\NormalTok{]}
\NormalTok{yo2 }\OtherTok{\textless{}{-}}\NormalTok{ xs }\SpecialCharTok{+}\NormalTok{ eps[,}\DecValTok{3}\NormalTok{]}
\FunctionTok{summary}\NormalTok{(tmp }\OtherTok{\textless{}{-}} \FunctionTok{selection}\NormalTok{(ys}\SpecialCharTok{\textasciitilde{}}\NormalTok{xs, }\FunctionTok{list}\NormalTok{(yo1 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ xs, yo2 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ xs), }\AttributeTok{iterlim=}\DecValTok{20}\NormalTok{))}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} Tobit 5 model (switching regression model)}
\CommentTok{\#\textgreater{} Maximum Likelihood estimation}
\CommentTok{\#\textgreater{} Newton{-}Raphson maximisation, 16 iterations}
\CommentTok{\#\textgreater{} Return code 8: successive function values within relative tolerance limit (reltol)}
\CommentTok{\#\textgreater{} Log{-}Likelihood: {-}1936.431 }
\CommentTok{\#\textgreater{} 1000 observations: 626 selection 1 (FALSE) and 374 selection 2 (TRUE)}
\CommentTok{\#\textgreater{} 10 free parameters (df = 990)}
\CommentTok{\#\textgreater{} Probit selection equation:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)  {-}0.3528     0.0424  {-}8.321 2.86e{-}16 ***}
\CommentTok{\#\textgreater{} xs            0.8354     0.0756  11.050  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} Outcome equation 1:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept) {-}0.55448    0.06339  {-}8.748   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} xs           0.81764    0.06048  13.519   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} Outcome equation 2:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)}
\CommentTok{\#\textgreater{} (Intercept)   0.6457     0.4994   1.293    0.196}
\CommentTok{\#\textgreater{} xs            0.3520     0.3197   1.101    0.271}
\CommentTok{\#\textgreater{}    Error terms:}
\CommentTok{\#\textgreater{}        Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} sigma1  0.59187    0.01853  31.935   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} sigma2  1.97257    0.07228  27.289   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} rho1    0.15568    0.15914   0.978    0.328    }
\CommentTok{\#\textgreater{} rho2   {-}0.01541    0.23370  {-}0.066    0.947    }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\end{Highlighting}
\end{Shaded}

Usually it will not converge. Even if it does, the results may be seriously biased.

\textbf{Note}

The log-likelihood function of the models might not be globally concave. Hence, it might not converge, or converge to a local maximum. To combat this, we can use

\begin{itemize}
\tightlist
\item
  Different starting value
\item
  Different maximization methods.
\item
  refer to {[}Non-linear Least Squares{]} for suggestions.
\end{itemize}

\hypertarget{pattern-mixture-models}{%
\paragraph{Pattern-Mixture Models}\label{pattern-mixture-models}}

\begin{itemize}
\tightlist
\item
  compared to the Heckman's model where it assumes the value of the missing data is predetermined, pattern-mixture models assume missingness affect the distribution of variable of interest (e.g., Y)
\item
  To read more, you can check \href{https://www4.stat.ncsu.edu/~davidian/st790/notes/chap6.pdf}{NCSU}, \href{https://stefvanbuuren.name/fimd/sec-nonignorable.html}{stefvanbuuren}.
\end{itemize}

\hypertarget{other-biases}{%
\chapter{Other Biases}\label{other-biases}}

In econometrics, the main objective is often to uncover causal relationships. However, coefficient estimates can be affected by various biases. Here's a list of common biases that can affect coefficient estimates:

What we've covered so far (see \protect\hyperlink{linear-regression}{Linear Regression} and \protect\hyperlink{endogeneity}{Endogeneity}):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Omitted Variable Bias (OVB)}:

  \begin{itemize}
  \tightlist
  \item
    Arises when a variable that affects the dependent variable and is correlated with an independent variable is left out of the regression.
  \end{itemize}
\item
  \textbf{Endogeneity Bias}:

  \begin{itemize}
  \item
    Occurs when an error term is correlated with an independent variable. This can be due to:

    \begin{itemize}
    \item
      Simultaneity: When the dependent variable simultaneously affects an independent variable.
    \item
      Omitted variables.
    \item
      Measurement error in the independent variable.
    \end{itemize}
  \end{itemize}
\item
  \textbf{Measurement Error}:

  \begin{itemize}
  \tightlist
  \item
    Bias introduced when variables in a model are measured with error. If the error is in an independent variable and is classical (mean zero and uncorrelated with the true value), it typically biases the coefficient towards zero.
  \end{itemize}
\item
  \textbf{Sample Selection Bias}:

  \begin{itemize}
  \tightlist
  \item
    Arises when the sample is not randomly selected and the selection is related to the dependent variable. A classic example is the Heckman correction for labor market studies where participants self-select into the workforce.
  \end{itemize}
\item
  \textbf{Simultaneity Bias (or Reverse Causality)}:

  \begin{itemize}
  \tightlist
  \item
    Happens when the dependent variable causes changes in the independent variable, leading to a two-way causation.
  \end{itemize}
\item
  \textbf{Multicollinearity}:

  \begin{itemize}
  \tightlist
  \item
    Not a bias in the strictest sense, but in the presence of high multicollinearity (when independent variables are highly correlated), coefficient estimates can become unstable and standard errors large. This makes it hard to determine the individual effect of predictors on the dependent variable.
  \end{itemize}
\item
  \textbf{Specification Errors}:

  \begin{itemize}
  \tightlist
  \item
    Arise when the functional form of the model is incorrectly specified, e.g., omitting interaction terms or polynomial terms when they are needed.
  \end{itemize}
\item
  \textbf{Autocorrelation (or Serial Correlation)}:

  \begin{itemize}
  \tightlist
  \item
    Occurs in time-series data when the error terms are correlated over time. This doesn't cause bias in the coefficient estimates of OLS, but it can make standard errors biased, leading to incorrect inference.
  \end{itemize}
\item
  \textbf{Heteroskedasticity}:

  \begin{itemize}
  \tightlist
  \item
    Occurs when the variance of the error term is not constant across observations. Like autocorrelation, heteroskedasticity doesn't bias the OLS estimates but can bias standard errors.
  \end{itemize}
\end{enumerate}

In this section, we will mention other biases that you may encounter when conducting your research

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{9}
\tightlist
\item
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Introduced when data are aggregated, and analysis is conducted at this aggregate level rather than the individual level.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{10}
\tightlist
\item
  {[}\textbf{Survivorship Bias}{]} \textbf{(very much related to Sample Selection)}:
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Arises when the sample only includes ``survivors'' or those who ``passed'' a certain threshold. Common in finance where only funds or firms that ``survive'' are analyzed.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{11}
\tightlist
\item
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Not a bias in econometric estimation per se, but relevant in the context of empirical studies. It refers to the tendency for journals to publish only significant or positive results, leading to an overrepresentation of such results in the literature.
\end{itemize}

\hypertarget{aggregation-bias}{%
\section{Aggregation Bias}\label{aggregation-bias}}

Aggregation bias, also known as ecological fallacy, refers to the error introduced when data are aggregated and an analysis is conducted at this aggregate level, rather than at the individual level. This can be especially problematic in econometrics, where analysts are often concerned with understanding individual behavior.

When the relationship between variables is different at the aggregate level than at the individual level, aggregation bias can result. The bias arises when inferences about individual behaviors are made based on aggregate data.

\textbf{Example}: Suppose we have data on individuals' incomes and their personal consumption. At the individual level, it's possible that as income rises, consumption also rises. However, when we aggregate the data to, say, a neighborhood level, neighborhoods with diverse income levels might all have similar average consumption due to other unobserved factors.

\textbf{Step 1}: Create individual level data

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\# Generate data for 1000 individuals}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{1000}

\NormalTok{income }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =} \DecValTok{50}\NormalTok{, }\AttributeTok{sd =} \DecValTok{10}\NormalTok{)}
\NormalTok{consumption }\OtherTok{\textless{}{-}} \FloatTok{0.5} \SpecialCharTok{*}\NormalTok{ income }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{5}\NormalTok{)}

\CommentTok{\# Individual level regression}
\NormalTok{individual\_lm }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(consumption }\SpecialCharTok{\textasciitilde{}}\NormalTok{ income)}
\FunctionTok{summary}\NormalTok{(individual\_lm)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = consumption \textasciitilde{} income)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}      Min       1Q   Median       3Q      Max }
\CommentTok{\#\textgreater{} {-}15.1394  {-}3.4572   0.0213   3.5436  16.4557 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept) {-}1.99596    0.82085  {-}2.432   0.0152 *  }
\CommentTok{\#\textgreater{} income       0.54402    0.01605  33.888   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 5.032 on 998 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.535,  Adjusted R{-}squared:  0.5346 }
\CommentTok{\#\textgreater{} F{-}statistic:  1148 on 1 and 998 DF,  p{-}value: \textless{} 2.2e{-}16}
\end{Highlighting}
\end{Shaded}

This would show a significant positive relationship between income and consumption.

\textbf{Step 2}: Aggregate data to `neighborhood' level

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Assume 100 neighborhoods with 10 individuals each}
\NormalTok{n\_neighborhoods }\OtherTok{\textless{}{-}} \DecValTok{100}

\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(income, consumption)}
\NormalTok{df}\SpecialCharTok{$}\NormalTok{neighborhood }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_neighborhoods, }\AttributeTok{each =}\NormalTok{ n }\SpecialCharTok{/}\NormalTok{ n\_neighborhoods)}

\NormalTok{aggregate\_data }\OtherTok{\textless{}{-}} \FunctionTok{aggregate}\NormalTok{(. }\SpecialCharTok{\textasciitilde{}}\NormalTok{ neighborhood, }\AttributeTok{data =}\NormalTok{ df, }\AttributeTok{FUN =}\NormalTok{ mean)}

\CommentTok{\# Aggregate level regression}
\NormalTok{aggregate\_lm }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(consumption }\SpecialCharTok{\textasciitilde{}}\NormalTok{ income, }\AttributeTok{data =}\NormalTok{ aggregate\_data)}
\FunctionTok{summary}\NormalTok{(aggregate\_lm)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = consumption \textasciitilde{} income, data = aggregate\_data)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}     Min      1Q  Median      3Q     Max }
\CommentTok{\#\textgreater{} {-}4.4517 {-}0.9322 {-}0.0826  1.0556  3.5728 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept) {-}4.94338    2.60699  {-}1.896   0.0609 .  }
\CommentTok{\#\textgreater{} income       0.60278    0.05188  11.618   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 1.54 on 98 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.5794, Adjusted R{-}squared:  0.5751 }
\CommentTok{\#\textgreater{} F{-}statistic:   135 on 1 and 98 DF,  p{-}value: \textless{} 2.2e{-}16}
\end{Highlighting}
\end{Shaded}

If aggregation bias is present, the coefficient for income in the aggregate regression might be different from the coefficient in the individual regression, even if the individual relationship is significant and strong.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}

\CommentTok{\# Individual scatterplot}
\NormalTok{p1 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(df, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ income, }\AttributeTok{y =}\NormalTok{ consumption)) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color =}\NormalTok{ neighborhood), }\AttributeTok{alpha =} \FloatTok{0.6}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{,}
                \AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{,}
                \AttributeTok{color =} \StringTok{"black"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Individual Level Data"}\NormalTok{) }\SpecialCharTok{+}
\NormalTok{    causalverse}\SpecialCharTok{::}\FunctionTok{ama\_theme}\NormalTok{()}

\CommentTok{\# Aggregate scatterplot}
\NormalTok{p2 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(aggregate\_data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ income, }\AttributeTok{y =}\NormalTok{ consumption)) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{color =} \StringTok{"red"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{,}
                \AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{,}
                \AttributeTok{color =} \StringTok{"black"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Aggregate Level Data"}\NormalTok{) }\SpecialCharTok{+}
\NormalTok{    causalverse}\SpecialCharTok{::}\FunctionTok{ama\_theme}\NormalTok{()}

\CommentTok{\# print(p1)}
\CommentTok{\# print(p2)}

\NormalTok{gridExtra}\SpecialCharTok{::}\FunctionTok{grid.arrange}\NormalTok{(}\AttributeTok{grobs =} \FunctionTok{list}\NormalTok{(p1, p2), }\AttributeTok{ncol =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{34-biases_files/figure-latex/unnamed-chunk-3-1} \end{center}

From these plots, you can see the relationship at the individual level, with each neighborhood being colored differently in the first plot. The second plot shows the aggregate data, where each point now represents a whole neighborhood.

\textbf{Direction of Bias}: The direction of the aggregation bias isn't predetermined. It depends on the underlying relationship and the data distribution. In some cases, aggregation might attenuate (reduce) a relationship, while in other cases, it might exaggerate it.

\textbf{Relation to Other Biases}: Aggregation bias is closely related to several other biases in econometrics:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Specification bias}: If you don't properly account for the hierarchical structure of your data (like individuals nested within neighborhoods), your model might be mis-specified, leading to biased estimates.
\item
  \textbf{\protect\hyperlink{measurement-error}{Measurement Error}}: Aggregation can introduce or amplify measurement errors. For instance, if you aggregate noisy measures, the aggregate might not accurately represent any underlying signal.
\item
  \textbf{Omitted Variable Bias (see \protect\hyperlink{endogeneity}{Endogeneity})}: When you aggregate data, you lose information. If the loss of this information results in omitting important predictors that are correlated with both the independent and dependent variables, it can introduce omitted variable bias.
\end{enumerate}

\hypertarget{simpsons-paradox}{%
\subsection{Simpson's Paradox}\label{simpsons-paradox}}

Simpson's Paradox, also known as the Yule-Simpson effect, is a phenomenon in probability and statistics where a trend that appears in different groups of data disappears or reverses when the groups are combined. It's a striking example of how aggregated data can sometimes provide a misleading representation of the actual situation.

\textbf{Illustration of Simpson's Paradox:}

Consider a hypothetical scenario involving two hospitals: Hospital A and Hospital B. We want to analyze the success rates of treatments at both hospitals. When we break the data down by the severity of the cases (i.e., minor cases vs.~major cases):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Hospital A}:

  \begin{itemize}
  \item
    Minor cases: 95\% success rate
  \item
    Major cases: 80\% success rate
  \end{itemize}
\item
  \textbf{Hospital B}:

  \begin{itemize}
  \item
    Minor cases: 90\% success rate
  \item
    Major cases: 85\% success rate
  \end{itemize}
\end{enumerate}

From this breakdown, Hospital A appears to be better in treating both minor and major cases since it has a higher success rate in both categories.

However, let's consider the overall success rates without considering case severity:

\begin{itemize}
\item
  \textbf{Hospital A}: 83\% overall success rate
\item
  \textbf{Hospital B}: 86\% overall success rate
\end{itemize}

Suddenly, Hospital B seems better overall. This surprising reversal happens because the two hospitals might handle very different proportions of minor and major cases. For example, if Hospital A treats many more major cases (which have lower success rates) than Hospital B, it can drag down its overall success rate.

\textbf{Causes}:

Simpson's Paradox can arise due to:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  A lurking or confounding variable that wasn't initially considered (in our example, the severity of the medical cases).
\item
  Different group sizes, where one group might be much larger than the other, influencing the aggregate results.
\end{enumerate}

\textbf{Implications}:

Simpson's Paradox highlights the dangers of interpreting aggregated data without considering potential underlying sub-group structures. It underscores the importance of disaggregating data and being aware of the context in which it's analyzed.

\textbf{Relation to \href{}{Aggregation Bias}}

In the most extreme case, aggregation bias can reverse the coefficient sign of the relationship of interest (i.e., Simpson's Paradox).

\textbf{Example}: Suppose we are studying the effect of a new study technique on student grades. We have two groups of students: those who used the new technique (\textbf{\texttt{treatment\ =\ 1}}) and those who did not (\textbf{\texttt{treatment\ =\ 0}}). We want to see if using the new study technique is related to higher grades.

Let's assume grades are influenced by the starting ability of the students. Perhaps in our sample, many high-ability students didn't use the new technique (because they felt they didn't need it), while many low-ability students did.

Here's a setup:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  High-ability students tend to have high grades regardless of the technique.
\item
  The new technique has a positive effect on grades, but this is masked by the fact that many low-ability students use it.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\# Generate data for 1000 students}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{1000}

\CommentTok{\# 500 students are of high ability, 500 of low ability}
\NormalTok{ability }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\StringTok{"high"}\NormalTok{, }\DecValTok{500}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\StringTok{"low"}\NormalTok{, }\DecValTok{500}\NormalTok{))}

\CommentTok{\# High ability students are less likely to use the technique}
\NormalTok{treatment }\OtherTok{\textless{}{-}}
  \FunctionTok{ifelse}\NormalTok{(ability }\SpecialCharTok{==} \StringTok{"high"}\NormalTok{, }\FunctionTok{rbinom}\NormalTok{(}\DecValTok{500}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.2}\NormalTok{), }\FunctionTok{rbinom}\NormalTok{(}\DecValTok{500}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.8}\NormalTok{))}

\CommentTok{\# Grades are influenced by ability and treatment (new technique),}
\CommentTok{\# but the treatment has opposite effects based on ability.}
\NormalTok{grades }\OtherTok{\textless{}{-}}
  \FunctionTok{ifelse}\NormalTok{(}
\NormalTok{    ability }\SpecialCharTok{==} \StringTok{"high"}\NormalTok{,}
    \FunctionTok{rnorm}\NormalTok{(}\DecValTok{500}\NormalTok{, }\AttributeTok{mean =} \DecValTok{85}\NormalTok{, }\AttributeTok{sd =} \DecValTok{5}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ treatment }\SpecialCharTok{*} \SpecialCharTok{{-}}\DecValTok{3}\NormalTok{,}
    \FunctionTok{rnorm}\NormalTok{(}\DecValTok{500}\NormalTok{, }\AttributeTok{mean =} \DecValTok{60}\NormalTok{, }\AttributeTok{sd =} \DecValTok{5}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ treatment }\SpecialCharTok{*} \DecValTok{5}
\NormalTok{  )}

\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(ability, treatment, grades)}

\CommentTok{\# Regression without considering ability}
\NormalTok{overall\_lm }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(grades }\SpecialCharTok{\textasciitilde{}} \FunctionTok{factor}\NormalTok{(treatment), }\AttributeTok{data =}\NormalTok{ df)}
\FunctionTok{summary}\NormalTok{(overall\_lm)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = grades \textasciitilde{} factor(treatment), data = df)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}     Min      1Q  Median      3Q     Max }
\CommentTok{\#\textgreater{} {-}33.490  {-}4.729   0.986   6.368  25.607 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}                    Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)         80.0133     0.4373   183.0   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} factor(treatment)1 {-}11.7461     0.6248   {-}18.8   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 9.877 on 998 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.2615, Adjusted R{-}squared:  0.2608 }
\CommentTok{\#\textgreater{} F{-}statistic: 353.5 on 1 and 998 DF,  p{-}value: \textless{} 2.2e{-}16}

\CommentTok{\# Regression within ability groups}
\NormalTok{high\_ability\_lm }\OtherTok{\textless{}{-}}
  \FunctionTok{lm}\NormalTok{(grades }\SpecialCharTok{\textasciitilde{}} \FunctionTok{factor}\NormalTok{(treatment), }\AttributeTok{data =}\NormalTok{ df[df}\SpecialCharTok{$}\NormalTok{ability }\SpecialCharTok{==} \StringTok{"high"}\NormalTok{,])}
\NormalTok{low\_ability\_lm }\OtherTok{\textless{}{-}}
  \FunctionTok{lm}\NormalTok{(grades }\SpecialCharTok{\textasciitilde{}} \FunctionTok{factor}\NormalTok{(treatment), }\AttributeTok{data =}\NormalTok{ df[df}\SpecialCharTok{$}\NormalTok{ability }\SpecialCharTok{==} \StringTok{"low"}\NormalTok{,])}
\FunctionTok{summary}\NormalTok{(high\_ability\_lm)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = grades \textasciitilde{} factor(treatment), data = df[df$ability == }
\CommentTok{\#\textgreater{}     "high", ])}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}      Min       1Q   Median       3Q      Max }
\CommentTok{\#\textgreater{} {-}14.2156  {-}3.4813   0.1186   3.4952  13.2919 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}                    Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)         85.1667     0.2504 340.088  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} factor(treatment)1  {-}3.9489     0.5776  {-}6.837 2.37e{-}11 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 5.046 on 498 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.08581,    Adjusted R{-}squared:  0.08398 }
\CommentTok{\#\textgreater{} F{-}statistic: 46.75 on 1 and 498 DF,  p{-}value: 2.373e{-}11}
\FunctionTok{summary}\NormalTok{(low\_ability\_lm)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = grades \textasciitilde{} factor(treatment), data = df[df$ability == }
\CommentTok{\#\textgreater{}     "low", ])}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}      Min       1Q   Median       3Q      Max }
\CommentTok{\#\textgreater{} {-}13.3717  {-}3.5413   0.1097   3.3531  17.0568 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}                    Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)         59.8950     0.4871 122.956   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} factor(treatment)1   5.2979     0.5474   9.679   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 4.968 on 498 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.1583, Adjusted R{-}squared:  0.1566 }
\CommentTok{\#\textgreater{} F{-}statistic: 93.68 on 1 and 498 DF,  p{-}value: \textless{} 2.2e{-}16}
\end{Highlighting}
\end{Shaded}

From this simulation:

\begin{itemize}
\item
  The \textbf{\texttt{overall\_lm}} might show that the new study technique is associated with lower grades (negative coefficient), because many of the high-ability students (who naturally have high grades) did not use it.
\item
  The \textbf{\texttt{high\_ability\_lm}} will likely show that high-ability students who used the technique had slightly lower grades than high-ability students who didn't.
\item
  The \textbf{\texttt{low\_ability\_lm}} will likely show that low-ability students who used the technique had much higher grades than low-ability students who didn't.
\end{itemize}

This is a classic example of Simpson's Paradox: within each ability group, the technique appears beneficial, but when data is aggregated, the effect seems negative because of the distribution of the technique across ability groups.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}

\CommentTok{\# Scatterplot for overall data}
\NormalTok{p1 }\OtherTok{\textless{}{-}}
  \FunctionTok{ggplot}\NormalTok{(df, }\FunctionTok{aes}\NormalTok{(}
    \AttributeTok{x =} \FunctionTok{factor}\NormalTok{(treatment),}
    \AttributeTok{y =}\NormalTok{ grades,}
    \AttributeTok{color =}\NormalTok{ ability}
\NormalTok{  )) }\SpecialCharTok{+}
  \FunctionTok{geom\_jitter}\NormalTok{(}\AttributeTok{width =} \FloatTok{0.2}\NormalTok{, }\AttributeTok{height =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.6}\NormalTok{, }\AttributeTok{outlier.shape =} \ConstantTok{NA}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Overall Effect of Study Technique on Grades"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Treatment (0 = No Technique, 1 = New Technique)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Grades"}\NormalTok{)}

\CommentTok{\# Scatterplot for high{-}ability students}
\NormalTok{p2 }\OtherTok{\textless{}{-}}
  \FunctionTok{ggplot}\NormalTok{(df[df}\SpecialCharTok{$}\NormalTok{ability }\SpecialCharTok{==} \StringTok{"high"}\NormalTok{, ], }\FunctionTok{aes}\NormalTok{(}
    \AttributeTok{x =} \FunctionTok{factor}\NormalTok{(treatment),}
    \AttributeTok{y =}\NormalTok{ grades,}
    \AttributeTok{color =}\NormalTok{ ability}
\NormalTok{  )) }\SpecialCharTok{+}
  \FunctionTok{geom\_jitter}\NormalTok{(}\AttributeTok{width =} \FloatTok{0.2}\NormalTok{, }\AttributeTok{height =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.6}\NormalTok{, }\AttributeTok{outlier.shape =} \ConstantTok{NA}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Effect of Study Technique on Grades (High Ability)"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Treatment (0 = No Technique, 1 = New Technique)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Grades"}\NormalTok{)}

\CommentTok{\# Scatterplot for low{-}ability students}
\NormalTok{p3 }\OtherTok{\textless{}{-}}
  \FunctionTok{ggplot}\NormalTok{(df[df}\SpecialCharTok{$}\NormalTok{ability }\SpecialCharTok{==} \StringTok{"low"}\NormalTok{, ], }\FunctionTok{aes}\NormalTok{(}
    \AttributeTok{x =} \FunctionTok{factor}\NormalTok{(treatment),}
    \AttributeTok{y =}\NormalTok{ grades,}
    \AttributeTok{color =}\NormalTok{ ability}
\NormalTok{  )) }\SpecialCharTok{+}
  \FunctionTok{geom\_jitter}\NormalTok{(}\AttributeTok{width =} \FloatTok{0.2}\NormalTok{, }\AttributeTok{height =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.6}\NormalTok{, }\AttributeTok{outlier.shape =} \ConstantTok{NA}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Effect of Study Technique on Grades (Low Ability)"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Treatment (0 = No Technique, 1 = New Technique)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Grades"}\NormalTok{)}

\CommentTok{\# print(p1)}
\CommentTok{\# print(p2)}
\CommentTok{\# print(p3)}
\NormalTok{gridExtra}\SpecialCharTok{::}\FunctionTok{grid.arrange}\NormalTok{(}\AttributeTok{grobs =} \FunctionTok{list}\NormalTok{(p1, p2, p3), }\AttributeTok{ncol =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{34-biases_files/figure-latex/unnamed-chunk-5-1} \end{center}

\hypertarget{contamination-bias}{%
\section{Contamination Bias}\label{contamination-bias}}

\citet{goldsmith2022contamination} show regressions with multiple treatments and flexible controls often fail to estimate convex averages of heterogeneous treatment effects, resulting in contamination by non-convex averages of other treatments' effects.

\begin{itemize}
\tightlist
\item
  3 estimation methods to avoid this bias and find significant contamination bias in observational studies, with experimental studies showing less due to lower variability in propensity scores.
\end{itemize}

\hypertarget{survivorship-bias}{%
\section{Survivorship Bias}\label{survivorship-bias}}

Survivorship bias refers to the logical error of concentrating on the entities that have made it past some selection process and overlooking those that didn't, typically because of a lack of visibility. This can skew results and lead to overly optimistic conclusions.

\textbf{Example}: If you were to analyze the success of companies based only on the ones that are still in business today, you'd miss out on the insights from all those that failed. This would give you a distorted view of what makes a successful company, as you wouldn't account for all those that had those same attributes but didn't succeed.

\textbf{Relation to Other Biases}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Sample Selection Bias}: Survivorship bias is a specific form of sample selection bias. While survivorship bias focuses on entities that ``survive'', sample selection bias broadly deals with any non-random sample.
\item
  \textbf{Confirmation Bias}: Survivorship bias can reinforce confirmation bias. By only looking at the ``winners'', we might confirm our existing beliefs about what leads to success, ignoring evidence to the contrary from those that didn't survive.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}

\CommentTok{\# Generating data for 100 companies}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{100}

\CommentTok{\# Randomly generate earnings; assume true average earnings is 50}
\NormalTok{earnings }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =} \DecValTok{50}\NormalTok{, }\AttributeTok{sd =} \DecValTok{10}\NormalTok{)}

\CommentTok{\# Threshold for bankruptcy}
\NormalTok{threshold }\OtherTok{\textless{}{-}} \DecValTok{40}

\CommentTok{\# Only companies with earnings above the threshold "survive"}
\NormalTok{survivor\_earnings }\OtherTok{\textless{}{-}}\NormalTok{ earnings[earnings }\SpecialCharTok{\textgreater{}}\NormalTok{ threshold]}

\CommentTok{\# Average earnings for all companies vs. survivors}
\NormalTok{true\_avg }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(earnings)}
\NormalTok{survivor\_avg }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(survivor\_earnings)}

\NormalTok{true\_avg}
\CommentTok{\#\textgreater{} [1] 50.32515}
\NormalTok{survivor\_avg}
\CommentTok{\#\textgreater{} [1] 53.3898}
\end{Highlighting}
\end{Shaded}

Using a histogram to visualize the distribution of earnings, highlighting the ``survivors''.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}

\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(earnings)}

\NormalTok{p }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(df, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ earnings)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}
    \AttributeTok{binwidth =} \DecValTok{2}\NormalTok{,}
    \AttributeTok{fill =} \StringTok{"grey"}\NormalTok{,}
    \AttributeTok{color =} \StringTok{"black"}\NormalTok{,}
    \AttributeTok{alpha =} \FloatTok{0.7}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{xintercept =}\NormalTok{ true\_avg, }\AttributeTok{color =} \StringTok{"True Avg"}\NormalTok{),}
             \AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{,}
             \AttributeTok{size =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}
    \FunctionTok{aes}\NormalTok{(}\AttributeTok{xintercept =}\NormalTok{ survivor\_avg, }\AttributeTok{color =} \StringTok{"Survivor Avg"}\NormalTok{),}
    \AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{,}
    \AttributeTok{size =} \DecValTok{1}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"True Avg"} \OtherTok{=} \StringTok{"blue"}\NormalTok{, }\StringTok{"Survivor Avg"} \OtherTok{=} \StringTok{"red"}\NormalTok{),}
                     \AttributeTok{name =} \StringTok{"Average Type"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Distribution of Company Earnings"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Earnings"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of Companies"}\NormalTok{) }\SpecialCharTok{+}
\NormalTok{  causalverse}\SpecialCharTok{::}\FunctionTok{ama\_theme}\NormalTok{()}

\FunctionTok{print}\NormalTok{(p)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{34-biases_files/figure-latex/unnamed-chunk-7-1} \end{center}

In the plot, the ``True Avg'' might be lower than the ``Survivor Avg'', indicating that by only looking at the survivors, we overestimate the average earnings.

\textbf{Remedies}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Awareness}: Recognizing the potential for survivorship bias is the first step.
\item
  \textbf{Inclusive Data Collection}: Wherever possible, try to include data from entities that didn't ``survive'' in your sample.
\item
  \textbf{Statistical Techniques}: In cases where the missing data is inherent, methods like Heckman's two-step procedure can be used to correct for sample selection bias.
\item
  \textbf{External Data Sources}: Sometimes, complementary datasets can provide insights into the missing ``non-survivors''.
\item
  \textbf{Sensitivity Analysis}: Test how sensitive your results are to assumptions about the non-survivors.
\end{enumerate}

\hypertarget{publication-bias}{%
\section{Publication Bias}\label{publication-bias}}

Publication bias occurs when the results of studies influence the likelihood of their being published. Typically, studies with significant, positive, or sensational results are more likely to be published than those with non-significant or negative results. This can skew the perceived effectiveness or results when researchers conduct meta-analyses or literature reviews, leading them to draw inaccurate conclusions.

\textbf{Example}: Imagine pharmaceutical research. If 10 studies are done on a new drug, and only 2 show a positive effect while 8 show no effect, but only the 2 positive studies get published, a later review of the literature might erroneously conclude the drug is effective.

\textbf{Relation to Other Biases}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Selection Bias}: Publication bias is a form of selection bias, where the selection (publication in this case) isn't random but based on the results of the study.
\item
  \textbf{Confirmation Bias}: Like survivorship bias, publication bias can reinforce confirmation bias. Researchers might only find and cite studies that confirm their beliefs, overlooking the unpublished studies that might contradict them.
\end{enumerate}

Let's simulate an experiment on a new treatment. We'll assume that the treatment has no effect, but due to random variation, some studies will show significant positive or negative effects.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}

\CommentTok{\# Number of studies}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{100}

\CommentTok{\# Assuming no real effect (effect size = 0)}
\NormalTok{true\_effect }\OtherTok{\textless{}{-}} \DecValTok{0}

\CommentTok{\# Random variation in results}
\NormalTok{results }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =}\NormalTok{ true\_effect, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)}

\CommentTok{\# Only "significant" results get published }
\CommentTok{\# (arbitrarily defining significant as abs(effect) \textgreater{} 1.5)}
\NormalTok{published\_results }\OtherTok{\textless{}{-}}\NormalTok{ results[}\FunctionTok{abs}\NormalTok{(results) }\SpecialCharTok{\textgreater{}} \FloatTok{1.5}\NormalTok{]}

\CommentTok{\# Average effect for all studies vs. published studies}
\NormalTok{true\_avg\_effect }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(results)}
\NormalTok{published\_avg\_effect }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(published\_results)}

\NormalTok{true\_avg\_effect}
\CommentTok{\#\textgreater{} [1] 0.03251482}
\NormalTok{published\_avg\_effect}
\CommentTok{\#\textgreater{} [1] {-}0.3819601}
\end{Highlighting}
\end{Shaded}

Using a histogram to visualize the distribution of study results, highlighting the ``published'' studies.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}

\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(results)}

\NormalTok{p }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(df, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ results)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}
    \AttributeTok{binwidth =} \FloatTok{0.2}\NormalTok{,}
    \AttributeTok{fill =} \StringTok{"grey"}\NormalTok{,}
    \AttributeTok{color =} \StringTok{"black"}\NormalTok{,}
    \AttributeTok{alpha =} \FloatTok{0.7}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}
    \FunctionTok{aes}\NormalTok{(}\AttributeTok{xintercept =}\NormalTok{ true\_avg\_effect,}
        \AttributeTok{color =} \StringTok{"True Avg Effect"}\NormalTok{),}
    \AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{,}
    \AttributeTok{size =} \DecValTok{1}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}
    \FunctionTok{aes}\NormalTok{(}\AttributeTok{xintercept =}\NormalTok{ published\_avg\_effect,}
        \AttributeTok{color =} \StringTok{"Published Avg Effect"}\NormalTok{),}
    \AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{,}
    \AttributeTok{size =} \DecValTok{1}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_manual}\NormalTok{(}
    \AttributeTok{values =} \FunctionTok{c}\NormalTok{(}
      \StringTok{"True Avg Effect"} \OtherTok{=} \StringTok{"blue"}\NormalTok{,}
      \StringTok{"Published Avg Effect"} \OtherTok{=} \StringTok{"red"}
\NormalTok{    ),}
    \AttributeTok{name =} \StringTok{"Effect Type"}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Distribution of Study Results"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Effect Size"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of Studies"}\NormalTok{) }\SpecialCharTok{+}
\NormalTok{  causalverse}\SpecialCharTok{::}\FunctionTok{ama\_theme}\NormalTok{()}

\FunctionTok{print}\NormalTok{(p)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{34-biases_files/figure-latex/unnamed-chunk-9-1} \end{center}

The plot might show that the ``True Avg Effect'' is around zero, while the ``Published Avg Effect'' is likely higher or lower, depending on which studies happen to have significant results in the simulation.

\textbf{Remedies}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Awareness}: Understand and accept that publication bias exists, especially when conducting literature reviews or meta-analyses.
\item
  \textbf{Study Registries}: Encourage the use of study registries where researchers register their studies before they start. This way, one can see all initiated studies, not just the published ones.
\item
  \textbf{Publish All Results}: Journals and researchers should make an effort to publish negative or null results. Some journals, known as ``null result journals'', specialize in this.
\item
  \textbf{Funnel Plots and Egger's Test}: In meta-analyses, these are methods to visually and statistically detect publication bias.
\item
  \textbf{Use of Preprints}: Promote the use of preprint servers where researchers can upload studies before they're peer-reviewed, ensuring that results are available regardless of eventual publication status.
\item
  \textbf{p-curve analysis}: addresses publication bias and p-hacking by analyzing the distribution of p-values below 0.05 in research studies. It posits that a right-skewed distribution of these p-values indicates a true effect, whereas a left-skewed distribution suggests p-hacking and no true underlying effect. The method includes a ``half-curve'' test to counteract extensive p-hacking \citep[\citet{simonsohn2014pa}, \citet{simonsohn2015better}]{simonsohn2014p}.
\end{enumerate}

\hypertarget{controls}{%
\chapter{Controls}\label{controls}}

This section follows \citep{cinelli2022crash} and \href{https://www.kaggle.com/code/carloscinelli/crash-course-in-good-and-bad-controls-linear-r/notebook\#Model-11---Bad-Control-(overcontrol-bias)}{code}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dagitty)}
\FunctionTok{library}\NormalTok{(ggdag)}
\end{Highlighting}
\end{Shaded}

Traditional literature usually considers adding additional control variables is harmless to analysis.

More specifically, this problem is most prevalent in the review process. Reviewers only ask authors to add more variables to ``control'' for such variable, which can be asked with only limited rationale. Rarely ever you will see a reviewer asks an author to remove some variables to see the behavior of the variable of interest (This is also related to \protect\hyperlink{coefficient-stability}{Coefficient stability}).

However, adding more controls is only good in limited cases.

\hypertarget{bad-controls}{%
\section{Bad Controls}\label{bad-controls}}

\hypertarget{m-bias}{%
\subsection{M-bias}\label{m-bias}}

Traditional textbooks \citep{imbens2015causal, angrist2009mostly} consider \(Z\) as a good control because it's a pre-treatment variable, where it correlates with the treatment and the outcome.

This is most prevalent in \protect\hyperlink{matching-methods}{Matching Methods}, where we are recommended to include all ``pre-treatment'' variables.

However, it is a bad control because it opens the back-door path \(Z \leftarrow U_1 \to Z \leftarrow U_2 \to Y\)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cleans workspace}
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\CommentTok{\# DAG}

\DocumentationTok{\#\# specify edges}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; u1{-}\textgreater{}x; u1{-}\textgreater{}z; u2{-}\textgreater{}z; u2{-}\textgreater{}y\}"}\NormalTok{)}

\CommentTok{\# set u as latent}
\FunctionTok{latents}\NormalTok{(model) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"u1"}\NormalTok{, }\StringTok{"u2"}\NormalTok{)}

\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}
    \AttributeTok{x =} \DecValTok{1}\NormalTok{,}
    \AttributeTok{u1 =} \DecValTok{1}\NormalTok{,}
    \AttributeTok{z =} \DecValTok{2}\NormalTok{,}
    \AttributeTok{u2 =} \DecValTok{3}\NormalTok{,}
    \AttributeTok{y =} \DecValTok{3}
\NormalTok{),}
\AttributeTok{y =} \FunctionTok{c}\NormalTok{(}
    \AttributeTok{x =} \DecValTok{1}\NormalTok{,}
    \AttributeTok{u1 =} \DecValTok{2}\NormalTok{,}
    \AttributeTok{z =} \FloatTok{1.5}\NormalTok{,}
    \AttributeTok{u2 =} \DecValTok{2}\NormalTok{,}
    \AttributeTok{y =} \DecValTok{1}
\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{35-controls_files/figure-latex/unnamed-chunk-2-1} \end{center}

Even though \(Z\) can correlate with both \(X\) and \(Y\) very well, it's not a confounder.

Controlling for \(Z\) can bias the \(X \to Y\) estimate, because it opens the colliding path \(X \leftarrow U_1 \rightarrow Z \leftarrow U_2 \leftarrow Y\)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \FloatTok{1e4}
\NormalTok{u1 }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{u2 }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{z }\OtherTok{\textless{}{-}}\NormalTok{ u1 }\SpecialCharTok{+}\NormalTok{ u2 }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{x }\OtherTok{\textless{}{-}}\NormalTok{ u1 }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{causal\_coef }\OtherTok{\textless{}{-}} \DecValTok{2}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ causal\_coef }\SpecialCharTok{*}\NormalTok{ x }\SpecialCharTok{{-}} \DecValTok{4}\SpecialCharTok{*}\NormalTok{u2 }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}


\NormalTok{jtools}\SpecialCharTok{::}\FunctionTok{export\_summs}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x), }\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z))}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-3} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l l}


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 1 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 2 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} (Intercept) \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.00\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.03\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.04)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.03)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} x \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 2.02 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 2.83 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.03)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.03)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} z \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -1.59 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} N \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} R2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.32\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.57\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{3}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  *** p $<$ 0.001;  ** p $<$ 0.01;  * p $<$ 0.05. \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

Another worse variation is

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cleans workspace}
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\CommentTok{\# DAG}

\DocumentationTok{\#\# specify edges}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; u1{-}\textgreater{}x; u1{-}\textgreater{}z; u2{-}\textgreater{}z; u2{-}\textgreater{}y; z{-}\textgreater{}y\}"}\NormalTok{)}

\CommentTok{\# set u as latent}
\FunctionTok{latents}\NormalTok{(model) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"u1"}\NormalTok{, }\StringTok{"u2"}\NormalTok{)}

\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{u1=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{u2=}\DecValTok{3}\NormalTok{, }\AttributeTok{y=}\DecValTok{3}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{u1=}\DecValTok{2}\NormalTok{, }\AttributeTok{z=}\FloatTok{1.5}\NormalTok{, }\AttributeTok{u2=}\DecValTok{2}\NormalTok{, }\AttributeTok{y=}\DecValTok{1}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{35-controls_files/figure-latex/unnamed-chunk-4-1} \end{center}

You can't do much in this case.

\begin{itemize}
\item
  If you don't control for \(Z\), then you have an open back-door path \(X \leftarrow U_1 \to Z \to Y\), and the unadjusted estimate is biased
\item
  If you control for \(Z\), then you open backdoor path \(X \leftarrow U_1 \to Z \leftarrow U_2 \to Y\), and the adjusted estimate is also biased
\end{itemize}

Hence, we cannot identify the causal effect in this case.

We can do sensitivity analyses to examine \citep{cinelli2019sensitivity, cinelli2020making}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  the plausible bounds on the strength of the direct effect of \(Z \to Y\)
\item
  the strength of the effects of the latent variables
\end{enumerate}

\hypertarget{bias-amplification-1}{%
\subsection{Bias Amplification}\label{bias-amplification-1}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cleans workspace}
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\CommentTok{\# DAG}

\DocumentationTok{\#\# specify edges}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; u{-}\textgreater{}x; u{-}\textgreater{}y; z{-}\textgreater{}x\}"}\NormalTok{)}

\CommentTok{\# set u as latent}
\FunctionTok{latents}\NormalTok{(model) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"u"}\NormalTok{)}

\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{z=}\DecValTok{1}\NormalTok{, }\AttributeTok{x=}\DecValTok{2}\NormalTok{, }\AttributeTok{u=}\DecValTok{3}\NormalTok{, }\AttributeTok{y=}\DecValTok{4}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{z=}\DecValTok{1}\NormalTok{, }\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{u=}\DecValTok{2}\NormalTok{, }\AttributeTok{y=}\DecValTok{1}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{35-controls_files/figure-latex/unnamed-chunk-5-1} \end{center}

Controlling for Z amplifies the omitted variable bias

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \FloatTok{1e4}
\NormalTok{z }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{u }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{x }\OtherTok{\textless{}{-}} \DecValTok{2}\SpecialCharTok{*}\NormalTok{z }\SpecialCharTok{+}\NormalTok{ u }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{+} \DecValTok{2}\SpecialCharTok{*}\NormalTok{u }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}

\NormalTok{jtools}\SpecialCharTok{::}\FunctionTok{export\_summs}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x), }\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z))}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-6} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l l}


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 1 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 2 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} (Intercept) \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.01\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.00\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} x \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.34 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 2.00 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} z \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -1.98 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.03)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} N \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} R2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.71\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.80\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{3}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  *** p $<$ 0.001;  ** p $<$ 0.01;  * p $<$ 0.05. \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

\hypertarget{overcontrol-bias}{%
\subsection{Overcontrol bias}\label{overcontrol-bias}}

Sometimes, this is similar to controlling for variables that are proxy of the dependent variable.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cleans workspace}
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\CommentTok{\# DAG}

\DocumentationTok{\#\# specify edges}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}z; z{-}\textgreater{}y\}"}\NormalTok{)}


\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{y=}\DecValTok{3}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{1}\NormalTok{, }\AttributeTok{y=}\DecValTok{1}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{35-controls_files/figure-latex/unnamed-chunk-7-1} \end{center}

If X is a proxy for Z (i.e., a mediator between Z and Y), controlling for Z is bad

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \FloatTok{1e4}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{z }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ z }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}

\NormalTok{jtools}\SpecialCharTok{::}\FunctionTok{export\_summs}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x), }\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z))}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-8} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l l}


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 1 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 2 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} (Intercept) \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.01\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.01\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} x \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.00 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.01\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} z \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.01 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} N \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} R2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.33\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.67\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{3}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  *** p $<$ 0.001;  ** p $<$ 0.01;  * p $<$ 0.05. \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

Now you see that \(Z\) is significant, which is technically true, but we are interested in the causal coefficient of \(X\) on \(Y\).

Another setting for overcontrol bias is

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cleans workspace}
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\CommentTok{\# DAG}

\DocumentationTok{\#\# specify edges}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}m; m{-}\textgreater{}z; m{-}\textgreater{}y\}"}\NormalTok{)}


\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{m=}\DecValTok{2}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{y=}\DecValTok{3}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{2}\NormalTok{, }\AttributeTok{m=}\DecValTok{2}\NormalTok{, }\AttributeTok{z=}\DecValTok{1}\NormalTok{, }\AttributeTok{y=}\DecValTok{2}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{35-controls_files/figure-latex/unnamed-chunk-9-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \FloatTok{1e4}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{m }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{z }\OtherTok{\textless{}{-}}\NormalTok{ m }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ m }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}


\NormalTok{jtools}\SpecialCharTok{::}\FunctionTok{export\_summs}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x), }\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z))}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-10} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l l}


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 1 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 2 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} (Intercept) \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.03 *\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.02 *\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} x \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.97 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.49 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} z \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.50 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} N \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} R2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.33\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.49\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{3}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  *** p $<$ 0.001;  ** p $<$ 0.01;  * p $<$ 0.05. \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

Another setting for this bias is

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cleans workspace}
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\CommentTok{\# DAG}

\DocumentationTok{\#\# specify edges}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}z; z{-}\textgreater{}y; u{-}\textgreater{}z; u{-}\textgreater{}y\}"}\NormalTok{)}

\CommentTok{\# set u as latent}
\FunctionTok{latents}\NormalTok{(model) }\OtherTok{\textless{}{-}} \StringTok{"u"}

\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{u=}\DecValTok{3}\NormalTok{, }\AttributeTok{y=}\DecValTok{4}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{1}\NormalTok{, }\AttributeTok{u=}\DecValTok{2}\NormalTok{, }\AttributeTok{y=}\DecValTok{1}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{35-controls_files/figure-latex/unnamed-chunk-11-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{n }\OtherTok{\textless{}{-}} \FloatTok{1e4}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{u }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{z }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ u }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ z }\SpecialCharTok{+}\NormalTok{ u }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}

\NormalTok{jtools}\SpecialCharTok{::}\FunctionTok{export\_summs}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x), }\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z))}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-12} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l l}


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 1 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 2 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} (Intercept) \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.01\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.01\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} x \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.01 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.47 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} z \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.48 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} N \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} R2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.15\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.78\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{3}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  *** p $<$ 0.001;  ** p $<$ 0.01;  * p $<$ 0.05. \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

The total effect of \(X\) on \(Y\) is not biased (i.e., \(1.01 \approx 1.48 - 0.47\)).

Controlling for Z will fail to identify the direct effect of \(X\) on \(Y\) and opens the biasing path \(X \rightarrow Z \leftarrow U \rightarrow Y\)

\hypertarget{selection-bias}{%
\subsection{Selection Bias}\label{selection-bias}}

Also known as ``collider stratification bias''

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\CommentTok{\# DAG}

\DocumentationTok{\#\# specify edges}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; x{-}\textgreater{}z; u{-}\textgreater{}z;u{-}\textgreater{}y\}"}\NormalTok{)}

\CommentTok{\# set u as latent}
\FunctionTok{latents}\NormalTok{(model) }\OtherTok{\textless{}{-}} \StringTok{"u"}

\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{u=}\DecValTok{2}\NormalTok{, }\AttributeTok{y=}\DecValTok{3}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{3}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{u=}\DecValTok{4}\NormalTok{, }\AttributeTok{y=}\DecValTok{3}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{35-controls_files/figure-latex/unnamed-chunk-13-1} \end{center}

Adjusting \(Z\) opens the colliding path \(X \to Z \leftarrow U \to Y\)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \FloatTok{1e4}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{u }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{z }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ u }\SpecialCharTok{+}  \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{+} \DecValTok{2}\SpecialCharTok{*}\NormalTok{u }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}

\NormalTok{jtools}\SpecialCharTok{::}\FunctionTok{export\_summs}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x), }\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z))}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-14} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l l}


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 1 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 2 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} (Intercept) \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.01\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.01\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} x \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.97 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.03\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} z \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.00 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} N \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} R2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.16\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.49\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{3}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  *** p $<$ 0.001;  ** p $<$ 0.01;  * p $<$ 0.05. \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

Another setting is

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\CommentTok{\# DAG}

\DocumentationTok{\#\# specify edges}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; x{-}\textgreater{}z; y{-}\textgreater{}z\}"}\NormalTok{)}

\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{y=}\DecValTok{3}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{2}\NormalTok{, }\AttributeTok{z=}\DecValTok{1}\NormalTok{, }\AttributeTok{y=}\DecValTok{2}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{35-controls_files/figure-latex/unnamed-chunk-15-1} \end{center}

Controlling \(Z\) opens the colliding path \(X \to Z \leftarrow Y\)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \FloatTok{1e4}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{z }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ y }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}

\NormalTok{jtools}\SpecialCharTok{::}\FunctionTok{export\_summs}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x), }\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z))}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-16} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l l}


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 1 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 2 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} (Intercept) \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.00\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.00\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} x \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.03 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.00\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} z \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.51 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.00)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} N \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} R2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.51\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.76\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{3}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  *** p $<$ 0.001;  ** p $<$ 0.01;  * p $<$ 0.05. \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

\hypertarget{case-control-bias}{%
\subsection{Case-control Bias}\label{case-control-bias}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\CommentTok{\# DAG}

\DocumentationTok{\#\# specify edges}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; y{-}\textgreater{}z\}"}\NormalTok{)}

\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{y=}\DecValTok{3}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{2}\NormalTok{, }\AttributeTok{z=}\DecValTok{1}\NormalTok{, }\AttributeTok{y=}\DecValTok{2}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{35-controls_files/figure-latex/unnamed-chunk-17-1} \end{center}

Controlling \(Z\) opens a virtual collider (a descendant of a collider).

However, if \(X\) truly has no causal effect on \(Y\). Then, controlling for \(Z\) is valid for testing whether the effect of \(X\) on \(Y\) is 0 because X is d-separated from \(Y\) regardless of adjusting for \(Z\)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \FloatTok{1e4}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{z }\OtherTok{\textless{}{-}}\NormalTok{ y }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}

\NormalTok{jtools}\SpecialCharTok{::}\FunctionTok{export\_summs}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x), }\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z))}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-18} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l l}


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 1 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 2 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} (Intercept) \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.00\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.00\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} x \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.00 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.50 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} z \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.50 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.00)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} N \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} R2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.50\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.75\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{3}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  *** p $<$ 0.001;  ** p $<$ 0.01;  * p $<$ 0.05. \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

\hypertarget{good-controls}{%
\section{Good Controls}\label{good-controls}}

\hypertarget{omitted-variable-bias-correction}{%
\subsection{Omitted Variable Bias Correction}\label{omitted-variable-bias-correction}}

This is when \(Z\) can block all back-door paths.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; z{-}\textgreater{}x; z{-}\textgreater{}y\}"}\NormalTok{)}

\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{y=}\DecValTok{3}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{y=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{35-controls_files/figure-latex/unnamed-chunk-19-1} \end{center}

Unadjusted estimate is biased

adjusting for \(Z\) blocks the backdoor path

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \FloatTok{1e4}
\NormalTok{z }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{causal\_coef }\OtherTok{=} \DecValTok{2}
\NormalTok{beta2 }\OtherTok{=} \DecValTok{3}
\NormalTok{x }\OtherTok{\textless{}{-}}\NormalTok{ z }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ causal\_coef }\SpecialCharTok{*}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ beta2 }\SpecialCharTok{*}\NormalTok{ z }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}

\NormalTok{jtools}\SpecialCharTok{::}\FunctionTok{export\_summs}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x), }\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z))}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-20} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l l}


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 1 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 2 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} (Intercept) \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.01\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.00\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} x \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 3.51 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 2.00 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} z \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 3.02 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} N \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} R2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.82\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.97\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{3}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  *** p $<$ 0.001;  ** p $<$ 0.01;  * p $<$ 0.05. \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cleans workspace}
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\CommentTok{\# Draw DAG}

\CommentTok{\# specify edges}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; u{-}\textgreater{}z; z{-}\textgreater{}x; u{-}\textgreater{}y\}"}\NormalTok{)}

\CommentTok{\# set u as latent}
\FunctionTok{latents}\NormalTok{(model) }\OtherTok{\textless{}{-}} \StringTok{"u"}

\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{u=}\DecValTok{3}\NormalTok{, }\AttributeTok{y =} \DecValTok{4}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{y=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{u =} \DecValTok{3}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{35-controls_files/figure-latex/unnamed-chunk-21-1} \end{center}

Unadjusted estimate is biased

adjusting for \(Z\) blocks the backdoor door path due to \(U\)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \FloatTok{1e4}
\NormalTok{u }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{z }\OtherTok{\textless{}{-}}\NormalTok{ u }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{causal\_coef }\OtherTok{=} \DecValTok{2}
\NormalTok{x }\OtherTok{\textless{}{-}}\NormalTok{ z }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ causal\_coef }\SpecialCharTok{*}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ u }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}

\NormalTok{jtools}\SpecialCharTok{::}\FunctionTok{export\_summs}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x), }\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z))}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-22} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l l}


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 1 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 2 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} (Intercept) \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.03 *\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.03 *\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} x \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 2.34 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 2.01 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} z \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.49 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} N \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} R2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.91\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.92\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{3}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  *** p $<$ 0.001;  ** p $<$ 0.01;  * p $<$ 0.05. \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

Even though \(Z\) is significant, we cannot give it a causal interpretation.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cleans workspace}
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\CommentTok{\# Draw DAG}

\CommentTok{\# specify edges}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; u{-}\textgreater{}z; u{-}\textgreater{}x; z{-}\textgreater{}y\}"}\NormalTok{)}

\CommentTok{\# set u as latent}
\FunctionTok{latents}\NormalTok{(model) }\OtherTok{\textless{}{-}} \StringTok{"u"}

\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{3}\NormalTok{, }\AttributeTok{u=}\DecValTok{2}\NormalTok{, }\AttributeTok{y =} \DecValTok{4}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{y=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{u =} \DecValTok{3}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{35-controls_files/figure-latex/unnamed-chunk-23-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n     }\OtherTok{\textless{}{-}} \FloatTok{1e4}
\NormalTok{u     }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{z     }\OtherTok{\textless{}{-}}\NormalTok{ u }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{x     }\OtherTok{\textless{}{-}}\NormalTok{ u }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{causal\_coef }\OtherTok{\textless{}{-}} \DecValTok{2}
\NormalTok{y     }\OtherTok{\textless{}{-}}\NormalTok{ causal\_coef }\SpecialCharTok{*}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}

\NormalTok{jtools}\SpecialCharTok{::}\FunctionTok{export\_summs}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x), }\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z))}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-24} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l l}


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 1 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 2 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} (Intercept) \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.03\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.01\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} x \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 2.51 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 2.01 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} z \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.01 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} N \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} R2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.84\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.93\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{3}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  *** p $<$ 0.001;  ** p $<$ 0.01;  * p $<$ 0.05. \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

Even though \(Z\) is significant, we cannot give it a causal interpretation.

\textbf{Summary}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cleans workspace}
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\CommentTok{\# Model 1 }

\NormalTok{model1 }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; z{-}\textgreater{}x; z{-}\textgreater{}y\}"}\NormalTok{)}

\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model1) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{y=}\DecValTok{3}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{y=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{))}



\CommentTok{\# Model 2}

\CommentTok{\# specify edges}
\NormalTok{model2 }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; u{-}\textgreater{}z; z{-}\textgreater{}x; u{-}\textgreater{}y\}"}\NormalTok{)}

\CommentTok{\# set u as latent}
\FunctionTok{latents}\NormalTok{(model2) }\OtherTok{\textless{}{-}} \StringTok{"u"}

\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model2) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{u=}\DecValTok{3}\NormalTok{, }\AttributeTok{y =} \DecValTok{4}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{y=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{u =} \DecValTok{3}\NormalTok{))}



\CommentTok{\# Model 3}

\CommentTok{\# specify edges}
\NormalTok{model3 }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; u{-}\textgreater{}z; u{-}\textgreater{}x; z{-}\textgreater{}y\}"}\NormalTok{)}

\CommentTok{\# set u as latent}
\FunctionTok{latents}\NormalTok{(model3) }\OtherTok{\textless{}{-}} \StringTok{"u"}

\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model3) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{3}\NormalTok{, }\AttributeTok{u=}\DecValTok{2}\NormalTok{, }\AttributeTok{y =} \DecValTok{4}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{y=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{u =} \DecValTok{3}\NormalTok{))}

\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model1) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{35-controls_files/figure-latex/unnamed-chunk-25-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model2) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{35-controls_files/figure-latex/unnamed-chunk-25-2} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model3) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{35-controls_files/figure-latex/unnamed-chunk-25-3} \end{center}

\hypertarget{omitted-variable-bias-in-mediation-correction}{%
\subsection{Omitted Variable Bias in Mediation Correction}\label{omitted-variable-bias-in-mediation-correction}}

Common causes of \(X\) and any mediator (between \(X\) and \(Y\)) confound the effect of \(X\) on \(Y\)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cleans workspace}
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\CommentTok{\# DAG}

\DocumentationTok{\#\# specify edges}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; z{-}\textgreater{}x; x{-}\textgreater{}m; z{-}\textgreater{}m; m{-}\textgreater{}y\}"}\NormalTok{)}

\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{m=}\DecValTok{3}\NormalTok{, }\AttributeTok{y=}\DecValTok{4}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{m=}\DecValTok{1}\NormalTok{, }\AttributeTok{y=}\DecValTok{1}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{35-controls_files/figure-latex/unnamed-chunk-26-1} \end{center}

\(Z\) is a confounder of both the mediator \(M\) and \(X\)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n     }\OtherTok{\textless{}{-}} \FloatTok{1e4}
\NormalTok{z     }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{x     }\OtherTok{\textless{}{-}}\NormalTok{ z }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{causal\_coef }\OtherTok{\textless{}{-}} \DecValTok{2}
\NormalTok{m     }\OtherTok{\textless{}{-}}\NormalTok{ causal\_coef }\SpecialCharTok{*}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{y     }\OtherTok{\textless{}{-}}\NormalTok{ m }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}

\NormalTok{jtools}\SpecialCharTok{::}\FunctionTok{export\_summs}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x), }\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z))}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-27} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l l}


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 1 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 2 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} (Intercept) \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.02\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.01\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} x \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 2.49 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.97 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} z \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.02 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} N \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} R2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.83\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.86\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{3}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  *** p $<$ 0.001;  ** p $<$ 0.01;  * p $<$ 0.05. \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cleans workspace}
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\CommentTok{\# DAG}

\DocumentationTok{\#\# specify edges}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; u{-}\textgreater{}z; z{-}\textgreater{}x; x{-}\textgreater{}m; u{-}\textgreater{}m; m{-}\textgreater{}y\}"}\NormalTok{)}

\CommentTok{\# set u as latent}
\FunctionTok{latents}\NormalTok{(model) }\OtherTok{\textless{}{-}} \StringTok{"u"}

\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{u=}\DecValTok{3}\NormalTok{, }\AttributeTok{m=}\DecValTok{4}\NormalTok{, }\AttributeTok{y=}\DecValTok{5}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{u=}\DecValTok{3}\NormalTok{, }\AttributeTok{m=}\DecValTok{1}\NormalTok{, }\AttributeTok{y=}\DecValTok{1}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{35-controls_files/figure-latex/unnamed-chunk-28-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n     }\OtherTok{\textless{}{-}} \FloatTok{1e4}
\NormalTok{u     }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{z     }\OtherTok{\textless{}{-}}\NormalTok{ u }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{x     }\OtherTok{\textless{}{-}}\NormalTok{ z }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{causal\_coef }\OtherTok{\textless{}{-}} \DecValTok{2}
\NormalTok{m     }\OtherTok{\textless{}{-}}\NormalTok{ causal\_coef }\SpecialCharTok{*}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ u }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{y     }\OtherTok{\textless{}{-}}\NormalTok{ m }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}

\NormalTok{jtools}\SpecialCharTok{::}\FunctionTok{export\_summs}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x), }\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z))}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-29} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l l}


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 1 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 2 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} (Intercept) \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.01\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.01\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} x \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 2.31 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 2.00 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} z \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.49 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} N \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} R2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.86\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.86\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{3}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  *** p $<$ 0.001;  ** p $<$ 0.01;  * p $<$ 0.05. \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cleans workspace}
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\CommentTok{\# DAG}

\DocumentationTok{\#\# specify edges}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; u{-}\textgreater{}z; z{-}\textgreater{}m; x{-}\textgreater{}m; u{-}\textgreater{}x; m{-}\textgreater{}y\}"}\NormalTok{)}

\CommentTok{\# set u as latent}
\FunctionTok{latents}\NormalTok{(model) }\OtherTok{\textless{}{-}} \StringTok{"u"}

\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{3}\NormalTok{, }\AttributeTok{u=}\DecValTok{2}\NormalTok{, }\AttributeTok{m=}\DecValTok{4}\NormalTok{, }\AttributeTok{y=}\DecValTok{5}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{u=}\DecValTok{3}\NormalTok{, }\AttributeTok{m=}\DecValTok{1}\NormalTok{, }\AttributeTok{y=}\DecValTok{1}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{35-controls_files/figure-latex/unnamed-chunk-30-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n     }\OtherTok{\textless{}{-}} \FloatTok{1e4}
\NormalTok{u     }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{z     }\OtherTok{\textless{}{-}}\NormalTok{ u }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{x     }\OtherTok{\textless{}{-}}\NormalTok{ u }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{causal\_coef }\OtherTok{\textless{}{-}} \DecValTok{2}
\NormalTok{m     }\OtherTok{\textless{}{-}}\NormalTok{ causal\_coef }\SpecialCharTok{*}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{y     }\OtherTok{\textless{}{-}}\NormalTok{ m }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}

\NormalTok{jtools}\SpecialCharTok{::}\FunctionTok{export\_summs}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x), }\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z))}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-31} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l l}


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 1 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 2 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} (Intercept) \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.01\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.00\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} x \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 2.50 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.99 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} z \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.02 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} N \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} R2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.78\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.87\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{3}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  *** p $<$ 0.001;  ** p $<$ 0.01;  * p $<$ 0.05. \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

\textbf{Summary}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# model 4}
\NormalTok{model4 }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; z{-}\textgreater{}x; x{-}\textgreater{}m; z{-}\textgreater{}m; m{-}\textgreater{}y\}"}\NormalTok{)}

\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model4) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{m=}\DecValTok{3}\NormalTok{, }\AttributeTok{y=}\DecValTok{4}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{m=}\DecValTok{1}\NormalTok{, }\AttributeTok{y=}\DecValTok{1}\NormalTok{))}


\CommentTok{\# model 5}
\NormalTok{model5 }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; u{-}\textgreater{}z; z{-}\textgreater{}x; x{-}\textgreater{}m; u{-}\textgreater{}m; m{-}\textgreater{}y\}"}\NormalTok{)}

\CommentTok{\# set u as latent}
\FunctionTok{latents}\NormalTok{(model5) }\OtherTok{\textless{}{-}} \StringTok{"u"}

\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model5) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{u=}\DecValTok{3}\NormalTok{, }\AttributeTok{m=}\DecValTok{4}\NormalTok{, }\AttributeTok{y=}\DecValTok{5}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{u=}\DecValTok{3}\NormalTok{, }\AttributeTok{m=}\DecValTok{1}\NormalTok{, }\AttributeTok{y=}\DecValTok{1}\NormalTok{))}


\CommentTok{\# model 6}

\NormalTok{model6 }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; u{-}\textgreater{}z; z{-}\textgreater{}m; x{-}\textgreater{}m; u{-}\textgreater{}x; m{-}\textgreater{}y\}"}\NormalTok{)}

\CommentTok{\# set u as latent}
\FunctionTok{latents}\NormalTok{(model6) }\OtherTok{\textless{}{-}} \StringTok{"u"}

\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model6) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{3}\NormalTok{, }\AttributeTok{u=}\DecValTok{2}\NormalTok{, }\AttributeTok{m=}\DecValTok{4}\NormalTok{, }\AttributeTok{y=}\DecValTok{5}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{u=}\DecValTok{3}\NormalTok{, }\AttributeTok{m=}\DecValTok{1}\NormalTok{, }\AttributeTok{y=}\DecValTok{1}\NormalTok{))}

\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model4) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{35-controls_files/figure-latex/unnamed-chunk-32-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model5) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{35-controls_files/figure-latex/unnamed-chunk-32-2} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model6) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{35-controls_files/figure-latex/unnamed-chunk-32-3} \end{center}

\hypertarget{neutral-controls}{%
\section{Neutral Controls}\label{neutral-controls}}

\hypertarget{good-predictive-controls}{%
\subsection{Good Predictive Controls}\label{good-predictive-controls}}

Good for precision

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cleans workspace}
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\CommentTok{\# DAG}

\DocumentationTok{\#\# specify edges}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; z{-}\textgreater{}y\}"}\NormalTok{)}


\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{y=}\DecValTok{2}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{y=}\DecValTok{1}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{35-controls_files/figure-latex/unnamed-chunk-33-1} \end{center}

Controlling for \(Z\) does not help or hurt identification, but it can increase precision (i.e., reducing SE)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \FloatTok{1e4}
\NormalTok{z }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{+} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ z }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}

\NormalTok{jtools}\SpecialCharTok{::}\FunctionTok{export\_summs}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x), }\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z))}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-34} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l l}


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 1 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 2 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} (Intercept) \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.01\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.01\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} x \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.00 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.01 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} z \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 2.00 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} N \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} R2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.17\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.83\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{3}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  *** p $<$ 0.001;  ** p $<$ 0.01;  * p $<$ 0.05. \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

Similar coefficients, but smaller SE when controlling for \(Z\)

Another variation is

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cleans workspace}
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\CommentTok{\# DAG}

\DocumentationTok{\#\# specify edges}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; x{-}\textgreater{}m; z{-}\textgreater{}m; m{-}\textgreater{}y\}"}\NormalTok{)}

\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{m=}\DecValTok{2}\NormalTok{, }\AttributeTok{y=}\DecValTok{3}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{m=}\DecValTok{1}\NormalTok{, }\AttributeTok{y=}\DecValTok{1}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{35-controls_files/figure-latex/unnamed-chunk-35-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \FloatTok{1e4}
\NormalTok{z }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{m }\OtherTok{\textless{}{-}} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ z }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{+} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ m }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}

\NormalTok{jtools}\SpecialCharTok{::}\FunctionTok{export\_summs}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x), }\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z))}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-36} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l l}


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 1 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 2 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} (Intercept) \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.00\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.00\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.05)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} x \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.97 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.99 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.05)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} z \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 4.02 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} N \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} R2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.04\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.77\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{3}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  *** p $<$ 0.001;  ** p $<$ 0.01;  * p $<$ 0.05. \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

Controlling for \(Z\) can reduce SE

\hypertarget{good-selection-bias}{%
\subsection{Good Selection Bias}\label{good-selection-bias}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cleans workspace}
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\CommentTok{\# DAG}

\DocumentationTok{\#\# specify edges}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; x{-}\textgreater{}z; z{-}\textgreater{}w; u{-}\textgreater{}w;u{-}\textgreater{}y\}"}\NormalTok{)}

\CommentTok{\# set u as latent}
\FunctionTok{latents}\NormalTok{(model) }\OtherTok{\textless{}{-}} \StringTok{"u"}

\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{w=}\DecValTok{3}\NormalTok{, }\AttributeTok{u=}\DecValTok{3}\NormalTok{, }\AttributeTok{y=}\DecValTok{5}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{3}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{w=}\DecValTok{1}\NormalTok{, }\AttributeTok{u=}\DecValTok{4}\NormalTok{, }\AttributeTok{y=}\DecValTok{3}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{35-controls_files/figure-latex/unnamed-chunk-37-1} \end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Unadjusted estimate is unbiased
\item
  Controlling for Z can increase SE
\item
  Controlling for Z while having on W can help identify X
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \FloatTok{1e4}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{u }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{z }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{w }\OtherTok{\textless{}{-}}\NormalTok{ z }\SpecialCharTok{+}\NormalTok{ u }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{{-}} \DecValTok{2}\SpecialCharTok{*}\NormalTok{u }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}

\NormalTok{jtools}\SpecialCharTok{::}\FunctionTok{export\_summs}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x), }\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ w), }\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z }\SpecialCharTok{+}\NormalTok{ w))}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-38} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l l l}


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 1 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 3 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} (Intercept) \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.01\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.01\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.03\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} x \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.99 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.65 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.99 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} w \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.67 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -1.01 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} z \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.02 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} N \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} R2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.16\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.39\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.50\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{4}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  *** p $<$ 0.001;  ** p $<$ 0.01;  * p $<$ 0.05. \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

\hypertarget{bad-predictive-controls}{%
\subsection{Bad Predictive Controls}\label{bad-predictive-controls}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cleans workspace}
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\CommentTok{\# DAG}

\DocumentationTok{\#\# specify edges}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; z{-}\textgreater{}x\}"}\NormalTok{)}


\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{1}\NormalTok{, }\AttributeTok{y=}\DecValTok{2}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{y=}\DecValTok{1}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{35-controls_files/figure-latex/unnamed-chunk-39-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \FloatTok{1e4}
\NormalTok{z }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{x }\OtherTok{\textless{}{-}} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ z }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{+} \DecValTok{2} \SpecialCharTok{*} \FunctionTok{rnorm}\NormalTok{(n)}

\NormalTok{jtools}\SpecialCharTok{::}\FunctionTok{export\_summs}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x), }\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z))}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-40} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l l}


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 1 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 2 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} (Intercept) \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.02\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.02\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} x \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.99 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.00 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} z \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.00\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.04)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} N \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} R2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.55\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.55\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{3}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  *** p $<$ 0.001;  ** p $<$ 0.01;  * p $<$ 0.05. \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

Similar coefficients, but greater SE when controlling for \(Z\)

Another variation is

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\CommentTok{\# DAG}

\DocumentationTok{\#\# specify edges}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; x{-}\textgreater{}z\}"}\NormalTok{)}


\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{1}\NormalTok{, }\AttributeTok{y=}\DecValTok{2}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{y=}\DecValTok{1}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{35-controls_files/figure-latex/unnamed-chunk-41-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{n }\OtherTok{\textless{}{-}} \FloatTok{1e4}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{z }\OtherTok{\textless{}{-}} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ x }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{+} \DecValTok{2} \SpecialCharTok{*} \FunctionTok{rnorm}\NormalTok{(n)}

\NormalTok{jtools}\SpecialCharTok{::}\FunctionTok{export\_summs}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x), }\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z))}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-42} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l l}


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 1 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 2 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} (Intercept) \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.02\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.02\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} x \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.00 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.99 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.05)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} z \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.00\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} N \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} R2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.20\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.20\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{3}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  *** p $<$ 0.001;  ** p $<$ 0.01;  * p $<$ 0.05. \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

Worse SE when controlling for \(Z\) (\(0.02 < 0.05\))

\hypertarget{bad-selection-bias}{%
\subsection{Bad Selection Bias}\label{bad-selection-bias}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cleans workspace}
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\CommentTok{\# DAG}

\DocumentationTok{\#\# specify edges}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; x{-}\textgreater{}z\}"}\NormalTok{)}


\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{y=}\DecValTok{2}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{y=}\DecValTok{1}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{35-controls_files/figure-latex/unnamed-chunk-43-1} \end{center}

Not all post-treatment variables are bad.

Controlling for \(Z\) is neutral, but it might hurt the precision of the causal effect.

\hypertarget{choosing-controls}{%
\section{Choosing Controls}\label{choosing-controls}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(pcalg)}
\FunctionTok{library}\NormalTok{(dagitty)}
\FunctionTok{library}\NormalTok{(causaleffect)}
\end{Highlighting}
\end{Shaded}

By providing a causal diagram, deciding the appropriateness of controls are automated.

\begin{itemize}
\item
  \href{https://causalfusion.net/login}{Fusion}
\item
  \href{http://dagitty.net/}{DAGitty}
\end{itemize}

Guide on how to choose confounders: \citet{vanderweele2019principles}

In cases where it's hard to determine the plausibility of controls, we might need to further analysis.

\texttt{sensemakr} provides such tools.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(sensemakr)}
\end{Highlighting}
\end{Shaded}

In simple cases, we can follow the simple rules of thumb provided by \citet{steinmetz2022meta} (p.~614, Fig 2)

\includegraphics[width=6.25in,height=4.16667in]{images/control_var_decision.png}

\hypertarget{mediation}{%
\chapter{Mediation}\label{mediation}}

\hypertarget{traditional-approach}{%
\section{Traditional Approach}\label{traditional-approach}}

\citet{baron1986moderator} is outdated because of step 1, but we could still see the original idea.

3 regressions

\begin{itemize}
\item
  Step 1: \(X \to Y\)
\item
  Step 2: \(X \to M\)
\item
  Step 3: \(X + M \to Y\)
\end{itemize}

where

\begin{itemize}
\item
  \(X\) = independent (causal) variable
\item
  \(Y\) = dependent (outcome) variable
\item
  \(M\) = mediating variable
\end{itemize}

\textbf{Note}: Originally, the first path from \(X \to Y\) suggested by \citep{baron1986moderator} needs to be significant. But there are cases in which you could have indirect of \(X\) on \(Y\) without significant direct effect of \(X\) on \(Y\) (e.g., when the effect is absorbed into M, or there are two counteracting effects \(M_1, M_2\) that cancel out each other effect).

\begin{figure}
\centering
\includegraphics[width=0.9\textwidth,height=\textheight]{images/mediation/direct_mod.png}
\caption{Unmediated model}
\end{figure}

where \(c\) is the \textbf{total effect}

\includegraphics[width=0.9\textwidth,height=\textheight]{images/mediation/full_mod.png}

where

\begin{itemize}
\item
  \(c'\) = \textbf{direct effect} (effect of \(X\) on \(Y\) after accounting for the indirect path)
\item
  \(ab\) = \textbf{indirect effect}
\end{itemize}

Hence,

\[
\begin{aligned}
\text{total effect} &= \text{direct effect} + \text{indirect effect} \\
c &= c' + ab
\end{aligned}
\]

However, this simple equation does not only hold in cases of

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Models with latent variables
\item
  Logistic models (only approximately). Hence, you can only calculate \(c\) as the total effect of \(c' + ab\)
\item
  Multi-level models \citep{bauer2006conceptualizing}
\end{enumerate}

To measure mediation (i.e., indirect effect),

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(1 - \frac{c'}{c}\) highly unstable \citep{mackinnon1995simulation}, especially in cases that \(c\) is small (not re* recommended)
\item
  \textbf{Product method}: \(a\times b\)
\item
  \textbf{Difference method}: \(c- c'\)
\end{enumerate}

For linear models, we have the following assumptions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  No unmeasured confound between \(X-Y\), \(X-M\) and \(M-Y\) relationships.
\item
  \(X \not\rightarrow C\) where \(C\) is a confounder between \(M-Y\) relationship
\item
  \protect\hyperlink{reliability}{Reliability}: No errors in measurement of \(M\) (also known as reliability assumption) (can consider errors-in-variables models)
\end{enumerate}

Mathematically,

\[
Y = b_0 + b_1 X + \epsilon
\]

\(b_1\) does \textbf{not} need to be \textbf{significant}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  We examine the effect of \(X\) on \(M\). This step requires that there is a significant effect of \(X\) on \(M\) to continue with the analysis
\end{enumerate}

Mathematically,

\[
M = b_0 + b_2 X + \epsilon
\]

where \(b_2\) needs to be \textbf{significant}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  In this step, we want to the effect of \(M\) on \(Y\) ``absorbs'' most of the direct effect of \(X\) on \(Y\) (or at least makes the effect smaller).
\end{enumerate}

Mathematically,

\[
Y = b_0 + b_4 X + b_3 M + \epsilon
\]

\(b_4\) needs to be either smaller or insignificant.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
The effect of \(X\) on \(Y\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
then, \(M\) \ldots{} mediates between \(X\) and \(Y\)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
completely disappear (\(b_4\) insignificant) & Fully (i.e., full mediation) \\
partially disappear (\(b_4 < b_1\) in step 1) & Partially (i.e., partial mediation) \\
\end{longtable}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Examine the mediation effect (i.e., whether it is significant)
\end{enumerate}

\begin{itemize}
\item
  \protect\hyperlink{sobel-test}{Sobel Test} \citep{sobel1982asymptotic}
\item
  \protect\hyperlink{joint-significance-test}{Joint Significance Test}
\item
  \protect\hyperlink{bootstrapping}{Bootstrapping} \citep[\citet{shrout2002mediation}]{preacher2004spss} (preferable)
\end{itemize}

\textbf{Notes}:

\begin{itemize}
\item
  Proximal mediation (\(a > b\)) can lead to multicollinearity and reduce statistical power, whereas distal mediation (\(b > a\)) is preferred for maximizing test power.
\item
  The ideal balance for maximizing power in mediation analysis involves slightly distal mediators (i.e., path \(b\) is somewhat larger than path \(a\)) \citep{hoyle1999statistical}.
\item
  Tests for direct effects (c and c') have lower power compared to the indirect effect (ab), making it possible for ab to be significant while c is not, even in cases where there seems to be complete mediation but no statistical evidence of a direct cause-effect relationship between X and Y without considering M \citep{kenny2014power}.
\item
  The testing of \(ab\) offers a power advantage over \(c\) because it effectively combines two tests. However, claims of complete mediation based solely on the non-significance of \(c\) should be approached with caution, emphasizing the need for sufficient sample size and power, especially in assessing partial mediation. Or one should never make complete mediation claim \citep{hayes2013relative}
\end{itemize}

\hypertarget{assumptions-2}{%
\subsection{Assumptions}\label{assumptions-2}}

\hypertarget{direction}{%
\subsubsection{Direction}\label{direction}}

\begin{itemize}
\item
  Quick fix but not convincing: Measure \(X\) before \(M\) and \(Y\) to prevent \(M\) or \(Y\) causing \(X\); measure \(M\) before \(Y\) to avoid \(Y\) causing \(M\).
\item
  \(Y\) may cause \(M\) in a feedback model.

  \begin{itemize}
  \item
    Assuming \(c' =0\) (full mediation) allows for estimating models with reciprocal causal effects between \(M\) and \(Y\) via IV estimation.
  \item
    \citet{smith1982beliefs} proposes treating both \(M\) and \(Y\) as outcomes with potential to mediate each other, requiring distinct instrumental variables for each that do not affect the other.
  \end{itemize}
\end{itemize}

\hypertarget{interaction}{%
\subsubsection{Interaction}\label{interaction}}

\begin{itemize}
\item
  When M interact with X to affect Y, M is both a mediator and a mediator \citep{baron1986moderator}.
\item
  Interaction between \(XM\) should always be estimated.
\item
  For the interpretation of this interaction, see \citep{vanderweele2015explanation}
\end{itemize}

\hypertarget{reliability}{%
\subsubsection{Reliability}\label{reliability}}

\begin{itemize}
\item
  When mediator contains measurement errors, \(b, c'\) are biased. Possible fix: mediator = latent variable (but loss of power) \citep{ledgerwood2011trade}

  \begin{itemize}
  \item
    \(b\) is attenuated (closer to 0)
  \item
    \(c'\) is

    \begin{itemize}
    \item
      overestimated when \(ab >0\)
    \item
      underestiamted when \(ab<0\)
    \end{itemize}
  \end{itemize}
\item
  When treatment contains measurement errors, \(a,b\) are biased

  \begin{itemize}
  \item
    \(a\) is attenuated
  \item
    \(b\) is

    \begin{itemize}
    \item
      overestimated when \(ac'>0\)
    \item
      underestimated when \(ac' <0\)
    \end{itemize}
  \end{itemize}
\item
  When outcome contains measurement errors,

  \begin{itemize}
  \item
    If unstandardized, no bias
  \item
    If standardized, attenuation bias
  \end{itemize}
\end{itemize}

\hypertarget{confounding}{%
\subsubsection{Confounding}\label{confounding}}

\begin{itemize}
\item
  Omitted variable bias can happen to any pair of relationships
\item
  To deal with this problem, one can either use

  \begin{itemize}
  \item
    \protect\hyperlink{design-strategies}{Design Strategies}
  \item
    \protect\hyperlink{statistical-strategies}{Statistical Strategies}
  \end{itemize}
\end{itemize}

\hypertarget{design-strategies}{%
\paragraph{Design Strategies}\label{design-strategies}}

\begin{itemize}
\item
  \textbf{Randomization} of treatment variable. If possible, also mediator
\item
  \textbf{Control} for the confounder (but still only for measureable observables)
\end{itemize}

\hypertarget{statistical-strategies}{%
\paragraph{Statistical Strategies}\label{statistical-strategies}}

\begin{itemize}
\item
  \textbf{Instrumental} \textbf{variable} on treatment

  \begin{itemize}
  \tightlist
  \item
    Specifically for confounder affecting the \(M-Y\) pair, front-door adjustment is possible when there is a variable that completely mediates the effect of the mediator on the outcome and is unaffected by the confounder.
  \end{itemize}
\item
  \textbf{Weighting} methods (e.g., inverse propensity) See \href{https://www.andrewheiss.com/blog/2020/12/01/ipw-binary-continuous/}{Heiss} for R code

  \begin{itemize}
  \tightlist
  \item
    Need strong ignorability assumption (i.e.., all confounders are included and measured without error \citep{westfall2016statistically}). Not fixable, but can be examined with robustness checks.
  \end{itemize}
\end{itemize}

\hypertarget{indirect-effect-tests}{%
\subsection{Indirect Effect Tests}\label{indirect-effect-tests}}

\hypertarget{sobel-test}{%
\subsubsection{Sobel Test}\label{sobel-test}}

\begin{itemize}
\item
  developed by \citet{sobel1982asymptotic}
\item
  also known as the \textbf{delta method}
\item
  not recommend because it assumes the indirect effect \(b\) has a normal distribution when it's not \citep{mackinnon1995simulation}.
\item
  Mediation can occur even if direct and indirect effects oppose each other, termed ``inconsistent mediation'' \citep{mackinnon2007mediation}. This is when the mediator acts as a suppressor variable.
\end{itemize}

Standard Error

\[
\sqrt{\hat{b}^2 s_{\hat{a}} + \hat{a}^2 s_{b}^2}
\]

The test of the indirect effect is

\[
z = \frac{\hat{ab}}{\sqrt{\hat{b}^2 s_{\hat{a}} + \hat{a}^2 s_{b}^2}}
\]

Disadvantages

\begin{itemize}
\item
  Assume \(a\) and \(b\) are independent.
\item
  Assume \(ab\) is normally distributed.
\item
  Does not work well for small sample sizes.
\item
  Power of the test is low and the test is conservative as compared to \protect\hyperlink{bootstrapping}{Bootstrapping}.
\end{itemize}

\hypertarget{joint-significance-test}{%
\subsubsection{Joint Significance Test}\label{joint-significance-test}}

\begin{itemize}
\item
  Effective for determining if the indirect effect is nonzero (by testing whether \(a\) and \(b\) are both statistically significant), assumes \(a \perp b\).
\item
  It's recommended to use it with other tests and has similar performance to a \protect\hyperlink{bootstrapping}{Bootstrapping} test \citep{hayes2013relative}.
\item
  The test's accuracy can be affected by heteroscedasticity \citep{fossum2023use} but not by non-normality.
\item
  Although helpful in computing power for the test of the indirect effect, it doesn't provide a confidence interval for the effect.
\end{itemize}

\hypertarget{bootstrapping}{%
\subsubsection{Bootstrapping}\label{bootstrapping}}

\begin{itemize}
\item
  First used by \citet{bollen1990direct}
\item
  It allows for the calculation of confidence intervals, p-values, etc.
\item
  It does not require \(a \perp b\) and corrects for bias in the bootstrapped distribution.
\item
  It can handle non-normality (in the sampling distribution of the indirect effect), complex models, and small samples.
\item
  Concerns exist about the bias-corrected bootstrapping being too liberal \citep{fritz2012explanation}. Hence, current recommendations favor percentile bootstrap without bias correction for better Type I error rates \citep{tibbe2022correcting}.
\item
  A special case of bootstrapping is a proposed by where you don't need access to raw data to generate resampling, you only need \(a, b, var(a), var(b), cov(a,b)\) (which can be taken from lots of primary studies)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{result }\OtherTok{\textless{}{-}}
\NormalTok{    causalverse}\SpecialCharTok{::}\FunctionTok{med\_ind}\NormalTok{(}
        \AttributeTok{a =} \FloatTok{0.5}\NormalTok{,}
        \AttributeTok{b =} \FloatTok{0.7}\NormalTok{,}
        \AttributeTok{var\_a =} \FloatTok{0.04}\NormalTok{,}
        \AttributeTok{var\_b =} \FloatTok{0.05}\NormalTok{,}
        \AttributeTok{cov\_ab =} \FloatTok{0.01}
\NormalTok{    )}
\NormalTok{result}\SpecialCharTok{$}\NormalTok{plot}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{36-mediation_files/figure-latex/unnamed-chunk-1-1} \end{center}

\hypertarget{with-instrument}{%
\paragraph{With Instrument}\label{with-instrument}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(DiagrammeR)}
\FunctionTok{grViz}\NormalTok{(}\StringTok{"}
\StringTok{digraph \{}
\StringTok{  graph []}
\StringTok{  node [shape = plaintext]}
\StringTok{    X [label = \textquotesingle{}Treatment\textquotesingle{}]}
\StringTok{    Y [label = \textquotesingle{}Outcome\textquotesingle{}]}
\StringTok{  edge [minlen = 2]}
\StringTok{    X{-}\textgreater{}Y}
\StringTok{  \{ rank = same; X; Y \}}
\StringTok{\}}
\StringTok{"}\NormalTok{)}

\FunctionTok{grViz}\NormalTok{(}\StringTok{"}
\StringTok{digraph \{}
\StringTok{  graph []}
\StringTok{  node [shape = plaintext]}
\StringTok{    X [label =\textquotesingle{}Treatment\textquotesingle{}, shape = box]}
\StringTok{    Y [label =\textquotesingle{}Outcome\textquotesingle{}, shape = box]}
\StringTok{    M [label =\textquotesingle{}Mediator\textquotesingle{}, shape = box]}
\StringTok{    IV [label =\textquotesingle{}Instrument\textquotesingle{}, shape = box]}
\StringTok{  edge [minlen = 2]}
\StringTok{    IV{-}\textgreater{}X}
\StringTok{    X{-}\textgreater{}M  }
\StringTok{    M{-}\textgreater{}Y }
\StringTok{    X{-}\textgreater{}Y }
\StringTok{  \{ rank = same; X; Y; M \}}
\StringTok{\}}
\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(mediation)}
\FunctionTok{data}\NormalTok{(}\StringTok{"boundsdata"}\NormalTok{)}
\FunctionTok{library}\NormalTok{(fixest)}

\CommentTok{\# Total Effect}
\NormalTok{out1 }\OtherTok{\textless{}{-}} \FunctionTok{feols}\NormalTok{(out }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ttt, }\AttributeTok{data =}\NormalTok{ boundsdata)}

\CommentTok{\# Indirect Effect}
\NormalTok{out2 }\OtherTok{\textless{}{-}} \FunctionTok{feols}\NormalTok{(med }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ttt, }\AttributeTok{data =}\NormalTok{ boundsdata)}

\CommentTok{\# Direct and Indirect Effect}
\NormalTok{out3 }\OtherTok{\textless{}{-}} \FunctionTok{feols}\NormalTok{(out }\SpecialCharTok{\textasciitilde{}}\NormalTok{ med }\SpecialCharTok{+}\NormalTok{ ttt, }\AttributeTok{data =}\NormalTok{ boundsdata)}

\CommentTok{\# Proportion Test}
\CommentTok{\# To what extent is the effect of the treatment mediated by the mediator?}
\FunctionTok{coef}\NormalTok{(out2)[}\StringTok{\textquotesingle{}ttt\textquotesingle{}}\NormalTok{] }\SpecialCharTok{*} \FunctionTok{coef}\NormalTok{(out3)[}\StringTok{\textquotesingle{}med\textquotesingle{}}\NormalTok{] }\SpecialCharTok{/} \FunctionTok{coef}\NormalTok{(out1)[}\StringTok{\textquotesingle{}ttt\textquotesingle{}}\NormalTok{] }\SpecialCharTok{*} \DecValTok{100}
\CommentTok{\#\textgreater{}      ttt }
\CommentTok{\#\textgreater{} 68.63609}


\CommentTok{\# Sobel Test}
\NormalTok{bda}\SpecialCharTok{::}\FunctionTok{mediation.test}\NormalTok{(boundsdata}\SpecialCharTok{$}\NormalTok{med, boundsdata}\SpecialCharTok{$}\NormalTok{ttt, boundsdata}\SpecialCharTok{$}\NormalTok{out) }\SpecialCharTok{|\textgreater{}} 
\NormalTok{    tibble}\SpecialCharTok{::}\FunctionTok{rownames\_to\_column}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
\NormalTok{    causalverse}\SpecialCharTok{::}\FunctionTok{nice\_tab}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\CommentTok{\#\textgreater{}   rowname Sobel Aroian Goodman}
\CommentTok{\#\textgreater{} 1 z.value  4.05   4.03    4.07}
\CommentTok{\#\textgreater{} 2 p.value  0.00   0.00    0.00}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Mediation Analysis using boot}
\FunctionTok{library}\NormalTok{(boot)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{mediation\_fn }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, i)\{}
    \CommentTok{\# sample the dataset}
\NormalTok{    df }\OtherTok{\textless{}{-}}\NormalTok{ data[i,]}
    
    
\NormalTok{    a\_path }\OtherTok{\textless{}{-}} \FunctionTok{feols}\NormalTok{(med }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ttt, }\AttributeTok{data =}\NormalTok{ df)}
\NormalTok{    a }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(a\_path)[}\StringTok{\textquotesingle{}ttt\textquotesingle{}}\NormalTok{]}
    
\NormalTok{    b\_path }\OtherTok{\textless{}{-}}  \FunctionTok{feols}\NormalTok{(out }\SpecialCharTok{\textasciitilde{}}\NormalTok{ med }\SpecialCharTok{+}\NormalTok{ ttt, }\AttributeTok{data =}\NormalTok{ df)}
\NormalTok{    b }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(b\_path)[}\StringTok{\textquotesingle{}med\textquotesingle{}}\NormalTok{]}
    
\NormalTok{    cp }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(b\_path)[}\StringTok{\textquotesingle{}ttt\textquotesingle{}}\NormalTok{]}
    
    \CommentTok{\# indirect effect}
\NormalTok{    ind\_ef }\OtherTok{\textless{}{-}}\NormalTok{ a}\SpecialCharTok{*}\NormalTok{b}
\NormalTok{    total\_ef }\OtherTok{\textless{}{-}}\NormalTok{ a}\SpecialCharTok{*}\NormalTok{b }\SpecialCharTok{+}\NormalTok{ cp}
    \FunctionTok{return}\NormalTok{(}\FunctionTok{c}\NormalTok{(ind\_ef, total\_ef))}
    
\NormalTok{\}}

\NormalTok{boot\_med }\OtherTok{\textless{}{-}} \FunctionTok{boot}\NormalTok{(boundsdata, mediation\_fn, }\AttributeTok{R =} \DecValTok{100}\NormalTok{, }\AttributeTok{parallel =} \StringTok{"multicore"}\NormalTok{, }\AttributeTok{ncpus =} \DecValTok{2}\NormalTok{)}
\NormalTok{boot\_med }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} ORDINARY NONPARAMETRIC BOOTSTRAP}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} boot(data = boundsdata, statistic = mediation\_fn, R = 100, parallel = "multicore", }
\CommentTok{\#\textgreater{}     ncpus = 2)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Bootstrap Statistics :}
\CommentTok{\#\textgreater{}       original        bias    std. error}
\CommentTok{\#\textgreater{} t1* 0.04112035  0.0006346725 0.009539903}
\CommentTok{\#\textgreater{} t2* 0.05991068 {-}0.0004462572 0.029556611}

\FunctionTok{summary}\NormalTok{(boot\_med) }\SpecialCharTok{|\textgreater{}} 
\NormalTok{    causalverse}\SpecialCharTok{::}\FunctionTok{nice\_tab}\NormalTok{()}
\CommentTok{\#\textgreater{}     R original bootBias bootSE bootMed}
\CommentTok{\#\textgreater{} 1 100     0.04        0   0.01    0.04}
\CommentTok{\#\textgreater{} 2 100     0.06        0   0.03    0.06}

\CommentTok{\# confidence intervals (percentile is always recommended)}
\FunctionTok{boot.ci}\NormalTok{(boot\_med, }\AttributeTok{type =} \FunctionTok{c}\NormalTok{(}\StringTok{"norm"}\NormalTok{, }\StringTok{"perc"}\NormalTok{))}
\CommentTok{\#\textgreater{} BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS}
\CommentTok{\#\textgreater{} Based on 100 bootstrap replicates}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} CALL : }
\CommentTok{\#\textgreater{} boot.ci(boot.out = boot\_med, type = c("norm", "perc"))}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Intervals : }
\CommentTok{\#\textgreater{} Level      Normal             Percentile     }
\CommentTok{\#\textgreater{} 95\%   ( 0.0218,  0.0592 )   ( 0.0249,  0.0623 )  }
\CommentTok{\#\textgreater{} Calculations and Intervals on Original Scale}
\CommentTok{\#\textgreater{} Some percentile intervals may be unstable}

\CommentTok{\# point estimates (Indirect, and Total Effects)}
\FunctionTok{colMeans}\NormalTok{(boot\_med}\SpecialCharTok{$}\NormalTok{t)}
\CommentTok{\#\textgreater{} [1] 0.04175502 0.05946442}
\end{Highlighting}
\end{Shaded}

Alternatively, one can use the \texttt{robmed} package

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(robmed)}
\end{Highlighting}
\end{Shaded}

Power test or use \href{https://davidakenny.shinyapps.io/MedPower/}{app}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(pwr2ppl)}

\CommentTok{\# indirect path ab power}
\FunctionTok{medjs}\NormalTok{(}
    \CommentTok{\# X on M (path a)}
    \AttributeTok{rx1m1 =}\NormalTok{ .}\DecValTok{3}\NormalTok{,}
    \CommentTok{\# correlation between X and Y (path c\textquotesingle{})}
    \AttributeTok{rx1y  =}\NormalTok{ .}\DecValTok{1}\NormalTok{,}
    \CommentTok{\# correlation between M and Y (path b)}
    \AttributeTok{rym1  =}\NormalTok{ .}\DecValTok{3}\NormalTok{,}
    \CommentTok{\# sample size}
    \AttributeTok{n     =} \DecValTok{100}\NormalTok{,}
    \AttributeTok{alpha =} \FloatTok{0.05}\NormalTok{,}
    \CommentTok{\# number of mediators}
    \AttributeTok{mvars =} \DecValTok{1}\NormalTok{,}
    \CommentTok{\# should use 10000}
    \AttributeTok{rep   =} \DecValTok{1000}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{multiple-mediation}{%
\subsection{Multiple Mediation}\label{multiple-mediation}}

The most general package to handle multiple cases is \texttt{manymome}

See \href{https://cran.r-project.org/web/packages/manymome/vignettes/med_lm.html}{vignette} for an example

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(manymome)}
\end{Highlighting}
\end{Shaded}

\hypertarget{multiple-mediators}{%
\subsubsection{Multiple Mediators}\label{multiple-mediators}}

\begin{itemize}
\item
  \href{https://openresearchsoftware.metajnl.com/articles/10.5334/jors.160}{Notes}
\item
  \href{https://cran.r-project.org/web/packages/mma/vignettes/MMAvignette.html}{Vignette}
\item
  \href{https://cran.r-project.org/web/packages/mma/mma.pdf}{Package}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(mma)}
\end{Highlighting}
\end{Shaded}

\hypertarget{lavaan}{%
\subparagraph{Lavaan}\label{lavaan}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load required packages}
\FunctionTok{library}\NormalTok{(MASS)  }\CommentTok{\# for mvrnorm}
\FunctionTok{library}\NormalTok{(lavaan)}

\CommentTok{\# Function to generate synthetic data with correctly correlated errors for mediators}
\NormalTok{generate\_data }\OtherTok{\textless{}{-}}
  \ControlFlowTok{function}\NormalTok{(}\AttributeTok{n =} \DecValTok{10000}\NormalTok{,}
           \AttributeTok{a1 =} \FloatTok{0.5}\NormalTok{,}
           \AttributeTok{a2 =} \SpecialCharTok{{-}}\FloatTok{0.35}\NormalTok{,}
           \AttributeTok{b1 =} \FloatTok{0.7}\NormalTok{,}
           \AttributeTok{b2 =} \FloatTok{0.48}\NormalTok{,}
           \AttributeTok{corr =} \ConstantTok{TRUE}\NormalTok{,}
           \AttributeTok{correlation\_value =} \FloatTok{0.7}\NormalTok{) \{}
    \FunctionTok{set.seed}\NormalTok{(}\DecValTok{12345}\NormalTok{)}
\NormalTok{    X }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
    
    \CommentTok{\# Generate correlated errors using a multivariate normal distribution}
    \ControlFlowTok{if}\NormalTok{ (corr) \{}
\NormalTok{        Sigma }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, correlation\_value, correlation\_value, }\DecValTok{1}\NormalTok{), }\AttributeTok{nrow =} \DecValTok{2}\NormalTok{)  }\CommentTok{\# Higher covariance matrix for errors}
\NormalTok{        errors }\OtherTok{\textless{}{-}} \FunctionTok{mvrnorm}\NormalTok{(n, }\AttributeTok{mu =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{), }\AttributeTok{Sigma =}\NormalTok{ Sigma)  }\CommentTok{\# Generate correlated errors}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{        errors }\OtherTok{\textless{}{-}} \FunctionTok{mvrnorm}\NormalTok{(n, }\AttributeTok{mu =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{), }\AttributeTok{Sigma =} \FunctionTok{diag}\NormalTok{(}\DecValTok{2}\NormalTok{))  }\CommentTok{\# Independent errors}
\NormalTok{    \}}
    
\NormalTok{    M1 }\OtherTok{\textless{}{-}}\NormalTok{ a1 }\SpecialCharTok{*}\NormalTok{ X }\SpecialCharTok{+}\NormalTok{ errors[, }\DecValTok{1}\NormalTok{]}
\NormalTok{    M2 }\OtherTok{\textless{}{-}}\NormalTok{ a2 }\SpecialCharTok{*}\NormalTok{ X }\SpecialCharTok{+}\NormalTok{ errors[, }\DecValTok{2}\NormalTok{]}
\NormalTok{    Y }\OtherTok{\textless{}{-}}\NormalTok{ b1 }\SpecialCharTok{*}\NormalTok{ M1 }\SpecialCharTok{+}\NormalTok{ b2 }\SpecialCharTok{*}\NormalTok{ M2 }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)  }\CommentTok{\# Y depends on M1 and M2}
    
    \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{X =}\NormalTok{ X, }\AttributeTok{M1 =}\NormalTok{ M1, }\AttributeTok{M2 =}\NormalTok{ M2, }\AttributeTok{Y =}\NormalTok{ Y)}
\NormalTok{\}}

\CommentTok{\# Ground truth for comparison}
\NormalTok{ground\_truth }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{Parameter =} \FunctionTok{c}\NormalTok{(}\StringTok{"b1"}\NormalTok{, }\StringTok{"b2"}\NormalTok{), }\AttributeTok{GroundTruth =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.7}\NormalTok{, }\FloatTok{0.48}\NormalTok{))}

\CommentTok{\# Function to extract relevant estimates, standard errors, and model fit}
\NormalTok{extract\_estimates\_b1\_b2 }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(fit) \{}
\NormalTok{    estimates }\OtherTok{\textless{}{-}} \FunctionTok{parameterEstimates}\NormalTok{(fit)}
\NormalTok{    estimates }\OtherTok{\textless{}{-}}\NormalTok{ estimates[estimates}\SpecialCharTok{$}\NormalTok{lhs }\SpecialCharTok{==} \StringTok{"Y"} \SpecialCharTok{\&}\NormalTok{ estimates}\SpecialCharTok{$}\NormalTok{rhs }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"M1"}\NormalTok{, }\StringTok{"M2"}\NormalTok{), }\FunctionTok{c}\NormalTok{(}\StringTok{"rhs"}\NormalTok{, }\StringTok{"est"}\NormalTok{, }\StringTok{"se"}\NormalTok{)]}
\NormalTok{    estimates}\SpecialCharTok{$}\NormalTok{Parameter }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(estimates}\SpecialCharTok{$}\NormalTok{rhs }\SpecialCharTok{==} \StringTok{"M1"}\NormalTok{, }\StringTok{"b1"}\NormalTok{, }\StringTok{"b2"}\NormalTok{)}
\NormalTok{    estimates }\OtherTok{\textless{}{-}}\NormalTok{ estimates[, }\FunctionTok{c}\NormalTok{(}\StringTok{"Parameter"}\NormalTok{, }\StringTok{"est"}\NormalTok{, }\StringTok{"se"}\NormalTok{)]}
\NormalTok{    fit\_stats }\OtherTok{\textless{}{-}} \FunctionTok{fitMeasures}\NormalTok{(fit, }\FunctionTok{c}\NormalTok{(}\StringTok{"aic"}\NormalTok{, }\StringTok{"bic"}\NormalTok{, }\StringTok{"rmsea"}\NormalTok{, }\StringTok{"chisq"}\NormalTok{))}
    \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{estimates =}\NormalTok{ estimates, }\AttributeTok{fit\_stats =}\NormalTok{ fit\_stats))}
\NormalTok{\}}

\CommentTok{\# Case 1: Correlated errors for mediators (modeled correctly)}
\NormalTok{Data\_corr }\OtherTok{\textless{}{-}} \FunctionTok{generate\_data}\NormalTok{(}\AttributeTok{n =} \DecValTok{10000}\NormalTok{, }\AttributeTok{corr =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{correlation\_value =} \FloatTok{0.7}\NormalTok{)}
\NormalTok{model\_corr }\OtherTok{\textless{}{-}} \StringTok{\textquotesingle{}}
\StringTok{  Y \textasciitilde{} b1 * M1 + b2 * M2 + c * X}
\StringTok{  M1 \textasciitilde{} a1 * X}
\StringTok{  M2 \textasciitilde{} a2 * X}
\StringTok{  M1 \textasciitilde{}\textasciitilde{} M2  \# Correlated mediators (errors)}
\StringTok{\textquotesingle{}}
\NormalTok{fit\_corr }\OtherTok{\textless{}{-}} \FunctionTok{sem}\NormalTok{(}\AttributeTok{model =}\NormalTok{ model\_corr, }\AttributeTok{data =}\NormalTok{ Data\_corr)}
\NormalTok{results\_corr }\OtherTok{\textless{}{-}} \FunctionTok{extract\_estimates\_b1\_b2}\NormalTok{(fit\_corr)}

\CommentTok{\# Case 2: Uncorrelated errors for mediators (modeled correctly)}
\NormalTok{Data\_uncorr }\OtherTok{\textless{}{-}} \FunctionTok{generate\_data}\NormalTok{(}\AttributeTok{n =} \DecValTok{10000}\NormalTok{, }\AttributeTok{corr =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{model\_uncorr }\OtherTok{\textless{}{-}} \StringTok{\textquotesingle{}}
\StringTok{  Y \textasciitilde{} b1 * M1 + b2 * M2 + c * X}
\StringTok{  M1 \textasciitilde{} a1 * X}
\StringTok{  M2 \textasciitilde{} a2 * X}
\StringTok{\textquotesingle{}}
\NormalTok{fit\_uncorr }\OtherTok{\textless{}{-}} \FunctionTok{sem}\NormalTok{(}\AttributeTok{model =}\NormalTok{ model\_uncorr, }\AttributeTok{data =}\NormalTok{ Data\_uncorr)}
\NormalTok{results\_uncorr }\OtherTok{\textless{}{-}} \FunctionTok{extract\_estimates\_b1\_b2}\NormalTok{(fit\_uncorr)}

\CommentTok{\# Case 3: Correlated errors, but not modeled as correlated}
\NormalTok{fit\_corr\_incorrect }\OtherTok{\textless{}{-}} \FunctionTok{sem}\NormalTok{(}\AttributeTok{model =}\NormalTok{ model\_uncorr, }\AttributeTok{data =}\NormalTok{ Data\_corr)}
\NormalTok{results\_corr\_incorrect }\OtherTok{\textless{}{-}} \FunctionTok{extract\_estimates\_b1\_b2}\NormalTok{(fit\_corr\_incorrect)}

\CommentTok{\# Case 4: Uncorrelated errors, but modeled as correlated}
\NormalTok{fit\_uncorr\_incorrect }\OtherTok{\textless{}{-}} \FunctionTok{sem}\NormalTok{(}\AttributeTok{model =}\NormalTok{ model\_corr, }\AttributeTok{data =}\NormalTok{ Data\_uncorr)}
\NormalTok{results\_uncorr\_incorrect }\OtherTok{\textless{}{-}} \FunctionTok{extract\_estimates\_b1\_b2}\NormalTok{(fit\_uncorr\_incorrect)}

\CommentTok{\# Combine all estimates for comparison}
\NormalTok{estimates\_combined }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
    \StringTok{"Correlated (Correct)"} \OtherTok{=}\NormalTok{ results\_corr}\SpecialCharTok{$}\NormalTok{estimates,}
    \StringTok{"Uncorrelated (Correct)"} \OtherTok{=}\NormalTok{ results\_uncorr}\SpecialCharTok{$}\NormalTok{estimates,}
    \StringTok{"Correlated (Incorrect)"} \OtherTok{=}\NormalTok{ results\_corr\_incorrect}\SpecialCharTok{$}\NormalTok{estimates,}
    \StringTok{"Uncorrelated (Incorrect)"} \OtherTok{=}\NormalTok{ results\_uncorr\_incorrect}\SpecialCharTok{$}\NormalTok{estimates}
\NormalTok{)}

\CommentTok{\# Combine all into a single table}
\NormalTok{comparison\_table }\OtherTok{\textless{}{-}} \FunctionTok{do.call}\NormalTok{(rbind, }\FunctionTok{lapply}\NormalTok{(}\FunctionTok{names}\NormalTok{(estimates\_combined), }\ControlFlowTok{function}\NormalTok{(case) \{}
\NormalTok{    df }\OtherTok{\textless{}{-}}\NormalTok{ estimates\_combined[[case]]}
\NormalTok{    df}\SpecialCharTok{$}\NormalTok{Case }\OtherTok{\textless{}{-}}\NormalTok{ case}
\NormalTok{    df}
\NormalTok{\}))}

\CommentTok{\# Merge with ground truth for final comparison}
\NormalTok{comparison\_table }\OtherTok{\textless{}{-}} \FunctionTok{merge}\NormalTok{(comparison\_table, ground\_truth, }\AttributeTok{by =} \StringTok{"Parameter"}\NormalTok{)}

\CommentTok{\# Display the comparison table}
\NormalTok{comparison\_table}
\CommentTok{\#\textgreater{}   Parameter       est          se                     Case GroundTruth}
\CommentTok{\#\textgreater{} 1        b1 0.7002984 0.013870433     Correlated (Correct)        0.70}
\CommentTok{\#\textgreater{} 2        b1 0.6973612 0.009859426   Uncorrelated (Correct)        0.70}
\CommentTok{\#\textgreater{} 3        b1 0.7002984 0.010010367   Correlated (Incorrect)        0.70}
\CommentTok{\#\textgreater{} 4        b1 0.6973612 0.009859634 Uncorrelated (Incorrect)        0.70}
\CommentTok{\#\textgreater{} 5        b2 0.4871118 0.013805615     Correlated (Correct)        0.48}
\CommentTok{\#\textgreater{} 6        b2 0.4868318 0.010009908   Uncorrelated (Correct)        0.48}
\CommentTok{\#\textgreater{} 7        b2 0.4871118 0.009963588   Correlated (Incorrect)        0.48}
\CommentTok{\#\textgreater{} 8        b2 0.4868318 0.010010119 Uncorrelated (Incorrect)        0.48}

\CommentTok{\# Display model fit statistics for each case}
\NormalTok{fit\_stats\_combined }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
    \StringTok{"Correlated (Correct)"} \OtherTok{=}\NormalTok{ results\_corr}\SpecialCharTok{$}\NormalTok{fit\_stats,}
    \StringTok{"Uncorrelated (Correct)"} \OtherTok{=}\NormalTok{ results\_uncorr}\SpecialCharTok{$}\NormalTok{fit\_stats,}
    \StringTok{"Correlated (Incorrect)"} \OtherTok{=}\NormalTok{ results\_corr\_incorrect}\SpecialCharTok{$}\NormalTok{fit\_stats,}
    \StringTok{"Uncorrelated (Incorrect)"} \OtherTok{=}\NormalTok{ results\_uncorr\_incorrect}\SpecialCharTok{$}\NormalTok{fit\_stats}
\NormalTok{)}

\NormalTok{fit\_stats\_combined}
\CommentTok{\#\textgreater{} $\textasciigrave{}Correlated (Correct)\textasciigrave{}}
\CommentTok{\#\textgreater{}      aic      bic    rmsea    chisq }
\CommentTok{\#\textgreater{} 77932.45 77997.34     0.00     0.00 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $\textasciigrave{}Uncorrelated (Correct)\textasciigrave{}}
\CommentTok{\#\textgreater{}       aic       bic     rmsea     chisq }
\CommentTok{\#\textgreater{} 84664.312 84721.995     0.000     0.421 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $\textasciigrave{}Correlated (Incorrect)\textasciigrave{}}
\CommentTok{\#\textgreater{}       aic       bic     rmsea     chisq }
\CommentTok{\#\textgreater{} 84453.208 84510.891     0.808  6522.762 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $\textasciigrave{}Uncorrelated (Incorrect)\textasciigrave{}}
\CommentTok{\#\textgreater{}      aic      bic    rmsea    chisq }
\CommentTok{\#\textgreater{} 84665.89 84730.78     0.00     0.00}
\end{Highlighting}
\end{Shaded}

\hypertarget{multiple-treatments-1}{%
\subsubsection{Multiple Treatments}\label{multiple-treatments-1}}

\citep{hayes2014statistical}

Code in \href{https://core.ecu.edu/wuenschk/MV/multReg/Mediation_Multicategorical.pdf}{Process}

\hypertarget{causal-inference-approach}{%
\section{Causal Inference Approach}\label{causal-inference-approach}}

\hypertarget{example-1-mediation-traditional}{%
\subsection{Example 1}\label{example-1-mediation-traditional}}

from \href{https://data.library.virginia.edu/introduction-to-mediation-analysis/}{Virginia's library}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{myData }\OtherTok{\textless{}{-}}
    \FunctionTok{read.csv}\NormalTok{(}\StringTok{\textquotesingle{}http://static.lib.virginia.edu/statlab/materials/data/mediationData.csv\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Step 1 (no longer necessary)}
\NormalTok{model}\FloatTok{.0} \OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X, myData)}
\FunctionTok{summary}\NormalTok{(model}\FloatTok{.0}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = Y \textasciitilde{} X, data = myData)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}     Min      1Q  Median      3Q     Max }
\CommentTok{\#\textgreater{} {-}5.0262 {-}1.2340 {-}0.3282  1.5583  5.1622 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)   2.8572     0.6932   4.122 7.88e{-}05 ***}
\CommentTok{\#\textgreater{} X             0.3961     0.1112   3.564 0.000567 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 1.929 on 98 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.1147, Adjusted R{-}squared:  0.1057 }
\CommentTok{\#\textgreater{} F{-}statistic:  12.7 on 1 and 98 DF,  p{-}value: 0.0005671}

\CommentTok{\# Step 2}
\NormalTok{model.M }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(M }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X, myData)}
\FunctionTok{summary}\NormalTok{(model.M)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = M \textasciitilde{} X, data = myData)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}     Min      1Q  Median      3Q     Max }
\CommentTok{\#\textgreater{} {-}4.3046 {-}0.8656  0.1344  1.1344  4.6954 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)  1.49952    0.58920   2.545   0.0125 *  }
\CommentTok{\#\textgreater{} X            0.56102    0.09448   5.938 4.39e{-}08 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 1.639 on 98 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.2646, Adjusted R{-}squared:  0.2571 }
\CommentTok{\#\textgreater{} F{-}statistic: 35.26 on 1 and 98 DF,  p{-}value: 4.391e{-}08}

\CommentTok{\# Step 3}
\NormalTok{model.Y }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X }\SpecialCharTok{+}\NormalTok{ M, myData)}
\FunctionTok{summary}\NormalTok{(model.Y)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = Y \textasciitilde{} X + M, data = myData)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}     Min      1Q  Median      3Q     Max }
\CommentTok{\#\textgreater{} {-}3.7631 {-}1.2393  0.0308  1.0832  4.0055 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)   1.9043     0.6055   3.145   0.0022 ** }
\CommentTok{\#\textgreater{} X             0.0396     0.1096   0.361   0.7187    }
\CommentTok{\#\textgreater{} M             0.6355     0.1005   6.321 7.92e{-}09 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 1.631 on 97 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.373,  Adjusted R{-}squared:  0.3601 }
\CommentTok{\#\textgreater{} F{-}statistic: 28.85 on 2 and 97 DF,  p{-}value: 1.471e{-}10}

\CommentTok{\# Step 4 (boostrapping)}
\FunctionTok{library}\NormalTok{(mediation)}
\NormalTok{results }\OtherTok{\textless{}{-}} \FunctionTok{mediate}\NormalTok{(}
\NormalTok{    model.M,}
\NormalTok{    model.Y,}
    \AttributeTok{treat =} \StringTok{\textquotesingle{}X\textquotesingle{}}\NormalTok{,}
    \AttributeTok{mediator =} \StringTok{\textquotesingle{}M\textquotesingle{}}\NormalTok{,}
    \AttributeTok{boot =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{sims =} \DecValTok{500}
\NormalTok{)}
\FunctionTok{summary}\NormalTok{(results)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Causal Mediation Analysis }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Nonparametric Bootstrap Confidence Intervals with the Percentile Method}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}                Estimate 95\% CI Lower 95\% CI Upper p{-}value    }
\CommentTok{\#\textgreater{} ACME             0.3565       0.2119         0.51  \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} ADE              0.0396      {-}0.1750         0.28   0.760    }
\CommentTok{\#\textgreater{} Total Effect     0.3961       0.1743         0.64   0.004 ** }
\CommentTok{\#\textgreater{} Prop. Mediated   0.9000       0.5042         1.94   0.004 ** }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Sample Size Used: 100 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Simulations: 500}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  Total Effect = 0.3961 = \(b_1\) (step 1) = total effect of \(X\) on \(Y\) without \(M\)
\item
  Direct Effect = ADE = 0.0396 = \(b_4\) (step 3) = direct effect of \(X\) on \(Y\) accounting for the indirect effect of \(M\)
\item
  ACME = Average Causal Mediation Effects = \(b_1 - b_4\) = 0.3961 - 0.0396 = 0.3565 = \(b_2 \times b_3\) = 0.56102 * 0.6355 = 0.3565
\end{itemize}

Using \texttt{mediation} package suggested by \citep[\citet{imai2010identification}]{imai2010general}. More details of the package can be found \href{https://cran.r-project.org/web/packages/mediation/vignettes/mediation.pdf}{here}

2 types of Inference in this package:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Model-based inference:

  \begin{itemize}
  \item
    Assumptions:

    \begin{itemize}
    \item
      Treatment is randomized (could use matching methods to achieve this).
    \item
      Sequential Ignorability: conditional on covariates, there is other confounders that affect the relationship between (1) treatment-mediator, (2) treatment-outcome, (3) mediator-outcome. Typically hard to argue in observational data. This assumption is for the identification of ACME (i.e., average causal mediation effects).
    \end{itemize}
  \end{itemize}
\item
  Design-based inference
\end{enumerate}

Notations: we stay consistent with package instruction

\begin{itemize}
\item
  \(M_i(t)\) = mediator
\item
  \(T_i\) = treatment status \((0,1)\)
\item
  \(Y_i(t,m)\) = outcome where \(t\) = treatment, and \(m\) = mediating variables.
\item
  \(X_i\) = vector of observed pre-treatment confounders
\item
  Treatment effect (per unit \(i\)) = \(\tau_i = Y_i(1,M_i(1)) - Y_i (0,M_i(0))\) which has 2 effects

  \begin{itemize}
  \item
    Causal mediation effects: \(\delta_i (t) \equiv Y_i (t,M_i(1)) - Y_i(t,M_i(0))\)
  \item
    Direct effects: \(\zeta (t) \equiv Y_i (1, M_i(1)) - Y_i(0, M_i(0))\)
  \item
    summing up to the treatment effect: \(\tau_i = \delta_i (t) + \zeta_i (1-t)\)
  \end{itemize}
\end{itemize}

More on sequential ignorability

\[
\{ Y_i (t', m) , M_i (t) \} \perp T_i |X_i = x 
\]

\[
Y_i(t',m) \perp M_i(t) | T_i = t, X_i = x
\]

where

\begin{itemize}
\item
  \(0 < P(T_i = t | X_i = x)\)
\item
  \(0 < P(M_i = m | T_i = t , X_i =x)\)
\end{itemize}

First condition is the standard strong ignorability condition where treatment assignment is random conditional on pre-treatment confounders.

Second condition is stronger where the mediators is also random given the observed treatment and pre-treatment confounders. This condition is satisfied only when there is no unobserved pre-treatment confounders, and post-treatment confounders, and multiple mediators that are correlated.

My understanding is that until the moment I write this note, there is \textbf{no way to test the sequential ignorability assumption}. Hence, researchers can only do sensitivity analysis to argue for their result.

\hypertarget{model-based-causal-mediation-analysis}{%
\section{Model-based causal mediation analysis}\label{model-based-causal-mediation-analysis}}

Other resources:

\begin{itemize}
\tightlist
\item
  \href{https://cran.ism.ac.jp/web/packages/mediation/vignettes/mediation-old.pdf}{here}
\end{itemize}

Fit 2 models

\begin{itemize}
\item
  mediator model: conditional distribution of the mediators \(M_i | T_i, X_i\)
\item
  Outcome model: conditional distribution of \(Y_i | T_i, M_i, X_i\)
\end{itemize}

\texttt{mediation} can accommodate almost all types of model for both mediator model and outcome model except Censored mediator model.

The update here is that estimation of ACME does not rely on product or difference of coefficients (see \ref{example-1-mediation-traditional} ,

which requires very strict assumption: (1) linear regression models of mediator and outcome, (2) \(T_i\) and \(M_i\) effects are additive and no interaction

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(mediation)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{2014}\NormalTok{)}
\FunctionTok{data}\NormalTok{(}\StringTok{"framing"}\NormalTok{, }\AttributeTok{package =} \StringTok{"mediation"}\NormalTok{)}

\NormalTok{med.fit }\OtherTok{\textless{}{-}}
    \FunctionTok{lm}\NormalTok{(emo }\SpecialCharTok{\textasciitilde{}}\NormalTok{ treat }\SpecialCharTok{+}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ educ }\SpecialCharTok{+}\NormalTok{ gender }\SpecialCharTok{+}\NormalTok{ income, }\AttributeTok{data =}\NormalTok{ framing)}
\NormalTok{out.fit }\OtherTok{\textless{}{-}}
    \FunctionTok{glm}\NormalTok{(}
\NormalTok{        cong\_mesg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ emo }\SpecialCharTok{+}\NormalTok{ treat }\SpecialCharTok{+}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ educ }\SpecialCharTok{+}\NormalTok{ gender }\SpecialCharTok{+}\NormalTok{ income,}
        \AttributeTok{data =}\NormalTok{ framing,}
        \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\StringTok{"probit"}\NormalTok{)}
\NormalTok{    )}

\CommentTok{\# Quasi{-}Bayesian Monte Carlo }
\NormalTok{med.out }\OtherTok{\textless{}{-}}
    \FunctionTok{mediate}\NormalTok{(}
\NormalTok{        med.fit,}
\NormalTok{        out.fit,}
        \AttributeTok{treat =} \StringTok{"treat"}\NormalTok{,}
        \AttributeTok{mediator =} \StringTok{"emo"}\NormalTok{,}
        \AttributeTok{robustSE =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{sims =} \DecValTok{100} \CommentTok{\# should be 10000 in practice}
\NormalTok{    )}
\FunctionTok{summary}\NormalTok{(med.out)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Causal Mediation Analysis }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Quasi{-}Bayesian Confidence Intervals}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}                          Estimate 95\% CI Lower 95\% CI Upper p{-}value    }
\CommentTok{\#\textgreater{} ACME (control)             0.0791       0.0351         0.15  \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} ACME (treated)             0.0804       0.0367         0.16  \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} ADE (control)              0.0206      {-}0.0976         0.12    0.70    }
\CommentTok{\#\textgreater{} ADE (treated)              0.0218      {-}0.1053         0.12    0.70    }
\CommentTok{\#\textgreater{} Total Effect               0.1009      {-}0.0497         0.23    0.14    }
\CommentTok{\#\textgreater{} Prop. Mediated (control)   0.6946      {-}6.3109         3.68    0.14    }
\CommentTok{\#\textgreater{} Prop. Mediated (treated)   0.7118      {-}5.7936         3.50    0.14    }
\CommentTok{\#\textgreater{} ACME (average)             0.0798       0.0359         0.15  \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} ADE (average)              0.0212      {-}0.1014         0.12    0.70    }
\CommentTok{\#\textgreater{} Prop. Mediated (average)   0.7032      {-}6.0523         3.59    0.14    }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Sample Size Used: 265 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Simulations: 100}
\end{Highlighting}
\end{Shaded}

Nonparametric bootstrap version

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{med.out }\OtherTok{\textless{}{-}}
    \FunctionTok{mediate}\NormalTok{(}
\NormalTok{        med.fit,}
\NormalTok{        out.fit,}
        \AttributeTok{boot =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{treat =} \StringTok{"treat"}\NormalTok{,}
        \AttributeTok{mediator =} \StringTok{"emo"}\NormalTok{,}
        \AttributeTok{sims =} \DecValTok{100}\NormalTok{, }\CommentTok{\# should be 10000 in practice}
        \AttributeTok{boot.ci.type =} \StringTok{"bca"} \CommentTok{\# bias{-}corrected and accelerated intervals}
\NormalTok{    )}
\FunctionTok{summary}\NormalTok{(med.out)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Causal Mediation Analysis }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Nonparametric Bootstrap Confidence Intervals with the BCa Method}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}                          Estimate 95\% CI Lower 95\% CI Upper p{-}value    }
\CommentTok{\#\textgreater{} ACME (control)             0.0848       0.0424         0.14  \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} ACME (treated)             0.0858       0.0410         0.14  \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} ADE (control)              0.0117      {-}0.0726         0.13    0.58    }
\CommentTok{\#\textgreater{} ADE (treated)              0.0127      {-}0.0784         0.14    0.58    }
\CommentTok{\#\textgreater{} Total Effect               0.0975       0.0122         0.25    0.06 .  }
\CommentTok{\#\textgreater{} Prop. Mediated (control)   0.8698       1.7460       151.20    0.06 .  }
\CommentTok{\#\textgreater{} Prop. Mediated (treated)   0.8804       1.6879       138.91    0.06 .  }
\CommentTok{\#\textgreater{} ACME (average)             0.0853       0.0434         0.14  \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} ADE (average)              0.0122      {-}0.0756         0.14    0.58    }
\CommentTok{\#\textgreater{} Prop. Mediated (average)   0.8751       1.7170       145.05    0.06 .  }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Sample Size Used: 265 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Simulations: 100}
\end{Highlighting}
\end{Shaded}

If theoretically understanding suggests that there is treatment and mediator interaction

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{med.fit }\OtherTok{\textless{}{-}}
    \FunctionTok{lm}\NormalTok{(emo }\SpecialCharTok{\textasciitilde{}}\NormalTok{ treat }\SpecialCharTok{+}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ educ }\SpecialCharTok{+}\NormalTok{ gender }\SpecialCharTok{+}\NormalTok{ income, }\AttributeTok{data =}\NormalTok{ framing)}
\NormalTok{out.fit }\OtherTok{\textless{}{-}}
    \FunctionTok{glm}\NormalTok{(}
\NormalTok{        cong\_mesg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ emo }\SpecialCharTok{*}\NormalTok{ treat }\SpecialCharTok{+}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ educ }\SpecialCharTok{+}\NormalTok{ gender }\SpecialCharTok{+}\NormalTok{ income,}
        \AttributeTok{data =}\NormalTok{ framing,}
        \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\StringTok{"probit"}\NormalTok{)}
\NormalTok{    )}
\NormalTok{med.out }\OtherTok{\textless{}{-}}
    \FunctionTok{mediate}\NormalTok{(}
\NormalTok{        med.fit,}
\NormalTok{        out.fit,}
        \AttributeTok{treat =} \StringTok{"treat"}\NormalTok{,}
        \AttributeTok{mediator =} \StringTok{"emo"}\NormalTok{,}
        \AttributeTok{robustSE =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{sims =} \DecValTok{100}
\NormalTok{    )}
\FunctionTok{summary}\NormalTok{(med.out)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Causal Mediation Analysis }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Quasi{-}Bayesian Confidence Intervals}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}                           Estimate 95\% CI Lower 95\% CI Upper p{-}value    }
\CommentTok{\#\textgreater{} ACME (control)             0.07417      0.02401         0.14  \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} ACME (treated)             0.09496      0.02702         0.16  \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} ADE (control)             {-}0.01353     {-}0.11855         0.11    0.76    }
\CommentTok{\#\textgreater{} ADE (treated)              0.00726     {-}0.11007         0.11    0.90    }
\CommentTok{\#\textgreater{} Total Effect               0.08143     {-}0.05646         0.19    0.26    }
\CommentTok{\#\textgreater{} Prop. Mediated (control)   0.64510    {-}14.31243         3.13    0.26    }
\CommentTok{\#\textgreater{} Prop. Mediated (treated)   0.98006    {-}17.83202         4.01    0.26    }
\CommentTok{\#\textgreater{} ACME (average)             0.08457      0.02738         0.15  \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} ADE (average)             {-}0.00314     {-}0.11457         0.12    1.00    }
\CommentTok{\#\textgreater{} Prop. Mediated (average)   0.81258    {-}16.07223         3.55    0.26    }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Sample Size Used: 265 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Simulations: 100}

\FunctionTok{test.TMint}\NormalTok{(med.out, }\AttributeTok{conf.level =}\NormalTok{ .}\DecValTok{95}\NormalTok{) }\CommentTok{\# test treatment{-}mediator interaction effect }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Test of ACME(1) {-} ACME(0) = 0}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  estimates from med.out}
\CommentTok{\#\textgreater{} ACME(1) {-} ACME(0) = 0.020796, p{-}value = 0.3}
\CommentTok{\#\textgreater{} alternative hypothesis: true ACME(1) {-} ACME(0) is not equal to 0}
\CommentTok{\#\textgreater{} 95 percent confidence interval:}
\CommentTok{\#\textgreater{}  {-}0.01757310  0.07110837}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(med.out)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{36-mediation_files/figure-latex/unnamed-chunk-14-1} \end{center}

\texttt{mediation} can be used in conjunction with any of your imputation packages.

And it can also handle \textbf{mediated moderation} or \textbf{non-binary treatment variables}, or \textbf{multi-level data}

Sensitivity Analysis for sequential ignorability

\begin{itemize}
\item
  test for unobserved pre-treatment covariates
\item
  \(\rho\) = correlation between the residuals of the mediator and outcome regressions.
\item
  If \(\rho\) is significant, we have evidence for violation of sequential ignorability (i.e., there is unobserved pre-treatment confounders).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{med.fit }\OtherTok{\textless{}{-}}
    \FunctionTok{lm}\NormalTok{(emo }\SpecialCharTok{\textasciitilde{}}\NormalTok{ treat }\SpecialCharTok{+}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ educ }\SpecialCharTok{+}\NormalTok{ gender }\SpecialCharTok{+}\NormalTok{ income, }\AttributeTok{data =}\NormalTok{ framing)}
\NormalTok{out.fit }\OtherTok{\textless{}{-}}
    \FunctionTok{glm}\NormalTok{(}
\NormalTok{        cong\_mesg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ emo }\SpecialCharTok{+}\NormalTok{ treat }\SpecialCharTok{+}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ educ }\SpecialCharTok{+}\NormalTok{ gender }\SpecialCharTok{+}\NormalTok{ income,}
        \AttributeTok{data =}\NormalTok{ framing,}
        \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\StringTok{"probit"}\NormalTok{)}
\NormalTok{    )}
\NormalTok{med.out }\OtherTok{\textless{}{-}}
    \FunctionTok{mediate}\NormalTok{(}
\NormalTok{        med.fit,}
\NormalTok{        out.fit,}
        \AttributeTok{treat =} \StringTok{"treat"}\NormalTok{,}
        \AttributeTok{mediator =} \StringTok{"emo"}\NormalTok{,}
        \AttributeTok{robustSE =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{sims =} \DecValTok{100}
\NormalTok{    )}
\NormalTok{sens.out }\OtherTok{\textless{}{-}}
    \FunctionTok{medsens}\NormalTok{(med.out,}
            \AttributeTok{rho.by =} \FloatTok{0.1}\NormalTok{, }\CommentTok{\# \textbackslash{}rho varies from {-}0.9 to 0.9 by 0.1}
            \AttributeTok{effect.type =} \StringTok{"indirect"}\NormalTok{, }\CommentTok{\# sensitivity on ACME}
            \CommentTok{\# effect.type = "direct", \# sensitivity on ADE}
            \CommentTok{\# effect.type = "both", \# sensitivity on ACME and ADE}
            \AttributeTok{sims =} \DecValTok{100}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(sens.out)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Mediation Sensitivity Analysis: Average Mediation Effect}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Sensitivity Region: ACME for Control Group}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}      Rho ACME(control) 95\% CI Lower 95\% CI Upper R\^{}2\_M*R\^{}2\_Y* R\^{}2\_M\textasciitilde{}R\^{}2\_Y\textasciitilde{}}
\CommentTok{\#\textgreater{} [1,] 0.3        0.0062      {-}0.0073       0.0188         0.09       0.0493}
\CommentTok{\#\textgreater{} [2,] 0.4       {-}0.0084      {-}0.0238       0.0017         0.16       0.0877}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Rho at which ACME for Control Group = 0: 0.3}
\CommentTok{\#\textgreater{} R\^{}2\_M*R\^{}2\_Y* at which ACME for Control Group = 0: 0.09}
\CommentTok{\#\textgreater{} R\^{}2\_M\textasciitilde{}R\^{}2\_Y\textasciitilde{} at which ACME for Control Group = 0: 0.0493 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Sensitivity Region: ACME for Treatment Group}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}      Rho ACME(treated) 95\% CI Lower 95\% CI Upper R\^{}2\_M*R\^{}2\_Y* R\^{}2\_M\textasciitilde{}R\^{}2\_Y\textasciitilde{}}
\CommentTok{\#\textgreater{} [1,] 0.3        0.0071      {-}0.0092       0.0213         0.09       0.0493}
\CommentTok{\#\textgreater{} [2,] 0.4       {-}0.0101      {-}0.0295       0.0023         0.16       0.0877}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Rho at which ACME for Treatment Group = 0: 0.3}
\CommentTok{\#\textgreater{} R\^{}2\_M*R\^{}2\_Y* at which ACME for Treatment Group = 0: 0.09}
\CommentTok{\#\textgreater{} R\^{}2\_M\textasciitilde{}R\^{}2\_Y\textasciitilde{} at which ACME for Treatment Group = 0: 0.0493}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(sens.out, }\AttributeTok{sens.par =} \StringTok{"rho"}\NormalTok{, }\AttributeTok{main =} \StringTok{"Anxiety"}\NormalTok{, }\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.2}\NormalTok{, }\FloatTok{0.2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

ACME confidence intervals contains 0 when \(\rho \in (0.3,0.4)\)

Alternatively, using \(R^2\) interpretation, we need to specify the direction of confounder that affects the mediator and outcome variables in \texttt{plot} using \texttt{sign.prod\ =\ "positive"} (i.e., same direction) or \texttt{sign.prod\ =\ "negative"} (i.e., opposite direction).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(sens.out, }\AttributeTok{sens.par =} \StringTok{"R2"}\NormalTok{, }\AttributeTok{r.type =} \StringTok{"total"}\NormalTok{, }\AttributeTok{sign.prod =} \StringTok{"positive"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{directed-acyclic-graph}{%
\chapter{Directed Acyclic Graph}\label{directed-acyclic-graph}}

Native R:

\begin{itemize}
\item
  \texttt{dagitty}
\item
  \texttt{ggdag}
\item
  \texttt{dagR}
\item
  \texttt{r-causal}: by \href{https://www.ccd.pitt.edu/data-science/}{Center for Causal Discovery}. Also available in Python
\end{itemize}

Publication-ready (with \texttt{R} and \texttt{Latex}): \href{https://www.gerkelab.com/project/shinydag/}{shinyDAG}

Standalone program: \href{https://hsz.dife.de/dag/}{DAG program} by Sven Knuppel

\hypertarget{basic-notations}{%
\section{Basic Notations}\label{basic-notations}}

Basic building blocks of DAG

\begin{itemize}
\item
  Mediators (chains): \(X \to Z \to Y\)

  \begin{itemize}
  \tightlist
  \item
    controlling for Z blocks (closes) the causal impact of \(X \to Y\)
  \end{itemize}
\item
  Common causes (forks): \(X \leftarrow Z \to Y\)

  \begin{itemize}
  \item
    Z (i.e., confounder) is a common cause in which it induces a non-causal association between \(X\) and \(Y\).
  \item
    Controlling for \(Z\) should close this association.
  \item
    \(Z\) d-separates \(X\) from \(Y\) when it blocks (closes) all paths from \(X\) to \(Y\) (i.e., \(X \perp Y |Z\)). This applies to both common causes and mediators.
  \end{itemize}
\item
  Common effects (colliders): \(X \to Z \leftarrow Y\)

  \begin{itemize}
  \item
    Not controlling for \(Z\) does not induce an association between \(X\) and \(Y\)
  \item
    Controlling for \(Z\) induces a non-causal association between \(X\) and \(Y\)
  \end{itemize}
\end{itemize}

Notes:

\begin{itemize}
\item
  A descendant of a variable behavior similarly to that variable (e.g., a descendant of \(Z\) can behave like \(Z\) and partially control for \(Z\))
\item
  Rule of thumb for multiple \protect\hyperlink{controls}{Controls}: o have \protect\hyperlink{causal-inference}{Causal inference} \(X \to Y\), we must

  \begin{itemize}
  \item
    Close all backdoor path between \(X\) and \(Y\) (to eliminate spurious correlation)
  \item
    Do not close any causal path between \(X\) and \(Y\) (any mediators).
  \end{itemize}
\end{itemize}

\hypertarget{part-v.-miscellaneous}{%
\part*{V. MISCELLANEOUS}\label{part-v.-miscellaneous}}
\addcontentsline{toc}{part}{V. MISCELLANEOUS}

\hypertarget{report}{%
\chapter{Report}\label{report}}

Structure

\begin{itemize}
\item
  Exploratory analysis

  \begin{itemize}
  \tightlist
  \item
    plots
  \item
    preliminary results
  \item
    interesting structure/features in the data
  \item
    outliers
  \end{itemize}
\item
  Model

  \begin{itemize}
  \tightlist
  \item
    Assumptions
  \item
    Why this model/ How is this model the best one?
  \item
    Consideration: interactions, collinearity, dependence
  \end{itemize}
\item
  Model Fit

  \begin{itemize}
  \item
    How well does it fit?
  \item
    Are the model assumptions met?

    \begin{itemize}
    \tightlist
    \item
      Residual analysis
    \end{itemize}
  \end{itemize}
\item
  Inference/ Prediction

  \begin{itemize}
  \tightlist
  \item
    Are there different way to support your inference?
  \end{itemize}
\item
  Conclusion

  \begin{itemize}
  \item
    Recommendation
  \item
    Limitation of the analysis
  \item
    How to correct those in the future
  \end{itemize}
\end{itemize}

This chapter is based on the \texttt{jtools} package. More information can be found \href{https://www.rdocumentation.org/packages/jtools/versions/2.1.0}{here.}

\hypertarget{one-summary-table}{%
\section{One summary table}\label{one-summary-table}}

Packages for reporting:

Summary Statistics Table:

\begin{itemize}
\tightlist
\item
  \href{https://cran.r-project.org/web/packages/qwraps2/vignettes/summary-statistics.html}{qwraps2}
\item
  \href{https://cran.r-project.org/web/packages/vtable/vignettes/sumtable.html}{vtable}
\item
  \href{http://www.danieldsjoberg.com/gtsummary/}{gtsummary}
\item
  \href{https://cran.r-project.org/web/packages/apaTables/apaTables.pdf}{apaTables}
\item
  \href{https://cran.r-project.org/web/packages/stargazer/vignettes/stargazer.pdf}{stargazer}
\end{itemize}

Regression Table

\begin{itemize}
\tightlist
\item
  \href{http://www.danieldsjoberg.com/gtsummary/}{gtsummary}
\item
  \href{https://cran.r-project.org/web/packages/sjPlot/vignettes/tab_model_estimates.html}{sjPlot,sjmisc, sjlabelled}
\item
  \href{https://cran.r-project.org/web/packages/stargazer/vignettes/stargazer.pdf}{stargazer}: recommended (\href{https://www.jakeruss.com/cheatsheets/stargazer/}{Example})
\item
  \href{https://github.com/vincentarelbundock/modelsummary\#a-simple-example}{modelsummary}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(jtools)}
\FunctionTok{data}\NormalTok{(movies)}
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(metascore }\SpecialCharTok{\textasciitilde{}}\NormalTok{ budget }\SpecialCharTok{+}\NormalTok{ us\_gross }\SpecialCharTok{+}\NormalTok{ year, }\AttributeTok{data =}\NormalTok{ movies)}
\FunctionTok{summ}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]
\centering
\begin{tabular}{lr}
\toprule
\cellcolor{gray!6}{Observations} & \cellcolor{gray!6}{831 (10 missing obs. deleted)}\\
Dependent variable & metascore\\
\cellcolor{gray!6}{Type} & \cellcolor{gray!6}{OLS linear regression}\\
\bottomrule
\end{tabular}
\end{table} \begin{table}[!h]
\centering
\begin{tabular}{lr}
\toprule
\cellcolor{gray!6}{F(3,827)} & \cellcolor{gray!6}{26.23}\\
R & 0.09\\
\cellcolor{gray!6}{Adj. R} & \cellcolor{gray!6}{0.08}\\
\bottomrule
\end{tabular}
\end{table} \begin{table}[!h]
\centering
\begin{threeparttable}
\begin{tabular}{lrrrr}
\toprule
  & Est. & S.E. & t val. & p\\
\midrule
\cellcolor{gray!6}{(Intercept)} & \cellcolor{gray!6}{52.06} & \cellcolor{gray!6}{139.67} & \cellcolor{gray!6}{0.37} & \cellcolor{gray!6}{0.71}\\
budget & -0.00 & 0.00 & -5.89 & 0.00\\
\cellcolor{gray!6}{us\_gross} & \cellcolor{gray!6}{0.00} & \cellcolor{gray!6}{0.00} & \cellcolor{gray!6}{7.61} & \cellcolor{gray!6}{0.00}\\
year & 0.01 & 0.07 & 0.08 & 0.94\\
\bottomrule
\end{tabular}
\begin{tablenotes}
\item Standard errors: OLS
\end{tablenotes}
\end{threeparttable}
\end{table}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summ}\NormalTok{(}
\NormalTok{    fit,}
    \AttributeTok{scale =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{vifs =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{part.corr =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{confint =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{pvals =} \ConstantTok{FALSE}
\NormalTok{) }\CommentTok{\# notice that scale here is TRUE}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]
\centering
\begin{tabular}{lr}
\toprule
\cellcolor{gray!6}{Observations} & \cellcolor{gray!6}{831 (10 missing obs. deleted)}\\
Dependent variable & metascore\\
\cellcolor{gray!6}{Type} & \cellcolor{gray!6}{OLS linear regression}\\
\bottomrule
\end{tabular}
\end{table} \begin{table}[!h]
\centering
\begin{tabular}{lr}
\toprule
\cellcolor{gray!6}{F(3,827)} & \cellcolor{gray!6}{26.23}\\
R & 0.09\\
\cellcolor{gray!6}{Adj. R} & \cellcolor{gray!6}{0.08}\\
\bottomrule
\end{tabular}
\end{table} \begin{table}[!h]
\centering
\begin{threeparttable}
\begin{tabular}{lrrrrrrr}
\toprule
  & Est. & 2.5\% & 97.5\% & t val. & VIF & partial.r & part.r\\
\midrule
\cellcolor{gray!6}{(Intercept)} & \cellcolor{gray!6}{63.01} & \cellcolor{gray!6}{61.91} & \cellcolor{gray!6}{64.11} & \cellcolor{gray!6}{112.23} & \cellcolor{gray!6}{NA} & \cellcolor{gray!6}{NA} & \cellcolor{gray!6}{NA}\\
budget & -3.78 & -5.05 & -2.52 & -5.89 & 1.31 & -0.20 & -0.20\\
\cellcolor{gray!6}{us\_gross} & \cellcolor{gray!6}{5.28} & \cellcolor{gray!6}{3.92} & \cellcolor{gray!6}{6.64} & \cellcolor{gray!6}{7.61} & \cellcolor{gray!6}{1.52} & \cellcolor{gray!6}{0.26} & \cellcolor{gray!6}{0.25}\\
year & 0.05 & -1.18 & 1.28 & 0.08 & 1.24 & 0.00 & 0.00\\
\bottomrule
\end{tabular}
\begin{tablenotes}
\item Standard errors: OLS; Continuous predictors are mean-centered and scaled by 1 s.d. The outcome variable remains in its original units.
\end{tablenotes}
\end{threeparttable}
\end{table}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\#obtain clsuter{-}robust SE}
\FunctionTok{data}\NormalTok{(}\StringTok{"PetersenCL"}\NormalTok{, }\AttributeTok{package =} \StringTok{"sandwich"}\NormalTok{)}
\NormalTok{fit2 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{data =}\NormalTok{ PetersenCL)}
\FunctionTok{summ}\NormalTok{(fit2, }\AttributeTok{robust =} \StringTok{"HC3"}\NormalTok{, }\AttributeTok{cluster =} \StringTok{"firm"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]
\centering
\begin{tabular}{lr}
\toprule
\cellcolor{gray!6}{Observations} & \cellcolor{gray!6}{5000}\\
Dependent variable & y\\
\cellcolor{gray!6}{Type} & \cellcolor{gray!6}{OLS linear regression}\\
\bottomrule
\end{tabular}
\end{table} \begin{table}[!h]
\centering
\begin{tabular}{lr}
\toprule
\cellcolor{gray!6}{F(1,4998)} & \cellcolor{gray!6}{1310.74}\\
R & 0.21\\
\cellcolor{gray!6}{Adj. R} & \cellcolor{gray!6}{0.21}\\
\bottomrule
\end{tabular}
\end{table} \begin{table}[!h]
\centering
\begin{threeparttable}
\begin{tabular}{lrrrr}
\toprule
  & Est. & S.E. & t val. & p\\
\midrule
\cellcolor{gray!6}{(Intercept)} & \cellcolor{gray!6}{0.03} & \cellcolor{gray!6}{0.07} & \cellcolor{gray!6}{0.44} & \cellcolor{gray!6}{0.66}\\
x & 1.03 & 0.05 & 20.36 & 0.00\\
\bottomrule
\end{tabular}
\begin{tablenotes}
\item Standard errors: Cluster-robust, type = HC3
\end{tablenotes}
\end{threeparttable}
\end{table}

Model to Equation

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# install.packages("equatiomatic") \# not available for R 4.2}
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(metascore }\SpecialCharTok{\textasciitilde{}}\NormalTok{ budget }\SpecialCharTok{+}\NormalTok{ us\_gross }\SpecialCharTok{+}\NormalTok{ year, }\AttributeTok{data =}\NormalTok{ movies)}
\CommentTok{\# show the theoretical model}
\NormalTok{equatiomatic}\SpecialCharTok{::}\FunctionTok{extract\_eq}\NormalTok{(fit)}
\CommentTok{\# display the actual coefficients}
\NormalTok{equatiomatic}\SpecialCharTok{::}\FunctionTok{extract\_eq}\NormalTok{(fit, }\AttributeTok{use\_coefs =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{model-comparison}{%
\section{Model Comparison}\label{model-comparison}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(metascore }\SpecialCharTok{\textasciitilde{}} \FunctionTok{log}\NormalTok{(budget), }\AttributeTok{data =}\NormalTok{ movies)}
\NormalTok{fit\_b }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(metascore }\SpecialCharTok{\textasciitilde{}} \FunctionTok{log}\NormalTok{(budget) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{(us\_gross), }\AttributeTok{data =}\NormalTok{ movies)}
\NormalTok{fit\_c }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(metascore }\SpecialCharTok{\textasciitilde{}} \FunctionTok{log}\NormalTok{(budget) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{(us\_gross) }\SpecialCharTok{+}\NormalTok{ runtime, }\AttributeTok{data =}\NormalTok{ movies)}
\NormalTok{coef\_names }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Budget"} \OtherTok{=} \StringTok{"log(budget)"}\NormalTok{, }\StringTok{"US Gross"} \OtherTok{=} \StringTok{"log(us\_gross)"}\NormalTok{,}
                \StringTok{"Runtime (Hours)"} \OtherTok{=} \StringTok{"runtime"}\NormalTok{, }\StringTok{"Constant"} \OtherTok{=} \StringTok{"(Intercept)"}\NormalTok{)}
\FunctionTok{export\_summs}\NormalTok{(fit, fit\_b, fit\_c, }\AttributeTok{robust =} \StringTok{"HC3"}\NormalTok{, }\AttributeTok{coefs =}\NormalTok{ coef\_names)}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-3} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l l l}


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 1 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 3 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} Budget \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -2.43 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -5.16 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -6.70 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.44)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.62)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.67)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} US Gross \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 3.96 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 3.85 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.51)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.48)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} Runtime (Hours) \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 14.29 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (1.63)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} Constant \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 105.29 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 81.84 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 83.35 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (7.65)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (8.66)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (8.82)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} N \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 831\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 831\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 831\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} R2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.03\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.09\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.17\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{4}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} Standard errors are heteroskedasticity robust.  *** p $<$ 0.001;  ** p $<$ 0.01;  * p $<$ 0.05. \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

Another package is \texttt{modelsummary}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(modelsummary)}
\NormalTok{lm\_mod }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ cyl, mtcars)}
\FunctionTok{msummary}\NormalTok{(lm\_mod, }\AttributeTok{vcov =} \FunctionTok{c}\NormalTok{(}\StringTok{"iid"}\NormalTok{,}\StringTok{"robust"}\NormalTok{,}\StringTok{"HC4"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{table}
\centering
\begin{tabular}[t]{lccc}
\toprule
  & (1) & (2) & (3)\\
\midrule
(Intercept) & \num{38.752} & \num{38.752} & \num{38.752}\\
 & (\num{1.787}) & (\num{2.286}) & (\num{2.177})\\
wt & \num{-3.167} & \num{-3.167} & \num{-3.167}\\
 & (\num{0.741}) & (\num{0.833}) & (\num{0.819})\\
hp & \num{-0.018} & \num{-0.018} & \num{-0.018}\\
 & (\num{0.012}) & (\num{0.010}) & (\num{0.013})\\
cyl & \num{-0.942} & \num{-0.942} & \num{-0.942}\\
 & (\num{0.551}) & (\num{0.573}) & (\num{0.572})\\
\midrule
Num.Obs. & \num{32} & \num{32} & \num{32}\\
R2 & \num{0.843} & \num{0.843} & \num{0.843}\\
R2 Adj. & \num{0.826} & \num{0.826} & \num{0.826}\\
AIC & \num{155.5} & \num{155.5} & \num{155.5}\\
BIC & \num{162.8} & \num{162.8} & \num{162.8}\\
Log.Lik. & \num{-72.738} & \num{-72.738} & \num{-72.738}\\
F & \num{50.171} & \num{31.065} & \num{32.623}\\
RMSE & \num{2.35} & \num{2.35} & \num{2.35}\\
Std.Errors & IID & HC3 & HC4\\
\bottomrule
\end{tabular}
\end{table}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{modelplot}\NormalTok{(lm\_mod, }\AttributeTok{vcov =} \FunctionTok{c}\NormalTok{(}\StringTok{"iid"}\NormalTok{,}\StringTok{"robust"}\NormalTok{,}\StringTok{"HC4"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{38-report_files/figure-latex/unnamed-chunk-4-1} \end{center}

Another package is \texttt{stargazer}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"stargazer"}\NormalTok{)}
\FunctionTok{stargazer}\NormalTok{(attitude)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} \% Table created by stargazer v.5.2.3 by Marek Hlavac, Social Policy Institute. E{-}mail: marek.hlavac at gmail.com}
\CommentTok{\#\textgreater{} \% Date and time: Thu, Aug 29, 2024 {-} 4:28:41 PM}
\CommentTok{\#\textgreater{} \textbackslash{}begin\{table\}[!htbp] \textbackslash{}centering }
\CommentTok{\#\textgreater{}   \textbackslash{}caption\{\} }
\CommentTok{\#\textgreater{}   \textbackslash{}label\{\} }
\CommentTok{\#\textgreater{} \textbackslash{}begin\{tabular\}\{@\{\textbackslash{}extracolsep\{5pt\}\}lccccc\} }
\CommentTok{\#\textgreater{} \textbackslash{}\textbackslash{}[{-}1.8ex]\textbackslash{}hline }
\CommentTok{\#\textgreater{} \textbackslash{}hline \textbackslash{}\textbackslash{}[{-}1.8ex] }
\CommentTok{\#\textgreater{} Statistic \& \textbackslash{}multicolumn\{1\}\{c\}\{N\} \& \textbackslash{}multicolumn\{1\}\{c\}\{Mean\} \& \textbackslash{}multicolumn\{1\}\{c\}\{St. Dev.\} \& \textbackslash{}multicolumn\{1\}\{c\}\{Min\} \& \textbackslash{}multicolumn\{1\}\{c\}\{Max\} \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{} \textbackslash{}hline \textbackslash{}\textbackslash{}[{-}1.8ex] }
\CommentTok{\#\textgreater{} rating \& 30 \& 64.633 \& 12.173 \& 40 \& 85 \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{} complaints \& 30 \& 66.600 \& 13.315 \& 37 \& 90 \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{} privileges \& 30 \& 53.133 \& 12.235 \& 30 \& 83 \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{} learning \& 30 \& 56.367 \& 11.737 \& 34 \& 75 \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{} raises \& 30 \& 64.633 \& 10.397 \& 43 \& 88 \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{} critical \& 30 \& 74.767 \& 9.895 \& 49 \& 92 \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{} advance \& 30 \& 42.933 \& 10.289 \& 25 \& 72 \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{} \textbackslash{}hline \textbackslash{}\textbackslash{}[{-}1.8ex] }
\CommentTok{\#\textgreater{} \textbackslash{}end\{tabular\} }
\CommentTok{\#\textgreater{} \textbackslash{}end\{table\}}
\DocumentationTok{\#\# 2 OLS models}
\NormalTok{linear}\FloatTok{.1} \OtherTok{\textless{}{-}}
    \FunctionTok{lm}\NormalTok{(rating }\SpecialCharTok{\textasciitilde{}}\NormalTok{ complaints }\SpecialCharTok{+}\NormalTok{ privileges }\SpecialCharTok{+}\NormalTok{ learning }\SpecialCharTok{+}\NormalTok{ raises }\SpecialCharTok{+}\NormalTok{ critical,}
       \AttributeTok{data =}\NormalTok{ attitude)}
\NormalTok{linear}\FloatTok{.2} \OtherTok{\textless{}{-}}
    \FunctionTok{lm}\NormalTok{(rating }\SpecialCharTok{\textasciitilde{}}\NormalTok{ complaints }\SpecialCharTok{+}\NormalTok{ privileges }\SpecialCharTok{+}\NormalTok{ learning, }\AttributeTok{data =}\NormalTok{ attitude)}
\DocumentationTok{\#\# create an indicator dependent variable, and run a probit model}
\NormalTok{attitude}\SpecialCharTok{$}\NormalTok{high.rating }\OtherTok{\textless{}{-}}\NormalTok{ (attitude}\SpecialCharTok{$}\NormalTok{rating }\SpecialCharTok{\textgreater{}} \DecValTok{70}\NormalTok{)}

\NormalTok{probit.model }\OtherTok{\textless{}{-}}
    \FunctionTok{glm}\NormalTok{(}
\NormalTok{        high.rating }\SpecialCharTok{\textasciitilde{}}\NormalTok{ learning }\SpecialCharTok{+}\NormalTok{ critical }\SpecialCharTok{+}\NormalTok{ advance,}
        \AttributeTok{data =}\NormalTok{ attitude,}
        \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"probit"}\NormalTok{)}
\NormalTok{    )}
\FunctionTok{stargazer}\NormalTok{(linear}\FloatTok{.1}\NormalTok{,}
\NormalTok{          linear}\FloatTok{.2}\NormalTok{,}
\NormalTok{          probit.model,}
          \AttributeTok{title =} \StringTok{"Results"}\NormalTok{,}
          \AttributeTok{align =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} \% Table created by stargazer v.5.2.3 by Marek Hlavac, Social Policy Institute. E{-}mail: marek.hlavac at gmail.com}
\CommentTok{\#\textgreater{} \% Date and time: Thu, Aug 29, 2024 {-} 4:28:41 PM}
\CommentTok{\#\textgreater{} \% Requires LaTeX packages: dcolumn }
\CommentTok{\#\textgreater{} \textbackslash{}begin\{table\}[!htbp] \textbackslash{}centering }
\CommentTok{\#\textgreater{}   \textbackslash{}caption\{Results\} }
\CommentTok{\#\textgreater{}   \textbackslash{}label\{\} }
\CommentTok{\#\textgreater{} \textbackslash{}begin\{tabular\}\{@\{\textbackslash{}extracolsep\{5pt\}\}lD\{.\}\{.\}\{{-}3\} D\{.\}\{.\}\{{-}3\} D\{.\}\{.\}\{{-}3\} \} }
\CommentTok{\#\textgreater{} \textbackslash{}\textbackslash{}[{-}1.8ex]\textbackslash{}hline }
\CommentTok{\#\textgreater{} \textbackslash{}hline \textbackslash{}\textbackslash{}[{-}1.8ex] }
\CommentTok{\#\textgreater{}  \& \textbackslash{}multicolumn\{3\}\{c\}\{\textbackslash{}textit\{Dependent variable:\}\} \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{} \textbackslash{}cline\{2{-}4\} }
\CommentTok{\#\textgreater{} \textbackslash{}\textbackslash{}[{-}1.8ex] \& \textbackslash{}multicolumn\{2\}\{c\}\{rating\} \& \textbackslash{}multicolumn\{1\}\{c\}\{high.rating\} \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{} \textbackslash{}\textbackslash{}[{-}1.8ex] \& \textbackslash{}multicolumn\{2\}\{c\}\{\textbackslash{}textit\{OLS\}\} \& \textbackslash{}multicolumn\{1\}\{c\}\{\textbackslash{}textit\{probit\}\} \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{} \textbackslash{}\textbackslash{}[{-}1.8ex] \& \textbackslash{}multicolumn\{1\}\{c\}\{(1)\} \& \textbackslash{}multicolumn\{1\}\{c\}\{(2)\} \& \textbackslash{}multicolumn\{1\}\{c\}\{(3)\}\textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{} \textbackslash{}hline \textbackslash{}\textbackslash{}[{-}1.8ex] }
\CommentTok{\#\textgreater{}  complaints \& 0.692\^{}\{***\} \& 0.682\^{}\{***\} \&  \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{}   \& (0.149) \& (0.129) \&  \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{}   \& \& \& \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{}  privileges \& {-}0.104 \& {-}0.103 \&  \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{}   \& (0.135) \& (0.129) \&  \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{}   \& \& \& \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{}  learning \& 0.249 \& 0.238\^{}\{*\} \& 0.164\^{}\{***\} \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{}   \& (0.160) \& (0.139) \& (0.053) \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{}   \& \& \& \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{}  raises \& {-}0.033 \&  \&  \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{}   \& (0.202) \&  \&  \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{}   \& \& \& \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{}  critical \& 0.015 \&  \& {-}0.001 \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{}   \& (0.147) \&  \& (0.044) \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{}   \& \& \& \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{}  advance \&  \&  \& {-}0.062 \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{}   \&  \&  \& (0.042) \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{}   \& \& \& \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{}  Constant \& 11.011 \& 11.258 \& {-}7.476\^{}\{**\} \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{}   \& (11.704) \& (7.318) \& (3.570) \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{}   \& \& \& \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{} \textbackslash{}hline \textbackslash{}\textbackslash{}[{-}1.8ex] }
\CommentTok{\#\textgreater{} Observations \& \textbackslash{}multicolumn\{1\}\{c\}\{30\} \& \textbackslash{}multicolumn\{1\}\{c\}\{30\} \& \textbackslash{}multicolumn\{1\}\{c\}\{30\} \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{} R$\^{}\{2\}$ \& \textbackslash{}multicolumn\{1\}\{c\}\{0.715\} \& \textbackslash{}multicolumn\{1\}\{c\}\{0.715\} \&  \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{} Adjusted R$\^{}\{2\}$ \& \textbackslash{}multicolumn\{1\}\{c\}\{0.656\} \& \textbackslash{}multicolumn\{1\}\{c\}\{0.682\} \&  \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{} Log Likelihood \&  \&  \& \textbackslash{}multicolumn\{1\}\{c\}\{{-}9.087\} \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{} Akaike Inf. Crit. \&  \&  \& \textbackslash{}multicolumn\{1\}\{c\}\{26.175\} \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{} Residual Std. Error \& \textbackslash{}multicolumn\{1\}\{c\}\{7.139 (df = 24)\} \& \textbackslash{}multicolumn\{1\}\{c\}\{6.863 (df = 26)\} \&  \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{} F Statistic \& \textbackslash{}multicolumn\{1\}\{c\}\{12.063$\^{}\{***\}$ (df = 5; 24)\} \& \textbackslash{}multicolumn\{1\}\{c\}\{21.743$\^{}\{***\}$ (df = 3; 26)\} \&  \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{} \textbackslash{}hline }
\CommentTok{\#\textgreater{} \textbackslash{}hline \textbackslash{}\textbackslash{}[{-}1.8ex] }
\CommentTok{\#\textgreater{} \textbackslash{}textit\{Note:\}  \& \textbackslash{}multicolumn\{3\}\{r\}\{$\^{}\{*\}$p$\textless{}$0.1; $\^{}\{**\}$p$\textless{}$0.05; $\^{}\{***\}$p$\textless{}$0.01\} \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{} \textbackslash{}end\{tabular\} }
\CommentTok{\#\textgreater{} \textbackslash{}end\{table\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Latex}
\FunctionTok{stargazer}\NormalTok{(}
\NormalTok{    linear}\FloatTok{.1}\NormalTok{,}
\NormalTok{    linear}\FloatTok{.2}\NormalTok{,}
\NormalTok{    probit.model,}
    \AttributeTok{title =} \StringTok{"Regression Results"}\NormalTok{,}
    \AttributeTok{align =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{dep.var.labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Overall Rating"}\NormalTok{, }\StringTok{"High Rating"}\NormalTok{),}
    \AttributeTok{covariate.labels =} \FunctionTok{c}\NormalTok{(}
        \StringTok{"Handling of Complaints"}\NormalTok{,}
        \StringTok{"No Special Privileges"}\NormalTok{,}
        \StringTok{"Opportunity to Learn"}\NormalTok{,}
        \StringTok{"Performance{-}Based Raises"}\NormalTok{,}
        \StringTok{"Too Critical"}\NormalTok{,}
        \StringTok{"Advancement"}
\NormalTok{    ),}
    \AttributeTok{omit.stat =} \FunctionTok{c}\NormalTok{(}\StringTok{"LL"}\NormalTok{, }\StringTok{"ser"}\NormalTok{, }\StringTok{"f"}\NormalTok{),}
    \AttributeTok{no.space =} \ConstantTok{TRUE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# ASCII text output}
\FunctionTok{stargazer}\NormalTok{(}
\NormalTok{    linear}\FloatTok{.1}\NormalTok{,}
\NormalTok{    linear}\FloatTok{.2}\NormalTok{,}
    \AttributeTok{type =} \StringTok{"text"}\NormalTok{,}
    \AttributeTok{title =} \StringTok{"Regression Results"}\NormalTok{,}
    \AttributeTok{dep.var.labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Overall Rating"}\NormalTok{, }\StringTok{"High Rating"}\NormalTok{),}
    \AttributeTok{covariate.labels =} \FunctionTok{c}\NormalTok{(}
        \StringTok{"Handling of Complaints"}\NormalTok{,}
        \StringTok{"No Special Privileges"}\NormalTok{,}
        \StringTok{"Opportunity to Learn"}\NormalTok{,}
        \StringTok{"Performance{-}Based Raises"}\NormalTok{,}
        \StringTok{"Too Critical"}\NormalTok{,}
        \StringTok{"Advancement"}
\NormalTok{    ),}
    \AttributeTok{omit.stat =} \FunctionTok{c}\NormalTok{(}\StringTok{"LL"}\NormalTok{, }\StringTok{"ser"}\NormalTok{, }\StringTok{"f"}\NormalTok{),}
    \AttributeTok{ci =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{ci.level =} \FloatTok{0.90}\NormalTok{,}
    \AttributeTok{single.row =} \ConstantTok{TRUE}
\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Regression Results}
\CommentTok{\#\textgreater{} ========================================================================}
\CommentTok{\#\textgreater{}                                        Dependent variable:              }
\CommentTok{\#\textgreater{}                          {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}                                          Overall Rating                 }
\CommentTok{\#\textgreater{}                                    (1)                     (2)          }
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} Handling of Complaints   0.692*** (0.447, 0.937) 0.682*** (0.470, 0.894)}
\CommentTok{\#\textgreater{} No Special Privileges    {-}0.104 ({-}0.325, 0.118)  {-}0.103 ({-}0.316, 0.109) }
\CommentTok{\#\textgreater{} Opportunity to Learn      0.249 ({-}0.013, 0.512)   0.238* (0.009, 0.467) }
\CommentTok{\#\textgreater{} Performance{-}Based Raises {-}0.033 ({-}0.366, 0.299)                         }
\CommentTok{\#\textgreater{} Too Critical              0.015 ({-}0.227, 0.258)                         }
\CommentTok{\#\textgreater{} Advancement              11.011 ({-}8.240, 30.262) 11.258 ({-}0.779, 23.296)}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} Observations                       30                      30           }
\CommentTok{\#\textgreater{} R2                                0.715                   0.715         }
\CommentTok{\#\textgreater{} Adjusted R2                       0.656                   0.682         }
\CommentTok{\#\textgreater{} ========================================================================}
\CommentTok{\#\textgreater{} Note:                                        *p\textless{}0.1; **p\textless{}0.05; ***p\textless{}0.01}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{stargazer}\NormalTok{(}
\NormalTok{    linear}\FloatTok{.1}\NormalTok{,}
\NormalTok{    linear}\FloatTok{.2}\NormalTok{,}
\NormalTok{    probit.model,}
    \AttributeTok{title =} \StringTok{"Regression Results"}\NormalTok{,}
    \AttributeTok{align =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{dep.var.labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Overall Rating"}\NormalTok{, }\StringTok{"High Rating"}\NormalTok{),}
    \AttributeTok{covariate.labels =} \FunctionTok{c}\NormalTok{(}
        \StringTok{"Handling of Complaints"}\NormalTok{,}
        \StringTok{"No Special Privileges"}\NormalTok{,}
        \StringTok{"Opportunity to Learn"}\NormalTok{,}
        \StringTok{"Performance{-}Based Raises"}\NormalTok{,}
        \StringTok{"Too Critical"}\NormalTok{,}
        \StringTok{"Advancement"}
\NormalTok{    ),}
    \AttributeTok{omit.stat =} \FunctionTok{c}\NormalTok{(}\StringTok{"LL"}\NormalTok{, }\StringTok{"ser"}\NormalTok{, }\StringTok{"f"}\NormalTok{),}
    \AttributeTok{no.space =} \ConstantTok{TRUE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Correlation Table

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{correlation.matrix }\OtherTok{\textless{}{-}}
    \FunctionTok{cor}\NormalTok{(attitude[, }\FunctionTok{c}\NormalTok{(}\StringTok{"rating"}\NormalTok{, }\StringTok{"complaints"}\NormalTok{, }\StringTok{"privileges"}\NormalTok{)])}
\FunctionTok{stargazer}\NormalTok{(correlation.matrix, }\AttributeTok{title =} \StringTok{"Correlation Matrix"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{changes-in-an-estimate}{%
\section{Changes in an estimate}\label{changes-in-an-estimate}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{coef\_names }\OtherTok{\textless{}{-}}\NormalTok{ coef\_names[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{] }\CommentTok{\# Dropping intercept for plots}
\FunctionTok{plot\_summs}\NormalTok{(fit, fit\_b, fit\_c, }\AttributeTok{robust =} \StringTok{"HC3"}\NormalTok{, }\AttributeTok{coefs =}\NormalTok{ coef\_names)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{38-report_files/figure-latex/unnamed-chunk-10-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot\_summs}\NormalTok{(}
\NormalTok{    fit\_c,}
    \AttributeTok{robust =} \StringTok{"HC3"}\NormalTok{,}
    \AttributeTok{coefs =}\NormalTok{ coef\_names,}
    \AttributeTok{plot.distributions =} \ConstantTok{TRUE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{38-report_files/figure-latex/unnamed-chunk-10-2} \end{center}

\hypertarget{standard-errors-3}{%
\section{Standard Errors}\label{standard-errors-3}}

\texttt{sandwich} \href{cran.r-project.org/web/packages/sandwich/vignettes/sandwich-CL.pdf}{vignette}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.0890}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.0890}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.6233}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1849}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Applicable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Usage
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Reference
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{const} & & Assume constant variances & \\
\texttt{HC} \texttt{HC0} & \texttt{vcovCL} & Heterogeneity

White's estimator

All other heterogeneity SE methods are derivatives of this.

No small sample bias adjustment & \citep{white1980} \\
\texttt{HC1} & \texttt{vcovCL} & Uses a degrees of freedom-based correction

When the number of clusters is small, \texttt{HC2} and \texttt{HC3} are better \citep{cameron2008bootstrap} & \citep{mackinnon1985some} \\
\texttt{HC2} & \texttt{vcovCL} & Better with the linear model, but still applicable for \protect\hyperlink{generalized-linear-models}{Generalized Linear Models}

Needs a hat (weighted) matrix & \\
\texttt{HC3} & \texttt{vcovCL} & Better with the linear model, but still applicable for \protect\hyperlink{generalized-linear-models}{Generalized Linear Models}

Needs a hat (weighted) matrix & \\
\texttt{HC4} & \texttt{vcovHC} & & \citep{cribari2004asymptotic} \\
\texttt{HC4m} & \texttt{vcovHC} & & \citep{cribari2007inference} \\
\texttt{HC5} & \texttt{vcovHC} & & \citep{cribari2011new} \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(cars)}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(speed }\SpecialCharTok{\textasciitilde{}}\NormalTok{ dist, }\AttributeTok{data =}\NormalTok{ cars)}
\FunctionTok{summary}\NormalTok{(model)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = speed \textasciitilde{} dist, data = cars)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}     Min      1Q  Median      3Q     Max }
\CommentTok{\#\textgreater{} {-}7.5293 {-}2.1550  0.3615  2.4377  6.4179 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)  8.28391    0.87438   9.474 1.44e{-}12 ***}
\CommentTok{\#\textgreater{} dist         0.16557    0.01749   9.464 1.49e{-}12 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 3.156 on 48 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.6511, Adjusted R{-}squared:  0.6438 }
\CommentTok{\#\textgreater{} F{-}statistic: 89.57 on 1 and 48 DF,  p{-}value: 1.49e{-}12}
\NormalTok{lmtest}\SpecialCharTok{::}\FunctionTok{coeftest}\NormalTok{(model, }\AttributeTok{vcov. =}\NormalTok{ sandwich}\SpecialCharTok{::}\FunctionTok{vcovHC}\NormalTok{(model, }\AttributeTok{type =} \StringTok{"HC1"}\NormalTok{))}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} t test of coefficients:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}             Estimate Std. Error t value  Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept) 8.283906   0.891860  9.2883 2.682e{-}12 ***}
\CommentTok{\#\textgreater{} dist        0.165568   0.019402  8.5335 3.482e{-}11 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

\hypertarget{coefficient-uncertainty-and-distribution}{%
\section{Coefficient Uncertainty and Distribution}\label{coefficient-uncertainty-and-distribution}}

The \texttt{ggdist} allows us to visualize uncertainty under both frequentist and Bayesian frameworks

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggdist)}
\end{Highlighting}
\end{Shaded}

\hypertarget{descriptive-tables}{%
\section{Descriptive Tables}\label{descriptive-tables}}

Export APA theme

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(}\StringTok{"mtcars"}\NormalTok{)}

\FunctionTok{library}\NormalTok{(flextable)}
\FunctionTok{theme\_apa}\NormalTok{(}\FunctionTok{flextable}\NormalTok{(mtcars[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

Export to Latex

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(xtable}\SpecialCharTok{::}\FunctionTok{xtable}\NormalTok{(mtcars, }\AttributeTok{type =} \StringTok{"latex"}\NormalTok{),}
      \AttributeTok{file =} \FunctionTok{file.path}\NormalTok{(}\FunctionTok{getwd}\NormalTok{(), }\StringTok{"output"}\NormalTok{, }\StringTok{"mtcars\_xtable.tex"}\NormalTok{))}

\CommentTok{\# American Economic Review style}
\NormalTok{stargazer}\SpecialCharTok{::}\FunctionTok{stargazer}\NormalTok{(}
\NormalTok{    mtcars,}
    \AttributeTok{title =} \StringTok{"Testing"}\NormalTok{,}
    \AttributeTok{style =} \StringTok{"aer"}\NormalTok{,}
    \AttributeTok{out =} \FunctionTok{file.path}\NormalTok{(}\FunctionTok{getwd}\NormalTok{(), }\StringTok{"output"}\NormalTok{, }\StringTok{"mtcars\_stargazer.tex"}\NormalTok{)}
\NormalTok{)}

\CommentTok{\# other styles include}
\CommentTok{\# Administrative Science Quarterly}
\CommentTok{\# Quarterly Journal of Economics}
\end{Highlighting}
\end{Shaded}

However, the above codes do not play well with notes. Hence, I create my own custom code that follows the AMA guidelines

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ama\_tbl }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, caption, label, note, output\_path) \{}
  \FunctionTok{library}\NormalTok{(tidyverse)}
  \FunctionTok{library}\NormalTok{(xtable)}
  \CommentTok{\# Function to determine column alignment}
\NormalTok{  get\_column\_alignment }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data) \{}
    \CommentTok{\# Start with the alignment for the header row}
\NormalTok{    alignment }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"l"}\NormalTok{, }\StringTok{"l"}\NormalTok{)}
    
    \CommentTok{\# Check each column}
    \ControlFlowTok{for}\NormalTok{ (col }\ControlFlowTok{in} \FunctionTok{seq\_len}\NormalTok{(}\FunctionTok{ncol}\NormalTok{(data))[}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]) \{}
      \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{is.numeric}\NormalTok{(data[[col]])) \{}
\NormalTok{        alignment }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(alignment, }\StringTok{"r"}\NormalTok{)  }\CommentTok{\# Right alignment for numbers}
\NormalTok{      \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{        alignment }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(alignment, }\StringTok{"c"}\NormalTok{)  }\CommentTok{\# Center alignment for other data}
\NormalTok{      \}}
\NormalTok{    \}}
    
    \FunctionTok{return}\NormalTok{(alignment)}
\NormalTok{  \}}
  
\NormalTok{  data }\SpecialCharTok{\%\textgreater{}\%}
    \CommentTok{\# bold + left align first column }
    \FunctionTok{rename\_with}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\FunctionTok{paste}\NormalTok{(}\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{multicolumn\{1\}\{l\}\{}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{textbf\{"}\NormalTok{, ., }\StringTok{"\}\}"}\NormalTok{), }\DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
    \CommentTok{\# bold + center align all other columns}
    \StringTok{\textasciigrave{}}\AttributeTok{colnames\textless{}{-}}\StringTok{\textasciigrave{}}\NormalTok{(}\FunctionTok{ifelse}\NormalTok{(}\FunctionTok{colnames}\NormalTok{(.) }\SpecialCharTok{!=} \FunctionTok{colnames}\NormalTok{(.)[}\DecValTok{1}\NormalTok{],}
                        \FunctionTok{paste}\NormalTok{(}\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{multicolumn\{1\}\{c\}\{}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{textbf\{"}\NormalTok{, }\FunctionTok{colnames}\NormalTok{(.), }\StringTok{"\}\}"}\NormalTok{),}
                        \FunctionTok{colnames}\NormalTok{(.))) }\SpecialCharTok{\%\textgreater{}\%} 
    
    \FunctionTok{xtable}\NormalTok{(}\AttributeTok{caption =}\NormalTok{ caption,}
           \AttributeTok{label =}\NormalTok{ label,}
           \AttributeTok{align =} \FunctionTok{get\_column\_alignment}\NormalTok{(data),}
           \AttributeTok{auto =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{print}\NormalTok{(}
      \AttributeTok{include.rownames =} \ConstantTok{FALSE}\NormalTok{,}
      \AttributeTok{caption.placement =} \StringTok{"top"}\NormalTok{,}
      
      \AttributeTok{hline.after=}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{),}
      
       \CommentTok{\# p\{0.9\textbackslash{}linewidth\} sets the width of the column to 90\% of the line width, and the @\{\} removes any extra padding around the cell.}
      
      \AttributeTok{add.to.row =} \FunctionTok{list}\NormalTok{(}\AttributeTok{pos =} \FunctionTok{list}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(data)), }\CommentTok{\# Add at the bottom of the table}
                        \AttributeTok{command =} \FunctionTok{c}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(}\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{hline }\SpecialCharTok{\textbackslash{}n}\StringTok{ }\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{multicolumn\{"}\NormalTok{,}\FunctionTok{ncol}\NormalTok{(data), }\StringTok{"\}\{l\} \{"}\NormalTok{, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{ }\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{begin\{tabular\}\{@\{\}p\{0.9}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{linewidth\}@\{\}\} }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{,}\StringTok{"Note: "}\NormalTok{, note, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{ }\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{end\{tabular\}  \} }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{))), }\CommentTok{\# Add your note here}
      
      \CommentTok{\# make sure your heading is untouched (because you manually change it above)}
      \AttributeTok{sanitize.colnames.function =}\NormalTok{ identity,}
      
      \CommentTok{\# place a the top of the page}
      \AttributeTok{table.placement =} \StringTok{"h"}\NormalTok{,}
      
      \AttributeTok{file =}\NormalTok{ output\_path}
\NormalTok{    )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ama\_tbl}\NormalTok{(}
\NormalTok{    mtcars,}
    \AttributeTok{caption     =} \StringTok{"This is caption"}\NormalTok{,}
    \AttributeTok{label       =} \StringTok{"tab:this\_is\_label"}\NormalTok{,}
    \AttributeTok{note        =} \StringTok{"this is note"}\NormalTok{,}
    \AttributeTok{output\_path =} \FunctionTok{file.path}\NormalTok{(}\FunctionTok{getwd}\NormalTok{(), }\StringTok{"output"}\NormalTok{, }\StringTok{"mtcars\_custom\_ama.tex"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{visualizations-and-plots}{%
\section{Visualizations and Plots}\label{visualizations-and-plots}}

You can customize your plots based on your preferred journals. Here, I am creating a custom setting for the American Marketing Association.

American-Marketing-Association-ready theme for plots

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}

\CommentTok{\# check available fonts}
\CommentTok{\# windowsFonts()}

\CommentTok{\# for Times New Roman}
\CommentTok{\# names(windowsFonts()[windowsFonts()=="TT Times New Roman"])}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Making a theme}
\NormalTok{amatheme }\OtherTok{=} \FunctionTok{theme\_bw}\NormalTok{(}\AttributeTok{base\_size =} \DecValTok{14}\NormalTok{, }\AttributeTok{base\_family =} \StringTok{"serif"}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# This is Time New Roman}
    
    \FunctionTok{theme}\NormalTok{(}
        \CommentTok{\# remove major gridlines}
        \AttributeTok{panel.grid.major   =} \FunctionTok{element\_blank}\NormalTok{(),}

        \CommentTok{\# remove minor gridlines}
        \AttributeTok{panel.grid.minor   =} \FunctionTok{element\_blank}\NormalTok{(),}

        \CommentTok{\# remove panel border}
        \AttributeTok{panel.border       =} \FunctionTok{element\_blank}\NormalTok{(),}

        \AttributeTok{line               =} \FunctionTok{element\_line}\NormalTok{(),}

        \CommentTok{\# change font}
        \AttributeTok{text               =} \FunctionTok{element\_text}\NormalTok{(),}

        \CommentTok{\# if you want to remove legend title}
        \CommentTok{\# legend.title     = element\_blank(),}

        \AttributeTok{legend.title       =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \FunctionTok{rel}\NormalTok{(}\FloatTok{0.6}\NormalTok{), }\AttributeTok{face =} \StringTok{"bold"}\NormalTok{),}

        \CommentTok{\# change font size of legend}
        \AttributeTok{legend.text        =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \FunctionTok{rel}\NormalTok{(}\FloatTok{0.6}\NormalTok{)),}
        
        \AttributeTok{legend.background  =} \FunctionTok{element\_rect}\NormalTok{(}\AttributeTok{color =} \StringTok{"black"}\NormalTok{),}
        
        \CommentTok{\# legend.margin    = margin(t = 5, l = 5, r = 5, b = 5),}
        \CommentTok{\# legend.key       = element\_rect(color = NA, fill = NA),}

        \CommentTok{\# change font size of main title}
        \AttributeTok{plot.title         =} \FunctionTok{element\_text}\NormalTok{(}
            \AttributeTok{size           =} \FunctionTok{rel}\NormalTok{(}\FloatTok{1.2}\NormalTok{),}
            \AttributeTok{face           =} \StringTok{"bold"}\NormalTok{,}
            \AttributeTok{hjust          =} \FloatTok{0.5}\NormalTok{,}
            \AttributeTok{margin         =} \FunctionTok{margin}\NormalTok{(}\AttributeTok{b =} \DecValTok{15}\NormalTok{)}
\NormalTok{        ),}
        
        \AttributeTok{plot.margin        =} \FunctionTok{unit}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }\StringTok{"cm"}\NormalTok{),}

        \CommentTok{\# add black line along axes}
        \AttributeTok{axis.line          =} \FunctionTok{element\_line}\NormalTok{(}\AttributeTok{colour =} \StringTok{"black"}\NormalTok{, }\AttributeTok{linewidth =}\NormalTok{ .}\DecValTok{8}\NormalTok{),}
        
        \AttributeTok{axis.ticks         =} \FunctionTok{element\_line}\NormalTok{(),}
        

        \CommentTok{\# axis title}
        \AttributeTok{axis.title.x       =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \FunctionTok{rel}\NormalTok{(}\FloatTok{1.2}\NormalTok{), }\AttributeTok{face =} \StringTok{"bold"}\NormalTok{),}
        \AttributeTok{axis.title.y       =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \FunctionTok{rel}\NormalTok{(}\FloatTok{1.2}\NormalTok{), }\AttributeTok{face =} \StringTok{"bold"}\NormalTok{),}

        \CommentTok{\# axis text size}
        \AttributeTok{axis.text.y        =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \FunctionTok{rel}\NormalTok{(}\DecValTok{1}\NormalTok{)),}
        \AttributeTok{axis.text.x        =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \FunctionTok{rel}\NormalTok{(}\DecValTok{1}\NormalTok{))}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

Example

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(ggsci)}
\FunctionTok{data}\NormalTok{(}\StringTok{"mtcars"}\NormalTok{)}
\NormalTok{yourplot }\OtherTok{\textless{}{-}}\NormalTok{ mtcars }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{select}\NormalTok{(mpg, cyl, gear) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{ggplot}\NormalTok{(., }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ mpg, }\AttributeTok{y =}\NormalTok{ cyl, }\AttributeTok{fill =}\NormalTok{ gear)) }\SpecialCharTok{+} 
    \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{title=}\StringTok{"Some Plot"}\NormalTok{) }

\NormalTok{yourplot }\SpecialCharTok{+} 
\NormalTok{    amatheme }\SpecialCharTok{+} 
    \CommentTok{\# choose different color theme}
    \FunctionTok{scale\_color\_npg}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{38-report_files/figure-latex/unnamed-chunk-19-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{yourplot }\SpecialCharTok{+} 
\NormalTok{    amatheme }\SpecialCharTok{+} 
    \FunctionTok{scale\_color\_continuous}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{38-report_files/figure-latex/unnamed-chunk-19-2} \end{center}

Other pre-specified themes

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggthemes)}


\CommentTok{\# Stata theme}
\NormalTok{yourplot }\SpecialCharTok{+}
    \FunctionTok{theme\_stata}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{38-report_files/figure-latex/unnamed-chunk-20-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# The economist theme}
\NormalTok{yourplot }\SpecialCharTok{+} 
    \FunctionTok{theme\_economist}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{38-report_files/figure-latex/unnamed-chunk-20-2} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{yourplot }\SpecialCharTok{+} 
    \FunctionTok{theme\_economist\_white}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{38-report_files/figure-latex/unnamed-chunk-20-3} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Wall street journal theme}
\NormalTok{yourplot }\SpecialCharTok{+} 
    \FunctionTok{theme\_wsj}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{38-report_files/figure-latex/unnamed-chunk-20-4} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# APA theme}
\NormalTok{yourplot }\SpecialCharTok{+}
\NormalTok{    jtools}\SpecialCharTok{::}\FunctionTok{theme\_apa}\NormalTok{(}
        \AttributeTok{legend.font.size =} \DecValTok{24}\NormalTok{,}
        \AttributeTok{x.font.size =} \DecValTok{20}\NormalTok{,}
        \AttributeTok{y.font.size =} \DecValTok{20}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{38-report_files/figure-latex/unnamed-chunk-20-5} \end{center}

\hypertarget{exploratory-data-analysis}{%
\chapter{Exploratory Data Analysis}\label{exploratory-data-analysis}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load to get txhousing data}
\FunctionTok{library}\NormalTok{(ggplot2)}
\end{Highlighting}
\end{Shaded}

Data Report

Feature Engineering

Missing Data

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# install.packages("DataExplorer")}
\FunctionTok{library}\NormalTok{(DataExplorer)}

\CommentTok{\# creat a html file that contain all reports}
\FunctionTok{create\_report}\NormalTok{(txhousing)}

\FunctionTok{introduce}\NormalTok{() }\CommentTok{\# see basic info}


\FunctionTok{dummify}\NormalTok{() }\CommentTok{\# create binary columns from discrete variables}
\FunctionTok{split\_columns}\NormalTok{() }\CommentTok{\# split data into discrete and continuous parts}



\FunctionTok{plot\_correlation}\NormalTok{() }\CommentTok{\# heatmap for discrete var}
\FunctionTok{plot\_intro}\NormalTok{() }

\FunctionTok{plot\_missing}\NormalTok{() }\CommentTok{\# plot missing value}
\FunctionTok{profile\_missing}\NormalTok{() }\CommentTok{\# profile missing values}


\FunctionTok{plot\_prcomp}\NormalTok{() }\CommentTok{\# plot PCA}
\end{Highlighting}
\end{Shaded}

Error Identification

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# install.packages("dataReporter")}
\FunctionTok{library}\NormalTok{(dataReporter)}
\FunctionTok{makeDataReport}\NormalTok{() }\CommentTok{\# detailed report like DataExplorer}
\end{Highlighting}
\end{Shaded}

Summary statistics

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(skimr)}
\FunctionTok{skim}\NormalTok{() }\CommentTok{\# give only few quick summary stat, not as detailed as the other two packages}
\end{Highlighting}
\end{Shaded}

Not so code-y process

Quick and dirty way to look at your data

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# install.packages("rpivotTable")}
\FunctionTok{library}\NormalTok{(rpivotTable)}
\CommentTok{\# give set up just like Excel table }
\NormalTok{data }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{    rpivotTable}\SpecialCharTok{::}\FunctionTok{rpivotTable}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Code generation and wrangling

Shiny-app based Tableu style

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# install.packages("esquisse")}
\FunctionTok{library}\NormalTok{(esquisse)}
\NormalTok{esquisse}\SpecialCharTok{::}\FunctionTok{esquisser}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Customized your daily/automatic report

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# install.packages("chronicle")}
\FunctionTok{library}\NormalTok{(chronicle)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# install.packages("dlookr")}
\CommentTok{\# install.packages("descriptr")}
\end{Highlighting}
\end{Shaded}

\hypertarget{sensitivity-analysis-robustness-check}{%
\chapter{Sensitivity Analysis/ Robustness Check}\label{sensitivity-analysis-robustness-check}}

\hypertarget{specification-curve}{%
\section{Specification curve}\label{specification-curve}}

\begin{itemize}
\tightlist
\item
  also known as Specification robustness graph or coefficient stability plot
\end{itemize}

Resources

\begin{itemize}
\item
  \href{https://github.com/hhsievertsen/speccurve}{In Stata} or \href{https://github.com/martin-andresen/speccurve}{speccurve}
\item
  \citep{simonsohn2020specification}
\end{itemize}

\hypertarget{starbility}{%
\subsection{starbility}\label{starbility}}

\begin{itemize}
\tightlist
\item
  Recommend
\end{itemize}

Installation

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{devtools}\SpecialCharTok{::}\FunctionTok{install\_github}\NormalTok{(}\StringTok{\textquotesingle{}https://github.com/AakaashRao/starbility\textquotesingle{}}\NormalTok{)}
\FunctionTok{library}\NormalTok{(starbility)}
\end{Highlighting}
\end{Shaded}

Example by the \href{https://htmlpreview.github.io/?https://github.com/AakaashRao/starbility/blob/master/doc/starbility.html}{package's author}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(starbility)}
\FunctionTok{library}\NormalTok{(lfe)}
\FunctionTok{data}\NormalTok{(}\StringTok{"diamonds"}\NormalTok{)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{43}\NormalTok{)}
\NormalTok{indices }\OtherTok{=} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(diamonds),}
                 \AttributeTok{replace =}\NormalTok{ F,}
                 \AttributeTok{size =} \FunctionTok{round}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(diamonds) }\SpecialCharTok{/} \DecValTok{20}\NormalTok{))}
\NormalTok{diamonds }\OtherTok{=}\NormalTok{ diamonds[indices, ]}
\end{Highlighting}
\end{Shaded}

Plot different combinations of controls

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# If you want to make the diamond dimensions as base control}
\NormalTok{base\_controls }\OtherTok{=} \FunctionTok{c}\NormalTok{(}
  \StringTok{\textquotesingle{}Diamond dimensions\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}x + y + z\textquotesingle{}} \CommentTok{\# include all variables under 1 dimension}
\NormalTok{)}


\NormalTok{perm\_controls }\OtherTok{=} \FunctionTok{c}\NormalTok{(}
  \StringTok{\textquotesingle{}Depth\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}depth\textquotesingle{}}\NormalTok{,}
  \StringTok{\textquotesingle{}Table width\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}table\textquotesingle{}}
\NormalTok{)}

\NormalTok{nonperm\_fe\_controls }\OtherTok{=} \FunctionTok{c}\NormalTok{(}
  \StringTok{\textquotesingle{}Clarity FE (granular)\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}clarity\textquotesingle{}}\NormalTok{,}
  \StringTok{\textquotesingle{}Clarity FE (binary)\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}high\_clarity\textquotesingle{}}
\NormalTok{)}

\CommentTok{\# Adding fixed effects}
\NormalTok{nonperm\_fe\_controls }\OtherTok{=} \FunctionTok{c}\NormalTok{(}
  \StringTok{\textquotesingle{}Clarity FE (granular)\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}clarity\textquotesingle{}}\NormalTok{,}
  \StringTok{\textquotesingle{}Clarity FE (binary)\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}high\_clarity\textquotesingle{}}
\NormalTok{)}

\CommentTok{\# Adding instrumental variables }
\NormalTok{instruments }\OtherTok{=} \StringTok{\textquotesingle{}x+y+z\textquotesingle{}}

\CommentTok{\# clustering and weights }
\NormalTok{diamonds}\SpecialCharTok{$}\NormalTok{sample\_weights }\OtherTok{=} \FunctionTok{runif}\NormalTok{(}\AttributeTok{n =} \FunctionTok{nrow}\NormalTok{(diamonds))}


\CommentTok{\# robust standard errors }
\NormalTok{starb\_felm\_custom }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(spec, data, rhs, ...) \{}
\NormalTok{  spec }\OtherTok{=} \FunctionTok{as.formula}\NormalTok{(spec)}
\NormalTok{  model }\OtherTok{=}\NormalTok{ lfe}\SpecialCharTok{::}\FunctionTok{felm}\NormalTok{(spec, }\AttributeTok{data=}\NormalTok{data) }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ broom}\SpecialCharTok{::}\FunctionTok{tidy}\NormalTok{()}

\NormalTok{  row }\OtherTok{=} \FunctionTok{which}\NormalTok{(model}\SpecialCharTok{$}\NormalTok{term}\SpecialCharTok{==}\NormalTok{rhs)}
\NormalTok{  coef }\OtherTok{=}\NormalTok{ model[row, }\StringTok{\textquotesingle{}estimate\textquotesingle{}}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as.numeric}\NormalTok{()}
\NormalTok{  se   }\OtherTok{=}\NormalTok{ model[row, }\StringTok{\textquotesingle{}std.error\textquotesingle{}}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as.numeric}\NormalTok{()}
\NormalTok{  p    }\OtherTok{=}\NormalTok{ model[row, }\StringTok{\textquotesingle{}p.value\textquotesingle{}}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as.numeric}\NormalTok{()}
  
  \CommentTok{\# 99\% confidence interval}
\NormalTok{  z }\OtherTok{=} \FunctionTok{qnorm}\NormalTok{(}\FloatTok{0.995}\NormalTok{) }
  \CommentTok{\# one{-}tailed test}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{c}\NormalTok{(coef, p}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, coef}\SpecialCharTok{+}\NormalTok{z}\SpecialCharTok{*}\NormalTok{se, coef}\SpecialCharTok{{-}}\NormalTok{z}\SpecialCharTok{*}\NormalTok{se))}
\NormalTok{\}}

\NormalTok{plots }\OtherTok{=} \FunctionTok{stability\_plot}\NormalTok{(}
    \AttributeTok{data =}\NormalTok{ diamonds,}
    \AttributeTok{lhs =} \StringTok{\textquotesingle{}price\textquotesingle{}}\NormalTok{,}
    \AttributeTok{rhs =} \StringTok{\textquotesingle{}carat\textquotesingle{}}\NormalTok{,}
    \AttributeTok{error\_geom =} \StringTok{\textquotesingle{}ribbon\textquotesingle{}}\NormalTok{, }\CommentTok{\# make the plot more aesthetics}
    \CommentTok{\# error\_geom = \textquotesingle{}none\textquotesingle{}, \# if you don\textquotesingle{}t want ribbon (i.e., error bar)}
    \AttributeTok{model =}\NormalTok{ starb\_felm\_custom,}
    \AttributeTok{cluster =} \StringTok{\textquotesingle{}cut\textquotesingle{}}\NormalTok{,}
    \AttributeTok{weights =} \StringTok{\textquotesingle{}sample\_weights\textquotesingle{}}\NormalTok{,}
    \CommentTok{\# iv = instruments,}
    \AttributeTok{perm =}\NormalTok{ perm\_controls,}
    \AttributeTok{base =}\NormalTok{ base\_controls,}
    \CommentTok{\# perm\_fe = perm\_fe\_controls,}
    
    \CommentTok{\# if you want to include fixed effects sequentially (not all combinations) }
    \CommentTok{\# (e.g., you want to test country or state fixed effect, not both )}
    \CommentTok{\# nonperm\_fe = nonperm\_fe\_controls, }
    \CommentTok{\# fe\_always = F,  \# if you want to have a model without any Fixed Effects}
    
    \CommentTok{\# sort "asc", "desc", or by fixed effects: "asc{-}by{-}fe" or "desc{-}by{-}fe"}
    \AttributeTok{sort =} \StringTok{"asc{-}by{-}fe"}\NormalTok{, }
    
    \CommentTok{\# if you have less variables and want more aesthetics }
    \CommentTok{\# control\_geom = \textquotesingle{}circle\textquotesingle{},}
    \CommentTok{\# point\_size = 2,}
    \CommentTok{\# control\_spacing = 0.3,}
    
    
    \CommentTok{\# error\_alpha = 0.2, \# change alpha of the error geom}
    \CommentTok{\# point\_size = 1.5, \# change the size of the coefficient points}
    \CommentTok{\# control\_text\_size = 10, \# change the size of the control labels}
    \CommentTok{\# coef\_ylim = c({-}5000, 35000), \# change the endpoints of the y{-}axis}
    \CommentTok{\# trip\_top = 3, \# change the spacing between the two panels}
    
    \AttributeTok{rel\_height =} \FloatTok{0.6}
\NormalTok{)}
\NormalTok{plots}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{40-sensitivity-robustness_files/figure-latex/unnamed-chunk-3-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# add comments}
\CommentTok{\# replacement\_coef\_panel = plots[[1]] +}
\CommentTok{\#   scale\_y\_reverse() +}
\CommentTok{\#   theme(panel.grid.minor = element\_blank()) +}
\CommentTok{\#   geom\_vline(xintercept = 41,}
\CommentTok{\#              linetype = \textquotesingle{}dashed\textquotesingle{},}
\CommentTok{\#              alpha = 0.4) +}
\CommentTok{\#   annotate(}
\CommentTok{\#     geom = \textquotesingle{}label\textquotesingle{},}
\CommentTok{\#     x = 52,}
\CommentTok{\#     y = 30000,}
\CommentTok{\#     label = \textquotesingle{}What a great\textbackslash{}nspecification!\textquotesingle{},}
\CommentTok{\#     alpha = 0.75}
\CommentTok{\#   )}
\CommentTok{\# }
\CommentTok{\# combine\_plots(replacement\_coef\_panel,}
\CommentTok{\#               plots[[2]],}
\CommentTok{\#               rel\_height = 0.6)}
\end{Highlighting}
\end{Shaded}

Note:

\begin{itemize}
\tightlist
\item
  \(p < 0.01\): red
\item
  \(p < 0.05\): green
\item
  \(p < 0.1\): blue
\item
  \(p > 0.1\): black
\end{itemize}

\href{https://htmlpreview.github.io/?https://github.com/AakaashRao/starbility/blob/master/doc/starbility-advanced.html}{More Advanced Stuff}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Step 1: Control Grid}

\NormalTok{diamonds}\SpecialCharTok{$}\NormalTok{high\_clarity }\OtherTok{=}\NormalTok{ diamonds}\SpecialCharTok{$}\NormalTok{clarity }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}VS1\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}VVS2\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}VVS1\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}IF\textquotesingle{}}\NormalTok{)}

\NormalTok{base\_controls }\OtherTok{=} \FunctionTok{c}\NormalTok{(}
  \StringTok{\textquotesingle{}Diamond dimensions\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}x + y + z\textquotesingle{}}
\NormalTok{)}

\NormalTok{perm\_controls }\OtherTok{=} \FunctionTok{c}\NormalTok{(}
  \StringTok{\textquotesingle{}Depth\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}depth\textquotesingle{}}\NormalTok{,}
  \StringTok{\textquotesingle{}Table width\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}table\textquotesingle{}}
\NormalTok{)}

\NormalTok{perm\_fe\_controls }\OtherTok{=} \FunctionTok{c}\NormalTok{(}
  \StringTok{\textquotesingle{}Cut FE\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}cut\textquotesingle{}}\NormalTok{,}
  \StringTok{\textquotesingle{}Color FE\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}color\textquotesingle{}}
\NormalTok{)}
\NormalTok{nonperm\_fe\_controls }\OtherTok{=} \FunctionTok{c}\NormalTok{(}
  \StringTok{\textquotesingle{}Clarity FE (granular)\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}clarity\textquotesingle{}}\NormalTok{,}
  \StringTok{\textquotesingle{}Clarity FE (binary)\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}high\_clarity\textquotesingle{}}
\NormalTok{)}

\NormalTok{grid1 }\OtherTok{=} \FunctionTok{stability\_plot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ diamonds, }
                      \AttributeTok{lhs =} \StringTok{\textquotesingle{}price\textquotesingle{}}\NormalTok{, }
                      \AttributeTok{rhs =} \StringTok{\textquotesingle{}carat\textquotesingle{}}\NormalTok{, }
                      \AttributeTok{perm =}\NormalTok{ perm\_controls,}
                      \AttributeTok{base =}\NormalTok{ base\_controls, }
                      \AttributeTok{perm\_fe =}\NormalTok{ perm\_fe\_controls, }
                      \AttributeTok{nonperm\_fe =}\NormalTok{ nonperm\_fe\_controls, }
                      \AttributeTok{run\_to=}\DecValTok{2}\NormalTok{)}

\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(grid1 }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{head}\NormalTok{(}\DecValTok{10}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|r|r|r|r|l}
\hline
Diamond dimensions & Depth & Table width & Cut FE & Color FE & np\_fe\\
\hline
1 & 0 & 0 & 0 & 0 & \\
\hline
1 & 1 & 0 & 0 & 0 & \\
\hline
1 & 0 & 1 & 0 & 0 & \\
\hline
1 & 1 & 1 & 0 & 0 & \\
\hline
1 & 0 & 0 & 1 & 0 & \\
\hline
1 & 1 & 0 & 1 & 0 & \\
\hline
1 & 0 & 1 & 1 & 0 & \\
\hline
1 & 1 & 1 & 1 & 0 & \\
\hline
1 & 0 & 0 & 0 & 1 & \\
\hline
1 & 1 & 0 & 0 & 1 & \\
\hline
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Step 2: Get model expression}

\NormalTok{grid2 }\OtherTok{=} \FunctionTok{stability\_plot}\NormalTok{(}\AttributeTok{grid =}\NormalTok{ grid1,}
                      \AttributeTok{data=}\NormalTok{diamonds, }
                      \AttributeTok{lhs=}\StringTok{\textquotesingle{}price\textquotesingle{}}\NormalTok{, }
                      \AttributeTok{rhs=}\StringTok{\textquotesingle{}carat\textquotesingle{}}\NormalTok{, }
                      \AttributeTok{perm=}\NormalTok{perm\_controls, }
                      \AttributeTok{base=}\NormalTok{base\_controls,}
                      \AttributeTok{run\_from=}\DecValTok{2}\NormalTok{,}
                      \AttributeTok{run\_to=}\DecValTok{3}\NormalTok{)}


\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(grid2 }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{head}\NormalTok{(}\DecValTok{10}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|r|r|l|l}
\hline
Diamond dimensions & Depth & Table width & np\_fe & expr\\
\hline
1 & 0 & 0 & 0 & price\textasciitilde{}carat+x+y+z|0|0|0\\
\hline
1 & 1 & 0 & 0 & price\textasciitilde{}carat+x+y+z+depth|0|0|0\\
\hline
1 & 0 & 1 & 0 & price\textasciitilde{}carat+x+y+z+table|0|0|0\\
\hline
1 & 1 & 1 & 0 & price\textasciitilde{}carat+x+y+z+depth+table|0|0|0\\
\hline
1 & 0 & 0 & 0 & price\textasciitilde{}carat+x+y+z|0|0|0\\
\hline
1 & 1 & 0 & 0 & price\textasciitilde{}carat+x+y+z+depth|0|0|0\\
\hline
1 & 0 & 1 & 0 & price\textasciitilde{}carat+x+y+z+table|0|0|0\\
\hline
1 & 1 & 1 & 0 & price\textasciitilde{}carat+x+y+z+depth+table|0|0|0\\
\hline
1 & 0 & 0 & 0 & price\textasciitilde{}carat+x+y+z|0|0|0\\
\hline
1 & 1 & 0 & 0 & price\textasciitilde{}carat+x+y+z+depth|0|0|0\\
\hline
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Step 3: Estimate models}
\NormalTok{grid3 }\OtherTok{=} \FunctionTok{stability\_plot}\NormalTok{(}\AttributeTok{grid =}\NormalTok{ grid2,}
                      \AttributeTok{data=}\NormalTok{diamonds, }
                      \AttributeTok{lhs=}\StringTok{\textquotesingle{}price\textquotesingle{}}\NormalTok{, }
                      \AttributeTok{rhs=}\StringTok{\textquotesingle{}carat\textquotesingle{}}\NormalTok{, }
                      \AttributeTok{perm=}\NormalTok{perm\_controls, }
                      \AttributeTok{base=}\NormalTok{base\_controls,}
                      \AttributeTok{run\_from=}\DecValTok{3}\NormalTok{,}
                      \AttributeTok{run\_to=}\DecValTok{4}\NormalTok{)}

\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(grid3 }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{head}\NormalTok{(}\DecValTok{10}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|r|r|l|l|r|l|r|r}
\hline
Diamond dimensions & Depth & Table width & np\_fe & expr & coef & p & error\_high & error\_low\\
\hline
1 & 0 & 0 & 0 & price\textasciitilde{}carat+x+y+z|0|0|0 & 10461.86 & p<0.01 & 11031.84 & 9891.876\\
\hline
1 & 1 & 0 & 0 & price\textasciitilde{}carat+x+y+z+depth|0|0|0 & 10808.25 & p<0.01 & 11388.81 & 10227.683\\
\hline
1 & 0 & 1 & 0 & price\textasciitilde{}carat+x+y+z+table|0|0|0 & 10423.42 & p<0.01 & 10992.00 & 9854.849\\
\hline
1 & 1 & 1 & 0 & price\textasciitilde{}carat+x+y+z+depth+table|0|0|0 & 10851.31 & p<0.01 & 11428.58 & 10274.037\\
\hline
1 & 0 & 0 & 0 & price\textasciitilde{}carat+x+y+z|0|0|0 & 10461.86 & p<0.01 & 11031.84 & 9891.876\\
\hline
1 & 1 & 0 & 0 & price\textasciitilde{}carat+x+y+z+depth|0|0|0 & 10808.25 & p<0.01 & 11388.81 & 10227.683\\
\hline
1 & 0 & 1 & 0 & price\textasciitilde{}carat+x+y+z+table|0|0|0 & 10423.42 & p<0.01 & 10992.00 & 9854.849\\
\hline
1 & 1 & 1 & 0 & price\textasciitilde{}carat+x+y+z+depth+table|0|0|0 & 10851.31 & p<0.01 & 11428.58 & 10274.037\\
\hline
1 & 0 & 0 & 0 & price\textasciitilde{}carat+x+y+z|0|0|0 & 10461.86 & p<0.01 & 11031.84 & 9891.876\\
\hline
1 & 1 & 0 & 0 & price\textasciitilde{}carat+x+y+z+depth|0|0|0 & 10808.25 & p<0.01 & 11388.81 & 10227.683\\
\hline
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Step 4: Get dataframe to draw}
\NormalTok{dfs }\OtherTok{=} \FunctionTok{stability\_plot}\NormalTok{(}\AttributeTok{grid =}\NormalTok{ grid3,}
                      \AttributeTok{data=}\NormalTok{diamonds, }
                      \AttributeTok{lhs=}\StringTok{\textquotesingle{}price\textquotesingle{}}\NormalTok{, }
                      \AttributeTok{rhs=}\StringTok{\textquotesingle{}carat\textquotesingle{}}\NormalTok{, }
                      \AttributeTok{perm=}\NormalTok{perm\_controls, }
                      \AttributeTok{base=}\NormalTok{base\_controls,}
                      \AttributeTok{run\_from=}\DecValTok{4}\NormalTok{,}
                      \AttributeTok{run\_to=}\DecValTok{5}\NormalTok{)}

\NormalTok{coef\_grid }\OtherTok{=}\NormalTok{ dfs[[}\DecValTok{1}\NormalTok{]]}
\NormalTok{control\_grid }\OtherTok{=}\NormalTok{ dfs[[}\DecValTok{2}\NormalTok{]]}

\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(coef\_grid }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{head}\NormalTok{(}\DecValTok{10}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|r|r|l|l|r|l|r|r|r}
\hline
Diamond dimensions & Depth & Table width & np\_fe & expr & coef & p & error\_high & error\_low & model\\
\hline
1 & 0 & 0 & 0 & price\textasciitilde{}carat+x+y+z|0|0|0 & 10461.86 & p<0.01 & 11031.84 & 9891.876 & 1\\
\hline
1 & 1 & 0 & 0 & price\textasciitilde{}carat+x+y+z+depth|0|0|0 & 10808.25 & p<0.01 & 11388.81 & 10227.683 & 2\\
\hline
1 & 0 & 1 & 0 & price\textasciitilde{}carat+x+y+z+table|0|0|0 & 10423.42 & p<0.01 & 10992.00 & 9854.849 & 3\\
\hline
1 & 1 & 1 & 0 & price\textasciitilde{}carat+x+y+z+depth+table|0|0|0 & 10851.31 & p<0.01 & 11428.58 & 10274.037 & 4\\
\hline
1 & 0 & 0 & 0 & price\textasciitilde{}carat+x+y+z|0|0|0 & 10461.86 & p<0.01 & 11031.84 & 9891.876 & 5\\
\hline
1 & 1 & 0 & 0 & price\textasciitilde{}carat+x+y+z+depth|0|0|0 & 10808.25 & p<0.01 & 11388.81 & 10227.683 & 6\\
\hline
1 & 0 & 1 & 0 & price\textasciitilde{}carat+x+y+z+table|0|0|0 & 10423.42 & p<0.01 & 10992.00 & 9854.849 & 7\\
\hline
1 & 1 & 1 & 0 & price\textasciitilde{}carat+x+y+z+depth+table|0|0|0 & 10851.31 & p<0.01 & 11428.58 & 10274.037 & 8\\
\hline
1 & 0 & 0 & 0 & price\textasciitilde{}carat+x+y+z|0|0|0 & 10461.86 & p<0.01 & 11031.84 & 9891.876 & 9\\
\hline
1 & 1 & 0 & 0 & price\textasciitilde{}carat+x+y+z+depth|0|0|0 & 10808.25 & p<0.01 & 11388.81 & 10227.683 & 10\\
\hline
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Step 5: plot the sensitivity graph }
\NormalTok{panels }\OtherTok{=} \FunctionTok{stability\_plot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ diamonds, }
                      \AttributeTok{lhs=}\StringTok{\textquotesingle{}price\textquotesingle{}}\NormalTok{, }
                      \AttributeTok{rhs=}\StringTok{\textquotesingle{}carat\textquotesingle{}}\NormalTok{, }
                      \AttributeTok{coef\_grid =}\NormalTok{ coef\_grid,}
                      \AttributeTok{control\_grid =}\NormalTok{ control\_grid,}
                      \AttributeTok{run\_from=}\DecValTok{5}\NormalTok{,}
                      \AttributeTok{run\_to=}\DecValTok{6}\NormalTok{)}

\FunctionTok{stability\_plot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ diamonds,}
               \AttributeTok{lhs=}\StringTok{\textquotesingle{}price\textquotesingle{}}\NormalTok{, }
               \AttributeTok{rhs=}\StringTok{\textquotesingle{}carat\textquotesingle{}}\NormalTok{, }
               \AttributeTok{coef\_panel =}\NormalTok{ panels[[}\DecValTok{1}\NormalTok{]],}
               \AttributeTok{control\_panel =}\NormalTok{ panels[[}\DecValTok{2}\NormalTok{]],}
               \AttributeTok{run\_from =} \DecValTok{6}\NormalTok{,}
               \AttributeTok{run\_to =} \DecValTok{7}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{40-sensitivity-robustness_files/figure-latex/unnamed-chunk-4-1} \end{center}

In step 2, we can modify to use other function (e.g., \texttt{glm})

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diamonds}\SpecialCharTok{$}\NormalTok{above\_med\_price }\OtherTok{=} \FunctionTok{as.numeric}\NormalTok{(diamonds}\SpecialCharTok{$}\NormalTok{price }\SpecialCharTok{\textgreater{}} \FunctionTok{median}\NormalTok{(diamonds}\SpecialCharTok{$}\NormalTok{price))}

\NormalTok{base\_controls }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Diamond dimensions\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}x + y + z\textquotesingle{}}\NormalTok{)}

\NormalTok{perm\_controls }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Depth\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}depth\textquotesingle{}}\NormalTok{,}
                  \StringTok{\textquotesingle{}Table width\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}table\textquotesingle{}}\NormalTok{,}
                  \StringTok{\textquotesingle{}Clarity\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}clarity\textquotesingle{}}\NormalTok{)}
\NormalTok{lhs\_var }\OtherTok{=} \StringTok{\textquotesingle{}above\_med\_price\textquotesingle{}}
\NormalTok{rhs\_var }\OtherTok{=} \StringTok{\textquotesingle{}carat\textquotesingle{}}

\NormalTok{grid1 }\OtherTok{=} \FunctionTok{stability\_plot}\NormalTok{(}
    \AttributeTok{data =}\NormalTok{ diamonds,}
    \AttributeTok{lhs =}\NormalTok{ lhs\_var,}
    \AttributeTok{rhs =}\NormalTok{ rhs\_var,}
    \AttributeTok{perm =}\NormalTok{ perm\_controls,}
    \AttributeTok{base =}\NormalTok{ base\_controls,}
    \AttributeTok{fe\_always =}\NormalTok{ F,}
    \AttributeTok{run\_to =} \DecValTok{2}
\NormalTok{)}

\CommentTok{\# Create control part of formula}
\NormalTok{base\_perm }\OtherTok{=} \FunctionTok{c}\NormalTok{(base\_controls, perm\_controls)}
\NormalTok{grid1}\SpecialCharTok{$}\NormalTok{expr }\OtherTok{=} \FunctionTok{apply}\NormalTok{(grid1[, }\DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(base\_perm)], }\DecValTok{1}\NormalTok{,}
                   \ControlFlowTok{function}\NormalTok{(x)}
                     \FunctionTok{paste}\NormalTok{(base\_perm[}\FunctionTok{names}\NormalTok{(base\_perm)[}\FunctionTok{which}\NormalTok{(x }\SpecialCharTok{==} \DecValTok{1}\NormalTok{)]], }
                           \AttributeTok{collapse =} \StringTok{\textquotesingle{}+\textquotesingle{}}\NormalTok{))}

\CommentTok{\# Complete formula with LHS and RHS variables}
\NormalTok{grid1}\SpecialCharTok{$}\NormalTok{expr }\OtherTok{=} \FunctionTok{paste}\NormalTok{(lhs\_var, }\StringTok{\textquotesingle{}\textasciitilde{}\textquotesingle{}}\NormalTok{, rhs\_var, }\StringTok{\textquotesingle{}+\textquotesingle{}}\NormalTok{, grid1}\SpecialCharTok{$}\NormalTok{expr, }\AttributeTok{sep =} \StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{)}

\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(grid1 }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{head}\NormalTok{(}\DecValTok{10}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|r|r|r|l|l}
\hline
Diamond dimensions & Depth & Table width & Clarity & np\_fe & expr\\
\hline
1 & 0 & 0 & 0 &  & above\_med\_price\textasciitilde{}carat+x + y + z\\
\hline
1 & 1 & 0 & 0 &  & above\_med\_price\textasciitilde{}carat+x + y + z+depth\\
\hline
1 & 0 & 1 & 0 &  & above\_med\_price\textasciitilde{}carat+x + y + z+table\\
\hline
1 & 1 & 1 & 0 &  & above\_med\_price\textasciitilde{}carat+x + y + z+depth+table\\
\hline
1 & 0 & 0 & 1 &  & above\_med\_price\textasciitilde{}carat+x + y + z+clarity\\
\hline
1 & 1 & 0 & 1 &  & above\_med\_price\textasciitilde{}carat+x + y + z+depth+clarity\\
\hline
1 & 0 & 1 & 1 &  & above\_med\_price\textasciitilde{}carat+x + y + z+table+clarity\\
\hline
1 & 1 & 1 & 1 &  & above\_med\_price\textasciitilde{}carat+x + y + z+depth+table+clarity\\
\hline
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# customer function for the logit model}
\NormalTok{starb\_logit }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(spec, data, rhs, ...) \{}
\NormalTok{  spec }\OtherTok{=} \FunctionTok{as.formula}\NormalTok{(spec)}
\NormalTok{  model }\OtherTok{=} \FunctionTok{glm}\NormalTok{(spec, }\AttributeTok{data=}\NormalTok{data, }\AttributeTok{family=}\StringTok{\textquotesingle{}binomial\textquotesingle{}}\NormalTok{, }\AttributeTok{weights=}\NormalTok{data}\SpecialCharTok{$}\NormalTok{weight) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    broom}\SpecialCharTok{::}\FunctionTok{tidy}\NormalTok{()}
\NormalTok{  row }\OtherTok{=} \FunctionTok{which}\NormalTok{(model}\SpecialCharTok{$}\NormalTok{term}\SpecialCharTok{==}\NormalTok{rhs)}
\NormalTok{  coef }\OtherTok{=}\NormalTok{ model[row, }\StringTok{\textquotesingle{}estimate\textquotesingle{}}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as.numeric}\NormalTok{()}
\NormalTok{  se   }\OtherTok{=}\NormalTok{ model[row, }\StringTok{\textquotesingle{}std.error\textquotesingle{}}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as.numeric}\NormalTok{()}
\NormalTok{  p    }\OtherTok{=}\NormalTok{ model[row, }\StringTok{\textquotesingle{}p.value\textquotesingle{}}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as.numeric}\NormalTok{()}

  \FunctionTok{return}\NormalTok{(}\FunctionTok{c}\NormalTok{(coef, p, coef}\FloatTok{+1.96}\SpecialCharTok{*}\NormalTok{se, coef}\FloatTok{{-}1.96}\SpecialCharTok{*}\NormalTok{se))}
\NormalTok{\}}

\FunctionTok{stability\_plot}\NormalTok{(}\AttributeTok{grid =}\NormalTok{ grid1,}
               \AttributeTok{data =}\NormalTok{ diamonds, }
               \AttributeTok{lhs =}\NormalTok{ lhs\_var, }
               \AttributeTok{rhs =}\NormalTok{ rhs\_var,}
               \AttributeTok{model =}\NormalTok{ starb\_logit,}
               \AttributeTok{perm =}\NormalTok{ perm\_controls,}
               \AttributeTok{base =}\NormalTok{ base\_controls,}
               \AttributeTok{fe\_always =}\NormalTok{ F,}
               \AttributeTok{run\_from=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{40-sensitivity-robustness_files/figure-latex/unnamed-chunk-5-1} \end{center}

For getting other specification (e.g., different CI)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(margins)}
\NormalTok{starb\_logit\_enhanced }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(spec, data, rhs, ...) \{}
  \CommentTok{\# Unpack ...}
\NormalTok{  l }\OtherTok{=} \FunctionTok{list}\NormalTok{(...)}
\NormalTok{  get\_mfx }\OtherTok{=} \FunctionTok{ifelse}\NormalTok{(}\FunctionTok{is.null}\NormalTok{(l}\SpecialCharTok{$}\NormalTok{get\_mfx), F, T) }\CommentTok{\# Set a default to F}
  
\NormalTok{  spec }\OtherTok{=} \FunctionTok{as.formula}\NormalTok{(spec)}
  \ControlFlowTok{if}\NormalTok{ (get\_mfx) \{}
\NormalTok{    model }\OtherTok{=} \FunctionTok{glm}\NormalTok{(spec, }\AttributeTok{data=}\NormalTok{data, }\AttributeTok{family=}\StringTok{\textquotesingle{}binomial\textquotesingle{}}\NormalTok{, }\AttributeTok{weights=}\NormalTok{data}\SpecialCharTok{$}\NormalTok{weight) }\SpecialCharTok{\%\textgreater{}\%}
      \FunctionTok{margins}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{      summary}
\NormalTok{    row }\OtherTok{=} \FunctionTok{which}\NormalTok{(model}\SpecialCharTok{$}\NormalTok{factor}\SpecialCharTok{==}\NormalTok{rhs)}
\NormalTok{    coef }\OtherTok{=}\NormalTok{ model[row, }\StringTok{\textquotesingle{}AME\textquotesingle{}}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as.numeric}\NormalTok{()}
\NormalTok{    se   }\OtherTok{=}\NormalTok{ model[row, }\StringTok{\textquotesingle{}SE\textquotesingle{}}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as.numeric}\NormalTok{()}
\NormalTok{    p    }\OtherTok{=}\NormalTok{ model[row, }\StringTok{\textquotesingle{}p\textquotesingle{}}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as.numeric}\NormalTok{()}
\NormalTok{  \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{    model }\OtherTok{=} \FunctionTok{glm}\NormalTok{(spec, }\AttributeTok{data=}\NormalTok{data, }\AttributeTok{family=}\StringTok{\textquotesingle{}binomial\textquotesingle{}}\NormalTok{, }\AttributeTok{weights=}\NormalTok{data}\SpecialCharTok{$}\NormalTok{weight) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{      broom}\SpecialCharTok{::}\FunctionTok{tidy}\NormalTok{()}
\NormalTok{    row }\OtherTok{=} \FunctionTok{which}\NormalTok{(model}\SpecialCharTok{$}\NormalTok{term}\SpecialCharTok{==}\NormalTok{rhs)}
\NormalTok{    coef }\OtherTok{=}\NormalTok{ model[row, }\StringTok{\textquotesingle{}estimate\textquotesingle{}}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as.numeric}\NormalTok{()}
\NormalTok{    se   }\OtherTok{=}\NormalTok{ model[row, }\StringTok{\textquotesingle{}std.error\textquotesingle{}}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as.numeric}\NormalTok{()}
\NormalTok{    p    }\OtherTok{=}\NormalTok{ model[row, }\StringTok{\textquotesingle{}p.value\textquotesingle{}}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as.numeric}\NormalTok{()}
\NormalTok{  \}}

\NormalTok{  z }\OtherTok{=} \FunctionTok{qnorm}\NormalTok{(}\FloatTok{0.995}\NormalTok{)}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{c}\NormalTok{(coef, p, coef}\SpecialCharTok{+}\NormalTok{z}\SpecialCharTok{*}\NormalTok{se, coef}\SpecialCharTok{{-}}\NormalTok{z}\SpecialCharTok{*}\NormalTok{se))}
\NormalTok{\}}

\FunctionTok{stability\_plot}\NormalTok{(}\AttributeTok{grid =}\NormalTok{ grid1,}
               \AttributeTok{data =}\NormalTok{ diamonds, }
               \AttributeTok{lhs =}\NormalTok{ lhs\_var, }
               \AttributeTok{rhs =}\NormalTok{ rhs\_var,}
               \AttributeTok{model =}\NormalTok{ starb\_logit\_enhanced,}
               \AttributeTok{get\_mfx =}\NormalTok{ T,}
               \AttributeTok{perm =}\NormalTok{ perm\_controls,}
               \AttributeTok{base =}\NormalTok{ base\_controls,}
               \AttributeTok{fe\_always =}\NormalTok{ F,}
               \AttributeTok{run\_from =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{40-sensitivity-robustness_files/figure-latex/unnamed-chunk-6-1} \end{center}

To get your customized plot

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dfs }\OtherTok{=} \FunctionTok{stability\_plot}\NormalTok{(}\AttributeTok{grid =}\NormalTok{ grid1,}
               \AttributeTok{data =}\NormalTok{ diamonds, }
               \AttributeTok{lhs =}\NormalTok{ lhs\_var, }
               \AttributeTok{rhs =}\NormalTok{ rhs\_var,}
               \AttributeTok{model =}\NormalTok{ starb\_logit\_enhanced,}
               \AttributeTok{get\_mfx =}\NormalTok{ T,}
               \AttributeTok{perm =}\NormalTok{ perm\_controls,}
               \AttributeTok{base =}\NormalTok{ base\_controls,}
               \AttributeTok{fe\_always =}\NormalTok{ F,}
               \AttributeTok{run\_from =} \DecValTok{3}\NormalTok{,}
               \AttributeTok{run\_to =} \DecValTok{5}\NormalTok{)}

\NormalTok{coef\_grid\_logit }\OtherTok{=}\NormalTok{ dfs[[}\DecValTok{1}\NormalTok{]]}
\NormalTok{control\_grid\_logit }\OtherTok{=}\NormalTok{ dfs[[}\DecValTok{2}\NormalTok{]]}

\NormalTok{min\_space }\OtherTok{=} \FloatTok{0.5}

\NormalTok{coef\_plot }\OtherTok{=}\NormalTok{ ggplot2}\SpecialCharTok{::}\FunctionTok{ggplot}\NormalTok{(coef\_grid\_logit, }\FunctionTok{aes}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ model,}
  \AttributeTok{y =}\NormalTok{ coef,}
  \AttributeTok{shape =}\NormalTok{ p,}
  \AttributeTok{group =}\NormalTok{ p}
\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{geom\_linerange}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{ymin =}\NormalTok{ error\_low, }\AttributeTok{ymax =}\NormalTok{ error\_high), }\AttributeTok{alpha =} \FloatTok{0.75}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{size =} \DecValTok{5}\NormalTok{, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{col =}\NormalTok{ p, }\AttributeTok{fill =}\NormalTok{ p), }\AttributeTok{alpha =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
\NormalTok{  viridis}\SpecialCharTok{::}\FunctionTok{scale\_color\_viridis}\NormalTok{(}\AttributeTok{discrete =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{option =} \StringTok{"D"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_shape\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\DecValTok{15}\NormalTok{, }\DecValTok{17}\NormalTok{, }\DecValTok{18}\NormalTok{, }\DecValTok{19}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{, }\AttributeTok{linetype =} \StringTok{\textquotesingle{}dotted\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{\textquotesingle{}A custom coefficient stability plot!\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{subtitle =} \StringTok{"Error bars represent 99\% confidence intervals"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}
    \AttributeTok{axis.text.x =} \FunctionTok{element\_blank}\NormalTok{(),}
    \AttributeTok{axis.title =} \FunctionTok{element\_blank}\NormalTok{(),}
    \AttributeTok{axis.ticks.x =} \FunctionTok{element\_blank}\NormalTok{()}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{coord\_cartesian}\NormalTok{(}\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ min\_space, }\FunctionTok{max}\NormalTok{(coef\_grid\_logit}\SpecialCharTok{$}\NormalTok{model) }\SpecialCharTok{+}\NormalTok{ min\_space),}
                  \AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.1}\NormalTok{, }\FloatTok{1.6}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{guides}\NormalTok{(}\AttributeTok{fill =}\NormalTok{ F, }\AttributeTok{shape =}\NormalTok{ F, }\AttributeTok{col =}\NormalTok{ F)}


\NormalTok{control\_plot }\OtherTok{=} \FunctionTok{ggplot}\NormalTok{(control\_grid\_logit) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ model, }\AttributeTok{y =}\NormalTok{ y, }\AttributeTok{fill=}\NormalTok{value), }\AttributeTok{shape=}\DecValTok{23}\NormalTok{, }\AttributeTok{size=}\DecValTok{4}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_manual}\NormalTok{(}\AttributeTok{values=}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}\#FFFFFF\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}\#000000\textquotesingle{}}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{guides}\NormalTok{(}\AttributeTok{fill=}\NormalTok{F) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{breaks =} \FunctionTok{unique}\NormalTok{(control\_grid\_logit}\SpecialCharTok{$}\NormalTok{y), }
                     \AttributeTok{labels =} \FunctionTok{unique}\NormalTok{(control\_grid\_logit}\SpecialCharTok{$}\NormalTok{key),}
                     \AttributeTok{limits=}\FunctionTok{c}\NormalTok{(}\FunctionTok{min}\NormalTok{(control\_grid\_logit}\SpecialCharTok{$}\NormalTok{y)}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\FunctionTok{max}\NormalTok{(control\_grid\_logit}\SpecialCharTok{$}\NormalTok{y)}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{breaks=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{max}\NormalTok{(control\_grid\_logit}\SpecialCharTok{$}\NormalTok{model))) }\SpecialCharTok{+}
  \FunctionTok{coord\_cartesian}\NormalTok{(}\AttributeTok{xlim=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{min\_space, }\FunctionTok{max}\NormalTok{(control\_grid\_logit}\SpecialCharTok{$}\NormalTok{model)}\SpecialCharTok{+}\NormalTok{min\_space)) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{panel.grid.major.y =} \FunctionTok{element\_blank}\NormalTok{(),}
        \AttributeTok{panel.grid.minor.y =} \FunctionTok{element\_blank}\NormalTok{(),}
        \AttributeTok{axis.title =} \FunctionTok{element\_blank}\NormalTok{(),}
        \AttributeTok{axis.text.y =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size=}\DecValTok{10}\NormalTok{),}
        \AttributeTok{axis.ticks =} \FunctionTok{element\_blank}\NormalTok{(),}
        \AttributeTok{axis.line =} \FunctionTok{element\_blank}\NormalTok{()) }

\NormalTok{cowplot}\SpecialCharTok{::}\FunctionTok{plot\_grid}\NormalTok{(coef\_plot, control\_plot, }\AttributeTok{rel\_heights=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\FloatTok{0.5}\NormalTok{), }
                   \AttributeTok{align=}\StringTok{\textquotesingle{}v\textquotesingle{}}\NormalTok{, }\AttributeTok{ncol=}\DecValTok{1}\NormalTok{, }\AttributeTok{axis=}\StringTok{\textquotesingle{}b\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{40-sensitivity-robustness_files/figure-latex/unnamed-chunk-7-1} \end{center}

To get different model specification (e.g., probit vs.~logit)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{starb\_probit }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(spec, data, rhs, ...) \{}
    \CommentTok{\# Unpack ...}
\NormalTok{    l }\OtherTok{=} \FunctionTok{list}\NormalTok{(...)}
\NormalTok{    get\_mfx }\OtherTok{=} \FunctionTok{ifelse}\NormalTok{(}\FunctionTok{is.null}\NormalTok{(l}\SpecialCharTok{$}\NormalTok{get\_mfx), F, T) }\CommentTok{\# Set a default to F}
    
\NormalTok{    spec }\OtherTok{=} \FunctionTok{as.formula}\NormalTok{(spec)}
    \ControlFlowTok{if}\NormalTok{ (get\_mfx) \{}
\NormalTok{        model }\OtherTok{=} \FunctionTok{glm}\NormalTok{(}
\NormalTok{            spec,}
            \AttributeTok{data =}\NormalTok{ data,}
            \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{\textquotesingle{}probit\textquotesingle{}}\NormalTok{),}
            \AttributeTok{weights =}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{weight}
\NormalTok{        ) }\SpecialCharTok{\%\textgreater{}\%}
            \FunctionTok{margins}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{            summary}
\NormalTok{        row }\OtherTok{=} \FunctionTok{which}\NormalTok{(model}\SpecialCharTok{$}\NormalTok{factor }\SpecialCharTok{==}\NormalTok{ rhs)}
\NormalTok{        coef }\OtherTok{=}\NormalTok{ model[row, }\StringTok{\textquotesingle{}AME\textquotesingle{}}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as.numeric}\NormalTok{()}
\NormalTok{        se   }\OtherTok{=}\NormalTok{ model[row, }\StringTok{\textquotesingle{}SE\textquotesingle{}}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as.numeric}\NormalTok{()}
\NormalTok{        p    }\OtherTok{=}\NormalTok{ model[row, }\StringTok{\textquotesingle{}p\textquotesingle{}}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as.numeric}\NormalTok{()}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{        model }\OtherTok{=} \FunctionTok{glm}\NormalTok{(}
\NormalTok{            spec,}
            \AttributeTok{data =}\NormalTok{ data,}
            \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{\textquotesingle{}probit\textquotesingle{}}\NormalTok{),}
            \AttributeTok{weights =}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{weight}
\NormalTok{        ) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{            broom}\SpecialCharTok{::}\FunctionTok{tidy}\NormalTok{()}
\NormalTok{        row }\OtherTok{=} \FunctionTok{which}\NormalTok{(model}\SpecialCharTok{$}\NormalTok{term }\SpecialCharTok{==}\NormalTok{ rhs)}
\NormalTok{        coef }\OtherTok{=}\NormalTok{ model[row, }\StringTok{\textquotesingle{}estimate\textquotesingle{}}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as.numeric}\NormalTok{()}
\NormalTok{        se   }\OtherTok{=}\NormalTok{ model[row, }\StringTok{\textquotesingle{}std.error\textquotesingle{}}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as.numeric}\NormalTok{()}
\NormalTok{        p    }\OtherTok{=}\NormalTok{ model[row, }\StringTok{\textquotesingle{}p.value\textquotesingle{}}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as.numeric}\NormalTok{()}
\NormalTok{    \}}
    
\NormalTok{    z }\OtherTok{=} \FunctionTok{qnorm}\NormalTok{(}\FloatTok{0.995}\NormalTok{)}
    \FunctionTok{return}\NormalTok{(}\FunctionTok{c}\NormalTok{(coef, p, coef }\SpecialCharTok{+}\NormalTok{ z }\SpecialCharTok{*}\NormalTok{ se, coef }\SpecialCharTok{{-}}\NormalTok{ z }\SpecialCharTok{*}\NormalTok{ se))}
\NormalTok{\}}

\NormalTok{probit\_dfs }\OtherTok{=} \FunctionTok{stability\_plot}\NormalTok{(}
    \AttributeTok{grid =}\NormalTok{ grid1,}
    \AttributeTok{data =}\NormalTok{ diamonds,}
    \AttributeTok{lhs =}\NormalTok{ lhs\_var,}
    \AttributeTok{rhs =}\NormalTok{ rhs\_var,}
    \AttributeTok{model =}\NormalTok{ starb\_probit,}
    \AttributeTok{get\_mfx =}\NormalTok{ T,}
    \AttributeTok{perm =}\NormalTok{ perm\_controls,}
    \AttributeTok{base =}\NormalTok{ base\_controls,}
    \AttributeTok{fe\_always =}\NormalTok{ F,}
    \AttributeTok{run\_from =} \DecValTok{3}\NormalTok{,}
    \AttributeTok{run\_to =} \DecValTok{5}
\NormalTok{)}

\CommentTok{\# We\textquotesingle{}ll put the probit DFs on the left, }
 \CommentTok{\#so we need to adjust the model numbers accordingly}
\CommentTok{\# so the probit and logit DFs don\textquotesingle{}t plot on top of one another!}
\NormalTok{coef\_grid\_probit }\OtherTok{=}\NormalTok{ probit\_dfs[[}\DecValTok{1}\NormalTok{]] }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{model =}\NormalTok{ model }\SpecialCharTok{+} \FunctionTok{max}\NormalTok{(coef\_grid\_logit}\SpecialCharTok{$}\NormalTok{model))}

\NormalTok{control\_grid\_probit }\OtherTok{=}\NormalTok{ probit\_dfs[[}\DecValTok{2}\NormalTok{]] }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{model =}\NormalTok{ model }\SpecialCharTok{+} \FunctionTok{max}\NormalTok{(control\_grid\_logit}\SpecialCharTok{$}\NormalTok{model))}

\NormalTok{coef\_grid    }\OtherTok{=} \FunctionTok{bind\_rows}\NormalTok{(coef\_grid\_logit, coef\_grid\_probit)}
\NormalTok{control\_grid }\OtherTok{=} \FunctionTok{bind\_rows}\NormalTok{(control\_grid\_logit, control\_grid\_probit)}

\NormalTok{panels }\OtherTok{=} \FunctionTok{stability\_plot}\NormalTok{(}
    \AttributeTok{coef\_grid =}\NormalTok{ coef\_grid,}
    \AttributeTok{control\_grid =}\NormalTok{ control\_grid,}
    \AttributeTok{data =}\NormalTok{ diamonds,}
    \AttributeTok{lhs =}\NormalTok{ lhs\_var,}
    \AttributeTok{rhs =}\NormalTok{ rhs\_var,}
    \AttributeTok{perm =}\NormalTok{ perm\_controls,}
    \AttributeTok{base =}\NormalTok{ base\_controls,}
    \AttributeTok{fe\_always =}\NormalTok{ F,}
    \AttributeTok{run\_from =} \DecValTok{5}\NormalTok{,}
    \AttributeTok{run\_to =} \DecValTok{6}
\NormalTok{)}

\NormalTok{coef\_plot }\OtherTok{=}\NormalTok{ panels[[}\DecValTok{1}\NormalTok{]] }\SpecialCharTok{+} \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \FloatTok{8.5}\NormalTok{,}
                                     \AttributeTok{linetype =} \StringTok{\textquotesingle{}dashed\textquotesingle{}}\NormalTok{,}
                                     \AttributeTok{alpha =} \FloatTok{0.8}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{annotate}\NormalTok{(}
        \AttributeTok{geom =} \StringTok{\textquotesingle{}label\textquotesingle{}}\NormalTok{,}
        \AttributeTok{x =} \FloatTok{4.25}\NormalTok{,}
        \AttributeTok{y =} \FloatTok{1.8}\NormalTok{,}
        \AttributeTok{label =} \StringTok{\textquotesingle{}Logit models\textquotesingle{}}\NormalTok{,}
        \AttributeTok{size =} \DecValTok{6}\NormalTok{,}
        \AttributeTok{fill =} \StringTok{\textquotesingle{}\#D3D3D3\textquotesingle{}}\NormalTok{,}
        \AttributeTok{alpha =} \FloatTok{0.7}
\NormalTok{    ) }\SpecialCharTok{+}
    \FunctionTok{annotate}\NormalTok{(}
        \AttributeTok{geom =} \StringTok{\textquotesingle{}label\textquotesingle{}}\NormalTok{,}
        \AttributeTok{x =} \FloatTok{12.75}\NormalTok{,}
        \AttributeTok{y =} \FloatTok{1.8}\NormalTok{,}
        \AttributeTok{label =} \StringTok{\textquotesingle{}Probit models\textquotesingle{}}\NormalTok{,}
        \AttributeTok{size =} \DecValTok{6}\NormalTok{,}
        \AttributeTok{fill =} \StringTok{\textquotesingle{}\#D3D3D3\textquotesingle{}}\NormalTok{,}
        \AttributeTok{alpha =} \FloatTok{0.7}
\NormalTok{    ) }\SpecialCharTok{+}
    \FunctionTok{coord\_cartesian}\NormalTok{(}\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.5}\NormalTok{, }\FloatTok{1.9}\NormalTok{))}

\NormalTok{control\_plot }\OtherTok{=}\NormalTok{ panels[[}\DecValTok{2}\NormalTok{]] }\SpecialCharTok{+} \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \FloatTok{8.5}\NormalTok{,}
                                        \AttributeTok{linetype =} \StringTok{\textquotesingle{}dashed\textquotesingle{}}\NormalTok{,}
                                        \AttributeTok{alpha =} \FloatTok{0.8}\NormalTok{)}

\NormalTok{cowplot}\SpecialCharTok{::}\FunctionTok{plot\_grid}\NormalTok{(}
\NormalTok{    coef\_plot,}
\NormalTok{    control\_plot,}
    \AttributeTok{rel\_heights =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{),}
    \AttributeTok{align =} \StringTok{\textquotesingle{}v\textquotesingle{}}\NormalTok{,}
    \AttributeTok{ncol =} \DecValTok{1}\NormalTok{,}
    \AttributeTok{axis =} \StringTok{\textquotesingle{}b\textquotesingle{}}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{40-sensitivity-robustness_files/figure-latex/unnamed-chunk-8-1} \end{center}

\hypertarget{rdfanalysis}{%
\subsection{rdfanalysis}\label{rdfanalysis}}

\begin{itemize}
\tightlist
\item
  Not recommend
\end{itemize}

Installation

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{devtools}\SpecialCharTok{::}\FunctionTok{install\_github}\NormalTok{(}\StringTok{"joachim{-}gassen/rdfanalysis"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Example by the \href{https://joachim-gassen.github.io/rdfanalysis/}{package's author}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rdfanalysis)}
\FunctionTok{load}\NormalTok{(}\FunctionTok{url}\NormalTok{(}\StringTok{"https://joachim{-}gassen.github.io/data/rdf\_ests.RData"}\NormalTok{))}
\FunctionTok{plot\_rdf\_spec\_curve}\NormalTok{(ests, }\StringTok{"est"}\NormalTok{, }\StringTok{"lb"}\NormalTok{, }\StringTok{"ub"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{40-sensitivity-robustness_files/figure-latex/unnamed-chunk-10-1} \end{center}

Shiny app for readers to explore

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{design }\OtherTok{\textless{}{-}} \FunctionTok{define\_design}\NormalTok{(}\AttributeTok{steps =} \FunctionTok{c}\NormalTok{(}\StringTok{"read\_data"}\NormalTok{,}
                                  \StringTok{"select\_idvs"}\NormalTok{,}
                                  \StringTok{"treat\_extreme\_obs"}\NormalTok{,}
                                  \StringTok{"specify\_model"}\NormalTok{,}
                                  \StringTok{"est\_model"}\NormalTok{),}
                        \AttributeTok{rel\_dir =} \StringTok{"vignettes/case\_study\_code"}\NormalTok{)}

\FunctionTok{shiny\_rdf\_spec\_curve}\NormalTok{(ests, }\FunctionTok{list}\NormalTok{(}\StringTok{"est"}\NormalTok{, }\StringTok{"lb"}\NormalTok{, }\StringTok{"ub"}\NormalTok{),}
\NormalTok{                     design, }\StringTok{"vignettes/case\_study\_code"}\NormalTok{,}
                     \StringTok{"https://joachim{-}gassen.github.io/data/wb\_new.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{coefficient-stability}{%
\section{Coefficient stability}\label{coefficient-stability}}

\citep{oster2019unobservable}

\begin{itemize}
\item
  Coefficient stability can be evident against omitted variable bias.
\item
  But coefficient stability alone can be misleading, but combing with \(R^2\) movement, it can become informative.
\end{itemize}

Packages

\begin{itemize}
\item
  \texttt{mplot}: graphical Model stability and Variable Selection
\item
  \texttt{robomit}: Robustness checks for omitted variable bias (implementation of
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(robomit)}

\CommentTok{\# estimate beta }
\FunctionTok{o\_beta}\NormalTok{(}
  \AttributeTok{y     =} \StringTok{"mpg"}\NormalTok{,       }\CommentTok{\# dependent variable}
  \AttributeTok{x     =} \StringTok{"wt"}\NormalTok{,        }\CommentTok{\# independent treatment variable}
  \AttributeTok{con   =} \StringTok{"hp + qsec"}\NormalTok{, }\CommentTok{\# related control variables}
  \AttributeTok{delta =} \DecValTok{1}\NormalTok{,           }\CommentTok{\# delta}
  \AttributeTok{R2max =} \FloatTok{0.9}\NormalTok{,         }\CommentTok{\# maximum R{-}square}
  \AttributeTok{type  =} \StringTok{"lm"}\NormalTok{,        }\CommentTok{\# model type}
  \AttributeTok{data  =}\NormalTok{ mtcars       }\CommentTok{\# dataset}
\NormalTok{) }
\CommentTok{\#\textgreater{} \# A tibble: 10 x 2}
\CommentTok{\#\textgreater{}    Name                           Value}
\CommentTok{\#\textgreater{}    \textless{}chr\textgreater{}                          \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1 beta*                         {-}2.00 }
\CommentTok{\#\textgreater{}  2 (beta*{-}beta controlled)\^{}2      5.56 }
\CommentTok{\#\textgreater{}  3 Alternative Solution 1        {-}7.01 }
\CommentTok{\#\textgreater{}  4 (beta[AS1]{-}beta controlled)\^{}2  7.05 }
\CommentTok{\#\textgreater{}  5 Uncontrolled Coefficient      {-}5.34 }
\CommentTok{\#\textgreater{}  6 Controlled Coefficient        {-}4.36 }
\CommentTok{\#\textgreater{}  7 Uncontrolled R{-}square          0.753}
\CommentTok{\#\textgreater{}  8 Controlled R{-}square            0.835}
\CommentTok{\#\textgreater{}  9 Max R{-}square                   0.9  }
\CommentTok{\#\textgreater{} 10 delta                          1}
\end{Highlighting}
\end{Shaded}

\hypertarget{omitted-variable-bias-quantification}{%
\section{Omitted Variable Bias Quantification}\label{omitted-variable-bias-quantification}}

To quantify the bias needed to change the substantive conclusion from a causal inference study.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(konfound)}
\FunctionTok{pkonfound}\NormalTok{(}
    \AttributeTok{est\_eff =} \DecValTok{5}\NormalTok{, }
    \AttributeTok{std\_err =} \DecValTok{2}\NormalTok{, }
    \AttributeTok{n\_obs =} \DecValTok{1000}\NormalTok{, }
    \AttributeTok{n\_covariates =} \DecValTok{5}
\NormalTok{)}
\CommentTok{\#\textgreater{} Robustness of Inference to Replacement (RIR):}
\CommentTok{\#\textgreater{} To invalidate an inference,  21.506 \% of the estimate would have to be due to bias. }
\CommentTok{\#\textgreater{} This is based on a threshold of 3.925 for statistical significance (alpha = 0.05).}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} To invalidate an inference,  215  observations would have to be replaced with cases}
\CommentTok{\#\textgreater{} for which the effect is 0 (RIR = 215).}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} See Frank et al. (2013) for a description of the method.}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Citation: Frank, K.A., Maroulis, S., Duong, M., and Kelcey, B. (2013).}
\CommentTok{\#\textgreater{} What would it take to change an inference?}
\CommentTok{\#\textgreater{} Using Rubin\textquotesingle{}s causal model to interpret the }
\CommentTok{\#\textgreater{}         robustness of causal inferences.}
\CommentTok{\#\textgreater{} Education, Evaluation and }
\CommentTok{\#\textgreater{}                        Policy Analysis, 35 437{-}460.}

\FunctionTok{pkonfound}\NormalTok{(}
    \AttributeTok{est\_eff =} \DecValTok{5}\NormalTok{, }
    \AttributeTok{std\_err =} \DecValTok{2}\NormalTok{, }
    \AttributeTok{n\_obs =} \DecValTok{1000}\NormalTok{, }
    \AttributeTok{n\_covariates =} \DecValTok{5}\NormalTok{, }
    \AttributeTok{to\_return =} \StringTok{"thresh\_plot"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{40-sensitivity-robustness_files/figure-latex/unnamed-chunk-13-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\FunctionTok{pkonfound}\NormalTok{(}
    \AttributeTok{est\_eff =} \DecValTok{5}\NormalTok{, }
    \AttributeTok{std\_err =} \DecValTok{2}\NormalTok{, }
    \AttributeTok{n\_obs =} \DecValTok{1000}\NormalTok{, }
    \AttributeTok{n\_covariates =} \DecValTok{5}\NormalTok{, }
    \AttributeTok{to\_return =} \StringTok{"corr\_plot"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{40-sensitivity-robustness_files/figure-latex/unnamed-chunk-13-2} \end{center}

\hypertarget{replication-and-synthetic-data}{%
\chapter{Replication and Synthetic Data}\label{replication-and-synthetic-data}}

Access to comprehensive data is pivotal for replication, especially in the realm of social sciences. Yet, often the data are inaccessible, making replication a challenge \citep{king1995replication}. This chapter dives into the nuances of replication, the exceptions to its norms, and the significance of synthetic data.

\hypertarget{the-replication-standard}{%
\section{The Replication Standard}\label{the-replication-standard}}

Replicability in research ensures:

\begin{itemize}
\tightlist
\item
  Credibility and comprehension of empirical studies.
\item
  Continuity and progression in the discipline.
\item
  Enhanced readership and academic citations.
\end{itemize}

For a research to be replicable, the ``replication standard'' is vital: it entails providing all requisite information for replication by third parties. While quantitative research can, to some extent, offer clear data, qualitative studies pose complexities due to data depth.

\hypertarget{solutions-for-empirical-replication}{%
\subsection{Solutions for Empirical Replication}\label{solutions-for-empirical-replication}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Role of Individual Authors}:

  \begin{itemize}
  \tightlist
  \item
    Authors need to vouch for the replication standard for enhancing their work's credibility.
  \item
    Archives like the Inter-University Consortium for Political and Social Research (ICPSR) serve as depositories for replication datasets.
  \end{itemize}
\item
  \textbf{Creation of a Replication Data Set}:

  \begin{itemize}
  \tightlist
  \item
    A public data set, consisting of both original and relevant complementary data, can serve replication purposes.
  \end{itemize}
\item
  \textbf{Professional Data Archives}:

  \begin{itemize}
  \tightlist
  \item
    Organizations like ICPSR provide solutions to data storage and accessibility problems.
  \end{itemize}
\item
  \textbf{Educational Implications}:

  \begin{itemize}
  \tightlist
  \item
    Replication can be an excellent educational tool, and many programs now emphasize its importance.
  \end{itemize}
\end{enumerate}

\hypertarget{free-data-repositories}{%
\subsection{Free Data Repositories}\label{free-data-repositories}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Zenodo}: Hosted by CERN, it provides a place for researchers to deposit datasets. It's not subject-specific, so it caters to various disciplines.
\item
  \textbf{figshare}: Allows researchers to upload, share, and cite their datasets.
\item
  \textbf{Dryad}: Primarily for datasets associated with published articles in the biological and medical sciences.
\item
  \textbf{OpenICPSR}: A public-facing version of the Inter-University Consortium for Political and Social Research (ICPSR) where researchers can deposit data without any cost.
\item
  \textbf{Harvard Dataverse}: Hosted by Harvard University, this is an open-source repository software application dedicated to archiving, sharing, and citing research data.
\item
  \textbf{Mendeley Data}: A multidisciplinary, free-to-use open access data repository where researchers can upload and share their datasets.
\item
  \textbf{Open Science Framework (OSF)}: Offers both a platform for conducting research and a place to deposit datasets.
\item
  \textbf{PubMed Central}: Specific to life sciences, but it's an open repository for journal articles, preprints, and datasets.
\item
  \textbf{Registry of Research Data Repositories (re3data)}: While not a repository itself, it provides a global registry of research data repositories from various academic disciplines.
\item
  \textbf{SocArXiv}: An open archive for the social sciences.
\item
  \textbf{EarthArXiv}: A preprints archive for earth science.
\item
  \textbf{Protein Data Bank (PDB)}: For 3D structures of large biological molecules.
\item
  \textbf{Gene Expression Omnibus (GEO)}: A public functional genomics data repository.
\item
  \textbf{The Language Archive (TLA)}: Dedicated to data on languages worldwide, especially endangered languages.
\item
  \textbf{B2SHARE}: A platform for storing and sharing research data sets in various disciplines, especially from European research projects.
\end{enumerate}

\hypertarget{exceptions-to-replication}{%
\subsection{Exceptions to Replication}\label{exceptions-to-replication}}

Some exceptions to the replication standard are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Confidentiality}: Sometimes data, even fragmented, is too sensitive to share.
\item
  \textbf{Proprietary Data}: Data sets owned by entities might restrict dissemination, but usually, parts of such data can still be shared.
\item
  \textbf{Rights of First Publication}: Embargos might be set, but the essential data used in a study should be accessible.
\end{enumerate}

\hypertarget{synthetic-data-an-overview}{%
\section{Synthetic Data: An Overview}\label{synthetic-data-an-overview}}

Synthetic data, modeling real data while ensuring anonymity, is becoming pivotal in research. While promising, it has its own complexities and should be approached with caution.

\hypertarget{benefits}{%
\subsection{Benefits}\label{benefits}}

\begin{itemize}
\tightlist
\item
  Privacy preservation.
\item
  Data fairness and augmentation.
\item
  Acceleration in research.
\end{itemize}

\hypertarget{concerns}{%
\subsection{Concerns}\label{concerns}}

\begin{itemize}
\tightlist
\item
  Misconceptions about inherent privacy.
\item
  Challenges with data outliers.
\item
  Models relying solely on synthetic data can pose risks.
\end{itemize}

\hypertarget{further-insights-on-synthetic-data}{%
\subsection{Further Insights on Synthetic Data}\label{further-insights-on-synthetic-data}}

Synthetic data bridges the model-centric and data-centric perspectives, making it an essential tool in modern research. Analogously, it's like viewing the Mona Lisa's replica, with the real painting stored securely.

Future projects, such as utilizing the R's diamonds dataset for synthetic data generation, hold promise in demonstrating the vast potentials of this technology.

For a deeper dive into synthetic data and its applications, refer to \citep{jordon2022synthetic}.

\hypertarget{application-9}{%
\section{Application}\label{application-9}}

The easiest way to create synthetic data is to use the \texttt{synthpop} package. Alternatively, you can do it \href{https://towardsdatascience.com/creating-synthetic-data-3774391c851d}{manually}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(synthpop)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(performance)}

\CommentTok{\# library(effectsize)}
\CommentTok{\# library(see)}
\CommentTok{\# library(patchwork)}
\CommentTok{\# library(knitr)}
\CommentTok{\# library(kableExtra)}

\FunctionTok{head}\NormalTok{(iris)}
\CommentTok{\#\textgreater{}   Sepal.Length Sepal.Width Petal.Length Petal.Width Species}
\CommentTok{\#\textgreater{} 1          5.1         3.5          1.4         0.2  setosa}
\CommentTok{\#\textgreater{} 2          4.9         3.0          1.4         0.2  setosa}
\CommentTok{\#\textgreater{} 3          4.7         3.2          1.3         0.2  setosa}
\CommentTok{\#\textgreater{} 4          4.6         3.1          1.5         0.2  setosa}
\CommentTok{\#\textgreater{} 5          5.0         3.6          1.4         0.2  setosa}
\CommentTok{\#\textgreater{} 6          5.4         3.9          1.7         0.4  setosa}

\NormalTok{synthpop}\SpecialCharTok{::}\FunctionTok{codebook.syn}\NormalTok{(iris)}
\CommentTok{\#\textgreater{} $tab}
\CommentTok{\#\textgreater{}       variable   class nmiss perctmiss ndistinct}
\CommentTok{\#\textgreater{} 1 Sepal.Length numeric     0         0        35}
\CommentTok{\#\textgreater{} 2  Sepal.Width numeric     0         0        23}
\CommentTok{\#\textgreater{} 3 Petal.Length numeric     0         0        43}
\CommentTok{\#\textgreater{} 4  Petal.Width numeric     0         0        22}
\CommentTok{\#\textgreater{} 5      Species  factor     0         0         3}
\CommentTok{\#\textgreater{}                             details}
\CommentTok{\#\textgreater{} 1                  Range: 4.3 {-} 7.9}
\CommentTok{\#\textgreater{} 2                    Range: 2 {-} 4.4}
\CommentTok{\#\textgreater{} 3                    Range: 1 {-} 6.9}
\CommentTok{\#\textgreater{} 4                  Range: 0.1 {-} 2.5}
\CommentTok{\#\textgreater{} 5 \textquotesingle{}setosa\textquotesingle{} \textquotesingle{}versicolor\textquotesingle{} \textquotesingle{}virginica\textquotesingle{}}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $labs}
\CommentTok{\#\textgreater{} NULL}

\NormalTok{syn\_df }\OtherTok{\textless{}{-}} \FunctionTok{syn}\NormalTok{(iris, }\AttributeTok{seed =} \DecValTok{3}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Synthesis}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}  Sepal.Length Sepal.Width Petal.Length Petal.Width Species}

\CommentTok{\# check for replciated uniques}
\FunctionTok{replicated.uniques}\NormalTok{(syn\_df, iris)}
\CommentTok{\#\textgreater{} $replications}
\CommentTok{\#\textgreater{}   [1]  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE}
\CommentTok{\#\textgreater{}  [13]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE}
\CommentTok{\#\textgreater{}  [25] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE}
\CommentTok{\#\textgreater{}  [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE}
\CommentTok{\#\textgreater{}  [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE}
\CommentTok{\#\textgreater{}  [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE}
\CommentTok{\#\textgreater{}  [73] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE}
\CommentTok{\#\textgreater{}  [85] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE}
\CommentTok{\#\textgreater{}  [97] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE}
\CommentTok{\#\textgreater{} [109] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE}
\CommentTok{\#\textgreater{} [121] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE}
\CommentTok{\#\textgreater{} [133] FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE}
\CommentTok{\#\textgreater{} [145] FALSE FALSE FALSE FALSE FALSE FALSE}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $no.uniques}
\CommentTok{\#\textgreater{} [1] 148}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $no.replications}
\CommentTok{\#\textgreater{} [1] 17}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $per.replications}
\CommentTok{\#\textgreater{} [1] 11.33333}


\CommentTok{\# remove replicated uniques and adds a FAKE\_DATA label }
\CommentTok{\# (in case a person can see his or own data in }
\CommentTok{\# the replicated data by chance)}

\NormalTok{syn\_df\_sdc }\OtherTok{\textless{}{-}} \FunctionTok{sdc}\NormalTok{(syn\_df, iris, }
                  \AttributeTok{label =} \StringTok{"FAKE\_DATA"}\NormalTok{,}
                  \AttributeTok{rm.replicated.uniques =}\NormalTok{ T)}
\CommentTok{\#\textgreater{} no. of replicated uniques: 17}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris }\SpecialCharTok{|\textgreater{}} 
\NormalTok{    GGally}\SpecialCharTok{::}\FunctionTok{ggpairs}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{41-rep_synthetic_data_files/figure-latex/unnamed-chunk-2-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{syn\_df}\SpecialCharTok{$}\NormalTok{syn }\SpecialCharTok{|\textgreater{}} 
\NormalTok{    GGally}\SpecialCharTok{::}\FunctionTok{ggpairs}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{41-rep_synthetic_data_files/figure-latex/unnamed-chunk-2-2} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm\_ori }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Sepal.Length }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Sepal.Width }\SpecialCharTok{+}\NormalTok{ Petal.Length , }\AttributeTok{data =}\NormalTok{ iris)}
\CommentTok{\# performance::check\_model(lm\_ori)}
\FunctionTok{summary}\NormalTok{(lm\_ori)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = Sepal.Length \textasciitilde{} Sepal.Width + Petal.Length, data = iris)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}      Min       1Q   Median       3Q      Max }
\CommentTok{\#\textgreater{} {-}0.96159 {-}0.23489  0.00077  0.21453  0.78557 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}              Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)   2.24914    0.24797    9.07 7.04e{-}16 ***}
\CommentTok{\#\textgreater{} Sepal.Width   0.59552    0.06933    8.59 1.16e{-}14 ***}
\CommentTok{\#\textgreater{} Petal.Length  0.47192    0.01712   27.57  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 0.3333 on 147 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.8402, Adjusted R{-}squared:  0.838 }
\CommentTok{\#\textgreater{} F{-}statistic: 386.4 on 2 and 147 DF,  p{-}value: \textless{} 2.2e{-}16}

\NormalTok{lm\_syn }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Sepal.Length }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Sepal.Width }\SpecialCharTok{+}\NormalTok{ Petal.Length , }\AttributeTok{data =}\NormalTok{ syn\_df}\SpecialCharTok{$}\NormalTok{syn)}
\CommentTok{\# performance::check\_model(lm\_syn)}
\FunctionTok{summary}\NormalTok{(lm\_syn)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = Sepal.Length \textasciitilde{} Sepal.Width + Petal.Length, data = syn\_df$syn)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}      Min       1Q   Median       3Q      Max }
\CommentTok{\#\textgreater{} {-}0.79165 {-}0.22790 {-}0.01448  0.15893  1.13360 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}              Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)   2.96449    0.24538  12.081  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} Sepal.Width   0.39214    0.06816   5.754  4.9e{-}08 ***}
\CommentTok{\#\textgreater{} Petal.Length  0.45267    0.01743  25.974  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 0.3658 on 147 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.8246, Adjusted R{-}squared:  0.8222 }
\CommentTok{\#\textgreater{} F{-}statistic: 345.6 on 2 and 147 DF,  p{-}value: \textless{} 2.2e{-}16}
\end{Highlighting}
\end{Shaded}

Open data can be assessed for its utility in two distinct ways:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{General Utility}: This refers to the broad resemblances within the dataset, allowing for preliminary data exploration.
\item
  \textbf{Specific Utility}: This focuses on the comparability of models derived from synthetic and original datasets, emphasizing analytical reproducibility.
\end{enumerate}

For General utility

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{compare}\NormalTok{(syn\_df, iris)}
\end{Highlighting}
\end{Shaded}

Specific utility

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# just like regular lm, but for synthetic data}
\NormalTok{lm\_syn }\OtherTok{\textless{}{-}} \FunctionTok{lm.synds}\NormalTok{(Sepal.Length }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Sepal.Width }\SpecialCharTok{+}\NormalTok{ Petal.Length , }\AttributeTok{data =}\NormalTok{ syn\_df)}
\FunctionTok{compare}\NormalTok{(lm\_syn, iris)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call used to fit models to the data:}
\CommentTok{\#\textgreater{} lm.synds(formula = Sepal.Length \textasciitilde{} Sepal.Width + Petal.Length, }
\CommentTok{\#\textgreater{}     data = syn\_df)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Differences between results based on synthetic and observed data:}
\CommentTok{\#\textgreater{}              Synthetic  Observed        Diff Std. coef diff CI overlap}
\CommentTok{\#\textgreater{} (Intercept)  2.9644900 2.2491402  0.71534988       2.884829  0.2640608}
\CommentTok{\#\textgreater{} Sepal.Width  0.3921429 0.5955247 {-}0.20338187      {-}2.933611  0.2516161}
\CommentTok{\#\textgreater{} Petal.Length 0.4526695 0.4719200 {-}0.01925058      {-}1.124602  0.7131064}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Measures for one synthesis and 3 coefficients}
\CommentTok{\#\textgreater{} Mean confidence interval overlap:  0.4095944}
\CommentTok{\#\textgreater{} Mean absolute std. coef diff:  2.314347}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Mahalanobis distance ratio for lack{-}of{-}fit (target 1.0): 3.08}
\CommentTok{\#\textgreater{} Lack{-}of{-}fit test: 9.23442; p{-}value 0.0263 for test that synthesis model is}
\CommentTok{\#\textgreater{} compatible with a chi{-}squared test with 3 degrees of freedom.}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Confidence interval plot:}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{41-rep_synthetic_data_files/figure-latex/unnamed-chunk-5-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# summary(lm\_syn)}
\end{Highlighting}
\end{Shaded}

You basically want your lack-of-fit test to be non-significant.

\hypertarget{appendix-appendix}{%
\appendix}


\hypertarget{appendix}{%
\chapter{Appendix}\label{appendix}}

\hypertarget{git}{%
\section{Git}\label{git}}

\href{https://training.github.com/downloads/github-git-cheat-sheet.pdf}{Cheat Sheet}

\href{https://training.github.com/}{Cheat Sheet in different languages}

\href{http://try.github.io/}{Learn Git}

\href{http://ndpsoftware.com/git-cheatsheet.html\#loc=remote_repo;}{Interactive Cheat Sheet}

\href{https://happygitwithr.com/}{Ultimate Guide of Git and GitHub for R user}

\begin{itemize}
\item
  Setting up Git: \texttt{git\ config} with \texttt{-\/-global} option to configure user name, email, editor, etc.
\item
  Creating a repository: \texttt{git\ init} to initialize a repo. Git stores all of its repo data in the \texttt{.git} directory.
\item
  Tracking changes:

  \begin{itemize}
  \item
    \texttt{git\ status} shows the status of the repo

    \begin{itemize}
    \item
      File are stored in the project's working directory (which users see)
    \item
      The staging area (where the next commit is being built)
    \item
      local repo is where commits are permanently recorded
    \end{itemize}
  \item
    \texttt{git\ add} put files in the staging area
  \item
    \texttt{git\ commit} saves the staged content as a new commit in the local repo.

    \begin{itemize}
    \tightlist
    \item
      \texttt{git\ commit\ -m\ "your\ own\ message"} to give a messages for the purpose of your commit.
    \end{itemize}
  \end{itemize}
\item
  History

  \begin{itemize}
  \item
    \texttt{git\ diff} shows differences between commits
  \item
    \texttt{git\ checkout} recovers old version of fields

    \begin{itemize}
    \item
      \texttt{git\ checkout\ HEAD} to go to the last commit
    \item
      \texttt{git\ checkout\ \textless{}unique\ ID\ of\ your\ commit\textgreater{}} to go to such commit
    \end{itemize}
  \end{itemize}
\item
  Ignoring

  \begin{itemize}
  \item
    \texttt{.gitignore} file tells Git what files to ignore
  \item
    \texttt{cat\ .\ gitignore\ *.dat\ results/} ignore files ending with ``dat'' and folder ``results''.
  \end{itemize}
\item
  Remotes in GitHub

  \begin{itemize}
  \item
    A local git repo can be connected to one or more remote repos.
  \item
    Use the HTTPS protocol to connect to remote repos
  \item
    \texttt{git\ push} copies changes from a local repo to a remote repo
  \item
    \texttt{git\ pull} copies changes from a remote repo to a local repo
  \end{itemize}
\item
  Collaborating

  \begin{itemize}
  \tightlist
  \item
    \texttt{git\ clone} copies remote repo to create a local repo with a remote called \texttt{origin} automatically set up
  \end{itemize}
\item
  Branching

  \begin{itemize}
  \item
    \texttt{git\ check\ -\ b\ \textless{}new-branch-name}
  \item
    \texttt{git\ checkout\ master} to switch to master branch.
  \end{itemize}
\item
  Conflicts

  \begin{itemize}
  \item
    occur when 2 or more people change the same lines of the same file
  \item
    the version control system does not allow to overwrite each other's changes blindly, but highlights conflicts so that they can be resolved.
  \end{itemize}
\item
  Licensing

  \begin{itemize}
  \item
    People who incorporate General Public License (GPL'd) software into their won software must make their software also open under the GPL license; most other open licenses do not require this.
  \item
    The Creative Commons family of licenses allow people to mix and match requirements and restrictions on attribution, creation of derivative works, further sharing and commercialization.
  \end{itemize}
\item
  Citation:

  \begin{itemize}
  \tightlist
  \item
    Add a CITATION file to a repo to explain how you want others to cite your work.
  \end{itemize}
\item
  Hosting

  \begin{itemize}
  \tightlist
  \item
    Rules regarding intellectual property and storage of sensitive info apply no matter where code and data are hosted.
  \end{itemize}
\end{itemize}

\hypertarget{short-cut}{%
\section{Short-cut}\label{short-cut}}

These are shortcuts that you probably you remember when working with R. Even though it might take a bit of time to learn and use them as your second nature, but they will save you a lot of time.\\
Just like learning another language, the more you speak and practice it, the more comfortable you are speaking it.\\

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.6494}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.3506}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
function
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
short-cut
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
navigate folders in console & \texttt{"\ "\ +\ tab} \\
pull up short-cut cheat sheet & \texttt{ctrl\ +\ shift\ +\ k} \\
go to file/function (everything in your project) & \texttt{ctrl\ +\ .} \\
search everything & \texttt{cmd\ +\ shift\ +\ f} \\
navigate between tabs & \texttt{Crtl\ +\ shift\ +\ .} \\
type function faster & \texttt{snip\ +\ shift\ +\ tab} \\
type faster & \texttt{use\ tab\ for\ fuzzy\ match} \\
\texttt{cmd\ +\ up} & \\
\texttt{ctrl\ +\ .} & \\
\end{longtable}

Sometimes you can't stage a folder because it's too large. In such case, use \texttt{Terminal} pane in Rstudio then type \texttt{git\ add\ -A} to stage all changes then commit and push like usual.

\hypertarget{function-short-cut}{%
\section{Function short-cut}\label{function-short-cut}}

apply one function to your data to create a new variable: \texttt{mutate(mod=map(data,function))}\\
instead of using \texttt{i\ in\ 1:length(object)}: \texttt{for\ (i\ in\ seq\_along(object))}\\
apply multiple function: \texttt{map\_dbl}\\
apply multiple function to multiple variables:\texttt{map2}\\
\texttt{autoplot(data)} plot times series data\\
\texttt{mod\_tidy\ =\ linear(reg)\ \%\textgreater{}\%\ set\_engine(\textquotesingle{}lm\textquotesingle{})\ \%\textgreater{}\%\ fit(price\ \textasciitilde{}\ .,\ data=data)} fit lm model. It could also fit other models (stan, spark, glmnet, keras)

\begin{itemize}
\tightlist
\item
  Sometimes, data-masking will not be able to recognize whether you're calling from environment or data variables. To bypass this, we use \texttt{.data\$variable} or \texttt{.env\$variable}. For example \texttt{data\ \%\textgreater{}\%\ mutate(x=.env\$variable/.data\$variable}\\
\item
  Problems with data-masking:\\

  \begin{itemize}
  \tightlist
  \item
    Unexpected masking by data-var: Use \texttt{.data} and \texttt{.env} to disambiguate\\
  \item
    Data-var cant get through:\\
  \item
    Tunnel data-var with \{\{\}\} + Subset \texttt{.data} with {[}{[}{]}{]}
  \end{itemize}
\item
  Passing Data-variables through arguments
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"dplyr"}\NormalTok{)}
\NormalTok{mean\_by }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data,by,var)\{}
\NormalTok{    data }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{group\_by}\NormalTok{(\{\{\{by\}\}\}) }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{summarise}\NormalTok{(}\StringTok{"\{\{var\}\}"}\SpecialCharTok{:=}\FunctionTok{mean}\NormalTok{(\{\{var\}\})) }\CommentTok{\# new name for each var will be created by tunnel data{-}var inside strings}
\NormalTok{\}}

\NormalTok{mean\_by }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data,by,var)\{}
\NormalTok{    data }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{group\_by}\NormalTok{(\{\{\{by\}\}\}) }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{summarise}\NormalTok{(}\StringTok{"\{var\}"}\SpecialCharTok{:=}\FunctionTok{mean}\NormalTok{(\{\{var\}\})) }\CommentTok{\# use single \{\} to glue the string, but hard to reuse code in functions}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Trouble with selection:\\
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"purrr"}\NormalTok{)}
\NormalTok{name }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"mass"}\NormalTok{,}\StringTok{"height"}\NormalTok{)}
\NormalTok{starwars }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(name) }\CommentTok{\# Data{-}var. Here you are referring to variable named "name"}

\NormalTok{starwars }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(}\FunctionTok{all\_of}\NormalTok{((name))) }\CommentTok{\# use all\_of() to disambiguate when }

\NormalTok{averages }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data,vars)\{ }\CommentTok{\# take character vectors with all\_of()}
\NormalTok{    data }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{select}\NormalTok{(}\FunctionTok{all\_of}\NormalTok{(vars)) }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{map\_dbl}\NormalTok{(mean,}\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{)}
\NormalTok{\} }

\NormalTok{x }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"Sepal.Length"}\NormalTok{,}\StringTok{"Petal.Length"}\NormalTok{)}
\NormalTok{iris }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{averages}\NormalTok{(x)}


\CommentTok{\# Another way}
\NormalTok{averages }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data,vars)\{ }\CommentTok{\# Tunnel selectiosn with \{\{\}\}}
\NormalTok{    data }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{select}\NormalTok{(\{\{vars\}\}) }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{map\_dbl}\NormalTok{(mean,}\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{)}
\NormalTok{\} }

\NormalTok{x }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"Sepal.Length"}\NormalTok{,}\StringTok{"Petal.Length"}\NormalTok{)}
\NormalTok{iris }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{averages}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\hypertarget{citation}{%
\section{Citation}\label{citation}}

include a citation by \texttt{{[}@Farjam\_2015{]}}

cite packages used in this session

\texttt{package=ls(sessionInfo()\$loadedOnly)\ for\ (i\ in\ package)\{print(toBibtex(citation(i)))\}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{package}\OtherTok{=}\FunctionTok{ls}\NormalTok{(}\FunctionTok{sessionInfo}\NormalTok{()}\SpecialCharTok{$}\NormalTok{loadedOnly) }
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in}\NormalTok{ package)\{}
    \FunctionTok{print}\NormalTok{(}\FunctionTok{toBibtex}\NormalTok{(}\FunctionTok{citation}\NormalTok{(i)))}
\NormalTok{    \}}
\end{Highlighting}
\end{Shaded}

\hypertarget{install-all-necessary-packageslibaries-on-your-local-machine}{%
\section{Install all necessary packages/libaries on your local machine}\label{install-all-necessary-packageslibaries-on-your-local-machine}}

Get a list of packages you need to install from this book (or your local device)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{installed }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{installed.packages}\NormalTok{())}

\FunctionTok{head}\NormalTok{(installed)}
\CommentTok{\#\textgreater{}         Package                            LibPath Version Priority}
\CommentTok{\#\textgreater{} abind     abind C:/Program Files/R/R{-}4.2.3/library   1.4{-}5     \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{} ade4       ade4 C:/Program Files/R/R{-}4.2.3/library  1.7{-}22     \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{} admisc   admisc C:/Program Files/R/R{-}4.2.3/library    0.33     \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{} AER         AER C:/Program Files/R/R{-}4.2.3/library  1.2{-}10     \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{} afex       afex C:/Program Files/R/R{-}4.2.3/library   1.3{-}0     \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{} agridat agridat C:/Program Files/R/R{-}4.2.3/library    1.21     \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{}                                                                                        Depends}
\CommentTok{\#\textgreater{} abind                                                                             R (\textgreater{}= 1.5.0)}
\CommentTok{\#\textgreater{} ade4                                                                               R (\textgreater{}= 2.10)}
\CommentTok{\#\textgreater{} admisc                                                                            R (\textgreater{}= 3.5.0)}
\CommentTok{\#\textgreater{} AER     R (\textgreater{}= 3.0.0), car (\textgreater{}= 2.0{-}19), lmtest, sandwich (\textgreater{}= 2.4{-}0),\textbackslash{}nsurvival (\textgreater{}= 2.37{-}5), zoo}
\CommentTok{\#\textgreater{} afex                                                             R (\textgreater{}= 3.5.0), lme4 (\textgreater{}= 1.1{-}8)}
\CommentTok{\#\textgreater{} agridat                                                                                   \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{}                                                                                 Imports}
\CommentTok{\#\textgreater{} abind                                                                    methods, utils}
\CommentTok{\#\textgreater{} ade4                graphics, grDevices, methods, stats, utils, MASS, pixmap, sp,\textbackslash{}nRcpp}
\CommentTok{\#\textgreater{} admisc                                                                          methods}
\CommentTok{\#\textgreater{} AER                                                           stats, Formula (\textgreater{}= 0.2{-}0)}
\CommentTok{\#\textgreater{} afex    pbkrtest (\textgreater{}= 0.4{-}1), lmerTest (\textgreater{}= 3.0{-}0), car, reshape2,\textbackslash{}nstats, methods, utils}
\CommentTok{\#\textgreater{} agridat                                                                            \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{}                   LinkingTo}
\CommentTok{\#\textgreater{} abind                  \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{} ade4    Rcpp, RcppArmadillo}
\CommentTok{\#\textgreater{} admisc                 \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{} AER                    \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{} afex                   \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{} agridat                \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{}                                                                                                                                                                                                                                                                                                                                                                                                Suggests}
\CommentTok{\#\textgreater{} abind                                                                                                                                                                                                                                                                                                                                                                                              \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{} ade4                                                                                                                                                                                                                                                  ade4TkGUI, adegraphics, adephylo, ape, CircStats, deldir,\textbackslash{}nlattice, spdep, splancs, waveslim, progress, foreach, parallel,\textbackslash{}ndoParallel, iterators}
\CommentTok{\#\textgreater{} admisc                                                                                                                                                                                                                                                                                                                                                                                     QCA (\textgreater{}= 3.7)}
\CommentTok{\#\textgreater{} AER                                                                                                                                  boot, dynlm, effects, fGarch, forecast, foreign, ineq,\textbackslash{}nKernSmooth, lattice, longmemo, MASS, mlogit, nlme, nnet, np,\textbackslash{}nplm, pscl, quantreg, rgl, ROCR, rugarch, sampleSelection,\textbackslash{}nscatterplot3d, strucchange, systemfit (\textgreater{}= 1.1{-}20), truncreg,\textbackslash{}ntseries, urca, vars}
\CommentTok{\#\textgreater{} afex    emmeans (\textgreater{}= 1.4), coin, xtable, parallel, plyr, optimx,\textbackslash{}nnloptr, knitr, rmarkdown, R.rsp, lattice, latticeExtra,\textbackslash{}nmultcomp, testthat, mlmRev, dplyr, tidyr, dfoptim, Matrix,\textbackslash{}npsychTools, ggplot2, MEMSS, effects, carData, ggbeeswarm, nlme,\textbackslash{}ncowplot, jtools, ggpubr, ggpol, MASS, glmmTMB, brms, rstanarm,\textbackslash{}nstatmod, performance (\textgreater{}= 0.7.2), see (\textgreater{}= 0.6.4), ez,\textbackslash{}nggResidpanel, grid, vdiffr}
\CommentTok{\#\textgreater{} agridat                    AER, agricolae, betareg, broom, car, coin, corrgram, desplot,\textbackslash{}ndplyr, effects, equivalence, emmeans, FrF2, gam, gge, ggplot2,\textbackslash{}ngnm, gstat, HH, knitr, lattice, latticeExtra, lme4, lucid,\textbackslash{}nmapproj, maps, MASS, MCMCglmm, metafor, mgcv, NADA, nlme,\textbackslash{}nnullabor, ordinal, pbkrtest, pls, pscl, reshape2, rgdal,\textbackslash{}nrmarkdown, qicharts, qtl, sp, SpATS, survival, vcd, testthat}
\CommentTok{\#\textgreater{}         Enhances       License License\_is\_FOSS License\_restricts\_use OS\_type}
\CommentTok{\#\textgreater{} abind       \textless{}NA\textgreater{}   LGPL (\textgreater{}= 2)            \textless{}NA\textgreater{}                  \textless{}NA\textgreater{}    \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{} ade4        \textless{}NA\textgreater{}    GPL (\textgreater{}= 2)            \textless{}NA\textgreater{}                  \textless{}NA\textgreater{}    \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{} admisc      \textless{}NA\textgreater{}    GPL (\textgreater{}= 3)            \textless{}NA\textgreater{}                  \textless{}NA\textgreater{}    \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{} AER         \textless{}NA\textgreater{} GPL{-}2 | GPL{-}3            \textless{}NA\textgreater{}                  \textless{}NA\textgreater{}    \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{} afex        \textless{}NA\textgreater{}    GPL (\textgreater{}= 2)            \textless{}NA\textgreater{}                  \textless{}NA\textgreater{}    \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{} agridat     \textless{}NA\textgreater{}  CC BY{-}SA 4.0            \textless{}NA\textgreater{}                  \textless{}NA\textgreater{}    \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{}         MD5sum NeedsCompilation Built}
\CommentTok{\#\textgreater{} abind     \textless{}NA\textgreater{}               no 4.2.0}
\CommentTok{\#\textgreater{} ade4      \textless{}NA\textgreater{}              yes 4.2.3}
\CommentTok{\#\textgreater{} admisc    \textless{}NA\textgreater{}              yes 4.2.3}
\CommentTok{\#\textgreater{} AER       \textless{}NA\textgreater{}               no 4.2.3}
\CommentTok{\#\textgreater{} afex      \textless{}NA\textgreater{}               no 4.2.3}
\CommentTok{\#\textgreater{} agridat   \textless{}NA\textgreater{}               no 4.2.3}

\FunctionTok{write.csv}\NormalTok{(installed, }\FunctionTok{file.path}\NormalTok{(}\FunctionTok{getwd}\NormalTok{(),}\StringTok{\textquotesingle{}installed.csv\textquotesingle{}}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

After having the \texttt{installed.csv} file on your new or local machine, you can just install the list of packages

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# import the list of packages}
\NormalTok{installed }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{\textquotesingle{}installed.csv\textquotesingle{}}\NormalTok{)}

\CommentTok{\# get the list of packages that you have on your device}
\NormalTok{baseR }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{installed.packages}\NormalTok{())}

\CommentTok{\# install only those that you don\textquotesingle{}t have}
\FunctionTok{install.packages}\NormalTok{(}\FunctionTok{setdiff}\NormalTok{(installed, baseR))}
\end{Highlighting}
\end{Shaded}

\hypertarget{bookdown-cheat-sheet}{%
\chapter{Bookdown cheat sheet}\label{bookdown-cheat-sheet}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# to see non{-}scientific notation a result}
\FunctionTok{format}\NormalTok{(}\FloatTok{12e{-}17}\NormalTok{, }\AttributeTok{scientific =} \ConstantTok{FALSE}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] "0.00000000000000012"}
\end{Highlighting}
\end{Shaded}

\hypertarget{operation}{%
\section{Operation}\label{operation}}

R commands to do derivatives of a defined function Taking derivatives in R involves using the \texttt{expression,} \texttt{D,} and \texttt{eval} functions. You wrap the function you want to take the derivative of in expression(), then use D, then eval as follows.

simple example

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#define a function}
\NormalTok{f}\OtherTok{=}\FunctionTok{expression}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(x))}

\CommentTok{\#take the first derivative}
\NormalTok{df.dx}\OtherTok{=}\FunctionTok{D}\NormalTok{(f,}\StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{)}
\NormalTok{df.dx}
\CommentTok{\#\textgreater{} 0.5 * x\^{}{-}0.5}

\CommentTok{\#take the second derivative}
\NormalTok{d2f.dx2}\OtherTok{=}\FunctionTok{D}\NormalTok{(}\FunctionTok{D}\NormalTok{(f,}\StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{),}\StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{)}
\NormalTok{d2f.dx2}
\CommentTok{\#\textgreater{} 0.5 * ({-}0.5 * x\^{}{-}1.5)}
\end{Highlighting}
\end{Shaded}

Evaluate

\begin{itemize}
\tightlist
\item
  The first argument passed to eval is the expression you want to evaluate
\item
  the second is a list containing the values of all quantities that are not defined elsewhere.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#evaluate the function at a given x}
\FunctionTok{eval}\NormalTok{(f,}\FunctionTok{list}\NormalTok{(}\AttributeTok{x=}\DecValTok{3}\NormalTok{))}
\CommentTok{\#\textgreater{} [1] 1.732051}

\CommentTok{\#evaluate the first derivative at a given x}
\FunctionTok{eval}\NormalTok{(df.dx,}\FunctionTok{list}\NormalTok{(}\AttributeTok{x=}\DecValTok{3}\NormalTok{))}
\CommentTok{\#\textgreater{} [1] 0.2886751}

\CommentTok{\#evaluate the second derivative at a given x}
\FunctionTok{eval}\NormalTok{(d2f.dx2,}\FunctionTok{list}\NormalTok{(}\AttributeTok{x=}\DecValTok{3}\NormalTok{))}
\CommentTok{\#\textgreater{} [1] {-}0.04811252}
\end{Highlighting}
\end{Shaded}

\hypertarget{math-expression-syntax}{%
\section{Math Expression/ Syntax}\label{math-expression-syntax}}

\href{http://tug.ctan.org/info/symbols/comprehensive/symbols-a4.pdf}{Full list}

Aligning equations

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textbackslash{}begin\{aligned\}}
\NormalTok{a }\SpecialCharTok{\&} \ErrorTok{=}\NormalTok{ b \textbackslash{}\textbackslash{}}
\NormalTok{X }\SpecialCharTok{\&}\NormalTok{\textbackslash{}sim \{Norm\}(}\DecValTok{10}\NormalTok{, }\DecValTok{3}\NormalTok{) \textbackslash{}\textbackslash{}}
\DecValTok{5} \SpecialCharTok{\&}\NormalTok{ \textbackslash{}le }\DecValTok{10}
\NormalTok{\textbackslash{}end\{aligned\}}
\end{Highlighting}
\end{Shaded}

\[
\begin{aligned}
a & = b \\
X &\sim {Norm}(10, 3) \\
5 & \le 10
\end{aligned}
\]

Cross-reference equation

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textbackslash{}begin\{equation\} }
\NormalTok{a }\OtherTok{=}\NormalTok{ b}
\NormalTok{(\textbackslash{}}\CommentTok{\#eq:test)}
\NormalTok{\textbackslash{}end\{equation\}}
\end{Highlighting}
\end{Shaded}

\begin{equation} 
a = b
\label{eq:test}
\end{equation}

to refer in a sentence \eqref{eq:test} (\texttt{\textbackslash{}@ref(eq:test)})

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5042}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.4958}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Math} Syntax
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Notation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{\$\textbackslash{}pm\$} & \(\pm\) \\
\texttt{\$\textbackslash{}ge\$} & \(\ge\) \\
\texttt{\$\textbackslash{}le\$} & \(\le\) \\
\texttt{\$\textbackslash{}neq\$} & \(\neq\) \\
\texttt{\$\textbackslash{}equiv\$} & \(\equiv\) \\
\texttt{\$\^{}\textbackslash{}circ\$} & \(^\circ\) \\
\texttt{\$\textbackslash{}times\$} & \(\times\) \\
\texttt{\$\textbackslash{}cdot\$} & \(\cdot\) \\
\texttt{\$\textbackslash{}leq\$} & \(\leq\) \\
\texttt{\$\textbackslash{}geq\$} & \(\geq\) \\
\texttt{\textbackslash{}propto} & \(\propto\) \\
\texttt{\$\textbackslash{}subset\$} & \(\subset\) \\
\texttt{\$\textbackslash{}subseteq\$} & \(\subseteq\) \\
\texttt{\$\textbackslash{}leftarrow\$} & \(\leftarrow\) \\
\texttt{\$\textbackslash{}rightarrow\$} & \(\rightarrow\) \\
\texttt{\$\textbackslash{}Leftarrow\$} & \(\Leftarrow\) \\
\texttt{\$\textbackslash{}Rightarrow\$} & \(\Rightarrow\) \\
\texttt{\$\textbackslash{}approx\$} & \(\approx\) \\
\texttt{\$\textbackslash{}mathbb\{R\}\$} & \(\mathbb{R}\) \\
\texttt{\$\textbackslash{}sum\_\{n=1\}\^{}\{10\}\ n\^{}2\$} & \(\sum_{n=1}^{10} n^2\) \\
\texttt{\$\$\textbackslash{}sum\_\{n=1\}\^{}\{10\}\ n\^{}2\$\$} & \(\sum_{n=1}^{10} n^2\) \\
\texttt{\$x\^{}\{n\}\$} & \(x^{n}\) \\
\texttt{\$x\_\{n\}\$} & \(x_{n}\) \\
\texttt{\$\textbackslash{}overline\{x\}\$} & \(\overline{x}\) \\
\texttt{\$\textbackslash{}hat\{x\}\$} & \(\hat{x}\) \\
\texttt{\$\textbackslash{}tilde\{x\}\$} & \(\tilde{x}\) \\
\texttt{\textbackslash{}check\{\}} & \(\check{}\) \\
\texttt{\textbackslash{}underset\{\textbackslash{}gamma\}\{\textbackslash{}operatorname\{argmin\}\}} & \(\underset{\gamma}{\operatorname{argmin}}\) \\
\texttt{\$\textbackslash{}frac\{a\}\{b\}\$} & \(\frac{a}{b}\) \\
\texttt{\$\textbackslash{}frac\{a\}\{b\}\$} & \(\frac{a}{b}\) \\
\texttt{\$\textbackslash{}displaystyle\ \textbackslash{}frac\{a\}\{b\}\$} & \(\displaystyle \frac{a}{b}\) \\
\texttt{\$\textbackslash{}binom\{n\}\{k\}\$} & \(\binom{n}{k}\) \\
\texttt{\$x\_\{1\}\ +\ x\_\{2\}\ +\ \textbackslash{}cdots\ +\ x\_\{n\}\$} & \(x_{1} + x_{2} + \cdots + x_{n}\) \\
\texttt{\$x\_\{1\},\ x\_\{2\},\ \textbackslash{}dots,\ x\_\{n\}\$} & \(x_{1}, x_{2}, \dots, x_{n}\) \\
\texttt{\textbackslash{}mathbf\{x\}\ =\ \textbackslash{}langle\ x\_\{1\},\ x\_\{2\},\ \textbackslash{}dots,\ x\_\{n\}\textbackslash{}rangle\$} & \(\mathbf{x} = \langle x_{1}, x_{2}, \dots, x_{n}\rangle\) \\
\texttt{\$x\ \textbackslash{}in\ A\$} & \(x \in A\) \\
\texttt{\$\textbar{}A\textbar{}\$} & \(|A|\) \\
\texttt{\$x\ \textbackslash{}in\ A\$} & \(x \in A\) \\
\texttt{\$x\ \textbackslash{}subset\ B\$} & \(x \subset B\) \\
\texttt{\$x\ \textbackslash{}subseteq\ B\$} & \(x \subseteq B\) \\
\texttt{\$A\ \textbackslash{}cup\ B\$} & \(A \cup B\) \\
\texttt{\$A\ \textbackslash{}cap\ B\$} & \(A \cap B\) \\
\texttt{\$X\ \textbackslash{}sim\ Binom(n,\ \textbackslash{}pi)\$} & \(X \sim Binom(n, \pi)\) \\
\texttt{\$\textbackslash{}mathrm\{P\}(X\ \textbackslash{}le\ x)\ =\ \textbackslash{}text\{pbinom\}(x,\ n,\ \textbackslash{}pi)\$} & \(\mathrm{P}(X \le x) = \text{pbinom}(x, n, \pi)\) \\
\texttt{\$P(A\ \textbackslash{}mid\ B)\$} & \(P(A \mid B)\) \\
\texttt{\$\textbackslash{}mathrm\{P\}(A\ \textbackslash{}mid\ B)\$} & \(\mathrm{P}(A \mid B)\) \\
\texttt{\$\textbackslash{}\{1,\ 2,\ 3\textbackslash{}\}\$} & \(\{1, 2, 3\}\) \\
\texttt{\$\textbackslash{}sin(x)\$} & \(\sin(x)\) \\
\texttt{\$\textbackslash{}log(x)\$} & \(\log(x)\) \\
\texttt{\$\textbackslash{}int\_\{a\}\^{}\{b\}\$} & \(\int_{a}^{b}\) \\
\texttt{\$\textbackslash{}left(\textbackslash{}int\_\{a\}\^{}\{b\}\ f(x)\ \textbackslash{};\ dx\textbackslash{}right)\$} & \(\left(\int_{a}^{b} f(x) \; dx\right)\) \\
\texttt{\$\textbackslash{}left{[}\textbackslash{}int\_\{\textbackslash{}-infty\}\^{}\{\textbackslash{}infty\}\ f(x)\ \textbackslash{};\ dx\textbackslash{}right{]}\$} & \(\left[\int_{-\infty}^{\infty} f(x) \; dx\right]\) \\
\texttt{\$\textbackslash{}left.\ F(x)\ \textbackslash{}right\textbar{}\_\{a\}\^{}\{b\}\$} & \(\left. F(x) \right|_{a}^{b}\) \\
\texttt{\$\textbackslash{}sum\_\{x\ =\ a\}\^{}\{b\}\ f(x)\$} & \(\sum_{x = a}^{b} f(x)\) \\
\texttt{\$\textbackslash{}prod\_\{x\ =\ a\}\^{}\{b\}\ f(x)\$} & \(\prod_{x = a}^{b} f(x)\) \\
\texttt{\$\textbackslash{}lim\_\{x\ \textbackslash{}to\ \textbackslash{}infty\}\ f(x)\$} & \(\lim_{x \to \infty} f(x)\) \\
\texttt{\$\textbackslash{}displaystyle\ \textbackslash{}lim\_\{x\ \textbackslash{}to\ \textbackslash{}infty\}\ f(x)\$} & \(\displaystyle \lim_{x \to \infty} f(x)\) \\
\textbf{Greek Letters} & \\
\texttt{\$\textbackslash{}alpha\ A\$} & \(\alpha A\) \\
\texttt{\$\textbackslash{}beta\ B\$} & \(\beta B\) \\
\texttt{\$\textbackslash{}gamma\ \textbackslash{}Gamma\$} & \(\gamma \Gamma\) \\
\texttt{\$\textbackslash{}delta\ \textbackslash{}Delta\$} & \(\delta \Delta\) \\
\texttt{\$\textbackslash{}epsilon\ \textbackslash{}varepsilon\ E\$} & \(\epsilon \varepsilon E\) \\
\texttt{\$\textbackslash{}zeta\ Z\ \textbackslash{}sigma\ \$} & \(\zeta Z \sigma\) \\
\texttt{\$\textbackslash{}eta\ H\$} & \(\eta H\) \\
\texttt{\$\textbackslash{}theta\ \textbackslash{}vartheta\ \textbackslash{}Theta\$} & \(\theta \vartheta \Theta\) \\
\texttt{\$\textbackslash{}iota\ I\$} & \(\iota I\) \\
\texttt{\$\textbackslash{}kappa\ K\$} & \(\kappa K\) \\
\texttt{\$\textbackslash{}lambda\ \textbackslash{}Lambda\$} & \(\lambda \Lambda\) \\
\texttt{\$\textbackslash{}mu\ M\$} & \(\mu M\) \\
\texttt{\$\textbackslash{}nu\ N\$} & \(\nu N\) \\
\texttt{\$\textbackslash{}xi\textbackslash{}Xi\$} & \(\xi\Xi\) \\
\texttt{\$o\ O\$} & \(o O\) \\
\texttt{\$\textbackslash{}pi\ \textbackslash{}Pi\$} & \(\pi \Pi\) \\
\texttt{\$\textbackslash{}rho\textbackslash{}varrho\ P\$} & \(\rho\varrho P\) \\
\texttt{\$\textbackslash{}sigma\ \textbackslash{}Sigma\$} & \(\sigma \Sigma\) \\
\texttt{\$\textbackslash{}tau\ T\$} & \(\tau T\) \\
\texttt{\$\textbackslash{}upsilon\ \textbackslash{}Upsilon\$} & \(\upsilon \Upsilon\) \\
\texttt{\$\textbackslash{}phi\ \textbackslash{}varphi\ \textbackslash{}Phi\$} & \(\phi \varphi \Phi\) \\
\texttt{\$\textbackslash{}chi\ X\$} & \(\chi X\) \\
\texttt{\$\textbackslash{}psi\ \textbackslash{}Psi\$} & \(\psi \Psi\) \\
\texttt{\$\textbackslash{}omega\ \textbackslash{}Omega\$} & \(\omega \Omega\) \\
\texttt{\$\textbackslash{}cdot\$} & \(\cdot\) \\
\texttt{\$\textbackslash{}cdots\$} & \(\cdots\) \\
\texttt{\$\textbackslash{}ddots\$} & \(\ddots\) \\
\texttt{\$\textbackslash{}ldots\$} & \(\ldots\) \\
\end{longtable}

Limit \texttt{P(\textbackslash{}lim\_\{n\textbackslash{}to\ \textbackslash{}infty\}\textbackslash{}bar\{X\}\_n\ =\textbackslash{}mu)\ =1}

\[
P(\lim_{n\to \infty}\bar{X}_n =\mu) =1
\]

Matrices

\begin{Shaded}
\begin{Highlighting}[]
\SpecialCharTok{$}\ErrorTok{$}\NormalTok{\textbackslash{}begin\{array\}}
\NormalTok{\{rrr\}}
\DecValTok{1} \SpecialCharTok{\&} \DecValTok{2} \SpecialCharTok{\&} \DecValTok{3}\NormalTok{ \textbackslash{}\textbackslash{}}
\DecValTok{4} \SpecialCharTok{\&} \DecValTok{5} \SpecialCharTok{\&} \DecValTok{6}\NormalTok{ \textbackslash{}\textbackslash{}}
\DecValTok{7} \SpecialCharTok{\&} \DecValTok{8} \SpecialCharTok{\&} \DecValTok{9}
\NormalTok{\textbackslash{}end\{array\}}
\SpecialCharTok{$}\ErrorTok{$}
\end{Highlighting}
\end{Shaded}

\[
\begin{array}
{rrr}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9
\end{array}
\]

\begin{Shaded}
\begin{Highlighting}[]
\SpecialCharTok{$}\ErrorTok{$}\NormalTok{\textbackslash{}mathbf\{X\} }\OtherTok{=}\NormalTok{ \textbackslash{}left[\textbackslash{}begin\{array\}}
\NormalTok{\{rrr\}}
\DecValTok{1} \SpecialCharTok{\&} \DecValTok{2} \SpecialCharTok{\&} \DecValTok{3}\NormalTok{ \textbackslash{}\textbackslash{}}
\DecValTok{4} \SpecialCharTok{\&} \DecValTok{5} \SpecialCharTok{\&} \DecValTok{6}\NormalTok{ \textbackslash{}\textbackslash{}}
\DecValTok{7} \SpecialCharTok{\&} \DecValTok{8} \SpecialCharTok{\&} \DecValTok{9}
\NormalTok{\textbackslash{}end\{array\}\textbackslash{}right]}
\SpecialCharTok{$}\ErrorTok{$}
\end{Highlighting}
\end{Shaded}

\[
\mathbf{X} = \left[\begin{array}
{rrr}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9
\end{array}\right]
\]

Aligning Equations

Aligning Equations with Comments

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textbackslash{}begin\{aligned\}}
    \DecValTok{3}\SpecialCharTok{+}\NormalTok{x }\SpecialCharTok{\&}\ErrorTok{=}\DecValTok{4} \SpecialCharTok{\&\&}\NormalTok{ \textbackslash{}text\{(Solve }\ControlFlowTok{for}\ErrorTok{\}}\NormalTok{ x \textbackslash{}text\{.}\ErrorTok{)}\NormalTok{\}\textbackslash{}}\SpecialCharTok{\textbackslash{}}
\NormalTok{    x }\SpecialCharTok{\&}\ErrorTok{=}\DecValTok{4{-}3} \SpecialCharTok{\&\&}\NormalTok{ \textbackslash{}text\{(Subtract }\DecValTok{3}\NormalTok{ from both sides.)\}\textbackslash{}}\SpecialCharTok{\textbackslash{}}
\NormalTok{    x }\SpecialCharTok{\&}\ErrorTok{=}\DecValTok{1}   \SpecialCharTok{\&\&}\NormalTok{ \textbackslash{}text\{(Yielding the solution.)\}}
\NormalTok{\textbackslash{}end\{aligned\}}
\end{Highlighting}
\end{Shaded}

\[
\begin{aligned}
    3+x &=4 & &\text{(Solve for} x \text{.)} \\
    x &=4-3 && \text{(Subtract 3 from both sides.)} \\
    x &=1   && \text{(Yielding the solution.)}
\end{aligned}
\]

\hypertarget{statistics-notation}{%
\subsection{Statistics Notation}\label{statistics-notation}}

\begin{Shaded}
\begin{Highlighting}[]
\SpecialCharTok{$}\ErrorTok{$}
\FunctionTok{f}\NormalTok{(y}\SpecialCharTok{|}\NormalTok{N,p) }\OtherTok{=}\NormalTok{ \textbackslash{}frac\{N}\SpecialCharTok{!}\NormalTok{\}\{y}\SpecialCharTok{!}\NormalTok{(N}\SpecialCharTok{{-}}\NormalTok{y)}\SpecialCharTok{!}\NormalTok{\}\textbackslash{}cdot p}\SpecialCharTok{\^{}}\NormalTok{y \textbackslash{}}\FunctionTok{cdot}\NormalTok{ (}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{p)}\SpecialCharTok{\^{}}\NormalTok{\{N}\SpecialCharTok{{-}}\NormalTok{y\} }\OtherTok{=}\NormalTok{ \{\{N\}\textbackslash{}choose\{y\}\} \textbackslash{}cdot p}\SpecialCharTok{\^{}}\NormalTok{y \textbackslash{}}\FunctionTok{cdot}\NormalTok{ (}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{p)}\SpecialCharTok{\^{}}\NormalTok{\{N}\SpecialCharTok{{-}}\NormalTok{y\}}
\SpecialCharTok{$}\ErrorTok{$}
\end{Highlighting}
\end{Shaded}

\[
f(y|N,p) = \frac{N!}{y!(N-y)!}\cdot p^y \cdot (1-p)^{N-y} = {{N}\choose{y}} \cdot p^y \cdot (1-p)^{N-y}
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textbackslash{}begin\{cases\}}
\NormalTok{\textbackslash{}frac\{}\DecValTok{1}\NormalTok{\}\{b}\SpecialCharTok{{-}}\NormalTok{a\}}\SpecialCharTok{\&}\NormalTok{\textbackslash{}text\{}\ControlFlowTok{for} \SpecialCharTok{$}\NormalTok{x\textbackslash{}}\ControlFlowTok{in}\NormalTok{[a,b]}\SpecialCharTok{$}\NormalTok{\}\textbackslash{}\textbackslash{}}
\DecValTok{0}\SpecialCharTok{\&}\NormalTok{\textbackslash{}text\{otherwise\}\textbackslash{}\textbackslash{}}
\NormalTok{\textbackslash{}end\{cases\}}
\end{Highlighting}
\end{Shaded}

\[
\begin{cases}
\frac{1}{b-a} & \text{for } x\in[a,b]\\
0 & \text{otherwise}\\
\end{cases}
\]

\hypertarget{table}{%
\section{Table}\label{table}}

\begin{Shaded}
\begin{Highlighting}[]
\SpecialCharTok{+{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}+{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}+{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}+}
\ErrorTok{|}\NormalTok{ Fruit         }\SpecialCharTok{|}\NormalTok{ Price         }\SpecialCharTok{|}\NormalTok{ Advantages         }\SpecialCharTok{|}
\SpecialCharTok{+}\ErrorTok{===============}\SpecialCharTok{+}\ErrorTok{===============}\SpecialCharTok{+}\ErrorTok{====================}\SpecialCharTok{+}
\ErrorTok{|} \ErrorTok{*}\NormalTok{Bananas}\SpecialCharTok{*}     \ErrorTok{|} \ErrorTok{$}\FloatTok{1.34}         \SpecialCharTok{|} \SpecialCharTok{{-}}\NormalTok{ built}\SpecialCharTok{{-}}\ControlFlowTok{in}\NormalTok{ wrapper }\SpecialCharTok{|}
\ErrorTok{|}               \ErrorTok{|}               \ErrorTok{|} \SpecialCharTok{{-}}\NormalTok{ bright color     }\SpecialCharTok{|}
\SpecialCharTok{+{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}+{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}+{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}+}
\ErrorTok{|}\NormalTok{ Oranges       }\SpecialCharTok{|} \ErrorTok{$}\FloatTok{2.10}         \SpecialCharTok{|} \SpecialCharTok{{-}}\NormalTok{ cures scurvy     }\SpecialCharTok{|}
\ErrorTok{|}               \ErrorTok{|}               \ErrorTok{|} \SpecialCharTok{{-}} \ErrorTok{**}\NormalTok{tasty}\SpecialCharTok{**}        \ErrorTok{|}
\SpecialCharTok{+{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}+{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}+{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}+}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2083}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2083}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3194}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Fruit
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Price
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Advantages
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\emph{Bananas} & \$1.34 & \begin{minipage}[t]{\linewidth}\raggedright
\begin{itemize}
\tightlist
\item
  built-in wrapper
\item
  bright color
\end{itemize}
\end{minipage} \\
Oranges & \$2.10 & \begin{minipage}[t]{\linewidth}\raggedright
\begin{itemize}
\tightlist
\item
  cures scurvy
\item
  \textbf{tasty}
\end{itemize}
\end{minipage} \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(\textbackslash{}mathbf\{x\}}\SpecialCharTok{\^{}}\NormalTok{T\textbackslash{}mathbf\{x\})}\SpecialCharTok{\^{}}\NormalTok{\{}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{\}\textbackslash{}mathbf\{x\}}\SpecialCharTok{\^{}}\NormalTok{T\textbackslash{}mathbf\{y\}}
\end{Highlighting}
\end{Shaded}

\((\mathbf{x}^T\mathbf{x})^{-1}\mathbf{x}^T\mathbf{y}\)

  \bibliography{book.bib,packages.bib,references.bib}

\end{document}
