% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\usepackage{soul}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{titling}
\pretitle{\begin{center} \includegraphics[width=2in,height=2in]{logo.png}\LARGE\\}
\posttitle{\end{center}}
\usepackage{array}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{siunitx}
\usepackage[normalem]{ulem}
\usepackage{colortbl}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{calc}
\usepackage{tabularx}
\usepackage{threeparttable}
\usepackage{wrapfig}
\usepackage{adjustbox}
\usepackage{hyperref}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={A Guide on Data Analysis},
  pdfauthor={Mike Nguyen},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{A Guide on Data Analysis}
\author{Mike Nguyen}
\date{2024-01-12}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}
\addcontentsline{toc}{chapter}{Preface}

This guide is an attempt to streamline and demystify the data analysis process. By no means this is the ultimate guide, or I am a great source of knowledge, or I claim myself to be a statistician/ econometrician, but I am a strong proponent of learning by teaching, and doing. Hence, this is more like a learning experience for both you and me. This book is completely free. My target audiences are those who have little to no experience in statistics and data science to those that have some interests in these fields and want to dive deeper and have a more holistic method. Even though my substantive domain of interest is marketing, this book can be used for other disciplines that use scientific methods or data analysis.

\includegraphics[width=3.125in,height=3.125in]{logo.png}

\hypertarget{how-to-cite-this-book}{%
\section*{How to cite this book}\label{how-to-cite-this-book}}
\addcontentsline{toc}{section}{How to cite this book}

\begin{quote}
\textbf{1. APA (7th edition):}

Nguyen, M. (2020). \emph{A Guide on Data Analysis}. Bookdown.

\href{https://bookdown.org/mike/data_analysis/}{\textbf{https://bookdown.org/mike/data\_analysis/}}

\textbf{2. MLA (8th edition):}

Nguyen, Mike. \emph{A Guide on Data Analysis}. Bookdown, 2020. \href{https://bookdown.org/mike/data_analysis/}{\textbf{https://bookdown.org/mike/data\_analysis/}}

\textbf{3. Chicago (17th edition):}

Nguyen, Mike. 2020. \emph{A Guide on Data Analysis}. Bookdown. \href{https://bookdown.org/mike/data_analysis/}{\textbf{https://bookdown.org/mike/data\_analysis/}}

\textbf{4. Harvard:}

Nguyen, M. (2020) \emph{A Guide on Data Analysis}. Bookdown. Available at: \href{https://bookdown.org/mike/data_analysis/}{\textbf{https://bookdown.org/mike/data\_analysis/}}
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\SpecialCharTok{@}\NormalTok{book\{nguyen2020guide,}
\NormalTok{  title}\OtherTok{=}\NormalTok{\{A Guide on Data Analysis\},}
\NormalTok{  author}\OtherTok{=}\NormalTok{\{Nguyen, Mike\},}
\NormalTok{  year}\OtherTok{=}\NormalTok{\{}\DecValTok{2020}\NormalTok{\},}
\NormalTok{  publisher}\OtherTok{=}\NormalTok{\{Bookdown\},}
\NormalTok{  url}\OtherTok{=}\NormalTok{\{https}\SpecialCharTok{:}\ErrorTok{//}\NormalTok{bookdown.org}\SpecialCharTok{/}\NormalTok{mike}\SpecialCharTok{/}\NormalTok{data\_analysis}\SpecialCharTok{/}\NormalTok{\}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{more-books}{%
\section*{More books}\label{more-books}}
\addcontentsline{toc}{section}{More books}

More books by the author can be found \href{https://mikenguyen.netlify.app/books/written_books/}{here}:

\begin{itemize}
\tightlist
\item
  \href{https://bookdown.org/mike/advanced_data_analysis/}{Advanced Data Analysis}: the second book in the data analysis series, which covers machine learning models (with a focus on prediction)
\item
  \href{https://bookdown.org/mike/marketing_research/}{Marketing Research}
\item
  \href{https://bookdown.org/mike/comm_theory/}{Communication Theory}
\end{itemize}

\hypertarget{introduction}{%
\chapter{Introduction}\label{introduction}}

Since the beginning of the century, we have been bombarded with amazing advancements and inventions, especially in the field of statistics, information technology, computer science, or a new emerging filed - data science. However, I believe the downside of this introduction is that we use \textbf{big} and \textbf{trendy} words too often (i.e., big data, machine learning, deep learning).

Each substantive field will have a metric subfield:

\begin{itemize}
\item
  Econometrics in economics
\item
  Psychometrics in psychology
\item
  Chemometrics in chemistry
\item
  Sabermetrics in sports
\item
  Biostatistics in public health and medicine
\end{itemize}

But to laymen, these are known as:

\begin{itemize}
\item
  Data Science
\item
  Applied Statistics
\item
  Computational Social Science
\end{itemize}

It's all fun and exciting when I learned these new tools. But I have to admit that I hardly retain any of these new ideas. However, writing down from the beginning till the end of a data analysis process is the solution that I came up with. Accordingly, let's dive right in.

\includegraphics[width=4.6875in,height=3.64583in]{images/meme.jpg}

\textbf{Some general recommendations}:

\begin{itemize}
\item
  The more you practice/habituate/condition, more line of codes that you write, more function that you memorize, I think the more you will like this journey.
\item
  Readers can follow this book several ways:

  \begin{itemize}
  \tightlist
  \item
    If you are interested in particular methods/tools, you can jump to that section by clicking the section name.
  \item
    If you want to follow a traditional path of data analysis, read the \protect\hyperlink{linear-regression}{Linear Regression} section.
  \item
    If you want to create your experiment and test your hypothesis, read the \protect\hyperlink{analysis-of-variance-anova}{Analysis of Variance (ANOVA)} section.
  \end{itemize}
\item
  Alternatively, if you rather see the application of models, and disregard any theory or underlying mechanisms, you can skip to summary and application portion of each section.
\item
  If you don't understand a part, search the title of that part of that part on Google, and read more into that subject. This is just a general guide.
\item
  If you want to customize your code beyond the ones provided in this book, run in the console \texttt{help(code)} or \texttt{?code}. For example, I want more information on \texttt{hist} function, I'll type in the console \texttt{?hist} or \texttt{help(hist)}.
\item
  Another way is that you can search on Google. Different people will use different packages to achieve the same result in R. Accordingly, if you want to create a histogram, search on Google \texttt{histogram\ in\ R}, then you should be able to find multiple ways to create histogram in R.
\end{itemize}

\textbf{Tools of statistics}

\begin{itemize}
\tightlist
\item
  Probability Theory
\item
  Mathematical Analysis
\item
  Computer Science
\item
  Numerical Analysis
\item
  Database Management
\end{itemize}

\textbf{Code Replication}

This book was built with R version 4.2.3 (2023-03-15 ucrt) and the following packages:

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
package & version & source \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
abind & 1.4-5 & CRAN (R 4.2.0) \\
agridat & 1.21 & CRAN (R 4.2.3) \\
ape & 5.7-1 & CRAN (R 4.2.3) \\
assertthat & 0.2.1 & CRAN (R 4.2.3) \\
backports & 1.4.1 & CRAN (R 4.2.0) \\
bookdown & 0.35 & CRAN (R 4.2.3) \\
boot & 1.3-28.1 & CRAN (R 4.2.3) \\
broom & 1.0.5 & CRAN (R 4.2.3) \\
bslib & 0.5.1 & CRAN (R 4.2.3) \\
cachem & 1.0.8 & CRAN (R 4.2.3) \\
callr & 3.7.3 & CRAN (R 4.2.3) \\
car & 3.1-2 & CRAN (R 4.2.3) \\
carData & 3.0-5 & CRAN (R 4.2.3) \\
cellranger & 1.1.0 & CRAN (R 4.2.3) \\
cli & 3.6.1 & CRAN (R 4.2.3) \\
coda & 0.19-4 & CRAN (R 4.2.3) \\
colorspace & 2.1-0 & CRAN (R 4.2.3) \\
corpcor & 1.6.10 & CRAN (R 4.2.0) \\
crayon & 1.5.2 & CRAN (R 4.2.3) \\
cubature & 2.1.0 & CRAN (R 4.2.3) \\
curl & 5.0.2 & CRAN (R 4.2.3) \\
data.table & 1.14.8 & CRAN (R 4.2.3) \\
DBI & 1.1.3 & CRAN (R 4.2.3) \\
dbplyr & 2.3.3 & CRAN (R 4.2.3) \\
desc & 1.4.2 & CRAN (R 4.2.3) \\
devtools & 2.4.5 & CRAN (R 4.2.3) \\
digest & 0.6.31 & CRAN (R 4.2.3) \\
dplyr & 1.1.2 & CRAN (R 4.2.3) \\
ellipsis & 0.3.2 & CRAN (R 4.2.3) \\
evaluate & 0.21 & CRAN (R 4.2.3) \\
extrafont & 0.19 & CRAN (R 4.2.2) \\
extrafontdb & 1.0 & CRAN (R 4.2.0) \\
fansi & 1.0.4 & CRAN (R 4.2.3) \\
faraway & 1.0.8 & CRAN (R 4.2.3) \\
fastmap & 1.1.1 & CRAN (R 4.2.3) \\
forcats & 1.0.0 & CRAN (R 4.2.3) \\
foreign & 0.8-84 & CRAN (R 4.2.3) \\
fs & 1.6.3 & CRAN (R 4.2.3) \\
generics & 0.1.3 & CRAN (R 4.2.3) \\
ggplot2 & 3.4.3 & CRAN (R 4.2.3) \\
glue & 1.6.2 & CRAN (R 4.2.3) \\
gtable & 0.3.3 & CRAN (R 4.2.3) \\
haven & 2.5.3 & CRAN (R 4.2.3) \\
Hmisc & 5.1-0 & CRAN (R 4.2.3) \\
hms & 1.1.3 & CRAN (R 4.2.3) \\
htmltools & 0.5.5 & CRAN (R 4.2.3) \\
htmlwidgets & 1.6.2 & CRAN (R 4.2.3) \\
httr & 1.4.7 & CRAN (R 4.2.3) \\
investr & 1.4.2 & CRAN (R 4.2.3) \\
jpeg & 0.1-10 & CRAN (R 4.2.2) \\
jquerylib & 0.1.4 & CRAN (R 4.2.3) \\
jsonlite & 1.8.7 & CRAN (R 4.2.3) \\
kableExtra & 1.3.4 & CRAN (R 4.2.3) \\
knitr & 1.43 & CRAN (R 4.2.3) \\
lattice & 0.21-8 & CRAN (R 4.2.3) \\
latticeExtra & 0.6-30 & CRAN (R 4.2.3) \\
lifecycle & 1.0.3 & CRAN (R 4.2.3) \\
lme4 & 1.1-34 & CRAN (R 4.2.3) \\
lmerTest & 3.1-3 & CRAN (R 4.2.3) \\
lsr & 0.5.2 & CRAN (R 4.2.3) \\
ltm & 1.2-0 & CRAN (R 4.2.3) \\
lubridate & 1.9.2 & CRAN (R 4.2.3) \\
magrittr & 2.0.3 & CRAN (R 4.2.3) \\
MASS & 7.3-60 & CRAN (R 4.2.3) \\
matlib & 0.9.6 & CRAN (R 4.2.3) \\
Matrix & 1.6-1 & CRAN (R 4.2.3) \\
MCMCglmm & 2.35 & CRAN (R 4.2.3) \\
memoise & 2.0.1 & CRAN (R 4.2.3) \\
mgcv & 1.9-0 & CRAN (R 4.2.3) \\
minqa & 1.2.5 & CRAN (R 4.2.3) \\
modelr & 0.1.11 & CRAN (R 4.2.3) \\
munsell & 0.5.0 & CRAN (R 4.2.3) \\
nlme & 3.1-163 & CRAN (R 4.2.3) \\
nloptr & 2.0.3 & CRAN (R 4.2.3) \\
nlstools & 2.0-0 & CRAN (R 4.2.3) \\
nnet & 7.3-19 & CRAN (R 4.2.3) \\
numDeriv & 2016.8-1.1 & CRAN (R 4.2.0) \\
openxlsx & 4.2.5.2 & CRAN (R 4.2.3) \\
pbkrtest & 0.5.2 & CRAN (R 4.2.3) \\
pillar & 1.9.0 & CRAN (R 4.2.3) \\
pkgbuild & 1.4.2 & CRAN (R 4.2.3) \\
pkgconfig & 2.0.3 & CRAN (R 4.2.3) \\
pkgload & 1.3.2.1 & CRAN (R 4.2.3) \\
png & 0.1-8 & CRAN (R 4.2.2) \\
ppsr & 0.0.2 & CRAN (R 4.2.3) \\
prettyunits & 1.1.1 & CRAN (R 4.2.3) \\
processx & 3.8.2 & CRAN (R 4.2.3) \\
ps & 1.7.5 & CRAN (R 4.2.3) \\
pscl & 1.5.5.1 & CRAN (R 4.2.3) \\
purrr & 1.0.2 & CRAN (R 4.2.3) \\
R6 & 2.5.1 & CRAN (R 4.2.3) \\
RColorBrewer & 1.1-3 & CRAN (R 4.2.0) \\
Rcpp & 1.0.11 & CRAN (R 4.2.3) \\
readr & 2.1.4 & CRAN (R 4.2.3) \\
readxl & 1.4.3 & CRAN (R 4.2.3) \\
remotes & 2.4.2.1 & CRAN (R 4.2.3) \\
reprex & 2.0.2 & CRAN (R 4.2.3) \\
rgl & 1.2.1 & CRAN (R 4.2.3) \\
rio & 0.5.29 & CRAN (R 4.2.3) \\
rlang & 1.1.1 & CRAN (R 4.2.3) \\
RLRsim & 3.1-8 & CRAN (R 4.2.3) \\
rmarkdown & 2.24 & CRAN (R 4.2.3) \\
rprojroot & 2.0.3 & CRAN (R 4.2.3) \\
rstudioapi & 0.15.0 & CRAN (R 4.2.3) \\
Rttf2pt1 & 1.3.12 & CRAN (R 4.2.2) \\
rvest & 1.0.3 & CRAN (R 4.2.3) \\
sass & 0.4.7 & CRAN (R 4.2.3) \\
scales & 1.2.1 & CRAN (R 4.2.3) \\
sessioninfo & 1.2.2 & CRAN (R 4.2.3) \\
stringi & 1.7.12 & CRAN (R 4.2.2) \\
stringr & 1.5.0 & CRAN (R 4.2.3) \\
svglite & 2.1.1 & CRAN (R 4.2.3) \\
systemfonts & 1.0.4 & CRAN (R 4.2.3) \\
tensorA & 0.36.2 & CRAN (R 4.2.0) \\
testthat & 3.1.10 & CRAN (R 4.2.3) \\
tibble & 3.2.1 & CRAN (R 4.2.3) \\
tidyr & 1.3.0 & CRAN (R 4.2.3) \\
tidyselect & 1.2.0 & CRAN (R 4.2.3) \\
tidyverse & 2.0.0 & CRAN (R 4.2.3) \\
tzdb & 0.4.0 & CRAN (R 4.2.3) \\
usethis & 2.2.2 & CRAN (R 4.2.3) \\
utf8 & 1.2.3 & CRAN (R 4.2.3) \\
vctrs & 0.6.3 & CRAN (R 4.2.3) \\
viridisLite & 0.4.2 & CRAN (R 4.2.3) \\
webshot & 0.5.5 & CRAN (R 4.2.3) \\
withr & 2.5.0 & CRAN (R 4.2.3) \\
xfun & 0.39 & CRAN (R 4.2.3) \\
xml2 & 1.3.5 & CRAN (R 4.2.3) \\
xtable & 1.8-4 & CRAN (R 4.2.3) \\
yaml & 2.3.7 & CRAN (R 4.2.3) \\
zip & 2.3.0 & CRAN (R 4.2.3) \\
\end{longtable}

\begin{verbatim}
#> - Session info ---------------------------------------------------------------
#>  setting  value
#>  version  R version 4.2.3 (2023-03-15 ucrt)
#>  os       Windows 10 x64 (build 22621)
#>  system   x86_64, mingw32
#>  ui       RTerm
#>  language (EN)
#>  collate  English_United States.utf8
#>  ctype    English_United States.utf8
#>  tz       America/Los_Angeles
#>  date     2023-08-21
#>  pandoc   3.1.1 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)
#> 
#> - Packages -------------------------------------------------------------------
#>  package     * version date (UTC) lib source
#>  bookdown      0.35    2023-08-09 [1] CRAN (R 4.2.3)
#>  cachem        1.0.8   2023-05-01 [1] CRAN (R 4.2.3)
#>  callr         3.7.3   2022-11-02 [1] CRAN (R 4.2.3)
#>  cli           3.6.1   2023-03-23 [1] CRAN (R 4.2.3)
#>  codetools     0.2-19  2023-02-01 [1] CRAN (R 4.2.3)
#>  colorspace    2.1-0   2023-01-23 [1] CRAN (R 4.2.3)
#>  crayon        1.5.2   2022-09-29 [1] CRAN (R 4.2.3)
#>  desc          1.4.2   2022-09-08 [1] CRAN (R 4.2.3)
#>  devtools      2.4.5   2022-10-11 [1] CRAN (R 4.2.3)
#>  digest        0.6.31  2022-12-11 [1] CRAN (R 4.2.3)
#>  dplyr       * 1.1.2   2023-04-20 [1] CRAN (R 4.2.3)
#>  ellipsis      0.3.2   2021-04-29 [1] CRAN (R 4.2.3)
#>  evaluate      0.21    2023-05-05 [1] CRAN (R 4.2.3)
#>  fansi         1.0.4   2023-01-22 [1] CRAN (R 4.2.3)
#>  fastmap       1.1.1   2023-02-24 [1] CRAN (R 4.2.3)
#>  forcats     * 1.0.0   2023-01-29 [1] CRAN (R 4.2.3)
#>  fs            1.6.3   2023-07-20 [1] CRAN (R 4.2.3)
#>  generics      0.1.3   2022-07-05 [1] CRAN (R 4.2.3)
#>  ggplot2     * 3.4.3   2023-08-14 [1] CRAN (R 4.2.3)
#>  glue          1.6.2   2022-02-24 [1] CRAN (R 4.2.3)
#>  gtable        0.3.3   2023-03-21 [1] CRAN (R 4.2.3)
#>  hms           1.1.3   2023-03-21 [1] CRAN (R 4.2.3)
#>  htmltools     0.5.5   2023-03-23 [1] CRAN (R 4.2.3)
#>  htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.2.3)
#>  httpuv        1.6.11  2023-05-11 [1] CRAN (R 4.2.3)
#>  jpeg        * 0.1-10  2022-11-29 [1] CRAN (R 4.2.2)
#>  knitr         1.43    2023-05-25 [1] CRAN (R 4.2.3)
#>  later         1.3.1   2023-05-02 [1] CRAN (R 4.2.3)
#>  lifecycle     1.0.3   2022-10-07 [1] CRAN (R 4.2.3)
#>  lubridate   * 1.9.2   2023-02-10 [1] CRAN (R 4.2.3)
#>  magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.2.3)
#>  memoise       2.0.1   2021-11-26 [1] CRAN (R 4.2.3)
#>  mime          0.12    2021-09-28 [1] CRAN (R 4.2.0)
#>  miniUI        0.1.1.1 2018-05-18 [1] CRAN (R 4.2.3)
#>  munsell       0.5.0   2018-06-12 [1] CRAN (R 4.2.3)
#>  pillar        1.9.0   2023-03-22 [1] CRAN (R 4.2.3)
#>  pkgbuild      1.4.2   2023-06-26 [1] CRAN (R 4.2.3)
#>  pkgconfig     2.0.3   2019-09-22 [1] CRAN (R 4.2.3)
#>  pkgload       1.3.2.1 2023-07-08 [1] CRAN (R 4.2.3)
#>  prettyunits   1.1.1   2020-01-24 [1] CRAN (R 4.2.3)
#>  processx      3.8.2   2023-06-30 [1] CRAN (R 4.2.3)
#>  profvis       0.3.8   2023-05-02 [1] CRAN (R 4.2.3)
#>  promises      1.2.1   2023-08-10 [1] CRAN (R 4.2.3)
#>  ps            1.7.5   2023-04-18 [1] CRAN (R 4.2.3)
#>  purrr       * 1.0.2   2023-08-10 [1] CRAN (R 4.2.3)
#>  R6            2.5.1   2021-08-19 [1] CRAN (R 4.2.3)
#>  Rcpp          1.0.11  2023-07-06 [1] CRAN (R 4.2.3)
#>  readr       * 2.1.4   2023-02-10 [1] CRAN (R 4.2.3)
#>  remotes       2.4.2.1 2023-07-18 [1] CRAN (R 4.2.3)
#>  rlang         1.1.1   2023-04-28 [1] CRAN (R 4.2.3)
#>  rmarkdown     2.24    2023-08-14 [1] CRAN (R 4.2.3)
#>  rprojroot     2.0.3   2022-04-02 [1] CRAN (R 4.2.3)
#>  rstudioapi    0.15.0  2023-07-07 [1] CRAN (R 4.2.3)
#>  scales      * 1.2.1   2022-08-20 [1] CRAN (R 4.2.3)
#>  sessioninfo   1.2.2   2021-12-06 [1] CRAN (R 4.2.3)
#>  shiny         1.7.5   2023-08-12 [1] CRAN (R 4.2.3)
#>  stringi       1.7.12  2023-01-11 [1] CRAN (R 4.2.2)
#>  stringr     * 1.5.0   2022-12-02 [1] CRAN (R 4.2.3)
#>  tibble      * 3.2.1   2023-03-20 [1] CRAN (R 4.2.3)
#>  tidyr       * 1.3.0   2023-01-24 [1] CRAN (R 4.2.3)
#>  tidyselect    1.2.0   2022-10-10 [1] CRAN (R 4.2.3)
#>  tidyverse   * 2.0.0   2023-02-22 [1] CRAN (R 4.2.3)
#>  timechange    0.2.0   2023-01-11 [1] CRAN (R 4.2.3)
#>  tzdb          0.4.0   2023-05-12 [1] CRAN (R 4.2.3)
#>  urlchecker    1.0.1   2021-11-30 [1] CRAN (R 4.2.3)
#>  usethis       2.2.2   2023-07-06 [1] CRAN (R 4.2.3)
#>  utf8          1.2.3   2023-01-31 [1] CRAN (R 4.2.3)
#>  vctrs         0.6.3   2023-06-14 [1] CRAN (R 4.2.3)
#>  withr         2.5.0   2022-03-03 [1] CRAN (R 4.2.3)
#>  xfun          0.39    2023-04-20 [1] CRAN (R 4.2.3)
#>  xtable        1.8-4   2019-04-21 [1] CRAN (R 4.2.3)
#>  yaml          2.3.7   2023-01-23 [1] CRAN (R 4.2.3)
#> 
#>  [1] C:/Program Files/R/R-4.2.3/library
#> 
#> ------------------------------------------------------------------------------
\end{verbatim}

\hypertarget{prerequisites}{%
\chapter{Prerequisites}\label{prerequisites}}

This chapter is just a quick review of \protect\hyperlink{matrix-theory}{Matrix Theory} and \protect\hyperlink{probability-theory}{Probability Theory}

If you feel you do not need to brush up on these theories, you can jump right into \protect\hyperlink{descriptive-stat}{Descriptive Statistics}

\hypertarget{matrix-theory}{%
\section{Matrix Theory}\label{matrix-theory}}

\[
A=
\left[
\begin{array}
{cc}
a_{11} & a_{12} \\
a_{21} & a_{22}
\end{array}
\right]
\]

\[
A' =
\left[
\begin{array}
{cc}
a_{11} & a_{21} \\
a_{12} & a_{22}
\end{array}
\right]
\]

\[
\begin{aligned}
\mathbf{(ABC)'}   & = \mathbf{C'B'A'} \\
\mathbf{A(B+C)}   & = \mathbf{AB + AC} \\
\mathbf{AB}       & \neq \mathbf{BA} \\
\mathbf{(A')'}    & = \mathbf{A} \\
\mathbf{(A+B)'}   & = \mathbf{A' + B'} \\
\mathbf{(AB)'}    & = \mathbf{B'A'} \\
\mathbf{(AB)^{-1}} & = \mathbf{B^{-1}A^{-1}} \\
\mathbf{A+B}      & = \mathbf{B +A} \\
\mathbf{AA^{-1}}  & = \mathbf{I}
\end{aligned}
\]

If A has an inverse, it is called \textbf{invertible.} If A is not invertible it is called \textbf{singular.}

\[
\begin{aligned}
\mathbf{A} &= 
\left(\begin{array}
{ccc} 
a_{11} & a_{12} & a_{13} \\ 
a_{21} & a_{22} & a_{23} \\ 
\end{array}\right)
\left(\begin{array}
{ccc}
b_{11} & b_{12} & b_{13} \\
b_{21} & b_{22} & b_{23} \\
b_{31} & b_{32} & b_{33} \\
\end{array}\right) \\
&= 
\left(\begin{array}
{ccc}
a_{11}b_{11}+a_{12}b_{21}+a_{13}b_{31} & \sum_{i=1}^{3}a_{1i}b_{i2} &  \sum_{i=1}^{3}a_{1i}b_{i3} \\
\sum_{i=1}^{3}a_{2i}b_{i1} & \sum_{i=1}^{3}a_{2i}b_{i2} & \sum_{i=1}^{3}a_{2i}b_{i3} \\
\end{array}\right) 
\end{aligned}
\]

Let \(\mathbf{a}\) be a \(3 \times 1\) vector, then the quadratic form is

\[
\mathbf{a'Ba} = \sum_{i=1}^{3}\sum_{i=1}^{3}a_i b_{ij} a_{j}
\]

\textbf{Length of a vector}\\
Let \(\mathbf{a}\) be a vector, \(||\mathbf{a}||\) (the 2-norm of the vector) is the length of vector \(\mathbf{a}\), is the square root of the inner product of the vector with itself:

\[
||\mathbf{a}|| = \sqrt{\mathbf{a'a}}
\]

\hypertarget{rank}{%
\subsection{Rank}\label{rank}}

\begin{itemize}
\tightlist
\item
  Dimension of space spanned by its columns (or its rows).
\item
  Number of linearly independent columns/rows
\end{itemize}

For a \(n \times k\) matrix \textbf{A} and \(k \times k\) matrix \textbf{B}

\begin{itemize}
\tightlist
\item
  \(rank(A)\leq min(n,k)\)
\item
  \(rank(A) = rank(A') = rank(A'A)=rank(AA')\)
\item
  \(rank(AB)=min(rank(A),rank(B))\)
\item
  \textbf{B} is invertible if and only if \(rank(B) = k\) (non-singular)
\end{itemize}

\hypertarget{inverse}{%
\subsection{Inverse}\label{inverse}}

In scalar, \(a = 0\) then \(1/a\) does not exist.

In matrix, a matrix is invertible when it's a non-zero matrix.

A non-singular square matrix \(\mathbf{A}\) is invertible if there exists a non-singular square matrix \(\mathbf{B}\) such that, \[AB=I\] Then \(A^{-1}=B\). For a \(2\times2\) matrix,

\[
A =
\left(\begin{array}{cc}
a & b \\
c & d \\
\end{array}
\right)
\]

\[
A^{-1}=
\frac{1}{ad-bc}
\left(\begin{array}{cc}
d & -b \\
-c & a \\
\end{array}
\right)
\]

For the partition matrix,

\[
\left[\begin{array}
{cc}
A & B \\
C & D \\
\end{array}
\right]^{-1}
 =
\left[\begin{array}
{cc}
\mathbf{(A-BD^{-1}C)^{-1}} & \mathbf{-(A-BD^{-1}C)^{-1}BD^-1} \\
\mathbf{-DC(A-BD^{-1}C)^{-1}} & \mathbf{D^{-1}+D^{-1}C(A-BD^{-1}C)^{-1}BD^{-1}}
\end{array}
\right]
\]

Properties for a non-singular square matrix

\begin{itemize}
\tightlist
\item
  \(\mathbf{A^{-1}}=A\)
\item
  for a non-zero scalar b, \(\mathbf{(bA)^{-1}=b^{-1}A^{-1}}\)
\item
  for a matrix B, \(\mathbf(BA)^{-1}=B^{-1}A^{-1}\) only if \(\mathbf{B}\) is non-singular
\item
  \(\mathbf{(A^{-1})'=(A')^{-1}}\)
\item
  Never notate \(\mathbf{1/A}\)
\end{itemize}

\hypertarget{definiteness}{%
\subsection{Definiteness}\label{definiteness}}

A symmetric square k x k matrix, \(\mathbf{A}\), is Positive Semi-Definite if for any non-zero \(k \times 1\) vector \(\mathbf{x}\), \[\mathbf{x'Ax \geq 0 }\]

A symmetric square k x k matrix, \(\mathbf{A}\), is Negative Semi-Definite if for any non-zero \(k \times 1\) vector \(\mathbf{x}\) \[\mathbf{x'Ax \leq 0 }\]

\(\mathbf{A}\) is indefinite if it is neither positive semi-definite or negative semi-definite.

The identity matrix is positive definite

\textbf{Example} Let \(\mathbf{x} =(x_1 x_2)'\), then for a \(2 \times 2\) identity matrix,

\[
\begin{aligned}
\mathbf{x'Ix} 
&= (x_1 x_2) 
\left(\begin{array}
{cc}
1 & 0 \\
0 & 1 \\
\end{array}
\right)
\left(\begin{array}{c}
x_1 \\
x_2 \\
\end{array}
\right) \\
&=
(x_1 x_2)
\left(\begin{array}
{c}
x_1 \\
x_2 \\
\end{array}
\right) \\
&=
x_1^2 + x_2^2 >0
\end{aligned}
\]

Definiteness gives us the ability to compare matrices \(\mathbf{A-B}\) is PSD

This property also helps us show efficiency (which variance covariance matrix of one estimator is smaller than another)

Properties

\begin{itemize}
\tightlist
\item
  any variance matrix is PSD
\item
  a matrix \(\mathbf{A}\) is PSD if and only if there exists a matrix \(\mathbf{B}\) such that \(\mathbf{A=B'B}\)
\item
  if \(\mathbf{A}\) is PSD, then \(\mathbf{B'AB}\) is PSD
\item
  if \(\mathbf{A}\) and \(\mathbf{C}\) are non-singular, then \(\mathbf{A-C}\) is PSD if and only if \(\mathbf{C^{-1}-A^{-1}}\)
\item
  if \(\mathbf{A}\) is PD (ND) then \(A^{-1}\) is PD (ND)
\end{itemize}

\textbf{Note}

\begin{itemize}
\tightlist
\item
  Indefinite \(\mathbf{A}\) is neither PSD nor NSD. There is no comparable concept in scalar.
\item
  If a square matrix is PSD and invertible then it is PD
\end{itemize}

Example:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Invertible / Indefinite
\end{enumerate}

\[
\left[
\begin{array}
{cc}
-1 & 0 \\
0 & 10 
\end{array}
\right]
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Non-invertible/ Indefinite
\end{enumerate}

\[
\left[
\begin{array}
{cc}
0 & 1 \\
0 & 0
\end{array}
\right]
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Invertible / PSD
\end{enumerate}

\[
\left[
\begin{array}
{cc}
1 & 0 \\
0 & 1
\end{array}
\right]
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Non-Invertible / PSD
\end{enumerate}

\[
\left[
\begin{array}
{cc}
0 & 0 \\
0 & 1 
\end{array}
\right]
\]

\hypertarget{matrix-calculus}{%
\subsection{Matrix Calculus}\label{matrix-calculus}}

\(y=f(x_1,x_2,...,x_k)=f(x)\) where \(x\) is a \(1 \times k\) row vector.

The Gradient (first order derivative with respect to a vector) is,

\[
\frac{\partial{f(x)}}{\partial{x}}=
\left(\begin{array}{c}
\frac{\partial{f(x)}}{\partial{x_1}} \\
\frac{\partial{f(x)}}{\partial{x_2}} \\
\dots \\
\frac{\partial{f(x)}}{\partial{x_k}}
\end{array}
\right)
\]

The \textbf{Hessian} (second order derivative with respect to a vector) is,

\[
\frac{\partial^2{f(x)}}{\partial{x}\partial{x'}}=
\left(\begin{array}
{cccc}
\frac{\partial^2{f(x)}}{\partial{x_1}\partial{x_1}} & \frac{\partial^2{f(x)}}{\partial{x_1}\partial{x_2}} & \dots & \frac{\partial^2{f(x)}}{\partial{x_1}\partial{x_k}} \\
\frac{\partial^2{f(x)}}{\partial{x_1}\partial{x_2}} & \frac{\partial^2{f(x)}}{\partial{x_2}\partial{x_2}} & \dots & \frac{\partial^2{f(x)}}{\partial{x_2}\partial{x_k}} \\
\dots & \dots & \ddots & \dots\\
\frac{\partial^2{f(x)}}{\partial{x_k}\partial{x_1}} & \frac{\partial^2{f(x)}}{\partial{x_k}\partial{x_2}} & \dots & \frac{\partial^2{f(x)}}{\partial{x_k}\partial{x_k}}
\end{array}
\right)
\]

Define the derivative of \(f(\mathbf{X})\) with respect to \(\mathbf{X}_{(n \times p)}\) as the matrix

\[
\frac{\partial f(\mathbf{X})}{\partial \mathbf{X}} = (\frac{\partial f(\mathbf{X})}{\partial x_{ij}})
\]

Define \(\mathbf{a}\) to be a vector and \(\mathbf{A}\) to be a matrix which does not depend upon \(\mathbf{y}\). Then

\[
\frac{\partial \mathbf{a'y}}{\partial \mathbf{y}} = \mathbf{a}
\]

\[
\frac{\partial \mathbf{y'y}}{\partial \mathbf{y}} = 2\mathbf{y}
\]

\[
\frac{\partial \mathbf{y'Ay}}{\partial \mathbf{y}} = \mathbf{(A + A')y}
\]

If \(\mathbf{X}\) is a symmetric matrix then

\[
\frac{\partial |\mathbf{X}|}{\partial x_{ij}} = 
\begin{cases}
X_{ii}, i = j \\
X_{ij}, i \neq j
\end{cases}
\]

where \(X_{ij}\) is the \((i,j)\)-th cofactor of \(\mathbf{X}\)

If \(\mathbf{X}\) is symmetric and \(\mathbf{A}\) is a matrix which does not depend upon \(\mathbf{X}\) then

\[
\frac{\partial tr \mathbf{XA}}{\partial \mathbf{X}} = \mathbf{A} + \mathbf{A}' - diag(\mathbf{A})
\]

If \(\mathbf{X}\) is symmetric and we let \(\mathbf{J}_{ij}\) be a matrix which has a 1 in the \((i,j)\)-th position and 0s elsewhere, then

\[
\frac{\partial \mathbf{X}6{-1}}{\partial x_{ij}} = 
\begin{cases}
- \mathbf{X}^{-1}\mathbf{J}_{ii} \mathbf{X}^{-1} &, i = j \\
- \mathbf{X}^{-1}(\mathbf{J}_{ij} + \mathbf{J}_{ji}) \mathbf{X}^{-1} &, i \neq j
\end{cases}
\]

\hypertarget{optimization}{%
\subsection{Optimization}\label{optimization}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1895}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2684}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5368}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\textbf{Scalar Optimization}
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\textbf{Vector Optimization}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
First Order Condition & \(\frac{\partial{f(x_0)}}{\partial{x}}=0\) & \(\frac{\partial{f(x_0)}}{\partial{x}}=\left(\begin{array}{c}0 \\ .\\ .\\ .\\ 0\end{array}\right)\) \\
Second Order Condition

\textbf{Convex} \(\rightarrow\) \textbf{Min} & \(\frac{\partial^2{f(x_0)}}{\partial{x^2}} > 0\) & \(\frac{\partial^2{f(x_0)}}{\partial{xx'}}>0\) \\
\textbf{Concave} \(\rightarrow\) \textbf{Max} & \(\frac{\partial^2{f(x_0)}}{\partial{x^2}} < 0\) & \(\frac{\partial^2{f(x_0)}}{\partial{xx'}}<0\) \\
\end{longtable}

\hypertarget{probability-theory}{%
\section{Probability Theory}\label{probability-theory}}

\hypertarget{axiom-and-theorems-of-probability}{%
\subsection{Axiom and Theorems of Probability}\label{axiom-and-theorems-of-probability}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Let \(S\) denote a sample space of an experiment \(P[S]=1\)
\item
  \(P[A] \ge 0\) for every event \(A\)
\item
  Let \(A_1,A_2,A_3,...\) be a finite or an infinite collection of mutually exclusive events. Then \(P[A_1\cup A_2 \cup A_3 ...]=P[A_1]+P[A_2]+P[A_3]+...\)
\item
  \(P[\emptyset]=0\)
\item
  \(P[A']=1-P[A]\)
\item
  \(P[A_1 \cup A_2] = P[A_1] + P[A_2] - P[A_1 \cap A_2]\)
\end{enumerate}

\textbf{Conditional Probability}

\[
P[A|B]=\frac{A \cap B}{P[B]}
\]

\textbf{Independent Events} Two events \(A\) and \(B\) are independent if and only if:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(P[A\cap B]=P[A]P[B]\)
\item
  \(P[A|B]=P[A]\)
\item
  \(P[B|A]=P[B]\)
\end{enumerate}

A finite collection of events \(A_1, A_2, ..., A_n\) is independent if and only if any subcollection is independent.

\textbf{Multiplication Rule} \(P[A \cap B] = P[A|B]P[B] = P[B|A]P[A]\)

\textbf{Bayes' Theorem}

Let \(A_1, A_2, ..., A_n\) be a collection of mutually exclusive events whose union is S.\\
Let \(b\) be an event such that \(P[B]\neq0\)\\
Then for any of the events \(A_j\), \(j = 1,2,â€¦,n\)

\[
P[A_|B]=\frac{P[B|A_j]P[A_j]}{\sum_{i=1}^{n}P[B|A_j]P[A_i]}
\]

\textbf{Jensen's Inequality}

\begin{itemize}
\tightlist
\item
  If \(g(x)\) is convex \(E(g(X)) \ge g(E(X))\)
\item
  If \(g(x)\) is concave \(E(g(X)) \le g(E(X))\)
\end{itemize}

\hypertarget{law-of-iterated-expectations}{%
\subsubsection{Law of Iterated Expectations}\label{law-of-iterated-expectations}}

\(E(Y)=E(E(Y|X))\)

\hypertarget{correlation-and-independence}{%
\subsubsection{Correlation and Independence}\label{correlation-and-independence}}

Strongest to Weakest

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Independence
\item
  Mean Independence
\item
  Uncorrelated
\end{enumerate}

\textbf{Independence}

\begin{itemize}
\tightlist
\item
  \(f(x,y)=f_X(x)f_Y(y)\)
\item
  \(f_{Y|X}(y|x)=f_Y(y)\) and \(f_{X|Y}(x|y)=f_X(x)\)
\item
  \(E(g_1(X)g_2(Y))=E(g_1(X))E(g_2(Y))\)
\end{itemize}

\textbf{Mean Independence} (implied by independence)

\begin{itemize}
\tightlist
\item
  \(Y\) is mean independent of \(X\) if and only if \(E(Y|X)=E(Y)\)
\item
  \(E(Xg(Y))=E(X)E(g(Y))\)
\end{itemize}

\textbf{Uncorrelated} (implied by independence and mean independence)

\begin{itemize}
\tightlist
\item
  \(Cov(X,Y)=0\)
\item
  \(Var(X+Y)=Var(X) + Var(Y)\)
\item
  \(E(XY)=E(X)E(Y)\)
\end{itemize}

\hypertarget{central-limit-theorem}{%
\subsection{Central Limit Theorem}\label{central-limit-theorem}}

Let \(X_1, X_2,...,X_n\) be a random sample of size \(n\) from a distribution (not necessarily normal) \(X\) with mean \(\mu\) and variance \(\sigma^2\). then for large n (\(n \ge 25\)),

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \(\bar{X}\) is approximately normal with with mean \(\mu_{\bar{X}}=\mu\) and variance \(\sigma^2_{\bar{X}} = Var(\bar{X})= \frac{\sigma^2}{n}\)
\item
  \(\hat{p}\) is approximately normal with \(\mu_{\hat{p}} = p, \sigma^2_{\hat{p}} = \frac{p(1-p)}{n}\)
\item
  \(\hat{p_1} - \hat{p_2}\) is approximately normal with \(\mu_{\hat{p_1} - \hat{p_2}} = p_1 - p_2, \sigma^2_{\hat{p_1} - \hat{p_2}}=\frac{p_1(1-p)}{n_1} + \frac{p_2(1-p)}{n_2}\)
\item
  \(\bar{X_1} - \bar{X_2}\) is approximately normal with \(\mu_{\bar{X_1} - \bar{X_2}} = \mu_1 - \mu_2, \sigma^2_{\bar{X_1} - \bar{X_2}} = \frac{\sigma^2_1}{n_1}+\frac{\sigma^2_2}{n_2}\)
\item
  The following random variables are approximately standard normal:\\

  \begin{itemize}
  \tightlist
  \item
    \(\frac{\bar{X}-\mu}{\sigma/\sqrt{n}}\)\\
  \item
    \(\frac{\hat{p}-p}{\sqrt{\frac{p(1-p)}{n}}}\)\\
  \item
    \(\frac{(\hat{p_1}-\hat{p_2})-(p_1-p_2)}{\sqrt{\frac{p_1(1-p_1)}{n_1}-\frac{p_2(1-p_2)}{n_2}}}\)\\
  \item
    \(\frac{(\bar{X_1}-\bar{X_2})-(\mu_1-\mu_2)}{\sqrt{\frac{\sigma^2_1}{n_1}-\frac{\sigma^2_2}{n_2}}}\)
  \end{itemize}
\end{enumerate}

If \(\{x_i\}_{i=1}^{n}\) is an iid random sample from a probability distribution with finite mean \(\mu\) and finite variance \(\sigma^2\) then the sample mean \(\bar{x}=n^{-1}\sum_{i=1}^{n}x_i\) scaled by \(\sqrt{n}\) has the following limiting distribution

\[
\sqrt{n}(\bar{x}-\mu) \to^d N(0,\sigma^2)
\]

or if we were to standardize the sample mean,

\[
\frac{\sqrt{n}(\bar{x}-\mu)}{\sigma} \to^d N(0,1)
\]

\begin{itemize}
\tightlist
\item
  holds for most random sample from any distribution (continuous, discrete, unknown).\\
\item
  extends to multivariate case: random sample of a random vector converges to a multivariate normal.\\
\item
  Variance from the limiting distribution is the asymptotic variance (Avar)
\end{itemize}

\[
\begin{aligned}
Avar(\sqrt{n}(\bar{x}-\mu)) &= \sigma^2 \\
\lim_{n \to \infty} Var(\sqrt{n}(\bar{x}-\mu)) &= \sigma^2 \\
Avar(.) &\neq lim_{n \to \infty} Var(.)
\end{aligned}
\]

Specifically, the last statement means that in some cases the asymptotic variance of a statistic is not necessarily equal to the limiting varinace of that statistic, such as

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Sample Quantiles}: Consider the sample quantile of order \(p\), for some \(0 < p <1\). Under regularity conditions, the asymptotic distribution of the sample quantile is normal with a variance that depends on \(p\) and the density of the distribution at the \(p\)-th quantile. However, the variance of the sample quantile itself does not necessarily converge to this limit as the sample size grows.
\item
  \textbf{Bootstrap Methods}: When using bootstrapping techniques to estimate the distribution of a statistic, the bootstrap distribution might converge to a different limiting distribution than the original statistic. In these cases, the variance of the bootstrap distribution (or the bootstrap variance) might differ from the limiting variance of the original statistic.
\item
  \textbf{Statistics with Randomly Varying Asymptotic Behavior}: In some cases, the asymptotic behavior of a statistic can vary randomly depending on the sample path. For such statistics, the asymptotic variance might not provide a consistent estimate of the limiting variance.
\item
  \textbf{M-estimators with Varying Asymptotic Behavior}: M-estimators can sometimes have different asymptotic behaviors depending on the tail behavior of the underlying distribution. For heavy-tailed distributions, the variance of the estimator might not stabilize even as the sample size grows large, making the asymptotic variance different from the variance of any limiting distribution.
\end{enumerate}

\hypertarget{random-variable}{%
\subsection{Random variable}\label{random-variable}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2440}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3056}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4477}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Discrete Variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Continuous Variable
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Definition} & A random variable is discrete if it can assume at most a finite or countably infinite number of possible values & A random variable is continuous if it can assume any value in some interval or intervals of real numbers and the probability that it assumes any specific value is 0 \\
\textbf{Density Function} & \begin{minipage}[t]{\linewidth}\raggedright
A function \(f\) is called a density for \(X\) if:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(f(x) \ge 0\)
\item
  \(\sum_{all~x}f(x)=1\)
\item
  \(f(x)=P(X=x)\) for \(x\) real
\end{enumerate}
\end{minipage} & \begin{minipage}[t]{\linewidth}\raggedright
A function \(f\) is called a density for \(X\) if:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(f(x) \ge 0\) for \(x\) real
\item
  \(\int_{-\infty}^{\infty} f(x) \; dx=1\)
\item
  \(P[a \le X \le] =\int_{a}^{b} f(x) \; dx\) for \(a,b\) real
\end{enumerate}
\end{minipage} \\
\textbf{Cumulative Distribution Function} for \(x\) real & \(F(x)=P[X \le x]\) & \(F(x)=P[X \le x]=\int_{-\infty}^{\infty}f(t)dt\) \\
\(E[H(X)]\) & \(\sum_{all ~x}H(x)f(x)\) & \(\int_{-\infty}^{\infty}H(x)f(x)\) \\
\(\mu=E[X]\) & \(\sum_{all ~ x}xf(x)\) & \(\int_{-\infty}^{\infty}xf(x)\) \\
\textbf{Ordinary Moments} the \(k\)-th ordinary moment for variable \(X\) is defined as: \(E[X^k]\) & \(\sum_{all ~ x \in X}(x^kf(x))\) & \(\int_{-\infty}^{\infty}(x^kf(x))\) \\
\textbf{Moment generating function (mgf)} \(m_X(t)=E[e^{tX}]\) & \(\sum_{all ~ x \in X}(e^{tx}f(x))\) & \(\int_{-\infty}^{\infty}(e^{tx}f(x)dx)\) \\
\end{longtable}

Expected Value Properties:

\begin{itemize}
\tightlist
\item
  \(E[c] = c\) for any constant \(c\)
\item
  \(E[cX] = cE[X\){]} for any constant \(c\)
\item
  \(E[X+Y] = E[X] = E[Y]\)
\item
  \(E[XY] = E[X].E[Y]\) (if \(X\) and \(Y\) are independent)
\end{itemize}

Expected Variance Properties:

\begin{itemize}
\tightlist
\item
  \(Var(c) = 0\) for any constant \(c\)
\item
  \(Var(cX) = c^2Var(X)\) for any constant \(c\)
\item
  \(Var(X) \ge 0\)
\item
  \(Var(X) = E(X^2) - (E(X))^2\)
\item
  \(Var(X+c)=Var(X)\)
\item
  \(Var (X+Y) = Var(X) + Var(Y)\) (if \(X\) and \(Y\) are independent)
\end{itemize}

Standard deviation \(\sigma=\sqrt(\sigma^2)=\sqrt(Var X)\)

Suppose \(y_1,...,y_p\) are possibly correlated random variables with means \(\mu_1,...,\mu_p\). then

\[
\begin{aligned}
\mathbf{y} &= (y_1,...,y_p)'  \\
E(\mathbf{y}) &= (\mu_1,...,\mu_p)' = \mathbf{\mu}
\end{aligned}
\]

Let \(\sigma_{ij} = cov(y_i,y_j)\) for \(i,j = 1,..,p\).

Define

\[
\mathbf{\Sigma} = (\sigma_{ij}) = 
\left(\begin{array}
{rrrr}
\sigma_{11} & \sigma_{12} & \dots & \sigma_{1p} \\
\sigma_{21} & \sigma_{22} & \dots & \sigma_{2p} \\
. & . & . & . \\
\sigma_{p1} & \sigma_{p2} & \dots & \sigma_{pp}\\
\end{array}\right)
\]

Hence, \(\mathbf{\Sigma}\) is the variance-covariance or dispersion matrix.

And \(\mathbf{\Sigma}\) is symmetric with \((p+1)p/2\) unique parameters.

Alternatively, let \(u_{p \times 1}\) and \(v_{v \times 1}\) be random vectors with means \(\mathbf{\mu_u}\) and \(\mathbf{\mu_v}\). then

\[
\mathbf{\Sigma_{uv}} = cov(\mathbf{u,v}) = E[\mathbf{(u-\mu_u)(v-\mu_v)'}]
\]

\(\Sigma_{uv} \neq \Sigma_{vu}\) (but \(\Sigma_{uv} = \Sigma_{vu}'\))

Properties of Covariance Matrices

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Symmetric: \(\mathbf{\Sigma' = \Sigma}\)\\
\item
  Eigen-decomposition (spectral decomposition,symmetric decomposition): \(\mathbf{\Sigma = \Phi \Lambda \Phi}\), where \(\mathbf{\Phi}\) is a matrix of eigenvectors such that \(\mathbf{\Phi \Phi' = I}\) (orthonormal), and \(\mathbf{\Lambda}\) is a diagonal matrix with eigenvalues \((\lambda_1,...,\lambda_p)\) on the diagonal.\\
\item
  Non-negative definite, \(\mathbf{a \Sigma a} \ge 0\) for any \(\mathbf{a} \in R^p\). Equivalently, the eigenvalues of \(\mathbf{\Sigma}\), \(\lambda_1 \ge ... \ge \lambda_p \ge 0\)\\
\item
  \(|\mathbf{\Sigma}| = \lambda_1,...\lambda_p \ge 0\) (generalized variance)\\
\item
  \(trace(\mathbf{\Sigma})= tr(\mathbf{\Sigma}) = \lambda_1 +... + \lambda_p = \sigma_{11}+...+ \sigma_{pp}\)= sum of variances (total variance)
\end{enumerate}

\textbf{Note}: \(\mathbf{\Sigma}\) is usually required to be positive definite. This implies that all eigenvalues are positive, and \(\mathbf{\Sigma}\) has an inverse \(\mathbf{\Sigma}^{-1}\), such that \(\mathbf{\Sigma}^{-1}\mathbf{\Sigma}= \mathbf{I}_{p \times p} = \mathbf{\Sigma}\mathbf{\Sigma}^{-1}\)

\textbf{Correlation Matrices}

Define the correlation \(\rho_{ij}\) and the correlation matrix by

\[
\rho_{ij} = \frac{\sigma_{ij}}{\sqrt{\sigma_{ii} \sigma_{jj}}}
\]

\[
\mathbf{R} = \left(
\begin{array}
{cccc}
\rho_{11} & \rho_{12} & \dots & \rho_{1p} \\
\rho_{21} & \rho_{22} & \dots & \rho_{2p} \\
\vdots & vdots & \ddots & vdots \\
\rho_{p1} & \rho_{p2} & \dots & \rho_{pp}\\
\end{array}
\right)
\]

where \(\rho_{ii}=1\) for all \(i\).

Let \(x\) and \(y\) be random vectors with means \(\mu_x\) and \(\mu_y\) and variance-covariance matrices \(\Sigma_x\) and \(\Sigma_y\).

Let \(\mathbf{A}\) and \(\mathbf{B}\) be matrices of constants and \(c\) and \(d\) be vectors of constants. Then,

\begin{itemize}
\tightlist
\item
  \(E(\mathbf{Ay+c}) = \mathbf{A \mu_y} +c\)
\item
  \(var(\mathbf{Ay +c}) = \mathbf{A}var(\mathbf{y}) \mathbf{A}' = \mathbf{A \Sigma_y A'}\)
\item
  \(cov(\mathbf{Ay + c,By+d} = \mathbf{A \Sigma_y B'}\)
\end{itemize}

\hypertarget{moment-generating-function}{%
\subsection{Moment generating function}\label{moment-generating-function}}

Moment generating function properties:

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\tightlist
\item
  \(\frac{d^k(m_X(t))}{dt^k}|_{t=0}=E[X^k]\)\\
\item
  \(\mu=E[X]=m_X'(0)\)\\
\item
  \(E[X^2]=m_X''(0)\)
\end{enumerate}

\textbf{mgf Theorems}

Let \(X_1,X_2,...X_n,Y\) be random variables with moment-generating functions \(m_{X_1}(t),m_{X_2}(t),...,m_{X_n}(t),m_{Y}(t)\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  If \(m_{X_1}(t)=m_{X_2}(t)\) for all t in some open interval about 0, then \(X_1\) and \(X_2\) have the same distribution
\item
  If \(Y = \alpha + \beta X_1\), then \(m_{Y}(t)= e^{\alpha t}m_{X_1}(\beta t)\)
\item
  If \(X_1,X_2,...X_n\) are independent and \(Y = \alpha_0 + \alpha_1 X_1 + \alpha_2 X_2 + ... + \alpha_n X_n\) (where \(\alpha_0, ... ,\alpha_n\) are real numbers), then \(m_{Y}(t)=e^{\alpha_0 t}m_{X_1}(\alpha_1t)m_{X_2}(\alpha_2 t)...m_{X_n}(\alpha_nt)\)
\item
  Suppose \(X_1,X_2,...X_n\) are independent normal random variables with means \(\mu_1,\mu_2,...\mu_n\) and variances \(\sigma^2_1,\sigma^2_2,...,\sigma^2_n\). If \(Y = \alpha_0 + \alpha_1 X_1 + \alpha_2 X_2 + ... + \alpha_n X_n\) (where \(\alpha_0, ... ,\alpha_n\) are real numbers), then Y is normally distributed with mean \(\mu_Y = \alpha_0 + \alpha_1 \mu_1 +\alpha_2 \mu_2 + ... + \alpha_n \mu_n\) and variance \(\sigma^2_Y = \alpha_1^2 \sigma_1^2 + \alpha_2^2 \sigma_2^2 + ... + \alpha_n^2 \sigma_n^2\)
\end{enumerate}

\hypertarget{moment}{%
\subsection{Moment}\label{moment}}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Moment & Uncentered & Centered \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1st & \(E(X)=\mu=Mean(X)\) & \\
2nd & \(E(X^2)\) & \(E((X-\mu)^2)=Var(X)=\sigma^2\) \\
3rd & \(E(X^3)\) & \(E((X-\mu)^3)\) \\
4th & \(E(X^4)\) & \(E((X-\mu)^4)\) \\
\end{longtable}

Skewness(X) = \(E((X-\mu)^3)/\sigma^3\)

Kurtosis(X) = \(E((X-\mu)^4)/\sigma^4\)

\textbf{Conditional Moments}

\[
E(Y|X=x)=
\begin{cases}
\sum_yyf_Y(y|x) & \text{for discrete RV}\\
\int_yyf_Y(y|x)dy & \text{for continous RV}\\
\end{cases}
\]

\[
Var(Y|X=x)=
\begin{cases}
\sum_y(y-E(Y|x))^2f_Y(y|x) & \text{for discrete RV}\\
\int_y(y-E(Y|x))^2f_Y(y|x)dy & \text{for continous RV}\\
\end{cases}
\]

\hypertarget{multivariate-moments}{%
\subsubsection{Multivariate Moments}\label{multivariate-moments}}

\[
E=
\left(
\begin{array}{c}
X \\
Y \\
\end{array}
\right)
=
\left(
\begin{array}{c}
E(X) \\
E(Y) \\
\end{array}
\right)
=
\left(
\begin{array}{c}
\mu_X \\
\mu_Y \\
\end{array}
\right)
\]

\[
\begin{aligned}
Var
\left(
\begin{array}{c}
X \\
Y \\
\end{array}
\right)
&=
\left(
\begin{array}
{cc}
Var(X) & Cov(X,Y) \\
Cov(X,Y) & Var(Y) 
\end{array}
\right) \\
&=
\left(
\begin{array}
{cc}
E((X-\mu_X)^2) & E((X-\mu_X)(Y-\mu_Y)) \\
E((X-\mu_X)(Y-\mu_Y)) & E((Y-\mu_Y)^2)
\end{array}
\right)
\end{aligned}
\]

\textbf{Properties}

\begin{itemize}
\tightlist
\item
  \(E(aX + bY + c)=aE(X) +bE(Y) + c\)
\item
  \(Var(aX + bY + c) = a^2Var(X) + b^2Var(Y) + 2abCov(X,Y)\)
\item
  \(Cov(aX + bY, cX + bY) =acVar(X)+bdVar(Y) + (ad+bc)Cov(X,Y)\)
\item
  Correlation: \(\rho_{XY} = \frac{Cov(X,Y)}{\sigma_X\sigma_Y}\)
\end{itemize}

\hypertarget{distributions}{%
\subsection{Distributions}\label{distributions}}

\textbf{Conditional Distributions}

\[
f_{X|Y}(X|Y=y)=\frac{f(X,Y)}{f_Y(y)}
\]

\(f_{X|Y}(X|Y=y)=f_X(X)\) if \(X\) and \(Y\) are independent

\hypertarget{discrete}{%
\subsubsection{Discrete}\label{discrete}}

CDF: Cumulative Density Function\\
MGF: Moment Generating Function

\hypertarget{bernoulli}{%
\paragraph{Bernoulli}\label{bernoulli}}

\(Bernoulli(p)\)

\textbf{PDF}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(mc2d}\SpecialCharTok{::}\FunctionTok{rbern}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\AttributeTok{prob=}\NormalTok{.}\DecValTok{5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{02.1-prerequisites_files/figure-latex/unnamed-chunk-1-1} \end{center}

\hypertarget{binomial}{%
\paragraph{Binomial}\label{binomial}}

\(B(n,p)\)

\begin{itemize}
\tightlist
\item
  the experiment consists of a fixed number (n) of Bernoulli trials, each of which results in a success (s) or failure (f)
\item
  The trials are identical and independent, and probability of success \((p)\) and probability of failure \((q = 1- p)\) remains the same for all trials.
\item
  The random variable \(X\) denotes the number of successes obtained in the \(n\) trials.
\end{itemize}

\textbf{Density}

\[
f(x)={{n}\choose{x}}p^xq^{n-x}
\]

\textbf{CDF}\\
You have to use table

\textbf{PDF}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Histogram of 1000 random values from a sample of 100 with probability of 0.5}
\FunctionTok{hist}\NormalTok{(}\FunctionTok{rbinom}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\AttributeTok{size =} \DecValTok{100}\NormalTok{, }\AttributeTok{prob =} \FloatTok{0.5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{02.1-prerequisites_files/figure-latex/unnamed-chunk-2-1} \end{center}

\textbf{MGF}

\[
m_X(t) =(q+pe^t)^n
\]

\textbf{Mean}

\[
\mu = E(x) = np
\]

\textbf{Variance}

\[
\sigma^2 =Var(X) = npq
\]

\hypertarget{poisson}{%
\paragraph{Poisson}\label{poisson}}

\(Pois(\lambda)\)

\begin{itemize}
\tightlist
\item
  Arises with Poisson process, which involves observing discrete events in a continuous ``interval'' of time, length, or space.
\item
  The random variable \(X\) is the number of occurrences of the event within an interval of \(s\) units
\item
  The parameter \(\lambda\) is the average number of occurrences of the event in question per measurement unit. For the distribution, we use the parameter \(k=\lambda s\)
\end{itemize}

\textbf{Density}

\[
f(x) = \frac{e^{-k}k^x}{x!} ,k > 0, x =0,1, \dots
\]

\textbf{CDF} Use table

\textbf{PDF}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Poisson dist with mean of 5 or Poisson(5)}
\FunctionTok{hist}\NormalTok{(}\FunctionTok{rpois}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\AttributeTok{lambda =} \DecValTok{5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{02.1-prerequisites_files/figure-latex/unnamed-chunk-3-1} \end{center}

\textbf{MGF}

\[
m_X(t)=e^{k(e^t-1)}
\]

\textbf{Mean}

\[
\mu = E(X) = k
\]

\textbf{Variance}

\[
\sigma^2 = Var(X) = k
\]

\hypertarget{geometric}{%
\paragraph{Geometric}\label{geometric}}

\begin{itemize}
\tightlist
\item
  The experiment consists of a series of trails. The outcome of each trial can be classed as being either a ``success'' (s) or ``failure'' (f). (This is called a Bernoulli trial).
\item
  The trials are identical and independent in the sense that the outcome of one trial has no effect on the outcome of any other. The probability of success \((p)\) and probability of failure \((q=1-p)\) remains the same from trial to trial.
\item
  lack of memory
\item
  \(X\): the number of trials needed to obtain the first success.
\end{itemize}

\textbf{Density}

\[
f(x)=pq^{x-1}
\]

\textbf{CDF}\\
\[
F(x) = 1- q^x
\]

\textbf{PDF}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# hist of Geometric distribution with probability of success = 0.5}
\FunctionTok{hist}\NormalTok{(}\FunctionTok{rgeom}\NormalTok{(}\AttributeTok{n =} \DecValTok{1000}\NormalTok{, }\AttributeTok{prob =} \FloatTok{0.5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{02.1-prerequisites_files/figure-latex/unnamed-chunk-4-1} \end{center}

\textbf{MGF}

\[
m_X(t) = \frac{pe^t}{1-qe^t}
\]

for \(t < -ln(q)\)

\textbf{Mean}

\[
\mu = \frac{1}{p}
\]

\textbf{Variance}

\[
\sigma^2 = Var(X) = \frac{q}{p^2}
\]

\hypertarget{hypergeometric}{%
\paragraph{Hypergeometric}\label{hypergeometric}}

\begin{itemize}
\tightlist
\item
  The experiment consists of drawing a random sample of size \(n\) without replacement and without regard to order from a collection of \(N\) objects.
\item
  Of the \(N\) objects, \(r\) have a trait of interest; \(N-r\) do not have the trait
\item
  \(X\) is the number of objects in the sample with the trait.
\end{itemize}

\textbf{Density}

\[
f(x)=\frac{{{r}\choose{x}}{{N-r}\choose{n-x}}}{{{N}\choose{n}}}
\]

where \(max[0,n-(N-r)] \le x \le min(n,r)\)

\textbf{PDF}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# hist of hypergeometric distribution with the number of white balls = 50, }
\CommentTok{\# and the number of black balls = 20, and number of balls drawn = 30. }
\FunctionTok{hist}\NormalTok{(}\FunctionTok{rhyper}\NormalTok{(}\AttributeTok{nn =} \DecValTok{1000}\NormalTok{ , }\AttributeTok{m=}\DecValTok{50}\NormalTok{, }\AttributeTok{n=}\DecValTok{20}\NormalTok{, }\AttributeTok{k=}\DecValTok{30}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{02.1-prerequisites_files/figure-latex/unnamed-chunk-5-1} \end{center}

\textbf{Mean}

\[
\mu = E(x)= \frac{nr}{N}
\]

\textbf{Variance}

\[
\sigma^2 = var(X) = n (\frac{r}{N})(\frac{N-r}{N})(\frac{N-n}{N-1})
\]

\textbf{Note} For large N (if \(\frac{n}{N} \le 0.05\)), this distribution can be approximated using a Binomial distribution with \(p = \frac{r}{N}\)

\hypertarget{section}{%
\paragraph{}\label{section}}

\hypertarget{continuous}{%
\subsubsection{Continuous}\label{continuous}}

\hypertarget{uniform}{%
\paragraph{Uniform}\label{uniform}}

\begin{itemize}
\tightlist
\item
  Defined over an interval \((a,b)\) in which the probabilities are ``equally likely'' for subintervals of equal length.
\end{itemize}

\textbf{Density}

\[
f(x)=\frac{1}{b-a}
\]

for \(a < x < b\)

\textbf{CDF}

\[
\begin{cases}
0 & \text{if } x <a \\
\frac{x-a}{b-a}  & a \le x \le b  \\
1 & \text{if } x >b
\end{cases}
\] \textbf{PDF}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(}\FunctionTok{runif}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\AttributeTok{min =} \DecValTok{0}\NormalTok{, }\AttributeTok{max =} \DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{02.1-prerequisites_files/figure-latex/unnamed-chunk-6-1} \end{center}

\textbf{MGF}

\[
\begin{cases}
\frac{e^{tb} - e^{ta}}{t(b-a)} & \text{if } t \neq 0\\
1 & \text{if } t \neq 0
\end{cases}
\]

\textbf{Mean}

\[
\mu = E(X) = \frac{a +b}{2}
\]

\textbf{Variance}

\[
\sigma^2 = Var(X) = \frac{(b-a)^2}{12}
\]

\hypertarget{gamma}{%
\paragraph{Gamma}\label{gamma}}

\begin{itemize}
\tightlist
\item
  is used to define the exponential and \(\chi^2\) distributions
\item
  The gamma function is defined as:
\end{itemize}

\[
\Gamma(\alpha) = \int_0^{\infty} z^{\alpha-1}e^{-z}dz
\]

where \(\alpha > 0\)

\begin{itemize}
\item
  Properties of The Gamma function:\\

  \begin{itemize}
  \tightlist
  \item
    \(\Gamma(1) = 1\) + For \(\alpha >1\), \(\Gamma(\alpha)=(\alpha-1)\Gamma(\alpha-1)\) + If n is an integer and \(n>1\), then \(\Gamma(n) = (n-1)!\)
  \end{itemize}
\end{itemize}

\textbf{Density}

\[
f(x) = \frac{1}{\Gamma(\alpha)\beta^{\alpha}}x^{\alpha-1}e^{-x/\beta}
\]

\textbf{CDF}

\[
F(x,n,\beta) = 1 -\sum_{k=0}^{n-1} \frac{(\frac{x}{\beta})^k e^{-x/\beta}}{k!}
\]

for \(x>0\), and \(\alpha = n\) (a positive integer)

\textbf{PDF}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(}\FunctionTok{rgamma}\NormalTok{(}\AttributeTok{n =} \DecValTok{1000}\NormalTok{, }\AttributeTok{shape =} \DecValTok{5}\NormalTok{, }\AttributeTok{rate =} \DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{02.1-prerequisites_files/figure-latex/unnamed-chunk-7-1} \end{center}

\textbf{MGF}

\[
m_X(t) = (1-\beta t)^{-\alpha}
\]

where \(t < \frac{1}{\beta}\)

\textbf{Mean}

\[
\mu = E(X) = \alpha \beta
\]

\textbf{Variance}

\[
\sigma^2 = Var(X) = \alpha \beta^2
\]

\hypertarget{normal}{%
\paragraph{Normal}\label{normal}}

\(N(\mu,\sigma^2)\)

\begin{itemize}
\tightlist
\item
  is symmetric, bell-shaped curve with parameters \(\mu\) and \(\sigma^2\)
\item
  also known as Gaussian.
\end{itemize}

\textbf{Density}

\[
f(x) = \frac{1}{\sigma \sqrt{2\pi }}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}
\]

for \(-\infty < x, \mu< \infty, \sigma > 0\)

\textbf{CDF}

Use table

\textbf{PDF}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{02.1-prerequisites_files/figure-latex/unnamed-chunk-8-1} \end{center}

\textbf{MGF}

\[
m_X(t) = e^{\mu t + \frac{\sigma^2 t^2}{2}}
\]

\textbf{Mean}

\[
\mu = E(X)
\]

\textbf{Variance}

\[
\sigma^2 = Var(X)
\]

\textbf{Standard Normal Random Variable}

\begin{itemize}
\tightlist
\item
  The normal random variable \(Z\) with mean \(\mu = 0\) and standard deviation \(\sigma =1\) is called standard normal
\item
  Any normal random variable X with mean \(\mu\) and standard deviation \(\sigma\) can be converted to the standard normal random variable \(Z = \frac{X-\mu}{\sigma}\)
\end{itemize}

\textbf{Normal Approximation to the Binomial Distribution}

Let \(X\) be binomial with parameters \(n\) and \(p\). For large \(n\) (so that \((A)p \le .5\) and \(np > 5\) or (B) \(p>.5\) and \(nq>5\)), X is approximately normally distributed with mean \(\mu = np\) and standard deviation \(\sigma = \sqrt{npq}\)

When using the normal approximation, add or subtract 0.5 as needed for the continuity correction

\begin{longtable}[]{@{}cc@{}}
\toprule\noalign{}
Discrete & Approximate Normal (corrected) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(P(X = c)\) & \(P(c -0.5 < Y < c + 0.5)\) \\
\(P(X < c)\) & \(P(Y < c - 0.5)\) \\
\(P(X \le c)\) & \(P(Y < c + 0.5)\) \\
\(P(X > c)\) & \(P(Y > c + 0.5)\) \\
\(P(X \ge c)\) & \(P(Y > c - 0.5)\) \\
\end{longtable}

\textbf{Normal Probability Rule}

If X is normally distributed with parameters \(\mu\) and \(\sigma\), then

\begin{itemize}
\tightlist
\item
  \(P(-\sigma < X - \mu < \sigma) \approx .68\)
\item
  \(P(-2\sigma < X - \mu < 2\sigma) \approx .95\)
\item
  \(P(-3\sigma < X - \mu < 3\sigma) \approx .997\)
\end{itemize}

\hypertarget{logistic}{%
\paragraph{Logistic}\label{logistic}}

\(Logistic(\mu,s)\)

\textbf{PDF}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(}\FunctionTok{rlogis}\NormalTok{(}\AttributeTok{n =} \DecValTok{1000}\NormalTok{, }\AttributeTok{location =} \DecValTok{0}\NormalTok{, }\AttributeTok{scale =} \DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{02.1-prerequisites_files/figure-latex/unnamed-chunk-9-1} \end{center}

\hypertarget{laplace}{%
\paragraph{Laplace}\label{laplace}}

\[
f(x) = \frac{1}{2} e^{-|x-\theta|}
\]

where \(-\infty < x < \infty\) and \(-\infty < \theta < \infty\)

\[
\mu = \theta, \sigma^2 = 2
\]

and

\[
m(t) = e^{t \theta} \frac{1}{1 - t^2}
\]

where \(-1 < t < 1\)

\hypertarget{log-normal}{%
\paragraph{Log-normal}\label{log-normal}}

\(lognormal(\mu,\sigma^2)\)

\textbf{PDF}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(}\FunctionTok{rlnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{1000}\NormalTok{, }\AttributeTok{meanlog =} \DecValTok{0}\NormalTok{, }\AttributeTok{sdlog =} \DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{02.1-prerequisites_files/figure-latex/unnamed-chunk-10-1} \end{center}

\hypertarget{exponential}{%
\paragraph{Exponential}\label{exponential}}

\(Exp(\lambda)\)

\begin{itemize}
\tightlist
\item
  A special case of the gamma distribution with \(\alpha = 1\)
\item
  Lack of memory
\item
  \(\lambda\) = rate Within a Poisson process with parameter \(\lambda\), if W is the waiting tine until the occurrence of the first event, then W has an exponential distribution with \(\beta = 1/\alpha\)
\end{itemize}

\textbf{Density}

\[
f(x) = \frac{1}{\beta} e^{-x/\beta}
\]

for \(x,\beta > 0\)

\textbf{CDF}

\[
F(x) = 
\begin{cases}
0 & \text{if } x \le 0 \\
1 - e^{-x/\beta} & \text{if } x > 0
\end{cases}
\]

\textbf{PDF}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(}\FunctionTok{rexp}\NormalTok{(}\AttributeTok{n =} \DecValTok{1000}\NormalTok{, }\AttributeTok{rate =} \DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{02.1-prerequisites_files/figure-latex/unnamed-chunk-11-1} \end{center}

\textbf{MGF}

\[
m_X(t) = (1-\beta t)^{-1}
\]

for \(t < 1/\beta\)

\textbf{Mean}

\[
\mu = E(X) = \beta
\]

\textbf{Variance}

\[
\sigma^2 = Var(X) =\beta^2
\]

\hypertarget{chi-squared}{%
\paragraph{Chi-squared}\label{chi-squared}}

\(\chi^2=\chi^2(k)\)

\begin{itemize}
\tightlist
\item
  A special case of the gamma distribution with \(\beta =2\), and \(\alpha = \gamma /2\) for a positive integer \(\gamma\)
\item
  The random variable \(X\) is denoted \(\chi_{\gamma}^2\) and is said to have a chi-squared distribution with \(\gamma\) degrees of freedom.
\end{itemize}

\textbf{Density} Use density for Gamma Distribution with \(\beta = 2\) and \(\alpha = \gamma/2\)

\textbf{CDF} Use table

\textbf{PDF}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(}\FunctionTok{rchisq}\NormalTok{(}\AttributeTok{n =} \DecValTok{1000}\NormalTok{, }\AttributeTok{df=}\DecValTok{2}\NormalTok{, }\AttributeTok{ncp =} \DecValTok{0}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{02.1-prerequisites_files/figure-latex/unnamed-chunk-12-1} \end{center}

\textbf{MGF}

\[
m_X(t) = (1-2t)^{-\gamma/2}
\]

\textbf{Mean}

\[
\mu = E(X) = \gamma
\]

\textbf{Variance}

\[
\sigma^2 = Var(X) = 2\gamma
\]

\hypertarget{student-t}{%
\paragraph{Student T}\label{student-t}}

\(T(v)\)

\begin{itemize}
\tightlist
\item
  \(T=\frac{Z}{\sqrt{\chi_{\gamma}^2/\gamma}}\), where Z is standard normal follows a student-t distribution with \(\gamma\) dof
\item
  The distribution is symmetric, bell-shaped , with a mean of \(\mu=0\)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(}\FunctionTok{rt}\NormalTok{(}\AttributeTok{n =} \DecValTok{1000}\NormalTok{, }\AttributeTok{df=}\DecValTok{2}\NormalTok{, }\AttributeTok{ncp =}\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{02.1-prerequisites_files/figure-latex/unnamed-chunk-13-1} \end{center}

\hypertarget{f-distribution}{%
\paragraph{F-Distribution}\label{f-distribution}}

\(F(d_1,d_2)\)

\begin{itemize}
\tightlist
\item
  F distribution is strictly positive
\item
  \(F=\frac{\chi_{\gamma_1}^2/\gamma_1}{\chi_{\gamma_2^2}/\gamma_2}\) follows an F distribution with dof \(\gamma_1\) and \(\gamma_2\), where \(\chi_{\gamma_1}^2\) and \(\chi_{\gamma_2}^2\) are independent chi-squared random variables.
\item
  The distribution is asymmetric and never negative.
\end{itemize}

\textbf{PDF}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(}\FunctionTok{rf}\NormalTok{(}\AttributeTok{n =} \DecValTok{1000}\NormalTok{, }\AttributeTok{df1=}\DecValTok{2}\NormalTok{, }\AttributeTok{df2=}\DecValTok{3}\NormalTok{, }\AttributeTok{ncp=}\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{02.1-prerequisites_files/figure-latex/unnamed-chunk-14-1} \end{center}

\hypertarget{cauchy}{%
\paragraph{Cauchy}\label{cauchy}}

\protect\hyperlink{central-limit-theorem}{Central Limit Theorem} and \protect\hyperlink{weak-law}{Weak Law} do not apply to Cauchy because it does not have finite mean and finite variance

\textbf{PDF}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(}\FunctionTok{rcauchy}\NormalTok{(}\AttributeTok{n =} \DecValTok{1000}\NormalTok{, }\AttributeTok{location =} \DecValTok{0}\NormalTok{, }\AttributeTok{scale =} \DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{02.1-prerequisites_files/figure-latex/unnamed-chunk-15-1} \end{center}

\hypertarget{multivariate-normal-distribution}{%
\paragraph{Multivariate Normal Distribution}\label{multivariate-normal-distribution}}

Let \(y\) be a \(p\)-dimensional multivariate normal (MVN) random variable with mean \(\mu\) and variance \(\Sigma\). Then, the density of \(y\) is

\[
f(\mathbf{y}) = \frac{1}{(2\pi)^{p/2}|\mathbf{\Sigma}|^{1/2}}exp(-\frac{1}{2}\mathbf{(y-\mu)'\Sigma^{-1}(y-\mu)})
\]

We have \(\mathbf{y} \sim N_p(\mathbf{\mu,\Sigma})\)

Properties:

\begin{itemize}
\tightlist
\item
  Let \(\mathbf{A}_{r \times p}\) be a fixed matrix. then \(\mathbf{Ay} \sim N_r(\mathbf{A \mu, A \Sigma A')}\). Note that \(r \le p\) and all rows of \(\mathbf{A}\) must be linearly independent to guarantee that \(\mathbf{A\Sigma A'}\) is non-singular.\\
\item
  Let \(\mathbf{G}\) be a matrix such that \(\mathbf{\Sigma^{-1}= GG'}\). then, \(\mathbf{G'y} \sim N_p (\mathbf{G'\mu,I})\) and \(\mathbf{G'(y-\mu)} \sim N_p (\mathbf{0,I})\).\\
\item
  Any fixed linear combination of \(y_1,...,y_p\) say \(\mathbf{c'y}\), follows \(\mathbf{c'y} \sim N_1(\mathbf{c'\mu,c'\Sigma c})\)
\end{itemize}

\textbf{Large Sample Properties}

Suppose that \(y_1,...,y_n\) are a random sample from some population with mean \(\mu\) and variance-variance matrix \(\Sigma\)

\[
\mathbf{Y} \sim MVN(\mathbf{\mu,\Sigma})
\]

Then

\begin{itemize}
\tightlist
\item
  \(\bar{\mathbf{y}} = \frac{1}{n}\sum_{i=1}^n \mathbf{y}_i\) is a consistent estimator for \(\mathbf{\mu}\)\\
\item
  \(\mathbf{S} = \frac{1}{n-1}\sum_{i=1}^n \mathbf{(y_i - \bar{y})(y_i - \bar{y})'}\) is a consistent estimator for \(\mathbf{\Sigma}\)\\
\item
  \textbf{Multivariate Central Limit Theorem}: Similar to the univariate case, \(\sqrt{n}(\mathbf{\bar{y}- \mu}) \sim N_p(\mathbf{0, \Sigma})\) when \(n\) is large relative to p (e.g., \(n \ge 25p\)), which is equivalent to \(\bar{y} \sim N_p(\mathbf{\mu,\Sigma/n})\)\\
\item
  Wald's Theorem: \(n(\mathbf{\bar{y}- \mu)'S^{-1}(\bar{y}- \mu)} \sim \chi^2_{(p)}\) when \(n\) is large relative to \(p\).
\end{itemize}

\hypertarget{general-math}{%
\section{General Math}\label{general-math}}

\hypertarget{number-sets}{%
\subsection{Number Sets}\label{number-sets}}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Notation & Denotes & Examples \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(\emptyset\) & Empty set & No members \\
\(\mathbb{N}\) & Natural numbers & \(\{1, 2, ...\}\) \\
\(\mathbb{Z}\) & Integers & \(\{ ..., -1, 0, 1, ...\}\) \\
\(\mathbb{Q}\) & Rational numbers & including fractions \\
\(\mathbb{R}\) & Real numbers & \\
\(\mathbb{C}\) & Complex numbers & \\
\end{longtable}

\hypertarget{summation-notation-and-series}{%
\subsection{Summation Notation and Series}\label{summation-notation-and-series}}

\textbf{Chebyshev's Inequality} Let \(X\) be a random variable with mean \(\mu\) and standard deviation \(\sigma\). Then for any positive number \(k\):

\[
P(|X-\mu| < k\sigma) \ge 1 - \frac{1}{k^2}
\]

Chebyshev's Inequality does not require that \(X\) be normally distributed

\textbf{Geometric sum}

\[
\sum_{k=0}^{n-1} ar^k = a\frac{1-r^n}{1-r}
\]

where \(r \neq 1\)

\textbf{Geometric series}

\[
\sum_{k=0}^\infty ar^k = \frac{a}{1-r}
\]

where \(|r| <1\)

\textbf{Binomial theorem}

\[
(x + y)^n = \sum_{k=0}^n \binom{n}{k} x^{n-k} y^k
\]

where \(n \ge 0\)

\textbf{Binomial series}

\[
\sum_k \binom{\alpha}{k} x^k  = (1 +x)^\alpha
\]

\(|x| < 1\) if \(\alpha \neq n \ge 0\)

\textbf{Telescoping sum}

When terms of a sum cancel each other out, leaving one term (i.e., it collapses like a telescope), we call it a telescoping sum

\[
\sum_{a \le k < b} \Delta F(k) = F(b) - F(a)
\]

where \(a \le b\) and \(a, b \in \mathbb{Z}\)

\textbf{Vandermonde convolution}

\[
\sum_k \binom{r}{k} \binom{s}{n-k} = \binom{r+s}{n}
\]

\(n \in \mathbb{Z}\)

\textbf{Exponential series}

\[
\sum_{k=0}^\infty \frac{x^k}{k!} = e^x
\]

where \(x \in \mathbb{C}\)

\textbf{Taylor series}

\[
\sum_{k=0}^{\infty} \frac{f^{(k)}(a)}{k!} (x-a)^k = f(x)
\]

where \(|x-a| < R =\) radius of convergence

when \(a = 0\), we have

\textbf{Maclaurin series expansion for}

\[
e^z = 1 + z + \frac{z^2}{2!} + \frac{z^3}{3!} + ...
\]

\textbf{Euler's summation formula}

\[\sum_{a \le k < b} f(k) = \int_a^b f(x) dx + \sum_{k=1}^m\frac{B_k}{k!} f^{(k-1)}(x) |_a^b \\+ (-1)^{m+1} \int^b_a \frac{B_m (x-|x|)}{m!} f^{(m)}(x)dx\] where \(a,b, c \in \mathbb{Z}\) and \(a \le b, m \ge 1\)

when \(m = 1\), we have trapezoidal rule

\[
\sum_{a \le k < b} f(k) \approx \int_a^b f(x) dx - \frac{1}{2} (f(b) - f(a))
\]

\hypertarget{taylor-expansion}{%
\subsection{Taylor Expansion}\label{taylor-expansion}}

A differentiable function, \(G(x)\) can be written as an infinite sum of its derivatives.

More specifically, an infinitely differentiable \(G(x)\) evaluated at \(a\) is

\[
G(x) = G(a) + \frac{G'(a)}{1!} (x-a) + \frac{G''(a)}{2!}(x-a) + \frac{G'''(a)}{3!}(x-a)^3 + \dots
\]

\hypertarget{law-of-large-numbers}{%
\subsection{Law of large numbers}\label{law-of-large-numbers}}

Let \(X_1,X_2,...\) be an infinite sequence of independent and identically distributed (i.i.d)

Then, the sample average is

\[
\bar{X}_n =\frac{1}{n} (X_1 + ... + X_n)
\]

converges to the expected value (\(\bar{X}_n \rightarrow \mu\)) as \(n \rightarrow \infty\)

\[
Var(X_i) = Var(\frac{1}{n}(X_1 + ... + X_n)) = \frac{1}{n^2}Var(X_1 + ... + X_n)= \frac{n\sigma^2}{n^2}=\frac{\sigma^2}{n}
\]

The difference between \protect\hyperlink{weak-law}{Weak Law} and \protect\hyperlink{strong-law}{Strong Law} regards the mode of convergence

\hypertarget{weak-law}{%
\subsubsection{Weak Law}\label{weak-law}}

The sample average converges in probability towards the expected value

\[
\bar{X}_n \rightarrow^{p} \mu
\]

when \(n \rightarrow \infty\)

\[
\lim_{n\to \infty}P(|\bar{X}_n - \mu| > \epsilon) = 0
\]

The sample mean from a iid random sample (\(\{ x_i \}_{i=1}^n\)) from any population with a finite mean and finite variance \(\sigma^2\) is ca consistent estimation for the population mean \(\mu\)

\[
plim(\bar{x})=plim(n^{-1}\sum_{i=1}^{n}x_i) =\mu
\]

\hypertarget{strong-law}{%
\subsubsection{Strong Law}\label{strong-law}}

The sample average converges almost surely to the expected value

\[
\bar{X}_n \rightarrow^{a.s} \mu 
\]

when \(n \rightarrow \infty\)

Equivalently,

\[
P(\lim_{n\to \infty}\bar{X}_n =\mu) =1
\]

\hypertarget{law-of-iterated-expectation}{%
\subsection{Law of Iterated Expectation}\label{law-of-iterated-expectation}}

Let \(X, Y\) be random variables. Then,

\[
E(X) = E(E(X|Y))
\]

means that the expected value of X can be calculated from the probability distribution of \(X|Y\) and \(Y\)

\hypertarget{convergence}{%
\subsection{Convergence}\label{convergence}}

\hypertarget{convergence-in-probability}{%
\subsubsection{Convergence in Probability}\label{convergence-in-probability}}

\begin{itemize}
\tightlist
\item
  \(n \rightarrow \infty\), an estimator (random variable) that is close to the true value.
\item
  The random variable \(\theta_n\) converges in probability to a constant \(c\) if
\end{itemize}

\[
\lim_{n\to \infty}P(|\theta_n - c| \ge \epsilon) = 0
\]

for any positive \(\epsilon\)

Notation

\[
plim(\theta_n)=c 
\]

Equivalently,

\[
\theta_n \rightarrow^p c
\]

\textbf{Properties of Convergence in Probability}

\begin{itemize}
\item
  Slutsky's Theorem: for a continuous function g(.), if \(plim(\theta_n)= \theta\) then \(plim(g(\theta_n)) = g(\theta)\)
\item
  if \(\gamma_n \rightarrow^p \gamma\) then

  \begin{itemize}
  \tightlist
  \item
    \(plim(\theta_n + \gamma_n)=\theta + \gamma\) + \(plim(\theta_n \gamma_n) = \theta \gamma\) + \(plim(\theta_n/\gamma_n) = \theta/\gamma\) if \(\gamma \neq 0\)
  \end{itemize}
\item
  Also hold for random vectors/ matrices
\end{itemize}

\hypertarget{convergence-in-distribution}{%
\subsubsection{Convergence in Distribution}\label{convergence-in-distribution}}

\begin{itemize}
\tightlist
\item
  As \(n \rightarrow \infty\), the distribution of a random variable may converge towards another (``fixed'') distribution.
\item
  The random variable \(X_n\) with CDF \(F_n(x)\) converges in distribution to a random variable \(X\) with CDF \(F(X)\) if
\end{itemize}

\[
\lim_{n\to \infty}|F_n(x) - F(x)| = 0
\]

at all points of continuity of \(F(X)\)

Notation \(F(x)\) is the limiting distribution of \(X_n\) or \(X_n \rightarrow^d X\)

\begin{itemize}
\tightlist
\item
  \(E(X)\) is the limiting mean (asymptotic mean)
\item
  \(Var(X)\) is the limiting variance (asymptotic variance)
\end{itemize}

\textbf{Note}

\[
\begin{aligned}
E(X) &\neq \lim_{n\to \infty}E(X_n) \\
Avar(X_n) &\neq \lim_{n\to \infty}Var(X_n)
\end{aligned}
\]

\textbf{Properties of Convergence in Distribution}

\begin{itemize}
\item
  Continuous Mapping Theorem: for a continuous function g(.), if \(X_n \to^{d} g(X)\) then \(g(X_n) \to^{d} g(X)\)
\item
  If \(Y_n\to^{d} c\), then

  \begin{itemize}
  \item
    \(X_n + Y_n \to^{d} X + c\)
  \item
    \(Y_nX_n \to^{d} cX\)
  \item
    \(X_nY_n \to^{d} X/c\) if \(c \neq 0\)
  \end{itemize}
\item
  also hold for random vectors/matrices
\end{itemize}

\hypertarget{summary}{%
\subsubsection{Summary}\label{summary}}

Properties of Convergence

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5139}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.4861}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Probability
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Distribution
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Slutsky's Theorem: for a continuous function g(.), if \(plim(\theta_n)= \theta\) then \(plim(g(\theta_n)) = g(\theta)\) & Continuous Mapping Theorem: for a continuous function g(.), if \(X_n \to^{d} g(X)\) then \(g(X_n) \to^{d} g(X)\) \\
if \(\gamma_n \rightarrow^p \gamma\) then & if \(Y_n\to^{d} c\), then \\
\(plim(\theta_n + \gamma_n)=\theta + \gamma\) & \(X_n + Y_n \to^{d} X + c\) \\
\(plim(\theta_n \gamma_n) = \theta \gamma\) & \(Y_nX_n \to^{d} cX\) \\
\(plim(\theta_n/\gamma_n) = \theta/\gamma\) if \(\gamma \neq 0\) & \(X_nY_n \to^{d} X/c\) if \(c \neq 0\) \\
\end{longtable}

\protect\hyperlink{convergence-in-probability}{Convergence in Probability} is stronger than \protect\hyperlink{convergence-in-distribution}{Convergence in Distribution}.

Hence, \protect\hyperlink{convergence-in-distribution}{Convergence in Distribution} does not guarantee \protect\hyperlink{convergence-in-probability}{Convergence in Probability}

\hypertarget{sufficient-statistics}{%
\subsection{Sufficient Statistics}\label{sufficient-statistics}}

\textbf{Likelihood}

\begin{itemize}
\tightlist
\item
  describes the extent to which the sample provides support for any particular parameter value.
\item
  Higher support corresponds to a higher value for the likelihood
\item
  The exact value of any likelihood is \textbf{meaningless},
\item
  The relative value, (i.e., comparing two values of \(\theta\)), is \textbf{informative}.
\end{itemize}

\[
L(\theta_0; y) = P(Y = y | \theta = \theta_0) = f_Y(y;\theta_0)
\]

\textbf{Likelihood Ratio}

\[
\frac{L(\theta_0;y)}{L(\theta_1;y)}
\]

\textbf{Likelihood Function}

For a given sample, you can create likelihoods for all possible values of \(\theta\), which is called \emph{likelihood function}

\[
L(\theta) = L(\theta; y) = f_Y(y;\theta)
\]

In a sample of size n, the likelihood function takes the form of a product

\[
L(\theta) = \prod_{i=1}^{n}f_i (y_i;\theta)
\]

Equivalently, the log likelihood function

\[
l(\theta) = \sum_{i=1}^{n} logf_i(y_i;\theta)
\]

\textbf{Sufficient statistics}

\begin{itemize}
\tightlist
\item
  A statistic, \(T(y)\), is any quantity that can be calculated purely from a sample (independent of \(\theta\))
\item
  A statistic is \textbf{sufficient} if it conveys all the available information about the parameter.
\end{itemize}

\[
L(\theta; y) = c(y)L^*(\theta;T(y))
\]

\textbf{Nuisance parameters} If we are interested in a parameter (e.g., mean). Other parameters requiring estimation (e.g., standard deviation) are \textbf{nuisance} parameters. We can replace nuisance parameters in likelihood function with their estimates to create a \textbf{profile likelihood}.

\hypertarget{parameter-transformations}{%
\subsection{Parameter transformations}\label{parameter-transformations}}

log-odds transformation

\[
Log odds = g(\theta)= ln[\frac{\theta}{1-\theta}]
\]

\hypertarget{data-importexport}{%
\section{Data Import/Export}\label{data-importexport}}

\href{https://cran.r-project.org/doc/manuals/r-release/R-data.html}{Extended Manual by R}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1892}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1892}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2162}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2162}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1892}}@{}}
\caption{Table by \href{https://cran.r-project.org/web/packages/rio/vignettes/rio.html}{Rio Vignette}}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Format
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Typical Extension
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Import Package
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Export Package
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Installed by Default
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Format
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Typical Extension
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Import Package
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Export Package
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Installed by Default
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Comma-separated data & .csv & \href{https://cran.r-project.org/package=data.table}{\textbf{data.table}} & \href{https://cran.r-project.org/package=data.table}{\textbf{data.table}} & Yes \\
Pipe-separated data & .psv & \href{https://cran.r-project.org/package=data.table}{\textbf{data.table}} & \href{https://cran.r-project.org/package=data.table}{\textbf{data.table}} & Yes \\
Tab-separated data & .tsv & \href{https://cran.r-project.org/package=data.table}{\textbf{data.table}} & \href{https://cran.r-project.org/package=data.table}{\textbf{data.table}} & Yes \\
CSVY (CSV + YAML metadata header) & .csvy & \href{https://cran.r-project.org/package=data.table}{\textbf{data.table}} & \href{https://cran.r-project.org/package=data.table}{\textbf{data.table}} & Yes \\
SAS & .sas7bdat & \href{https://cran.r-project.org/package=haven}{\textbf{haven}} & \href{https://cran.r-project.org/package=haven}{\textbf{haven}} & Yes \\
SPSS & .sav & \href{https://cran.r-project.org/package=haven}{\textbf{haven}} & \href{https://cran.r-project.org/package=haven}{\textbf{haven}} & Yes \\
SPSS (compressed) & .zsav & \href{https://cran.r-project.org/package=haven}{\textbf{haven}} & \href{https://cran.r-project.org/package=haven}{\textbf{haven}} & Yes \\
Stata & .dta & \href{https://cran.r-project.org/package=haven}{\textbf{haven}} & \href{https://cran.r-project.org/package=haven}{\textbf{haven}} & Yes \\
SAS XPORT & .xpt & \href{https://cran.r-project.org/package=haven}{\textbf{haven}} & \href{https://cran.r-project.org/package=haven}{\textbf{haven}} & Yes \\
SPSS Portable & .por & \href{https://cran.r-project.org/package=haven}{\textbf{haven}} & & Yes \\
Excel & .xls & \href{https://cran.r-project.org/package=readxl}{\textbf{readxl}} & & Yes \\
Excel & .xlsx & \href{https://cran.r-project.org/package=readxl}{\textbf{readxl}} & \href{https://cran.r-project.org/package=openxlsx}{\textbf{openxlsx}} & Yes \\
R syntax & .R & \textbf{base} & \textbf{base} & Yes \\
Saved R objects & .RData, .rda & \textbf{base} & \textbf{base} & Yes \\
Serialized R objects & .rds & \textbf{base} & \textbf{base} & Yes \\
Epiinfo & .rec & \href{https://cran.r-project.org/package=foreign}{\textbf{foreign}} & & Yes \\
Minitab & .mtp & \href{https://cran.r-project.org/package=foreign}{\textbf{foreign}} & & Yes \\
Systat & .syd & \href{https://cran.r-project.org/package=foreign}{\textbf{foreign}} & & Yes \\
``XBASE'' database files & .dbf & \href{https://cran.r-project.org/package=foreign}{\textbf{foreign}} & \href{https://cran.r-project.org/package=foreign}{\textbf{foreign}} & Yes \\
Weka Attribute-Relation File Format & .arff & \href{https://cran.r-project.org/package=foreign}{\textbf{foreign}} & \href{https://cran.r-project.org/package=foreign}{\textbf{foreign}} & Yes \\
Data Interchange Format & .dif & \textbf{utils} & & Yes \\
Fortran data & no recognized extension & \textbf{utils} & & Yes \\
Fixed-width format data & .fwf & \textbf{utils} & \textbf{utils} & Yes \\
gzip comma-separated data & .csv.gz & \textbf{utils} & \textbf{utils} & Yes \\
Apache Arrow (Parquet) & .parquet & \href{https://cran.r-project.org/package=arrow}{\textbf{arrow}} & \href{https://cran.r-project.org/package=arrow}{\textbf{arrow}} & No \\
EViews & .wf1 & \href{https://cran.r-project.org/package=hexView}{\textbf{hexView}} & & No \\
Feather R/Python interchange format & .feather & \href{https://cran.r-project.org/package=feather}{\textbf{feather}} & \href{https://cran.r-project.org/package=feather}{\textbf{feather}} & No \\
Fast Storage & .fst & \href{https://cran.r-project.org/package=fst}{\textbf{fst}} & \href{https://cran.r-project.org/package=fst}{\textbf{fst}} & No \\
JSON & .json & \href{https://cran.r-project.org/package=jsonlite}{\textbf{jsonlite}} & \href{https://cran.r-project.org/package=jsonlite}{\textbf{jsonlite}} & No \\
Matlab & .mat & \href{https://cran.r-project.org/package=rmatio}{\textbf{rmatio}} & \href{https://cran.r-project.org/package=rmatio}{\textbf{rmatio}} & No \\
OpenDocument Spreadsheet & .ods & \href{https://cran.r-project.org/package=readODS}{\textbf{readODS}} & \href{https://cran.r-project.org/package=readODS}{\textbf{readODS}} & No \\
HTML Tables & .html & \href{https://cran.r-project.org/package=xml2}{\textbf{xml2}} & \href{https://cran.r-project.org/package=xml2}{\textbf{xml2}} & No \\
Shallow XML documents & .xml & \href{https://cran.r-project.org/package=xml2}{\textbf{xml2}} & \href{https://cran.r-project.org/package=xml2}{\textbf{xml2}} & No \\
YAML & .yml & \href{https://cran.r-project.org/package=yaml}{\textbf{yaml}} & \href{https://cran.r-project.org/package=yaml}{\textbf{yaml}} & No \\
Clipboard & default is tsv & \href{https://cran.r-project.org/package=clipr}{\textbf{clipr}} & \href{https://cran.r-project.org/package=clipr}{\textbf{clipr}} & No \\
\href{https://www.google.com/sheets/about/}{Google Sheets} & as Comma-separated data & & & \\
\end{longtable}

R limitations:

\begin{itemize}
\item
  By default, R use 1 core in CPU
\item
  R puts data into memory (limit around 2-4 GB), while SAS uses data from files on demand
\item
  Categorization

  \begin{itemize}
  \item
    Medium-size file: within RAM limit, around 1-2 GB
  \item
    Large file: 2-10 GB, there might be some workaround solution
  \item
    Very large file \textgreater{} 10 GB, you have to use distributed or parallel computing
  \end{itemize}
\end{itemize}

Solutions:

\begin{itemize}
\item
  buy more RAM
\item
  HPC packages

  \begin{itemize}
  \item
    Explicit Parallelism
  \item
    Implicit Parallelism
  \item
    Large Memory
  \item
    Map/Reduce
  \end{itemize}
\item
  specify number of rows and columns, typically including command \texttt{nrow\ =}
\item
  Use packages that store data differently

  \begin{itemize}
  \item
    \texttt{bigmemory}, \texttt{biganalytics}, \texttt{bigtabulate} , \texttt{synchronicity}, \texttt{bigalgebra}, \texttt{bigvideo} use C++ to store matrices, but also support one class type
  \item
    For multiple class types, use \texttt{ff} package
  \end{itemize}
\item
  Very Large datasets use

  \begin{itemize}
  \tightlist
  \item
    \texttt{RHaddop} package
  \item
    \texttt{HadoopStreaming}
  \item
    \texttt{Rhipe}
  \end{itemize}
\end{itemize}

\hypertarget{medium-size}{%
\subsection{Medium size}\label{medium-size}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"rio"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To import multiple files in a directory

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(}\FunctionTok{import\_list}\NormalTok{(}\FunctionTok{dir}\NormalTok{()), }\AttributeTok{which =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To export a single data file

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{export}\NormalTok{(data, }\StringTok{"data.csv"}\NormalTok{)}
\FunctionTok{export}\NormalTok{(data,}\StringTok{"data.dta"}\NormalTok{)}
\FunctionTok{export}\NormalTok{(data,}\StringTok{"data.txt"}\NormalTok{)}
\FunctionTok{export}\NormalTok{(data,}\StringTok{"data\_cyl.rds"}\NormalTok{)}
\FunctionTok{export}\NormalTok{(data,}\StringTok{"data.rdata"}\NormalTok{)}
\FunctionTok{export}\NormalTok{(data,}\StringTok{"data.R"}\NormalTok{)}
\FunctionTok{export}\NormalTok{(data,}\StringTok{"data.csv.zip"}\NormalTok{)}
\FunctionTok{export}\NormalTok{(data,}\StringTok{"list.json"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To export multiple data files

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{export}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{mtcars =}\NormalTok{ mtcars, }\AttributeTok{iris =}\NormalTok{ iris), }\StringTok{"data\_file\_type"}\NormalTok{) }
\CommentTok{\# where data\_file\_type should substituted with the extension listed above}
\end{Highlighting}
\end{Shaded}

To convert between data file types

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# convert Stata to SPSS}
\FunctionTok{convert}\NormalTok{(}\StringTok{"data.dta"}\NormalTok{, }\StringTok{"data.sav"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{large-size}{%
\subsection{Large size}\label{large-size}}

Use R on a cluster

\begin{itemize}
\tightlist
\item
  Amazon Web Service (AWS): \$1/hr
\end{itemize}

Import files as chunks

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{file\_in    }\OtherTok{\textless{}{-}} \FunctionTok{file}\NormalTok{(}\StringTok{"in.csv"}\NormalTok{,}\StringTok{"r"}\NormalTok{)}
\NormalTok{chunk\_size }\OtherTok{\textless{}{-}} \DecValTok{100000} \CommentTok{\# choose the best size for you}
\NormalTok{x          }\OtherTok{\textless{}{-}} \FunctionTok{readLines}\NormalTok{(file\_in, }\AttributeTok{n=}\NormalTok{chunk\_size)}
\end{Highlighting}
\end{Shaded}

\texttt{data.table} method

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{require}\NormalTok{(data.table)}
\NormalTok{mydata }\OtherTok{=} \FunctionTok{fread}\NormalTok{(}\StringTok{"in.csv"}\NormalTok{, }\AttributeTok{header =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\texttt{ff} package: this method does not allow you to pass connections

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"ff"}\NormalTok{)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{read.csv.ffdf}\NormalTok{(}
    \AttributeTok{file =} \StringTok{"file.csv"}\NormalTok{,}
    \AttributeTok{nrow =} \DecValTok{10}\NormalTok{,}
    \AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{VERBOSE =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{first.rows =} \DecValTok{10000}\NormalTok{,}
    \AttributeTok{next.rows =} \DecValTok{50000}\NormalTok{,}
    \AttributeTok{colClasses =} \ConstantTok{NA}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\texttt{bigmemory} package

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_data }\OtherTok{\textless{}{-}} \FunctionTok{read.big.matrix}\NormalTok{(}\StringTok{\textquotesingle{}in.csv\textquotesingle{}}\NormalTok{, }\AttributeTok{header =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\texttt{sqldf} package

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(sqldf)}
\NormalTok{my\_data }\OtherTok{\textless{}{-}} \FunctionTok{read.csv.sql}\NormalTok{(}\StringTok{\textquotesingle{}in.csv\textquotesingle{}}\NormalTok{)}

\NormalTok{iris2 }\OtherTok{\textless{}{-}} \FunctionTok{read.csv.sql}\NormalTok{(}\StringTok{"iris.csv"}\NormalTok{, }
    \AttributeTok{sql =} \StringTok{"select * from file where Species = \textquotesingle{}setosa\textquotesingle{} "}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(RMySQL)}
\end{Highlighting}
\end{Shaded}

\texttt{RQLite} package

\begin{itemize}
\tightlist
\item
  \href{https://sqlite.org/download.html}{Download SQLite}, pick ``A bundle of command-line tools for managing SQLite database files'' for Window 10
\item
  Unzip file, and open \texttt{sqlite3.exe.}
\item
  Type in the prompt

  \begin{itemize}
  \tightlist
  \item
    \texttt{sqlite\textgreater{}\ .cd\ \textquotesingle{}C:\textbackslash{}Users\textbackslash{}data\textquotesingle{}} specify path to your desired directory
  \item
    \texttt{sqlite\textgreater{}\ .open\ database\_name.db} to open a database
  \item
    To import the CSV file into the database

    \begin{itemize}
    \tightlist
    \item
      \texttt{sqlite\textgreater{}\ .mode\ csv} specify to SQLite that the next file is .csv file
    \item
      \texttt{sqlite\textgreater{}\ .import\ file\_name.csv\ datbase\_name} to import the csv file to the database
    \end{itemize}
  \item
    \texttt{sqlite\textgreater{}\ .exit} After you're done, exit the sqlite program
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(DBI)}
\FunctionTok{library}\NormalTok{(dplyr)}
\FunctionTok{library}\NormalTok{(}\StringTok{"RSQLite"}\NormalTok{)}
\FunctionTok{setwd}\NormalTok{(}\StringTok{""}\NormalTok{)}
\NormalTok{con }\OtherTok{\textless{}{-}} \FunctionTok{dbConnect}\NormalTok{(RSQLite}\SpecialCharTok{::}\FunctionTok{SQLite}\NormalTok{(), }\StringTok{"data\_base.db"}\NormalTok{)}
\NormalTok{tbl }\OtherTok{\textless{}{-}} \FunctionTok{tbl}\NormalTok{(con, }\StringTok{"data\_table"}\NormalTok{)}
\NormalTok{tbl }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{filter}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{select}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{collect}\NormalTok{() }\CommentTok{\# to actually pull the data into the workspace}
\FunctionTok{dbDisconnect}\NormalTok{(con)}
\end{Highlighting}
\end{Shaded}

\texttt{arrow} package

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"arrow"}\NormalTok{)}
\FunctionTok{read\_csv\_arrow}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\texttt{vroom} package

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(vroom)}
\FunctionTok{spec}\NormalTok{(}\FunctionTok{vroom}\NormalTok{(file\_path))}
\NormalTok{compressed }\OtherTok{\textless{}{-}} \FunctionTok{vroom\_example}\NormalTok{(}\StringTok{"mtcars.csv.zip"}\NormalTok{)}
\FunctionTok{vroom}\NormalTok{(compressed)}
\end{Highlighting}
\end{Shaded}

\texttt{data.table} package

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{s }\OtherTok{=} \FunctionTok{fread}\NormalTok{(}\StringTok{"sample.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Comparisons regarding storage space

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test }\OtherTok{=}\NormalTok{ ff}\SpecialCharTok{::}\FunctionTok{read.csv.ffdf}\NormalTok{(}\AttributeTok{file =} \StringTok{""}\NormalTok{)}
\FunctionTok{object.size}\NormalTok{(test) }\CommentTok{\# worst}

\NormalTok{test1 }\OtherTok{=}\NormalTok{ data.table}\SpecialCharTok{::}\FunctionTok{fread}\NormalTok{(}\AttributeTok{file =} \StringTok{""}\NormalTok{)}
\FunctionTok{object.size}\NormalTok{(test1) }\CommentTok{\# best}

\NormalTok{test2 }\OtherTok{=}\NormalTok{ readr}\SpecialCharTok{::}\FunctionTok{read\_csv}\NormalTok{(}\StringTok{""}\NormalTok{)}\ErrorTok{)}
\FunctionTok{object.size}\NormalTok{(test2) }\CommentTok{\# 2nd}

\NormalTok{test3 }\OtherTok{=} \FunctionTok{vroom}\NormalTok{(}\AttributeTok{file =} \StringTok{""}\NormalTok{)}
\FunctionTok{object.size}\NormalTok{(test3) }\CommentTok{\# equal to read\_csv}
\end{Highlighting}
\end{Shaded}

To work with big data, you can convert it to \texttt{csv.gz} , but since typically, R would require you to load the whole data then export it. With data greater than 10 GB, we have to do it sequentially. Even though \texttt{read.csv} is much slower than \texttt{readr::read\_csv} , we still have to use it because it can pass connection, and it allows you to loop sequentially. On the other, because currently \texttt{readr::read\_csv} does not have the \texttt{skip} function, and even if we can use the skip, we still have to read and skip lines in previous loop.

For example, say you \texttt{read\_csv(,\ n\_max\ =\ 100,\ skip\ =0)} and then \texttt{read\_csv(,\ n\_max\ =\ 200,\ skip\ =\ 100)} you actually have to read again the first 100 rows. However, \texttt{read.csv} without specifying anything, will continue at the 100 mark.

Notice, sometimes you might have error looking like this

``Error in (function (con, what, n = 1L, size = NA\_integer\_, signed = TRUE, : can only read from a binary connection''

then you can change it instead of \texttt{r} in the connection into \texttt{rb} . Even though an author of the package suggested that \texttt{file} should be able to recognize the appropriate form, so far I did not prevail.

\hypertarget{data-manipulation}{%
\section{Data Manipulation}\label{data-manipulation}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load packages}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(lubridate)}


\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{23}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{45}\NormalTok{)}
\NormalTok{n }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{g }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"M"}\NormalTok{, }\StringTok{"M"}\NormalTok{, }\StringTok{"F"}\NormalTok{)}
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(n, g)}
\NormalTok{df}
\CommentTok{\#\textgreater{}   n g}
\CommentTok{\#\textgreater{} 1 1 M}
\CommentTok{\#\textgreater{} 2 3 M}
\CommentTok{\#\textgreater{} 3 5 F}
\FunctionTok{str}\NormalTok{(df)}
\CommentTok{\#\textgreater{} \textquotesingle{}data.frame\textquotesingle{}:    3 obs. of  2 variables:}
\CommentTok{\#\textgreater{}  $ n: num  1 3 5}
\CommentTok{\#\textgreater{}  $ g: chr  "M" "M" "F"}

\CommentTok{\#Similarly}
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(n, g)}
\NormalTok{df}
\CommentTok{\#\textgreater{} \# A tibble: 3 x 2}
\CommentTok{\#\textgreater{}       n g    }
\CommentTok{\#\textgreater{}   \textless{}dbl\textgreater{} \textless{}chr\textgreater{}}
\CommentTok{\#\textgreater{} 1     1 M    }
\CommentTok{\#\textgreater{} 2     3 M    }
\CommentTok{\#\textgreater{} 3     5 F}
\FunctionTok{str}\NormalTok{(df)}
\CommentTok{\#\textgreater{} tibble [3 x 2] (S3: tbl\_df/tbl/data.frame)}
\CommentTok{\#\textgreater{}  $ n: num [1:3] 1 3 5}
\CommentTok{\#\textgreater{}  $ g: chr [1:3] "M" "M" "F"}

\CommentTok{\# list form}
\NormalTok{lst }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(x, n, g, df)}
\NormalTok{lst}
\CommentTok{\#\textgreater{} [[1]]}
\CommentTok{\#\textgreater{} [1]  1  4 23  4 45}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} [[2]]}
\CommentTok{\#\textgreater{} [1] 1 3 5}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} [[3]]}
\CommentTok{\#\textgreater{} [1] "M" "M" "F"}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} [[4]]}
\CommentTok{\#\textgreater{} \# A tibble: 3 x 2}
\CommentTok{\#\textgreater{}       n g    }
\CommentTok{\#\textgreater{}   \textless{}dbl\textgreater{} \textless{}chr\textgreater{}}
\CommentTok{\#\textgreater{} 1     1 M    }
\CommentTok{\#\textgreater{} 2     3 M    }
\CommentTok{\#\textgreater{} 3     5 F}

\CommentTok{\# Or}
\NormalTok{lst2 }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{num =}\NormalTok{ x, }\AttributeTok{size =}\NormalTok{ n, }\AttributeTok{sex =}\NormalTok{ g, }\AttributeTok{data =}\NormalTok{ df)}
\NormalTok{lst2}
\CommentTok{\#\textgreater{} $num}
\CommentTok{\#\textgreater{} [1]  1  4 23  4 45}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $size}
\CommentTok{\#\textgreater{} [1] 1 3 5}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $sex}
\CommentTok{\#\textgreater{} [1] "M" "M" "F"}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $data}
\CommentTok{\#\textgreater{} \# A tibble: 3 x 2}
\CommentTok{\#\textgreater{}       n g    }
\CommentTok{\#\textgreater{}   \textless{}dbl\textgreater{} \textless{}chr\textgreater{}}
\CommentTok{\#\textgreater{} 1     1 M    }
\CommentTok{\#\textgreater{} 2     3 M    }
\CommentTok{\#\textgreater{} 3     5 F}

\CommentTok{\# Or}
\NormalTok{lst3 }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{7}\NormalTok{),}
             \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{),}
             \AttributeTok{z =} \FunctionTok{c}\NormalTok{(}\DecValTok{22}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{))}
\NormalTok{lst3}
\CommentTok{\#\textgreater{} $x}
\CommentTok{\#\textgreater{} [1] 1 3 5 7}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $y}
\CommentTok{\#\textgreater{} [1] 2 2 2 4 5 5 5 6}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $z}
\CommentTok{\#\textgreater{} [1] 22  3  3  3  5 10}

\CommentTok{\# find the means of x, y, z.}

\CommentTok{\# can do one at a time}
\FunctionTok{mean}\NormalTok{(lst3}\SpecialCharTok{$}\NormalTok{x)}
\CommentTok{\#\textgreater{} [1] 4}
\FunctionTok{mean}\NormalTok{(lst3}\SpecialCharTok{$}\NormalTok{y)}
\CommentTok{\#\textgreater{} [1] 3.875}
\FunctionTok{mean}\NormalTok{(lst3}\SpecialCharTok{$}\NormalTok{z)}
\CommentTok{\#\textgreater{} [1] 7.666667}

\CommentTok{\# list apply}
\FunctionTok{lapply}\NormalTok{(lst3, mean)}
\CommentTok{\#\textgreater{} $x}
\CommentTok{\#\textgreater{} [1] 4}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $y}
\CommentTok{\#\textgreater{} [1] 3.875}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $z}
\CommentTok{\#\textgreater{} [1] 7.666667}

\CommentTok{\# OR}
\FunctionTok{sapply}\NormalTok{(lst3, mean)}
\CommentTok{\#\textgreater{}        x        y        z }
\CommentTok{\#\textgreater{} 4.000000 3.875000 7.666667}

\CommentTok{\# Or, tidyverse function map() }
\FunctionTok{map}\NormalTok{(lst3, mean)}
\CommentTok{\#\textgreater{} $x}
\CommentTok{\#\textgreater{} [1] 4}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $y}
\CommentTok{\#\textgreater{} [1] 3.875}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $z}
\CommentTok{\#\textgreater{} [1] 7.666667}

\CommentTok{\# The tidyverse requires a modified map function called map\_dbl()}
\FunctionTok{map\_dbl}\NormalTok{(lst3, mean)}
\CommentTok{\#\textgreater{}        x        y        z }
\CommentTok{\#\textgreater{} 4.000000 3.875000 7.666667}


\CommentTok{\# Binding }
\NormalTok{dat01 }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{x =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{, }\AttributeTok{y =} \DecValTok{5}\SpecialCharTok{:}\DecValTok{1}\NormalTok{)}
\NormalTok{dat01}
\CommentTok{\#\textgreater{} \# A tibble: 5 x 2}
\CommentTok{\#\textgreater{}       x     y}
\CommentTok{\#\textgreater{}   \textless{}int\textgreater{} \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1     1     5}
\CommentTok{\#\textgreater{} 2     2     4}
\CommentTok{\#\textgreater{} 3     3     3}
\CommentTok{\#\textgreater{} 4     4     2}
\CommentTok{\#\textgreater{} 5     5     1}
\NormalTok{dat02 }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{x =} \DecValTok{10}\SpecialCharTok{:}\DecValTok{16}\NormalTok{, }\AttributeTok{y =}\NormalTok{ x}\SpecialCharTok{/}\DecValTok{2}\NormalTok{)}
\NormalTok{dat02}
\CommentTok{\#\textgreater{} \# A tibble: 7 x 2}
\CommentTok{\#\textgreater{}       x     y}
\CommentTok{\#\textgreater{}   \textless{}int\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1    10   5  }
\CommentTok{\#\textgreater{} 2    11   5.5}
\CommentTok{\#\textgreater{} 3    12   6  }
\CommentTok{\#\textgreater{} 4    13   6.5}
\CommentTok{\#\textgreater{} 5    14   7  }
\CommentTok{\#\textgreater{} 6    15   7.5}
\CommentTok{\#\textgreater{} 7    16   8}
\NormalTok{dat03 }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{z =} \FunctionTok{runif}\NormalTok{(}\DecValTok{5}\NormalTok{)) }\CommentTok{\# 5 random numbers from interval (0,1)}
\NormalTok{dat03}
\CommentTok{\#\textgreater{} \# A tibble: 5 x 1}
\CommentTok{\#\textgreater{}        z}
\CommentTok{\#\textgreater{}    \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 0.256 }
\CommentTok{\#\textgreater{} 2 0.317 }
\CommentTok{\#\textgreater{} 3 0.560 }
\CommentTok{\#\textgreater{} 4 0.745 }
\CommentTok{\#\textgreater{} 5 0.0436}

\CommentTok{\# row binding}
\FunctionTok{bind\_rows}\NormalTok{(dat01, dat02, dat01)}
\CommentTok{\#\textgreater{} \# A tibble: 17 x 2}
\CommentTok{\#\textgreater{}        x     y}
\CommentTok{\#\textgreater{}    \textless{}int\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1     1   5  }
\CommentTok{\#\textgreater{}  2     2   4  }
\CommentTok{\#\textgreater{}  3     3   3  }
\CommentTok{\#\textgreater{}  4     4   2  }
\CommentTok{\#\textgreater{}  5     5   1  }
\CommentTok{\#\textgreater{}  6    10   5  }
\CommentTok{\#\textgreater{}  7    11   5.5}
\CommentTok{\#\textgreater{}  8    12   6  }
\CommentTok{\#\textgreater{}  9    13   6.5}
\CommentTok{\#\textgreater{} 10    14   7  }
\CommentTok{\#\textgreater{} 11    15   7.5}
\CommentTok{\#\textgreater{} 12    16   8  }
\CommentTok{\#\textgreater{} 13     1   5  }
\CommentTok{\#\textgreater{} 14     2   4  }
\CommentTok{\#\textgreater{} 15     3   3  }
\CommentTok{\#\textgreater{} 16     4   2  }
\CommentTok{\#\textgreater{} 17     5   1}

\CommentTok{\# use ".id" argument to create a new column }
\CommentTok{\# that contains an identifier for the original data.}
\FunctionTok{bind\_rows}\NormalTok{(dat01, dat02, }\AttributeTok{.id =} \StringTok{"id"}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 12 x 3}
\CommentTok{\#\textgreater{}    id        x     y}
\CommentTok{\#\textgreater{}    \textless{}chr\textgreater{} \textless{}int\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1 1         1   5  }
\CommentTok{\#\textgreater{}  2 1         2   4  }
\CommentTok{\#\textgreater{}  3 1         3   3  }
\CommentTok{\#\textgreater{}  4 1         4   2  }
\CommentTok{\#\textgreater{}  5 1         5   1  }
\CommentTok{\#\textgreater{}  6 2        10   5  }
\CommentTok{\#\textgreater{}  7 2        11   5.5}
\CommentTok{\#\textgreater{}  8 2        12   6  }
\CommentTok{\#\textgreater{}  9 2        13   6.5}
\CommentTok{\#\textgreater{} 10 2        14   7  }
\CommentTok{\#\textgreater{} 11 2        15   7.5}
\CommentTok{\#\textgreater{} 12 2        16   8}

\CommentTok{\# with name}
\FunctionTok{bind\_rows}\NormalTok{(}\StringTok{"dat01"} \OtherTok{=}\NormalTok{ dat01, }\StringTok{"dat02"} \OtherTok{=}\NormalTok{ dat02, }\AttributeTok{.id =} \StringTok{"id"}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 12 x 3}
\CommentTok{\#\textgreater{}    id        x     y}
\CommentTok{\#\textgreater{}    \textless{}chr\textgreater{} \textless{}int\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1 dat01     1   5  }
\CommentTok{\#\textgreater{}  2 dat01     2   4  }
\CommentTok{\#\textgreater{}  3 dat01     3   3  }
\CommentTok{\#\textgreater{}  4 dat01     4   2  }
\CommentTok{\#\textgreater{}  5 dat01     5   1  }
\CommentTok{\#\textgreater{}  6 dat02    10   5  }
\CommentTok{\#\textgreater{}  7 dat02    11   5.5}
\CommentTok{\#\textgreater{}  8 dat02    12   6  }
\CommentTok{\#\textgreater{}  9 dat02    13   6.5}
\CommentTok{\#\textgreater{} 10 dat02    14   7  }
\CommentTok{\#\textgreater{} 11 dat02    15   7.5}
\CommentTok{\#\textgreater{} 12 dat02    16   8}

\CommentTok{\# bind\_rows() also works on lists of data frames}
\NormalTok{list01 }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\StringTok{"dat01"} \OtherTok{=}\NormalTok{ dat01, }\StringTok{"dat02"} \OtherTok{=}\NormalTok{ dat02)}
\NormalTok{list01}
\CommentTok{\#\textgreater{} $dat01}
\CommentTok{\#\textgreater{} \# A tibble: 5 x 2}
\CommentTok{\#\textgreater{}       x     y}
\CommentTok{\#\textgreater{}   \textless{}int\textgreater{} \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1     1     5}
\CommentTok{\#\textgreater{} 2     2     4}
\CommentTok{\#\textgreater{} 3     3     3}
\CommentTok{\#\textgreater{} 4     4     2}
\CommentTok{\#\textgreater{} 5     5     1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $dat02}
\CommentTok{\#\textgreater{} \# A tibble: 7 x 2}
\CommentTok{\#\textgreater{}       x     y}
\CommentTok{\#\textgreater{}   \textless{}int\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1    10   5  }
\CommentTok{\#\textgreater{} 2    11   5.5}
\CommentTok{\#\textgreater{} 3    12   6  }
\CommentTok{\#\textgreater{} 4    13   6.5}
\CommentTok{\#\textgreater{} 5    14   7  }
\CommentTok{\#\textgreater{} 6    15   7.5}
\CommentTok{\#\textgreater{} 7    16   8}
\FunctionTok{bind\_rows}\NormalTok{(list01)}
\CommentTok{\#\textgreater{} \# A tibble: 12 x 2}
\CommentTok{\#\textgreater{}        x     y}
\CommentTok{\#\textgreater{}    \textless{}int\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1     1   5  }
\CommentTok{\#\textgreater{}  2     2   4  }
\CommentTok{\#\textgreater{}  3     3   3  }
\CommentTok{\#\textgreater{}  4     4   2  }
\CommentTok{\#\textgreater{}  5     5   1  }
\CommentTok{\#\textgreater{}  6    10   5  }
\CommentTok{\#\textgreater{}  7    11   5.5}
\CommentTok{\#\textgreater{}  8    12   6  }
\CommentTok{\#\textgreater{}  9    13   6.5}
\CommentTok{\#\textgreater{} 10    14   7  }
\CommentTok{\#\textgreater{} 11    15   7.5}
\CommentTok{\#\textgreater{} 12    16   8}
\FunctionTok{bind\_rows}\NormalTok{(list01, }\AttributeTok{.id =} \StringTok{"source"}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 12 x 3}
\CommentTok{\#\textgreater{}    source     x     y}
\CommentTok{\#\textgreater{}    \textless{}chr\textgreater{}  \textless{}int\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1 dat01      1   5  }
\CommentTok{\#\textgreater{}  2 dat01      2   4  }
\CommentTok{\#\textgreater{}  3 dat01      3   3  }
\CommentTok{\#\textgreater{}  4 dat01      4   2  }
\CommentTok{\#\textgreater{}  5 dat01      5   1  }
\CommentTok{\#\textgreater{}  6 dat02     10   5  }
\CommentTok{\#\textgreater{}  7 dat02     11   5.5}
\CommentTok{\#\textgreater{}  8 dat02     12   6  }
\CommentTok{\#\textgreater{}  9 dat02     13   6.5}
\CommentTok{\#\textgreater{} 10 dat02     14   7  }
\CommentTok{\#\textgreater{} 11 dat02     15   7.5}
\CommentTok{\#\textgreater{} 12 dat02     16   8}

\CommentTok{\# The extended example below demonstrates how this can be very handy.}

\CommentTok{\# column binding}
\FunctionTok{bind\_cols}\NormalTok{(dat01, dat03)}
\CommentTok{\#\textgreater{} \# A tibble: 5 x 3}
\CommentTok{\#\textgreater{}       x     y      z}
\CommentTok{\#\textgreater{}   \textless{}int\textgreater{} \textless{}int\textgreater{}  \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1     1     5 0.256 }
\CommentTok{\#\textgreater{} 2     2     4 0.317 }
\CommentTok{\#\textgreater{} 3     3     3 0.560 }
\CommentTok{\#\textgreater{} 4     4     2 0.745 }
\CommentTok{\#\textgreater{} 5     5     1 0.0436}


\CommentTok{\# Regular expressions {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{names }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Ford, MS"}\NormalTok{, }\StringTok{"Jones, PhD"}\NormalTok{, }\StringTok{"Martin, Phd"}\NormalTok{, }\StringTok{"Huck, MA, MLS"}\NormalTok{)}

\CommentTok{\# pattern: first comma and everything after it}
\FunctionTok{str\_remove}\NormalTok{(names, }\AttributeTok{pattern =} \StringTok{", [[:print:]]+"}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] "Ford"   "Jones"  "Martin" "Huck"}

\CommentTok{\# [[:print:]]+ = one or more printable characters}


\CommentTok{\# Reshaping {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}

\CommentTok{\# Example of a wide data frame. Notice each person has multiple test scores}
\CommentTok{\# that span columns.}
\NormalTok{wide }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{name=}\FunctionTok{c}\NormalTok{(}\StringTok{"Clay"}\NormalTok{,}\StringTok{"Garrett"}\NormalTok{,}\StringTok{"Addison"}\NormalTok{), }
                   \AttributeTok{test1=}\FunctionTok{c}\NormalTok{(}\DecValTok{78}\NormalTok{, }\DecValTok{93}\NormalTok{, }\DecValTok{90}\NormalTok{), }
                   \AttributeTok{test2=}\FunctionTok{c}\NormalTok{(}\DecValTok{87}\NormalTok{, }\DecValTok{91}\NormalTok{, }\DecValTok{97}\NormalTok{),}
                   \AttributeTok{test3=}\FunctionTok{c}\NormalTok{(}\DecValTok{88}\NormalTok{, }\DecValTok{99}\NormalTok{, }\DecValTok{91}\NormalTok{))}
\NormalTok{wide}
\CommentTok{\#\textgreater{}      name test1 test2 test3}
\CommentTok{\#\textgreater{} 1    Clay    78    87    88}
\CommentTok{\#\textgreater{} 2 Garrett    93    91    99}
\CommentTok{\#\textgreater{} 3 Addison    90    97    91}

\CommentTok{\# Example of a long data frame. This is the same data as above, but in long}
\CommentTok{\# format. We have one row per person per test.}
\NormalTok{long }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{name=}\FunctionTok{rep}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"Clay"}\NormalTok{,}\StringTok{"Garrett"}\NormalTok{,}\StringTok{"Addison"}\NormalTok{),}\AttributeTok{each=}\DecValTok{3}\NormalTok{),}
                   \AttributeTok{test=}\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{),}
                   \AttributeTok{score=}\FunctionTok{c}\NormalTok{(}\DecValTok{78}\NormalTok{, }\DecValTok{87}\NormalTok{, }\DecValTok{88}\NormalTok{, }\DecValTok{93}\NormalTok{, }\DecValTok{91}\NormalTok{, }\DecValTok{99}\NormalTok{, }\DecValTok{90}\NormalTok{, }\DecValTok{97}\NormalTok{, }\DecValTok{91}\NormalTok{))}
\NormalTok{long}
\CommentTok{\#\textgreater{}      name test score}
\CommentTok{\#\textgreater{} 1    Clay    1    78}
\CommentTok{\#\textgreater{} 2    Clay    2    87}
\CommentTok{\#\textgreater{} 3    Clay    3    88}
\CommentTok{\#\textgreater{} 4 Garrett    1    93}
\CommentTok{\#\textgreater{} 5 Garrett    2    91}
\CommentTok{\#\textgreater{} 6 Garrett    3    99}
\CommentTok{\#\textgreater{} 7 Addison    1    90}
\CommentTok{\#\textgreater{} 8 Addison    2    97}
\CommentTok{\#\textgreater{} 9 Addison    3    91}

\CommentTok{\# mean score per student}
\FunctionTok{aggregate}\NormalTok{(score }\SpecialCharTok{\textasciitilde{}}\NormalTok{ name, }\AttributeTok{data =}\NormalTok{ long, mean)}
\CommentTok{\#\textgreater{}      name    score}
\CommentTok{\#\textgreater{} 1 Addison 92.66667}
\CommentTok{\#\textgreater{} 2    Clay 84.33333}
\CommentTok{\#\textgreater{} 3 Garrett 94.33333}
\FunctionTok{aggregate}\NormalTok{(score }\SpecialCharTok{\textasciitilde{}}\NormalTok{ test, }\AttributeTok{data =}\NormalTok{ long, mean)}
\CommentTok{\#\textgreater{}   test    score}
\CommentTok{\#\textgreater{} 1    1 87.00000}
\CommentTok{\#\textgreater{} 2    2 91.66667}
\CommentTok{\#\textgreater{} 3    3 92.66667}

\CommentTok{\# line plot of scores over test, grouped by name}
\FunctionTok{ggplot}\NormalTok{(long, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{factor}\NormalTok{(test), }\AttributeTok{y =}\NormalTok{ score, }\AttributeTok{color =}\NormalTok{ name, }\AttributeTok{group =}\NormalTok{ name)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"Test"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{02.3-data-mani_files/figure-latex/unnamed-chunk-17-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]


\DocumentationTok{\#\#\#\# reshape wide to long}
\FunctionTok{pivot\_longer}\NormalTok{(wide, test1}\SpecialCharTok{:}\NormalTok{test3, }\AttributeTok{names\_to =} \StringTok{"test"}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{"score"}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 9 x 3}
\CommentTok{\#\textgreater{}   name    test  score}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}   \textless{}chr\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Clay    test1    78}
\CommentTok{\#\textgreater{} 2 Clay    test2    87}
\CommentTok{\#\textgreater{} 3 Clay    test3    88}
\CommentTok{\#\textgreater{} 4 Garrett test1    93}
\CommentTok{\#\textgreater{} 5 Garrett test2    91}
\CommentTok{\#\textgreater{} 6 Garrett test3    99}
\CommentTok{\#\textgreater{} 7 Addison test1    90}
\CommentTok{\#\textgreater{} 8 Addison test2    97}
\CommentTok{\#\textgreater{} 9 Addison test3    91}

\CommentTok{\# Or}
\FunctionTok{pivot\_longer}\NormalTok{(wide, }\SpecialCharTok{{-}}\NormalTok{name, }\AttributeTok{names\_to =} \StringTok{"test"}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{"score"}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 9 x 3}
\CommentTok{\#\textgreater{}   name    test  score}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}   \textless{}chr\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Clay    test1    78}
\CommentTok{\#\textgreater{} 2 Clay    test2    87}
\CommentTok{\#\textgreater{} 3 Clay    test3    88}
\CommentTok{\#\textgreater{} 4 Garrett test1    93}
\CommentTok{\#\textgreater{} 5 Garrett test2    91}
\CommentTok{\#\textgreater{} 6 Garrett test3    99}
\CommentTok{\#\textgreater{} 7 Addison test1    90}
\CommentTok{\#\textgreater{} 8 Addison test2    97}
\CommentTok{\#\textgreater{} 9 Addison test3    91}

\CommentTok{\# drop "test" from the test column with names\_prefix argument}
\FunctionTok{pivot\_longer}\NormalTok{(wide, }\SpecialCharTok{{-}}\NormalTok{name, }\AttributeTok{names\_to =} \StringTok{"test"}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{"score"}\NormalTok{, }
             \AttributeTok{names\_prefix =} \StringTok{"test"}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 9 x 3}
\CommentTok{\#\textgreater{}   name    test  score}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}   \textless{}chr\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Clay    1        78}
\CommentTok{\#\textgreater{} 2 Clay    2        87}
\CommentTok{\#\textgreater{} 3 Clay    3        88}
\CommentTok{\#\textgreater{} 4 Garrett 1        93}
\CommentTok{\#\textgreater{} 5 Garrett 2        91}
\CommentTok{\#\textgreater{} 6 Garrett 3        99}
\CommentTok{\#\textgreater{} 7 Addison 1        90}
\CommentTok{\#\textgreater{} 8 Addison 2        97}
\CommentTok{\#\textgreater{} 9 Addison 3        91}

\DocumentationTok{\#\#\#\# reshape long to wide }
\FunctionTok{pivot\_wider}\NormalTok{(long, name, }\AttributeTok{names\_from =}\NormalTok{ test, }\AttributeTok{values\_from =}\NormalTok{ score)}
\CommentTok{\#\textgreater{} \# A tibble: 3 x 4}
\CommentTok{\#\textgreater{}   name      \textasciigrave{}1\textasciigrave{}   \textasciigrave{}2\textasciigrave{}   \textasciigrave{}3\textasciigrave{}}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}   \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Clay       78    87    88}
\CommentTok{\#\textgreater{} 2 Garrett    93    91    99}
\CommentTok{\#\textgreater{} 3 Addison    90    97    91}

\CommentTok{\# using the names\_prefix argument lets us prepend text to the column names.}
\FunctionTok{pivot\_wider}\NormalTok{(long, name, }\AttributeTok{names\_from =}\NormalTok{ test, }\AttributeTok{values\_from =}\NormalTok{ score,}
            \AttributeTok{names\_prefix =} \StringTok{"test"}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 3 x 4}
\CommentTok{\#\textgreater{}   name    test1 test2 test3}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}   \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Clay       78    87    88}
\CommentTok{\#\textgreater{} 2 Garrett    93    91    99}
\CommentTok{\#\textgreater{} 3 Addison    90    97    91}
\end{Highlighting}
\end{Shaded}

The verbs of data manipulation

\begin{itemize}
\tightlist
\item
  \texttt{select}: selecting (or not selecting) columns based on their names (eg: select columns Q1 through Q25)
\item
  \texttt{slice}: selecting (or not selecting) rows based on their position (eg: select rows 1:10)
\item
  \texttt{mutate}: add or derive new columns (or variables) based on existing columns (eg: create a new column that expresses measurement in cm based on existing measure in inches)
\item
  \texttt{rename}: rename variables or change column names (eg: change ``GraduationRate100'' to ``grad100'')
\item
  \texttt{filter}: selecting rows based on a condition (eg: all rows where gender = Male)
\item
  \texttt{arrange}: ordering rows based on variable(s) numeric or alphabetical order (eg: sort in descending order of Income)
\item
  \texttt{sample}: take random samples of data (eg: sample 80\% of data to create a ``training'' set)
\item
  \texttt{summarize}: condense or aggregate multiple values into single summary values (eg: calculate median income by age group)
\item
  \texttt{group\_by}: convert a tbl into a grouped tbl so that operations are performed ``by group''; allows us to summarize data or apply verbs to data by groups (eg, by gender or treatment)
\item
  the pipe: \texttt{\%\textgreater{}\%}

  \begin{itemize}
  \item
    Use Ctrl + Shift + M (Win) or Cmd + Shift + M (Mac) to enter in RStudio
  \item
    The pipe takes the output of a function and ``pipes'' into the first argument of the next function.
  \item
    new pipe is \texttt{\textbar{}\textgreater{}} It should be identical to the old one, except for certain special cases.
  \end{itemize}
\item
  \texttt{:=} (Walrus operator): similar to \texttt{=} , but for cases where you want to use the \texttt{glue} package (i.e., dynamic changes in the variable name in the left-hand side)
\end{itemize}

Writing function in R

Tunneling

\texttt{\{\{} (called curly-curly) allows you to tunnel data-variables through arg-variables (i.e., function arguments)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{get\_mean }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, group\_var, var\_to\_mean)\{}
\NormalTok{    data }\SpecialCharTok{\%\textgreater{}\%} 
        \FunctionTok{group\_by}\NormalTok{(\{\{group\_var\}\}) }\SpecialCharTok{\%\textgreater{}\%} 
        \FunctionTok{summarize}\NormalTok{(}\AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(\{\{var\_to\_mean\}\}))}
\NormalTok{\}}

\FunctionTok{data}\NormalTok{(}\StringTok{"mtcars"}\NormalTok{)}
\FunctionTok{head}\NormalTok{(mtcars)}
\CommentTok{\#\textgreater{}                    mpg cyl disp  hp drat    wt  qsec vs am gear carb}
\CommentTok{\#\textgreater{} Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4}
\CommentTok{\#\textgreater{} Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4}
\CommentTok{\#\textgreater{} Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1}
\CommentTok{\#\textgreater{} Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1}
\CommentTok{\#\textgreater{} Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2}
\CommentTok{\#\textgreater{} Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1}

\NormalTok{mtcars }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{get\_mean}\NormalTok{(}\AttributeTok{group\_var =}\NormalTok{ cyl, }\AttributeTok{var\_to\_mean =}\NormalTok{ mpg)}
\CommentTok{\#\textgreater{} \# A tibble: 3 x 2}
\CommentTok{\#\textgreater{}     cyl  mean}
\CommentTok{\#\textgreater{}   \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1     4  26.7}
\CommentTok{\#\textgreater{} 2     6  19.7}
\CommentTok{\#\textgreater{} 3     8  15.1}

\CommentTok{\# to change the resulting variable name dynamically, }
\CommentTok{\# you can use the glue interpolation (i.e., \textasciigrave{}\{\{\textasciigrave{}) and Walrus operator (\textasciigrave{}:=\textasciigrave{})}
\NormalTok{get\_mean }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, group\_var, var\_to\_mean, }\AttributeTok{prefix =} \StringTok{"mean\_of"}\NormalTok{)\{}
\NormalTok{    data }\SpecialCharTok{\%\textgreater{}\%} 
        \FunctionTok{group\_by}\NormalTok{(\{\{group\_var\}\}) }\SpecialCharTok{\%\textgreater{}\%} 
        \FunctionTok{summarize}\NormalTok{(}\StringTok{"\{prefix\}\_\{\{var\_to\_mean\}\}"} \SpecialCharTok{:=} \FunctionTok{mean}\NormalTok{(\{\{var\_to\_mean\}\}))}
\NormalTok{\}}

\NormalTok{mtcars }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{get\_mean}\NormalTok{(}\AttributeTok{group\_var =}\NormalTok{ cyl, }\AttributeTok{var\_to\_mean =}\NormalTok{ mpg)}
\CommentTok{\#\textgreater{} \# A tibble: 3 x 2}
\CommentTok{\#\textgreater{}     cyl mean\_of\_mpg}
\CommentTok{\#\textgreater{}   \textless{}dbl\textgreater{}       \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1     4        26.7}
\CommentTok{\#\textgreater{} 2     6        19.7}
\CommentTok{\#\textgreater{} 3     8        15.1}
\end{Highlighting}
\end{Shaded}

\hypertarget{part-i.-basic}{%
\part*{I. BASIC}\label{part-i.-basic}}
\addcontentsline{toc}{part}{I. BASIC}

\hypertarget{descriptive-stat}{%
\chapter{Descriptive Statistics}\label{descriptive-stat}}

When you have an area of interest that you want to research, a problem that you want to solve, a relationship that you want to investigate, theoretical and empirical processes will help you.

Estimand is defined as ``a quantity of scientific interest that can be calculated in the population and does not change its value depending on the data collection design used to measure it (i.e., it does not vary with sample size and survey design, or the number of non-respondents, or follow-up efforts).'' \citep{Rubin_1996}

Estimands include:

\begin{itemize}
\tightlist
\item
  population means
\item
  Population variances
\item
  correlations
\item
  factor loading
\item
  regression coefficients
\end{itemize}

\hypertarget{numerical-measures}{%
\section{Numerical Measures}\label{numerical-measures}}

There are differences between a population and a sample

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.0772}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3049}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3618}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2480}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Measures of
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Category
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Population
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Sample
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
- & What is it? & Reality & A small fraction of reality (inference) \\
- & Characteristics described by & Parameters & Statistics \\
Central Tendency & Mean & \(\mu = E(Y)\) & \(\hat{\mu} = \overline{y}\) \\
Central Tendency & Median & 50-th percentile & \(y_{(\frac{n+1}{2})}\) \\
Dispersion & Variance & \(\begin{aligned} \sigma^2 &= var(Y) \\ &= E(Y- \mu^2) \end{aligned}\) & \(s^2=\frac{1}{n-1} \sum_{i = 1}^{n} (y_i-\overline{y})^2\) \\
Dispersion & Coefficient of Variation & \(\frac{\sigma}{\mu}\) & \(\frac{s}{\overline{y}}\) \\
Dispersion & Interquartile Range & difference between 25th and 75th percentiles. Robust to outliers & \\
Shape & Skewness Standardized 3rd central moment (unitless) & \(g_1=\frac{\mu_3}{\mu_2^{3/2}}\) & \(\hat{g_1}=\frac{m_3}{m_2sqrt(m_2)}\) \\
Shape & Central moments & \(\mu=E(Y)\) \(\mu_2 = \sigma^2=E(Y-\mu)^2\) \(\mu_3 = E(Y-\mu)^3\) \(\mu_4 = E(Y-\mu)^4\) \textbar{} & \(m_2=\sum_{i=1}^{n}(y_1-\overline{y})^2/n\)

\(m_3=\sum_{i=1}^{n}(y_1-\overline{y})^3/n\) \\
Shape & Kurtosis (peakedness and tail thickness) Standardized 4th central moment & \(g_2^*=\frac{E(Y-\mu)^4}{\sigma^4}\) & \(\hat{g_2}=\frac{m_4}{m_2^2}-3\) \\
\end{longtable}

Note:

\begin{itemize}
\item
  Order Statistics: \(y_{(1)},y_{(2)},...,y_{(n)}\) where \(y_{(1)}<y_{(2)}<...<y_{(n)}\)
\item
  Coefficient of variation: standard deviation over mean. This metric is stable, dimensionless statistic for comparison.
\item
  Symmetric: mean = median, skewness = 0
\item
  Skewed right: mean \textgreater{} median, skewness \textgreater{} 0
\item
  Skewed left: mean \textless{} median, skewness \textless{} 0
\item
  Central moments: \(\mu=E(Y)\) , \(\mu_2 = \sigma^2=E(Y-\mu)^2\) , \(\mu_3 = E(Y-\mu)^3\), \(\mu_4 = E(Y-\mu)^4\)
\item
  For normal distributions, \(\mu_3=0\), so \(g_1=0\)
\item
  \(\hat{g_1}\) is distributed approximately as \(N(0,6/n)\) if sample is from a normal population. (valid when \(n > 150\))

  \begin{itemize}
  \tightlist
  \item
    For large samples, inference on skewness can be based on normal tables with 95\% confidence interval for \(g_1\) as \(\hat{g_1}\pm1.96\sqrt{6/n}\)
  \item
    For small samples, special tables from Snedecor and Cochran 1989, Table A 19(i) or Monte Carlo test
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2344}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1172}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.6406}}@{}}
\toprule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Kurtosis \textgreater{} 0 (leptokurtic) & heavier tail & compared to a normal distribution with the same \(\sigma\) (e.g., t-distribution) \\
Kurtosis \textless{} 0 (platykurtic) & lighter tail & compared to a normal distribution with the same \(\sigma\) \\
\end{longtable}

\begin{itemize}
\item
  For a normal distribution, \(g_2^*=3\). Kurtosis is often redefined as: \(g_2=\frac{E(Y-\mu)^4}{\sigma^4}-3\) where the 4th central moment is estimated by \(m_4=\sum_{i=1}^{n}(y_i-\overline{y})^4/n\)

  \begin{itemize}
  \tightlist
  \item
    the asymptotic sampling distribution for \(\hat{g_2}\) is approximately \(N(0,24/n)\) (with \(n > 1000\))
  \item
    large sample on kurtosis uses standard normal tables
  \item
    small sample uses tables by Snedecor and Cochran, 1989, Table A 19(ii) or Geary 1936
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OtherTok{=} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\FunctionTok{library}\NormalTok{(e1071)}
\FunctionTok{skewness}\NormalTok{(data)}
\CommentTok{\#\textgreater{} [1] {-}0.5285128}
\FunctionTok{kurtosis}\NormalTok{(data)}
\CommentTok{\#\textgreater{} [1] 0.6497711}
\end{Highlighting}
\end{Shaded}

\hypertarget{graphical-measures}{%
\section{Graphical Measures}\label{graphical-measures}}

\hypertarget{shape}{%
\subsection{Shape}\label{shape}}

It's a good habit to label your graph, so others can easily follow.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OtherTok{=} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{)}

\CommentTok{\# Histogram}
\FunctionTok{hist}\NormalTok{(data,}\AttributeTok{labels =}\NormalTok{ T,}\AttributeTok{col=}\StringTok{"grey"}\NormalTok{,}\AttributeTok{breaks =} \DecValTok{12}\NormalTok{) }

\CommentTok{\# Interactive histogram  }
\NormalTok{pacman}\SpecialCharTok{::}\FunctionTok{p\_load}\NormalTok{(}\StringTok{"highcharter"}\NormalTok{)}
\FunctionTok{hchart}\NormalTok{(data) }

\CommentTok{\# Box{-}and{-}Whisker plot}
\FunctionTok{boxplot}\NormalTok{(count }\SpecialCharTok{\textasciitilde{}}\NormalTok{ spray, }\AttributeTok{data =}\NormalTok{ InsectSprays,}\AttributeTok{col =} \StringTok{"lightgray"}\NormalTok{,}\AttributeTok{main=}\StringTok{"boxplot"}\NormalTok{)}

\CommentTok{\# Notched Boxplot}
\FunctionTok{boxplot}\NormalTok{(len}\SpecialCharTok{\textasciitilde{}}\NormalTok{supp}\SpecialCharTok{*}\NormalTok{dose, }\AttributeTok{data=}\NormalTok{ToothGrowth, }\AttributeTok{notch=}\ConstantTok{TRUE}\NormalTok{,}
  \AttributeTok{col=}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"gold"}\NormalTok{,}\StringTok{"darkgreen"}\NormalTok{)),}
  \AttributeTok{main=}\StringTok{"Tooth Growth"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Suppliment and Dose"}\NormalTok{)}
\CommentTok{\# If notches differ {-}\textgreater{} medians differ}

\CommentTok{\# Stem{-}and{-}Leaf Plots}
\FunctionTok{stem}\NormalTok{(data)}


\CommentTok{\# Bagplot {-} A 2D Boxplot Extension}
\NormalTok{pacman}\SpecialCharTok{::}\FunctionTok{p\_load}\NormalTok{(aplpack)}
\FunctionTok{attach}\NormalTok{(mtcars)}
\FunctionTok{bagplot}\NormalTok{(wt,mpg, }\AttributeTok{xlab=}\StringTok{"Car Weight"}\NormalTok{, }\AttributeTok{ylab=}\StringTok{"Miles Per Gallon"}\NormalTok{,}
  \AttributeTok{main=}\StringTok{"Bagplot Example"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Others more advanced plots

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# boxplot.matrix()  \#library("sfsmisc")}
\CommentTok{\# boxplot.n()       \#library("gplots")}
\CommentTok{\# vioplot()         \#library("vioplot")}
\end{Highlighting}
\end{Shaded}

\hypertarget{scatterplot}{%
\subsection{Scatterplot}\label{scatterplot}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# pairs(mtcars)}
\end{Highlighting}
\end{Shaded}

\hypertarget{normality-assessment}{%
\section{Normality Assessment}\label{normality-assessment}}

Since Normal (Gaussian) distribution has many applications, we typically want/ wish our data or our variable is normal. Hence, we have to assess the normality based on not only \protect\hyperlink{numerical-measures}{Numerical Measures} but also \protect\hyperlink{graphical-measures}{Graphical Measures}

\hypertarget{graphical-assessment}{%
\subsection{Graphical Assessment}\label{graphical-assessment}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pacman}\SpecialCharTok{::}\FunctionTok{p\_load}\NormalTok{(}\StringTok{"car"}\NormalTok{)}
\FunctionTok{qqnorm}\NormalTok{(precip, }\AttributeTok{ylab =} \StringTok{"Precipitation [in/yr] for 70 US cities"}\NormalTok{)}
\FunctionTok{qqline}\NormalTok{(precip)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{03-descriptive-stat_files/figure-latex/unnamed-chunk-5-1} \end{center}

The straight line represents the theoretical line for normally distributed data. The dots represent real empirical data that we are checking. If all the dots fall on the straight line, we can be confident that our data follow a normal distribution. If our data wiggle and deviate from the line, we should be concerned with the normality assumption.

\hypertarget{summary-statistics}{%
\subsection{Summary Statistics}\label{summary-statistics}}

Sometimes it's hard to tell whether your data follow the normal distribution by just looking at the graph. Hence, we often have to conduct statistical test to aid our decision. Common tests are

\begin{itemize}
\item
  \protect\hyperlink{methods-based-on-normal-probability-plot}{Methods based on normal probability plot}

  \begin{itemize}
  \tightlist
  \item
    \protect\hyperlink{correlation-coefficient-with-normal-probability-plots}{Correlation Coefficient with Normal Probability Plots}
  \item
    \protect\hyperlink{shapiro-wilk-test}{Shapiro-Wilk Test}
  \end{itemize}
\item
  \protect\hyperlink{methods-based-on-empirical-cumulative-distribution-function}{Methods based on empirical cumulative distribution function}

  \begin{itemize}
  \tightlist
  \item
    \protect\hyperlink{anderson-darling-test}{Anderson-Darling Test}
  \item
    \protect\hyperlink{kolmogorov-smirnov-test}{Kolmogorov-Smirnov Test}
  \item
    \protect\hyperlink{cramer-von-mises-test}{Cramer-von Mises Test}
  \item
    \protect\hyperlink{jarquebera-test}{Jarque--Bera Test}
  \end{itemize}
\end{itemize}

\hypertarget{methods-based-on-normal-probability-plot}{%
\subsubsection{Methods based on normal probability plot}\label{methods-based-on-normal-probability-plot}}

\hypertarget{correlation-coefficient-with-normal-probability-plots}{%
\paragraph{Correlation Coefficient with Normal Probability Plots}\label{correlation-coefficient-with-normal-probability-plots}}

\citep{Looney_1985} \citep{Shapiro_1972} The correlation coefficient between \(y_{(i)}\) and \(m_i^*\) as given on the normal probability plot:

\[W^*=\frac{\sum_{i=1}^{n}(y_{(i)}-\bar{y})(m_i^*-0)}{(\sum_{i=1}^{n}(y_{(i)}-\bar{y})^2\sum_{i=1}^{n}(m_i^*-0)^2)^.5}\]

where \(\bar{m^*}=0\)

Pearson product moment formula for correlation:

\[\hat{p}=\frac{\sum_{i-1}^{n}(y_i-\bar{y})(x_i-\bar{x})}{(\sum_{i=1}^{n}(y_{i}-\bar{y})^2\sum_{i=1}^{n}(x_i-\bar{x})^2)^.5}\]

\begin{itemize}
\tightlist
\item
  When the correlation is 1, the plot is exactly linear and normality is assumed.
\item
  The closer the correlation is to zero, the more confident we are to reject normality
\item
  Inference on W* needs to be based on special tables \citep{Looney_1985}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"EnvStats"}\NormalTok{)}
\FunctionTok{gofTest}\NormalTok{(data,}\AttributeTok{test=}\StringTok{"ppcc"}\NormalTok{)}\SpecialCharTok{$}\NormalTok{p.value }\CommentTok{\#Probability Plot Correlation Coefficient }
\CommentTok{\#\textgreater{} [1] 0.02720557}
\end{Highlighting}
\end{Shaded}

\hypertarget{shapiro-wilk-test}{%
\paragraph{Shapiro-Wilk Test}\label{shapiro-wilk-test}}

\citep{Shapiro_1965}

\[W=(\frac{\sum_{i=1}^{n}a_i(y_{(i)}-\bar{y})(m_i^*-0)}{(\sum_{i=1}^{n}a_i^2(y_{(i)}-\bar{y})^2\sum_{i=1}^{n}(m_i^*-0)^2)^.5})^2\]

where \(a_1,..,a_n\) are weights computed from the covariance matrix for the order statistics.

\begin{itemize}
\tightlist
\item
  Researchers typically use this test to assess normality. (n \textless{} 2000) Under normality, W is close to 1, just like \(W^*\). Notice that the only difference between W and W* is the ``weights''.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gofTest}\NormalTok{(data,}\AttributeTok{test=}\StringTok{"sw"}\NormalTok{)}\SpecialCharTok{$}\NormalTok{p.value }\CommentTok{\#Shapiro{-}Wilk is the default.}
\CommentTok{\#\textgreater{} [1] 0.03744017}
\end{Highlighting}
\end{Shaded}

\hypertarget{methods-based-on-empirical-cumulative-distribution-function}{%
\subsubsection{Methods based on empirical cumulative distribution function}\label{methods-based-on-empirical-cumulative-distribution-function}}

The formula for the empirical cumulative distribution function (CDF) is:

\(F_n(t)\) = estimate of probability that an observation \(\le\) t = (number of observation \(\le\) t)/n

This method requires large sample sizes. However, it can apply to distributions other than the normal (Gaussian) one.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Empirical CDF hand{-}code}
\FunctionTok{plot.ecdf}\NormalTok{(data,}\AttributeTok{verticals =}\NormalTok{ T, }\AttributeTok{do.points=}\NormalTok{F)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{03-descriptive-stat_files/figure-latex/unnamed-chunk-8-1} \end{center}

\hypertarget{anderson-darling-test}{%
\paragraph{Anderson-Darling Test}\label{anderson-darling-test}}

The Anderson-Darling statistic \citep{Anderson_1952}:

\[A^2=\int_{-\infty}^{\infty}(F_n(t)=F(t))^2\frac{dF(t)}{F(t)(1-F(t))}\]

\begin{itemize}
\tightlist
\item
  a weight average of squared deviations (it weights small and large values of t more)
\end{itemize}

For the normal distribution,

\(A^2 = - (\sum_{i=1}^{n}(2i-1)(ln(p_i) +ln(1-p_{n+1-i}))/n-n\)

where \(p_i=\Phi(\frac{y_{(i)}-\bar{y}}{s})\), the probability that a standard normal variable is less than \(\frac{y_{(i)}-\bar{y}}{s}\)

\begin{itemize}
\item
  Reject normal assumption when \(A^2\) is too large
\item
  Evaluate the null hypothesis that the observations are randomly selected from a normal population based on the critical value provided by \citep{Marsaglia_2004} and \citep{Stephens_1974}
\item
  This test can be applied to other distributions:

  \begin{itemize}
  \tightlist
  \item
    Exponential
  \item
    Logistic
  \item
    Gumbel
  \item
    Extreme-value
  \item
    Weibull: log(Weibull) = Gumbel
  \item
    Gamma
  \item
    Logistic
  \item
    Cauchy
  \item
    von Mises
  \item
    Log-normal (two-parameter)
  \end{itemize}
\end{itemize}

Consult \citep{Stephens_1974} for more detailed transformation and critical values.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gofTest}\NormalTok{(data,}\AttributeTok{test=}\StringTok{"ad"}\NormalTok{)}\SpecialCharTok{$}\NormalTok{p.value }\CommentTok{\#Anderson{-}Darling}
\CommentTok{\#\textgreater{} [1] 0.04245868}
\end{Highlighting}
\end{Shaded}

\hypertarget{kolmogorov-smirnov-test}{%
\paragraph{Kolmogorov-Smirnov Test}\label{kolmogorov-smirnov-test}}

\begin{itemize}
\tightlist
\item
  Based on the largest absolute difference between empirical and expected cumulative distribution
\item
  Another deviation of K-S test is Kuiper's test
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gofTest}\NormalTok{(data,}\AttributeTok{test=}\StringTok{"ks"}\NormalTok{)}\SpecialCharTok{$}\NormalTok{p.value }\CommentTok{\#Komogorov{-}Smirnov }
\CommentTok{\#\textgreater{} [1] 0.4097154}
\end{Highlighting}
\end{Shaded}

\hypertarget{cramer-von-mises-test}{%
\paragraph{Cramer-von Mises Test}\label{cramer-von-mises-test}}

\begin{itemize}
\tightlist
\item
  Based on the average squared discrepancy between the empirical distribution and a given theoretical distribution. Each discrepancy is weighted equally (unlike Anderson-Darling test weights end points more heavily)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gofTest}\NormalTok{(data,}\AttributeTok{test=}\StringTok{"cvm"}\NormalTok{)}\SpecialCharTok{$}\NormalTok{p.value }\CommentTok{\#Cramer{-}von Mises}
\CommentTok{\#\textgreater{} [1] 0.06912725}
\end{Highlighting}
\end{Shaded}

\hypertarget{jarquebera-test}{%
\paragraph{Jarque--Bera Test}\label{jarquebera-test}}

\citep{Bera_1981}

Based on the skewness and kurtosis to test normality.

\(JB = \frac{n}{6}(S^2+(K-3)^2/4)\) where \(S\) is the sample skewness and \(K\) is the sample kurtosis

\(S=\frac{\hat{\mu_3}}{\hat{\sigma}^3}=\frac{\sum_{i=1}^{n}(x_i-\bar{x})^3/n}{(\sum_{i=1}^{n}(x_i-\bar{x})^2/n)^\frac{3}{2}}\)

\(K=\frac{\hat{\mu_4}}{\hat{\sigma}^4}=\frac{\sum_{i=1}^{n}(x_i-\bar{x})^4/n}{(\sum_{i=1}^{n}(x_i-\bar{x})^2/n)^2}\)

recall \(\hat{\sigma^2}\) is the estimate of the second central moment (variance) \(\hat{\mu_3}\) and \(\hat{\mu_4}\) are the estimates of third and fourth central moments.

If the data comes from a normal distribution, the JB statistic asymptotically has a chi-squared distribution with two degrees of freedom.

The null hypothesis is a joint hypothesis of the skewness being zero and the excess kurtosis being zero.

\hypertarget{bivariate-statistics}{%
\section{Bivariate Statistics}\label{bivariate-statistics}}

Correlation between

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{two-continuous}{Two Continuous} variables
\item
  \protect\hyperlink{two-discrete}{Two Discrete} variables
\item
  \protect\hyperlink{categorical-and-continuous}{Categorical and Continuous}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1856}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5464}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2577}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Categorical
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Continuous
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Categorical} & \protect\hyperlink{phi-coefficient}{Phi coefficient}

\protect\hyperlink{cramers-v}{Cramer's V}

\protect\hyperlink{tschuprows-t}{Tschuprow's T}

\protect\hyperlink{freemans-theta}{Freeman's Theta}

\protect\hyperlink{epsilon-squared}{Epsilon-squared}

\protect\hyperlink{goodman-kruskals-gamma}{Goodman Kruskal's Gamma}

\protect\hyperlink{somers-d}{Somers' D}

\protect\hyperlink{kendalls-tau-b}{Kendall's Tau-b}

\protect\hyperlink{yules-q-and-y}{Yule's Q and Y}

\protect\hyperlink{tetrachoric-correlation}{Tetrachoric Correlation}

\protect\hyperlink{polychoric-correlation}{Polychoric Correlation} & \\
\textbf{Continuous} & \protect\hyperlink{point-biserial-correlation}{Point-Biserial Correlation}

\protect\hyperlink{logistic-regression}{Logistic Regression} & \protect\hyperlink{pearson-correlation}{Pearson Correlation}

\protect\hyperlink{spearman-correlation}{Spearman Correlation} \\
\end{longtable}

Questions to keep in mind:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Is the relationship linear or non-linear?
\item
  If the variable is continuous, is it normal and homoskadastic?
\item
  How big is your dataset?
\end{enumerate}

\hypertarget{two-continuous}{%
\subsection{Two Continuous}\label{two-continuous}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{=} \DecValTok{100} \CommentTok{\# (sample size)}

\NormalTok{data }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{A =} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{20}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{size =}\NormalTok{ n),}
                  \AttributeTok{B =} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{30}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{size =}\NormalTok{ n))}
\end{Highlighting}
\end{Shaded}

\hypertarget{pearson-correlation}{%
\subsubsection{Pearson Correlation}\label{pearson-correlation}}

\begin{itemize}
\tightlist
\item
  Good with linear relationship
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(Hmisc)}
\FunctionTok{rcorr}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{A, data}\SpecialCharTok{$}\NormalTok{B, }\AttributeTok{type=}\StringTok{"pearson"}\NormalTok{) }
\CommentTok{\#\textgreater{}       x     y}
\CommentTok{\#\textgreater{} x  1.00 {-}0.13}
\CommentTok{\#\textgreater{} y {-}0.13  1.00}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} n= 100 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} P}
\CommentTok{\#\textgreater{}   x      y     }
\CommentTok{\#\textgreater{} x        0.1984}
\CommentTok{\#\textgreater{} y 0.1984}
\end{Highlighting}
\end{Shaded}

\hypertarget{spearman-correlation}{%
\subsubsection{Spearman Correlation}\label{spearman-correlation}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(Hmisc)}
\FunctionTok{rcorr}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{A, data}\SpecialCharTok{$}\NormalTok{B, }\AttributeTok{type=}\StringTok{"spearman"}\NormalTok{) }
\CommentTok{\#\textgreater{}       x     y}
\CommentTok{\#\textgreater{} x  1.00 {-}0.13}
\CommentTok{\#\textgreater{} y {-}0.13  1.00}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} n= 100 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} P}
\CommentTok{\#\textgreater{}   x     y    }
\CommentTok{\#\textgreater{} x       0.203}
\CommentTok{\#\textgreater{} y 0.203}
\end{Highlighting}
\end{Shaded}

\hypertarget{categorical-and-continuous}{%
\subsection{Categorical and Continuous}\label{categorical-and-continuous}}

\hypertarget{point-biserial-correlation}{%
\subsubsection{Point-Biserial Correlation}\label{point-biserial-correlation}}

Similar to the Pearson correlation coefficient, the point-biserial correlation coefficient is between -1 and 1 where:

\begin{itemize}
\item
  -1 means a perfectly negative correlation between two variables
\item
  0 means no correlation between two variables
\item
  1 means a perfectly positive correlation between two variables
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{12}\NormalTok{, }\DecValTok{14}\NormalTok{, }\DecValTok{17}\NormalTok{, }\DecValTok{17}\NormalTok{, }\DecValTok{11}\NormalTok{, }\DecValTok{22}\NormalTok{, }\DecValTok{23}\NormalTok{, }\DecValTok{11}\NormalTok{, }\DecValTok{19}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{12}\NormalTok{)}

\CommentTok{\#calculate point{-}biserial correlation}
\FunctionTok{cor.test}\NormalTok{(x, y)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Pearson\textquotesingle{}s product{-}moment correlation}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  x and y}
\CommentTok{\#\textgreater{} t = 0.67064, df = 9, p{-}value = 0.5193}
\CommentTok{\#\textgreater{} alternative hypothesis: true correlation is not equal to 0}
\CommentTok{\#\textgreater{} 95 percent confidence interval:}
\CommentTok{\#\textgreater{}  {-}0.4391885  0.7233704}
\CommentTok{\#\textgreater{} sample estimates:}
\CommentTok{\#\textgreater{}       cor }
\CommentTok{\#\textgreater{} 0.2181635}
\end{Highlighting}
\end{Shaded}

Alternatively

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ltm}\SpecialCharTok{::}\FunctionTok{biserial.cor}\NormalTok{(y,x, }\AttributeTok{use =} \FunctionTok{c}\NormalTok{(}\StringTok{"all.obs"}\NormalTok{), }\AttributeTok{level =} \DecValTok{2}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 0.2181635}
\end{Highlighting}
\end{Shaded}

\hypertarget{logistic-regression}{%
\subsubsection{Logistic Regression}\label{logistic-regression}}

See \ref{logistic-regression}

\hypertarget{two-discrete}{%
\subsection{Two Discrete}\label{two-discrete}}

\hypertarget{distance-metrics}{%
\subsubsection{Distance Metrics}\label{distance-metrics}}

Some consider distance is not a correlation metric because it isn't unit independent (i.e., if you scale the distance, the metrics will change), but it's still a useful proxy. Distance metrics are more likely to be used for similarity measure.

\begin{itemize}
\item
  Euclidean Distance
\item
  Manhattan Distance
\item
  Chessboard Distance
\item
  Minkowski Distance
\item
  Canberra Distance
\item
  Hamming Distance
\item
  Cosine Distance
\item
  Sum of Absolute Distance
\item
  Sum of Squared Distance
\item
  Mean-Absolute Error
\end{itemize}

\hypertarget{statistical-metrics}{%
\subsubsection{Statistical Metrics}\label{statistical-metrics}}

\hypertarget{chi-squared-test}{%
\paragraph{Chi-squared test}\label{chi-squared-test}}

\hypertarget{phi-coefficient}{%
\subparagraph{Phi coefficient}\label{phi-coefficient}}

\begin{itemize}
\tightlist
\item
  2 binary
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{5}\NormalTok{), }\AttributeTok{nrow =} \DecValTok{2}\NormalTok{)}
\NormalTok{dt}
\CommentTok{\#\textgreater{}      [,1] [,2]}
\CommentTok{\#\textgreater{} [1,]    1    3}
\CommentTok{\#\textgreater{} [2,]    4    5}
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{phi}\NormalTok{(dt)}
\CommentTok{\#\textgreater{} [1] {-}0.18}
\end{Highlighting}
\end{Shaded}

\hypertarget{cramers-v}{%
\subparagraph{Cramer's V}\label{cramers-v}}

\begin{itemize}
\tightlist
\item
  between nominal categorical variables (no natural order)
\end{itemize}

\[
\text{Cramer's V} = \sqrt{\frac{\chi^2/n}{\min(c-1,r-1)}}
\]

where

\begin{itemize}
\item
  \(\chi^2\) = Chi-square statistic
\item
  \(n\) = sample size
\item
  \(r\) = \# of rows
\item
  \(c\) = \# of columns
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{\textquotesingle{}lsr\textquotesingle{}}\NormalTok{)}
\NormalTok{n }\OtherTok{=} \DecValTok{100} \CommentTok{\# (sample size)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{data }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{A =} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{size =}\NormalTok{ n),}
                  \AttributeTok{B =} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{size =}\NormalTok{ n))}


\FunctionTok{cramersV}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{A, data}\SpecialCharTok{$}\NormalTok{B)}
\CommentTok{\#\textgreater{} [1] 0.1944616}
\end{Highlighting}
\end{Shaded}

Alternatively,

\begin{itemize}
\item
  \texttt{ncchisq} noncentral Chi-square
\item
  \texttt{nchisqadj} Adjusted noncentral Chi-square
\item
  \texttt{fisher} Fisher Z transformation
\item
  \texttt{fisheradj} bias correction Fisher z transformation
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{DescTools}\SpecialCharTok{::}\FunctionTok{CramerV}\NormalTok{(data, }\AttributeTok{conf.level =} \FloatTok{0.95}\NormalTok{,}\AttributeTok{method =} \StringTok{"ncchisqadj"}\NormalTok{)}
\CommentTok{\#\textgreater{}  Cramer V    lwr.ci    upr.ci }
\CommentTok{\#\textgreater{} 0.3472325 0.3929964 0.4033053}
\end{Highlighting}
\end{Shaded}

\hypertarget{tschuprows-t}{%
\subparagraph{Tschuprow's T}\label{tschuprows-t}}

\begin{itemize}
\tightlist
\item
  2 nominal variables
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{DescTools}\SpecialCharTok{::}\FunctionTok{TschuprowT}\NormalTok{(data)}
\CommentTok{\#\textgreater{} [1] 0.1100808}
\end{Highlighting}
\end{Shaded}

\hypertarget{ordinal-association-rank-correlation}{%
\subsubsection{Ordinal Association (Rank correlation)}\label{ordinal-association-rank-correlation}}

\begin{itemize}
\tightlist
\item
  Good with non-linear relationship
\end{itemize}

\hypertarget{ordinal-and-nominal}{%
\paragraph{Ordinal and Nominal}\label{ordinal-and-nominal}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{=} \DecValTok{100} \CommentTok{\# (sample size)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{dt }\OtherTok{=} \FunctionTok{table}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(}
    \AttributeTok{A =} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{size =}\NormalTok{ n), }\CommentTok{\# ordinal}
    \AttributeTok{B =} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{size =}\NormalTok{ n)  }\CommentTok{\# nominal}
\NormalTok{)) }
\NormalTok{dt}
\CommentTok{\#\textgreater{}    B}
\CommentTok{\#\textgreater{} A    1  2  3}
\CommentTok{\#\textgreater{}   1  7 11  9}
\CommentTok{\#\textgreater{}   2 11  6 14}
\CommentTok{\#\textgreater{}   3  7 11  4}
\CommentTok{\#\textgreater{}   4  6  4 10}
\end{Highlighting}
\end{Shaded}

\hypertarget{freemans-theta}{%
\subparagraph{Freeman's Theta}\label{freemans-theta}}

\begin{itemize}
\tightlist
\item
  Ordinal and nominal
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# this package is not available for R \textgreater{}= 4.0.0}
\NormalTok{rcompanion}\SpecialCharTok{::}\FunctionTok{freemanTheta}\NormalTok{(dt, }\AttributeTok{group =} \StringTok{"column"}\NormalTok{) }
\CommentTok{\# because column is the grouping variable (i.e., nominal)}
\end{Highlighting}
\end{Shaded}

\hypertarget{epsilon-squared}{%
\subparagraph{Epsilon-squared}\label{epsilon-squared}}

\begin{itemize}
\tightlist
\item
  Ordinal and nominal
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# this package is not available for R \textgreater{}= 4.0.0}
\NormalTok{rcompanion}\SpecialCharTok{::}\FunctionTok{epsilonSquared}\NormalTok{(dt,}\AttributeTok{group =} \StringTok{"column"}\NormalTok{ ) }
\CommentTok{\# because column is the grouping variable (i.e., nominal)}
\end{Highlighting}
\end{Shaded}

\hypertarget{two-ordinal}{%
\paragraph{Two Ordinal}\label{two-ordinal}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{=} \DecValTok{100} \CommentTok{\# (sample size)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{dt }\OtherTok{=} \FunctionTok{table}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(}
    \AttributeTok{A =} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{size =}\NormalTok{ n), }\CommentTok{\# ordinal}
    \AttributeTok{B =} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{size =}\NormalTok{ n)  }\CommentTok{\# ordinal}
\NormalTok{)) }
\NormalTok{dt}
\CommentTok{\#\textgreater{}    B}
\CommentTok{\#\textgreater{} A    1  2  3}
\CommentTok{\#\textgreater{}   1  7 11  9}
\CommentTok{\#\textgreater{}   2 11  6 14}
\CommentTok{\#\textgreater{}   3  7 11  4}
\CommentTok{\#\textgreater{}   4  6  4 10}
\end{Highlighting}
\end{Shaded}

\hypertarget{goodman-kruskals-gamma}{%
\subparagraph{Goodman Kruskal's Gamma}\label{goodman-kruskals-gamma}}

\begin{itemize}
\tightlist
\item
  2 ordinal variables
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{DescTools}\SpecialCharTok{::}\FunctionTok{GoodmanKruskalGamma}\NormalTok{(dt, }\AttributeTok{conf.level =} \FloatTok{0.95}\NormalTok{)}
\CommentTok{\#\textgreater{}        gamma       lwr.ci       upr.ci }
\CommentTok{\#\textgreater{}  0.006781013 {-}0.229032069  0.242594095}
\end{Highlighting}
\end{Shaded}

\hypertarget{somers-d}{%
\subparagraph{Somers' D}\label{somers-d}}

\begin{itemize}
\item
  or Somers' Delta
\item
  2 ordinal variables
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{DescTools}\SpecialCharTok{::}\FunctionTok{SomersDelta}\NormalTok{(dt, }\AttributeTok{conf.level =} \FloatTok{0.95}\NormalTok{)}
\CommentTok{\#\textgreater{}       somers       lwr.ci       upr.ci }
\CommentTok{\#\textgreater{}  0.005115859 {-}0.172800185  0.183031903}
\end{Highlighting}
\end{Shaded}

\hypertarget{kendalls-tau-b}{%
\subparagraph{Kendall's Tau-b}\label{kendalls-tau-b}}

\begin{itemize}
\tightlist
\item
  2 ordinal variables
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{DescTools}\SpecialCharTok{::}\FunctionTok{KendallTauB}\NormalTok{(dt, }\AttributeTok{conf.level =} \FloatTok{0.95}\NormalTok{)}
\CommentTok{\#\textgreater{}        tau\_b       lwr.ci       upr.ci }
\CommentTok{\#\textgreater{}  0.004839732 {-}0.163472443  0.173151906}
\end{Highlighting}
\end{Shaded}

\hypertarget{yules-q-and-y}{%
\subparagraph{Yule's Q and Y}\label{yules-q-and-y}}

\begin{itemize}
\tightlist
\item
  2 ordinal variables
\end{itemize}

Special version \((2 \times 2)\) of the \protect\hyperlink{goodman-kruskals-gamma}{Goodman Kruskal's Gamma} coefficient.

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
& Variable 1 & \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Variable 2} & a & b \\
& c & d \\
\end{longtable}

\[
\text{Yule's Q} = \frac{ad - bc}{ad + bc}
\]

We typically use Yule's \(Q\) in practice while Yule's Y has the following relationship with \(Q\).

\[
\text{Yule's Y} = \frac{\sqrt{ad} - \sqrt{bc}}{\sqrt{ad} + \sqrt{bc}}
\]

\[
Q = \frac{2Y}{1 + Y^2}
\]

\[
Y = \frac{1 = \sqrt{1-Q^2}}{Q}
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{=} \DecValTok{100} \CommentTok{\# (sample size)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{dt }\OtherTok{=} \FunctionTok{table}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{A =} \FunctionTok{sample}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{size =}\NormalTok{ n),}
                  \AttributeTok{B =} \FunctionTok{sample}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{size =}\NormalTok{ n)))}
\NormalTok{dt}
\CommentTok{\#\textgreater{}    B}
\CommentTok{\#\textgreater{} A    0  1}
\CommentTok{\#\textgreater{}   0 25 24}
\CommentTok{\#\textgreater{}   1 28 23}

\NormalTok{DescTools}\SpecialCharTok{::}\FunctionTok{YuleQ}\NormalTok{(dt)}
\CommentTok{\#\textgreater{} [1] {-}0.07778669}
\end{Highlighting}
\end{Shaded}

\hypertarget{tetrachoric-correlation}{%
\subparagraph{Tetrachoric Correlation}\label{tetrachoric-correlation}}

\begin{itemize}
\tightlist
\item
  is a special case of \protect\hyperlink{polychoric-correlation}{Polychoric Correlation} when both variables are binary
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(psych)}

\NormalTok{n }\OtherTok{=} \DecValTok{100} \CommentTok{\# (sample size)}

\NormalTok{data }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{A =} \FunctionTok{sample}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{size =}\NormalTok{ n),}
                  \AttributeTok{B =} \FunctionTok{sample}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{size =}\NormalTok{ n))}

\CommentTok{\#view table}
\FunctionTok{head}\NormalTok{(data)}
\CommentTok{\#\textgreater{}   A B}
\CommentTok{\#\textgreater{} 1 1 0}
\CommentTok{\#\textgreater{} 2 1 0}
\CommentTok{\#\textgreater{} 3 0 0}
\CommentTok{\#\textgreater{} 4 1 0}
\CommentTok{\#\textgreater{} 5 1 0}
\CommentTok{\#\textgreater{} 6 1 0}

\FunctionTok{table}\NormalTok{(data)}
\CommentTok{\#\textgreater{}    B}
\CommentTok{\#\textgreater{} A    0  1}
\CommentTok{\#\textgreater{}   0 21 23}
\CommentTok{\#\textgreater{}   1 34 22}


\CommentTok{\#calculate tetrachoric correlation}
\FunctionTok{tetrachoric}\NormalTok{(data)}
\CommentTok{\#\textgreater{} Call: tetrachoric(x = data)}
\CommentTok{\#\textgreater{} tetrachoric correlation }
\CommentTok{\#\textgreater{}   A    B   }
\CommentTok{\#\textgreater{} A  1.0     }
\CommentTok{\#\textgreater{} B {-}0.2  1.0}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  with tau of }
\CommentTok{\#\textgreater{}     A     B }
\CommentTok{\#\textgreater{} {-}0.15  0.13}
\end{Highlighting}
\end{Shaded}

\hypertarget{polychoric-correlation}{%
\subparagraph{Polychoric Correlation}\label{polychoric-correlation}}

\begin{itemize}
\tightlist
\item
  between ordinal categorical variables (natural order).
\item
  Assumption: Ordinal variable is a discrete representation of a latent normally distributed continuous variable. (Income = low, normal, high).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(polycor)}

\NormalTok{n }\OtherTok{=} \DecValTok{100} \CommentTok{\# (sample size)}

\NormalTok{data }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{A =} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{size =}\NormalTok{ n),}
                  \AttributeTok{B =} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{size =}\NormalTok{ n))}

\FunctionTok{head}\NormalTok{(data)}
\CommentTok{\#\textgreater{}   A B}
\CommentTok{\#\textgreater{} 1 1 3}
\CommentTok{\#\textgreater{} 2 1 1}
\CommentTok{\#\textgreater{} 3 3 5}
\CommentTok{\#\textgreater{} 4 2 3}
\CommentTok{\#\textgreater{} 5 3 5}
\CommentTok{\#\textgreater{} 6 4 4}


\CommentTok{\#calculate polychoric correlation between ratings}
\FunctionTok{polychor}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{A, data}\SpecialCharTok{$}\NormalTok{B)}
\CommentTok{\#\textgreater{} [1] 0.01607982}
\end{Highlighting}
\end{Shaded}

\hypertarget{summary-1}{%
\section{Summary}\label{summary-1}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\FunctionTok{data}\NormalTok{(}\StringTok{"mtcars"}\NormalTok{)}
\NormalTok{df }\OtherTok{=}\NormalTok{ mtcars }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(cyl, vs, carb)}


\NormalTok{df\_factor }\OtherTok{=}\NormalTok{ df }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}
        \AttributeTok{cyl =} \FunctionTok{factor}\NormalTok{(cyl),}
        \AttributeTok{vs =} \FunctionTok{factor}\NormalTok{(vs),}
        \AttributeTok{carb =} \FunctionTok{factor}\NormalTok{(carb)}
\NormalTok{    )}
\CommentTok{\# summary(df)}
\FunctionTok{str}\NormalTok{(df)}
\CommentTok{\#\textgreater{} \textquotesingle{}data.frame\textquotesingle{}:    32 obs. of  3 variables:}
\CommentTok{\#\textgreater{}  $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...}
\CommentTok{\#\textgreater{}  $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...}
\CommentTok{\#\textgreater{}  $ carb: num  4 4 1 1 2 1 4 2 2 4 ...}
\FunctionTok{str}\NormalTok{(df\_factor)}
\CommentTok{\#\textgreater{} \textquotesingle{}data.frame\textquotesingle{}:    32 obs. of  3 variables:}
\CommentTok{\#\textgreater{}  $ cyl : Factor w/ 3 levels "4","6","8": 2 2 1 2 3 2 3 1 1 2 ...}
\CommentTok{\#\textgreater{}  $ vs  : Factor w/ 2 levels "0","1": 1 1 2 2 1 2 1 2 2 2 ...}
\CommentTok{\#\textgreater{}  $ carb: Factor w/ 6 levels "1","2","3","4",..: 4 4 1 1 2 1 4 2 2 4 ...}
\end{Highlighting}
\end{Shaded}

Get the correlation table for continuous variables only

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(df)}
\CommentTok{\#\textgreater{}             cyl         vs       carb}
\CommentTok{\#\textgreater{} cyl   1.0000000 {-}0.8108118  0.5269883}
\CommentTok{\#\textgreater{} vs   {-}0.8108118  1.0000000 {-}0.5696071}
\CommentTok{\#\textgreater{} carb  0.5269883 {-}0.5696071  1.0000000}

\CommentTok{\# only complete obs}
\CommentTok{\# cor(df, use = "complete.obs")}
\end{Highlighting}
\end{Shaded}

Alternatively, you can also have the

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Hmisc}\SpecialCharTok{::}\FunctionTok{rcorr}\NormalTok{(}\FunctionTok{as.matrix}\NormalTok{(df), }\AttributeTok{type =} \StringTok{"pearson"}\NormalTok{)}
\CommentTok{\#\textgreater{}        cyl    vs  carb}
\CommentTok{\#\textgreater{} cyl   1.00 {-}0.81  0.53}
\CommentTok{\#\textgreater{} vs   {-}0.81  1.00 {-}0.57}
\CommentTok{\#\textgreater{} carb  0.53 {-}0.57  1.00}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} n= 32 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} P}
\CommentTok{\#\textgreater{}      cyl    vs     carb  }
\CommentTok{\#\textgreater{} cyl         0.0000 0.0019}
\CommentTok{\#\textgreater{} vs   0.0000        0.0007}
\CommentTok{\#\textgreater{} carb 0.0019 0.0007}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{modelsummary}\SpecialCharTok{::}\FunctionTok{datasummary\_correlation}\NormalTok{(df)}
\end{Highlighting}
\end{Shaded}

\begin{table}
\centering
\begin{tabular}[t]{lrrr}
\toprule
  & cyl & vs & carb\\
\midrule
cyl & 1 & . & .\\
vs & \num{-.81} & 1 & .\\
carb & \num{.53} & \num{-.57} & 1\\
\bottomrule
\end{tabular}
\end{table}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ggcorrplot}\SpecialCharTok{::}\FunctionTok{ggcorrplot}\NormalTok{(}\FunctionTok{cor}\NormalTok{(df))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{03-descriptive-stat_files/figure-latex/unnamed-chunk-35-1} \end{center}

Different comparison between different correlation between different types of variables (i.e., continuous vs.~categorical) can be problematic. Moreover, the problem of detecting non-linear vs.~linear relationship/correlation is another one. Hence, a solution is that using mutual information from information theory (i.e., knowing one variable can reduce uncertainty about the other).

To implement mutual information, we have the following approximations

\[
\downarrow \text{prediction error} \approx \downarrow \text{uncertainty} \approx \downarrow \text{association strength}
\]

More specifically, following the \href{https://rviews.rstudio.com/2021/04/15/an-alternative-to-the-correlation-coefficient-that-works-for-numeric-and-categorical-variables/}{X2Y metric}, we have the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Predict \(y\) without \(x\) (i.e., baseline model)

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    Average of \(y\) when \(y\) is continuous
  \item
    Most frequent value when \(y\) is categorical
  \end{enumerate}
\item
  Predict \(y\) with \(x\) (e.g., linear, random forest, etc.)
\item
  Calculate the prediction error difference between 1 and 2
\end{enumerate}

To have a comprehensive table that could handle

\begin{itemize}
\item
  continuous vs.~continuous
\item
  categorical vs.~continuous
\item
  continuous vs.~categorical
\item
  categorical vs.~categorical
\end{itemize}

the suggested model would be Classification and Regression Trees (CART). But we can certainly use other models as well.

The downfall of this method is that you might suffer

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Symmetry: \((x,y) \neq (y,x)\)
\item
  Comparability : Different pair of comparison might use different metrics (e.g., misclassification error vs.~MAE)
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ppsr)}

\NormalTok{iris }\OtherTok{\textless{}{-}}\NormalTok{ iris }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{)}

\CommentTok{\# ppsr::score\_df(iris) \# if you want a dataframe}
\NormalTok{ppsr}\SpecialCharTok{::}\FunctionTok{score\_matrix}\NormalTok{(iris,}
                   \AttributeTok{do\_parallel =} \ConstantTok{TRUE}\NormalTok{,}
                   \AttributeTok{n\_cores =}\NormalTok{ parallel}\SpecialCharTok{::}\FunctionTok{detectCores}\NormalTok{() }\SpecialCharTok{/} \DecValTok{2}\NormalTok{)}
\CommentTok{\#\textgreater{}              Sepal.Length Sepal.Width Petal.Length}
\CommentTok{\#\textgreater{} Sepal.Length   1.00000000  0.04632352    0.5491398}
\CommentTok{\#\textgreater{} Sepal.Width    0.06790301  1.00000000    0.2376991}
\CommentTok{\#\textgreater{} Petal.Length   0.61608360  0.24263851    1.0000000}

\CommentTok{\# if you want a similar correlation matrix}
\NormalTok{ppsr}\SpecialCharTok{::}\FunctionTok{score\_matrix}\NormalTok{(df,}
                   \AttributeTok{do\_parallel =} \ConstantTok{TRUE}\NormalTok{,}
                   \AttributeTok{n\_cores =}\NormalTok{ parallel}\SpecialCharTok{::}\FunctionTok{detectCores}\NormalTok{() }\SpecialCharTok{/} \DecValTok{2}\NormalTok{)}
\CommentTok{\#\textgreater{}             cyl        vs      carb}
\CommentTok{\#\textgreater{} cyl  1.00000000 0.3982789 0.2092533}
\CommentTok{\#\textgreater{} vs   0.02514286 1.0000000 0.2000000}
\CommentTok{\#\textgreater{} carb 0.30798148 0.2537309 1.0000000}
\end{Highlighting}
\end{Shaded}

\hypertarget{visualization}{%
\subsection{Visualization}\label{visualization}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{corrplot}\SpecialCharTok{::}\FunctionTok{corrplot}\NormalTok{(}\FunctionTok{cor}\NormalTok{(df))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{03-descriptive-stat_files/figure-latex/unnamed-chunk-37-1} \end{center}

Alternatively,

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PerformanceAnalytics}\SpecialCharTok{::}\FunctionTok{chart.Correlation}\NormalTok{(df, }\AttributeTok{histogram =}\NormalTok{ T, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{03-descriptive-stat_files/figure-latex/unnamed-chunk-38-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{heatmap}\NormalTok{(}\FunctionTok{as.matrix}\NormalTok{(df))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{03-descriptive-stat_files/figure-latex/unnamed-chunk-39-1} \end{center}

More general form,

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ppsr}\SpecialCharTok{::}\FunctionTok{visualize\_pps}\NormalTok{(}
    \AttributeTok{df =}\NormalTok{ iris,}
    \AttributeTok{do\_parallel =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{n\_cores =}\NormalTok{ parallel}\SpecialCharTok{::}\FunctionTok{detectCores}\NormalTok{() }\SpecialCharTok{/} \DecValTok{2}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{03-descriptive-stat_files/figure-latex/unnamed-chunk-40-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ppsr}\SpecialCharTok{::}\FunctionTok{visualize\_correlations}\NormalTok{(}
    \AttributeTok{df =}\NormalTok{ iris}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{03-descriptive-stat_files/figure-latex/unnamed-chunk-41-1} \end{center}

Both heat map and correlation at the same time

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ppsr}\SpecialCharTok{::}\FunctionTok{visualize\_both}\NormalTok{(}
    \AttributeTok{df =}\NormalTok{ iris,}
    \AttributeTok{do\_parallel =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{n\_cores =}\NormalTok{ parallel}\SpecialCharTok{::}\FunctionTok{detectCores}\NormalTok{() }\SpecialCharTok{/} \DecValTok{2}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{03-descriptive-stat_files/figure-latex/unnamed-chunk-42-1} \end{center}

More elaboration with \texttt{ggplot2}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ppsr}\SpecialCharTok{::}\FunctionTok{visualize\_pps}\NormalTok{(}
    \AttributeTok{df =}\NormalTok{ iris,}
    \AttributeTok{color\_value\_high =} \StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{,}
    \AttributeTok{color\_value\_low =} \StringTok{\textquotesingle{}yellow\textquotesingle{}}\NormalTok{,}
    \AttributeTok{color\_text =} \StringTok{\textquotesingle{}black\textquotesingle{}}
\NormalTok{) }\SpecialCharTok{+}
\NormalTok{    ggplot2}\SpecialCharTok{::}\FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
\NormalTok{    ggplot2}\SpecialCharTok{::}\FunctionTok{theme}\NormalTok{(}\AttributeTok{plot.background =} 
\NormalTok{                       ggplot2}\SpecialCharTok{::}\FunctionTok{element\_rect}\NormalTok{(}\AttributeTok{fill =} \StringTok{"lightgrey"}\NormalTok{)) }\SpecialCharTok{+}
\NormalTok{    ggplot2}\SpecialCharTok{::}\FunctionTok{theme}\NormalTok{(}\AttributeTok{title =}\NormalTok{ ggplot2}\SpecialCharTok{::}\FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{15}\NormalTok{)) }\SpecialCharTok{+}
\NormalTok{    ggplot2}\SpecialCharTok{::}\FunctionTok{labs}\NormalTok{(}
        \AttributeTok{title =} \StringTok{\textquotesingle{}Correlation aand Heatmap\textquotesingle{}}\NormalTok{,}
        \AttributeTok{subtitle =} \StringTok{\textquotesingle{}Subtitle\textquotesingle{}}\NormalTok{,}
        \AttributeTok{caption =} \StringTok{\textquotesingle{}Caption\textquotesingle{}}\NormalTok{,}
        \AttributeTok{x =} \StringTok{\textquotesingle{}More info\textquotesingle{}}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{03-descriptive-stat_files/figure-latex/unnamed-chunk-43-1} \end{center}

\hypertarget{basic-statistical-inference}{%
\chapter{Basic Statistical Inference}\label{basic-statistical-inference}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \protect\hyperlink{one-sample-inference}{One Sample Inference}
\item
  \protect\hyperlink{two-sample-inference}{Two Sample Inference}
\item
  \protect\hyperlink{categorical-data-analysis}{Categorical Data Analysis}
\item
  \protect\hyperlink{divergence-metrics-and-test-for-comparing-distributions}{Divergence Metrics and Test for Comparing Distributions}
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Make \textbf{inferences} (an interpretation) about the true parameter value \(\beta\) based on our estimator/estimate
\item
  Test whether our underlying assumptions (about the true population parameters, random variables, or model specification) hold true.
\end{itemize}

Testing does not

\begin{itemize}
\tightlist
\item
  Confirm with 100\% a hypothesis is true
\item
  Confirm with 100\% a hypothesis is false
\item
  Tell you how to interpret the estimate value (Economic vs.~Practical vs.~Statistical Significance)
\end{itemize}

Hypothesis: Translate an objective in better understanding the results in terms of specifying a value (or sets of values) in which our population parameters should/should not lie.

\begin{itemize}
\tightlist
\item
  \textbf{Null hypothesis} (\(H_0\)): A statement about the population parameter that we take to be true in which we would need the data to provide substantial evidence that against it.

  \begin{itemize}
  \tightlist
  \item
    Can be either a single value (ex: \(H_0: \beta=0\)) or a set of values (ex: \(H_0: \beta_1 \ge 0\))
  \item
    Will generally be the value you would not like the population parameter to be (subjective)

    \begin{itemize}
    \tightlist
    \item
      \(H_0: \beta_1=0\) means you would like to see a non-zero coefficient
    \item
      \(H_0: \beta_1 \ge 0\) means you would like to see a negative effect
    \end{itemize}
  \item
    ``Test of Significance'' refers to the two-sided test: \(H_0: \beta_j=0\)
  \end{itemize}
\item
  \textbf{Alternative hypothesis} (\(H_a\) or \(H_1\)) (Research Hypothesis): All other possible values that the population parameter may be if the null hypothesis does not hold.
\end{itemize}

\textbf{Type I Error}

Error made when \(H_0\) is rejected when, in fact, \(H_0\) is true.\\
The probability of committing a Type I error is \(\alpha\) (known as \textbf{level of significance} of the test)

Type I error (\(\alpha\)): probability of rejecting \(H_0\) when it is true.

Legal analogy: In U.S. law, a defendant is presumed to be ``innocent until proven guilty''.\\
If the null hypothesis is that a person is innocent, the Type I error is the probability that you conclude the person is guilty when he is innocent.

\textbf{Type II Error}

Type II error level (\(\beta\)): probability that you fail to reject the null hypothesis when it is false.

In the legal analogy, this is the probability that you fail to find the person guilty when he or she is guilty.

Error made when \(H_0\) is not rejected when, in fact, \(H_1\) is true\\
The probability of committing a Type II error is \(\beta\) (known as the \textbf{power} of the test)

Random sample of size n: A collection of n independent random variables taken from the distribution X, each with the same distribution as X.

\textbf{Sample mean}

\[
\bar{X}= (\sum_{i=1}^{n}X_i)/n
\]

\textbf{Sample Median}

\(\tilde{x}\) = the middle observation in a sample of observation order from smallest to largest (or vice versa).

If n is odd, \(\tilde{x}\) is the middle observation,\\
If n is even, \(\tilde{x}\) is the average of the two middle observations.

\textbf{Sample variance} \[
S^2 = \frac{\sum_{i=1}^{n}(X_i = \bar{X})^2}{n-1}= \frac{n\sum_{i=1}^{n}X_i^2 -(\sum_{i=1}^{n}X_i)^2}{n(n-1)}
\]

\textbf{Sample standard deviation} \[
S = \sqrt{S^2}
\]

\textbf{Sample proportions} \[
\hat{p} = \frac{X}{n} = \frac{\text{number in the sample with trait}}{\text{sample size}}
\]

\[
\widehat{p_1-p_2} = \hat{p_1}-\hat{p_2} = \frac{X_1}{n_1} - \frac{X_2}{n_2} = \frac{n_2X_1 = n_1X_2}{n_1n_2}
\]

\textbf{Estimators}\\
\textbf{Point Estimator}\\
\(\hat{\theta}\) is a statistic used to approximate a population parameter \(\theta\)

\textbf{Point estimate}\\
The numerical value assumed by \(\hat{\theta}\) when evaluated for a given sample

\textbf{Unbiased estimator}\\
If \(E(\hat{\theta}) = \theta\), then \(\hat{\theta}\) is an unbiased estimator for \(\theta\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(\bar{X}\) is an unbiased estimator for \(\mu\)
\item
  \(S^2\) is an unbiased estimator for \(\sigma^2\)
\item
  \(\hat{p}\) is an unbiased estimator for p
\item
  \(\widehat{p_1-p_2}\) is an unbiased estimator for \(p_1- p_2\)
\item
  \(\bar{X_1} - \bar{X_2}\) is an unbiased estimator for \(\mu_1 - \mu_2\)
\end{enumerate}

\textbf{Note}: \(S\) is a biased estimator for \(\sigma\)

\textbf{Distribution of the sample mean}

If \(\bar{X}\) is the sample mean based on a random sample of size \(n\) drawn from a normal distribution \(X\) with mean \(\mu\) and standard deviation \(\sigma\), the \(\bar{X}\) is normally distributed, with mean \(\mu_{\bar{X}} = \mu\) and variance \(\sigma_{\bar{X}}^2 = Var(\bar{X}) = \frac{\sigma^2}{n}\). Then the \textbf{standard error of the mean} is: \(\sigma_{\bar{X}}= \frac{\sigma}{\sqrt{n}}\)

\hypertarget{one-sample-inference}{%
\section{One Sample Inference}\label{one-sample-inference}}

\(Y_i \sim i.i.d. N(\mu, \sigma^2)\)

i.i.d. standards for ``independent and identically distributed''

Hence, we have the following model:

\(Y_i=\mu +\epsilon_i\) where

\begin{itemize}
\tightlist
\item
  \(\epsilon_i \sim^{iid} N(0,\sigma^2)\)\\
\item
  \(E(Y_i)=\mu\)\\
\item
  \(Var(Y_i)=\sigma^2\)\\
\item
  \(\bar{y} \sim N(\mu,\sigma^2/n)\)
\end{itemize}

\hypertarget{the-mean}{%
\subsection{The Mean}\label{the-mean}}

When \(\sigma^2\) is estimated by \(s^2\), then

\[
\frac{\bar{y}-\mu}{s/\sqrt{n}} \sim t_{n-1}
\]

Then, a \(100(1-\alpha) \%\) confidence interval for \(\mu\) is obtained from:

\[
1 - \alpha = P(-t_{\alpha/2;n-1} \le \frac{\bar{y}-\mu}{s/\sqrt{n}} \le t_{\alpha/2;n-1}) \\
= P(\bar{y} - (t_{\alpha/2;n-1})s/\sqrt{n} \le \mu \le \bar{y} + (t_{\alpha/2;n-1})s/\sqrt{n})
\]

And the interval is

\[
\bar{y} \pm (t_{\alpha/2;n-1})s/\sqrt{n}
\]

and \(s/\sqrt{n}\) is the standard error of \(\bar{y}\)

If the experiment were repeated many times, \(100(1-\alpha) \%\) of these intervals would contain \(\mu\)

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2703}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2432}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2432}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2432}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Confidence Interval \(100(1-\alpha)%
\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Sample Sizes Confidence \(\alpha\), Error \(d\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Hypothesis Testing Test Statistic
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
When \(\sigma^2\) is known, X is normal (or \(n \ge 25\)) & \(\bar{X} \pm z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\) & \(n \approx \frac{z_{\alpha/2}^2 \sigma^2}{d^2}\) & \(z = \frac{\bar{X}-\mu_0}{\sigma/\sqrt{n}}\) \\
When \(\sigma^2\) is unknown, X is normal (or \(n \ge 25\)) & \(\bar{X} \pm t_{\alpha/2}\frac{s}{\sqrt{n}}\) & \(n \approx \frac{z_{\alpha/2}^2 s^2}{d^2}\) & \(t = \frac{\bar{X}-\mu_0}{s/\sqrt{n}}\) \\
\end{longtable}

\hypertarget{for-difference-of-means-mu_1-mu_2-independent-samples}{%
\subsubsection{\texorpdfstring{For Difference of Means (\(\mu_1-\mu_2\)), Independent Samples}{For Difference of Means (\textbackslash mu\_1-\textbackslash mu\_2), Independent Samples}}\label{for-difference-of-means-mu_1-mu_2-independent-samples}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2192}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2192}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2329}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3288}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(100(1-\alpha)%
\) Confidence Interval
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Hypothesis Testing Test Statistic
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
When \(\sigma^2\) is known & \(\bar{X}_1 - \bar{X}_2 \pm z_{\alpha/2}\sqrt{\frac{\sigma^2_1}{n_1}+\frac{\sigma^2_2}{n_2}}\) & \(z= \frac{(\bar{X}_1-\bar{X}_2)-(\mu_1-\mu_2)_0}{\sqrt{\frac{\sigma^2_1}{n_1}+\frac{\sigma^2_2}{n_2}}}\) & \\
When \(\sigma^2\) is unknown, Variances Assumed EQUAL & \(\bar{X}_1 - \bar{X}_2 \pm t_{\alpha/2}\sqrt{s^2_p(\frac{1}{n_1}+\frac{1}{n_2})}\) & \(t = \frac{(\bar{X}_1-\bar{X}_2)-(\mu_1-\mu_2)_0}{\sqrt{s^2_p(\frac{1}{n_1}+\frac{1}{n_2})}}\) & Pooled Variance: \(s_p^2 = \frac{(n_1 -1)s^2_1 - (n_2-1)s^2_2}{n_1 + n_2 -2}\) Degrees of Freedom: \(\gamma = n_1 + n_2 -2\) \\
When \(\sigma^2\) is unknown, Variances Assumed UNEQUAL & \(\bar{X}_1 - \bar{X}_2 \pm t_{\alpha/2}\sqrt{(\frac{s^2_1}{n_1}+\frac{s^2_2}{n_2})}\) & \(t = \frac{(\bar{X}_1-\bar{X}_2)-(\mu_1-\mu_2)_0}{\sqrt{(\frac{s^2_1}{n_1}+\frac{s^2_2}{n_2})}}\) & Degrees of Freedom: \(\gamma = \frac{(\frac{s_1^2}{n_1}+\frac{s^2_2}{n_2})^2}{\frac{(\frac{s_1^2}{n_1})^2}{n_1-1}+\frac{(\frac{s_2^2}{n_2})^2}{n_2-1}}\) \\
\end{longtable}

\hypertarget{for-difference-of-means-mu_1---mu_2-paired-samples-d-x-y}{%
\subsubsection{\texorpdfstring{For Difference of Means (\(\mu_1 - \mu_2\)), Paired Samples (D = X-Y)}{For Difference of Means (\textbackslash mu\_1 - \textbackslash mu\_2), Paired Samples (D = X-Y)}}\label{for-difference-of-means-mu_1---mu_2-paired-samples-d-x-y}}

\(100(1-\alpha)%
\) Confidence Interval\\
\[
\bar{D} \pm t_{\alpha/2}\frac{s_d}{\sqrt{n}}
\]

\textbf{Hypothesis Testing Test Statistic}

\[
t = \frac{\bar{D}-D_0}{s_d / \sqrt{n}}
\]

\hypertarget{difference-of-two-proportions}{%
\subsubsection{Difference of Two Proportions}\label{difference-of-two-proportions}}

\textbf{Mean}

\[
\hat{p_1}-\hat{p_2}
\]

\textbf{Variance} \[
\frac{p_1(1-p_1)}{n_1} + \frac{p_2(1-p_2)}{n_2}
\]

\(100(1-\alpha)%
\) Confidence Interval

\[
\hat{p_1}-\hat{p_2} + z_{\alpha/2}\sqrt{\frac{p_1(1-p_1)}{n_1} + \frac{p_2(1-p_2)}{n_2}}
\]

\textbf{Sample Sizes, Confidence} \(\alpha\), Error d\\
(Prior Estimate fo \(\hat{p_1},\hat{p_2}\))

\[
n \approx \frac{z_{\alpha/2}^2[p_1(1-p_1)+p_2(1-p_2)]}{d^2}
\]

(No Prior Estimates for \(\hat{p}\))

\[
n \approx \frac{z_{\alpha/2}^2}{2d^2}
\]

\textbf{Hypothesis Testing - Test Statistics}

Null Value \((p_1 - p_2) \neq 0\)

\[
z = \frac{(\hat{p_1} - \hat{p_2})-(p_1 - p_2)_0}{\sqrt{\frac{p_1(1-p_1)}{n_1} + \frac{p_2(1-p_2)}{n_2}}}
\]

Null Value \((p_1 - p_2)_0 = 0\)

\[
z = \frac{\hat{p_1} - \hat{p_2}}{\sqrt{\hat{p}(1-\hat{p})(\frac{1}{n_1}+\frac{1}{n_2})}}
\]

where

\[
\hat{p}= \frac{x_1 + x_2}{n_1 + n_2} = \frac{n_1 \hat{p_1} + n_2 \hat{p_2}}{n_1 + n_2}
\]

\hypertarget{single-variance}{%
\subsection{Single Variance}\label{single-variance}}

\[
1 - \alpha = P( \chi_{1-\alpha/2;n-1}^2) \le (n-1)s^2/\sigma^2 \le \chi_{\alpha/2;n-1}^2) \\
= P(\frac{(n-1)s^2}{\chi_{\alpha/2}^2} \le \sigma^2 \le \frac{(n-1)s^2}{\chi_{1-\alpha/2}^2})
\]

and a \(100(1-\alpha) \%\) confidence interval for \(\sigma^2\) is:

\[
(\frac{(n-1)s^2}{\chi_{\alpha/2;n-1}^2},\frac{(n-1)s^2}{\chi_{1-\alpha/2;n-1}^2})
\] Confidence limits for \(\sigma^2\) are obtained by computing the positive square roots of these limits

Equivalently,

\(100(1-\alpha)%
\) Confidence Interval

\[
L_1 = \frac{(n-1)s^2}{\chi^2_{\alpha/2}} \\
L_1 = \frac{(n-1)s^2}{\chi^2_{1-\alpha/2}}
\] \textbf{Hypothesis Testing Test Statistic}

\[
\chi^2 = \frac{(n-1)s^2}{\sigma^2_0}
\]

\hypertarget{single-proportion-p}{%
\subsection{Single Proportion (p)}\label{single-proportion-p}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2466}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2877}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2329}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2329}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Confidence Interval \(100(1-\alpha)%
\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Sample Sizes Confidence \(\alpha\), Error d (prior estimate for \(\hat{p}\))
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
(No prior estimate for \(\hat{p}\))
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Hypothesis Testing Test Statistic
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(\hat{p} \pm z_{\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}\) & \(n \approx \frac{z_{\alpha/2}^2 \hat{p}(1-\hat{p})}{d^2}\) & \(n \approx \frac{z_{\alpha/2}^2}{4d^2}\) & \(z = \frac{\hat{p}-p_0}{\sqrt{\frac{p_0(1-p_0)}{n}}}\) \\
\end{longtable}

\hypertarget{power}{%
\subsection{Power}\label{power}}

Formally, power (for the test of the mean) is given by:

\[
\pi(\mu) = 1 - \beta = P(\text{test rejects } H_0|\mu)
\] To evaluate the power, one needs to know the distribution of the test statistic if the null hypothesis is false.

For 1-sided z-test where \(H_0: \mu \le \mu_0 \\ H_A: \mu >0\)

The power is:

\[
\begin{aligned}
\pi(\mu) &= P(\bar{y} > \mu_0 + z_{\alpha} \sigma/\sqrt{n}|\mu) \\
&= P(Z = \frac{\bar{y} - \mu}{\sigma / \sqrt{n}} > z_{\alpha} + \frac{\mu_0 - \mu}{\sigma/ \sqrt{n}}|\mu) \\
&= 1 - \Phi(z_{\alpha} + \frac{(\mu_0 - \mu)\sqrt{n}}{\sigma}) \\
&= \Phi(-z_{\alpha}+\frac{(\mu -\mu_0)\sqrt{n}}{\sigma})
\end{aligned}
\]

where \(1-\Phi(x) = \Phi(-x)\) since the normal pdf is symmetric

Power is correlated to the difference in \(\mu - \mu_0\), sample size n, variance \(\sigma^2\), and the \(\alpha\)-level of the test (through \(z_{\alpha}\))\\
Equivalently, power can be increased by making \(\alpha\) large, \(\sigma^2\) smaller, or n larger.

For 2-sided z-test is:

\[
\pi(\mu) = \Phi(-z_{\alpha/2} + \frac{(\mu_0 - \mu)\sqrt{n}}{\sigma}) + \Phi(-z_{\alpha/2}+\frac{(\mu - \mu_0)\sqrt{n}}{\sigma})
\]

\hypertarget{sample-size}{%
\subsection{Sample Size}\label{sample-size}}

\hypertarget{sided-z-test}{%
\subsubsection{1-sided Z-test}\label{sided-z-test}}

Example: to show that the mean response \(\mu\) under the treatment is higher than the mean response \(\mu_0\) without treatment (show that the treatment effect \(\delta = \mu -\mu_0\) is large)

Because power is an increasing function of \(\mu - \mu_0\), it is only necessary to find n that makes the power equal to \(1- \beta\) at \(\mu = \mu_0 + \delta\)

Hence, we have

\[
\pi(\mu_0 + \delta) = \Phi(-z_{\alpha} + \frac{\delta \sqrt{n}}{\sigma}) = 1 - \beta
\]

Since \(\Phi (z_{\beta})= 1-\beta\), we have

\[
-z_{\alpha} + \frac{\delta \sqrt{n}}{\sigma} = z_{\beta}
\]

Then n is

\[
n = (\frac{(z_{\alpha}+z_{\beta})\sigma}{\delta})^2
\]

Then, we need larger samples, when

\begin{itemize}
\tightlist
\item
  the sample variability is large (\(\sigma\) is large)
\item
  \(\alpha\) is small (\(z_{\alpha}\) is large)
\item
  power \(1-\beta\) is large (\(z_{\beta}\) is large)
\item
  The magnitude of the effect is smaller (\(\delta\) is small)
\end{itemize}

Since we don't know \(\delta\) and \(\sigma\). We can base \(\sigma\) on previous studies, pilot studies. Or, obtain an estimate of \(\sigma\) by anticipating the range of the observation (without outliers). divide this range by 4 and use the resulting number as an approximate estimate of \(\sigma\). For normal (distribution) data, this is reasonable.

\hypertarget{sided-z-test-1}{%
\subsubsection{2-sided Z-test}\label{sided-z-test-1}}

We want to know the min n, required to guarantee \(1-\beta\) power when the treatment effect \(\delta = |\mu - \mu_0|\) is at least greater than 0. Since the power function for the 2-sided is increasing and symmetric in \(|\mu - \mu_0|\), we only need to find n that makes the power equal to \(1-\beta\) when \(\mu = \mu_0 + \delta\)

\[
n = (\frac{(z_{\alpha/2} + z_{\beta}) \sigma}{\delta})^2
\]

We could also use the confidence interval approach. If we require that an \(\alpha\)-level two-sided CI for \(\mu\) be

\[
\bar{y} \pm D
\] where \(D = z_{\alpha/2}\sigma/\sqrt{n}\) gives

\[
n = (\frac{z_{\alpha/2}\sigma}{D})^2
\] (round up to the nearest integer)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OtherTok{=} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\FunctionTok{t.test}\NormalTok{(data, }\AttributeTok{conf.level=}\FloatTok{0.95}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  One Sample t{-}test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  data}
\CommentTok{\#\textgreater{} t = {-}1.7813, df = 99, p{-}value = 0.07793}
\CommentTok{\#\textgreater{} alternative hypothesis: true mean is not equal to 0}
\CommentTok{\#\textgreater{} 95 percent confidence interval:}
\CommentTok{\#\textgreater{}  {-}0.40186463  0.02165643}
\CommentTok{\#\textgreater{} sample estimates:}
\CommentTok{\#\textgreater{}  mean of x }
\CommentTok{\#\textgreater{} {-}0.1901041}
\end{Highlighting}
\end{Shaded}

\[
H_0: \mu \ge 30 \\
H_a: \mu < 30
\]

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(data, }\AttributeTok{mu=}\DecValTok{30}\NormalTok{,}\AttributeTok{alternative=}\StringTok{"less"}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  One Sample t{-}test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  data}
\CommentTok{\#\textgreater{} t = {-}282.88, df = 99, p{-}value \textless{} 2.2e{-}16}
\CommentTok{\#\textgreater{} alternative hypothesis: true mean is less than 30}
\CommentTok{\#\textgreater{} 95 percent confidence interval:}
\CommentTok{\#\textgreater{}         {-}Inf {-}0.01290305}
\CommentTok{\#\textgreater{} sample estimates:}
\CommentTok{\#\textgreater{}  mean of x }
\CommentTok{\#\textgreater{} {-}0.1901041}
\end{Highlighting}
\end{Shaded}

\hypertarget{note}{%
\subsection{Note}\label{note}}

For t-tests, the sample and power are not as easy as z-test.

\[
\pi(\mu) = P(\frac{\bar{y}-\mu_0}{s/\sqrt{n}}> t_{n-1;\alpha}|\mu)
\]

when \(\mu > \mu_0\) (i.e., \(\mu - \mu_0 = \delta\)), the random variable \((\bar{y} - \mu_0)/(s/\sqrt{n})\) does not have a \protect\hyperlink{student-t}{Student's t distribution}, but rather is distributed as a non-central t-distribution with non-centrality parameter \(\delta \sqrt{n}/\sigma\) and d.f. of \(n-1\)

\begin{itemize}
\tightlist
\item
  The power is an increasing function of this non-centrality parameter (note, when \(\delta = 0\) the distribution is usual Student's t-distribution).
\item
  To evaluate power, one must consider numerical procedure or use special charts
\end{itemize}

Approximate Sample Size Adjustment for t-test. We use an adjustment to the z-test determination for sample size.

Let \(v = n-1\), where n is sample size derived based on the z-test power. Then the 2-sided t-test sample size (approximate) is given:

\[
n^* = \frac{(t_{v;\alpha/2}+t_{v;\beta})^2 \sigma^2}{\delta^2}
\]

\hypertarget{one-sample-non-parametric-methods}{%
\subsection{One-sample Non-parametric Methods}\label{one-sample-non-parametric-methods}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lecture.data }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FloatTok{0.76}\NormalTok{, }\FloatTok{0.82}\NormalTok{, }\FloatTok{0.80}\NormalTok{, }\FloatTok{0.79}\NormalTok{, }\FloatTok{1.06}\NormalTok{, }\FloatTok{0.83}\NormalTok{,}\SpecialCharTok{{-}}\FloatTok{0.43}\NormalTok{,}\SpecialCharTok{{-}}\FloatTok{0.34}\NormalTok{, }\FloatTok{3.34}\NormalTok{, }\FloatTok{2.33}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{sign-test}{%
\subsubsection{Sign Test}\label{sign-test}}

If we want to test \(H_0: \mu_{(0.5)} = 0; H_a: \mu_{(0.5)} >0\) where \(\mu_{(0.5)}\) is the population median. We can

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  Count the number of observation (\(y_i\)'s) that exceed 0. Denote this number by \(s_+\), called the number of plus signs. Let \(s_- = n - s_+\), which is the number of minus signs.
\item
  Reject \(H_0\) if \(s_+\) is large or equivalently, if \(s_-\) is small.
\end{enumerate}

To determine how large \(s_+\) must be to reject \(H_0\) at a given significance level, we need to know the distribution of the corresponding random variable \(S_+\) under the null hypothesis, which is a \protect\hyperlink{binomial}{binomial} with p = 1/2,w hen the null is true.

To work out the null distribution using the binomial formula, we have \(\alpha\)-level test rejects \(H_0\) if \(s_+ \ge b_{n,\alpha}\), where \(b_{n,\alpha}\) is the upper \(\alpha\) critical point of the \(Bin(n,1/2)\) distribution. Both \(S_+\) and \(S_-\) have this same distribution (\(S = S_+ = S_-\)).

\[
\text{p-value} = P(S \ge s_+) = \sum_{i = s_+}^{n} {{n}\choose{i}} (\frac{1}{2})^n
\] equivalently,

\[
P(S \le s_-) = \sum_{i=0}^{s_-}{{n}\choose{i}} (\frac{1}{2})^2
\] For large sample sizes, we could use the normal approximation for the binomial, in which case reject \(H_0\) if

\[
s_+ \ge n/2 + 1/2 + z_{\alpha}\sqrt{n/4}
\]

For the 2-sided test, we use the tests statistic \(s_{max} = max(s_+,s_-)\) or \(s_{min} = min(s_+, s_-)\). An \(\alpha\)-level test rejects \(H_0\) if the p-value is \(\le \alpha\), where the p-value is computed from:

\[
p-value = 2 \sum_{i=s_{max}}^{n} {{n}\choose{i}} (\frac{1}{2})^n = s \sum_{i=0}^{s_{min}} {{n}\choose{i}} (\frac{1}{2})^n
\] Equivalently, rejecting \(H_0\) if \(s_{max} \ge b_{n,\alpha/2}\)

A large sample normal approximation can be used, where

\[
z = \frac{s_{max}- n/2 -1/2}{\sqrt{n/4}}
\] and reject \(H_0\) at \(\alpha\) if \(z \ge z_{\alpha/2}\)

However, treatment of 0 is problematic for this test.

\begin{itemize}
\tightlist
\item
  Solution 1: randomly assign 0 to the positive or negative (2 researchers might get different results).\\
\item
  Solution 2: count each 0 as a contribution 1/2 toward \(s_+\) and \(s_-\) (but then could not apply the \protect\hyperlink{binomial}{binomial} distribution)\\
\item
  Solution 3: ignore 0 (reduces the power of test due to decreased sample size).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{binom.test}\NormalTok{(}\FunctionTok{sum}\NormalTok{(lecture.data }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{), }\FunctionTok{length}\NormalTok{(lecture.data)) }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Exact binomial test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  sum(lecture.data \textgreater{} 0) and length(lecture.data)}
\CommentTok{\#\textgreater{} number of successes = 8, number of trials = 10, p{-}value = 0.1094}
\CommentTok{\#\textgreater{} alternative hypothesis: true probability of success is not equal to 0.5}
\CommentTok{\#\textgreater{} 95 percent confidence interval:}
\CommentTok{\#\textgreater{}  0.4439045 0.9747893}
\CommentTok{\#\textgreater{} sample estimates:}
\CommentTok{\#\textgreater{} probability of success }
\CommentTok{\#\textgreater{}                    0.8}
\CommentTok{\# alternative = "greater" or alternative = "less"}
\end{Highlighting}
\end{Shaded}

\hypertarget{wilcoxon-signed-rank-test}{%
\subsubsection{Wilcoxon Signed Rank Test}\label{wilcoxon-signed-rank-test}}

Since the \protect\hyperlink{sign-test}{Sign Test} could not consider the magnitude of each observation from 0, the \protect\hyperlink{wilcoxon-signed-rank-test}{Wilcoxon Signed Rank Test} improves by taking account the ordered magnitudes of the observation, but it will impose the requirement of symmetric to this test (while \protect\hyperlink{sign-test}{Sign Test} does not)

\[
H_0: \mu_{0.5} = 0 \\
H_a: \mu_{0.5} > 0
\] (assume no ties or same observations)

The signed rank test procedure:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  rank order the observation \(y_i\) in terms of their absolute values. Let \(r_i\) be the rank of \(y_i\) in this ordering. Since we assume no ties, the ranks \(r_i\) are uniquely determined and are a permutation of the integers \(1,2,â€¦,n\).\\
\item
  Calculate \(w_+\), which is the sum of the ranks of the positive values, and \(w_-\), which is the sum of the ranks of the negative values. Note that \(w_+ + w_- = r_1 + r_2 + ... = 1 + 2 + ... + n = n(n+1)/2\)\\
\item
  Reject \(H_0\) if \(w_+\) is large (or if \(w_-\) is small)
\end{enumerate}

To know what is large or small with regard to \(w_+\) and \(w_-\), we need the distribution of \(W_+\) and \(W_-\) when the null is true.

Since these null distributions are identical and symmetric, the p-value is \(P(W \ge w_+) = P(W \le w_-)\)

An \(\alpha\)-level test rejects the null if the p-value is \(\le \alpha\), or if \(w_+ \ge w_{n,\alpha}\), where \(w_{n,\alpha}\) is the upper \(\alpha\) critical point of the null distribution of W.

This distribution of W has a special table. For large n, the distribution of W is approximately normal.

\[
z = \frac{w_+ - n(n+1) /4 -1/2}{\sqrt{n(n+1)(2n+1)/24}}
\]

The test rejects \(H_0\) at level \(\alpha\) if

\[
w_+ \ge n(n+1)/4 +1/2 + z_{\alpha}\sqrt{n(n+1)(2n+1)/24} \approx w_{n,\alpha}
\]

For the 2-sided test, we use \(w_{max}=max(w_+,w_-)\) or \(w_{min}=min(w_+,w_-)\), with p-value given by:

\[
p-value = 2P(W \ge w_{max}) = 2P(W \le w_{min})
\] Same as \protect\hyperlink{sign-test}{Sign Test},we ignore 0. In some cases where some of the \(|y_i|\)'s may be tied for the same rank, we simply assign each of the tied ranks the average rank (or ``midrank'').

Example, if \(y_1 = -1\), \(y_3 = 3\) and \(y_3 = -3\), and \(y_4 =5\), then \(r_1 = 1\), \(r_2 = r_3=(2+3)/2 = 2.5\), \(r_4 = 4\)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{wilcox.test}\NormalTok{(lecture.data) }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Wilcoxon signed rank exact test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  lecture.data}
\CommentTok{\#\textgreater{} V = 52, p{-}value = 0.009766}
\CommentTok{\#\textgreater{} alternative hypothesis: true location is not equal to 0}
\CommentTok{\# does not use normal approximation}
\CommentTok{\# (using the underlying W distribution)}

\FunctionTok{wilcox.test}\NormalTok{(lecture.data,}\AttributeTok{exact=}\NormalTok{F) }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Wilcoxon signed rank test with continuity correction}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  lecture.data}
\CommentTok{\#\textgreater{} V = 52, p{-}value = 0.01443}
\CommentTok{\#\textgreater{} alternative hypothesis: true location is not equal to 0}
\CommentTok{\# uses normal approximation}
\end{Highlighting}
\end{Shaded}

\hypertarget{two-sample-inference}{%
\section{Two Sample Inference}\label{two-sample-inference}}

\hypertarget{means}{%
\subsection{Means}\label{means}}

Suppose we have 2 sets of observations,

\begin{itemize}
\tightlist
\item
  \(y_1,..., y_{n_y}\)\\
\item
  \(x_1,...,x_{n_x}\)
\end{itemize}

that are random samples from two independent populations with means \(\mu_y\) and \(\mu_x\) and variances \(\sigma^2_y\),\(\sigma^2_x\). Our goal is to compare \(\mu_x\) and \(\mu_y\) or \(\sigma^2_y = \sigma^2_x\)

\hypertarget{large-sample-tests}{%
\subsubsection{Large Sample Tests}\label{large-sample-tests}}

Assume that \(n_y\) and \(n_x\) are large (\(\ge 30\)). Then,

\[
E(\bar{y} - \bar{x}) = \mu_y - \mu_x \\
Var(\bar{y} - \bar{x}) = \sigma^2_y /n_y + \sigma^2_x/n_x
\]

Then,

\[
Z = \frac{\bar{y}-\bar{x} - (\mu_y - \mu_x)}{\sqrt{\sigma^2_y /n_y + \sigma^2_x/n_x}} \sim N(0,1)
\] (according to \protect\hyperlink{central-limit-theorem}{Central Limit Theorem}). For large samples, we can replace variances by their unbiased estimators (\(s^2_y,s^2_x\)), and get the same large sample distribution.

An approximate \(100(1-\alpha) \%\) CI for \(\mu_y - \mu_x\) is given by:

\[
\bar{y} - \bar{x} \pm z_{\alpha/2}\sqrt{s^2_y/n_y + s^2_x/n_x}
\]

\[
H_0: \mu_y - \mu_x = \delta_0 \\
H_A: \mu_y - \mu_x \neq \delta_0
\]

at the \(\alpha\)-level with the statistic:

\[
z = \frac{\bar{y}-\bar{x} - \delta_0}{\sqrt{s^2_y /n_y + s^2_x/n_x}}
\]

and reject \(H_0\) if \(|z| > z_{\alpha/2}\)

If \(\delta = )\), it means that we are testing whether two means are equal.

\hypertarget{small-sample-tests}{%
\subsubsection{Small Sample Tests}\label{small-sample-tests}}

If the two samples are from normal distribution, iid \(N(\mu_y,\sigma^2_y)\) and iid \(N(\mu_x,\sigma^2_x)\) and the two samples are independent, we can do inference based on the \protect\hyperlink{student-t}{t-distribution}

Then we have 2 cases

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{equal-variance}{Equal Variance}
\item
  \protect\hyperlink{unequal-variance}{Unequal Variance}
\end{itemize}

\hypertarget{equal-variance}{%
\paragraph{Equal variance}\label{equal-variance}}

\textbf{Assumptions}

\begin{itemize}
\tightlist
\item
  iid: so that \(var(\bar{y}) = \sigma^2_y / n_y ; var(\bar{x}) = \sigma^2_x / n_x\)\\
\item
  Independence between samples: No observation from one sample can influence any observation from the other sample, to have
\end{itemize}

\[
\begin{aligned}
var(\bar{y} - \bar{x}) &= var(\bar{y}) + var{\bar{x}} - 2cov(\bar{y},\bar{x}) \\
&= var(\bar{y}) + var{\bar{x}} \\
&= \sigma^2_y / n_y + \sigma^2_x / n_x 
\end{aligned}
\]

\begin{itemize}
\tightlist
\item
  Normality: Justifies the use of the \protect\hyperlink{student-t}{t-distribution}
\end{itemize}

Let \(\sigma^2 = \sigma^2_y = \sigma^2_x\). Then, \(s^2_y\) and \(s^2_x\) are both unbiased estimators of \(\sigma^2\). We then can pool them.

Then the pooled variance estimate is \[
s^2 = \frac{(n_y - 1)s^2_y + (n_x - 1)s^2_x}{(n_y-1)+(n_x-1)}
\] has \(n_y + n_x -2\) df.

Then the test statistic

\[
T = \frac{\bar{y}- \bar{x} -(\mu_y - \mu_x)}{s\sqrt{1/n_y + 1/n_x}} \sim t_{n_y + n_x -2}
\]

\(100(1 - \alpha) \%\) CI for \(\mu_y - \mu_x\) is

\[
\bar{y} - \bar{x} \pm (t_{n_y + n_x -2})s\sqrt{1/n_y + 1/n_x}
\]

Hypothesis testing:\\
\[
H_0: \mu_y - \mu_x = \delta_0 \\
H_1: \mu_y - \mu_x \neq \delta_0
\]

we reject \(H_0\) if \(|t| > t_{n_y + n_x -2;\alpha/2}\)

\hypertarget{unequal-variance}{%
\paragraph{Unequal Variance}\label{unequal-variance}}

\textbf{Assumptions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Two samples are independent\\

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Scatter plots\\
  \item
    \protect\hyperlink{correlation-coefficient-with-normal-probability-plots}{Correlation coefficient (if normal)}
  \end{enumerate}
\item
  Independence of observation in each sample\\

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Test for serial correlation\\
  \end{enumerate}
\item
  For each sample, homogeneity of variance\\

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Scatter plots\\
  \item
    Formal tests\\
  \end{enumerate}
\item
  \protect\hyperlink{normality-assessment}{Normality}\\
\item
  Equality of variances (homogeneity of variance between samples)\\

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    \protect\hyperlink{f-test}{F-test}\\
  \item
    Barlett test\\
  \item
    {[}Modified Levene Test{]}
  \end{enumerate}
\end{enumerate}

To compare 2 normal \(\sigma^2_y \neq \sigma^2_x\), we use the test statistic:

\[
T = \frac{\bar{y}- \bar{x} -(\mu_y - \mu_x)}{\sqrt{s^2_y/n_y + s^2_x/n_x}} 
\] In this case, T does not follow the \protect\hyperlink{student-t}{t-distribution} (its distribution depends on the ratio of the unknown variances \(\sigma^2_y,\sigma^2_x\)). In the case of small sizes, we can can approximate tests by using the Welch-Satterthwaite method \citep{Satterthwaite_1946}. We assume T can be approximated by a \protect\hyperlink{student-t}{t-distribution}, and adjust the degrees of freedom.

Let \(w_y = s^2_y /n_y\) and \(w_x = s^2_x /n_x\) (the w's are the square of the respective standard errors)\\
Then, the degrees of freedom are

\[
v = \frac{(w_y + w_x)^2}{w^2_y / (n_y-1) + w^2_x / (n_x-1)}
\]

Since v is usually fractional, we truncate down to the nearest integer.

\(100 (1-\alpha) \%\) CI for \(\mu_y - \mu_x\) is

\[
\bar{y} - \bar{x} \pm t_{v,\alpha/2} \sqrt{s^2_y/n_y + s^2_x /n_x}
\]

Reject \(H_0\) if \(|t| > t_{v,\alpha/2}\), where

\[
t = \frac{\bar{y} - \bar{x}-\delta_0}{\sqrt{s^2_y/n_y + s^2_x /n_x}}
\]

\hypertarget{variances}{%
\subsection{Variances}\label{variances}}

\[
F_{ndf,ddf}= \frac{s^2_1}{s^2_2}
\]

where \(s^2_1>s^2_2, ndf = n_1-1,ddf = n_2-1\)

\hypertarget{f-test}{%
\subsubsection{F-test}\label{f-test}}

Test

\[
H_0: \sigma^2_y = \sigma^2_x \\
H_a: \sigma^2_y \neq \sigma^2_x
\]

Consider the test statistic,

\[
F= \frac{s^2_y}{s^2_x}
\]

Reject \(H_0\) if

\begin{itemize}
\tightlist
\item
  \(F>f_{n_y -1,n_x -1,\alpha/2}\) or\\
\item
  \(F<f_{n_y -1,n_x -1,1-\alpha/2}\)
\end{itemize}

Where \(F>f_{n_y -1,n_x -1,\alpha/2}\) and \(F<f_{n_y -1,n_x -1,1-\alpha/2}\) are the upper and lower \(\alpha/2\) critical points of an \protect\hyperlink{f-distribution}{F-distribution}, with a \(n_y-1\) and \(n_x-1\) degrees of freedom.

\textbf{Note}

\begin{itemize}
\tightlist
\item
  This test depends heavily on the assumption Normality.\\
\item
  In particular, it could give to many significant results when observations come from long-tailed distributions (i.e., positive kurtosis).\\
\item
  If we cannot find support for \protect\hyperlink{normality-assessment}{normality}, then we can use nonparametric tests such as the \protect\hyperlink{modified-levene-test-brown-forsythe-test}{Modified Levene Test (Brown-Forsythe Test)}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(iris)}
\NormalTok{irisVe}\OtherTok{=}\NormalTok{iris}\SpecialCharTok{$}\NormalTok{Petal.Width[iris}\SpecialCharTok{$}\NormalTok{Species}\SpecialCharTok{==}\StringTok{"versicolor"}\NormalTok{] }
\NormalTok{irisVi}\OtherTok{=}\NormalTok{iris}\SpecialCharTok{$}\NormalTok{Petal.Width[iris}\SpecialCharTok{$}\NormalTok{Species}\SpecialCharTok{==}\StringTok{"virginica"}\NormalTok{]}

\FunctionTok{var.test}\NormalTok{(irisVe,irisVi)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  F test to compare two variances}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  irisVe and irisVi}
\CommentTok{\#\textgreater{} F = 0.51842, num df = 49, denom df = 49, p{-}value = 0.02335}
\CommentTok{\#\textgreater{} alternative hypothesis: true ratio of variances is not equal to 1}
\CommentTok{\#\textgreater{} 95 percent confidence interval:}
\CommentTok{\#\textgreater{}  0.2941935 0.9135614}
\CommentTok{\#\textgreater{} sample estimates:}
\CommentTok{\#\textgreater{} ratio of variances }
\CommentTok{\#\textgreater{}          0.5184243}
\end{Highlighting}
\end{Shaded}

\hypertarget{modified-levene-test-brown-forsythe-test}{%
\subsubsection{Modified Levene Test (Brown-Forsythe Test)}\label{modified-levene-test-brown-forsythe-test}}

\begin{itemize}
\tightlist
\item
  considers averages of absolute deviations rather than squared deviations. Hence, less sensitive to long-tailed distributions.\\
\item
  This test is still good for normal data
\end{itemize}

For each sample, we consider the absolute deviation of each observation form the median:

\[
d_{y,i} = |y_i - y_{.5}| \\
d_{x,i} = |x_i - x_{.5}|
\] Then,

\[
t_L^* = \frac{\bar{d}_y-\bar{d}_x}{s \sqrt{1/n_y + 1/n_x}}
\]

The pooled variance \(s^2\) is given by:

\[
s^2 = \frac{\sum_i^{n_y}(d_{y,i}-\bar{d}_y)^2 + \sum_j^{n_x}(d_{x,i}-\bar{d}_x)^2}{n_y + n_x -2}
\]

\begin{itemize}
\tightlist
\item
  If the error terms have constant variance and \(n_y\) and \(n_x\) are not extremely small, then \(t_L^* \sim t_{n_x + n_y -2}\)\\
\item
  We reject the null hypothesis when \(|t_L^*| > t_{n_y + n_x -2;\alpha/2}\)\\
\item
  This is just the two-sample t-test applied to the absolute deviations.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dVe}\OtherTok{=}\FunctionTok{abs}\NormalTok{(irisVe}\SpecialCharTok{{-}}\FunctionTok{median}\NormalTok{(irisVe)) }
\NormalTok{dVi}\OtherTok{=}\FunctionTok{abs}\NormalTok{(irisVi}\SpecialCharTok{{-}}\FunctionTok{median}\NormalTok{(irisVi)) }
\FunctionTok{t.test}\NormalTok{(dVe,dVi,}\AttributeTok{var.equal=}\NormalTok{T)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Two Sample t{-}test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  dVe and dVi}
\CommentTok{\#\textgreater{} t = {-}2.5584, df = 98, p{-}value = 0.01205}
\CommentTok{\#\textgreater{} alternative hypothesis: true difference in means is not equal to 0}
\CommentTok{\#\textgreater{} 95 percent confidence interval:}
\CommentTok{\#\textgreater{}  {-}0.12784786 {-}0.01615214}
\CommentTok{\#\textgreater{} sample estimates:}
\CommentTok{\#\textgreater{} mean of x mean of y }
\CommentTok{\#\textgreater{}     0.154     0.226}

\CommentTok{\# small samples t{-}test  }
\FunctionTok{t.test}\NormalTok{(irisVe,irisVi,}\AttributeTok{var.equal=}\NormalTok{F)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Welch Two Sample t{-}test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  irisVe and irisVi}
\CommentTok{\#\textgreater{} t = {-}14.625, df = 89.043, p{-}value \textless{} 2.2e{-}16}
\CommentTok{\#\textgreater{} alternative hypothesis: true difference in means is not equal to 0}
\CommentTok{\#\textgreater{} 95 percent confidence interval:}
\CommentTok{\#\textgreater{}  {-}0.7951002 {-}0.6048998}
\CommentTok{\#\textgreater{} sample estimates:}
\CommentTok{\#\textgreater{} mean of x mean of y }
\CommentTok{\#\textgreater{}     1.326     2.026}
\end{Highlighting}
\end{Shaded}

\hypertarget{power-1}{%
\subsection{Power}\label{power-1}}

Consider \(\sigma^2_y = \sigma^2_x = \sigma^2\)\\
Under the assumption of equal variances, we take size samples from both groups (\(n_y = n_x = n\))

For 1-sided testing,

\[
H_0: \mu_y - \mu_x \le 0 \\
H_a: \mu_y - \mu_x > 0
\]

\(\alpha\)-level z-test rejects \(H_0\) if

\[
z = \frac{\bar{y} - \bar{x}}{\sigma \sqrt{2/n}} > z_{\alpha}
\]

\[
\pi(\mu_y - \mu_x) = \Phi(-z_{\alpha} + \frac{\mu_y -\mu_x}{\sigma}\sqrt{n/2})
\]

We need sample size n that give at least \(1-\beta\) power when \(\mu_y - \mu_x = \delta\), where \(\delta\) is the smallest difference that we want to see.

Power is given by:

\[
\Phi(-z_{\alpha} + \frac{\delta}{\sigma}\sqrt{n/2}) = 1 - \beta
\]

\hypertarget{sample-size-1}{%
\subsection{Sample Size}\label{sample-size-1}}

Then, the sample size is

\[
n = 2(\frac{\sigma (z_{\alpha} + z_{\beta}}{\delta})^2
\]

For 2-sided test, replace \(z_{\alpha}\) with \(z_{\alpha/2}\).\\
As with the one-sample case, to perform an exact 2-sample t-test sample size calculation, we must use a non-central \protect\hyperlink{student-t}{t-distribution}.

A correction that gives the approximate t-test sample size can be obtained by using the z-test n value in the formula:\\
\[
n^* = 2(\frac{\sigma (t_{2n-2;\alpha} + t_{2n-2;\beta})}{\delta})^2
\]

where we use \(\alpha/2\) for the two-sided test

\hypertarget{matched-pair-designs}{%
\subsection{Matched Pair Designs}\label{matched-pair-designs}}

We have two treatments

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Subject & Treatment A & Treatment B & Difference \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & \(y_1\) & \(x_1\) & \(d_1 = y_1 - x_1\) \\
2 & \(y_2\) & \(x_2\) & \(d_2 = y_2 - x_2\) \\
. & . & . & . \\
n & \(y_n\) & \(x_n\) & \(d_n = y_n - x_n\) \\
\end{longtable}

we assume \(y_i \sim^{iid} N(\mu_y, \sigma^2_y)\) and \(x_i \sim^{iid} N(\mu_x,\sigma^2_x)\), but since \(y_i\) and \(x_i\) are measured on the same subject, they are correlated.

Let

\[
\mu_D = E(y_i - x_i) = \mu_y -\mu_x \\
\sigma^2_D = var(y_i - x_i) = Var(y_i) + Var(x_i) -2cov(y_i,x_i)
\]

If the matching induces \textbf{positive} correlation, then the variance of the difference of the measurements is reduced as compared to the independent case. This is the point of \protect\hyperlink{matched-pair-designs}{Matched Pair Designs}. Although covariance can be negative, giving a larger variance of the difference than the independent sample case, usually the covariance is positive. This means both \(y_i\) and \(x_i\) are large for many of the same subjects, and for others, both measurement are small. (we still assume that various subjects respond independently of each other, which is necessary for the iid assumption within groups).

Let \(d_i = y_i - x_i\), then

\begin{itemize}
\tightlist
\item
  \(\bar{d} = \bar{y}-\bar{x}\) is the sample mean of the \(d_i\)\\
\item
  \(s_d^2=\frac{1}{n-1}\sum_{i=1}^n (d_i - \bar{d})^2\) is the sample variance of the difference
\end{itemize}

Once the data are converted to differences, we are back to \protect\hyperlink{one-sample-inference}{One Sample Inference} and can use its tests and CIs.

\hypertarget{nonparametric-tests-for-two-samples}{%
\subsection{Nonparametric Tests for Two Samples}\label{nonparametric-tests-for-two-samples}}

For \protect\hyperlink{matched-pair-designs}{Matched Pair Designs}, we can use the \protect\hyperlink{one-sample-non-parametric-methods}{One-sample Non-parametric Methods}.

Assume that Y and X are random variables with CDF \(F_y\) and \(F_x\). then, Y is \textbf{stochastically} larger than X for all real number u, \(P(Y > u) \ge P(X > u)\).

Equivalently, \(P(Y \le u) \le P(X \le u)\), which is \(F_Y(u) \le F_X(u)\), same thing as \(F_Y < F_X\)

If two distributions are identical, except that one is shifted relative to the other, then each of distribution can be indexed by a location parameter, say \(\theta_y\) and \(\theta_x\). In this case, \(Y>X\) if \(\theta_y > \theta_x\)

Consider the hypotheses,

\[
H_0: F_Y = F_X \\
H_a: F_Y < F_X
\] where the alternative is an upper one-sided alternative.

\begin{itemize}
\tightlist
\item
  We can also consider the lower one-sided alternative
\end{itemize}

\[
H_a: F_Y > F_X \text{ or} \\
H_a: F_Y < F_X \text{ or } F_Y > F_X
\]

\begin{itemize}
\tightlist
\item
  In this case, we don't use \(H_a: F_Y \neq F_X\) as that allows arbitrary differences between the distributions, without requiring one be stochastically larger than the other.
\end{itemize}

If the distributions only differ in terms of their location parameters, we can focus hypothesis tests on the parameters (e.g., \(H_0: \theta_y = \theta_x\) vs.~\(\theta_y > \theta_x\))

We have 2 equivalent nonparametric tests that consider the hypothesis mentioned above

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \protect\hyperlink{wilcoxon-rank-test}{Wilcoxon rank test}\\
\item
  \protect\hyperlink{mann-whitney-u-test}{Mann-Whitney U test}
\end{enumerate}

\hypertarget{wilcoxon-rank-test}{%
\subsubsection{Wilcoxon rank test}\label{wilcoxon-rank-test}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Combine all \(n= n_y + n_x\) observations and rank them in ascending order.\\
\item
  Sum the ranks of the \(y\)'s and \(x\)'s separately. Let \(w_y\) and \(w_x\) be these sums. (\(w_y + w_x = 1 + 2 + ... + n = n(n+1)/2\))\\
\item
  Reject \(H_0\) if \(w_y\) is large (equivalently, \(w_x\) is small)
\end{enumerate}

Under \(H_0\), any arrangement of the \(y\)'s and \(x\)'s is equally likely to occur, and there are \((n_y + n_x)!/(n_y! n_x!)\) possible arrangements.

\begin{itemize}
\tightlist
\item
  Technically, for each arrangement we can compute the values of \(w_y\) and \(w_x\), and thus generate the distribution of the statistic under the null hypothesis.\\
\item
  This could lead to computationally intensive.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{wilcox.test}\NormalTok{(}
\NormalTok{    irisVe,}
\NormalTok{    irisVi,}
    \AttributeTok{alternative =} \StringTok{"two.sided"}\NormalTok{,}
    \AttributeTok{conf.level =} \FloatTok{0.95}\NormalTok{,}
    \AttributeTok{exact =}\NormalTok{ F,}
    \AttributeTok{correct =}\NormalTok{ T}
\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Wilcoxon rank sum test with continuity correction}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  irisVe and irisVi}
\CommentTok{\#\textgreater{} W = 49, p{-}value \textless{} 2.2e{-}16}
\CommentTok{\#\textgreater{} alternative hypothesis: true location shift is not equal to 0}
\end{Highlighting}
\end{Shaded}

\hypertarget{mann-whitney-u-test}{%
\subsubsection{Mann-Whitney U test}\label{mann-whitney-u-test}}

The Mann-Whitney test is computed as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compare each \(y_i\) with each \(x_i\).\\
  Let \(u_y\) be the number of pairs in which \(y_i > x_i\) Let \(u_x\) be the number of pairs in which \(y_i < x_i\). (assume there are no ties). There are \(n_y n_x\) such comparisons and \(u_y + u_x = n_y n_x\).\\
\item
  Reject \(H_0\) if \(u_y\) is large (or \(u_x\) is small)
\end{enumerate}

\protect\hyperlink{mann-whitney-u-test}{Mann-Whitney U test} and \protect\hyperlink{wilcoxon-rank-test}{Wilcoxon rank test} are related:\\
\[
u_y = w_y - n_y(n_y+1) /2 \\
u_x = w_x - n_x(n_x +1)/2
\]

An \(\alpha\)-level test rejects \(H_0\) if \(u_y \ge u_{n_y,n_x,\alpha}\), where \(u_{n_y,n_x,\alpha}\) is the upper \(\alpha\) critical point of the null distribution of the random variable, U.

The p-value is defined to be \(P(Y \ge u_y) = P(U \le u_x)\). One advantage of \protect\hyperlink{mann-whitney-u-test}{Mann-Whitney U test} is that we can use either \(u_y\) or \(u_x\) to carry out the test.

For large \(n_y\) and \(n_x\), the null distribution of U can be well approximated by a normal distribution with mean \(E(U) = n_y n_x /2\) and variance \(var(U) = n_y n_x (n+1)/12\). A large sample z-test can be based on the statistic:

\[
z = \frac{u_y - n_y n_x /2 -1/2}{\sqrt{n_y n_x (n+1)/12}}
\]

The test rejects \(H_0\) at level \(\alpha\) if \(z \ge z_{\alpha}\) or if \(u_y \ge u_{n_y,n_x,\alpha}\) where

\[
u_{n_y, n_x, \alpha} \approx n_y n_x /2 + 1/2 + z_{\alpha}\sqrt{n_y n_x (n+1)/12}
\]

For the 2-sided test, we use the test statistic \(u_{max} = max(u_y,u_x)\) and \(u_{min} = min(u_y, u_x)\) and p-value is given by

\[
p-value = 2P(U \ge u_{max}) = 2P(U \le u_{min})
\] Since we assume there are no ties (when \(y_i = x_j\)), we count 1/2 towards both \(u_y\) and \(u_x\). Even though the sampling distribution is not the same, but large sample approximation is still reasonable,

\hypertarget{categorical-data-analysis}{%
\section{Categorical Data Analysis}\label{categorical-data-analysis}}

\protect\hyperlink{categorical-data-analysis}{Categorical Data Analysis} when we have categorical outcomes

\begin{itemize}
\tightlist
\item
  Nominal variables: no logical ordering (e.g., sex)\\
\item
  Ordinal variables: logical order, but relative distances between values are not clear (e.g., small, medium, large)
\end{itemize}

The distribution of one variable changes when the level (or values) of the other variable change. The row percentages are different in each column.

\hypertarget{inferences-for-small-samples}{%
\subsection{Inferences for Small Samples}\label{inferences-for-small-samples}}

The approximate tests based on the asymptotic normality of \(\hat{p}_1 - \hat{p}_2\) do not apply for small samples.

Using \textbf{Fisher's Exact Test} to evaluate \(H_0: p_1 = p_2\)

\begin{itemize}
\tightlist
\item
  Assume \(X_1\) and \(X_2\) are independent \protect\hyperlink{binomial}{Binomial}\\
\item
  Let \(x_1\) and \(x_2\) be the corresponding observed values.
\item
  Let \(n= n_1 + n_2\) be the total sample size
\item
  \(m = x_1 + x_2\) be the observed number of successes.\\
\item
  By assuming that m (total successes) is fixed, and conditioning on this value, one can show that the conditional distribution of the number of successes from sample 1 is \protect\hyperlink{hypergeometric}{Hypergeometric}\\
\item
  If we want to test \(H_0: p_1 = p_2\) and \(H_a: p_1 \neq p_2\), we have
\end{itemize}

\[
Z^2 = (\frac{\hat{p}_1 - \hat{p}_2}{\sqrt{\hat{p}(1-\hat{p})(1/n_1 + 1/n_2)}})^2 \sim \chi_{1,\alpha}^2
\]

where \(\chi_{1,\alpha}^2\) is the upper \(\alpha\) percentage point for the central \protect\hyperlink{chi-squared}{Chi-squared} with one d.f.

This extends to the contingency table setting: whether the observed frequencies are equal to those expected under a null hypothesis of no association.

\hypertarget{test-of-association}{%
\subsection{Test of Association}\label{test-of-association}}

Pearson Chi-square test statistic is

\[
\chi^2 = \sum_{\text{all categories}} \frac{(observed - epxected)^2}{expected}
\]

Comparison of proportions for several independent surveys or experiments

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.3088}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2059}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2059}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.0735}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2059}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Experiment 1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Experiment 2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\ldots{}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Experiment k
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Number of successes & \(x_1\) & \(x_2\) & \ldots{} & \(x_k\) \\
Number of failures & \(n_1 - x_1\) & \(n_2 - x_2\) & \ldots{} & \(n_k - x_k\) \\
& \(n_1\) & \(n_2\) & \ldots{} & \(n_k\) \\
\end{longtable}

\(H_0: p_1 = p_2 = \dots = p_k\) vs.~the alternative that the null is not true (at least one pair are not equal).

We estimate the common value of the probability of success on a single trial assuming \(H_0\) is true:

\[
\hat{p} = \frac{x_1 + x_2 + ... + x_k}{n_1 + n_2 + ...+ n_k}
\]

we use table of expected counts when \(H_0\) is true:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1304}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2609}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2609}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.0725}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2754}}@{}}
\toprule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
success & \(n_1 \hat{p}\) & \(n_2 \hat{p}\) & \ldots{} & \(n_k \hat{p}\) \\
failure & \(n_1(1-\hat{p})\) & \(n_2(1-\hat{p})\) & \ldots{} & \(n_k (1-\hat{p})\) \\
& \(n_1\) & \(n_2\) & \ldots{} & \(n_k\) \\
\end{longtable}

\[
\chi^2 = \sum_{\text{all cells in table}} \frac{(observed - expected)^2}{expected}
\]

with \(k-1\) degrees of freedom

\hypertarget{two-way-count-data}{%
\subsubsection{Two-way Count Data}\label{two-way-count-data}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1867}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.0667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.0667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1467}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\ldots{}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
j
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\ldots{}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
c
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Row Total
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & \(n_{11}\) & \(n_{12}\) & \ldots{} & \(n_{1j}\) & \ldots{} & \(n_{1c}\) & \(n_{1.}\) \\
2 & \(n_{21}\) & \(n_{22}\) & \ldots{} & \(n_{2j}\) & \ldots{} & \(n_{2c}\) & \(n_{2.}\) \\
. & . & . & . & . & . & . & . \\
r & \(n_{r1}\) & \(n_{r2}\) & \ldots{} & \(n_{rj}\) & \ldots{} & \(n_{rc}\) & \(n_{r.}\) \\
Column Total & \(n_{.1}\) & \(n_{.2}\) & \ldots{} & \(n_{.j}\) & \ldots{} & \(n_{.c}\) & \(n_{}\) \\
\end{longtable}

\textbf{Design 1}\\
total sample size fixed \(n\) = constant (e.g., survey on job satisfaction and income); both row and column totals are random variables

\textbf{Design 2}\\
Fix the sample size in each group (in each row) (e.g., Drug treatments success or failure); fixed number of participants for each treatment; independent random samples from the two row populations.

These different sampling designs imply two different probability models.

\hypertarget{total-sample-size-fixed}{%
\subsubsection{Total Sample Size Fixed}\label{total-sample-size-fixed}}

\textbf{Design 1}

random sample of size n drawn from a single population, and sample units are cross-classified into \(r\) row categories and \(c\) column

This results in an \(r \times c\) table of observed counts

\(n_{ij} = 1,...,r;j=1,...,c\)

Let \(p_{ij}\) be the probability of classification into cell \((i,j)\) and \(\sum_{i=1}^r \sum_{j=1}^c p_{ij} = 1\). Let \(N_{ij}\) be the random variable corresponding to \(n_{ij}\)\\
The joint distribution of the \(N_{ij}\) is multinomial with unknown parameters \(p_{ij}\)

Denote the row variable by \(X\) and column variable by Y, then \(p_{ij} = P(X=i,Y = j)\) and \(p_{i.} = P(X = i)\) and \(p_{.j} = P(Y = j)\) are the marginal probabilities.

\hfill\break
The null hypothesis that X and Y are statistically independent (i.e., no association) is just:

\[
H_0: p_{ij} = P(X =i,Y=j) = P(X =i) P(Y =j) = p_{i.}p_{.j} \\
H_a: p_{ij} \neq p_{i.}p_{.j}
\] for all \(i,j\).

\hypertarget{row-total-fixed}{%
\subsubsection{Row Total Fixed}\label{row-total-fixed}}

\textbf{Design 2}

Random samples of sizes \(n_1,...,n_r\) are drawn independently from \(r \ge 2\) row populations. In this case, the 2-way table row totals are \(n_{i.} = n_i\) for \(i = 1,...,r\).

The counts from each row are modeled by independent multinomial distributions.

\(X\) is fixed, \(Y\) is observed.

Then, \(p_{ij}\) represent conditional probabilities \(p_{ij} = P(Y=j|X=i)\)

The null hypothesis is the probability of response j is the same, regardless of the row population (i.e., no association):

\[
\begin{cases}
H_0: p_{ij} = P(Y = j | X = i) = p_j & \text{for all } i,j =1,2,...,c \\
\text{or } H_0: (p_{i1},p_{i2},...,p_{ic}) = (p_1,p_2,...,p_c) & \text{ for all } i\\
H_a: (p_{i1},p_{i2},...,p_{ic}) & \text{ are not the same for all } i
\end{cases}
\]

Although the hypotheses to be tested are different for two sampling designs, \textbf{The} \(\chi^2\) \textbf{test is identical}

We have estimated expected frequencies:

\[
\hat{e}_{ij} = \frac{n_{i.}n_{.j}}{n}
\]

The Chi-square statistic is

\[
\chi^2 = \sum_{i=1}^{r} \sum_{j=1}^{c} \frac{(n_{ij}-\hat{e}_{ij})^2}{\hat{e}_{ij}} \sim \chi_{(r-1)(c-1)}
\]

\(\alpha\)-level test rejects \(H_0\) if \(\chi^2 > \chi^2_{(r-1)(c-1),\alpha}\)

\hypertarget{pearson-chi-square-test}{%
\subsubsection{Pearson Chi-square Test}\label{pearson-chi-square-test}}

\begin{itemize}
\tightlist
\item
  Determine whether an association exists\\
\item
  Sometimes, \(H_0\) represents the model whose validity is to be tested. Contrast this with the conventional formulation of \(H_0\) as the hypothesis that is to be disproved. The goal in this case is not to disprove the model, but to see whether data are consistent with the model and if deviation can be attributed to chance.\\
\item
  These tests do not measure the strength of an association.\\
\item
  These tests depend on and reflect the sample size - double the sample size by copying each observation, double the \(\chi^2\) statistic even thought the strength of the association does not change.\\
\item
  The \protect\hyperlink{pearson-chi-square-test}{Pearson Chi-square Test} is not appropriate when more than about 20\% of the cells have an expected cell frequency of less than 5 (large-sample p-values not appropriate).\\
\item
  When the sample size is small the exact p-values can be calculated (this is prohibitive for large samples); calculation of the exact p-values assumes that the column totals and row totals are fixed.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{july.x}\OtherTok{=}\DecValTok{480} 
\NormalTok{july.n}\OtherTok{=}\DecValTok{1000} 
\NormalTok{sept.x}\OtherTok{=}\DecValTok{704} 
\NormalTok{sept.n}\OtherTok{=}\DecValTok{1600}
\end{Highlighting}
\end{Shaded}

\[
H_0: p_J = 0.5 \\
H_a: p_J < 0.5
\]

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{prop.test}\NormalTok{(}
    \AttributeTok{x =}\NormalTok{ july.x,}
    \AttributeTok{n =}\NormalTok{ july.n,}
    \AttributeTok{p =} \FloatTok{0.5}\NormalTok{,}
    \AttributeTok{alternative =} \StringTok{"less"}\NormalTok{,}
    \AttributeTok{correct =}\NormalTok{ F}
\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  1{-}sample proportions test without continuity correction}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  july.x out of july.n, null probability 0.5}
\CommentTok{\#\textgreater{} X{-}squared = 1.6, df = 1, p{-}value = 0.103}
\CommentTok{\#\textgreater{} alternative hypothesis: true p is less than 0.5}
\CommentTok{\#\textgreater{} 95 percent confidence interval:}
\CommentTok{\#\textgreater{}  0.0000000 0.5060055}
\CommentTok{\#\textgreater{} sample estimates:}
\CommentTok{\#\textgreater{}    p }
\CommentTok{\#\textgreater{} 0.48}
\end{Highlighting}
\end{Shaded}

\[
H_0: p_J = p_S \\
H_a: p_j \neq p_S
\]

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{prop.test}\NormalTok{(}
    \AttributeTok{x =} \FunctionTok{c}\NormalTok{(july.x, sept.x),}
    \AttributeTok{n =} \FunctionTok{c}\NormalTok{(july.n, sept.n),}
    \AttributeTok{correct =}\NormalTok{ F}
\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  2{-}sample test for equality of proportions without continuity correction}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  c(july.x, sept.x) out of c(july.n, sept.n)}
\CommentTok{\#\textgreater{} X{-}squared = 3.9701, df = 1, p{-}value = 0.04632}
\CommentTok{\#\textgreater{} alternative hypothesis: two.sided}
\CommentTok{\#\textgreater{} 95 percent confidence interval:}
\CommentTok{\#\textgreater{}  0.0006247187 0.0793752813}
\CommentTok{\#\textgreater{} sample estimates:}
\CommentTok{\#\textgreater{} prop 1 prop 2 }
\CommentTok{\#\textgreater{}   0.48   0.44}
\end{Highlighting}
\end{Shaded}

\hypertarget{ordinal-association}{%
\subsection{Ordinal Association}\label{ordinal-association}}

\begin{itemize}
\tightlist
\item
  An ordinal association implies that as one variable increases, the other tends to increase or decrease (depending on the nature of the association).\\
\item
  For tests for variables with two or more levels, the levels must be in a logical ordering.
\end{itemize}

\hypertarget{mantel-haenszel-chi-square-test}{%
\subsubsection{Mantel-Haenszel Chi-square Test}\label{mantel-haenszel-chi-square-test}}

The \protect\hyperlink{mantel-haenszel-chi-square-test}{Mantel-Haenszel Chi-square Test} is more powerful for testing ordinal associations, but does not test for the strength of the association.

This test is presented in the case where one has a series of \(2 \times 2\) tables that examine the same effects under different conditions (If there are \(K\) such tables, we have \(2 \times 2 \times K\) table)

In stratum \(k\), given the marginal totals \((n_{.1k},n_{.2k},n_{1.k},n_{2.k})\), the sampling model for cell counts is the \protect\hyperlink{hypergeometric}{Hypergeometric} (knowing \(n_{11k}\) determines \((n_{12k},n_{21k},n_{22k})\), given the marginal totals)

Assuming conditional independence, the \protect\hyperlink{hypergeometric}{Hypergeometric} mean and variance of \(n_{11k}\) are

\[
m_{11k} = E(n_{11k}) = \frac{n_{1.k} n_{.1k}}{n_{..k}} \\
var(n_{11k}) = \frac{n_{1.k} n_{2.k} n_{.1k} n_{.2k}}{n_{..k}^2(n_{..k}-1)}
\]

To test conditional independence, Mantel and Haenszel proposed

\[
M^2 = \frac{(|\sum_{k} n_{11k} - \sum_k m_{11k}| -.5)^2}{\sum_{k}var(n_{11k})} \sim \chi^2_{1}
\] This method can be extended to general \(I \times J \times K\) tables.

\((2 \times 2 \times 3)\) table

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Bron }\OtherTok{=} \FunctionTok{array}\NormalTok{(}
    \FunctionTok{c}\NormalTok{(}\DecValTok{20}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{382}\NormalTok{, }\DecValTok{214}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{172}\NormalTok{, }\DecValTok{120}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{327}\NormalTok{, }\DecValTok{183}\NormalTok{),}
    \AttributeTok{dim =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{),}
    \AttributeTok{dimnames =} \FunctionTok{list}\NormalTok{(}
        \AttributeTok{Particulate =} \FunctionTok{c}\NormalTok{(}\StringTok{"High"}\NormalTok{, }\StringTok{"Low"}\NormalTok{),}
        \AttributeTok{Bronchitis =} \FunctionTok{c}\NormalTok{(}\StringTok{"Yes"}\NormalTok{, }\StringTok{"No"}\NormalTok{),}
        \AttributeTok{Age =} \FunctionTok{c}\NormalTok{(}\StringTok{"15{-}24"}\NormalTok{, }\StringTok{"25{-}39"}\NormalTok{, }\StringTok{"40+"}\NormalTok{)}
\NormalTok{    )}
\NormalTok{)}
\FunctionTok{margin.table}\NormalTok{(Bron, }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\CommentTok{\#\textgreater{}            Bronchitis}
\CommentTok{\#\textgreater{} Particulate Yes  No}
\CommentTok{\#\textgreater{}        High  42 881}
\CommentTok{\#\textgreater{}        Low   22 517}
\CommentTok{\# assess whether the relationship between }
\CommentTok{\# Bronchitis by Particulate Level varies by Age}
\FunctionTok{library}\NormalTok{(samplesizeCMH)}
\NormalTok{marginal\_table }\OtherTok{=} \FunctionTok{margin.table}\NormalTok{(Bron, }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\FunctionTok{odds.ratio}\NormalTok{(marginal\_table)}
\CommentTok{\#\textgreater{} [1] 1.120318}

\CommentTok{\#  whether these odds vary by age. }
\CommentTok{\# The conditional odds can be calculated using the original table.}
\FunctionTok{apply}\NormalTok{(Bron, }\DecValTok{3}\NormalTok{, odds.ratio)}
\CommentTok{\#\textgreater{}     15{-}24     25{-}39       40+ }
\CommentTok{\#\textgreater{} 1.2449098 0.9966777 1.1192661}

\CommentTok{\# Mantel{-}Haenszel Test}
\FunctionTok{mantelhaen.test}\NormalTok{(Bron, }\AttributeTok{correct =}\NormalTok{ T)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Mantel{-}Haenszel chi{-}squared test with continuity correction}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  Bron}
\CommentTok{\#\textgreater{} Mantel{-}Haenszel X{-}squared = 0.11442, df = 1, p{-}value = 0.7352}
\CommentTok{\#\textgreater{} alternative hypothesis: true common odds ratio is not equal to 1}
\CommentTok{\#\textgreater{} 95 percent confidence interval:}
\CommentTok{\#\textgreater{}  0.6693022 1.9265813}
\CommentTok{\#\textgreater{} sample estimates:}
\CommentTok{\#\textgreater{} common odds ratio }
\CommentTok{\#\textgreater{}          1.135546}
\end{Highlighting}
\end{Shaded}

\hypertarget{mcnemars-test}{%
\paragraph{McNemar's Test}\label{mcnemars-test}}

special case of \protect\hyperlink{mantel-haenszel-chi-square-test}{Mantel-Haenszel Chi-square Test}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vote }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{682}\NormalTok{, }\DecValTok{22}\NormalTok{), }\FunctionTok{c}\NormalTok{(}\DecValTok{86}\NormalTok{, }\DecValTok{810}\NormalTok{))}
\FunctionTok{mcnemar.test}\NormalTok{(vote, }\AttributeTok{correct =}\NormalTok{ T)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  McNemar\textquotesingle{}s Chi{-}squared test with continuity correction}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  vote}
\CommentTok{\#\textgreater{} McNemar\textquotesingle{}s chi{-}squared = 36.75, df = 1, p{-}value = 1.343e{-}09}
\end{Highlighting}
\end{Shaded}

\hypertarget{spearman-rank-correlation}{%
\subsubsection{Spearman Rank Correlation}\label{spearman-rank-correlation}}

To test for the strength of association between two ordinally scaled variables, we can use \protect\hyperlink{spearman-rank-correlation}{Spearman Rank Correlation} statistic

Let \(X\) and \(Y\) be two random variables measured on an ordinal scale. Consider \(n\) pairs of observations (\(x_i,y_i\)), \(i = 1,\dots,n\)

The \protect\hyperlink{spearman-rank-correlation}{Spearman Rank Correlation} coefficient (denoted by \(r_S\) is calculated using the Pearson correlation formula, but based on the ranks of \(x_i\) and \(y_i\)).

\protect\hyperlink{spearman-rank-correlation}{Spearman Rank Correlation} be calculated

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Assign ranks to \(x_i\)'s and \(y_i\)'s separately. Let \(u_i = rank(x_i)\) and \(v_i = rank(y_i)\)\\
\item
  Calculate \(r_S\) using the formula for the Pearson correlation coefficient, but applied to the ranks:
\end{enumerate}

\[
r_S = \frac{\sum_{i=1}^{n}(u_i - \bar{u})(v_i - \bar{v})}{\sqrt{(\sum_{i = 1}^{n}(u_i - \bar{u})^2)(\sum_{i=1}^{n}(v_i - \bar{v})^2)}}
\]

\(r_S\) ranges between -1 and +1 , with

\begin{itemize}
\tightlist
\item
  \(r_S = -1\) if there is a perfect negative monotone association
\item
  \(r_S = +1\) if there is a perfect positive monotone association between X and Y.
\end{itemize}

To test

\begin{itemize}
\item
  \(H_0:\) \(X\) and \(Y\) independent
\item
  \(H_a\): \(X\) and \(Y\) positively associated
\end{itemize}

For large \(n\) (e.g., \(n \ge 10\)),

\[
r_S \sim N(0,1/(n-1))
\]

Then,

\[
Z = r_s \sqrt{n-1} \sim N(0,1)
\]

\hypertarget{divergence-metrics-and-test-for-comparing-distributions}{%
\section{Divergence Metrics and Test for Comparing Distributions}\label{divergence-metrics-and-test-for-comparing-distributions}}

Similarity among distributions using divergence statistics, which is different from

\begin{itemize}
\item
  Deviation statistics: difference between the realization of a variable and some value (e.g., mean). Statistics of the deviation distributions consist of standard deviation, average absolute deviation, median absolute deviation , maximum absolute deviation.
\item
  Deviance statistics: goodness-of-fit statistic for statistical models (comparable to the sum of squares of residuals in OLS to cases that use ML estimation). Usually used in generalized linear models.
\end{itemize}

Divergence statistics is a statistical distance (different from metrics)

\begin{itemize}
\item
  Divergences do not require symmetry
\item
  Divergences generalize squared distance (instead of linear distance). Hence, fail the triangle inequity
\end{itemize}

Can be used for

\begin{itemize}
\item
  Detecting data drift in machine learning
\item
  Feature selections
\item
  Variational Auto Encoder
\item
  Detect similarity between policies (i.e., distributions) in reinforcement learning
\item
  To see consistency in two measured variables of two constructs.
\end{itemize}

Techniques

\begin{itemize}
\item
  \protect\hyperlink{kullback-leibler-divergence}{Kullback-Leibler Divergence}
\item
  \protect\hyperlink{jensen-shannon-divergence}{Jensen-Shannon Divergence}
\item
  \protect\hyperlink{kolmogorov-smirnov-test}{Kolmogorov-Smirnov Test}
\end{itemize}

Packages

\begin{itemize}
\item
  \texttt{entropy}
\item
  \texttt{philentropy}
\end{itemize}

\hypertarget{kullback-leibler-divergence}{%
\subsection{Kullback-Leibler Divergence}\label{kullback-leibler-divergence}}

\begin{itemize}
\item
  Also known as relative entropy
\item
  Not a metric (does not satisfy the triangle inequality)
\item
  Can be generalized to the multivariate case
\item
  Measure the similarity between two discrete probability distributions

  \begin{itemize}
  \item
    \(P\) = true data distribution
  \item
    \(Q\) = predicted data distribution
  \end{itemize}
\item
  It quantifies info loss when moving from \(P\) to \(Q\) (i.e., information loss when \(P\) is approximated by \(Q\))
\end{itemize}

Discrete

\[
D_{KL}(P ||Q) = \sum_i P_i \log(\frac{P_i}{Q_i})
\]

Continuous

\[
D_{KL}(P||Q) = \int P(x) \log(\frac{P(x)}{Q(x)}) dx
\]

where

\begin{itemize}
\item
  \(K \in [0, \infty)\) from similar to diverge
\item
  Non-symmetric between two distributions: \(D_{KL}(P|Q) \neq D_{KL}(Q|P)\)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(philentropy)}
\CommentTok{\# philentropy::dist.diversity(rbind(X = 1:10 / sum(1:10), }
\CommentTok{\#                                   Y = 1:20 / sum(1:20)),}
\CommentTok{\#                             p = 2,}
\CommentTok{\#                             unit = "log2")}


\CommentTok{\# continuous}
\FunctionTok{KL}\NormalTok{(}\FunctionTok{rbind}\NormalTok{(}\AttributeTok{X =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10} \SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{), }\AttributeTok{Y =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10} \SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{)), }\AttributeTok{unit =} \StringTok{"log2"}\NormalTok{)}
\CommentTok{\#\textgreater{} kullback{-}leibler }
\CommentTok{\#\textgreater{}                0}

\CommentTok{\# discrete}
\FunctionTok{KL}\NormalTok{(}\FunctionTok{rbind}\NormalTok{(}\AttributeTok{X =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{, }\AttributeTok{Y =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{), }\AttributeTok{est.prob =} \StringTok{"empirical"}\NormalTok{)}
\CommentTok{\#\textgreater{} kullback{-}leibler }
\CommentTok{\#\textgreater{}                0}
\end{Highlighting}
\end{Shaded}

\hypertarget{jensen-shannon-divergence}{%
\subsection{Jensen-Shannon Divergence}\label{jensen-shannon-divergence}}

\begin{itemize}
\tightlist
\item
  Also known as info radius or total divergence to the average
\end{itemize}

\[
D_{JS} (P ||Q) = \frac{1}{2}( D_{KL}(P||M)+ D_{KL}(Q||M))
\]

where

\begin{itemize}
\item
  \(M = \frac{1}{2} (P + Q)\) is a mixed distribution
\item
  \(D_{JS} \in [0,1]\) for \(\log_2\) and \(D_{JS} \in [0,\ln(2)]\) for \(\log_e\)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(philentropy)}
\CommentTok{\# continous}
\FunctionTok{JSD}\NormalTok{(}\FunctionTok{rbind}\NormalTok{(}\AttributeTok{X =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{, }\AttributeTok{Y =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{20}\NormalTok{), }\AttributeTok{unit =} \StringTok{"log2"}\NormalTok{)}
\CommentTok{\#\textgreater{} jensen{-}shannon }
\CommentTok{\#\textgreater{}       20.03201}

\CommentTok{\# discrete}
\FunctionTok{JSD}\NormalTok{(}\FunctionTok{rbind}\NormalTok{(}\AttributeTok{X =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{, }\AttributeTok{Y =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{20}\NormalTok{), }\AttributeTok{est.prob =} \StringTok{"empirical"}\NormalTok{)}
\CommentTok{\#\textgreater{} jensen{-}shannon }
\CommentTok{\#\textgreater{}     0.06004756}
\end{Highlighting}
\end{Shaded}

\hypertarget{wasserstein-distance}{%
\subsection{Wasserstein Distance}\label{wasserstein-distance}}

\begin{itemize}
\tightlist
\item
  measure the distance between two empirical CDFs
\end{itemize}

\[
W = \int_{x \in R}|E(x) - F(X)|^p
\]

\begin{itemize}
\tightlist
\item
  This is also a test statistics
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{transport}\SpecialCharTok{::}\FunctionTok{wasserstein1d}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\AttributeTok{mean =} \DecValTok{1}\NormalTok{))}
\CommentTok{\#\textgreater{} [1] 0.8533046}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\CommentTok{\# Wasserstein metric }
\NormalTok{twosamples}\SpecialCharTok{::}\FunctionTok{wass\_stat}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\AttributeTok{mean =} \DecValTok{1}\NormalTok{))}
\CommentTok{\#\textgreater{} [1] 0.8533046}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\CommentTok{\# permutation{-}based tw sample test using Wasserstein metric}
\NormalTok{twosamples}\SpecialCharTok{::}\FunctionTok{wass\_test}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\AttributeTok{mean =} \DecValTok{1}\NormalTok{))}
\CommentTok{\#\textgreater{} Test Stat   P{-}Value }
\CommentTok{\#\textgreater{} 0.8533046 0.0002500}
\end{Highlighting}
\end{Shaded}

\hypertarget{kolmogorov-smirnov-test-1}{%
\subsection{Kolmogorov-Smirnov Test}\label{kolmogorov-smirnov-test-1}}

\begin{itemize}
\tightlist
\item
  Can be used for continuous distribution
\end{itemize}

\(H_0\): Empirical distribution follows a specified distribution

\(H_1\): Empirical distribution does not follow a specified distribution

\begin{itemize}
\tightlist
\item
  Using non-parametric
\end{itemize}

\[
D= \max|P(X) - Q(X)|
\]

\begin{itemize}
\tightlist
\item
  \(D \in [0,1]\) from the densities are evenly distributed to not evenly distributed
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(entropy)}
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{lst }\OtherTok{=} \FunctionTok{list}\NormalTok{(}\AttributeTok{sample\_1 =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{20}\NormalTok{), }\AttributeTok{sample\_2 =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\SpecialCharTok{:}\DecValTok{30}\NormalTok{), }\AttributeTok{sample\_3 =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\SpecialCharTok{:}\DecValTok{30}\NormalTok{))}

\FunctionTok{expand.grid}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(lst), }\DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(lst)) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{rowwise}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{KL =} \FunctionTok{KL.empirical}\NormalTok{(lst[[Var1]], lst[[Var2]]))}
\CommentTok{\#\textgreater{} \# A tibble: 9 x 3}
\CommentTok{\#\textgreater{} \# Rowwise: }
\CommentTok{\#\textgreater{}    Var1  Var2     KL}
\CommentTok{\#\textgreater{}   \textless{}int\textgreater{} \textless{}int\textgreater{}  \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1     1     1 0     }
\CommentTok{\#\textgreater{} 2     2     1 0.150 }
\CommentTok{\#\textgreater{} 3     3     1 0.183 }
\CommentTok{\#\textgreater{} 4     1     2 0.704 }
\CommentTok{\#\textgreater{} 5     2     2 0     }
\CommentTok{\#\textgreater{} 6     3     2 0.0679}
\CommentTok{\#\textgreater{} 7     1     3 0.622 }
\CommentTok{\#\textgreater{} 8     2     3 0.0870}
\CommentTok{\#\textgreater{} 9     3     3 0}
\end{Highlighting}
\end{Shaded}

To use the test for discrete date, use bootstrap version of the KS test (bypass the continuity requirement)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Matching}\SpecialCharTok{::}\FunctionTok{ks.boot}\NormalTok{(}\AttributeTok{Tr =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\SpecialCharTok{:}\DecValTok{10}\NormalTok{), }\AttributeTok{Co =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\SpecialCharTok{:}\DecValTok{10}\NormalTok{))}
\CommentTok{\#\textgreater{} $ks.boot.pvalue}
\CommentTok{\#\textgreater{} [1] 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $ks}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Exact two{-}sample Kolmogorov{-}Smirnov test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  Tr and Co}
\CommentTok{\#\textgreater{} D = 0, p{-}value = 1}
\CommentTok{\#\textgreater{} alternative hypothesis: two{-}sided}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $nboots}
\CommentTok{\#\textgreater{} [1] 1000}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} attr(,"class")}
\CommentTok{\#\textgreater{} [1] "ks.boot"}
\end{Highlighting}
\end{Shaded}

\hypertarget{part-ii.-regression}{%
\part*{II. REGRESSION}\label{part-ii.-regression}}
\addcontentsline{toc}{part}{II. REGRESSION}

\hypertarget{linear-regression}{%
\chapter{Linear Regression}\label{linear-regression}}

\includegraphics[width=4.6875in,height=2.08333in]{images/econometrics.PNG}

\begin{itemize}
\tightlist
\item
  Estimating parameters -\textgreater{} parametric (finite parameters)
\item
  Estimating functions -\textgreater{} non-parametric
\end{itemize}

\textbf{Estimator Desirable Properties}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Unbiased}
\item
  \textbf{Consistency}
\end{enumerate}

\begin{itemize}
\tightlist
\item
  \(plim\hat{\beta_n}=\beta\)
\item
  based on the law of large numbers, we can derive consistency
\item
  More observations means more precise, closer to the true value.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Efficiency}
\end{enumerate}

\begin{itemize}
\item
  Minimum variance in comparison to another estimator.

  \begin{itemize}
  \tightlist
  \item
    OLS is BLUE (best linear unbiased estimator) means that OLS is the most efficient among the class of linear unbiased estimator \protect\hyperlink{gauss-markov-theorem}{Gauss-Markov Theorem}
  \item
    If we have correct distributional assumptions, then the Maximum Likelihood is asymptotically efficient among consistent estimators.
  \end{itemize}
\end{itemize}

\hypertarget{ordinary-least-squares}{%
\section{Ordinary Least Squares}\label{ordinary-least-squares}}

The most fundamental model in statistics or econometric is a OLS linear regression. OLS = Maximum likelihood when the error term is assumed to be normally distributed.

Regression is still great if the underlying CEF (conditional expectation function) is not linear. Because regression has the following properties:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  For \(E[Y_i | X_{1i}, \dots, X_{Ki}] = a + \sum_{k=1}^K b_k X_{ki}\) (i.e., the CEF of \(Y_i\) on \(X_{1i}, \dots, X_{Ki}\) is linear, then the regression of \(Y_i\) on \(X_{1i}, \dots, X_{Ki}\) is the CEF
\item
  For \(E[Y_i | X_{1i} , \dots, X_{Ki}]\) is a nonlinear function of the conditioning variables, the regression of \(Y_i\) on \(X_{1i}, \dots, X_{Ki}\) will give you the best linear approximation to the nonlinear CEF (i.e., minimize the expected squared deviation between the fitted values from the linear model and the CEF).
\end{enumerate}

\hypertarget{simple-regression-basic-model}{%
\subsection{Simple Regression (Basic Model)}\label{simple-regression-basic-model}}

\[
Y_i = \beta_0 + \beta_1 X_i + \epsilon_i
\]

\begin{itemize}
\tightlist
\item
  \(Y_i\): response (dependent) variable at i-th observation
\item
  \(\beta_0,\beta_1\): regression parameters for intercept and slope.
\item
  \(X_i\): known constant (independent or predictor variable) for i-th observation
\item
  \(\epsilon_i\): random error term
\end{itemize}

\[
\begin{aligned}
E(\epsilon_i) &= 0 \\
var(\epsilon_i) &= \sigma^2 \\
cov(\epsilon_i,\epsilon_j) &= 0  \text{ for all } i \neq j
\end{aligned}
\]

\(Y_i\) is random since \(\epsilon_i\) is:

\[
\begin{aligned}
E(Y_i) &= E(\beta_0 + \beta_1 X_i + \epsilon_i) \\
&= E(\beta_0) + E(\beta_1 X_i) + E(\epsilon) \\
&= \beta_0 + \beta_1 X_i
\end{aligned}
\]

\[
\begin{aligned}
var(Y_i) &= var(\beta_0 + \beta_1 X_i + \epsilon_i) \\
&= var(\epsilon_i) \\
&= \sigma^2
\end{aligned}
\]

Since \(cov(\epsilon_i, \epsilon_j) = 0\) (uncorrelated), the outcome in any one trail has no effect on the outcome of any other. Hence, \(Y_i, Y_j\) are uncorrelated as well (conditioned on the \(X\)'s)

\textbf{Note}\\
\protect\hyperlink{ordinary-least-squares}{Least Squares} does not require a distributional assumption

Relationship between bivariate regression and covariance

Covariance between 2 variables:

\[
C(X_i, Y_i) = E[(X_i - E[X_i])(Y_i - E[Y_i])]
\]

Which has the following properties

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(C(X_i, X_i) = \sigma^2_X\)
\item
  If either \(E(X_i) = 0 | E(Y_i) = 0\), then \(Cov(X_i, Y_i) = E[X_i Y_i]\)
\item
  Given \(W_i = a + b X_i\) and \(Z_i = c + d Y_i\), then \(Cov(W_i, Z_i) = bdC(X_i, Y_i)\)
\end{enumerate}

For the bivariate regression, the slope is

\[
\beta = \frac{Cov(Y_i, X_i)}{Var(X_i)}
\]

To extend this to a multivariate case

\[
\beta_k = \frac{C(Y_i, \tilde{X}_{ki})}{Var(\tilde{X}_{ki})}
\]

Where \(\tilde{X}_{ki}\) is the residual from a regression of \(X_{ki}\) on the \(K-1\) other covariates included in the model

And intercept

\[
\alpha = E[Y_i] - \beta E(X_i)
\]

\hypertarget{estimation}{%
\subsubsection{Estimation}\label{estimation}}

Deviation of \(Y_i\) from its expected value:

\[
Y_i - E(Y_i) = Y_i - (\beta_0 + \beta_1 X_i)
\]

Consider the sum of the square of such deviations:

\[
Q = \sum_{i=1}^{n} (Y_i - \beta_0 -\beta_1 X_i)^2
\]

\[
\begin{aligned}
b_1 &= \frac{\sum_{i=1}^{n} (X_i - \bar{X})(Y_i - \bar{Y})}{\sum_{i=1}^{n}(X_i - \bar{X})^2} \\
b_0 &= \frac{1}{n}(\sum_{i=1}^{n}Y_i - b_1\sum_{i=1}^{n}X_i) = \bar{Y} - b_1 \bar{X}
\end{aligned}
\]

\hypertarget{properties-of-least-least-estimators}{%
\subsubsection{Properties of Least Least Estimators}\label{properties-of-least-least-estimators}}

\[
\begin{aligned}
E(b_1) &= \beta_1 \\
E(b_0) &= E(\bar{Y}) - \bar{X}\beta_1 \\
E(\bar{Y}) &= \beta_0 + \beta_1 \bar{X} \\
E(b_0) &= \beta_0 \\
var(b_1) &= \frac{\sigma^2}{\sum_{i=1}^{n}(X_i - \bar{X})^2} \\
var(b_0) &= \sigma^2 (\frac{1}{n} + \frac{\bar{X}^2}{\sum (X_i - \bar{X})^2})
\end{aligned}
\]

\(var(b_1) \to 0\) as more measurements are taken at more \(X_i\) values (unless \(X_i\) is at its mean value)\\
\(var(b_0) \to 0\) as \(n\) increases when the \(X_i\) values are judiciously selected.

\textbf{Mean Square Error}

\[
MSE = \frac{SSE}{n-2} = \frac{\sum_{i=1}^{n}e_i^2}{n-2} = \frac{\sum(Y_i - \hat{Y_i})^2}{n-2}
\]

Unbiased estimator of MSE:

\[
E(MSE) = \sigma^2
\]

\[
\begin{aligned}
s^2(b_1) &= \widehat{var(b_1)} = \frac{MSE}{\sum_{i=1}^{n}(X_i - \bar{X})^2} \\
s^2(b_0) &= \widehat{var(b_0)} = MSE(\frac{1}{n} + \frac{\bar{X}^2}{\sum_{i=1}^{n}(X_i - \bar{X})^2})
\end{aligned}
\]

\[
\begin{aligned}
E(s^2(b_1)) &= var(b_1) \\
E(s^2(b_0)) &= var(b_0)
\end{aligned}
\]

\hypertarget{residuals}{%
\subsubsection{Residuals}\label{residuals}}

\[
e_i = Y_i - \hat{Y} = Y_i - (b_0 + b_1 X_i)
\]

\begin{itemize}
\tightlist
\item
  \(e_i\) is an estimate of \(\epsilon_i = Y_i - E(Y_i)\)
\item
  \(\epsilon_i\) is always unknown since we don't know the true \(\beta_0, \beta_1\)
\end{itemize}

\[
\begin{aligned}
\sum_{i=1}^{n} e_i &= 0 \\
\sum_{i=1}^{n} X_i e_i &= 0
\end{aligned}
\]

Residual properties

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(E[e_i] =0\)
\item
  \(E[X_i e_i] = 0\) and \(E[\hat{Y}_i e_i ] = 0\)
\end{enumerate}

\hypertarget{inference}{%
\subsubsection{Inference}\label{inference}}

\textbf{Normality Assumption}

\begin{itemize}
\tightlist
\item
  Least Squares estimation does not require assumptions of normality.
\item
  However, to do inference on the parameters, we need distributional assumptions.
\item
  Inference on \(\beta_0,\beta_1\) and \(Y_h\) are not extremely sensitive to moderate departures from normality, especially if the sample size is large
\item
  Inference on \(Y_{pred}\) is very sensitive to the normality assumptions.
\end{itemize}

\textbf{Normal Error Regression Model}

\[
Y_i \sim N(\beta_0+\beta_1X_i, \sigma^2)
\]

\hypertarget{beta_1}{%
\paragraph{\texorpdfstring{\(\beta_1\)}{\textbackslash beta\_1}}\label{beta_1}}

Under the normal error model,

\[
b_1 \sim N(\beta_1,\frac{\sigma^2}{\sum_{i=1}^{n}(X_i - \bar{X})^2})
\]

A linear combination of independent normal random variable is normally distributed

Hence,

\[
\frac{b_1 - \beta_1}{s(b_1)} \sim t_{n-2}
\]

A \((1-\alpha) 100 \%\) confidence interval for \(\beta_1\) is

\[
b_1 \pm t_{t-\alpha/2 ; n-2}s(b_1)
\]

\hypertarget{beta_0}{%
\paragraph{\texorpdfstring{\(\beta_0\)}{\textbackslash beta\_0}}\label{beta_0}}

Under the normal error model, the sampling distribution for \(b_0\) is

\[
b_0 \sim N(\beta_0,\sigma^2(\frac{1}{n} + \frac{\bar{X}^2}{\sum_{i=1}^{n}(X_i - \bar{X})^2}))
\]

Hence,

\[
\frac{b_0 - \beta_0}{s(b_0)} \sim t_{n-2}
\] A \((1-\alpha)100 \%\) confidence interval for \(\beta_0\) is

\[
b_0 \pm t_{1-\alpha/2;n-2}s(b_0)
\]

\hypertarget{mean-response}{%
\paragraph{Mean Response}\label{mean-response}}

Let \(X_h\) denote the level of X for which we wish to estimate the mean response

\begin{itemize}
\tightlist
\item
  We denote the mean response when \(X = X_h\) by \(E(Y_h)\)\\
\item
  A point estimator of \(E(Y_h)\) is \(\hat{Y}_h\):
\end{itemize}

\[
\hat{Y}_h = b_0 + b_1 X_h
\] \textbf{Note}

\[
\begin{aligned}
E(\bar{Y}_h) &= E(b_0 + b_1X_h) \\
&= \beta_0 + \beta_1 X_h \\
&= E(Y_h)
\end{aligned}
\] (unbiased estimator)

\[
\begin{aligned}
var(\hat{Y}_h) &= var(b_0 + b_1 X_h) \\
&= var(\hat{Y} + b_1 (X_h - \bar{X})) \\
&= var(\bar{Y}) + (X_h - \bar{X})^2var(b_1) + 2(X_h - \bar{X})cov(\bar{Y},b_1) \\
&= \frac{\sigma^2}{n} + (X_h - \bar{X})^2 \frac{\sigma^2}{\sum(X_i - \bar{X})^2} \\
&= \sigma^2(\frac{1}{n} + \frac{(X_h - \bar{X})^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2})
\end{aligned}
\]

Since \(cov(\bar{Y},b_1) = 0\) due to the iid assumption on \(\epsilon_i\)

An estimate of this variance is

\[
s^2(\hat{Y}_h) = MSE (\frac{1}{n} + \frac{(X_h - \bar{X})^2}{\sum_{i=1}^{n}(X_i - \bar{X})^2})
\]

the sampling distribution for the mean response is

\[
\begin{aligned}
\hat{Y}_h &\sim N(E(Y_h),var(\hat{Y_h})) \\
\frac{\hat{Y}_h - E(Y_h)}{s(\hat{Y}_h)} &\sim t_{n-2}
\end{aligned}
\]

A \(100(1-\alpha) \%\) CI for \(E(Y_h)\) is

\[
\hat{Y}_h \pm t_{1-\alpha/2;n-2}s(\hat{Y}_h)
\]

\hypertarget{prediction-of-a-new-observation}{%
\paragraph{Prediction of a new observation}\label{prediction-of-a-new-observation}}

Regarding the \protect\hyperlink{mean-response}{Mean Response}, we are interested in estimating \textbf{mean} of the distribution of Y given a certain X.

Now, we want to \textbf{predict} an individual outcome for the distribution of Y at a given X. We call \(Y_{pred}\)

Estimation of mean response versus prediction of a new observation:

\begin{itemize}
\item
  the point estimates are the same in both cases: \(\hat{Y}_{pred} = \hat{Y}_h\)
\item
  It is the variance of the prediction that is different; hence, prediction intervals are different than confidence intervals. The prediction variance must consider:

  \begin{itemize}
  \tightlist
  \item
    Variation in the mean of the distribution of \(Y\)
  \item
    Variation within the distribution of \(Y\)
  \end{itemize}
\end{itemize}

We want to predict: mean response + error

\[
\beta_0 + \beta_1 X_h + \epsilon
\]

Since \(E(\epsilon) = 0\), use the least squares predictor:

\[
\hat{Y}_h = b_0 + b_1 X_h
\]

The variance of the predictor is

\[
\begin{aligned}
var(b_0 + b_1 X_h + \epsilon) &= var(b_0 + b_1 X_h) + var(\epsilon) \\
&= \sigma^2(\frac{1}{n} + \frac{(X_h - \bar{X})^2}{\sum_{i=1}^{n}(X_i - \bar{X})^2}) + \sigma^2 \\
&= \sigma^2(1+\frac{1}{n} + \frac{(X_h - \bar{X})^2}{\sum_{i=1}^{n}(X_i - \bar{X})^2})
\end{aligned}
\]

An estimate of the variance is given by

\[
s^2(pred)= MSE (1+ \frac{1}{n} + \frac{(X_h - \bar{X})^2}{\sum_{i=1}^{n}(X_i - \bar{X})^2})
\]

and

\[
\frac{Y_{pred}-\hat{Y}_h}{s(pred)} \sim t_{n-2}
\]

\(100(1-\alpha) \%\) prediction interval is

\[
\bar{Y}_h \pm t_{1-\alpha/2; n-2}s(pred)
\]

The prediction interval is very sensitive to the distributional assumption on the errors, \(\epsilon\)

\hypertarget{confidence-band}{%
\paragraph{Confidence Band}\label{confidence-band}}

We want to know the confidence interval for the entire regression line, so we can draw conclusions about any and all mean response fo the entire regression line \(E(Y) = \beta_0 + \beta_1 X\) rather than for a given response \(Y\)

\textbf{Working-Hotelling Confidence Band}

For a given \(X_h\), this band is

\[
\hat{Y}_h \pm W s(\hat{Y}_h)
\] where \(W^2 = 2F_{1-\alpha;2,n-2}\), which is just 2 times the F-stat with 2 and \(n-2\) degrees of freedom

\begin{itemize}
\tightlist
\item
  the interval width will change with each \(X_h\) (since \(s(\hat{Y}_h)\) changes)\\
\item
  the boundary values for this confidence band will always define a hyperbole containing the regression line\\
\item
  will be smallest at \(X = \bar{X}\)
\end{itemize}

\hypertarget{anova}{%
\subsubsection{ANOVA}\label{anova}}

Partitioning the Total Sum of Squares: Consider the corrected Total sum of squares:

\[
SSTO = \sum_{i=1}^{n} (Y_i -\bar{Y})^2
\]

Measures the overall dispersion in the response variable\\
We use the term corrected because we correct for mean, the uncorrected total sum of squares is given by \(\sum Y_i^2\)

use \(\hat{Y}_i = b_0 + b_1 X_i\) to estimate the conditional mean for Y at \(X_i\)

\[
\begin{aligned}
\sum_{i=1}^n (Y_i - \bar{Y})^2 &= \sum_{i=1}^n (Y_i - \hat{Y}_i + \hat{Y}_i - \bar{Y})^2 \\
&= \sum_{i=1}^n(Y_i - \hat{Y}_i)^2 + \sum_{i=1}^n(\hat{Y}_i - \bar{Y})^2 + 2\sum_{i=1}^n(Y_i - \hat{Y}_i)(\hat{Y}_i-\bar{Y}) \\
&= \sum_{i=1}^n(Y_i - \hat{Y}_i)^2 + \sum_{i=1}^n(\bar{Y}_i -\bar{Y})^2 \\
STTO &= SSE + SSR \\
\end{aligned}
\]

where SSR is the regression sum of squares, which measures how the conditional mean varies about a central value.

The cross-product term in the decomposition is 0:

\[
\begin{aligned}
\sum_{i=1}^n (Y_i - \hat{Y}_i)(\hat{Y}_i - \bar{Y}) &= \sum_{i=1}^{n}(Y_i - \bar{Y} -b_1 (X_i - \bar{X}))(\bar{Y} + b_1 (X_i - \bar{X})-\bar{Y}) \\
&= b_1 \sum_{i=1}^{n} (Y_i - \bar{Y})(X_i - \bar{X}) - b_1^2\sum_{i=1}^{n}(X_i - \bar{X})^2 \\
&= b_1 \frac{\sum_{i=1}^{n}(Y_i -\bar{Y})(X_i - \bar{X})}{\sum_{i=1}^{n}(X_i - \bar{X})^2} \sum_{i=1}^{n}(X_i - \bar{X})^2 - b_1^2\sum_{i=1}^{n}(X_i - \bar{X})^2 \\
&= b_1^2 \sum_{i=1}^{n}(X_i - \bar{X})^2 - b_1^2 \sum_{i=1}^{n}(X_i - \bar{X})^2 \\
&= 0
\end{aligned}
\]

and

\[
\begin{aligned}
SSTO &= SSR + SSE \\
(n-1 d.f) &= (1 d.f.) + (n-2 d.f.)
\end{aligned}
\]

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.3134}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2388}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1045}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2090}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1343}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Source of Variation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Sum of Squares
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
df
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Mean Square
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
F
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Regression (model) & SSR & \(1\) & MSR = SSR/df & MSR/MSE \\
Error & SSE & \(n-2\) & MSE = SSE/df & \\
Total (Corrected) & SSTO & \(n-1\) & & \\
\end{longtable}

\[
\begin{aligned}
E(MSE) &= \sigma^2 \\
E(MSR) &= \sigma^2 + \beta_1^2 \sum_{i=1}^{n} (X_i - \bar{X})^2 
\end{aligned}
\]

\begin{itemize}
\tightlist
\item
  If \(\beta_1 = 0\), then these two expected values are the same\\
\item
  if \(\beta_1 \neq 0\) then E(MSR) will be larger than E(MSE)
\end{itemize}

which means the ratio of these two quantities, we can infer something about \(\beta_1\)

Distribution theory tells us that if \(\epsilon_i \sim iid N(0,\sigma^2)\) and assuming \(H_0: \beta_1 = 0\) is true,

\[
\begin{aligned}
\frac{MSE}{\sigma^2} &\sim \chi_{n-2}^2 \\
\frac{MSR}{\sigma^2} &\sim \chi_{1}^2 \text{ if } \beta_1=0
\end{aligned}
\]

where these two chi-square random variables are independent.

Since the ratio of 2 independent chi-square random variable follows an F distribution, we consider:

\[
F = \frac{MSR}{MSE} \sim F_{1,n-2}
\]

when \(\beta_1 =0\). Thus, we reject \(H_0: \beta_1 = 0\) (or \(E(Y_i)\) = constant) at \(\alpha\) if

\[
F > F_{1 - \alpha;1,n-2}
\]

this is the only null hypothesis that can be tested with this approach.

\textbf{Coefficient of Determination}

\[
R^2 = \frac{SSR}{SSTO} = 1- \frac{SSE}{SSTO}
\]

where \(0 \le R^2 \le 1\)

\textbf{Interpretation}: The proportionate reduction of the total variation in \(Y\) after fitting a linear model in \(X\).

It is not really correct to say that \(R^2\) is the ``variation in \(Y\) explained by \(X\)''.

\(R^2\) is related to the correlation coefficient between \(Y\) and \(X\):

\[
R^2 = (r)^2
\]

where \(r= corr(x,y)\) is an estimate of the Pearson correlation coefficient. Also, note

\[
\begin{aligned}
b_1 &= (\frac{\sum_{i=1}^{n}(Y_i - \bar{Y})^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2})^{1/2} \\
r &= \frac{s_y}{s_x} r
\end{aligned}
\]

\textbf{Lack of Fit}

\(Y_{11},Y_{21}, \dots ,Y_{n_1,1}\): \(n_1\) repeat obs at \(X_1\)

\(Y_{1c},Y_{2c}, \dots ,Y_{n_c,c}\): \(n_c\) repeat obs at \(X_c\)

So, there are \(c\) distinct \(X\) values.

Let \(\bar{Y}_j\) be the mean over replicates for \(X_j\)

Partition the Error Sum of Squares:

\[
\begin{aligned}
\sum_{i} \sum_{j} (Y_{ij} - \hat{Y}_{ij})^2 &= \sum_{i} \sum_{j} (Y_{ij} - \bar{Y}_j + \bar{Y}_j + \hat{Y}_{ij})^2 \\
&=  \sum_{i} \sum_{j} (Y_{ij} - \bar{Y}_j)^2 + \sum_{i} \sum_{j} (\bar{Y}_j - \hat{Y}_{ij})^2 + \text{cross product term} \\
&= \sum_{i} \sum_{j}(Y_{ij} - \bar{Y}_j)^2 + \sum_j n_j (\bar{Y}_j- \hat{Y}_{ij})^2 \\
SSE &= SSPE + SSLF \\
\end{aligned}
\]

\begin{itemize}
\tightlist
\item
  SSPE: ``pure error sum of squares'' has \(n-c\) degrees of freedom since we need to estimate \(c\) means\\
\item
  SSLF: ``lack of fit sum of squares'' has \(c - 2\) degrees of freedom (the number of unique \(X\) values - number of parameters used to specify the conditional mean regression model)
\end{itemize}

\[
\begin{aligned}
MSPE &= \frac{SSPE}{df_{pe}} = \frac{SSPE}{n-c} \\
MSLF &= \frac{SSLF}{df_{lf}} = \frac{SSLF}{c-2}
\end{aligned}
\]

The \textbf{F-test for Lack-of-Fit} tests

\[
\begin{aligned}
H_0: Y_{ij} &= \beta_0 + \beta_1 X_i + \epsilon_{ij}, \epsilon_{ij} \sim iid N(0,\sigma^2) \\
H_a: Y_{ij} &= \alpha_0 + \alpha_1 X_i + f(X_i, Z_1,...) + \epsilon_{ij}^*,\epsilon_{ij}^* \sim iid N(0, \sigma^2)
\end{aligned}
\]

\(E(MSPE) = \sigma^2\) under either \(H_0\), \(H_a\)

\(E(MSLF) = \sigma^2 + \frac{\sum n_j(f(X_i,...))^2}{n-2}\) in general and

\(E(MSLF) = \sigma^2\) when \(H_0\) is true

We reject \(H_0\) (i.e., the model \(Y_i = \beta_0 + \beta_1 X_i + \epsilon_i\) is not adequate) if

\[
F = \frac{MSLF}{MSPE} > F_{1-\alpha;c-2,n-c}
\]

Failing to reject \(H_0\) does not imply that \(H_0: Y_{ij} = \beta_0 + \beta_1 X_i + \epsilon_{ij}\) is exactly true, but it suggests that this model may provide a reasonable approximation to the true model.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.3000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2286}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1857}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1857}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Source of Variation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Sum of Squares
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
df
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Mean Square
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
F
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Regression & SSR & \(1\) & MSR & MSR / MSE \\
Error & SSE & \(n-2\) & MSE & \\
Lack of fit & SSLF & \(c-2\) & MSLF & MSLF / MSPE \\
Pure Error & SSPE & \(n-c\) & MSPE & \\
Total (Corrected) & SSTO & \(n-1\) & & \\
\end{longtable}

Repeat observations have an effect on \(R^2\):

\begin{itemize}
\tightlist
\item
  It is impossible for \(R^2\) to attain 1 when repeat obs. exist (SSE can't be 0)
\item
  The maximum \(R^2\) attainable in this situation:
\end{itemize}

\[
R^2_{max} = \frac{SSTo - SSPE}{SSTO}
\]

\begin{itemize}
\tightlist
\item
  Not all levels of X need have repeat observations.
\item
  Typically, when \(H_0\) is appropriate, one still uses MSE as the estimate for \(\sigma^2\) rather than MSPE, Since MSE has more degrees of freedom, sometimes people will pool these estimates.
\end{itemize}

\textbf{Joint Inference}\\
The confidence coefficient for both \(\beta_0\) and \(\beta_1\) considered simultaneously is \(\le \alpha\)

Let

\begin{itemize}
\tightlist
\item
  \(\bar{A}_1\) be the event that the first interval covers \(\beta_0\)
\item
  \(\bar{A}_2\) be the event that the second interval covers \(\beta_1\)
\end{itemize}

\[
\begin{aligned}
P(\bar{A}_1) &= 1 - \alpha \\
P(\bar{A}_2) &= 1 - \alpha
\end{aligned}
\]

The probability that both \(\bar{A}_1\) and \(\bar{A}_2\)

\[
\begin{aligned}
P(\bar{A}_1 \cap \bar{A}_2) &= 1 - P(\bar{A}_1 \cup \bar{A}_2) \\
&= 1 - P(A_1) - P(A_2) + P(A_1 \cap A_2) \\
&\ge 1 - P(A_1) - P(A_2) \\
&= 1 - 2\alpha
\end{aligned}
\]

If \(\beta_0\) and \(\beta_1\) have separate 95\% confidence intervals, the joint (family) confidence coefficient is at least \(1 - 2(0.05) = 0.9\). This is called a \textbf{Bonferroni Inequality}

We could use a procedure in which we obtained \(1-\alpha/2\) confidence intervals for the two regression parameters separately, then the joint (Bonferroni) family confidence coefficient would be at least \(1- \alpha\)

The \(1-\alpha\) joint Bonferroni confidence interval for \(\beta_0\) and \(\beta_1\) is given by calculating:

\[
\begin{aligned}
b_0 &\pm B s(b_0) \\
b_1 &\pm B s(b_1) 
\end{aligned}
\]

where \(B= t_{1-\alpha/4;n-2}\)

Interpretation: If repeated samples were taken and the joint \((1-\alpha)\) intervals for \(\beta_0\) and \(\beta_1\) were obtained, \((1-\alpha)100\)\% of the joint intervals would contain the true pair \((\beta_0, \beta_1)\). That is, in \(\alpha \times 100\)\% of the samples, one or both intervals would not contain the true value.

\begin{itemize}
\tightlist
\item
  The Bonferroni interval is \textbf{conservative}. It is a lower bound and the joint intervals will tend to be correct more than \((1-\alpha)100\)\% of the time (lower power). People usually consider a larger \(\alpha\) for the Bonferroni joint tests (e.g, \(\alpha=0.1\))
\item
  The Bonferroni procedure extends to testing more than 2 parameters. Say we are interested in testing \(\beta_0,\beta_1,..., \beta_{g-1}\) (g parameters to test). Then, the joint Bonferroni interval is obtained by calculating the \((1-\alpha/g)\) 100\% level interval for each separately.
\item
  For example, if \(\alpha = 0.05\) and \(g=10\), each individual test is done at the \(1- \frac{.05}{10}\) level. For 2-sided intervals, this corresponds to using \(t_{1-\frac{0.05}{2(10)};n-p}\) in the CI formula. This procedure works best if g is relatively small, otherwise the intervals for each individual parameter are very wide and the test is way too conservative.
\item
  \(b_0,b_1\) are usually correlated (negatively if \(\bar{X} >0\) and positively if \(\bar{X}<0\))
\item
  Other multiple comparison procedures are available.
\end{itemize}

\hypertarget{assumptions}{%
\subsubsection{Assumptions}\label{assumptions}}

\begin{itemize}
\tightlist
\item
  Linearity of the regression function
\item
  Error terms have constant variance
\item
  Error terms are independent
\item
  No outliers
\item
  Error terms are normally distributed
\item
  No Omitted variables
\end{itemize}

\hypertarget{diagnostics}{%
\subsubsection{Diagnostics}\label{diagnostics}}

\begin{itemize}
\item
  Constant Variance

  \begin{itemize}
  \tightlist
  \item
    Plot residuals vs.~X
  \end{itemize}
\item
  Outliers

  \begin{itemize}
  \item
    plot residuals vs.~X
  \item
    box plots
  \item
    stem-leaf plots
  \item
    scatter plots
  \end{itemize}
\end{itemize}

We could use standardize the residuals to have unit variance. These standardized residuals are called studentized residuals:

\[
r_i = \frac{e_i -\bar{e}}{s(e_i)} = \frac{e_i}{s(e_i)}
\]

A simplified standardization procedure gives semi-studentized residuals:

\[
e_i^* = \frac{e_i - \bar{e}}{\sqrt{MSE}} = \frac{e_i}{\sqrt{MSE}}
\]

\textbf{Non-independent of Error Terms}

\begin{itemize}
\tightlist
\item
  plot residuals vs.~time
\end{itemize}

Residuals \(e_i\) are not independent random variables because they involve the fitted values \(\hat{Y}_i\), which are based on the same fitted regression function.

If the sample size is large, the dependency among \(e_i\) is relatively unimportant.

To detect non-independence, it helps to plot the residual for the \(i\)-th response vs.~the \((i-1)\)-th

\textbf{Non-normality of Error Terms}

to detect non-normality (distribution plots of residuals, box plots of residuals, stem-leaf plots of residuals, normal probability plots of residuals)

\begin{itemize}
\tightlist
\item
  Need relatively large sample sizes.
\item
  Other types of departure affect the distribution of the residuals (wrong regression function, non-constant error variance,\ldots)
\end{itemize}

\hypertarget{objective-tests-of-model-assumptions}{%
\paragraph{Objective Tests of Model Assumptions}\label{objective-tests-of-model-assumptions}}

\begin{itemize}
\item
  Normality

  \begin{itemize}
  \tightlist
  \item
    Use \protect\hyperlink{methods-based-on-empirical-cumulative-distribution-function}{Methods based on empirical cumulative distribution function} to test on residuals.
  \end{itemize}
\item
  Constancy of error variance

  \begin{itemize}
  \tightlist
  \item
    \protect\hyperlink{brown-forsythe-test-modified-levene-test}{Brown-Forsythe Test (Modified Levene Test)}
  \item
    \protect\hyperlink{breusch-pagan-test-cook-weisberg-test}{Breusch-Pagan Test (Cook-Weisberg Test)}
  \end{itemize}
\end{itemize}

\hypertarget{remedial-measures}{%
\subsubsection{Remedial Measures}\label{remedial-measures}}

If the simple linear regression is not appropriate, one can:

\begin{itemize}
\tightlist
\item
  more complicated models
\item
  transformations on \(X\) and/or \(Y\) (may not be ``optimal'' results)
\end{itemize}

Remedial measures based on deviations:

\begin{itemize}
\item
  Non-linearity:

  \begin{itemize}
  \tightlist
  \item
    \href{Non-normality\%20often\%20occurs\%20with\%20non-constant\%20error\%20variances;\%20need\%20to\%20transform\%20to\%20constant\%20error\%20variance\%20first,\%20then\%20check\%20normality.}{Transformations}
  \item
    more complicated models
  \end{itemize}
\item
  Non-constant error variance:

  \begin{itemize}
  \tightlist
  \item
    \protect\hyperlink{weighted-least-squares}{Weighted Least Squares}
  \item
    \href{Non-normality\%20often\%20occurs\%20with\%20non-constant\%20error\%20variances;\%20need\%20to\%20transform\%20to\%20constant\%20error\%20variance\%20first,\%20then\%20check\%20normality.}{Transformations}
  \end{itemize}
\item
  Correlated errors:

  \begin{itemize}
  \tightlist
  \item
    serially correlated error models (times series)
  \end{itemize}
\item
  Non-normality
\item
  Additional variables: multiple regression.
\item
  Outliers:

  \begin{itemize}
  \tightlist
  \item
    Robust estimation.
  \end{itemize}
\end{itemize}

\hypertarget{transformations}{%
\paragraph{Transformations}\label{transformations}}

use transformations of one or both variables before performing the regression analysis.\\
The properties of least-squares estimates apply to the transformed regression, not the original variable.

If we transform the Y variable and perform regression to get:

\[
g(Y_i) = b_0 + b_1 X_i
\]

Transform back:

\[
\hat{Y}_i = g^{-1}(b_0 + b_1 X_i)
\]

\(\hat{Y}_i\) will be biased. we can correct this bias.

\textbf{Box-Cox Family Transformations}

\[
Y'= Y^{\lambda}
\]

where \(\lambda\) is a parameter to be determined from the data.

\begin{longtable}[]{@{}cc@{}}
\toprule\noalign{}
\(\lambda\) & \(Y'\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
2 & \(Y^2\) \\
0.5 & \(\sqrt{Y}\) \\
0 & \(ln(Y)\) \\
-0.5 & \(1/\sqrt{Y}\) \\
-1 & \(1/Y\) \\
\end{longtable}

To pick \(\lambda\), we can do estimation by:

\begin{itemize}
\tightlist
\item
  trial and error
\item
  maximum likelihood
\item
  numerical search
\end{itemize}

\textbf{Variance Stabilizing Transformations}

A general method for finding a variance stabilizing transformation, when the standard deviation is a function of the mean, is the \textbf{delta method} - an application of a Taylor series expansion.

\[
\sigma = \sqrt{var(Y)} = f(\mu)
\]

where \(\mu = E(Y)\) and \(f(\mu)\) is some smooth function of the mean.

Consider the transformation \(h(Y)\). Expand this function in a Taylor series about \(\mu\). Then,

\[
h(Y) = h(\mu) + h'(\mu)(Y-\mu) + \text{small terms}
\]

we want to select the function h(.) so that the variance of h(Y) is nearly constant for all values of \(\mu= E(Y)\):

\[
\begin{aligned}
const &= var(h(Y)) \\
&= var(h(\mu) + h'(\mu)(Y-\mu)) \\
&= (h'(\mu))^2 var(Y-\mu) \\
&= (h'(\mu))^2 var(Y) \\
&= (h'(\mu))^2(f(\mu))^2 \\
\end{aligned}
\]

we must have,

\[
h'(\mu) \propto \frac{1}{f(\mu)}
\]

then,

\[
h(\mu) = \int\frac{1}{f(\mu)}d\mu
\]

Example: For the Poisson distribution: \(\sigma^2 = var(Y) = E(Y) = \mu\)

Then,

\[
\begin{aligned}
\sigma = f(\mu) &= \sqrt{mu} \\
h'(\mu) &\propto \frac{1}{\mu} = \mu^{-.5}
\end{aligned}
\]

Then, the variance stabilizing transformation is:

\[
h(\mu) = \int \mu^{-.5} d\mu = \frac{1}{2} \sqrt{\mu}
\]

hence, \(\sqrt{Y}\) is used as the variance stabilizing transformation.

If we don't know \(f(\mu)\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Trial and error. Look at residuals plots
\item
  Ask researchers about previous studies or find published results on similar experiments and determine what transformation was used.
\item
  If you have multiple observations \(Y_{ij}\) at the same X values, compute \(\bar{Y}_i\) and \(s_i\) and plot them\\
  If \(s_i \propto \bar{Y}_i^{\lambda}\) then consider \(s_i = a \bar{Y}_i^{\lambda}\) or \(ln(s_i) = ln(a) + \lambda ln(\bar{Y}_i)\). So regression the natural log of \(s_i\) on the natural log of \(\bar{Y}_i\) gives \(\hat{a}\) and \(\hat{\lambda}\) and suggests the form of \(f(\mu)\) If we don't have multiple obs, might still be able to ``group'' the observations to get \(\bar{Y}_i\) and \(s_i\).
\end{enumerate}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3750}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3750}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Transformation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Situation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Comments
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(\sqrt{Y}\) & \(var(\epsilon_i) = k E(Y_i)\) & counts from Poisson dist \\
\(\  sqrt{Y} + \sqrt{Y+1}\) & \(var(\epsilon_i) = k E(Y_i)\) & small counts or zeroes \\
\(log(Y)\) & \(var(\epsilon_i) = k (E(Y_i))^2\) & positive integers with wide range \\
\(log(Y+1)\) & \(var(\epsilon_i) = k(E(Y_i))^2\) & some counts zero \\
1/Y & \(var(\epsilon_i) = k(E(Y_i))^4\) & most responses near zero, others large \\
\(arcsin(\sqrt{Y})\) & \(var(\epsilon_i) = k E(Y_i)(1-E(Y_i))\) & data are binomial proportions or \% \\
\end{longtable}

\hypertarget{multiple-linear-regression}{%
\subsection{Multiple Linear Regression}\label{multiple-linear-regression}}

Geometry of Least Squares

\[
\begin{aligned}
\mathbf{\hat{y}} &= \mathbf{Xb} \\
&= \mathbf{X(X'X)^{-1}X'y} \\
&= \mathbf{Hy}
\end{aligned}
\]

sometimes \(\mathbf{H}\) is denoted as \(\mathbf{P}\).

\(\mathbf{H}\) is the projection operator.\\
\[
\mathbf{\hat{y}= Hy}
\]

is the projection of y onto the linear space spanned by the columns of \(\mathbf{X}\) (model space). The dimension of the model space is the rank of \(\mathbf{X}\).

Facts:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(\mathbf{H}\) is symmetric (i.e., \(\mathbf{H} = \mathbf{H}'\))\\
\item
  \(\mathbf{HH} = \mathbf{H}\)
\end{enumerate}

\[
\begin{aligned}
\mathbf{HH} &= \mathbf{X(X'X)^{-1}X'X(X'X)^{-1}X'} \\
&= \mathbf{X(X'X)^{-1}IX'} \\
&= \mathbf{X(X'X)^{-1}X'}
\end{aligned}
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \(\mathbf{H}\) is an \(n \times n\) matrix with \(rank(\mathbf{H}) = rank(\mathbf{X})\)\\
\item
  \(\mathbf{(I-H) = I - X(X'X)^{-1}X'}\) is also a projection operator. It projects onto the \(n - k\) dimensional space that is orthogonal to the \(k\) dimensional space spanned by the columns of \(\mathbf{X}\)
\item
  \(\mathbf{H(I-H)=(I-H)H = 0}\)
\end{enumerate}

Partition of uncorrected total sum of squares:

\[
\begin{aligned}
\mathbf{y'y} &= \mathbf{\hat{y}'\hat{y} + e'e} \\
&= \mathbf{(Hy)'(Hy) + ((I-H)y)'((I-H)y)} \\
&= \mathbf{y'H'Hy + y'(I-H)'(I-H)y} \\
&= \mathbf{y'Hy + y'(I-H)y} \\
\end{aligned}
\]

or partition for the corrected total sum of squares:

\[
\mathbf{y'(I-H_1)y = y'(H-H_1)y + y'(I-H)y}
\]

where \(H_1 = \frac{1}{n} J = 1'(1'1)1\)

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Source
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SS
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
df
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MS
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
F
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Regression & \(SSR = \mathbf{y' (H-\frac{1}{n}J)y}\) & \(p - 1\) & \(SSR/(p-1)\) & \(MSR /MSE\) \\
Error & \(SSE = \mathbf{y'(I - H)y}\) & \(n - p\) & \(SSE /(n-p)\) & \\
Total & \(\mathbf{y'y - y'Jy/n}\) & \(n -1\) & & \\
\end{longtable}

Equivalently, we can express

\[
\mathbf{Y = X\hat{\beta} + (Y - X\hat{\beta})}
\]

where

\begin{itemize}
\tightlist
\item
  \(\mathbf{\hat{Y} = X \hat{\beta}}\) = sum of a vector of fitted values
\item
  \(\mathbf{e = ( Y - X \hat{\beta})}\) = residual
\item
  \(\mathbf{Y}\) is the \(n \times 1\) vector in a n-dimensional space \(R^n\)
\item
  \(\mathbf{X}\) is an \(n \times p\) full rank matrix. and its columns generate a \(p\)-dimensional subspace of \(R^n\). Hence, any estimator \(\mathbf{X \hat{\beta}}\) is also in this subspace.
\end{itemize}

We choose least squares estimator that minimize the distance between \(\mathbf{Y}\) and \(\mathbf{X \hat{\beta}}\), which is the \textbf{orthogonal projection} of \(\mathbf{Y}\) onto \(\mathbf{X\beta}\).

\[
\begin{aligned}
||\mathbf{Y} - \mathbf{X}\hat{\beta}||^2 &= \mathbf{||Y - X\hat{\beta}||}^2 + \mathbf{||X \hat{\beta}||}^2 \\
&= \mathbf{(Y - X\hat{\beta})'(Y - X\hat{\beta}) +(X \hat{\beta})'(X \hat{\beta})} \\
&= \mathbf{(Y - X\hat{\beta})'Y - (Y - X\hat{\beta})'X\hat{\beta} + \hat{\beta}' X'X\hat{\beta}} \\
&= \mathbf{(Y-X\hat{\beta})'Y + \hat{\beta}'X'X(XX')^{-1}X'Y} \\
&= \mathbf{Y'Y - \hat{\beta}'X'Y + \hat{\beta}'X'Y}
\end{aligned}
\]

where the norm of a \((p \times 1)\) vector \(\mathbf{a}\) is defined by:

\[
\mathbf{||a|| = \sqrt{a'a}} = \sqrt{\sum_{i=1}^p a^2_i}
\]

Coefficient of Multiple Determination

\[
 R^2 = \frac{SSR}{SSTO}= 1- \frac{SSE}{SSTO}
\]

Adjusted Coefficient of Multiple Determination

\[
R^2_a = 1 - \frac{SSE/(n-p)}{SSTO/(n-1)} = 1 - \frac{(n-1)SSE}{(n-p)SSTO}
\]

Sequential and Partial Sums of Squares:

In a regression model with coefficients \(\beta = (\beta_0, \beta_1,...,\beta_{p-1})'\), we denote the uncorrected and corrected SS by

\[
\begin{aligned}
SSM &= SS(\beta_0, \beta_1,...,\beta_{p-1}) \\
SSM_m &= SS(\beta_0, \beta_1,...,\beta_{p-1}|\beta_0)
\end{aligned}
\]

There are 2 decompositions of \(SSM_m\):

\begin{itemize}
\tightlist
\item
  \textbf{Sequential SS}: (not unique -depends on order, also referred to as Type I SS, and is the default of \texttt{anova()} in R)\\
  \[
  SSM_m = SS(\beta_1 | \beta_0) + SS(\beta_2 | \beta_0, \beta_1) + ...+ SS(\beta_{p-1}| \beta_0,...,\beta_{p-2})
  \]
\item
  \textbf{Partial SS}: (use more in practice - contribution of each given all of the others)
\end{itemize}

\[
SSM_m = SS(\beta_1 | \beta_0,\beta_2,...,\beta_{p-1}) + ... + SS(\beta_{p-1}| \beta_0, \beta_1,...,\beta_{p-2})
\]

\hypertarget{ols-assumptions}{%
\subsection{OLS Assumptions}\label{ols-assumptions}}

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{a1-linearity}{A1 Linearity}
\item
  \protect\hyperlink{a2-full-rank}{A2 Full rank}
\item
  \protect\hyperlink{a3-exogeneity-of-independent-variables}{A3 Exogeneity of Independent Variables}
\item
  \protect\hyperlink{a4-homoskedasticity}{A4 Homoskedasticity}
\item
  \protect\hyperlink{a5-data-generation-random-sampling}{A5 Data Generation (random Sampling)}
\item
  \protect\hyperlink{a6-normal-distribution}{A6 Normal Distribution}
\end{itemize}

\hypertarget{a1-linearity}{%
\subsubsection{A1 Linearity}\label{a1-linearity}}

\begin{equation}
A1: y=\mathbf{x}\beta + \epsilon
\label{eq:A1}
\end{equation}

Not restrictive

\begin{itemize}
\tightlist
\item
  \(x\) can be nonlinear transformation including interactions, natural log, quadratic
\end{itemize}

With A3 (Exogeneity of Independent), linearity can be restrictive

\hypertarget{log-model}{%
\paragraph{Log Model}\label{log-model}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2466}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2466}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2466}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2603}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Model
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Form
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Interpretation of \(\beta\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
In words
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Level-Level & \(y =\beta_0+\beta_1x+\epsilon\) & \(\Delta y = \beta_1 \Delta x\) & A unit change in \(x\) will result in \(\beta_1\) unit change in \(y\) \\
Log-Level & \(ln(y) = \beta_0 + \beta_1x + \epsilon\) & \(\% \Delta y=100 \beta_1 \Delta x\) & A unit change in \(x\) result in 100 \(\beta_1\) \% change in \(y\) \\
Level-Log & \(y = \beta _0 + \beta_1 ln (x) + \epsilon\) & \(\Delta y = (\beta_1/ 100)\%\Delta x\) & One percent change in \(x\) result in \(\beta_1/100\) units change in \(y\) \\
Log-Log & \(ln(y) = \beta_0 + \beta_1 l n(x) +\epsilon\) & \(\% \Delta y= \beta _1 \% \Delta x\) & One percent change in \(x\) result in \(\beta_1\) percent change in \(y\) \\
\end{longtable}

\hypertarget{higher-orders}{%
\paragraph{Higher Orders}\label{higher-orders}}

\(y=\beta_0 + x_1\beta_1 + x_1^2\beta_2 + \epsilon\)

\[
\frac{\partial y}{\partial x_1}=\beta_1 + 2x_1\beta_2
\]

\begin{itemize}
\tightlist
\item
  The effect of \(x_1\) on y depends on the level of \(x_1\)
\item
  The partial effect at the average = \(\beta_1+2E(x_1)\beta_2\)
\item
  Average Partial Effect = \(E(\beta_1 + 2x_1\beta_2)\)
\end{itemize}

\hypertarget{interactions}{%
\paragraph{Interactions}\label{interactions}}

\(y=\beta_0 + x_1\beta_1 + x_2\beta_2 + x_1x_2\beta_3 + \epsilon\)

\begin{itemize}
\tightlist
\item
  \(\beta_1\) is the average effect on y for a unit change in \(x_1\) when \(x_2=0\)
\item
  \(\beta_1 + x_2\beta_3\) is the partial effect of \(x_1\) on y which depends on the level of \(x_2\)
\end{itemize}

\hypertarget{a2-full-rank}{%
\subsubsection{A2 Full rank}\label{a2-full-rank}}

\begin{equation}
A2: rank(E(x'x))=k
\label{eq:A2}
\end{equation}

also known as \textbf{identification condition}

\begin{itemize}
\tightlist
\item
  columns of \(\mathbf{x}\) cannot be written as a linear function of the other columns
\item
  which ensures that each parameter is unique and exists in the population regression equation
\end{itemize}

\hypertarget{a3-exogeneity-of-independent-variables}{%
\subsubsection{A3 Exogeneity of Independent Variables}\label{a3-exogeneity-of-independent-variables}}

\begin{equation}
A3: E[\epsilon|x_1,x_2,...,x_k]=E[\epsilon|\mathbf{x}]=0
\label{eq:A3}
\end{equation}

\textbf{strict exogeneity}

\begin{itemize}
\tightlist
\item
  also known as \textbf{mean independence} check back on \protect\hyperlink{correlation-and-independence}{Correlation and Independence}
\item
  by the \protect\hyperlink{law-of-iterated-expectations}{Law of Iterated Expectations} \(E(\epsilon)=0\), which can be satisfied by always including an intercept.
\item
  independent variables do not carry information for prediction of \(\epsilon\)
\item
  A3 implies \(E(y|x)=x\beta\), which means the conditional mean function must be a linear function of \(x\) \protect\hyperlink{a1-linearity}{A1 Linearity}
\end{itemize}

\hypertarget{a3a}{%
\paragraph{A3a}\label{a3a}}

Weaker Exogeneity Assumption

\textbf{Exogeneity of Independent variables}

A3a: \(E(\mathbf{x_i'}\epsilon_i)=0\)

\begin{itemize}
\item
  \(x_i\) is \textbf{uncorrelated} with \(\epsilon_i\) \protect\hyperlink{correlation-and-independence}{Correlation and Independence}
\item
  Weaker than \textbf{mean independence} A3

  \begin{itemize}
  \tightlist
  \item
    A3 implies A3a, not the reverse
  \item
    No causality interpretations
  \item
    Cannot test the difference
  \end{itemize}
\end{itemize}

\hypertarget{a4-homoskedasticity}{%
\subsubsection{A4 Homoskedasticity}\label{a4-homoskedasticity}}

\begin{equation}
A4: Var(\epsilon|x)=Var(\epsilon)=\sigma^2
\label{eq:A4}
\end{equation}

\begin{itemize}
\tightlist
\item
  Variation in the disturbance to be the same over the independent variables
\end{itemize}

\hypertarget{a5-data-generation-random-sampling}{%
\subsubsection{A5 Data Generation (random Sampling)}\label{a5-data-generation-random-sampling}}

\begin{equation}
A5: {y_i,x_{i1},...,x_{ik-1}: i = 1,..., n}
\label{eq:A5}
\end{equation}

is a random sample

\begin{itemize}
\item
  random sample mean samples are independent and identically distributed (iid) from a joint distribution of \((y,\mathbf{x})\)
\item
  with \protect\hyperlink{a3-exogeneity-of-independent-variables}{A3} and \protect\hyperlink{a4-homoskedasticity}{A4}, we have

  \begin{itemize}
  \tightlist
  \item
    \textbf{Strict Exogeneity}: \(E(\epsilon_i|x_1,...,x_n)=0\). independent variables do not carry information for prediction of \(\epsilon\)
  \item
    \textbf{Non-autocorrelation}: \(E(\epsilon_i\epsilon_j|x_1,...,x_n)=0\) The error term is uncorrelated across the draws conditional on the independent variables \(\rightarrow\) \(A4: Var(\epsilon|\mathbf{X})=Var(\epsilon)=\sigma^2I_n\)
  \end{itemize}
\item
  In times series and spatial settings, A5 is less likely to hold.
\end{itemize}

\hypertarget{a5a}{%
\paragraph{A5a}\label{a5a}}

A stochastic process \(\{x_t\}_{t=1}^T\) is \textbf{stationary} if for every collection fo time indices \(\{t_1,t_2,...,t_m\}\), the joint distribution of

\[
x_{t_1},x_{t_2},...,x_{t_m}
\]

is the same as the joint distribution of

\[
x_{t_1+h},x_{t_2+h},...,x_{t_m+h}
\]

for any \(h \ge 1\)

\begin{itemize}
\tightlist
\item
  The joint distribution for the first ten observation is the same for the next ten, etc.
\item
  Independent draws automatically satisfies this
\end{itemize}

A stochastic process \(\{x_t\}_{t=1}^T\) is \textbf{weakly stationary} if \(x_t\) and \(x_{t+h}\) are ``almost independent'' as h increases without bounds.

\begin{itemize}
\tightlist
\item
  two observation that are very far apart should be ``almost independent''
\end{itemize}

Common Weakly Dependent Processes

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Moving Average process of order 1 (MA(1))
\end{enumerate}

MA(1) means that there is only one period lag.

\[
\begin{aligned}
y_t &= u_t + \alpha_1 u_{t-1} \\
E(y_t) &= E(u_t) + \alpha_1E(u_{t-1}) = 0 \\
Var(y_t) &= var(u_t) + \alpha_1 var(u_{t-1}) \\
&= \sigma^2 + \alpha_1^2 \sigma^2 \\
&= \sigma^2(1+\alpha_1^2)
\end{aligned}
\]

where \(u_t\) is drawn iid over t with variance \(\sigma^2\)

An increase in the absolute value of \(\alpha_1\) increases the variance

When the MA(1) process can be \textbf{inverted} (\(|\alpha|<1\) then

\[
u_t = y_t - \alpha_1u_{t-1}
\]

called the autoregressive representation (express current observation in term of past observation).

We can expand it to more than 1 lag, then we have MA(q) process

\[
y_t = u_t + \alpha_1 u_{t-1} + ... + \alpha_q u_{t-q}
\]

where \(u_t \sim WN(0,\sigma^2)\)

\begin{itemize}
\tightlist
\item
  Covariance stationary: irrespective of the value of the parameters.
\item
  Invertibility when \(\alpha < 1\)
\item
  The conditional mean of MA(q) depends on the q lags (long-term memory).
\item
  In MA(q), all autocorrelations beyond q are 0.
\end{itemize}

\[
\begin{aligned}
Cov(y_t,y_{t-1}) &= Cov(u_t + \alpha_1 u_{t-1},u_{t-1}+\alpha_1u_{t-2}) \\
&= \alpha_1var(u_{t-1}) \\
&= \alpha_1\sigma^2
\end{aligned}
\]

\[
\begin{aligned}
Cov(y_t,y_{t-2}) &= Cov(u_t + \alpha_1 u_{t-1},u_{t-2}+\alpha_{1}u_{t-3}) \\
&= 0
\end{aligned}
\]

An MA models a linear relationship between the dependent variable and the current and past values of a stochastic term.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Auto regressive process of order 1 (AR(1))
\end{enumerate}

\[
y_t = \rho y_{t-1}+ u_t, |\rho|<1
\]

where \(u_t\) is drawn iid over t with variance \(\sigma^2\)

\[
\begin{aligned}
Cov(y_t,y_{t-1}) &= Cov(\rho y_{t-1} + u-t,y_{t-1}) \\
&= \rho Var(y_{t-1}) \\
&= \rho \frac{\sigma^2}{1-\rho^2}
\end{aligned}
\]

\[
\begin{aligned}
Cov(y_t,y_{t-h}) &= \rho^h \frac{\sigma^2}{1-\rho^2}
\end{aligned}
\]

Stationarity: in the continuum of t, the distribution of each t is the same

\[
\begin{aligned}
E(y_t) &= E(y_{t-1}) = ...= E(y_0) \\
y_1 &= \rho y_0 + u_1
\end{aligned}
\]

where the initial observation \(y_0=0\)

Assume \(E(y_t)=0\)

\[
\begin{aligned}
y_t &= \rho^t y_{t-t} + \rho^{t-1}u_1 + \rho^{t-2}u_2 +...+ \rho u_{t-1} + u_t \\
&= \rho^t y_0 + \rho^{t-1}u_1 + \rho^{t-2}u_2 +...+ \rho u_{t-1} + u_t
\end{aligned}
\]

Hence, \(y_t\) is the weighted of all of the \(u_t\) time observations before. y will be correlated with all the previous observations as well as future observations.

\[
\begin{aligned}
Var(y_t) &= Var(\rho y_{t-1} + u_t) \\
&= \rho^2 Var(y_{t-1}) + Var(u_t) + 2\rho Cov(y_{t-1}u_t) \\
&= \rho^2 Var(y_{t-1}) + \sigma^2
\end{aligned}
\]

Hence,

\[
Var(y_t) = \frac{\sigma^2}{1-\rho^2}
\]

to have Variance constantly over time, then \(\rho \neq 1\) or \(-1\).

\textbf{Then} stationarity requires \(\rho \neq 1\) or -1. weakly dependent process \(|\rho|<1\)

To estimate the AR(1) process, we use \textbf{Yule-Walker Equation}

\[
\begin{aligned}
y_t &= \epsilon_t + \phi y_{t-1} \\
y_t y_{t-\tau} &= \epsilon_t y_{t-\tau} + \phi y_{t-1}y_{t-\tau} 
\end{aligned}
\]

For \(\tau \ge 1\), we have

\[
\gamma \tau = \phi \gamma (\tau -1)
\]

\[
\rho_t = \phi^t
\]

when you generalize to \(p\)-th order autoregressive process, AR(p):

\[
y_t = \phi_1 y_{t-1} + \phi_2 y_{t-2} + ... + \phi_p y_{t-p} + \epsilon_t
\]

AR(p) process is \textbf{covariance stationary}, and decay in autocorrelations.

When we combine MA(q) and AR(p), we have ARMA(p,q) process, where you can see seasonality. For example, ARMA(1,1)

\[
y_t = \phi y_{t-1} + \epsilon_t + \alpha \epsilon_{t-1}
\]

Random Walk process

\[
y_t = y_0 + \sum_{s=1}^{t}u_t
\]

\begin{itemize}
\tightlist
\item
  not stationary : when \(y_0 = 0\) then \(E(y_t)= 0\), but \(Var(y_t)=t\sigma^2\). Further along in the spectrum, the variance will be larger
\item
  not weakly dependent: \(Cov(\sum_{s=1}^{t}u_s,\sum_{s=1}^{t-h}u_s) = (t-h)\sigma^2\). So the covariance (fixed) is not diminishing as h increases
\end{itemize}

Assumption \protect\hyperlink{a5a}{A5a}: \(\{y_t,x_{t1},..,x_{tk-1} \}\)

where \(t=1,...,T\) are \textbf{stationary and weakly dependent processes}.

Alternative \protect\hyperlink{weak-law}{Weak Law}, \protect\hyperlink{central-limit-theorem}{Central Limit Theorem}\\
If \(z_t\) is a weakly dependent stationary process with a finite first absolute moment and \(E(z_t) = \mu\), then

\[
T^{-1}\sum_{t=1}^{T}z_t \to^p \mu
\]

If additional regulatory conditions hold \citep{greene1990gamma}, then

\[
\sqrt{T}(\bar{z}-\mu) \to^d N(0,B)
\]

where \(B= Var(z_t) + 2\sum_{h=1}^{\infty}Cov(z_t,z_{t-h})\)

\hypertarget{a6-normal-distribution}{%
\subsubsection{A6 Normal Distribution}\label{a6-normal-distribution}}

\begin{equation}
A6: \epsilon|\mathbf{x}\sim N(0,\sigma^2I_n)
\label{eq:A6}
\end{equation}

The error term is normally distributed

From A1-A3, we have \textbf{identification} (also known as \textbf{Orthogonality Condition}) of the population parameter \(\beta\)

\[
\begin{aligned}
y &= {x}\beta + \epsilon && \text{A1} \\
x'y &= x'x\beta + x'\epsilon && \text{} \\
E(x'y) &= E(x'x)\beta + E(x'\epsilon)  && \text{} \\
E(x'y) &= E(x'x)\beta && \text{A3} \\
[E(x'x)]^{-1}E(x'y) &= [E(x'x)]^{-1}E(x'x)\beta && \text{A2} \\
[E(x'x)]^{-1}E(x'y) &= \beta
\end{aligned}
\]

\(\beta\) is the row vector of parameters that produces the best predictor of y we choose the min of \(\gamma\) :

\[
\underset{\gamma}{\operatorname{argmin}}E((y-x\gamma)^2)
\]

First Order Condition

\[
\begin{aligned}
\frac{\partial((y-x\gamma)^2)}{\partial\gamma}&=0 \\
-2E(x'(y-x\gamma))&=0 \\
E(x'y)-E(x'x\gamma) &=0 \\
E(x'y) &= E(x'x)\gamma \\
(E(x'x))^{-1}E(x'y) &= \gamma
\end{aligned}
\]

Second Order Condition

\[
\begin{aligned}
\frac{\partial^2E((y-x\gamma)^2)}{\partial \gamma'^2}&=0 \\
E(\frac{\partial(y-x\partial)^2)}{\partial\gamma\partial\gamma'}) &= 2E(x'x)
\end{aligned}
\]

If \protect\hyperlink{a3-exogeneity-of-independent-variables}{A3} holds, then \(2E(x'x)\) is PSD \(\rightarrow\) minimum

\hypertarget{theorems}{%
\subsection{Theorems}\label{theorems}}

\hypertarget{frisch-waugh-lovell-theorem}{%
\subsubsection{Frisch-Waugh-Lovell Theorem}\label{frisch-waugh-lovell-theorem}}

\[
\mathbf{y=X\beta + \epsilon=X_1\beta_1+X_2\beta_2 +\epsilon}
\]

Equivalently,

\[
\left(
\begin{array}
{cc}
X_1'X_1 & X_1'X_2 \\
X_2'X_1 & X_2'X_2
\end{array}
\right)
\left(
\begin{array}
{c}
\hat{\beta_1} \\
\hat{\beta_2}
\end{array}
\right)
=
\left(
\begin{array}{c}
X_1'y \\
X_2'y
\end{array}
\right)
\]

Hence,

\[
\mathbf{\hat{\beta_1}=(X_1'X_1)^{-1}X_1'y - (X_1'X_1)^{-1}X_1'X_2\hat{\beta_2}}
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Betas from the multiple regression are not the same as the betas from each of the individual simple regression
\item
  Different set of X will affect all the coefficient estimates.
\item
  If \(X_1'X_2 = 0\) or \(\hat{\beta_2}=0\), then 1 and 2 do not hold.
\end{enumerate}

\hypertarget{gauss-markov-theorem}{%
\subsubsection{Gauss-Markov Theorem}\label{gauss-markov-theorem}}

For a linear regression model

\[
\mathbf{y=X\beta + \epsilon}
\]

Under \protect\hyperlink{a1-linearity}{A1}, \protect\hyperlink{a2-full-rank}{A2}, \protect\hyperlink{a3-exogeneity-of-independent-variables}{A3}, \protect\hyperlink{a4-homoskedasticity}{A4}, OLS estimator defined as

\[
\hat{\beta} = \mathbf{(X'X)^{-1}X'y}
\]

is the minimum variance linear (in \(y\)) unbiased estimator of \(\beta\)

Let \(\tilde{\beta}=\mathbf{Cy}\), be another linear estimator where \(\mathbf{C}\) is \(k \times n\) and only function of \(\mathbf{X}\)), then for it be unbiased,

\[
\begin{aligned}
E(\tilde{\beta}|\mathbf{X}) &= E(\mathbf{Cy|X}) \\
&= E(\mathbf{CX\beta + C\epsilon|X}) \\
&= \mathbf{CX\beta}
\end{aligned}
\]

which equals the true parameter \(\beta\) only if \(\mathbf{CX=I}\)\\
Equivalently, \(\tilde{\beta} = \beta + \mathbf{C}\epsilon\) and the variance of the estimator is \(Var(\tilde{\beta}|\mathbf{X}) = \sigma^2\mathbf{CC'}\)

To show minimum variance,

\[
\begin{aligned}
&=\sigma^2\mathbf{(C-(X'X)^{-1}X')(C-(X'X)^{-1}X')'} \\
&= \sigma^2\mathbf{(CC' - CX(X'X)^{-1})-(X'X)^{-1}X'C + (X'X)^{-1}X'X(X'X)^{-1})} \\
&= \sigma^2 (\mathbf{CC' - (X'X)^{-1}-(X'X)^{-1} + (X'X)^{-1}}) \\
&= \sigma^2\mathbf{CC'} - \sigma^2(\mathbf{X'X})^{-1} \\
&= Var(\tilde{\beta}|\mathbf{X}) - Var(\hat{\beta}|\mathbf{X})
\end{aligned}
\]

\textbf{Hierarchy of OLS Assumptions}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2432}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2432}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2703}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2432}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Identification Data Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Unbiasedness Consistency
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\protect\hyperlink{gauss-ux5cux2520markov-theorem}{Gauss- Markov} (BLUE) Asymptotic Inference (z and Chi-squared)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Classical LM (BUE) Small-sample Inference (t and F)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Variation in \(\mathbf{X}\) & Variation in \(\mathbf{X}\) & Variation in \(\mathbf{X}\) & Variation in \(\mathbf{X}\) \\
& Random Sampling & Random Sampling & Random Sampling \\
& Linearity in Parameters & Linearity in Parameters & Linearity in Parameters \\
& Zero Conditional Mean & Zero Conditional Mean & Zero Conditional Mean \\
& & \(\mathbf{H}\) homoskedasticity & \(\mathbf{H}\) homoskedasticity \\
& & & Normality of Errors \\
\end{longtable}

\hypertarget{variable-selection}{%
\subsection{Variable Selection}\label{variable-selection}}

depends on

\begin{itemize}
\tightlist
\item
  Objectives or goals
\item
  Previously acquired expertise
\item
  Availability of data
\item
  Availability of computer software
\end{itemize}

Let \(P - 1\) be the number of possible \(X\) variables

\hypertarget{mallowss-c_p-statistic}{%
\subsubsection{\texorpdfstring{Mallows's \(C_p\) Statistic}{Mallows's C\_p Statistic}}\label{mallowss-c_p-statistic}}

(Mallows, 1973, Technometrics, 15, 661-675)

A measure of the predictive ability of a fitted model

Let \(\hat{Y}_{ip}\) be the predicted value of \(Y_i\) using the model with \(p\) parameters.

The total standardized mean square error of prediction is:

\[
\begin{aligned}
\Gamma_p &= \frac{\sum_{i=1}^n E(\hat{Y}_{ip}-E(Y_i))^2}{\sigma^2} \\
&= \frac{\sum_{i=1}^n [E(\hat{Y}_{ip})-E(Y_i)]^2+\sum_{i=1}^n var(\hat{Y}_{ip})}{\sigma^2}
\end{aligned}
\]

the first term in the numerator is the (bias)\^{}2 term and the 2nd term is the prediction variance term.

\begin{itemize}
\tightlist
\item
  bias term decreases as more variables are added to the model.\\
\item
  if we assume the full model \((p=P)\) is the true model, then \(E(\hat{Y}_{ip}) - E(Y_i) = 0\) and the bias is 0.\\
\item
  Prediction variance increase as more variables are added to the model \(\sum var(\hat{Y}_{ip}) = p \sigma^2\)\\
\item
  thus, a tradeoff between bias and variance terns is achieved by minimizing \(\Gamma_p\).\\
\item
  Since \(\Gamma_p\) is unknown (due to \(\beta\)). we use an estimate: \(C_p = \frac{SSE_p}{\hat{\sigma^2}}- (n-2p)\) which is an unbiased estimate of \(\Gamma_p\)\\
\item
  As more variables are added to the model, the \(SSE_p\) decreases but 2p increases. where \(\hat{\sigma^2}=MSE(X_1,..,X_{P-1})\) the MSE with all possible X variables in the model.\\
\item
  when there is no bias then \(E(C_p) \approx p\). Thus, good models have \(C_p\) close to p.\\
\item
  Prediction: consider models with \(C_p \le p\)\\
\item
  Parameter estimation: consider models with \(C_p \le 2p -(P-1)\). Fewer variables should be eliminated from the model to avoid excess bias in the estimates.
\end{itemize}

\hypertarget{akaike-information-criterion-aic}{%
\subsubsection{Akaike Information Criterion (AIC)}\label{akaike-information-criterion-aic}}

\[
AUC = n ln(\frac{SSE_p}{n}) + 2p
\]

\begin{itemize}
\tightlist
\item
  increasing \(p\) (number of parameters) leads first-term decreases, and second-term increases.
\item
  We want model with small values of AIC. If the AIC increases when a parameter is added to the model, that parameter is not needed.
\item
  AIC represents a tradeoff between precision of fit against the number of parameters used.
\end{itemize}

\hypertarget{bayes-or-schwarz-information-criterion}{%
\subsubsection{Bayes (or Schwarz) Information Criterion}\label{bayes-or-schwarz-information-criterion}}

\[
BIC = n \ln(\frac{SSE_p}{n})+ (\ln n)p
\]

The coefficient in front of p tends to penalize more heavily models with a larger number of parameters (as compared to AIC).

\hypertarget{prediction-error-sum-of-squares-press}{%
\subsubsection{Prediction Error Sum of Squares (PRESS)}\label{prediction-error-sum-of-squares-press}}

\[
PRESS_p = \sum_{i=1}^{n} (Y_i - \hat{Y}_{i(i)})^2
\]

where \(\hat{Y}_{i(i)}\) is the prediction of the i-th response when the i-th observation is not used, obtained for the model with p parameters.

\begin{itemize}
\tightlist
\item
  evaluates the predictive ability of a postulated model by omitting one observation at a time.\\
\item
  We want small \(PRESS_p\) values\\
\item
  It can be computationally intensive when you have large p.
\end{itemize}

\hypertarget{best-subsets-algorithm}{%
\subsubsection{Best Subsets Algorithm}\label{best-subsets-algorithm}}

\begin{itemize}
\tightlist
\item
  ``leap and bounds'' algorithm of \citep{furnival2000regressions} combines comparison of SSE for different subset models with control over the sequence in which the subset regression are computed.\\
\item
  Guarantees finding the best m subset regressions within each subset size with less computational burden than all possible subsets.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"leaps"}\NormalTok{)}
\FunctionTok{regsubsets}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\hypertarget{stepwise-selection-procedures}{%
\subsubsection{Stepwise Selection Procedures}\label{stepwise-selection-procedures}}

The \textbf{forward stepwise} procedure:

\begin{itemize}
\tightlist
\item
  finds a plausible subset sequentially.
\item
  at each step, a variable is added or deleted.
\item
  criterion for adding or deleting is based on SSE, \(R^2\), t, or F-statistic.
\end{itemize}

Note:

\begin{itemize}
\tightlist
\item
  Instead of using exact F-values, computer packages usually specify the equivalent ``significance'' level. For example, SLE is the ``significance'' level to enter, and SLS is the ``significance'' level to stay. The SLE and SLS are guides rather than true tests of significance.\\
\item
  The choice of SLE and SLS represents a balancing of opposing tendencies. Use of large SLE values tends to result in too many predictor variables; models with small SLE tend to be under-specified resulting in \(\sigma^2\) being badly overestimated.\\
\item
  As for choice of SLE, can choose between 0.05 and 0.5.\\
\item
  If SLE \textgreater{} SLS then a cycling pattern may occur. Although most computer packages can detect can stop when it happens. A quick fix: SLS = SLE /2 \citep{bendel1977comparison}.\\
\item
  If SLE \textless{} SLS then the procedure is conservative and may lead variables with low contribution to be retained.\\
\item
  Order of variable entry does not matter.
\end{itemize}

Automated Selection Procedures:

\begin{itemize}
\tightlist
\item
  Forward selection: Same idea as forward stepwise except it doesn't test if variables should be dropped once enter. (not as good as forward stepwise).\\
\item
  Backward Elimination: begin with all variables and identifies the one with the smallest F-value to be dropped.
\end{itemize}

\hypertarget{diagnostics-1}{%
\subsection{Diagnostics}\label{diagnostics-1}}

\hypertarget{normality-of-errors}{%
\subsubsection{Normality of errors}\label{normality-of-errors}}

could use \protect\hyperlink{methods-based-on-normal-probability-plot}{Methods based on normal probability plot} or \protect\hyperlink{methods-based-on-empirical-cumulative-distribution-function}{Methods based on empirical cumulative distribution function}

or plots such as

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y }\OtherTok{=} \DecValTok{1}\SpecialCharTok{:}\DecValTok{100}
\NormalTok{x }\OtherTok{=} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\FunctionTok{qqplot}\NormalTok{(x,y)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{05-regression_files/figure-latex/unnamed-chunk-2-1} \end{center}

\hypertarget{influential-observationsoutliers}{%
\subsubsection{Influential observations/outliers}\label{influential-observationsoutliers}}

\hypertarget{hat-matrix}{%
\paragraph{Hat matrix}\label{hat-matrix}}

\[
\mathbf{H = X(X'X)^{-1}}
\]

where \(\mathbf{\hat{Y}= HY, e = (I-H)Y}\) and \(var(\mathbf{e}) = \sigma^2 (\mathbf{I-H})\)

\begin{itemize}
\tightlist
\item
  \(\sigma^2(e_i) = \sigma^2 (1-h_{ii})\), where \(h_{ii}\) is the \(i\)-th element of the main diagonal of \(\mathbf{H}\) (must be between 0 and 1).\\
\item
  \(\sum_{i=1}^{n} h_{ii} = p\)\\
\item
  \(cov(e_i,e_j) = -h_{ii}\sigma^2\) where \(i \neq j\)\\
\item
  Estimate: \(s^2(e_i) = MSE (1-h_{ii})\)\\
\item
  Estimate: \(\hat{cov}(e_i,e_j) = -h_{ij}(MSE)\); if model assumption are correct, this covariance is very small for large data sets.\\
\item
  If \(\mathbf{x}_i = [1 X_{i,1} ... X_{i,p-1}]'\) (the vector of X-values for a given response), then \(h_{ii} = \mathbf{x_i'(X'X)^{-1}x_i}\) (depends on relative positions of the design points \(X_{i,1},...,X_{i,p-1}\))
\end{itemize}

\hypertarget{studentized-residuals}{%
\paragraph{Studentized Residuals}\label{studentized-residuals}}

\[
\begin{aligned}
r_i &= \frac{e_i}{s(e_i)} \\
r_i &\sim N(0,1)
\end{aligned}
\]

where \(s(e_i) = \sqrt{MSE(1-h_{ii})}\). \(r_i\) is called the studentized residual or standardized residual.

\begin{itemize}
\tightlist
\item
  you can use the semi-studentized residual before, \(e_i^*= e_i \sqrt{MSE}\). This doesn't take into account the different variances for each \(e_i\).
\end{itemize}

We would want to see the model without a particular value. You delete the \(i\)-th case, fit the regression to the remaining \(n-1\) cases, get estimated responses for the \(i\)-th case, \(\hat{Y}_{i(i)}\), and find the difference, called the \textbf{deleted residual}:

\[
\begin{aligned}
d_i &= Y_i - \hat{Y}_{i(i)} \\
&= \frac{e_i}{1-h_{ii}} 
\end{aligned}
\]

we don't need to recompute the regression model for each case

As \(h_{ii}\) increases, \(d_i\) increases.

\[
s^2(d_i)= \frac{MSE_{(i)}}{1-h_{ii}}
\]

where \(MSE_{(i)}\) is the mean square error when the i-th case is omitted.

Let

\[
t_i = \frac{d_i}{s(d_i)} = \frac{e_i}{\sqrt{MSE_{(i)}(1-h_{ii})}}
\]

be the \textbf{studentized deleted residual}, which follows a t-distribution with \(n-p-1\) df.

\[
(n-p)MSE = (n-p-1)MSE_{(i)}+ \frac{e^2_{i}}{1-h_{ii}}
\]

hence, we do not need to fit regressions for each case and

\[
t_i = e_i (\frac{n-p-1}{SSE(1-h_{ii})-e^2_i})^{1/2}
\]

The outlying \(Y\)-observations are those cases whose studentized deleted residuals are large in absolute value. If there are many residuals to consider, a Bonferroni critical value can be can (\(t_{1-\alpha/2n;n-p-1}\))

\textbf{Outlying X Observations}

Recall, \(0 \le h_{ii} \le 1\) and \(\sum_{i=1}^{n}h_{ii}=p\) (the total number of parameters)

A large \(h_{ii}\) indicates that the \(i\)-th case is distant from the center of all \(X\) observations (the \textbf{leverage} of the \(i\)-th case). That is, a large value suggests that the observation exercises substantial leverage in determining the fitted value \(\hat{Y}_i\)

We have \(\mathbf{\hat{Y}=HY}\), a linear combination of Y-values; \(h_{ii}\) is the weight of the observation \(Y_i\); so \(h_{ii}\) measures the role of the X values in determining how important \(Y_i\) is in affecting the \(\hat{Y}_i\).

Large \(h_{ii}\) implies \(var(e_i)\) is small, so larger \(h_{ii}\) implies that \(\hat{Y}_i\) is close to \(Y_i\)

\begin{itemize}
\tightlist
\item
  small data sets: \(h_{ii} > .5\) suggests ``large''.\\
\item
  large data sets: \(h_{ii} > \frac{2p}{n}\) is ``large.
\end{itemize}

Using the hat matrix to identify extrapolation:

\begin{itemize}
\tightlist
\item
  Let \(\mathbf{x_{new}}\) be a vector containing the X values for which an inference about a mean response or a new observation is to be made.\\
\item
  Let \(\mathbf{X}\) be the data design matrix used to fit the data. Then, if \(h_{new,new} = \mathbf{x_{new}(X'X)^{-1}x_{new}}\) is within the range of leverage values (\(h_{ii}\)) for cases in the data set, no extrapolation is involved; otherwise; extrapolation is indicated.
\end{itemize}

\textbf{Identifying Influential Cases}:

by influential we mean that exclusion of an observation causes major changes int he fitted regression. (not all outliers are influential)

\begin{itemize}
\tightlist
\item
  Influence on Single Fitted Values: \protect\hyperlink{dffits}{DFFITS}
\item
  Influence on All Fitted Values: \protect\hyperlink{cooks-d}{Cook's D}
\item
  Influence on the Regression Coefficients: \protect\hyperlink{dfbetas}{DFBETAS}
\end{itemize}

\hypertarget{dffits}{%
\paragraph{DFFITS}\label{dffits}}

Influence on Single Fitted Values: \protect\hyperlink{dffits}{DFFITS}

\[
\begin{aligned}
(DFFITS)_i &= \frac{\hat{Y}_i - \hat{Y}_{i(i)}}{\sqrt{MSE_{(i)}h_{ii}}} \\
&= t_i (\frac{h_{ii}}{1-h_{ii}})^{1/2}
\end{aligned}
\]

\begin{itemize}
\item
  the standardized difference between the i-th fitted value with all observations and with the i-th case removed.
\item
  studentized deleted residual multiplied by a factor that is a function fo the i-th leverage value.
\item
  influence if:

  \begin{itemize}
  \tightlist
  \item
    small to medium data sets: \(|DFFITS|>1\)
  \item
    large data sets: \(|DFFITS|> 2 \sqrt{p/n}\)
  \end{itemize}
\end{itemize}

\hypertarget{cooks-d}{%
\paragraph{Cook's D}\label{cooks-d}}

Influence on All Fitted Values: \protect\hyperlink{cooks-d}{Cook's D}

\[
\begin{aligned}
D_i &= \frac{\sum_{j=1}^{n}(\hat{Y}_j - \hat{Y}_{j(i)})^2}{p(MSE)} \\
&= \frac{e^2_i}{p(MSE)}(\frac{h_{ii}}{(1-h_{ii})^2})
\end{aligned}
\]

gives the influence of i-th case on all fitted values.

If \(e_i\) increases or \(h_{ii}\) increases, then \(D_i\) increases.

\(D_i\) is a percentile of an \(F_{(p,n-p)}\) distribution. If the percentile is greater than \(.5(50\%)\) then the \(i\)-th case has major influence. In practice, if \(D_i >4/n\), then the \(i\)-th case has major influence.

\hypertarget{dfbetas}{%
\paragraph{DFBETAS}\label{dfbetas}}

Influence on the Regression Coefficients: \protect\hyperlink{dfbetas}{DFBETAS}

\[
(DFBETAS)_{k(i)} = \frac{b_k - b_{k(i)}}{\sqrt{MSE_{(i)}c_{kk}}}
\]

for \(k = 0,...,p-1\) and \(c_{kk}\) is the k-th diagonal element of \(\mathbf{X'X}^{-1}\)

Influence of the \(i\)-th case on each regression coefficient \(b_k\) \((k=0,\dots,p-1)\) is the difference between the estimated regression coefficients based on all \(n\) cases and the regression coefficients obtained when the \(i\)-th case is omitted (\(b_{k(i)}\))

\begin{itemize}
\tightlist
\item
  small data sets: \(|DFBETA|>1\)
\item
  large data sets: \(|DFBETA| > 2\sqrt{n}\)
\item
  Sign of DFBETA inculcates whether inclusion of a case leads to an increase or a decrease in estimates of the regression coefficient.
\end{itemize}

\hypertarget{collinearity}{%
\subsubsection{Collinearity}\label{collinearity}}

Multicollinearity refers to correlation among explanatory variables.

\begin{itemize}
\tightlist
\item
  large changes in the estimated regression coefficient when a predictor variable is added or deleted, or when an observation is altered or deleted.\\
\item
  non insignificant results in individual tests on regression coefficients for important predictor variables.\\
\item
  estimated regression coefficients with an algebraic sign that is the opposite of that expected from theoretical consideration or prior experience.\\
\item
  large coefficients of simple correlation between pairs of predictor variables in the correlation matrix.\\
\item
  wide confidence intervals for the regression coefficients representing important predictor variables.
\end{itemize}

When some of \(X\) variables are so highly correlated that the inverse \((X'X)^{-1}\) does not exist or is very computationally unstable.

Correlated Predictor Variables: if some X variables are ``perfectly'' correlated, the system is undetermined and there are an infinite number of models that fit the data. That is, if \(X'X\) is singular, then \((X'X)^{-1}\) doesn't exist. Then,

\begin{itemize}
\tightlist
\item
  parameters cannot be interpreted (\(\mathbf{b = (X'X)^{-1}X'y}\))\\
\item
  sampling variability is infinite (\(\mathbf{s^2(b) = MSE (X'X)^{-1}}\))
\end{itemize}

\hypertarget{vifs}{%
\paragraph{VIFs}\label{vifs}}

Let \(R^2_k\) be the coefficient of multiple determination when \(X_k\) is regressed on the \(p - 2\) other \(X\) variables in the model. Then,

\[
VIF_k = \frac{1}{1-R^2_k}
\]

\begin{itemize}
\tightlist
\item
  large values indicate that a near collinearity is causing the variance of \(b_k\) to be inflated, \(var(b_k) \propto \sigma^2 (VIF_k)\)\\
\item
  Typically, the rule of thumb is that \(VIF > 4\) mean you should see why this is the case, and \(VIF_k > 10\) indicates a serious problem collinearity problem that could result in poor parameters estimates.\\
\item
  the mean of all VIF's provide an estimate of the ratio of the true multicollinearity to a model where the \(X\) variables are uncorrelated\\
\item
  serious multicollinearity if \(avg(VIF) >>1\)
\end{itemize}

\hypertarget{condition-number}{%
\paragraph{Condition Number}\label{condition-number}}

\textbf{Condition Number}

spectral decomposition

\[
\mathbf{X'X}= \sum_{i=1}^{p} \lambda_i \mathbf{u_i u_i'}
\]

where \(\lambda_i\) is the eigenvalue and \(\mathbf{u}_i\) is the eigenvector. \(\lambda_1 > ...>\lambda_p\) and the eigenvecotrs are orthogonal:

\[
\begin{cases}
\mathbf{u_i'u_j} =
0&\text{for $i \neq j$}\\
1&\text{for $i =j$}\\
\end{cases}
\]

The condition number is then

\[
k = \sqrt{\frac{\lambda_{max}}{\lambda_{min}}}
\]

\begin{itemize}
\tightlist
\item
  values \(k>30\) are cause for concern\\
\item
  values \(30<k<100\) imply moderate dependencies.\\
\item
  values \(k>100\) imply strong collinearity
\end{itemize}

\textbf{Condition index}

\[
\delta_i = \sqrt{\frac{\lambda_{max}}{\lambda_i}}
\]

where \(i = 1,...,p\)

we can find the proportion of the total variance associated with the k-th regression coefficient and the i-th eigen mode:

\[
\frac{u_{ik}^2/\lambda_i}{\sum_j (u^2_{jk}/\lambda_j)}
\]

These variance proportions can be helpful for identifying serious collinearity

\begin{itemize}
\tightlist
\item
  the condition index must be large
\item
  the variance proportions must be large (\textgreater,5) for at least two regression coefficients.
\end{itemize}

\hypertarget{constancy-of-error-variance}{%
\subsubsection{Constancy of Error Variance}\label{constancy-of-error-variance}}

\hypertarget{brown-forsythe-test-modified-levene-test}{%
\paragraph{Brown-Forsythe Test (Modified Levene Test)}\label{brown-forsythe-test-modified-levene-test}}

\begin{itemize}
\tightlist
\item
  Does not depend on normality\\
\item
  Applicable when error variance increases or decreases with \(X\)\\
\item
  relatively large sample size needed (so we can ignore dependency between residuals)\\
\item
  Split residuals into 2 groups (\(e_{i1}, i = 1, ..., n_1; e_{i2}, j=1,...,n_2\))\\
\item
  Let \(d_{i1}= |e_{i1}-\tilde{e}_{1}|\) where \(\tilde{e}_{1}\) is the median of group 1.\\
\item
  Let \(d_{j2}=|e_{j2}-\tilde{e}_{2}|\).\\
\item
  Then, a 2-sample t-test:\\
  \[
  t_L = \frac{\bar{d}_1 - \bar{d}_2}{s\sqrt{1/n_1+1/n_2}}
  \] where \[
  s^2 = \frac{\sum_i(d_{i1}-\bar{d}_1)^2+\sum_j(d_{j2}-\bar{d}_2)^2}{n-2}
  \] If \(|t_L|>t_{1-\alpha/2;n-2}\) conclude the error variance is not constant.
\end{itemize}

\hypertarget{breusch-pagan-test-cook-weisberg-test}{%
\paragraph{Breusch-Pagan Test (Cook-Weisberg Test)}\label{breusch-pagan-test-cook-weisberg-test}}

Assume the error terms are independent and normally distributed, and

\[
\sigma^2_i = \gamma_0 + \gamma_1 X_i
\]

Constant error variance corresponds to \(\gamma_1 = 0\), i.e., test

\begin{itemize}
\tightlist
\item
  \(H_0: \gamma_1 =0\)
\item
  \(H_1: \gamma_1 \neq 0\)
\end{itemize}

by regressing the squared residuals on X in the usual manner. Obtain the regression sum of squares from this: \(SSR^*\) (the SSR from the regression of \(e^2_i\) on \(X_i\)). Then, define

\[
X^2_{BP} = \frac{SSR^*/2}{(SSE/n)^2}
\]

where SSE is the error sum of squares from the regression of Y on X.

If \(H_0: \gamma_1 = 0\) holds and n is reasonably large, \(X^2_{BP}\) follows approximately the \(\chi^2\) distribution with 1 d.f. We reject \(H_0\) (Homogeneous variance) if \(X^2_{BP} > \chi^2_{1-\alpha;1}\)

\hypertarget{independence}{%
\subsubsection{Independence}\label{independence}}

\hypertarget{plots}{%
\paragraph{Plots}\label{plots}}

\hypertarget{durbin-watson}{%
\paragraph{Durbin-Watson}\label{durbin-watson}}

\hypertarget{time-series}{%
\paragraph{Time-series}\label{time-series}}

\hypertarget{spatial-statistics}{%
\paragraph{Spatial Statistics}\label{spatial-statistics}}

\hypertarget{model-validation}{%
\subsection{Model Validation}\label{model-validation}}

\begin{itemize}
\item
  split data into 2 groups: training (model building) sample and validation (prediction) sample.\\
\item
  the model MSE will tend to underestimate the inherent variability in making future predictions. to consider actual predictive ability, consider mean squared prediction error (MSPE):\\
  \[
  MSPE = \frac{\sum_{i=1}^{n} (Y_i- \hat{Y}_i)^2}{n^*}
  \]

  \begin{itemize}
  \tightlist
  \item
    where \(Y_i\) is the known value of the response variable in the \(i\)-th validation case.
  \item
    \(\hat{Y}_i\) is the predicted value based on a model fit with the training data set.
  \item
    \(n^*\) is the number of cases in the validation set.
  \end{itemize}
\item
  we want MSPE to be close to MSE (in which MSE is not biased); so look at the the ratio MSPE / MSE (closer to 1, the better).
\end{itemize}

\hypertarget{finite-sample-properties}{%
\subsection{Finite Sample Properties}\label{finite-sample-properties}}

\begin{itemize}
\item
  \(n\) is fixed
\item
  \textbf{Bias} On average, how close is our estimate to the true value

  \begin{itemize}
  \item
    \(Bias = E(\hat{\beta}) -\beta\) where \(\beta\) is the true parameter value and \(\hat{\beta}\) is the estimator for \(\beta\)
  \item
    An estimator is \textbf{unbiased} when

    \begin{itemize}
    \tightlist
    \item
      \(Bias = E(\hat{\beta}) -\beta = 0\) or \(E(\hat{\beta})=\beta\)
    \item
      means that the estimator will produce estimates that are, on average, equal to the value it it trying to estimate
    \end{itemize}
  \end{itemize}
\item
  \textbf{Distribution of an estimator}: An estimator is a function of random variables (data)
\item
  \textbf{Standard Deviation}: the spread of the estimator.
\end{itemize}

\textbf{OLS}

Under \protect\hyperlink{a1-linearity}{A1} \protect\hyperlink{a2-full-rank}{A2} \protect\hyperlink{a3-exogeneity-of-independent-variables}{A3}, OLS is unbiased

\[
\begin{aligned}
E(\hat{\beta}) &= E(\mathbf{(X'X)^{-1}X'y}) && \text{A2}\\
     &= E(\mathbf{(X'X)^{-1}X'(X\beta + \epsilon)}) && \text{A1}\\
     &= E(\mathbf{(X'X)^{-1}X'X\beta + (X'X)^{-1}X'\epsilon})  && \text{} \\
     &= E(\beta + \mathbf{(X'X)^{-1}X'\epsilon}) \\
     &= \beta + E(\mathbf{(X'X^{-1}\epsilon)}) \\
     &= \beta + E(E((\mathbf{X'X)^{-1}X'\epsilon|X})) &&\text{LIE} \\
     &= \beta + E((\mathbf{X'X)^{-1}X'}E\mathbf{(\epsilon|X})) \\
     &= \beta + E((\mathbf{X'X)^{-1}X'}0)) && \text{A3} \\
     &= \beta
\end{aligned}
\]

where LIE stands for \protect\hyperlink{law-of-iterated-expectation}{Law of Iterated Expectation}

If \protect\hyperlink{a3-exogeneity-of-independent-variables}{A3} does not hold, then OLS will be \textbf{biased}

From \textbf{Frisch-Waugh-Lovell Theorem}, if we have the omitted variable \(\hat{\beta}_2 \neq 0\) and \(\mathbf{X_1'X_2} \neq 0\), then the omitted variable will cause OLS estimator to be biased.

Under \protect\hyperlink{a1-linearity}{A1} \protect\hyperlink{a2-full-rank}{A2} \protect\hyperlink{a3-exogeneity-of-independent-variables}{A3} \protect\hyperlink{a4-homoskedasticity}{A4}, we have the conditional variance of the OLS estimator as follows{]}

\[
\begin{aligned}
Var(\hat{\beta}|\mathbf{X}) &= Var(\beta + \mathbf{(X'X)^{-1}X'\epsilon|X}) && \text{A1-A2}\\
    &= Var((\mathbf{X'X)^{-1}X'\epsilon|X)} \\
    &= \mathbf{X'X^{-1}X'} Var(\epsilon|\mathbf{X})\mathbf{X(X'X)^{-1}} \\
    &= \mathbf{X'X^{-1}X'} \sigma^2I \mathbf{X(X'X)^{-1}} && \text{A4} \\
    &= \sigma^2\mathbf{X'X^{-1}X'} I \mathbf{X(X'X)^{-1}} \\
    &= \sigma^2\mathbf{(X'X)^{-1}}
\end{aligned}
\]

Sources of variation

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \(\sigma^2=Var(\epsilon_i|\mathbf{X})\)

  \begin{itemize}
  \tightlist
  \item
    The amount of unexplained variation \(\epsilon_i\) is large relative to the explained \(\mathbf{x_i \beta}\) variation
  \end{itemize}
\item
  ``Small'' \(Var(x_{i1}), Var(x_{i1}),..\)

  \begin{itemize}
  \tightlist
  \item
    Not a lot of variation in \(\mathbf{X}\) (no information)
  \item
    small sample size
  \end{itemize}
\item
  ``Strong'' correlation between the explanatory variables

  \begin{itemize}
  \tightlist
  \item
    \(x_{i1}\) is highly correlated with a linear combination of 1, \(x_{i2}\), \(x_{i3}\), \ldots{}
  \item
    include many irrelevant variables will contribute to this.
  \item
    If \(x_1\) is perfectly determined in the regression \(\rightarrow\) \textbf{Perfect Collinearity} \(\rightarrow\) \protect\hyperlink{a2-full-rank}{A2} is violated.
  \item
    If \(x_1\) is highly correlated with a linear combination of other variables, then we have \textbf{Multicollinearity}
  \end{itemize}
\end{enumerate}

\hypertarget{check-for-multicollinearity}{%
\subsubsection{Check for Multicollinearity}\label{check-for-multicollinearity}}

\textbf{Variance Inflation Factor} (VIF) Rule of thumb \(VIF \ge 10\) is large

\[
VIF = \frac{1}{1-R_1^2} 
\]

\hypertarget{standard-errors}{%
\subsubsection{Standard Errors}\label{standard-errors}}

\begin{itemize}
\tightlist
\item
  \(Var(\hat{\beta}|\mathbf{X})=\sigma^2\mathbf{(X'X)^{-1}}\) is the variance of the estimate \(\hat{\beta}\)
\item
  \textbf{Standard Errors} are estimators/estimates of the standard deviation (square root of the variance) of the estimator \(\hat{\beta}\)
\item
  Under A1-A5, then we can estimate \(\sigma^2=Var(\epsilon^2|\mathbf{X})\) the standard errors as
\end{itemize}

\[
\begin{aligned}
s^2 &= \frac{1}{n-k}\sum_{i=1}^{n}e_i^2 \\
&= \frac{1}{n-k}SSR
\end{aligned}
\]

\begin{itemize}
\tightlist
\item
  degrees of freedom adjustment: because \(e_i \neq \epsilon_i\) and are estimated using k estimates for \(\beta\), we lose degrees of freedom in our variance estimate.
\item
  \(s=\sqrt{s^2}\) is a biased estimator for the standard deviation ({[}Jensen's Inequality{]})
\end{itemize}

\textbf{Standard Errors for} \(\hat{\beta}\)

\[
\begin{aligned}
SE(\hat{\beta}_{j-1})&=s\sqrt{[(\mathbf{X'X})^{-1}]_{jj}} \\
&= \frac{s}{\sqrt{SST_{j-1}(1-R_{j-1}^2)}}
\end{aligned}
\]

where \(SST_{j-1}\) and \(R_{j-1}^2\) from the following regression

\(x_{j-1}\) on 1, \(x_1\),\ldots{} \(x_{j-2}\),\(x_j\),\(x_{j+1}\), \ldots, \(x_{k-1}\)

\textbf{Summary of Finite Sample Properties}

\begin{itemize}
\tightlist
\item
  Under A1-A3: OLS is unbiased
\item
  Under A1-A4: The variance of the OLS estimator is \(Var(\hat{\beta}|\mathbf{X})=\sigma^2\mathbf{(X'X)^{-1}}\)
\item
  Under A1-A4, A6: OLS estimator \(\hat{\beta} \sim N(\beta,\sigma^2\mathbf{(X'X)^{-1}})\)
\item
  Under A1-A4, Gauss-Markov Theorem holds \(\rightarrow\) OLS is BLUE
\item
  Under A1-A5, the above standard errors are unbiased estimator of standard deviation for \(\hat{\beta}\)
\end{itemize}

\hypertarget{large-sample-properties}{%
\subsection{Large Sample Properties}\label{large-sample-properties}}

\begin{itemize}
\tightlist
\item
  Let \(n \rightarrow \infty\)
\item
  A perspective that allows us to evaluate the ``quality'' of estimators when finite sample properties are not informative, or impossible to compute
\item
  Consistency, asymptotic distribution, asymptotic variance
\end{itemize}

\textbf{Motivation}

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{finite-sample-properties}{Finite Sample Properties} need strong assumption \protect\hyperlink{a1-linearity}{A1} \protect\hyperlink{a3-exogeneity-of-independent-variables}{A3} \protect\hyperlink{a4-homoskedasticity}{A4} \protect\hyperlink{a6-normal-distribution}{A6}
\item
  Other estimation such as GLS, MLE need to be analyzed using \protect\hyperlink{large-sample-properties}{Large Sample Properties}
\end{itemize}

Let \(\mu(\mathbf{X})=E(y|\mathbf{X})\) be the \textbf{Conditional Expectation Function}

\begin{itemize}
\tightlist
\item
  \(\mu(\mathbf{X})\) is the minimum mean squared predictor (over all possible functions)
\end{itemize}

\[
minE((y-f(\mathbf{X}))^2)
\]

under A1 and A3,

\[
\mu(\mathbf{X})=\mathbf{X}\beta
\]

Then the \textbf{linear projection}

\[
L(y|1,\mathbf{X})=\gamma_0 + \mathbf{X}Var(X)^{-1}Cov(X,Y)
\]

where \(\mathbf{X}Var(X)^{-1}Cov(X,Y)=\gamma\)

is the minimum mean squared linear approximation to be conditional mean function

\[
(\gamma_0,\gamma) = arg min E((E(y|\mathbf{X})-(a+\mathbf{Xb})^2)
\]

\begin{itemize}
\tightlist
\item
  OLS is always \textbf{consistent} for the linear projection, but not necessarily unbiased.
\item
  Linear projection has no causal interpretation
\item
  Linear projection does not depend on assumption A1 and A3
\end{itemize}

Evaluating an estimator using large sample properties:

\begin{itemize}
\tightlist
\item
  Consistency: measure of centrality
\item
  Limiting Distribution: the shape of the scaled estimator as the sample size increases
\item
  Asymptotic variance: spread of the estimator with regards to its limiting distribution.
\end{itemize}

An estimator \(\hat{\theta}\) is consistent for \(\theta\) if \(\hat{\theta}_n \to^p \theta\)

\begin{itemize}
\tightlist
\item
  As n increases, the estimator converges to the population parameter value.
\item
  Unbiased does not imply consistency and consistency does not imply unbiased.
\end{itemize}

Based on \protect\hyperlink{weak-law}{Weak Law} of Large Numbers

\[
\begin{aligned}
\hat{\beta} &= \mathbf{(X'X)^{-1}X'y} \\
&= \mathbf{(\sum_{i=1}^{n}x_i'x_i)^{-1} \sum_{i=1}^{n}x_i'y_i} \\
&= (n^{-1}\mathbf{\sum_{i=1}^{n}x_i'x_i)^{-1}} n^{-1}\mathbf{\sum_{i=1}^{n}x_i'y_i}
\end{aligned}
\]

\[
\begin{aligned}
plim(\hat{\beta}) &= plim((n^{-1}\mathbf{\sum_{i=1}^{n}x_i'x_i)^{-1}} n^{-1}\mathbf{\sum_{i=1}^{n}x_i'y_i}) \\
&= plim((n^{-1}\mathbf{\sum_{i=1}^{n}x_i'x_i)^{-1}})plim(n^{-1}\mathbf{\sum_{i=1}^{n}x_i'y_i}) \\
&= (plim(n^{-1}\mathbf{\sum_{i=1}^{n}x_i'x_i)^{-1}})plim(n^{-1}\mathbf{\sum_{i=1}^{n}x_i'y_i}) \text{ due to A2, A5} \\
&= E(\mathbf{x_i'x_i})^{-1}E(\mathbf{x_i'y_i})
\end{aligned}
\]

\[
E(\mathbf{x_i'x_i})^{-1}E(\mathbf{x_i'y_i}) = \beta + E(\mathbf{x_i'x_i})^{-1}E(\mathbf{x_i'\epsilon_i})
\]

Under \protect\hyperlink{a1-linearity}{A1}, \protect\hyperlink{a2-full-rank}{A2}, \protect\hyperlink{a3a}{A3a}, \protect\hyperlink{a5-data-generation-random-sampling}{A5} OLS is consistent, but not guarantee unbiased.

Under \protect\hyperlink{a1-linearity}{A1}, \protect\hyperlink{a2-full-rank}{A2}, \protect\hyperlink{a3a}{A3a}, \protect\hyperlink{a5-data-generation-random-sampling}{A5}, and \(\mathbf{x_i'x_i}\) has finite first and second moments (\protect\hyperlink{central-limit-theorem}{CLT}), \(Var(\mathbf{x_i'}\epsilon_i)=\mathbf{B}\)

\begin{itemize}
\tightlist
\item
  \((n^{-1}\sum_{i=1}^{n}\mathbf{x_i'x_i})^{-1} \to^p (E(\mathbf{x'_ix_i}))^{-1}\)
\item
  \(\sqrt{n}(n^{-1}\sum_{i=1}^{n}\mathbf{x_i'}\epsilon_i) \to^d N(0,\mathbf{B})\)
\end{itemize}

\[
\sqrt{n}(\hat{\beta}-\beta) = (n^{-1}\sum_{i=1}^{n}\mathbf{x_i'x_i})^{-1}\sqrt{n}(n^{-1}\sum_{i=1}^{n}\mathbf{x_i'x_i}) \to^{d} N(0,\Sigma)
\]

where \(\Sigma=(E(\mathbf{x_i'x_i}))^{-1}\mathbf{B}(E(\mathbf{x_i'x_i}))^{-1}\)

\begin{itemize}
\item
  holds under \protect\hyperlink{a3a}{A3a}
\item
  Do not need \protect\hyperlink{a4-homoskedasticity}{A4} and \protect\hyperlink{a6-normal-distribution}{A6} to apply CLT

  \begin{itemize}
  \tightlist
  \item
    If \protect\hyperlink{a4-homoskedasticity}{A4} does not hold, then \(\mathbf{B}=Var(\mathbf{x_i'}\epsilon_i)=\sigma^2E(x_i'x_i)\) which means \(\Sigma=\sigma^2(E(\mathbf{x_i'x_i}))^{-1}\), use standard errors
  \end{itemize}
\end{itemize}

Heteroskedasticity can be from

\begin{itemize}
\tightlist
\item
  Limited dependent variable
\item
  Dependent variables with large/skewed ranges
\end{itemize}

Solving Asymptotic Variance

\[
\begin{aligned}
\Sigma &= (E(\mathbf{x_i'x_i}))^{-1}\mathbf{B}(E(\mathbf{x_i'x_i}))^{-1} \\
&= (E(\mathbf{x_i'x_i}))^{-1}Var(\mathbf{x_i'}\epsilon_i)(E(\mathbf{x_i'x_i}))^{-1} \\
&= (E(\mathbf{x_i'x_i}))^{-1}E[(\mathbf{x_i'}\epsilon_i-0)(\mathbf{x_i'}\epsilon_i-0)](E(\mathbf{x_i'x_i}))^{-1} & \text{A3a} \\
&= (E(\mathbf{x_i'x_i}))^{-1}E[E(\mathbf{\epsilon_i^2|x_i)x_i'x_i]}(E(\mathbf{x_i'x_i}))^{-1} & \text{LIE} \\
&= (E(\mathbf{x_i'x_i}))^{-1}\sigma^2E(\mathbf{x_i'x_i})(E(\mathbf{x_i'x_i}))^{-1} & \text{A4} \\
&= \sigma^2(E(\mathbf{x_i'x_i}))
\end{aligned}
\]

Under \protect\hyperlink{a1-linearity}{A1}, \protect\hyperlink{a2-full-rank}{A2}, \protect\hyperlink{a3a}{A3a}, \protect\hyperlink{a4-homoskedasticity}{A4}, \protect\hyperlink{a5-data-generation-random-sampling}{A5}:

\[
\sqrt{n}(\hat{\beta}-\beta) \to^d N(0,\sigma^2(E(\mathbf{x_i'x_i}))^{-1})
\]

\begin{itemize}
\tightlist
\item
  The Asymptotic variance is approximation for the variance in the scaled random variable for \(\sqrt{n}(\hat{\beta}-\beta)\) when n is large.
\item
  use \(Avar(\sqrt{n}(\hat{\beta}-\beta))/n\) as an approximation for finite sample variance for large n:
\end{itemize}

\[
\begin{aligned}
Avar(\sqrt{n}(\hat{\beta}-\beta)) &\approx Var(\sqrt{n}(\hat{\beta}-\beta)) \\
Avar(\sqrt{n}(\hat{\beta}-\beta))/n &\approx Var(\sqrt{n}(\hat{\beta}-\beta))/n = Var(\hat{\beta})
\end{aligned}
\]

\begin{itemize}
\tightlist
\item
  \(Avar(.)\) does not behave the same way as \(Var(.)\)
\end{itemize}

\[
\begin{aligned}
Avar(\sqrt{n}(\hat{\beta}-\beta))/n &\neq Avar(\sqrt{n}(\hat{\beta}-\beta)/\sqrt{n}) \\
&\neq Avar(\hat{\beta})
\end{aligned}
\]

In \protect\hyperlink{finite-sample-properties}{Finite Sample Properties}, we calculate standard errors as an estimate for the conditional standard deviation:

\[
SE_{fs}(\hat{\beta}_{j-1})=\sqrt{\hat{Var}}(\hat{\beta}_{j-1}|\mathbf{X}) = \sqrt{s^2[\mathbf{(X'X)}^{-1}]_{jj}}
\]

In \protect\hyperlink{large-sample-properties}{Large Sample Properties}, we calculate standard errors as an estimate for the square root of asymptotic variance

\[
SE_{ls}(\hat{\beta}_{j-1})=\sqrt{\hat{Avar}(\sqrt{n}\hat{\beta}_{j-1})/n} = \sqrt{s^2[\mathbf{(X'X)}^{-1}]_{jj}}
\]

Hence, the standard error estimator is the same for finite sample and large sample.

\begin{itemize}
\tightlist
\item
  Same estimator, but conceptually estimating two different things.
\item
  Valid under weaker assumptions: the assumptions needed to produce a consistent estimator for the finite sample conditional variance (A1-A5) are stronger than those needed to produce a consistent estimator for the asymptotic variance (A1,A2,A3a,A4,A5)
\end{itemize}

Suppose that \(y_1,...,y_n\) are a random sample from some population with mean \(\mu\) and variance-covariance matrix \(\Sigma\)

\begin{itemize}
\tightlist
\item
  \(\bar{y} = \frac{1}{n}\sum_{i=1}^{n} y_i\) is a consistent estimator for \(\mu\)\\
\item
  \(S = \frac{1}{n-1}\sum_{i=1}^{n} (y_i -\bar{y})(y_i-\bar{y})'\) is a consistent estimator for \(\Sigma\).\\
\item
  Multivariate Central limit Theorem: Similar to the univariate case, \(\sqrt{n}(\bar{y}-\mu) \sim N_p(0,\Sigma)\), when \(n\) is large relative to p (e.g., \(n \ge 25p\)). Equivalently, \(\bar{y} \sim N_p(\mu,\Sigma/n)\).\\
\item
  Wald's Theorem: \(n(\bar{y} - \mu)'S^{-1}(\bar{y}-\mu) \sim \chi^2_{(p)}\) when \(n\) is large relative to \(p\).
\end{itemize}

\hypertarget{feasible-generalized-least-squares}{%
\section{Feasible Generalized Least Squares}\label{feasible-generalized-least-squares}}

Motivation for a more efficient estimator

\begin{itemize}
\item
  \protect\hyperlink{gauss-markov-theorem}{Gauss-Markov Theorem} holds under A1-A4
\item
  A4: \(Var(\epsilon| \mathbf{X} )=\sigma^2I_n\)

  \begin{itemize}
  \tightlist
  \item
    Heteroskedasticity: \(Var(\epsilon_i|\mathbf{X}) \neq \sigma^2I_n\)
  \item
    Serial Correlation: \(Cov(\epsilon_i,\epsilon_j|\mathbf{X}) \neq 0\)
  \end{itemize}
\item
  Without A4, how can we know which unbiased estimator is the most efficient?
\end{itemize}

Original (unweighted) model:

\[
\mathbf{y=X\beta+ \epsilon}
\]

Suppose A1-A3 hold, but A4 does not hold,

\[
\mathbf{Var(\epsilon|X)=\Omega \neq \sigma^2 I_n}
\]

We will try to use OLS to estimate the transformed (weighted) model

\[
\mathbf{wy=wX\beta + w\epsilon}
\]

We need to choose \(\mathbf{w}\) so that

\[
\mathbf{w'w = \Omega^{-1}}
\]

then \(\mathbf{w}\) (full-rank matrix) is the \textbf{Cholesky decomposition} of \(\mathbf{\Omega^{-1}}\) (full-rank matrix)

In other words, \(\mathbf{w}\) is the squared root of \(\Omega\) (squared root version in matrix)

\[
\begin{aligned}
\Omega &= var(\epsilon | X) \\
\Omega^{-1} &= var(\epsilon | X)^{-1}
\end{aligned}
\]

Then, the transformed equation (IGLS) will have the following properties.

\[
\begin{aligned}
\mathbf{\hat{\beta}_{IGLS}} &= \mathbf{(X'w'wX)^{-1}X'w'wy} \\
& = \mathbf{(X'\Omega^{-1}X)^{-1}X'\Omega^{-1}y} \\
& = \mathbf{\beta + X'\Omega^{-1}X'\Omega^{-1}\epsilon}
\end{aligned}
\]

Since A1-A3 hold for the unweighted model

\[
\begin{aligned}
\mathbf{E(\hat{\beta}_{IGLS}|X)} & = E(\mathbf{\beta + (X'\Omega^{-1}X'\Omega^{-1}\epsilon)}|X)\\
& = \mathbf{\beta + E(X'\Omega^{-1}X'\Omega^{-1}\epsilon)|X)} \\
& = \mathbf{\beta + X'\Omega^{-1}X'\Omega^{-1}E(\epsilon|X)}  && \text{since A3}: E(\epsilon|X)=0 \\
& = \mathbf{\beta}
\end{aligned}
\]

\(\rightarrow\) IGLS estimator is unbiased

\[
\begin{aligned}
\mathbf{Var(w\epsilon|X)} &= \mathbf{wVar(\epsilon|X)w'} \\
& = \mathbf{w\Omega w'} \\
& = \mathbf{w(w'w)^{-1}w'} && \text{since w is a full-rank matrix}\\
& = \mathbf{ww^{-1}(w')^{-1}w'} \\
& = \mathbf{I_n}
\end{aligned}
\]

\(\rightarrow\) A4 holds for the transformed (weighted) equation

Then, the variance for the estimator is

\[
\begin{aligned}
Var(\hat{\beta}_{IGLS}|\mathbf{X}) & = \mathbf{Var(\beta + (X'\Omega ^{-1}X)^{-1}X'\Omega^{-1}\epsilon|X)} \\
&= \mathbf{Var((X'\Omega ^{-1}X)^{-1}X'\Omega^{-1}\epsilon|X)} \\
&= \mathbf{(X'\Omega ^{-1}X)^{-1}X'\Omega^{-1} Var(\epsilon|X)   \Omega^{-1}X(X'\Omega ^{-1}X)^{-1}} && \text{because A4 holds}\\
&= \mathbf{(X'\Omega ^{-1}X)^{-1}X'\Omega^{-1} \Omega \Omega^{-1} \Omega^{-1}X(X'\Omega ^{-1}X)^{-1}} \\
&= \mathbf{(X'\Omega ^{-1}X)^{-1}}
\end{aligned}
\]

Let \(A = \mathbf{(X'X)^{-1}X'-(X'\Omega ^{-1} X)X' \Omega^{-1}}\) then \[
Var(\hat{\beta}_{OLS}|X)- Var(\hat{\beta}_{IGLS}|X) = A\Omega A'
\] And \(\Omega\) is Positive Semi Definite, then \(A\Omega A'\) also PSD, then IGLS is more efficient

The name \textbf{Infeasible} comes from the fact that it is impossible to compute this estimator.

\[
\mathbf{w} = 
\left(
\begin{array}{ccccc}
w_{11} & 0 & 0 & ... & 0 \\
w_{21} & w_{22} & 0 & ... & 0 \\
w_{31} & w_{32} & w_{33} & ... & ... \\
w_{n1} & w_{n2} & w_{n3} & ... & w_{nn} \\
\end{array}
\right)
\]

With \(n(n+1)/2\) number of elements and n observations \(\rightarrow\) infeasible to estimate. (number of equation \textgreater{} data)

Hence, we need to make assumption on \(\Omega\) to make it feasible to estimate \(\mathbf{w}\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \protect\hyperlink{heteroskedasticity}{Heteroskedasticity} : multiplicative exponential model
\item
  \protect\hyperlink{ar1}{AR(1)}
\item
  \protect\hyperlink{cluster}{Cluster}
\end{enumerate}

\hypertarget{heteroskedasticity}{%
\subsection{Heteroskedasticity}\label{heteroskedasticity}}

\begin{equation}
\begin{aligned}
Var(\epsilon_i |x_i) & = E(\epsilon^2|x_i) \neq \sigma^2 \\
& = h(x_i) = \sigma_i^2 \text{(variance of the error term is a function of x)}
\end{aligned}
\label{eq:h-var-error-term}
\end{equation}

For our model,

\[
\begin{aligned}
y_i &= x_i\beta + \epsilon_i \\
(1/\sigma_i)y_i &= (1/\sigma_i)x_i\beta + (1/\sigma_i)\epsilon_i
\end{aligned}
\]

then, from \eqref{eq:h-var-error-term}

\[
\begin{aligned}
Var((1/\sigma_i)\epsilon_i|X) &= (1/\sigma_i^2) Var(\epsilon_i|X) \\
&= (1/\sigma_i^2)\sigma_i^2 \\
&= 1
\end{aligned}
\]

then the weight matrix \(\mathbf{w}\) in the matrix equation

\[
\mathbf{wy=wX\beta + w\epsilon}
\]

\[
\mathbf{w}= 
\left(
\begin{array}{ccccc}
1/\sigma_1 & 0 & 0 & ... & 0 \\
0 & 1/\sigma_2 & 0 & ... & 0 \\
0 & 0 & 1/\sigma_3 & ... & . \\
. & . & . & . & 0 \\
0 & 0 & . & . & 1/\sigma_n
\end{array}
\right)
\]

\textbf{Infeasible Weighted Least Squares}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Assume we know \(\sigma_i^2\) (Infeasible)
\item
  The IWLS estimator is obtained as the least squared estimated for the following weighted equation
\end{enumerate}

\[
(1/\sigma_i)y_i = (1/\sigma_i)\mathbf{x}_i\beta + (1/\sigma_i)\epsilon_i 
\]

\begin{itemize}
\tightlist
\item
  Usual standard errors for the weighted equation are valid if \(Var(\epsilon | \mathbf{X}) = \sigma_i^2\)
\item
  If \(Var(\epsilon | \mathbf{X}) \neq \sigma_i^2\) then heteroskedastic robust standard errors are valid.
\end{itemize}

\textbf{Problem}: We do not know \(\sigma_i^2=Var(\epsilon_i|\mathbf{x_i})=E(\epsilon_i^2|\mathbf{x}_i)\)

\begin{itemize}
\item
  One observation \(\epsilon_i\) cannot estimate a sample variance estimate \(\sigma_i^2\)

  \begin{itemize}
  \tightlist
  \item
    Model \(\epsilon_i^2\) as reasonable (strictly positive) function of \(x_i\) and independent error \(v_i\) (strictly positive)
  \end{itemize}
\end{itemize}

\[
\epsilon_i^2=v_i exp(\mathbf{x_i\gamma})
\]

Then we can apply a log transformation to recover a linear in parameters model,

\[
ln(\epsilon_i^2) = \mathbf{x_i\gamma} + ln(v_i)
\]

where \(ln(v_i)\) is independent \(\mathbf{x}_i\)

We do not observe \(\epsilon_i\) * OLS residual (\(e_i\)) as an approximate

\hypertarget{serial-correlation}{%
\subsection{Serial Correlation}\label{serial-correlation}}

\[
Cov(\epsilon_i, \epsilon_j | \mathbf{X}) \neq 0
\]

Under covariance stationary,

\[
Cov(\epsilon_i,\epsilon_j|\mathbf{X}) = Cov(\epsilon_i, \epsilon_{i+h}|\mathbf{x_i,x_{i+h}})=\gamma_h
\]

And the variance covariance matrix is

\[
Var(\epsilon|\mathbf{X}) = \Omega = 
\left(
\begin{array}{ccccc}
\sigma^2 & \gamma_1 & \gamma_2 & ... & \gamma_{n-1} \\
\gamma_1 & \sigma^2 & \gamma_1 & ... & \gamma_{n-2} \\
\gamma_2 & \gamma_1 & \sigma^2 & ... & ... \\
. & . & . & . & \gamma_1 \\
\gamma_{n-1} & \gamma_{n-2} & . & \gamma_1 & \sigma^2
\end{array}
\right)
\]

There n parameters to estimate - need some sort fo structure to reduce number of parameters to estimate.

\begin{itemize}
\item
  \protect\hyperlink{ar1}{Time Series}

  \begin{itemize}
  \tightlist
  \item
    Effect of inflation and deficit on Treasury Bill interest rates
  \end{itemize}
\item
  \protect\hyperlink{cluster}{Cross-sectional}

  \begin{itemize}
  \tightlist
  \item
    Clustering
  \end{itemize}
\end{itemize}

\hypertarget{ar1}{%
\subsubsection{AR(1)}\label{ar1}}

\[
\begin{aligned}
y_t &= \beta_0 + x_t\beta_1 + \epsilon_t \\
\epsilon_t &= \rho \epsilon_{t-1} + u_t
\end{aligned}
\]

and the variance covariance matrix is

\[
Var(\epsilon | \mathbf{X})= \frac{\sigma^2_u}{1-\rho}
\left(
\begin{array}{ccccc}
1 & \rho & \rho^2 & ... & \rho^{n-1} \\
\rho & 1 & \rho & ... & \rho^{n-2} \\
\rho^2 & \rho & 1 & . & . \\
. & . & . & . & \rho \\
\rho^{n-1} & \rho^{n-2} & . & \rho & 1 \\
\end{array}
\right)
\]

Hence, there is only 1 parameter to estimate: \(\rho\)

\begin{itemize}
\tightlist
\item
  Under A1, A2, A3a, A5a, OLS is consistent and asymptotically normal
\item
  Use \protect\hyperlink{newey-west-standard-errors}{Newey West Standard Errors} for valid inference.
\item
  Apply \protect\hyperlink{infeasible-cochrane-orcutt}{Infeasible Cochrane Orcutt} (as if we knew \(\rho\))
\item
  Because
\end{itemize}

\[
u_t = \epsilon_t - \rho \epsilon_{t-1}
\]

satisfies A3, A4, A5 we'd like to to transform the above equation to one that has \(u_t\) as the error.

\[
\begin{aligned}
y_t - \rho y_{t-1} &= (\beta_0 + x\beta_1 + \epsilon_t) - \rho (\beta_0 + x_{t-1}\beta_1 + \epsilon_{t-1}) \\
& = (1-\rho)\beta_0 + (x_t - \rho x_{t-1})\beta_1 + u_t
\end{aligned}
\]

\hypertarget{infeasible-cochrane-orcutt}{%
\paragraph{Infeasible Cochrane Orcutt}\label{infeasible-cochrane-orcutt}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Assume that we know \(\rho\) (Infeasible)
\item
  The ICO estimator is obtained as the least squared estimated for the following weighted first difference equation
\end{enumerate}

\[
y_t -\rho y_{t-1} = (1-\rho)\beta_0 + (x_t - \rho x_{t-1})\beta_1 + u_t
\]

\begin{itemize}
\tightlist
\item
  Usual standard errors for the weighted first difference equation are valid if the errors truly follow an AR(1) process
\item
  If the serial correlation is generated from a more complex dynamic process then \protect\hyperlink{newey-west-standard-errors}{Newey-West HAC standard errors} are valid
\end{itemize}

\textbf{Problem} We do not know \(\rho\)

\begin{itemize}
\tightlist
\item
  \(\rho\) is the correlation between \(\epsilon_t\) and \(\epsilon_{t-1}\): estimate using OLS residuals (\(e_i\)) as proxy
\end{itemize}

\[
\hat{\rho} = \frac{\sum_{t=1}^{T}e_te_{t-1}}{\sum_{t=1}^{T}e_t^2}
\]

which can be obtained from the OLS regression of

\[
e_t = \rho e_{t-1} + u_t
\]

where we suppress the intercept.

\begin{itemize}
\item
  We are losing an observation
\item
  By taking the first difference we are dropping the first observation
\end{itemize}

\[
y_1 = \beta_0 + x_1 \beta_1 + \epsilon_1
\]

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{feasiable-prais-winsten}{Feasiable Prais Winsten} Transformation applies the \protect\hyperlink{infeasible-cochrane-orcutt}{Infeasible Cochrane Orcutt} but includes a weighted version of the first observation
\end{itemize}

\[
(\sqrt{1-\rho^2})y_1 = \beta_0 + (\sqrt{1-\rho^2})x_1 \beta_1 + (\sqrt{1-\rho^2}) \epsilon_1
\]

\hypertarget{cluster}{%
\subsubsection{Cluster}\label{cluster}}

\[
y_{gi} = \mathbf{x}_{gi}\beta + \epsilon_{gi}
\]

\[
Cov(\epsilon_{gi}, \epsilon_{hj})
\begin{cases}
= 0 & \text{for $g \neq h$ and any pair (i,j)} \\
\neq 0 & \text{for any (i,j) pair}\\
\end{cases}
\]

\textbf{Intra-group Correlation}\\
Each individual in a single group may be correlated but independent across groups.

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{a4-homoskedasticity}{A4} is violated. usual standard errors for OLS are valid.
\item
  Use \textbf{cluster robust standard errors} for OLS.
\end{itemize}

Suppose there are 3 groups with different n

\[
Var(\epsilon| \mathbf{X})= \Omega =
\left(
\begin{array}{cccccc}
\sigma^2 & \delta_{12}^1 & \delta_{13}^1 & 0 & 0 & 0 \\
\delta_{12}^1 & \sigma^2 & \delta_{23}^1 & 0 & 0 & 0 \\
\delta_{13}^1 & \delta_{23}^1 & \sigma^2 & 0 & 0 & 0 \\
0 & 0 & 0 & \sigma^2 & \delta_{12}^2 & 0 \\
0 & 0 & 0 & \delta_{12}^2 & \sigma^2 & 0 \\
0 & 0 & 0 & 0 & 0 & \sigma^2
\end{array}
\right)
\]

where \(Cov(\epsilon_{gi}, \epsilon_{gj}) = \delta_{ij}^g\) and \(Cov(\epsilon_{gi}, \epsilon_{hj}) = 0\) for any i and j

\textbf{Infeasible Generalized Least Squares (Cluster)}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Assume that \(\sigma^2\) and \(\delta_{ij}^g\) are known, plug into \(\Omega\) and solve for the inverse \(\Omega^{-1}\) (infeasible)
\item
  The Infeasible Generalized Least Squares Estimator is
\end{enumerate}

\[
\hat{\beta}_{IGLS} = \mathbf{(X'\Omega^{-1}X)^{-1}X'\Omega^{-1}y}
\]

\textbf{Problem} * We do not know \(\sigma^2\) and \(\delta_{ij}^g\) + Can make assumptions about data generating process that is causing the clustering behavior. - Will give structure to \(Cov(\epsilon_{gi},\epsilon_{gj})= \delta_{ij}^g\) which makes it feasible to estimate - if the assumptions are wrong then we should use cluster robust standard errors.

\textbf{Solution} Assume \textbf{group level random effects} specification in the error

\[
\begin{aligned}
y_{gi} &= \mathbf{g}_i \beta + c_g + u_{gi} \\
Var(c_g|\mathbf{x}_i) &= \sigma^2_c \\
Var(u_{gi}|\mathbf{x}_i) &= \sigma^2_u
\end{aligned}
\]

where \(c_g\) and \(u_{gi}\) are independent of each other, and mean independent of \(\mathbf{x}_i\)

\begin{itemize}
\tightlist
\item
  \(c_g\) captures the common group shocks (independent across groups)
\item
  \(u_{gi}\) captures the individual shocks (independent across individuals and groups)
\end{itemize}

Then the error variance is

\[
Var(\epsilon| \mathbf{X})= \Omega =
\left(
\begin{array}{cccccc}
\sigma^2_c + \sigma^2_u & \sigma^2_c & \sigma^2_c & 0 & 0 & 0 \\
\sigma^2_c & \sigma^2 + \sigma^2_u & \sigma^2_c & 0 & 0 & 0 \\
\sigma^2_c & \sigma^2_c  & \sigma^2+ \sigma^2_u & 0 & 0 & 0 \\
0 & 0 & 0 & \sigma^2+ \sigma^2_u & \sigma^2_c & 0 \\
0 & 0 & 0 & \sigma^2_c & \sigma^2+ \sigma^2_u & 0 \\
0 & 0 & 0 & 0 & 0 & \sigma^2+ \sigma^2_u
\end{array}
\right)
\]

Use \protect\hyperlink{feasible-group-level-random-effects}{Feasible group level Random Effects}

\hypertarget{weighted-least-squares}{%
\section{Weighted Least Squares}\label{weighted-least-squares}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Estimate the following equation using OLS
\end{enumerate}

\[
y_i = \mathbf{x}_i \beta + \epsilon_i
\]

and obtain the residuals \(e_i=y_i -\mathbf{x}_i \hat{\beta}\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Transform the residual and estimate the following by OLS,
\end{enumerate}

\[
ln(e_i^2)= \mathbf{x}_i\gamma + ln(v_i)
\]

and obtain the predicted values \(g_i=\mathbf{x}_i \hat{\gamma}\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  The weights will be the untransformed predicted outcome,
\end{enumerate}

\[
\hat{\sigma}_i =\sqrt{exp(g_i)}
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  The FWLS (Feasible WLS) estimator is obtained as the least squared estimated for the following weighted equation
\end{enumerate}

\[
(1/\hat{\sigma}_i)y_i = (1/\hat{\sigma}_i) \mathbf{x}_i\beta + (1/\hat{\sigma}_i)\epsilon_i
\]

\textbf{Properties of the FWLS}

\begin{itemize}
\item
  The infeasible WLS estimator is unbiased under A1-A3 for the unweighted equation.
\item
  The FWLS estimator is NOT an unbiased estimator.
\item
  The FWLS estimator is consistent under \protect\hyperlink{a1-linearity}{A1}, \protect\hyperlink{a2-full-rank}{A2}, (for the unweighted equation), \protect\hyperlink{a5-data-generation-random-sampling}{A5}, and \(E(\mathbf{x}_i'\epsilon_i/\sigma^2_i)=0\)

  \begin{itemize}
  \tightlist
  \item
    \protect\hyperlink{a3a}{A3a} is not sufficient for the above equation
  \item
    \protect\hyperlink{a3-exogeneity-of-independent-variables}{A3} is sufficient for the above equation.
  \end{itemize}
\item
  The FWLS estimator is asymptotically more efficient than OLS if the errors have multiplicative exponential heteroskedasticity.

  \begin{itemize}
  \tightlist
  \item
    If the errors are truly multiplicative exponential heteroskedasticity, then usual standard errors are valid
  \item
    If we believe that there may be some mis-specification with the \textbf{multiplicative exponential model}, then we should report heteroskedastic robust standard errors.
  \end{itemize}
\end{itemize}

\hypertarget{generalized-least-squares}{%
\section{Generalized Least Squares}\label{generalized-least-squares}}

Consider

\[
\mathbf{y = X\beta + \epsilon} 
\]

where,

\[
var(\epsilon) = \mathbf{G} = 
\left(
\begin{array}
{cccc}
g_{11} & g_{12} & ... & g_{1n} \\
g_{21} & g_{22} & ... & g_{2n} \\
. & . & . & . \\
g_{n1} & . & . & g_{nn}\\
\end{array}
\right)
\]

The variances are heterogeneous, and the errors are correlated.

\[
\mathbf{\hat{b}_G = (X'G^{-1}X)^{-1}X'G^{-1}Y}
\]

if we know G, we can estimate \(\mathbf{b}\) just like OLS. However, we do not know G. Hence, we model the structure of G.

\hypertarget{feasiable-prais-winsten}{%
\section{Feasible Prais Winsten}\label{feasiable-prais-winsten}}

Weighting Matrix

\[
\mathbf{w} = 
\left(
\begin{array}{ccccc}
\sqrt{1- \hat{\rho}^2} & 0 & 0 &... & 0 \\
-\hat{\rho} & 1 & 0 & ... & 0 \\
0 &  -\hat{\rho} & 1 & & . \\
. & . & . & . & 0 \\
0 & . & 0 & -\hat{\rho} & 1
\end{array}
\right)
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Estimate the following equation using OLS
\end{enumerate}

\[
y_t = \mathbf{x}_t \beta + \epsilon_t
\]

and obtain the residuals \(e_t = y_t - \mathbf{x}_t \hat{\beta}\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Estimate the correlation coefficient for the \protect\hyperlink{ar1}{AR(1)} process by estimating the following by OLS (without no intercept)
\end{enumerate}

\[
e_t = \rho e_{t-1} + u_t
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Transform the outcome and independent variables \(\mathbf{wy}\) and \(\mathbf{wX}\) respectively (weight matrix as stated).
\item
  The \protect\hyperlink{feasiable-prais-winsten}{FPW} estimator is obtained as the least squared estimated for the following weighted equation
\end{enumerate}

\[
\mathbf{wy = wX\beta + w\epsilon}
\]

\textbf{Properties of Feasible\protect\hyperlink{feasiable-prais-winsten}{Prais Winsten} Estimator}

\begin{itemize}
\tightlist
\item
  The Infeasible PW estimator is under A1-A3 for the unweighted equation
\item
  The \protect\hyperlink{feasiable-prais-winsten}{FPW} estimator is biased
\item
  The \protect\hyperlink{feasiable-prais-winsten}{FPW} is consistent under \protect\hyperlink{a1-linearity}{A1} \protect\hyperlink{a2-full-rank}{A2} \protect\hyperlink{a5-data-generation-random-sampling}{A5} and
\end{itemize}

\[
E((\mathbf{x_t - \rho x_{t-1}})')(\epsilon_t - \rho \epsilon_{t-1})=0
\]

\begin{itemize}
\item
  \protect\hyperlink{a3a}{A3a} is not sufficient for the above equation
\item
  \protect\hyperlink{a3-exogeneity-of-independent-variables}{A3} is sufficient for the above equation
\item
  The \protect\hyperlink{feasiable-prais-winsten}{FPW} estimator is asymptotically more efficient than OLS if the errors are truly generated as AR(1) process

  \begin{itemize}
  \tightlist
  \item
    If the errors are truly generated as AR(1) process then usual standard errors are valid
  \item
    If we are concerned that there may be a more complex dependence structure of heteroskedasticity, then we use \protect\hyperlink{newey-west-standard-errors}{Newey West Standard Errors}
  \end{itemize}
\end{itemize}

\hypertarget{feasible-group-level-random-effects}{%
\section{Feasible group level Random Effects}\label{feasible-group-level-random-effects}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Estimate the following equation using OLS
\end{enumerate}

\[
y_{gi} = \mathbf{x}_{gi}\beta + \epsilon_{gi}
\]

and obtain the residuals \(e_{gi} = y_{gi} - \mathbf{x}_{gi}\hat{\beta}\) 2. Estimate the variance using the usual \$s\^{}2 estimator

\[
s^2 = \frac{1}{n-k}\sum_{i=1}^{n}e_i^2
\]

as an estimator for \(\sigma^2_c + \sigma^2_u\) and estimate the within group correlation,

\[
\hat{\sigma}^2_c = \frac{1}{G} \sum_{g=1}^{G} (\frac{1}{\sum_{i=1}^{n_g-1}i}\sum_{i\neq j}\sum_{j}^{n_g}e_{gi}e_{gj})
\]

and plug in the estimates to obtain \(\hat{\Omega}\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  The feasible group level RE estimator is obtained as
\end{enumerate}

\[
\hat{\beta}= \mathbf{(X'\hat{\Omega}^{-1}X)^{-1}X'\hat{\Omega}^{-1}y}
\]

\textbf{Properties of the \protect\hyperlink{feasible-group-level-random-effects}{Feasible group level Random Effects} Estimator}

\begin{itemize}
\item
  The infeasible group RE estimator is a linear estimator and is unbiased under A1-A3 for the unweighted equation

  \begin{itemize}
  \tightlist
  \item
    A3 requires \(E(\epsilon_{gi}|\mathbf{x}_i) = E(c_{g}|\mathbf{x}_i)+ (u_{gi}|\mathbf{x}_i)=0\) so we generally assume \(E(c_{g}|\mathbf{x}_i)+ (u_{gi}|\mathbf{x}_i)=0\). The assumption \(E(c_{g}|\mathbf{x}_i)=0\) is generally called \textbf{random effects assumption}
  \end{itemize}
\item
  The \protect\hyperlink{feasible-group-level-random-effects}{Feasible group level Random Effects} is biased
\item
  The \protect\hyperlink{feasible-group-level-random-effects}{Feasible group level Random Effects} is consistent under A1-A3a, and A5a for the unweighted equation.

  \begin{itemize}
  \tightlist
  \item
    \protect\hyperlink{a3a}{A3a} requires \(E(\mathbf{x}_i'\epsilon_{gi}) = E(\mathbf{x}_i'c_{g})+ (\mathbf{x}_i'u_{gi})=0\)
  \end{itemize}
\item
  The \protect\hyperlink{feasible-group-level-random-effects}{Feasible group level Random Effects} estimator is asymptotically more efficient than OLS if the errors follow the random effects specification

  \begin{itemize}
  \tightlist
  \item
    If the errors do follow the random effects specification than the usual standard errors are consistent
  \item
    If there might be a more complex dependence structure or heteroskedasticity, then we need cluster robust standard errors.
  \end{itemize}
\end{itemize}

\hypertarget{ridge-regression}{%
\section{Ridge Regression}\label{ridge-regression}}

When we have the Collinearity problem, we could use the Ridge regression.

The main problem with multicollinearity is that \(\mathbf{X'X}\) is ``ill-conditioned''. The idea for ridge regression: adding a constant to the diagonal of \(\mathbf{X'X}\) improves the conditioning

\[
\mathbf{X'X} + c\mathbf{I} (c>0)
\]

The choice of c is hard. The estimator

\[
\mathbf{b}^R = (\mathbf{X'X}+c\mathbf{I})^{-1}\mathbf{X'y}
\]

is \textbf{biased}.

\begin{itemize}
\tightlist
\item
  It has smaller variance than the \protect\hyperlink{ordinary-least-squares}{OLS} estimator; as c increases, the bias increases but the variance decreases.
\item
  Always exists some value of c for which the ridge regression estimator has a smaller total MSE than the \protect\hyperlink{ordinary-least-squares}{OLS}
\item
  The optimal c varies with application and data set.
\item
  To find the ``optimal'' \(c\) we could use ``ridge trace''.
\end{itemize}

We plot the values of the \(p - 1\) parameter estimates for different values of c, simultaneously.

\begin{itemize}
\tightlist
\item
  Typically, as c increases toward 1 the coefficients decreases to 0.
\item
  The values of the VIF tend to decrease rapidly as c gets bigger than 0. The VIF values begin to change slowly as \(c \to 1\).
\item
  Then we can examine the ridge trace and VIF values and chooses the smallest value of c where the regression coefficients first become stable in the ridge trace and the VIF values have become sufficiently small (which is very subjective).
\item
  Typically, this procedure is applied to the standardized regression model.
\end{itemize}

\hypertarget{principal-component-regression}{%
\section{Principal Component Regression}\label{principal-component-regression}}

This also addresses the problem of multicollinearity

\hypertarget{robust-regression}{%
\section{Robust Regression}\label{robust-regression}}

\begin{itemize}
\tightlist
\item
  To address the problem of influential cases.
\item
  Can be used when a known functional form is to be fitted, and when the errors are not normal due to a few outlying cases.
\end{itemize}

\hypertarget{least-absolute-residuals-lar-regression}{%
\subsection{Least Absolute Residuals (LAR) Regression}\label{least-absolute-residuals-lar-regression}}

also known as minimum \(L_1\)-norm regression.

\[
L_1 = \sum_{i=1}^{n}|Y_i - (\beta_0 + \beta_1 X_{i1} + .. + \beta_{p-1}X_{i,p-1})
\]

which is not sensitive to outliers and inadequacies of the model specification.

\hypertarget{least-median-of-squares-lms-regression}{%
\subsection{Least Median of Squares (LMS) Regression}\label{least-median-of-squares-lms-regression}}

\[
median\{[Y_i - (\beta_0 - \beta_1X_{i1} + ... + \beta_{p-1}X_{i,p-1})]^2 \}
\]

\hypertarget{iteratively-reweighted-least-squares-irls-robust-regression}{%
\subsection{Iteratively Reweighted Least Squares (IRLS) Robust Regression}\label{iteratively-reweighted-least-squares-irls-robust-regression}}

\begin{itemize}
\tightlist
\item
  uses \protect\hyperlink{weighted-least-squares}{Weighted Least Squares} to lessen the influence of outliers.
\item
  the weights \(w_i\) are inversely proportional to how far an outlying case is (e.g., based on the residual)
\item
  the weights are revised iteratively until a robust fit
\end{itemize}

Process:

Step 1: Choose a weight function for weighting the cases.\\
Step 2: obtain starting weights for all cases.\\
Step 3: Use the starting weights in WLS and obtain the residuals from the fitted regression function.\\
Step 4: use the residuals in Step 3 to obtain revised weights.\\
Step 5: continue until convergence.

\textbf{Note}:

\begin{itemize}
\tightlist
\item
  If you don't know the form of the regression function, consider using nonparametric regression (e.g., locally weighted regression, regression trees, projection pursuit, neural networks, smoothing splines, loess, wavelets).
\item
  could use to detect outliers or confirm OLS.
\end{itemize}

\hypertarget{maximum-likelihood-regression}{%
\section{Maximum Likelihood}\label{maximum-likelihood-regression}}

Premise: find values of the parameters that maximize the probability of observing the data In other words, we try to maximize the value of theta in the likelihood function

\[
L(\theta)=\prod_{i=1}^{n}f(y_i|\theta)
\]

\(f(y|\theta)\) is the probability density of observing a single value of \(Y\) given some value of \(\theta\) \(f(y|\theta)\) can be specify as various type of distributions. You can review back section \protect\hyperlink{distributions}{Distributions}. For example, if \(y\) is a dichotomous variable, then

\[
L(\theta)=\prod_{i=1}^{n}\theta^{y_i}(1-\theta)^{1-y_i}
\]

\(\hat{\theta}\) is the Maximum Likelihood estimate if \(L(\hat{\theta}) > L(\theta_0)\) for all values of \(\theta_0\) in the parameter space.

\hypertarget{motivation-for-mle}{%
\subsection{Motivation for MLE}\label{motivation-for-mle}}

Suppose we know the conditional distribution of y given x:

\[
f_{Y|X}(y,x;\theta)
\]

where \(\theta\) is the unknown parameter of distribution. Sometimes we are only concerned with the unconditional distribution \(f_{Y}(y;\theta)\)

Then given a sample of iid data, we can calculate the joint distribution of the entire sample,

\[
f_{Y_1,...,Y_n|X_1,...,X_n(y_1,...y_n,x_1,...,x_n;\theta)}= \prod_{i=1}^{n}f_{Y|X}(y_i,x_i;\theta)
\]

The joint distribution evaluated at the sample is the likelihood (probability) that we observed this particular sample (depends on \(\theta\))

Idea for MLE: Given a sample, we choose our estimates of the parameters that gives the highest likelihood (probability) of observing our particular sample

\[
max_{\theta} \prod_{i=1}^{n}f_{Y|X}(y_i,x_i; \theta)
\]

Equivalently,

\[
max_{\theta} \prod_{i=1}^{n} ln(f_{Y|X}(y_i,x_i; \theta))
\]

Solving for the Maximum Likelihood Estimator

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Solve First Order Condition
\end{enumerate}

\[
\frac{\partial}{\partial \theta}\sum_{i=1}^{n} ln(f_{Y|X}(y_i,x_i;\hat{\theta}_{MLE})) = 0
\]

where \(\hat{\theta}_{MLE}\) is defined.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Evaluate Second Order Condition
\end{enumerate}

\[
\frac{\partial^2}{\partial \theta^2} \sum_{i=1}^{n} ln(f_{Y|X}(y_i,x_i;\hat{\theta}_{MLE})) < 0
\]

where the above condition ensures we can solve for a maximum

Examples:\\
Unconditional Poisson Distribution: Number of products ordered on Amazon within an hour, number of website visits a day for a political campaign.

Exponential Distribution: Length of time until an earthquake occurs, length of time a car battery lasts.

\[
\begin{aligned}
f_{Y|X}(y,x;\theta) &= exp(-y/x\theta)/x\theta \\
f_{Y_1,..Y_n|X_1,...,X_n(y_1,...,y_n,x_1,...,x_n;\theta)} &= \prod_{i=1}^{n}exp(-y_i/x_i \theta)/x_i \theta
\end{aligned}
\]

\hypertarget{assumption}{%
\subsection{Assumption}\label{assumption}}

\begin{itemize}
\item
  \textbf{High Level Regulatory Assumptions} is the sufficient condition used to show large sample properties

  \begin{itemize}
  \tightlist
  \item
    Hence, for each MLE, we will need to either assume or verify if the regulatory assumption holds.
  \end{itemize}
\item
  observations are independent and have the same density function.
\item
  Under multivariate normal assumption, maximum likelihood yields consistent estimates of the means and the covariance matrix for multivariate distribution with finite fourth moments \citep{little1988test}
\end{itemize}

To find the MLE, we usually differentiate the \textbf{log-likelihood} function and set it equal to 0.

\[
\frac{d}{d\theta}l(\theta) = 0 
\]

This is the \textbf{score} equation

Our confidence in the MLE is quantified by the ``pointedness'' of the log-likelihood

\[
I_O(\theta)= \frac{d^2}{d\theta^2}l(\theta) = 0 
\]

called the \textbf{observed information}

while

\[
I(\theta)=E[I_O(\theta;Y)]
\]

is the expected information. (also known as Fisher Information). which we base our variance of the estimator.

\[
V(\hat{\Theta}) \approx I(\theta)^{-1}
\]

\textbf{Consistency} of MLE\\
Suppose that \(y_i\) and \(x_i\) are iid drawn from the true conditional pdf \(f_{Y|X}(y_i,x_i;\theta_0)\). If the following regulatory assumptions hold,

\begin{itemize}
\item
  R1: If \(\theta \neq \theta_0\) then \(f_{Y|X}(y_i,x_i;\theta) \neq f_{Y|X}(y_i,x_i;\theta_0)\)
\item
  R2: The set \(\Theta\) that contains the true parameters \(\theta_0\) is compact
\item
  R3: The log-likelihood \(ln(f_{Y|X}(y_i,x_i;\theta_0))\) is continuous at each \(\theta\) with probability 1
\item
  R4: \(E(sup_{\theta \in \Theta}|ln(f_{Y|X}(y_i,x_i;\theta_0))|)\)
\end{itemize}

then the MLE estimator is consistent,

\[
\hat{\theta}_{MLE} \to^p \theta_0
\]

\textbf{Asymptotic Normality} of MLE

Suppose that \(y_1\) and \(x_i\) are iid drawn from the true conditional pdf \(f_{Y|X}(y_i,x_i;\theta)\). If R1-R4 and the following hold

\begin{itemize}
\item
  R5: \(\theta_0\) is in the interior of the set \(\Theta\)
\item
  R6: \(f_{Y|X}(y_i,x_i;\theta)\) is twice continuously differentiable in \(\theta\) and \(f_{Y|X}(y_i,x_i;\theta) >0\) for a neighborhood \(N \in \Theta\) around \(\theta_0\)
\item
  R7: \(\int sup_{\theta \in N}||\partial f_{Y|X}(y_i,x_i;\theta)\partial\theta||d(y,x) <\infty\), \(\int sup_{\theta \in N} || \partial^2 f_{Y|X}(y_i,x_i;\theta)/\partial \theta \partial \theta' || d(y,x) < \infty\) and \(E(sup_{\theta \in N} || \partial^2ln(f_{Y|X}(y_i,x_i;\theta)) / \partial \theta \partial \theta' ||) < \infty\)
\item
  R8: The information matrix \(I(\theta_0) = Var(\partial f_{Y|X}(y,x_i; \theta_0)/\partial \theta)\) exists and is non-singular
\end{itemize}

then the MLE estimator is asymptotically normal,

\[
\sqrt{n}(\hat{\theta}_{MLE} - \theta_0) \to^d N(0,I(\theta_0)^{-1})
\]

\hypertarget{properties}{%
\subsection{Properties}\label{properties}}

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\item
  Consistent: estimates are approximately unbiased in large samples
\item
  Asymptotically efficient: approximately smaller standard errors compared to other estimator
\item
  Asymptotically normal: with repeated sampling, the estimates will have an approximately normal distribution. Suppose that \(\hat{\theta}_n\) is the MLE for \(\theta\) based on n independent observations. then \(\hat{\theta}_n \sim N(\theta,H^{-1})\).\\

  \begin{itemize}
  \tightlist
  \item
    where H is called the Fisher information matrix. It contains the expected values of the second partial derivatives of the log-likelihood function. The (i.j)th element of H is \(-E(\frac{\partial^2l(\theta)}{\partial \theta_i \partial \theta_j})\)\\
  \item
    We can estimate H by finding the form determined above, and evaluating it at \(\theta = \hat{\theta}_n\)
  \end{itemize}
\item
  Invariance: MLE for \(g(\theta) = g(\hat{\theta})\) for any function g(.)
\end{enumerate}

\[
\hat{\Theta} \approx^d (\theta,I(\hat{\theta)^{-1}}))
\]

Explicit vs Implicit MLE

\begin{itemize}
\tightlist
\item
  If we solve the score equation to get an expression of MLE, then it's called \textbf{explicit}
\item
  If there is no closed form for MLE, and we need some algorithms to derive its expression, it's called \textbf{implicit}
\end{itemize}

\textbf{Large Sample Property} of MLE

Implicit in these theorems is the assumption that we know what the conditional distribution,

\[
f_{Y|X}(y_i,x_i;\theta_0)
\]

but just do now know the exact parameter value.

\begin{itemize}
\tightlist
\item
  Any Distributional mis-specification will result in inconsistent parameter estimates.
\item
  Quasi-MLE: Particular settings/ assumption that allow for certain types of distributional mis-specification (Ex: as long as the distribution is part of particular class or satisfies a particular assumption, then estimating with a wrong distribution will not lead to inconsistent parameter estimates).
\item
  non-parametric/ Semi-parametric estimation: no or very little distributional assumption are made. (hard to implement, derive properties, and interpret)
\end{itemize}

The asymptotic variance of the MLE achieves the \textbf{Cramer-Rao Lower Bound} \citep[\citet{rao1992information}]{cramer1999mathematical}

\begin{itemize}
\tightlist
\item
  The \textbf{Cramer-Rao Lower Bound} is a lower brand for the asymptotic variance of a consistent and asymptotically normally distributed estimator.
\item
  If an estimator achieves the lower bound then it is the most efficient estimator.
\end{itemize}

The maximum Likelihood estimator (assuming the distribution is correctly specified and R1-R8 hold) is the most efficient consistent and asymptotically normal estimator. * most efficient among ALL consistent estimators (not limited to unbiased or linear estimators).

\textbf{Note}

\begin{itemize}
\item
  ML is better choice for binary, strictly positive, count, or inherent heteroskedasticity than linear model.
\item
  ML will assume that we know the conditional distribution of the outcome, and derive an estimator using that information.

  \begin{itemize}
  \tightlist
  \item
    Adds an assumption that we know the distribution (which is similar to \protect\hyperlink{a6-normal-distribution}{A6 Normal Distribution} in linear model)
  \item
    will produce a more efficient estimator.
  \end{itemize}
\end{itemize}

\hypertarget{compare-to-ols}{%
\subsection{Compare to OLS}\label{compare-to-ols}}

MLE is not a cure for most of OLS problems:

\begin{itemize}
\tightlist
\item
  To do joint inference in MLE, we typically use log-likelihood calculation, instead of F-score
\item
  Functional form affects estimation of MLE and OLS.
\item
  Perfect Collinearity/Multicollinearity: highly correlated are likely to yield large standard errors.
\item
  Endogeneity (Omitted variables bias, Simultaneous equations): Like OLS, MLE is also biased against this problem
\end{itemize}

\hypertarget{application}{%
\subsection{Application}\label{application}}

Other applications of MLE

\begin{itemize}
\item
  Corner Solution

  \begin{itemize}
  \tightlist
  \item
    Ex: hours worked, donations to charity
  \item
    Estimate with Tobit
  \end{itemize}
\item
  Non-negative count

  \begin{itemize}
  \tightlist
  \item
    Ex: Numbers of arrest, Number of cigarettes smoked a day
  \item
    Estimate with Poisson regression
  \end{itemize}
\item
  Multinomial Choice

  \begin{itemize}
  \tightlist
  \item
    Ex: Demand for cars, votes for primary election
  \item
    Estimate with mutinomial probit or logit
  \end{itemize}
\item
  Ordinal Choice

  \begin{itemize}
  \tightlist
  \item
    Ex: Levels of Happiness, Levels of Income
  \item
    Ordered Probit
  \end{itemize}
\end{itemize}

Model for binary Response\\
A binary variable will have a \protect\hyperlink{bernoulli}{Bernoulli} distribution:

\[
f_Y(y_i;p) = p^{y_i}(1-p)^{(1-y_i)}
\]

where \(p\) is the probability of success. The conditional distribution is:

\[
f_{Y|X}(y_i,x_i;p(.)) = p(x_i)^{y_i} (1-p(x_i))^{(1-y_i)}
\]

So choose \(p(x_i)\) to be a reasonable function of \(x_i\) and unknown parameters \(\theta\)

We can use \textbf{latent variable model} as probability functions

\[
\begin{aligned}
y_i &= 1\{y_i^* > 0 \}  \\
y_i^* &= x_i \beta-\epsilon_i
\end{aligned}
\]

\begin{itemize}
\tightlist
\item
  \(y_i^*\) is a latent variable (unobserved) that is not well-defined in terms of units/magnitudes
\item
  \(\epsilon_i\) is a mean 0 unobserved random variable.
\end{itemize}

We can rewrite the model without the latent variable,

\[
y_i = 1\{x_i beta > \epsilon_i \}
\]

Then the probability function,

\[
\begin{aligned}
p(x_i) &= P(y_i = 1|x_i) \\
&= P(x_i \beta > \epsilon_i | x_i) \\
&= F_{\epsilon|X}(x_i \beta | x_i)
\end{aligned}
\]

then we need to choose a conditional distribution for \(\epsilon_i\). Hence, we can make additional strong independence assumption

\(\epsilon_i\) is independent of \(x_i\)

Then the probability function is simply,

\[
p(x_i) = F_\epsilon(x_i \beta)
\]

The probability function is also the conditional expectation function,

\[
E(y_i | x_i) = P(y_i = 1|x_i) = F_\epsilon (x_i \beta)
\]

so we allow the conditional expectation function to be non-linear.

Common distributional assumption

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Probit}: Assume \(\epsilon_i\) is standard normally distributed, then \(F_\epsilon(.) = \Phi(.)\) is the standard normal CDF.\\
\item
  \textbf{Logit}: Assume \(\epsilon_i\) is standard logistically distributed, then \(F_\epsilon(.) = \Lambda(.)\) is the standard normal CDF.
\end{enumerate}

Step to derive

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Choose a distribution (normal or logistic) and plug into the following log likelihood,
\end{enumerate}

\[
ln(f_{Y|X} (y_i , x_i; \beta)) = y_i ln(F_\epsilon(x_i \beta)) + (1-y_i)ln(1-F_\epsilon(x_i \beta))
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Solve the MLE by finding the Maximum of
\end{enumerate}

\[
\hat{\beta}_{MLE} = argmax \sum_{i=1}^{n}ln(f_{Y|X}(y_i,x_i; \beta))
\]

\textbf{Properties} of the Probit and Logit Estimators

\begin{itemize}
\item
  Probit or Logit is consistent and asymptotically normal if

  \begin{itemize}
  \tightlist
  \item
    \protect\hyperlink{a2-full-rank}{A2 Full rank} holds: \(E(x_i' x_i)\) exists and is non-singular
  \item
    \protect\hyperlink{a5-data-generation-random-sampling}{A5 Data Generation (random Sampling)} (or \protect\hyperlink{a5a}{A5a}) holds: \{y\_i,x\_i\} are iid (or stationary and weakly dependent).
  \item
    Distributional assumptions on \(\epsilon_i\) hold: Normal/Logistic and independent of \(x_i\)
  \end{itemize}
\item
  Under the same assumptions, Probit or Logit is also asymptotically efficient with asymptotic variance,
\end{itemize}

\[
I(\beta_0)^{-1} = [E(\frac{(f_\epsilon(x_i \beta_0))^2}{F_\epsilon(x_i\beta_0)(1-F_\epsilon(x_i\beta_0))}x_i' x_i)]^{-1}
\]

where \(F_\epsilon(x_i\beta_0)\) is the probability density function (derivative of the CDF)

\hypertarget{interpretation}{%
\subsubsection{Interpretation}\label{interpretation}}

\(\beta\) is the average response in the latent variable associated with a change in \(x_i\)

\begin{itemize}
\tightlist
\item
  Magnitudes do not have meaning
\item
  Direction does have meaning
\end{itemize}

The \textbf{partial effect} for a Non-linear binary response model

\[
\begin{aligned}
E(y_i |x_i) &= F_\epsilon (x_i \beta) \\
PE(x_{ij}) &= \frac{\partial E(y_i |x_i)}{\partial x_{ij}} = f_\epsilon (x_i \beta)\beta_j
\end{aligned}
\]

\begin{itemize}
\tightlist
\item
  The partial effect is the coefficient parameter \(\beta_j\) multiplied by a scaling factor \(f_\epsilon (x_i \beta)\)\\
\item
  The scaling factor depends on \(x_i\) so the partial effect changes depending on what \(x_i\) is
\end{itemize}

Single value for the partial effect

\begin{itemize}
\tightlist
\item
  \textbf{Partial Effect at the Average (PEA)} is the partial effect for an average individual
\end{itemize}

\[
f_{\epsilon}(\bar{x}\hat{\beta})\hat{\beta}_j
\]

\begin{itemize}
\tightlist
\item
  \textbf{Average Partial Effect (APE)} is the average of all partial effect for each individual.
\end{itemize}

\[
\frac{1}{n}\sum_{i=1}^{n}f_\epsilon(x_i \hat{\beta})\hat{\beta}_j
\]

In the linear model, \(APE = PEA\).

In a non-linear model (e.g., binary response), \(APE \neq PEA\)

\hypertarget{non-linear-regression}{%
\chapter{Non-linear Regression}\label{non-linear-regression}}

\textbf{Definition}: models in which the derivatives of the mean function with respect to the parameters depend on one or more of the parameters.

To approximate data, we can approximate the function

\begin{itemize}
\tightlist
\item
  by a high-order polynomial
\item
  by a linear model (e.g., a Taylor expansion around \(X\)'s)
\item
  a collection of locally linear models or basis function
\end{itemize}

but it would not easy to interpret, or not enough data, or can't interpret them globally.

\textbf{intrinsically nonlinear} models:

\[
Y_i = f(\mathbf{x_i;\theta}) + \epsilon_i
\]

where \(f(\mathbf{x_i;\theta})\) is a nonlinear function relating \(E(Y_i)\) to the independent variables \(x_i\)

\begin{itemize}
\tightlist
\item
  \(\mathbf{x}_i\) is a \(k \times 1\) vector of independent variables (fixed).
\item
  \(\mathbf{\theta}\) is a \(p \times 1\) vector of parameters.
\item
  \(\epsilon_i\)s are iid variables mean 0 and variance \(\sigma^2\). (sometimes it's normal).
\end{itemize}

\hypertarget{inference-1}{%
\section{Inference}\label{inference-1}}

Since \(Y_i = f(\mathbf{x}_i,\theta) + \epsilon_i\), where \(\epsilon_i \sim iid(0,\sigma^2)\), we can obtain \(\hat{\theta}\) by minimizing \(\sum_{i=1}^{n}(Y_i - f(x_i,\theta))^2\) and estimate \(s^2 = \hat{\sigma}^2_{\epsilon}=\frac{\sum_{i=1}^{n}(Y_i - f(x_i,\theta))^2}{n-p}\)

\hypertarget{linear-function-of-the-parameters}{%
\subsection{Linear Function of the Parameters}\label{linear-function-of-the-parameters}}

If we assume \(\epsilon_i \sim N(0,\sigma^2)\), then

\[
\hat{\theta} \sim AN(\mathbf{\theta},\sigma^2[\mathbf{F}(\theta)'\mathbf{F}(\theta)]^{-1})
\]

where AN = asymptotic normality

Asymptotic means we have enough data to make inference (As your sample size increases, this becomes more and more accurate (to the true value)).

Since we want to do inference on linear combinations of parameters or contrasts.

If we have \(\mathbf{\theta} = (\theta_0,\theta_1,\theta_2)'\) and we want to look at \(\theta_1 - \theta_2\); we can define vector \(\mathbf{a} = (0,1,-1)'\), consider inference for \(\mathbf{a'\theta}\)

Rules for expectation and variance of a fixed vector \(\mathbf{a}\) and random vector \(\mathbf{Z}\);

\[
\begin{aligned}
E(\mathbf{a'Z}) &= \mathbf{a'}E(\mathbf{Z}) \\
var(\mathbf{a'Z}) &= \mathbf{a'}var(\mathbf{Z}) \mathbf{a}
\end{aligned}
\]

Then,

\[
\mathbf{a'\hat{\theta}} \sim AN(\mathbf{a'\theta},\sigma^2\mathbf{a'[F(\theta)'F(\theta)]^{-1}a})
\]

and \(\mathbf{a'\hat{\theta}}\) is asymptotically independent of \(s^2\) (to order 1/n) then

\[
\frac{\mathbf{a'\hat{\theta}-a'\theta}}{s(\mathbf{a'[F(\theta)'F(\theta)]^{-1}a})^{1/2}} \sim t_{n-p}
\]

to construct \(100(1-\alpha)\%\) confidence interval for \(\mathbf{a'\theta}\)

\[
\mathbf{a'\theta} \pm t_{(1-\alpha/2,n-p)}s(\mathbf{a'[F(\theta)'F(\theta)]^{-1}a})^{1/2}
\]

Suppose \(\mathbf{a'} = (0,...,j,...,0)\). Then, a confidence interval for the \(j\)-th element of \(\mathbf{\theta}\) is

\[
\hat{\theta}_j \pm t_{(1-\alpha/2,n-p)}s\sqrt{\hat{c}^{j}}
\]

where \(\hat{c}^{j}\) is the \(j\)-th diagonal element of \([\mathbf{F(\hat{\theta})'F(\hat{\theta})}]^{-1}\)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#set a seed value}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{23}\NormalTok{)}

\CommentTok{\#Generate x as 100 integers using seq function}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{)}

\CommentTok{\#Generate y as a*e\^{}(bx)+c}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{20}\NormalTok{) }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(}\FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{0.005}\NormalTok{, }\FloatTok{0.075}\NormalTok{) }\SpecialCharTok{*}\NormalTok{ x) }\SpecialCharTok{+} \FunctionTok{runif}\NormalTok{(}\DecValTok{101}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{)}

\CommentTok{\# visualize}
\FunctionTok{plot}\NormalTok{(x, y)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{06-nonlinear-regession_files/figure-latex/unnamed-chunk-1-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\#define our data frame}
\NormalTok{datf }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(x, y)}

\CommentTok{\#define our model function}
\NormalTok{mod }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(a, b, x)}
\NormalTok{    a }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(b }\SpecialCharTok{*}\NormalTok{ x)}
\end{Highlighting}
\end{Shaded}

In this example, we can get the starting values by using linearized version of the function \(\log y = \log a + b x\). Then, we can fit a linear regression to this and use our estimates as starting values

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#get starting values by linearizing}
\NormalTok{lin\_mod }\OtherTok{=} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(y) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{data =}\NormalTok{ datf)}

\CommentTok{\#convert the a parameter back from the log scale; b is ok}
\NormalTok{astrt }\OtherTok{=} \FunctionTok{exp}\NormalTok{(}\FunctionTok{as.numeric}\NormalTok{(lin\_mod}\SpecialCharTok{$}\NormalTok{coef[}\DecValTok{1}\NormalTok{]))}
\NormalTok{bstrt }\OtherTok{=} \FunctionTok{as.numeric}\NormalTok{(lin\_mod}\SpecialCharTok{$}\NormalTok{coef[}\DecValTok{2}\NormalTok{])}
\FunctionTok{print}\NormalTok{(}\FunctionTok{c}\NormalTok{(astrt, bstrt))}
\CommentTok{\#\textgreater{} [1] 14.07964761  0.01855635}
\end{Highlighting}
\end{Shaded}

with \texttt{nls}, we can fit the nonlinear model via least squares

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nlin\_mod }\OtherTok{=} \FunctionTok{nls}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}} \FunctionTok{mod}\NormalTok{(a, b, x),}
               \AttributeTok{start =} \FunctionTok{list}\NormalTok{(}\AttributeTok{a =}\NormalTok{ astrt, }\AttributeTok{b =}\NormalTok{ bstrt),}
               \AttributeTok{data =}\NormalTok{ datf)}

\CommentTok{\#look at model fit summary}
\FunctionTok{summary}\NormalTok{(nlin\_mod)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Formula: y \textasciitilde{} mod(a, b, x)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Parameters:}
\CommentTok{\#\textgreater{}    Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} a 13.603909   0.165390   82.25   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} b  0.019110   0.000153  124.90   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 1.542 on 99 degrees of freedom}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of iterations to convergence: 3 }
\CommentTok{\#\textgreater{} Achieved convergence tolerance: 7.006e{-}07}

\CommentTok{\#add prediction to plot}
\FunctionTok{plot}\NormalTok{(x, y)}
\FunctionTok{lines}\NormalTok{(x, }\FunctionTok{predict}\NormalTok{(nlin\_mod), }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{06-nonlinear-regession_files/figure-latex/unnamed-chunk-3-1} \end{center}

\hypertarget{nonlinear}{%
\subsection{Nonlinear}\label{nonlinear}}

Suppose that \(h(\theta)\) is a nonlinear function of the parameters. We can use Taylor series about \(\theta\)

\[
h(\hat{\theta}) \approx h(\theta) + \mathbf{h}'[\hat{\theta}-\theta]
\]

where \(\mathbf{h} = (\frac{\partial h}{\partial \theta_1},...,\frac{\partial h}{\partial \theta_p})'\)

with

\[
\begin{aligned}
E( \hat{\theta}) &\approx \theta \\
var(\hat{\theta}) &\approx  \sigma^2[\mathbf{F(\theta)'F(\theta)}]^{-1} \\
E(h(\hat{\theta})) &\approx h(\theta) \\
var(h(\hat{\theta})) &\approx \sigma^2 \mathbf{h'[F(\theta)'F(\theta)]^{-1}h}
\end{aligned}
\]

Thus,

\[
h(\hat{\theta}) \sim AN(h(\theta),\sigma^2\mathbf{h'[F(\theta)'F(\theta)]^{-1}h})
\]

and an approximate \(100(1-\alpha)\%\) confidence interval for \(h(\theta)\) is

\[
h(\hat{\theta}) \pm t_{(1-\alpha/2;n-p)}s(\mathbf{h'[F(\theta)'F(\theta)]^{-1}h})^{1/2}
\]

where \(\mathbf{h}\) and \(\mathbf{F}(\theta)\) are evaluated at \(\hat{\theta}\)

Regarding \textbf{prediction interval} for Y at \(x=x_0\)

\[
\begin{aligned}
Y_0 &= f(x_0;\theta) + \epsilon_0, \epsilon_0 \sim N(0,\sigma^2) \\
\hat{Y}_0 &= f(x_0,\hat{\theta})
\end{aligned}
\]

As \(n \to \infty\), \(\hat{\theta} \to \theta\), so we

\[
f(x_0, \hat{\theta}) \approx f(x_0,\theta) + \mathbf{f}_0(\mathbf{\theta})'[\hat{\theta}-\theta]
\]

where

\[
f_0(\theta)= (\frac{\partial f(x_0,\theta)}{\partial \theta_1},..,\frac{\partial f(x_0,\theta)}{\partial \theta_p})'
\]

(note: this \(f_0(\theta)\) is different from \(f(\theta)\)).

\[
\begin{aligned}
Y_0 - \hat{Y}_0 &\approx Y_0  - f(x_0,\theta) - f_0(\theta)'[\hat{\theta}-\theta]  \\
&= \epsilon_0 - f_0(\theta)'[\hat{\theta}-\theta]
\end{aligned}
\]

\[
\begin{aligned}
E(Y_0 - \hat{Y}_0) &\approx E(\epsilon_0)E(\hat{\theta}-\theta) = 0 \\
var(Y_0 - \hat{Y}_0) &\approx var(\epsilon_0 - \mathbf{(f_0(\theta)'[\hat{\theta}-\theta])}) \\
&= \sigma^2 + \sigma^2 \mathbf{f_0 (\theta)'[F(\theta)'F(\theta)]^{-1}f_0(\theta)} \\
&= \sigma^2 (1 + \mathbf{f_0 (\theta)'[F(\theta)'F(\theta)]^{-1}f_0(\theta)})
\end{aligned}
\]

Hence, combining

\[
Y_0 - \hat{Y}_0 \sim AN (0,\sigma^2 (1 + \mathbf{f_0 (\theta)'[F(\theta)'F(\theta)]^{-1}f_0(\theta)}))
\]

\textbf{Note}:

Confidence intervals for the mean response \(Y_i\) (which is different from prediction intervals) can be obtained similarly.

\hypertarget{non-linear-least-squares}{%
\section{Non-linear Least Squares}\label{non-linear-least-squares}}

\begin{itemize}
\tightlist
\item
  The LS estimate of \(\theta\), \(\hat{\theta}\) is the set of parameters that minimizes the residual sum of squares:\\
  \[
  S(\hat{\theta}) = SSE(\hat{\theta}) = \sum_{i=1}^{n}\{Y_i - f(\mathbf{x_i};\hat{\theta})\}^2
  \]
\item
  to obtain the solution, we can consider the partial derivatives of \(S(\theta)\) with respect to each \(\theta_j\) and set them to 0, which gives a system of p equations. Each normal equation is \[
  \frac{\partial S(\theta)}{\partial \theta_j} = -2\sum_{i=1}^{n}\{Y_i -f(\mathbf{x}_i;\theta)\}[\frac{\partial(\mathbf{x}_i;\theta)}{\partial \theta_j}] = 0
  \]
\item
  but we can't obtain a solution directly/analytically for this equation.
\end{itemize}

\textbf{Numerical Solutions}

\begin{itemize}
\item
  Grid search

  \begin{itemize}
  \tightlist
  \item
    A ``grid'' of possible parameter values and see which one minimize the residual sum of squares.
  \item
    finer grid = greater accuracy
  \item
    could be inefficient, and hard when p is large.
  \end{itemize}
\item
  Gauss-Newton Algorithm

  \begin{itemize}
  \tightlist
  \item
    we have an initial estimate of \(\theta\) denoted as \(\hat{\theta}^{(0)}\)
  \item
    use a Taylor expansions of \(f(\mathbf{x}_i;\theta)\) as a function of \(\theta\) about the point \(\hat{\theta}^{(0)}\)
  \end{itemize}
\end{itemize}

\[
\begin{aligned} 
Y_i &= f(x_i;\theta) + \epsilon_i \\
&= f(x_i;\theta) + \sum_{j=1}^{p}\{\frac{\partial f(x_i;\theta)}{\partial \theta_j}\}_{\theta = \hat{\theta}^{(0)}} (\theta_j - \hat{\theta}^{(0)}) + \text{remainder} + \epsilon_i
\end{aligned}
\]

Equivalently,

In matrix notation,

\[
\mathbf{Y} = 
\left[ 
\begin{array}
{c}
Y_1 \\
. \\
Y_n
\end{array} 
\right]
\]

\[
\mathbf{f}(\hat{\theta}^{(0)}) =
\left[ 
\begin{array}
{c}
f(\mathbf{x_1,\hat{\theta}}^{(0)}) \\
. \\
f(\mathbf{x_n,\hat{\theta}}^{(0)})
\end{array} 
\right]
\]

\[
\mathbf{\epsilon} = 
\left[ 
\begin{array}
{c}
\epsilon_1 \\
. \\
\epsilon_n
\end{array} 
\right]
\]

\[
\mathbf{F}(\hat{\theta}^{(0)}) = 
\left[ 
\begin{array}
{ccc}
\frac{\partial f(x_1,\mathbf{\theta})}{\partial \theta_1} & ... & \frac{\partial f(x_1,\mathbf{\theta})}{\partial \theta_p}\\
. & . & . \\
\frac{\partial f(x_n,\mathbf{\theta})}{\partial \theta_1} & ... & \frac{\partial f(x_n,\mathbf{\theta})}{\partial \theta_p}
\end{array} \right]_{\theta = \hat{\theta}^{(0)}}
\]

Hence,

\[
\mathbf{Y} = \mathbf{f}(\hat{\theta}^{(0)}) + \mathbf{F}(\hat{\theta}^{(0)})(\theta - \hat{\theta}^{(0)}) + \epsilon + \text{remainder}
\]

where we assume that the remainder is small and the error term is only assumed to be iid with mean 0 and variance \(\sigma^2\).

We can rewrite the above equation as

\[
\mathbf{Y} - \mathbf{f}(\hat{\theta}^{(0)}) \approx \mathbf{F}(\hat{\theta}^{(0)})(\theta - \hat{\theta}^{(0)}) + \epsilon
\]

where it is in the form of linear model. After we solve for \((\theta - \hat{\theta}^{(0)})\) and let it equal to \(\hat{\delta}^{(1)}\)\\
Then we new estimate is given by adding the Gauss increment adjustment to the initial estimate \(\hat{\theta}^{(1)} = \hat{\theta}^{(0)} + \hat{\delta}^{(1)}\)\\
We can repeat this process.

Gauss-Newton Algorithm Steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  initial estimate \(\hat{\theta}^{(0)}\), set j = 0
\item
  Taylor series expansion and calculate \(\mathbf{f}(\hat{\theta}^{(j)})\) and \(\mathbf{F}(\hat{\theta}^{(j)})\)
\item
  Use OLS to get \(\hat{\delta}^{(j+1)}\)
\item
  get the new estimate \(\hat{\theta}^{(j+1)}\), return to step 2
\item
  continue until ``convergence''
\item
  With the final parameter estimate \(\hat{\theta}\), we can estimate \(\sigma^2\) if \(\epsilon \sim (\mathbf{0}, \sigma^2 \mathbf{I})\) by
\end{enumerate}

\[
\hat{\sigma}^2= \frac{1}{n-p}(\mathbf{Y}-\mathbf{f}(x;\hat{\theta}))'(\mathbf{Y}-\mathbf{f}(x;\hat{\theta}))
\]

\textbf{Criteria for convergence}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Minor change in the objective function (SSE = residual sum of squares)\\
  \[
  \frac{|SSE(\hat{\theta}^{(j+1)})-SSE(\hat{\theta}^{(j)})|}{SSE(\hat{\theta}^{(j)})} < \gamma_1
  \]
\item
  Minor change in the parameter estimates\\
  \[
  |\hat{\theta}^{(j+1)}-\hat{\theta}^{(j)}| < \gamma_2
  \]
\item
  ``residual projection'' criterion of \citep{bates1981relative}
\end{enumerate}

\hypertarget{alternative-of-gauss-newton-algorithm}{%
\subsection{Alternative of Gauss-Newton Algorithm}\label{alternative-of-gauss-newton-algorithm}}

\hypertarget{gauss-newton-algorithm}{%
\subsubsection{Gauss-Newton Algorithm}\label{gauss-newton-algorithm}}

Normal equations:

\[
\frac{\partial SSE(\theta)}{\partial \theta} = 2\mathbf{F}(\theta)'[\mathbf{Y}-\mathbf{f}(\theta)]
\]

\[
\begin{aligned}
\hat{\theta}^{(j+1)} &= \hat{\theta}^{(j)} + \hat{\delta}^{(j+1)} \\
&= \hat{\theta}^{(j)} + [\mathbf{F}((\hat{\theta})^{(j)})'\mathbf{F}(\hat{\theta}^{(j)})]^{-1}\mathbf{F}(\hat{\theta})^{(j)} \\
&= \hat{\theta}^{(j)} - \frac{1}{2}[\mathbf{F}(\hat{\theta}^{(j)})'\mathbf{F}(\hat{\theta}^{(j)})]^{-1}\frac{\partial SSE(\hat{\theta}^{(j)})}{\partial \theta}
\end{aligned}
\]

where

\begin{itemize}
\tightlist
\item
  \(\frac{\partial SSE(\hat{\theta}^{(j)})}{\partial \theta}\) is a gradient vector (points in the direction in which the SSE increases most rapidly). This path is known as steepest ascent.\\
\item
  \([\mathbf{F}(\hat{\theta}^{(j)})'\mathbf{F}(\hat{\theta}^{(j)})]^{-1}\) indicates how far to move\\
\item
  \(-1/2\): indicator of the direction of steepest descent.
\end{itemize}

\hypertarget{modified-gauss-newton-algorithm}{%
\subsubsection{Modified Gauss-Newton Algorithm}\label{modified-gauss-newton-algorithm}}

To avoid overstepping (the local min), we can use the modified Gauss-Newton Algorithm. We define a new proposal for \(\theta\)

\[
\hat{\theta}^{(j+1)} = \hat{\theta}^{(j)} + \alpha_j \hat{\delta}^{(j+1)}, 0 < \alpha_j < 1
\]

where

\begin{itemize}
\tightlist
\item
  \(\alpha_j\) (called the ``learning rate''): is used to modify the step length.
\end{itemize}

We could also have \(\alpha \times 1/2\), but typically it is assumed to be absorbed into the learning rate.

A way to choose \(\alpha_j\), we can use \textbf{step halving}

\[
\hat{\theta}^{(j+1)} = \hat{\theta}^{(j)} + \frac{1}{2^k}\hat{\delta}^{(j+1)}
\]

where

\begin{itemize}
\tightlist
\item
  \(k\) is the smallest non-negative integer such that\\
  \[
  SSE(\hat{\theta}^{(j)}+\frac{1}{2^k}\hat{\delta}^{(j+1)}) < SSE(\hat{\theta}^{(j)})
  \] which means we try \(\hat{\delta}^{(j+1)}\), then \(\hat{\delta}^{(j+1)}/2\), \(\hat{\delta}^{(j+1)}/4\), etc.
\end{itemize}

The most general form of the convergence algorithm is

\[
\hat{\theta}^{(j+1)} = \hat{\theta}^{(j)} - \alpha_j \mathbf{A}_j \frac{\partial Q(\hat{\theta}^{(j)})}{\partial \theta} 
\]

where

\begin{itemize}
\tightlist
\item
  \(\mathbf{A}_j\) is a positive definite matrix
\item
  \(\alpha_j\) is the learning rate
\item
  \(\frac{\partial Q(\hat{\theta}^{(j)})}{\partial \theta}\)is the gradient based on some objective function Q (a function of \(\theta\)), which is typically the SSE in nonlinear regression applications (e.g., cross-entropy for classification).
\end{itemize}

Refer back to the \textbf{Modified Gauss-Newton Algorithm}, we can see it is in this form

\[
\hat{\theta}^{(j+1)} =\hat{\theta}^{(j)} - \alpha_j[\mathbf{F}(\hat{\theta}^{(j)})'\mathbf{F}(\hat{\theta}^{(j)})]^{-1}\frac{\partial SSE(\hat{\theta}^{(j)})}{\partial \theta}
\]

where Q = SSE, \([\mathbf{F}(\hat{\theta}^{(j)})'\mathbf{F}(\hat{\theta}^{(j)})]^{-1} = \mathbf{A}\)

\hypertarget{steepest-descent}{%
\subsubsection{Steepest Descent}\label{steepest-descent}}

(also known just ``gradient descent'')

\[
\hat{\theta}^{(j+1)} = \hat{\theta}^{(j)} - \alpha_j \mathbf{I}_{p \times p}\frac{\partial \mathbf{Q}(\hat{\theta}^{(j)})}{\partial \theta}
\]

\begin{itemize}
\tightlist
\item
  slow to converge, moves rapidly initially.
\item
  could be use for starting values
\end{itemize}

\hypertarget{levenberg--marquardt}{%
\subsubsection{Levenberg -Marquardt}\label{levenberg--marquardt}}

\[
\hat{\theta}^{(j+1)} = \hat{\theta}^{(j)} - \alpha_j [\mathbf{F}(\hat{\theta}^{(j)})'\mathbf{F}(\hat{\theta}^{(j)})+ \tau \mathbf{I}_{p \times p}]\frac{\partial \mathbf{Q}(\hat{\theta}^{(j)})}{\partial \theta}
\]

which is a compromise between the \protect\hyperlink{gauss-newton-algorithm}{Gauss-Newton Algorithm} and the \protect\hyperlink{steepest-descent}{Steepest Descent}.

\begin{itemize}
\tightlist
\item
  best when \(\mathbf{F}(\hat{\theta}^{(j)})'\mathbf{F}(\hat{\theta}^{(j)})\) is nearly singular (\(\mathbf{F}(\hat{\theta}^{(j)})\) isn't of full rank)\\
\item
  similar to ridge regression\\
\item
  If \(SSE(\hat{\theta}^{(j+1)}) < SSE(\hat{\theta}^{(j)})\), then \(\tau= \tau/10\) for the next iteration. Otherwise, \(\tau = 10 \tau\)
\end{itemize}

\hypertarget{newton-raphson}{%
\subsubsection{Newton-Raphson}\label{newton-raphson}}

\[
\hat{\theta}^{(j+1)} = \hat{\theta}^{(j)} - \alpha_j [\frac{\partial^2Q(\hat{\theta}^{(j)})}{\partial \theta \partial \theta'}]^{-1}\frac{\partial \mathbf{Q}(\hat{\theta}^{(j)})}{\partial \theta}
\]

The \textbf{Hessian matrix} can be rewritten as:

\[
\frac{ \partial^2Q(\hat{ \theta}^{(j)})}{ \partial \theta \partial \theta'} = 2 \mathbf{F}((\hat{ \theta})^{(j)})' \mathbf{F} ( \hat{\theta}^{(j)}) - 2\sum_{i=1}^{n} [Y_i - f(x_i;\theta)] \frac{\partial^2f(x_i;\theta)}{\partial \theta \partial \theta'}
\]

which contains the same term that \protect\hyperlink{gauss-newton-algorithm}{Gauss-Newton Algorithm}, combined with one containing the second partial derivatives of f(). (methods that require the second derivatives of the objective function are known as ``second-order methods''.)\\
However, the last term \(\frac{\partial^2f(x_i;\theta)}{\partial \theta \partial \theta'}\) can sometimes be non-singular.

\hypertarget{quasi-newton}{%
\subsubsection{Quasi-Newton}\label{quasi-newton}}

update \(\theta\) according to

\[
\hat{\theta}^{(j+1)} = \hat{\theta}^{(j)} - \alpha_j \mathbf{H}_j^{-1}\frac{\partial \mathbf{Q}(\hat{\theta}^{(j)})}{\partial \theta}
\]

where \(H_j\) is a symmetric positive definite approximation to the Hessian, which gets closer as \(j \to \infty\).

\begin{itemize}
\tightlist
\item
  \(\mathbf{H}_j\) is computed iteratively\\
\item
  Among first-order methods (where only first derivatives are required), this method performs best.
\end{itemize}

\hypertarget{derivative-free-methods}{%
\subsubsection{Derivative Free Methods}\label{derivative-free-methods}}

\begin{itemize}
\tightlist
\item
  \textbf{secant Method}: like \protect\hyperlink{gauss-newton-algorithm}{Gauss-Newton Algorithm}, but calculates the derivatives numerically from past iterations.
\item
  \textbf{Simplex Methods}
\item
  \textbf{Genetic Algorithm}
\item
  \textbf{Differential Evolution Algorithms}
\item
  \textbf{Particle Swarm Optimization}
\item
  \textbf{Ant Colony Optimization}
\end{itemize}

\hypertarget{practical-considerations}{%
\subsection{Practical Considerations}\label{practical-considerations}}

To converge, algorithm need good initial estimates.

\begin{itemize}
\item
  Starting values:

  \begin{itemize}
  \tightlist
  \item
    Prior or theoretical info
  \item
    A grid search or a graph of \(SSE(\theta)\)
  \item
    could also use OLS to get starting values.
  \item
    Model interpretation: if you have some idea regarding the form of the objective function, then you can try to guess the initial value.
  \item
    Expected Value Parameterization
  \end{itemize}
\item
  Constrained Parameters: (constraints on parameters like \(\theta_i>a,a< \theta_i <b\))

  \begin{itemize}
  \tightlist
  \item
    fit the model first to see if the converged parameter estimates satisfy the constraints.
  \item
    if they don't satisfy, then try re-parameterizing
  \end{itemize}
\end{itemize}

\hypertarget{failure-to-converge}{%
\subsubsection{Failure to converge}\label{failure-to-converge}}

\begin{itemize}
\tightlist
\item
  \(SSE(\theta)\) may be ``flat'' in a neighborhood of the minimum.
\item
  You can try different or ``better'' starting values.
\item
  Might suggest the model is too complex for the data, might consider simpler model.
\end{itemize}

\hypertarget{convergence-to-a-local-minimum}{%
\subsubsection{Convergence to a Local Minimum}\label{convergence-to-a-local-minimum}}

\begin{itemize}
\tightlist
\item
  Linear least squares has the property that \(SSE(\theta) = \mathbf{(Y-X\beta)'(Y-X\beta)}\), which is quadratic and has a unique minimum (or maximum).
\item
  Nonlinear east squares need not have a unique minimum
\item
  Using different starting values can help
\item
  If the dimension of \(\theta\) is low, graph \(SSE(\theta)\) as a function of \(\theta_i\)
\item
  Different algorithm can help (e.g., genetic algorithm, particle swarm)
\end{itemize}

To converge, algorithms need good initial estimates.

\begin{itemize}
\item
  Starting values:

  \begin{itemize}
  \tightlist
  \item
    prior or theoretical info
  \item
    A grid search or a graph
  \item
    OLS estimates as starting values
  \item
    Model interpretation
  \item
    Expected Value Parameterization
  \end{itemize}
\item
  Constrained Parameters:

  \begin{itemize}
  \tightlist
  \item
    try the model without the constraints first.
  \item
    If the resulted parameter estimates does not satisfy the constraint, try re-parameterizing
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Grid search}
\CommentTok{\# choose grid of a and b values}
\NormalTok{aseq }\OtherTok{=} \FunctionTok{seq}\NormalTok{(}\DecValTok{10}\NormalTok{,}\DecValTok{18}\NormalTok{,.}\DecValTok{2}\NormalTok{)}
\NormalTok{bseq }\OtherTok{=} \FunctionTok{seq}\NormalTok{(.}\DecValTok{001}\NormalTok{,.}\DecValTok{075}\NormalTok{,.}\DecValTok{001}\NormalTok{)}

\NormalTok{na }\OtherTok{=} \FunctionTok{length}\NormalTok{(aseq)}
\NormalTok{nb }\OtherTok{=} \FunctionTok{length}\NormalTok{(bseq)}
\NormalTok{SSout }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{,na}\SpecialCharTok{*}\NormalTok{nb,}\DecValTok{3}\NormalTok{) }\CommentTok{\#matrix to save output}
\NormalTok{cnt }\OtherTok{=} \DecValTok{0}
\ControlFlowTok{for}\NormalTok{ (k }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{na)\{}
   \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{nb)\{}
\NormalTok{      cnt }\OtherTok{=}\NormalTok{ cnt}\SpecialCharTok{+}\DecValTok{1}
\NormalTok{      ypred }\OtherTok{=} \FunctionTok{mod}\NormalTok{(aseq[k],bseq[j],x) }\CommentTok{\#evaluate model w/ these parms}
\NormalTok{      ss }\OtherTok{=} \FunctionTok{sum}\NormalTok{((y}\SpecialCharTok{{-}}\NormalTok{ypred)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)  }\CommentTok{\#this is our SSE objective function}
      \CommentTok{\#save values of a, b, and SSE}
\NormalTok{      SSout[cnt,}\DecValTok{1}\NormalTok{]}\OtherTok{=}\NormalTok{aseq[k]}
\NormalTok{      SSout[cnt,}\DecValTok{2}\NormalTok{]}\OtherTok{=}\NormalTok{bseq[j]}
\NormalTok{      SSout[cnt,}\DecValTok{3}\NormalTok{]}\OtherTok{=}\NormalTok{ss}
\NormalTok{   \}}
\NormalTok{\}}
\CommentTok{\#find minimum SSE and associated a,b values}
\NormalTok{mn\_indx }\OtherTok{=} \FunctionTok{which.min}\NormalTok{(SSout[,}\DecValTok{3}\NormalTok{])}
\NormalTok{astrt }\OtherTok{=}\NormalTok{ SSout[mn\_indx,}\DecValTok{1}\NormalTok{]}
\NormalTok{bstrt }\OtherTok{=}\NormalTok{ SSout[mn\_indx,}\DecValTok{2}\NormalTok{]}
\CommentTok{\#now, run nls function with these starting values}
\NormalTok{nlin\_modG}\OtherTok{=}\FunctionTok{nls}\NormalTok{(y}\SpecialCharTok{\textasciitilde{}}\FunctionTok{mod}\NormalTok{(a,b,x),}\AttributeTok{start=}\FunctionTok{list}\NormalTok{(}\AttributeTok{a=}\NormalTok{astrt,}\AttributeTok{b=}\NormalTok{bstrt)) }

\NormalTok{nlin\_modG}
\CommentTok{\#\textgreater{} Nonlinear regression model}
\CommentTok{\#\textgreater{}   model: y \textasciitilde{} mod(a, b, x)}
\CommentTok{\#\textgreater{}    data: parent.frame()}
\CommentTok{\#\textgreater{}        a        b }
\CommentTok{\#\textgreater{} 13.60391  0.01911 }
\CommentTok{\#\textgreater{}  residual sum{-}of{-}squares: 235.5}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of iterations to convergence: 3 }
\CommentTok{\#\textgreater{} Achieved convergence tolerance: 2.293e{-}07}
\CommentTok{\# Note, the package \textasciigrave{}nls\_multstart\textasciigrave{} will allow you }
\CommentTok{\# to do a grid search without programming your own loop}
\end{Highlighting}
\end{Shaded}

For prediction interval

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plotFit}\NormalTok{(}
\NormalTok{  nlin\_modG,}
  \AttributeTok{interval =} \StringTok{"both"}\NormalTok{,}
  \AttributeTok{pch =} \DecValTok{19}\NormalTok{,}
  \AttributeTok{shade =} \ConstantTok{TRUE}\NormalTok{,}
  \AttributeTok{col.conf =} \StringTok{"skyblue4"}\NormalTok{,}
  \AttributeTok{col.pred =} \StringTok{"lightskyblue2"}\NormalTok{,}
  \AttributeTok{data =}\NormalTok{ datf}
\NormalTok{)  }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{06-nonlinear-regession_files/figure-latex/unnamed-chunk-5-1} \end{center}

Based on the forms of your function, you can also have programmed starting values from \texttt{nls} function (e.e.g, logistic growth, asymptotic regression, etc).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{apropos}\NormalTok{(}\StringTok{"\^{}SS"}\NormalTok{)}
\CommentTok{\#\textgreater{}  [1] "ss"          "SSasymp"     "SSasympOff"  "SSasympOrig" "SSbiexp"    }
\CommentTok{\#\textgreater{}  [6] "SSD"         "SSfol"       "SSfpl"       "SSgompertz"  "SSlogis"    }
\CommentTok{\#\textgreater{} [11] "SSmicmen"    "SSout"       "SSweibull"}
\end{Highlighting}
\end{Shaded}

For example, a logistic growth model:

\[
P = \frac{K}{1+ exp(P_0+ rt)} + \epsilon
\]

where

\begin{itemize}
\tightlist
\item
  P = population at time t
\item
  K = carrying capacity
\item
  r = population growth rate
\end{itemize}

but in \texttt{R} you have slight different parameterization:

\[
P = \frac{asym}{1 + exp(\frac{xmid - t}{scal})}
\]

where

\begin{itemize}
\tightlist
\item
  \(asym\) = carrying capacity
\item
  \(xmid\) = the x value at the inflection point of the curve
\item
  \(scal\) = scaling parameter.
\end{itemize}

Hence, you have

\begin{itemize}
\tightlist
\item
  \(K = asym\)
\item
  \(r = -1/scal\)
\item
  \(P_0 = -rxmid\)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# simulated data}
\NormalTok{time }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{35}\NormalTok{)}
\NormalTok{population }\OtherTok{\textless{}{-}}
    \FunctionTok{c}\NormalTok{(}\FloatTok{2.8}\NormalTok{, }\FloatTok{4.2}\NormalTok{, }\FloatTok{3.5}\NormalTok{, }\FloatTok{6.3}\NormalTok{, }\FloatTok{15.7}\NormalTok{, }\FloatTok{21.3}\NormalTok{, }\FloatTok{23.7}\NormalTok{, }\FloatTok{25.1}\NormalTok{, }\FloatTok{25.8}\NormalTok{, }\FloatTok{25.9}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(time, population, }\AttributeTok{las =} \DecValTok{1}\NormalTok{, }\AttributeTok{pch =} \DecValTok{16}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{06-nonlinear-regession_files/figure-latex/unnamed-chunk-7-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# model fitting}
\NormalTok{logisticModelSS }\OtherTok{\textless{}{-}} \FunctionTok{nls}\NormalTok{(population }\SpecialCharTok{\textasciitilde{}} \FunctionTok{SSlogis}\NormalTok{(time, Asym, xmid, scal))}
\FunctionTok{summary}\NormalTok{(logisticModelSS)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Formula: population \textasciitilde{} SSlogis(time, Asym, xmid, scal)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Parameters:}
\CommentTok{\#\textgreater{}      Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} Asym  25.5029     0.3666   69.56 3.34e{-}11 ***}
\CommentTok{\#\textgreater{} xmid   8.7347     0.3007   29.05 1.48e{-}08 ***}
\CommentTok{\#\textgreater{} scal   3.6353     0.2186   16.63 6.96e{-}07 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 0.6528 on 7 degrees of freedom}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of iterations to convergence: 1 }
\CommentTok{\#\textgreater{} Achieved convergence tolerance: 1.908e{-}06}
\FunctionTok{coef}\NormalTok{(logisticModelSS)}
\CommentTok{\#\textgreater{}      Asym      xmid      scal }
\CommentTok{\#\textgreater{} 25.502890  8.734698  3.635333}
\end{Highlighting}
\end{Shaded}

Other parameterization

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#convert to other parameterization}
\NormalTok{Ks }\OtherTok{=} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{coef}\NormalTok{(logisticModelSS)[}\DecValTok{1}\NormalTok{])}
\NormalTok{rs }\OtherTok{=} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{/} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{coef}\NormalTok{(logisticModelSS)[}\DecValTok{3}\NormalTok{])}
\NormalTok{Pos }\OtherTok{=} \SpecialCharTok{{-}}\NormalTok{rs }\SpecialCharTok{*} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{coef}\NormalTok{(logisticModelSS)[}\DecValTok{2}\NormalTok{])}
\CommentTok{\#let\textquotesingle{}s refit with these parameters}
\NormalTok{logisticModel }\OtherTok{\textless{}{-}}
    \FunctionTok{nls}\NormalTok{(population }\SpecialCharTok{\textasciitilde{}}\NormalTok{ K }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(Po }\SpecialCharTok{+}\NormalTok{ r }\SpecialCharTok{*}\NormalTok{ time)),}
        \AttributeTok{start =} \FunctionTok{list}\NormalTok{(}\AttributeTok{Po =}\NormalTok{ Pos, }\AttributeTok{r =}\NormalTok{ rs, }\AttributeTok{K =}\NormalTok{ Ks))}
\FunctionTok{summary}\NormalTok{(logisticModel)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Formula: population \textasciitilde{} K/(1 + exp(Po + r * time))}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Parameters:}
\CommentTok{\#\textgreater{}    Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} Po  2.40272    0.12702   18.92 2.87e{-}07 ***}
\CommentTok{\#\textgreater{} r  {-}0.27508    0.01654  {-}16.63 6.96e{-}07 ***}
\CommentTok{\#\textgreater{} K  25.50289    0.36665   69.56 3.34e{-}11 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 0.6528 on 7 degrees of freedom}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of iterations to convergence: 0 }
\CommentTok{\#\textgreater{} Achieved convergence tolerance: 1.924e{-}06}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#note: initial values =  solution (highly unusual, but ok)}
\FunctionTok{plot}\NormalTok{(time, population, }\AttributeTok{las =} \DecValTok{1}\NormalTok{, }\AttributeTok{pch =} \DecValTok{16}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(time, }\FunctionTok{predict}\NormalTok{(logisticModel), }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{06-nonlinear-regession_files/figure-latex/unnamed-chunk-9-1} \end{center}

If can also define your own self-starting function if your models are uncommon (built in \texttt{nls})

Example is based on \citep{Schabenberger_2001}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Load data}
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{read.table}\NormalTok{(}\StringTok{"images/dat.txt"}\NormalTok{, }\AttributeTok{header =}\NormalTok{ T)}
\CommentTok{\# plot}
\NormalTok{dat.plot }\OtherTok{\textless{}{-}}
    \FunctionTok{ggplot}\NormalTok{(dat) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}
        \AttributeTok{x =}\NormalTok{ no3,}
        \AttributeTok{y =}\NormalTok{ ryp,}
        \AttributeTok{color =} \FunctionTok{as.factor}\NormalTok{(depth)}
\NormalTok{    )) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{color =} \StringTok{\textquotesingle{}Depth (cm)\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{xlab}\NormalTok{(}\StringTok{\textquotesingle{}Soil NO3\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+} 
    \FunctionTok{ylab}\NormalTok{(}\StringTok{\textquotesingle{}relative yield percent\textquotesingle{}}\NormalTok{)}

\NormalTok{dat.plot}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{06-nonlinear-regession_files/figure-latex/unnamed-chunk-10-1} \end{center}

The suggested model (known as plateau model) is

\[
E(Y_{ij}) = (\beta_{0j} + \beta_{1j}N_{ij})I_{N_{ij}\le \alpha_j} + (\beta_{0j} + \beta_{1j}\alpha_j)I_{N_{ij} > \alpha_j}
\]

where

\begin{itemize}
\tightlist
\item
  N is an observation
\item
  i is a particular observation
\item
  j = 1,2 corresponding to depths (30,60)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#First define model as a function}
\NormalTok{nonlinModel }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(predictor, b0, b1, alpha) \{}
    \FunctionTok{ifelse}\NormalTok{(predictor }\SpecialCharTok{\textless{}=}\NormalTok{ alpha,}
           \CommentTok{\#if observation less than cutoff simple linear model}
\NormalTok{           b0 }\SpecialCharTok{+}\NormalTok{ b1 }\SpecialCharTok{*}\NormalTok{ predictor, }
\NormalTok{           b0 }\SpecialCharTok{+}\NormalTok{ b1 }\SpecialCharTok{*}\NormalTok{ alpha) }\CommentTok{\#otherwise flat line}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

define \texttt{selfStart} function. Because we defined our model to be linear in the first part and then plateau (remain constant) we can use the first half of our predictors (sorted by increasing value) to get an initial estimate for the slope and intercept of the model, and the last predictor value (alpha) can be the starting value for the plateau parameter.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nonlinModelInit }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(mCall, LHS, data) \{}
    \CommentTok{\# sort data by increasing predictor value {-}}
    \CommentTok{\# done so we can just use the low level }
    \CommentTok{\# no3 conc to fit a simple model}
\NormalTok{    xy }\OtherTok{\textless{}{-}} \FunctionTok{sortedXyData}\NormalTok{(mCall[[}\StringTok{\textquotesingle{}predictor\textquotesingle{}}\NormalTok{]], LHS, data)}
\NormalTok{    n }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(xy)}
    \CommentTok{\#For the first half of the data a simple linear model is fit}
\NormalTok{    lmFit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(xy[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(n }\SpecialCharTok{/} \DecValTok{2}\NormalTok{), }\StringTok{\textquotesingle{}y\textquotesingle{}}\NormalTok{] }\SpecialCharTok{\textasciitilde{}}\NormalTok{ xy[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(n }\SpecialCharTok{/} \DecValTok{2}\NormalTok{), }\StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{])}
\NormalTok{    b0 }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(lmFit)[}\DecValTok{1}\NormalTok{]}
\NormalTok{    b1 }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(lmFit)[}\DecValTok{2}\NormalTok{]}
    \CommentTok{\# for the cut off to the flat part select the last }
    \CommentTok{\# x value used in creating linear model}
\NormalTok{    alpha }\OtherTok{\textless{}{-}}\NormalTok{ xy[(n }\SpecialCharTok{/} \DecValTok{2}\NormalTok{), }\StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{]}
\NormalTok{    value }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(b0, b1, alpha)}
    \FunctionTok{names}\NormalTok{(value) }\OtherTok{\textless{}{-}}\NormalTok{ mCall[}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}b0\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}b1\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}alpha\textquotesingle{}}\NormalTok{)]}
\NormalTok{    value}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

combine model and custom function to calculate starting values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SS\_nonlinModel }\OtherTok{\textless{}{-}} \FunctionTok{selfStart}\NormalTok{(nonlinModel,nonlinModelInit,}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}b0\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}b1\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}alpha\textquotesingle{}}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Above code defined model and selfStart now just need to call it for each of the depths}
\NormalTok{sep30\_nls }\OtherTok{\textless{}{-}}
    \FunctionTok{nls}\NormalTok{(ryp }\SpecialCharTok{\textasciitilde{}} \FunctionTok{SS\_nonlinModel}\NormalTok{(}\AttributeTok{predictor =}\NormalTok{ no3, b0, b1, alpha),}
        \AttributeTok{data =}\NormalTok{ dat[dat}\SpecialCharTok{$}\NormalTok{depth }\SpecialCharTok{==} \DecValTok{30}\NormalTok{,])}

\NormalTok{sep60\_nls }\OtherTok{\textless{}{-}}
    \FunctionTok{nls}\NormalTok{(ryp }\SpecialCharTok{\textasciitilde{}} \FunctionTok{SS\_nonlinModel}\NormalTok{(}\AttributeTok{predictor =}\NormalTok{ no3, b0, b1, alpha),}
        \AttributeTok{data =}\NormalTok{ dat[dat}\SpecialCharTok{$}\NormalTok{depth }\SpecialCharTok{==} \DecValTok{60}\NormalTok{,])}

\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\FunctionTok{plotFit}\NormalTok{(}
\NormalTok{    sep30\_nls,}
    \AttributeTok{interval =} \StringTok{"both"}\NormalTok{,}
    \AttributeTok{pch =} \DecValTok{19}\NormalTok{,}
    \AttributeTok{shade =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{col.conf =} \StringTok{"skyblue4"}\NormalTok{,}
    \AttributeTok{col.pred =} \StringTok{"lightskyblue2"}\NormalTok{,}
    \AttributeTok{data =}\NormalTok{ dat[dat}\SpecialCharTok{$}\NormalTok{depth }\SpecialCharTok{==} \DecValTok{30}\NormalTok{,],}
    \AttributeTok{main =} \StringTok{\textquotesingle{}Results 30 cm depth\textquotesingle{}}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{\textquotesingle{}relative yield percent\textquotesingle{}}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{\textquotesingle{}Soil NO3 concentration\textquotesingle{}}\NormalTok{,}
    \AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{120}\NormalTok{)}
\NormalTok{)}
\FunctionTok{plotFit}\NormalTok{(}
\NormalTok{    sep60\_nls,}
    \AttributeTok{interval =} \StringTok{"both"}\NormalTok{,}
    \AttributeTok{pch =} \DecValTok{19}\NormalTok{,}
    \AttributeTok{shade =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{col.conf =} \StringTok{"lightpink4"}\NormalTok{,}
    \AttributeTok{col.pred =} \StringTok{"lightpink2"}\NormalTok{,}
    \AttributeTok{data =}\NormalTok{ dat[dat}\SpecialCharTok{$}\NormalTok{depth }\SpecialCharTok{==} \DecValTok{60}\NormalTok{,],}
    \AttributeTok{main =} \StringTok{\textquotesingle{}Results 60 cm depth\textquotesingle{}}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{\textquotesingle{}relative yield percent\textquotesingle{}}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{\textquotesingle{}Soil NO3 concentration\textquotesingle{}}\NormalTok{,}
    \AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{120}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{06-nonlinear-regession_files/figure-latex/unnamed-chunk-14-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(sep30\_nls)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Formula: ryp \textasciitilde{} SS\_nonlinModel(predictor = no3, b0, b1, alpha)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Parameters:}
\CommentTok{\#\textgreater{}       Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} b0     15.1943     2.9781   5.102 6.89e{-}07 ***}
\CommentTok{\#\textgreater{} b1      3.5760     0.1853  19.297  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} alpha  23.1324     0.5098  45.373  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 8.258 on 237 degrees of freedom}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of iterations to convergence: 6 }
\CommentTok{\#\textgreater{} Achieved convergence tolerance: 3.608e{-}09}
\FunctionTok{summary}\NormalTok{(sep60\_nls)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Formula: ryp \textasciitilde{} SS\_nonlinModel(predictor = no3, b0, b1, alpha)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Parameters:}
\CommentTok{\#\textgreater{}       Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} b0      5.4519     2.9785    1.83   0.0684 .  }
\CommentTok{\#\textgreater{} b1      5.6820     0.2529   22.46   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} alpha  16.2863     0.2818   57.80   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 7.427 on 237 degrees of freedom}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of iterations to convergence: 5 }
\CommentTok{\#\textgreater{} Achieved convergence tolerance: 8.571e{-}09}
\end{Highlighting}
\end{Shaded}

Instead of modeling the depths model separately we model them together - so there is a common slope, intercept, and plateau.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{red\_nls }\OtherTok{\textless{}{-}}
  \FunctionTok{nls}\NormalTok{(ryp }\SpecialCharTok{\textasciitilde{}} \FunctionTok{SS\_nonlinModel}\NormalTok{(}\AttributeTok{predictor =}\NormalTok{ no3, b0, b1, alpha), }
      \AttributeTok{data =}\NormalTok{ dat)}

\FunctionTok{summary}\NormalTok{(red\_nls)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Formula: ryp \textasciitilde{} SS\_nonlinModel(predictor = no3, b0, b1, alpha)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Parameters:}
\CommentTok{\#\textgreater{}       Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} b0      8.7901     2.7688   3.175   0.0016 ** }
\CommentTok{\#\textgreater{} b1      4.8995     0.2207  22.203   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} alpha  18.0333     0.3242  55.630   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 9.13 on 477 degrees of freedom}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of iterations to convergence: 7 }
\CommentTok{\#\textgreater{} Achieved convergence tolerance: 7.126e{-}09}

\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\FunctionTok{plotFit}\NormalTok{(}
\NormalTok{  red\_nls,}
  \AttributeTok{interval =} \StringTok{"both"}\NormalTok{,}
  \AttributeTok{pch =} \DecValTok{19}\NormalTok{,}
  \AttributeTok{shade =} \ConstantTok{TRUE}\NormalTok{,}
  \AttributeTok{col.conf =} \StringTok{"lightblue4"}\NormalTok{,}
  \AttributeTok{col.pred =} \StringTok{"lightblue2"}\NormalTok{,}
  \AttributeTok{data =}\NormalTok{ dat,}
  \AttributeTok{main =} \StringTok{\textquotesingle{}Results combined\textquotesingle{}}\NormalTok{,}
  \AttributeTok{ylab =} \StringTok{\textquotesingle{}relative yield percent\textquotesingle{}}\NormalTok{,}
  \AttributeTok{xlab =} \StringTok{\textquotesingle{}Soil NO3 concentration\textquotesingle{}}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{06-nonlinear-regession_files/figure-latex/reduce-model-1} \end{center}

Examine residual values for the combined model.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(nlstools)}
\CommentTok{\# using nlstools nlsResiduals function to get some quick residual plots}
\CommentTok{\# can also use test.nlsResiduals(resid)}
\CommentTok{\# https://www.rdocumentation.org/packages/nlstools/versions/1.0{-}2}
\NormalTok{resid }\OtherTok{\textless{}{-}} \FunctionTok{nlsResiduals}\NormalTok{(red\_nls)}
\FunctionTok{plot}\NormalTok{(resid)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{06-nonlinear-regession_files/figure-latex/reduce-model-resid-1} \end{center}

can we test whether the parameters for the two soil depth fits are significantly different? To know if the combined model is appropriate, we consider a parameterization where we let the parameters for the 60cm model be equal to the parameters from the 30cm model plus some increment:

\[
\begin{aligned}
\beta_{02} &= \beta_{01} + d_0 \\
\beta_{12} &= \beta_{11} + d_1 \\
\alpha_{2} &= \alpha_{1} + d_a
\end{aligned}
\]

We can implement this in the following function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nonlinModelF }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(predictor,soildep,b01,b11,a1,d0,d1,da)\{}
\NormalTok{   b02 }\OtherTok{=}\NormalTok{ b01 }\SpecialCharTok{+}\NormalTok{ d0 }\CommentTok{\#make 60cm parms = 30cm parms + increment}
\NormalTok{   b12 }\OtherTok{=}\NormalTok{ b11 }\SpecialCharTok{+}\NormalTok{ d1}
\NormalTok{   a2 }\OtherTok{=}\NormalTok{ a1 }\SpecialCharTok{+}\NormalTok{ da}
   
\NormalTok{   y1 }\OtherTok{=} \FunctionTok{ifelse}\NormalTok{(predictor}\SpecialCharTok{\textless{}=}\NormalTok{a1, }
        \CommentTok{\#if observation less than cutoff simple linear model}
\NormalTok{         b01}\SpecialCharTok{+}\NormalTok{b11}\SpecialCharTok{*}\NormalTok{predictor, }
\NormalTok{         b01}\SpecialCharTok{+}\NormalTok{b11}\SpecialCharTok{*}\NormalTok{a1) }\CommentTok{\# otherwise flat line}
\NormalTok{   y2 }\OtherTok{=} \FunctionTok{ifelse}\NormalTok{(predictor}\SpecialCharTok{\textless{}=}\NormalTok{a2, }
\NormalTok{               b02}\SpecialCharTok{+}\NormalTok{b12}\SpecialCharTok{*}\NormalTok{predictor, }
\NormalTok{               b02}\SpecialCharTok{+}\NormalTok{b12}\SpecialCharTok{*}\NormalTok{a2) }
\NormalTok{   y }\OtherTok{=}\NormalTok{  y1}\SpecialCharTok{*}\NormalTok{(soildep }\SpecialCharTok{==} \DecValTok{30}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ y2}\SpecialCharTok{*}\NormalTok{(soildep }\SpecialCharTok{==} \DecValTok{60}\NormalTok{)  }\CommentTok{\#combine models}
   \FunctionTok{return}\NormalTok{(y)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Starting values are easy now because we fit each model individually.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Soil\_full }\OtherTok{=} \FunctionTok{nls}\NormalTok{(}
\NormalTok{    ryp }\SpecialCharTok{\textasciitilde{}} \FunctionTok{nonlinModelF}\NormalTok{(}
        \AttributeTok{predictor =}\NormalTok{ no3,}
        \AttributeTok{soildep =}\NormalTok{ depth,}
\NormalTok{        b01,}
\NormalTok{        b11,}
\NormalTok{        a1,}
\NormalTok{        d0,}
\NormalTok{        d1,}
\NormalTok{        da}
\NormalTok{    ),}
    \AttributeTok{data =}\NormalTok{ dat,}
    \AttributeTok{start =} \FunctionTok{list}\NormalTok{(}
        \AttributeTok{b01 =} \FloatTok{15.2}\NormalTok{,}
        \AttributeTok{b11 =} \FloatTok{3.58}\NormalTok{,}
        \AttributeTok{a1 =} \FloatTok{23.13}\NormalTok{,}
        \AttributeTok{d0 =} \SpecialCharTok{{-}}\FloatTok{9.74}\NormalTok{,}
        \AttributeTok{d1 =} \FloatTok{2.11}\NormalTok{,}
        \AttributeTok{da =} \SpecialCharTok{{-}}\FloatTok{6.85}
\NormalTok{    )}
\NormalTok{)}

\FunctionTok{summary}\NormalTok{(Soil\_full)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Formula: ryp \textasciitilde{} nonlinModelF(predictor = no3, soildep = depth, b01, b11, }
\CommentTok{\#\textgreater{}     a1, d0, d1, da)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Parameters:}
\CommentTok{\#\textgreater{}     Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} b01  15.1943     2.8322   5.365 1.27e{-}07 ***}
\CommentTok{\#\textgreater{} b11   3.5760     0.1762  20.291  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} a1   23.1324     0.4848  47.711  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} d0   {-}9.7424     4.2357  {-}2.300   0.0219 *  }
\CommentTok{\#\textgreater{} d1    2.1060     0.3203   6.575 1.29e{-}10 ***}
\CommentTok{\#\textgreater{} da   {-}6.8461     0.5691 {-}12.030  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 7.854 on 474 degrees of freedom}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of iterations to convergence: 1 }
\CommentTok{\#\textgreater{} Achieved convergence tolerance: 3.742e{-}06}
\end{Highlighting}
\end{Shaded}

So, the increment parameters, \(d_1\),\(d_2\),\(d_a\) are all significantly different from 0, suggesting that we should have two models here.

\hypertarget{modelestimation-adequacy}{%
\subsection{Model/Estimation Adequacy}\label{modelestimation-adequacy}}

\citep{bates1980relative} assess nonlinearity in terms of 2 components of curvature:

\begin{itemize}
\item
  \textbf{Intrinsic nonlinearity}: the degree of bending and twisting in \(f(\theta)\); our estimation approach assumes that the true function is relatively flat (planar) in the neighborhood fo \(\hat{\theta}\), which would not be true if \(f()\) has a lot of ``bending'' in the neighborhood of \(\hat{\theta}\) (independent of parameterization)

  \begin{itemize}
  \item
    If bad, the distribution of residuals will be seriously distorted
  \item
    slow to converge
  \item
    difficult to identify ( could use this function \texttt{rms.curve})
  \item
    Solution:

    \begin{itemize}
    \tightlist
    \item
      could use higher order Taylor expansions estimation
    \item
      Bayesian method
    \end{itemize}
  \end{itemize}
\item
  \textbf{Parameter effects nonlinearity}: degree to which curvature (nonlinearity) is affected by choice of \(\theta\) (data dependent; dependent on parameterization)

  \begin{itemize}
  \tightlist
  \item
    leads to problems with inference on \(\hat{\theta}\)
  \item
    \texttt{rms.curve} in \texttt{MASS} can identify
  \item
    bootstrap-based inference can also be used
  \item
    Solution: try to reparaemterize.
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#check parameter effects and intrinsic curvature}

\NormalTok{modD }\OtherTok{=} \FunctionTok{deriv3}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ a}\SpecialCharTok{*}\FunctionTok{exp}\NormalTok{(b}\SpecialCharTok{*}\NormalTok{x), }\FunctionTok{c}\NormalTok{(}\StringTok{"a"}\NormalTok{,}\StringTok{"b"}\NormalTok{),}\ControlFlowTok{function}\NormalTok{(a,b,x) }\ConstantTok{NULL}\NormalTok{)}

\NormalTok{nlin\_modD }\OtherTok{=} \FunctionTok{nls}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}} \FunctionTok{modD}\NormalTok{(a, b, x),}
                \AttributeTok{start =} \FunctionTok{list}\NormalTok{(}\AttributeTok{a =}\NormalTok{ astrt, }\AttributeTok{b =}\NormalTok{ bstrt),}
                \AttributeTok{data =}\NormalTok{ datf)}

\FunctionTok{rms.curv}\NormalTok{(nlin\_modD)}
\CommentTok{\#\textgreater{} Parameter effects: c\^{}theta x sqrt(F) = 0.0626 }
\CommentTok{\#\textgreater{}         Intrinsic: c\^{}iota  x sqrt(F) = 0.0062}
\end{Highlighting}
\end{Shaded}

In linear model, we have \protect\hyperlink{linear-regression}{Linear Regression}, we have goodness of fit measure as \(R^2\):

\[
\begin{aligned}
R^2 &= \frac{SSR}{SSTO} = 1- \frac{SSE}{SSTO} \\
&= \frac{\sum_{i=1}^n (\hat{Y}_i- \bar{Y})^2}{\sum_{i=1}^n (Y_i- \bar{Y})^2} = 1- \frac{\sum_{i=1}^n ({Y}_i- \hat{Y})^2}{\sum_{i=1}^n (Y_i- \bar{Y})^2}
\end{aligned}
\]

but not valid in the nonlinear case because the error sum of squares and model sum of squares do not add to the total corrected sum of squares

\[
SSR + SSE \neq SST
\]

but we can use pseudo-\(R^2\):

\[
R^2_{pseudo} = 1 - \frac{\sum_{i=1}^n ({Y}_i- \hat{Y})^2}{\sum_{i=1}^n (Y_i- \bar{Y})^2}
\]

But we can't interpret this as the proportion of variability explained by the model. We should use as a relative comparison of different models.

\textbf{Residual Plots}: standardize, similar to OLS. useful when the intrinsic curvature is small:

The studentized residuals

\[
r_i = \frac{e_i}{s\sqrt{1-\hat{c}_i}}
\]

where \(\hat{c}_i\)is the i-th diagonal of \(\mathbf{\hat{H}= F(\hat{\theta})[F(\hat{\theta})'F(\hat{\theta})]^{-1}F(\hat{\theta})'}\)

We could have problems of

\begin{itemize}
\item
  Collinearity: the condition number of \(\mathbf{[F(\hat{\theta})'F(\hat{\theta})]^{-1}}\) should be less than 30. Follow \citep{Magel_1987}; reparameterize if possible
\item
  Leverage: Like \protect\hyperlink{ordinary-least-squares}{OLS}, but consider \(\mathbf{\hat{H}= F(\hat{\theta})[F(\hat{\theta})'F(\hat{\theta})]^{-1}F(\hat{\theta})'}\) (also known as ``tangent plant hat matrix'') \citep{Laurent_1992}
\item
  Heterogeneous Errors: weighted Non-linear Least Squares
\item
  Correlated Errors:

  \begin{itemize}
  \tightlist
  \item
    Generalized Nonlinear Least Squares
  \item
    Nonlinear Mixed Models
  \item
    Bayesian methods
  \end{itemize}
\end{itemize}

\hypertarget{application-1}{%
\subsection{Application}\label{application-1}}

\[
y_i = \frac{\theta_0 + \theta_1 x_i}{1 + \theta_2 \exp(0.4 x_i)} + \epsilon_i
\]

where \(i = 1,..,n\)

Get the starting values

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(my\_data)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{06-nonlinear-regession_files/figure-latex/unnamed-chunk-18-1} \end{center}

We notice that \(Y_{max} = \theta_0 + \theta_1 x_i\) in which we can find x\_i from data

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{max}\NormalTok{(my\_data}\SpecialCharTok{$}\NormalTok{y)}
\CommentTok{\#\textgreater{} [1] 2.6722}
\NormalTok{my\_data}\SpecialCharTok{$}\NormalTok{x[}\FunctionTok{which.max}\NormalTok{(my\_data}\SpecialCharTok{$}\NormalTok{y)]}
\CommentTok{\#\textgreater{} [1] 0.0094}
\end{Highlighting}
\end{Shaded}

hence, \(x = 0.0094\) when \(y = 2.6722\) when we have the first equation as

\[
2.6722 = \theta_0 + 0.0094 \theta_1
\]

\[
\theta_0 + 0.0094 \theta_1 + 0 \theta_2 = 2.6722
\]

Secondly, we notice that we can obtain the ``average'' of y when

\[
1+ \theta_2 exp(0.4 x) = 2
\]

then we can find this average numbers of x and y

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# find mean y}
\FunctionTok{mean}\NormalTok{(my\_data}\SpecialCharTok{$}\NormalTok{y) }
\CommentTok{\#\textgreater{} [1] {-}0.0747864}

\CommentTok{\# find y closest to its mean}
\NormalTok{my\_data}\SpecialCharTok{$}\NormalTok{y[}\FunctionTok{which.min}\NormalTok{(}\FunctionTok{abs}\NormalTok{(my\_data}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{{-}}\NormalTok{ (}\FunctionTok{mean}\NormalTok{(my\_data}\SpecialCharTok{$}\NormalTok{y))))] }
\CommentTok{\#\textgreater{} [1] {-}0.0773}


\CommentTok{\# find x closest to the mean y}
\NormalTok{my\_data}\SpecialCharTok{$}\NormalTok{x[}\FunctionTok{which.min}\NormalTok{(}\FunctionTok{abs}\NormalTok{(my\_data}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{{-}}\NormalTok{ (}\FunctionTok{mean}\NormalTok{(my\_data}\SpecialCharTok{$}\NormalTok{y))))] }
\CommentTok{\#\textgreater{} [1] 11.0648}
\end{Highlighting}
\end{Shaded}

we have the second equation

\[
1 + \theta_2 exp(0.4*11.0648) = 2
\]

\[
0 \theta_1 + 0 \theta_1 + 83.58967 \theta_2 = 1
\]

Thirdly, we can plug in the value of x closest to 1 to find the value of y

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# find value of x closet to 1}
\NormalTok{my\_data}\SpecialCharTok{$}\NormalTok{x[}\FunctionTok{which.min}\NormalTok{(}\FunctionTok{abs}\NormalTok{(my\_data}\SpecialCharTok{$}\NormalTok{x }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{))] }
\CommentTok{\#\textgreater{} [1] 0.9895}

\CommentTok{\# find index of x closest to 1}
\FunctionTok{match}\NormalTok{(my\_data}\SpecialCharTok{$}\NormalTok{x[}\FunctionTok{which.min}\NormalTok{(}\FunctionTok{abs}\NormalTok{(my\_data}\SpecialCharTok{$}\NormalTok{x }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{))], my\_data}\SpecialCharTok{$}\NormalTok{x) }
\CommentTok{\#\textgreater{} [1] 14}

\CommentTok{\# find y value}
\NormalTok{my\_data}\SpecialCharTok{$}\NormalTok{y[}\FunctionTok{match}\NormalTok{(my\_data}\SpecialCharTok{$}\NormalTok{x[}\FunctionTok{which.min}\NormalTok{(}\FunctionTok{abs}\NormalTok{(my\_data}\SpecialCharTok{$}\NormalTok{x }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{))], my\_data}\SpecialCharTok{$}\NormalTok{x)]}
\CommentTok{\#\textgreater{} [1] 1.4577}
\end{Highlighting}
\end{Shaded}

hence we have

\[
1.457 = \frac{\theta_0 + \theta_1*0.9895}{1 + \theta_2 exp(0.4*0.9895)}
\]

\[
1.457 + 2.164479 *\theta_2 = \theta_0 + \theta_1*0.9895
\]

\[
\theta_0 + \theta_1*0.9895 -  2.164479 *\theta_2 = 1.457
\]

with 3 equations, we can solve them to get the starting value for \(\theta_0,\theta_1, \theta_2\)

\[
\theta_0 + 0.0094 \theta_1 + 0 \theta_2 = 2.6722
\]

\[
0 \theta_1 + 0 \theta_1 + 83.58967 \theta_2 = 1
\]

\[
\theta_0 + \theta_1*0.9895 -  2.164479 *\theta_2 = 1.457
\]

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(matlib)}
\NormalTok{A }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}
    \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.0094}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\FloatTok{83.58967}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.9895}\NormalTok{,}\SpecialCharTok{{-}}\FloatTok{2.164479}\NormalTok{),}
    \AttributeTok{nrow =} \DecValTok{3}\NormalTok{,}
    \AttributeTok{ncol =} \DecValTok{3}\NormalTok{,}
    \AttributeTok{byrow =}\NormalTok{ T}
\NormalTok{)}
\NormalTok{b }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FloatTok{2.6722}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{1.457}\NormalTok{)}
\FunctionTok{showEqn}\NormalTok{(A, b)}
\CommentTok{\#\textgreater{} 0*x1 + 0.0094*x2        + 0*x3  =  2.6722 }
\CommentTok{\#\textgreater{} 0*x1      + 0*x2 + 83.58967*x3  =       1 }
\CommentTok{\#\textgreater{} 1*x1 + 0.9895*x2 {-} 2.164479*x3  =   1.457}
\FunctionTok{Solve}\NormalTok{(A, b, }\AttributeTok{fractions =}\NormalTok{ F)}
\CommentTok{\#\textgreater{} x1      =  {-}279.80879739 }
\CommentTok{\#\textgreater{}   x2    =   284.27659574 }
\CommentTok{\#\textgreater{}     x3  =      0.0119632}
\end{Highlighting}
\end{Shaded}

Construct manually \protect\hyperlink{gauss-newton-algorithm}{Gauss-Newton Algorithm}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#starting value}
\NormalTok{theta\_0\_strt }\OtherTok{=} \SpecialCharTok{{-}}\FloatTok{279.80879739}
\NormalTok{theta\_1\_strt }\OtherTok{=}  \FloatTok{284.27659574}
\NormalTok{theta\_2\_strt }\OtherTok{=} \FloatTok{0.0119632}

\CommentTok{\#model}
\NormalTok{mod\_4 }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(theta\_0, theta\_1, theta\_2, x) \{}
\NormalTok{    (theta\_0 }\SpecialCharTok{+}\NormalTok{ theta\_1 }\SpecialCharTok{*}\NormalTok{ x) }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+}\NormalTok{ theta\_2 }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(}\FloatTok{0.4} \SpecialCharTok{*}\NormalTok{ x))}
\NormalTok{\}}

\CommentTok{\#define a function}
\NormalTok{f\_4 }\OtherTok{=} \FunctionTok{expression}\NormalTok{((theta\_0 }\SpecialCharTok{+}\NormalTok{ theta\_1 }\SpecialCharTok{*}\NormalTok{ x) }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+}\NormalTok{ theta\_2 }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(}\FloatTok{0.4} \SpecialCharTok{*}\NormalTok{ x)))}

\CommentTok{\#take the first derivative}
\NormalTok{df\_4.d\_theta\_0 }\OtherTok{=} \FunctionTok{D}\NormalTok{(f\_4, }\StringTok{\textquotesingle{}theta\_0\textquotesingle{}}\NormalTok{)}

\NormalTok{df\_4.d\_theta\_1 }\OtherTok{=} \FunctionTok{D}\NormalTok{(f\_4, }\StringTok{\textquotesingle{}theta\_1\textquotesingle{}}\NormalTok{)}

\NormalTok{df\_4.d\_theta\_2 }\OtherTok{=} \FunctionTok{D}\NormalTok{(f\_4, }\StringTok{\textquotesingle{}theta\_2\textquotesingle{}}\NormalTok{)}

\CommentTok{\# save the result of all iterations}
\NormalTok{theta\_vec }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(theta\_0\_strt, theta\_1\_strt, theta\_2\_strt))}
\NormalTok{delta }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\AttributeTok{nrow =} \DecValTok{3}\NormalTok{, }\AttributeTok{ncol =} \DecValTok{1}\NormalTok{)}

\NormalTok{f\_theta }\OtherTok{=} \FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{eval}\NormalTok{(}
\NormalTok{    f\_4,}
    \FunctionTok{list}\NormalTok{(}
        \AttributeTok{x =}\NormalTok{ my\_data}\SpecialCharTok{$}\NormalTok{x,}
        \AttributeTok{theta\_0 =}\NormalTok{ theta\_vec[}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{],}
        \AttributeTok{theta\_1 =}\NormalTok{ theta\_vec[}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{],}
        \AttributeTok{theta\_2 =}\NormalTok{ theta\_vec[}\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{]}
\NormalTok{    )}
\NormalTok{))}

\NormalTok{i }\OtherTok{=} \DecValTok{1}

\ControlFlowTok{repeat}\NormalTok{ \{}
\NormalTok{    F\_theta\_0 }\OtherTok{=} \FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(}
        \FunctionTok{eval}\NormalTok{(}
\NormalTok{            df\_4.d\_theta\_0,}
            \FunctionTok{list}\NormalTok{(}
                \AttributeTok{x =}\NormalTok{ my\_data}\SpecialCharTok{$}\NormalTok{x,}
                \AttributeTok{theta\_0 =}\NormalTok{ theta\_vec[}\DecValTok{1}\NormalTok{, i],}
                \AttributeTok{theta\_1 =}\NormalTok{ theta\_vec[}\DecValTok{2}\NormalTok{, i],}
                \AttributeTok{theta\_2 =}\NormalTok{ theta\_vec[}\DecValTok{3}\NormalTok{, i]}
\NormalTok{            )}
\NormalTok{        ),}
        \FunctionTok{eval}\NormalTok{(}
\NormalTok{            df\_4.d\_theta\_1,}
            \FunctionTok{list}\NormalTok{(}
                \AttributeTok{x =}\NormalTok{ my\_data}\SpecialCharTok{$}\NormalTok{x,}
                \AttributeTok{theta\_0 =}\NormalTok{ theta\_vec[}\DecValTok{1}\NormalTok{, i],}
                \AttributeTok{theta\_1 =}\NormalTok{ theta\_vec[}\DecValTok{2}\NormalTok{, i],}
                \AttributeTok{theta\_2 =}\NormalTok{ theta\_vec[}\DecValTok{3}\NormalTok{, i]}
\NormalTok{            )}
\NormalTok{        ),}
        \FunctionTok{eval}\NormalTok{(}
\NormalTok{            df\_4.d\_theta\_2,}
            \FunctionTok{list}\NormalTok{(}
                \AttributeTok{x =}\NormalTok{ my\_data}\SpecialCharTok{$}\NormalTok{x,}
                \AttributeTok{theta\_0 =}\NormalTok{ theta\_vec[}\DecValTok{1}\NormalTok{, i],}
                \AttributeTok{theta\_1 =}\NormalTok{ theta\_vec[}\DecValTok{2}\NormalTok{, i],}
                \AttributeTok{theta\_2 =}\NormalTok{ theta\_vec[}\DecValTok{3}\NormalTok{, i]}
\NormalTok{            )}
\NormalTok{        )}
\NormalTok{    ))}
\NormalTok{    delta[, i] }\OtherTok{=}\NormalTok{ (}\FunctionTok{solve}\NormalTok{(}\FunctionTok{t}\NormalTok{(F\_theta\_0) }\SpecialCharTok{\%*\%}\NormalTok{ F\_theta\_0)) }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(F\_theta\_0) }\SpecialCharTok{\%*\%}\NormalTok{ (my\_data}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{{-}}\NormalTok{ f\_theta[, i])}
\NormalTok{    theta\_vec }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(theta\_vec, }\FunctionTok{matrix}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\AttributeTok{nrow =} \DecValTok{3}\NormalTok{, }\AttributeTok{ncol =} \DecValTok{1}\NormalTok{))}
\NormalTok{    theta\_vec[, i }\SpecialCharTok{+} \DecValTok{1}\NormalTok{] }\OtherTok{=}\NormalTok{ theta\_vec[, i] }\SpecialCharTok{+}\NormalTok{ delta[, i]}
\NormalTok{    i }\OtherTok{=}\NormalTok{ i }\SpecialCharTok{+} \DecValTok{1}
    
\NormalTok{    f\_theta }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(f\_theta, }\FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{eval}\NormalTok{(}
\NormalTok{        f\_4,}
        \FunctionTok{list}\NormalTok{(}
            \AttributeTok{x =}\NormalTok{ my\_data}\SpecialCharTok{$}\NormalTok{x,}
            \AttributeTok{theta\_0 =}\NormalTok{ theta\_vec[}\DecValTok{1}\NormalTok{, i],}
            \AttributeTok{theta\_1 =}\NormalTok{ theta\_vec[}\DecValTok{2}\NormalTok{, i],}
            \AttributeTok{theta\_2 =}\NormalTok{ theta\_vec[}\DecValTok{3}\NormalTok{, i]}
\NormalTok{        )}
\NormalTok{    )))}
\NormalTok{    delta }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(delta, }\FunctionTok{matrix}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\AttributeTok{nrow =} \DecValTok{3}\NormalTok{, }\AttributeTok{ncol =} \DecValTok{1}\NormalTok{))}
    
    \CommentTok{\#convergence criteria based on SSE}
    \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{abs}\NormalTok{(}\FunctionTok{sum}\NormalTok{((my\_data}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{{-}}\NormalTok{ f\_theta[, i]) }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{) }\SpecialCharTok{{-}} \FunctionTok{sum}\NormalTok{((my\_data}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{{-}}\NormalTok{ f\_theta[, i }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{]) }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{)) }\SpecialCharTok{/}\NormalTok{ (}\FunctionTok{sum}\NormalTok{((my\_data}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{{-}}\NormalTok{ f\_theta[, i }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{]) }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{)) }\SpecialCharTok{\textless{}} \FloatTok{0.001}\NormalTok{) \{}
        \ControlFlowTok{break}
\NormalTok{    \}}
\NormalTok{\}}
\NormalTok{delta}
\CommentTok{\#\textgreater{}               [,1]        [,2]        [,3]       [,4]       [,5]       [,6]}
\CommentTok{\#\textgreater{} [1,]  2.811840e+02 {-}0.03929013  0.43160654  0.6904856  0.6746748  0.4056460}
\CommentTok{\#\textgreater{} [2,] {-}2.846545e+02  0.03198446 {-}0.16403964 {-}0.2895487 {-}0.2933345 {-}0.1734087}
\CommentTok{\#\textgreater{} [3,] {-}1.804567e{-}05  0.01530258  0.05137285  0.1183271  0.1613129  0.1160404}
\CommentTok{\#\textgreater{}             [,7] [,8]}
\CommentTok{\#\textgreater{} [1,]  0.09517681   NA}
\CommentTok{\#\textgreater{} [2,] {-}0.03928239   NA}
\CommentTok{\#\textgreater{} [3,]  0.03004911   NA}
\NormalTok{theta\_vec}
\CommentTok{\#\textgreater{}              [,1]        [,2]        [,3]        [,4]       [,5]       [,6]}
\CommentTok{\#\textgreater{} [1,] {-}279.8087974  1.37521388  1.33592375  1.76753029  2.4580158  3.1326907}
\CommentTok{\#\textgreater{} [2,]  284.2765957 {-}0.37788712 {-}0.34590266 {-}0.50994230 {-}0.7994910 {-}1.0928255}
\CommentTok{\#\textgreater{} [3,]    0.0119632  0.01194515  0.02724773  0.07862059  0.1969477  0.3582607}
\CommentTok{\#\textgreater{}            [,7]       [,8]}
\CommentTok{\#\textgreater{} [1,]  3.5383367  3.6335135}
\CommentTok{\#\textgreater{} [2,] {-}1.2662342 {-}1.3055166}
\CommentTok{\#\textgreater{} [3,]  0.4743011  0.5043502}

\FunctionTok{head}\NormalTok{(f\_theta)}
\CommentTok{\#\textgreater{}           [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]}
\CommentTok{\#\textgreater{} [1,] {-}273.8482 1.355410 1.297194 1.633802 2.046023 2.296554 2.389041 2.404144}
\CommentTok{\#\textgreater{} [2,] {-}209.0859 1.268192 1.216738 1.514575 1.863098 2.059505 2.126009 2.135969}
\CommentTok{\#\textgreater{} [3,] {-}190.3323 1.242916 1.193433 1.480136 1.810629 1.992095 2.051603 2.060202}
\CommentTok{\#\textgreater{} [4,] {-}177.1891 1.225196 1.177099 1.456024 1.774000 1.945197 1.999945 2.007625}
\CommentTok{\#\textgreater{} [5,] {-}148.5872 1.186618 1.141549 1.403631 1.694715 1.844154 1.888953 1.894730}
\CommentTok{\#\textgreater{} [6,] {-}119.9585 1.147980 1.105961 1.351301 1.615968 1.744450 1.779859 1.783866}

\CommentTok{\# estimate sigma\^{}2}

\NormalTok{sigma2 }\OtherTok{=} \DecValTok{1} \SpecialCharTok{/}\NormalTok{ (}\FunctionTok{nrow}\NormalTok{(my\_data) }\SpecialCharTok{{-}} \DecValTok{3}\NormalTok{) }\SpecialCharTok{*}\NormalTok{ (}\FunctionTok{t}\NormalTok{(my\_data}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{{-}}\NormalTok{ (f\_theta[, }\FunctionTok{ncol}\NormalTok{(f\_theta)]))) }\SpecialCharTok{\%*\%}
\NormalTok{    (my\_data}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{{-}}\NormalTok{ (f\_theta[, }\FunctionTok{ncol}\NormalTok{(f\_theta)])) }\CommentTok{\# p = 3}
\NormalTok{sigma2}
\CommentTok{\#\textgreater{}           [,1]}
\CommentTok{\#\textgreater{} [1,] 0.0801686}
\end{Highlighting}
\end{Shaded}

After 8 iterations, my function has converged. And objective function value at convergence is

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sum}\NormalTok{((my\_data}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{{-}}\NormalTok{ f\_theta[,i])}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 19.80165}
\end{Highlighting}
\end{Shaded}

and the parameters of \(\theta\)s are

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{theta\_vec[,}\FunctionTok{ncol}\NormalTok{(theta\_vec)]}
\CommentTok{\#\textgreater{} [1]  3.6335135 {-}1.3055166  0.5043502}
\end{Highlighting}
\end{Shaded}

and the asymptotic variance covariance matrix is

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{as.numeric}\NormalTok{(sigma2)}\SpecialCharTok{*}\FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{solve}\NormalTok{(}\FunctionTok{crossprod}\NormalTok{(F\_theta\_0)))}
\CommentTok{\#\textgreater{}             [,1]        [,2]        [,3]}
\CommentTok{\#\textgreater{} [1,]  0.11552571 {-}0.04817428  0.02685848}
\CommentTok{\#\textgreater{} [2,] {-}0.04817428  0.02100861 {-}0.01158212}
\CommentTok{\#\textgreater{} [3,]  0.02685848 {-}0.01158212  0.00703916}
\end{Highlighting}
\end{Shaded}

Issue that I encounter in this problem was that it was very sensitive to starting values. when I tried the value of 1 for all \(\theta\)s, I have vastly different parameter estimates. Then, I try to use the model interpretation to try to find reasonable starting values.

Check with predefined function in \texttt{nls}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nlin\_4 }\OtherTok{=} \FunctionTok{nls}\NormalTok{(}
\NormalTok{    y }\SpecialCharTok{\textasciitilde{}} \FunctionTok{mod\_4}\NormalTok{(theta\_0, theta\_1, theta\_2, x),}
    \AttributeTok{start =} \FunctionTok{list}\NormalTok{(}
        \AttributeTok{theta\_0 =} \SpecialCharTok{{-}}\FloatTok{279.80879739}\NormalTok{ ,}
        \AttributeTok{theta\_1 =} \FloatTok{284.27659574}\NormalTok{ ,}
        \AttributeTok{theta\_2 =} \FloatTok{0.0119632}
\NormalTok{    ),}
    \AttributeTok{data =}\NormalTok{ my\_data}
\NormalTok{)}
\NormalTok{nlin\_4}
\CommentTok{\#\textgreater{} Nonlinear regression model}
\CommentTok{\#\textgreater{}   model: y \textasciitilde{} mod\_4(theta\_0, theta\_1, theta\_2, x)}
\CommentTok{\#\textgreater{}    data: my\_data}
\CommentTok{\#\textgreater{} theta\_0 theta\_1 theta\_2 }
\CommentTok{\#\textgreater{}  3.6359 {-}1.3064  0.5053 }
\CommentTok{\#\textgreater{}  residual sum{-}of{-}squares: 19.8}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of iterations to convergence: 9 }
\CommentTok{\#\textgreater{} Achieved convergence tolerance: 2.294e{-}07}
\end{Highlighting}
\end{Shaded}

\hypertarget{generalized-linear-models}{%
\chapter{Generalized Linear Models}\label{generalized-linear-models}}

Even though we call it generalized linear model, it is still under the paradigm of non-linear regression, because the form of the regression model is non-linear. The name generalized linear model derived from the fact that we have \(\mathbf{x'_i \beta}\) (which is linear form) in the model.

\hypertarget{logistic-regression-1}{%
\section{Logistic Regression}\label{logistic-regression-1}}

\[
p_i = f(\mathbf{x}_i ; \beta) = \frac{exp(\mathbf{x_i'\beta})}{1 + exp(\mathbf{x_i'\beta})}
\]

Equivalently,

\[
logit(p_i) = log(\frac{p_i}{1+p_i}) = \mathbf{x_i'\beta}
\]

where \(\frac{p_i}{1+p_i}\)is the \textbf{odds}.

In this form, the model is specified such that \textbf{a function of the mean response is linear}. Hence, \textbf{Generalized Linear Models}

The likelihood function

\[
L(p_i) = \prod_{i=1}^{n} p_i^{Y_i}(1-p_i)^{1-Y_i}
\]

where \(p_i = \frac{\mathbf{x'_i \beta}}{1+\mathbf{x'_i \beta}}\) and \(1-p_i = (1+ exp(\mathbf{x'_i \beta}))^{-1}\)

Hence, our objective function is

\[
Q(\beta) = log(L(\beta)) = \sum_{i=1}^n Y_i \mathbf{x'_i \beta} - \sum_{i=1}^n  log(1+ exp(\mathbf{x'_i \beta}))
\]

we could maximize this function numerically using the optimization method above, which allows us to find numerical MLE for \(\hat{\beta}\). Then we can use the standard asymptotic properties of MLEs to make inference.

Property of MLEs is that parameters are asymptotically unbiased with sample variance-covariance matrix given by the \textbf{inverse Fisher information matrix}

\[
\hat{\beta} \dot{\sim} AN(\beta,[\mathbf{I}(\beta)]^{-1})
\]

where the \textbf{Fisher Information matrix}, \(\mathbf{I}(\beta)\) is

\[
\begin{aligned}
\mathbf{I}(\beta) &= E[\frac{\partial \log(L(\beta))}{\partial (\beta)}\frac{\partial \log(L(\beta))}{\partial \beta'}] \\
&= E[(\frac{\partial \log(L(\beta))}{\partial \beta_i} \frac{\partial \log(L(\beta))}{\partial \beta_j})_{ij}]
\end{aligned}
\]

Under \textbf{regularity conditions}, this is equivalent to the negative of the expected value of the Hessian Matrix

\[
\begin{aligned}
\mathbf{I}(\beta) &= -E[\frac{\partial^2 \log(L(\beta))}{\partial \beta \partial \beta'}] \\
&= -E[(\frac{\partial^2 \log(L(\beta))}{\partial \beta_i \partial \beta_j})_{ij}]
\end{aligned}
\]

Example:

\[
x_i' \beta = \beta_0 + \beta_1 x_i
\]

\[
\begin{aligned}
- \frac{\partial^2 \ln(L(\beta))}{\partial \beta^2_0} &= \sum_{i=1}^n \frac{\exp(x'_i \beta)}{1 + \exp(x'_i \beta)} - [\frac{\exp(x_i' \beta)}{1+ \exp(x'_i \beta)}]^2 = \sum_{i=1}^n p_i (1-p_i) \\
- \frac{\partial^2 \ln(L(\beta))}{\partial \beta^2_1} &= \sum_{i=1}^n \frac{x_i^2\exp(x'_i \beta)}{1 + \exp(x'_i \beta)} - [\frac{x_i\exp(x_i' \beta)}{1+ \exp(x'_i \beta)}]^2 = \sum_{i=1}^n x_i^2p_i (1-p_i) \\
- \frac{\partial^2 \ln(L(\beta))}{\partial \beta_0 \partial \beta_1} &= \sum_{i=1}^n \frac{x_i\exp(x'_i \beta)}{1 + \exp(x'_i \beta)} - x_i[\frac{\exp(x_i' \beta)}{1+ \exp(x'_i \beta)}]^2 = \sum_{i=1}^n x_ip_i (1-p_i)
\end{aligned}
\]

Hence,

\[
\mathbf{I} (\beta) = 
\left[
\begin{array}
{cc}
\sum_i p_i(1-p_i) & \sum_i x_i p_i(1-p_i) \\
\sum_i x_i p_i(1-p_i) & \sum_i x_i^2 p_i(1-p_i)
\end{array}
\right]
\]

\textbf{Inference}

\textbf{Likelihood Ratio Tests}

To formulate the test, let \(\beta = [\beta_1', \beta_2']'\). If you are interested in testing a hypothesis about \(\beta_1\), then we leave \(\beta_2\) unspecified (called \textbf{nuisance parameters}). \(\beta_1\) and \(\beta_2\) can either a \textbf{vector} or \textbf{scalar}, or \(\beta_2\) can be null.

Example: \(H_0: \beta_1 = \beta_{1,0}\) (where \(\beta_{1,0}\) is specified) and \(\hat{\beta}_{2,0}\) be the MLE of \(\beta_2\) under the restriction that \(\beta_1 = \beta_{1,0}\). The likelihood ratio test statistic is

\[
-2\log\Lambda = -2[\log(L(\beta_{1,0},\hat{\beta}_{2,0})) - \log(L(\hat{\beta}_1,\hat{\beta}_2))]
\]

where

\begin{itemize}
\tightlist
\item
  the first term is the value fo the likelihood for the fitted restricted model
\item
  the second term is the likelihood value of the fitted unrestricted model
\end{itemize}

Under the null,

\[
-2 \log \Lambda \sim \chi^2_{\upsilon}
\]

where \(\upsilon\) is the dimension of \(\beta_1\)

We reject the null when \(-2\log \Lambda > \chi_{\upsilon,1-\alpha}^2\)

\textbf{Wald Statistics}

Based on

\[
\hat{\beta} \sim AN (\beta, [\mathbf{I}(\beta)^{-1}])
\]

\[
H_0: \mathbf{L}\hat{\beta} = 0 
\]

where \(\mathbf{L}\) is a \(q \times p\) matrix with \(q\) linearly independent rows. Then

\[
W = (\mathbf{L\hat{\beta}})'(\mathbf{L[I(\hat{\beta})]^{-1}L'})^{-1}(\mathbf{L\hat{\beta}})
\]

under the null hypothesis

Confidence interval

\[
\hat{\beta}_i \pm 1.96 \hat{s}_{ii}^2
\]

where \(\hat{s}_{ii}^2\) is the i-th diagonal of \(\mathbf{[I(\hat{\beta})]}^{-1}\)

If you have

\begin{itemize}
\tightlist
\item
  large sample size, the likelihood ratio and Wald tests have similar results.
\item
  small sample size, the likelihood ratio test is better.
\end{itemize}

\textbf{Logistic Regression: Interpretation of} \(\beta\)

For single regressor, the model is

\[
logit\{\hat{p}_{x_i}\} \equiv logit (\hat{p}_i) = \log(\frac{\hat{p}_i}{1 - \hat{p}_i}) = \hat{\beta}_0 + \hat{\beta}_1 x_i
\]

When \(x= x_i + 1\)

\[
logit\{\hat{p}_{x_i +1}\} = \hat{\beta}_0 + \hat{\beta}(x_i + 1) = logit\{\hat{p}_{x_i}\} + \hat{\beta}_1
\]

Then,

\[
\begin{aligned}
logit\{\hat{p}_{x_i +1}\} - logit\{\hat{p}_{x_i}\} &= log\{odds[\hat{p}_{x_i +1}]\} - log\{odds[\hat{p}_{x_i}]\} \\
&= log(\frac{odds[\hat{p}_{x_i + 1}]}{odds[\hat{p}_{x_i}]}) = \hat{\beta}_1
\end{aligned}
\]

and

\[
exp(\hat{\beta}_1) = \frac{odds[\hat{p}_{x_i + 1}]}{odds[\hat{p}_{x_i}]}
\]

the estimated \textbf{odds ratio}

the estimated odds ratio, when there is a difference of c units in the regressor x, is \(exp(c\hat{\beta}_1)\). When there are multiple covariates, \(exp(\hat{\beta}_k)\) is the estimated odds ratio for the variable \(x_k\), assuming that all of the other variables are held constant.

\textbf{Inference on the Mean Response}

Let \(x_h = (1, x_{h1}, ...,x_{h,p-1})'\). Then

\[
\hat{p}_h = \frac{exp(\mathbf{x'_h \hat{\beta}})}{1 + exp(\mathbf{x'_h \hat{\beta}})}
\]

and \(s^2(\hat{p}_h) = \mathbf{x'_h[I(\hat{\beta})]^{-1}x_h}\)

For new observation, we can have a cutoff point to decide whether y = 0 or 1.

\hypertarget{application-2}{%
\subsection{Application}\label{application-2}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(kableExtra)}
\FunctionTok{library}\NormalTok{(dplyr)}
\FunctionTok{library}\NormalTok{(pscl)}
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(faraway)}
\FunctionTok{library}\NormalTok{(nnet)}
\FunctionTok{library}\NormalTok{(agridat)}
\FunctionTok{library}\NormalTok{(nlstools)}
\end{Highlighting}
\end{Shaded}

Logistic Regression

\(x \sim Unif(-0.5,2.5)\). Then \(\eta = 0.5 + 0.75 x\)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{23}\NormalTok{) }\CommentTok{\#set seed for reproducibility}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\AttributeTok{min =} \SpecialCharTok{{-}}\FloatTok{0.5}\NormalTok{, }\AttributeTok{max =} \FloatTok{2.5}\NormalTok{)}
\NormalTok{eta1 }\OtherTok{\textless{}{-}} \FloatTok{0.5} \SpecialCharTok{+} \FloatTok{0.75} \SpecialCharTok{*}\NormalTok{ x}
\end{Highlighting}
\end{Shaded}

Passing \(\eta\)'s into the inverse-logit function, we get

\[
p = \frac{\exp(\eta)}{1+ \exp(\eta)}
\]

where \(p \in [0,1]\)

Then, we generate \(y \sim Bernoulli(p)\)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\OtherTok{\textless{}{-}} \FunctionTok{exp}\NormalTok{(eta1) }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(eta1))}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\DecValTok{1}\NormalTok{, p)}
\NormalTok{BinData }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{X =}\NormalTok{ x, }\AttributeTok{Y =}\NormalTok{ y)}
\end{Highlighting}
\end{Shaded}

\textbf{Model Fit}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Logistic\_Model }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X,}
                      \AttributeTok{family =}\NormalTok{ binomial, }\CommentTok{\# family = specifies the response distribution}
                      \AttributeTok{data =}\NormalTok{ BinData)}
\FunctionTok{summary}\NormalTok{(Logistic\_Model)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} glm(formula = Y \textasciitilde{} X, family = binomial, data = BinData)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Deviance Residuals: }
\CommentTok{\#\textgreater{}     Min       1Q   Median       3Q      Max  }
\CommentTok{\#\textgreater{} {-}2.2317   0.4153   0.5574   0.7922   1.1469  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}             Estimate Std. Error z value Pr(\textgreater{}|z|)    }
\CommentTok{\#\textgreater{} (Intercept)  0.46205    0.10201   4.530 5.91e{-}06 ***}
\CommentTok{\#\textgreater{} X            0.78527    0.09296   8.447  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} (Dispersion parameter for binomial family taken to be 1)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Null deviance: 1106.7  on 999  degrees of freedom}
\CommentTok{\#\textgreater{} Residual deviance: 1027.4  on 998  degrees of freedom}
\CommentTok{\#\textgreater{} AIC: 1031.4}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Fisher Scoring iterations: 4}
\NormalTok{nlstools}\SpecialCharTok{::}\FunctionTok{confint2}\NormalTok{(Logistic\_Model)}
\CommentTok{\#\textgreater{}                 2.5 \%    97.5 \%}
\CommentTok{\#\textgreater{} (Intercept) 0.2618709 0.6622204}
\CommentTok{\#\textgreater{} X           0.6028433 0.9676934}
\NormalTok{OddsRatio }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(Logistic\_Model) }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ exp}
\NormalTok{OddsRatio }
\CommentTok{\#\textgreater{} (Intercept)           X }
\CommentTok{\#\textgreater{}    1.587318    2.192995}
\end{Highlighting}
\end{Shaded}

Based on the odds ratio, when

\begin{itemize}
\tightlist
\item
  \(x = 0\) , the odds of success of 1.59
\item
  \(x = 1\), the odds of success increase by a factor of 2.19 (i.e., 119.29\% increase).
\end{itemize}

Deviance Tests

\begin{itemize}
\tightlist
\item
  \(H_0\): No variables are related to the response (i.e., model with just the intercept)
\item
  \(H_1\): At least one variable is related to the response
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Test\_Dev }\OtherTok{\textless{}{-}}\NormalTok{ Logistic\_Model}\SpecialCharTok{$}\NormalTok{null.deviance }\SpecialCharTok{{-}}\NormalTok{ Logistic\_Model}\SpecialCharTok{$}\NormalTok{deviance}
\NormalTok{p\_val\_dev }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{pchisq}\NormalTok{(}\AttributeTok{q =}\NormalTok{ Test\_Dev, }\AttributeTok{df =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Since we see the p-value of 0, we reject the null that no variables are related to the response

\textbf{Deviance residuals}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Logistic\_Resids }\OtherTok{\textless{}{-}} \FunctionTok{residuals}\NormalTok{(Logistic\_Model, }\AttributeTok{type =} \StringTok{"deviance"}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(}
    \AttributeTok{y =}\NormalTok{ Logistic\_Resids,}
    \AttributeTok{x =}\NormalTok{ BinData}\SpecialCharTok{$}\NormalTok{X,}
    \AttributeTok{xlab =} \StringTok{\textquotesingle{}X\textquotesingle{}}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{\textquotesingle{}Deviance Resids\textquotesingle{}}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{07-generalized-linear-models_files/figure-latex/unnamed-chunk-6-1} \end{center}

However, this plot is not informative. Hence, we can can see the residuals plots that are grouped into bins based on prediction values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot\_bin }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(Y,}
\NormalTok{                     X,}
                     \AttributeTok{bins =} \DecValTok{100}\NormalTok{,}
                     \AttributeTok{return.DF =} \ConstantTok{FALSE}\NormalTok{) \{}
\NormalTok{    Y\_Name }\OtherTok{\textless{}{-}} \FunctionTok{deparse}\NormalTok{(}\FunctionTok{substitute}\NormalTok{(Y))}
\NormalTok{    X\_Name }\OtherTok{\textless{}{-}} \FunctionTok{deparse}\NormalTok{(}\FunctionTok{substitute}\NormalTok{(X))}
\NormalTok{    Binned\_Plot }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{Plot\_Y =}\NormalTok{ Y, }\AttributeTok{Plot\_X =}\NormalTok{ X)}
\NormalTok{    Binned\_Plot}\SpecialCharTok{$}\NormalTok{bin }\OtherTok{\textless{}{-}}
        \FunctionTok{cut}\NormalTok{(Binned\_Plot}\SpecialCharTok{$}\NormalTok{Plot\_X, }\AttributeTok{breaks =}\NormalTok{ bins) }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ as.numeric}
\NormalTok{    Binned\_Plot\_summary }\OtherTok{\textless{}{-}}\NormalTok{ Binned\_Plot }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{group\_by}\NormalTok{(bin) }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{summarise}\NormalTok{(}
            \AttributeTok{Y\_ave =} \FunctionTok{mean}\NormalTok{(Plot\_Y),}
            \AttributeTok{X\_ave =} \FunctionTok{mean}\NormalTok{(Plot\_X),}
            \AttributeTok{Count =} \FunctionTok{n}\NormalTok{()}
\NormalTok{        ) }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ as.data.frame}
    \FunctionTok{plot}\NormalTok{(}
        \AttributeTok{y =}\NormalTok{ Binned\_Plot\_summary}\SpecialCharTok{$}\NormalTok{Y\_ave,}
        \AttributeTok{x =}\NormalTok{ Binned\_Plot\_summary}\SpecialCharTok{$}\NormalTok{X\_ave,}
        \AttributeTok{ylab =}\NormalTok{ Y\_Name,}
        \AttributeTok{xlab =}\NormalTok{ X\_Name}
\NormalTok{    )}
    \ControlFlowTok{if}\NormalTok{ (return.DF)}
        \FunctionTok{return}\NormalTok{(Binned\_Plot\_summary)}
\NormalTok{\}}


\FunctionTok{plot\_bin}\NormalTok{(}\AttributeTok{Y =}\NormalTok{ Logistic\_Resids,}
         \AttributeTok{X =}\NormalTok{ BinData}\SpecialCharTok{$}\NormalTok{X,}
         \AttributeTok{bins =} \DecValTok{100}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{07-generalized-linear-models_files/figure-latex/unnamed-chunk-7-1} \end{center}

We can also see the predicted value against the residuals.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Logistic\_Predictions }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(Logistic\_Model, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\FunctionTok{plot\_bin}\NormalTok{(}\AttributeTok{Y =}\NormalTok{ Logistic\_Resids, }\AttributeTok{X =}\NormalTok{ Logistic\_Predictions, }\AttributeTok{bins =} \DecValTok{100}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{07-generalized-linear-models_files/figure-latex/unnamed-chunk-8-1} \end{center}

We can also look at a binned plot of the logistic prediction versus the true category

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{NumBins }\OtherTok{\textless{}{-}} \DecValTok{10}
\NormalTok{Binned\_Data }\OtherTok{\textless{}{-}} \FunctionTok{plot\_bin}\NormalTok{(}
    \AttributeTok{Y =}\NormalTok{ BinData}\SpecialCharTok{$}\NormalTok{Y,}
    \AttributeTok{X =}\NormalTok{ Logistic\_Predictions,}
    \AttributeTok{bins =}\NormalTok{ NumBins,}
    \AttributeTok{return.DF =} \ConstantTok{TRUE}
\NormalTok{)}
\NormalTok{Binned\_Data}
\CommentTok{\#\textgreater{}    bin     Y\_ave     X\_ave Count}
\CommentTok{\#\textgreater{} 1    1 0.5833333 0.5382095    72}
\CommentTok{\#\textgreater{} 2    2 0.5200000 0.5795887    75}
\CommentTok{\#\textgreater{} 3    3 0.6567164 0.6156540    67}
\CommentTok{\#\textgreater{} 4    4 0.7014925 0.6579674    67}
\CommentTok{\#\textgreater{} 5    5 0.6373626 0.6984765    91}
\CommentTok{\#\textgreater{} 6    6 0.7500000 0.7373341    72}
\CommentTok{\#\textgreater{} 7    7 0.7096774 0.7786747    93}
\CommentTok{\#\textgreater{} 8    8 0.8503937 0.8203819   127}
\CommentTok{\#\textgreater{} 9    9 0.8947368 0.8601232   133}
\CommentTok{\#\textgreater{} 10  10 0.8916256 0.9004734   203}
\FunctionTok{abline}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{, }\AttributeTok{col =} \StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{07-generalized-linear-models_files/figure-latex/unnamed-chunk-9-1} \end{center}

\textbf{Formal deviance test}

\textbf{Hosmer-Lemeshow test}

Null hypothesis: the observed events match the expected evens

\[
X^2_{HL} = \sum_{j=1}^{J} \frac{(y_j - m_j \hat{p}_j)^2}{m_j \hat{p}_j(1-\hat{p}_j)}
\]

where

\begin{itemize}
\tightlist
\item
  within the j-th bin, \(y_j\) is the number of successes
\item
  \(m_j\) = number of observations
\item
  \(\hat{p}_j\) = predicted probability
\end{itemize}

Under the null hypothesis, \(X^2_{HLL} \sim \chi^2_{J-1}\)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{HL\_BinVals }\OtherTok{\textless{}{-}}
\NormalTok{    (Binned\_Data}\SpecialCharTok{$}\NormalTok{Count }\SpecialCharTok{*}\NormalTok{ Binned\_Data}\SpecialCharTok{$}\NormalTok{Y\_ave }\SpecialCharTok{{-}}\NormalTok{ Binned\_Data}\SpecialCharTok{$}\NormalTok{Count }\SpecialCharTok{*}\NormalTok{ Binned\_Data}\SpecialCharTok{$}\NormalTok{X\_ave) }\SpecialCharTok{\^{}} \DecValTok{2} \SpecialCharTok{/}\NormalTok{   Binned\_Data}\SpecialCharTok{$}\NormalTok{Count }\SpecialCharTok{*}\NormalTok{ Binned\_Data}\SpecialCharTok{$}\NormalTok{X\_ave }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ Binned\_Data}\SpecialCharTok{$}\NormalTok{X\_ave)}
\NormalTok{HLpval }\OtherTok{\textless{}{-}} \FunctionTok{pchisq}\NormalTok{(}\AttributeTok{q =} \FunctionTok{sum}\NormalTok{(HL\_BinVals),}
                 \AttributeTok{df =}\NormalTok{ NumBins,}
                 \AttributeTok{lower.tail =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{HLpval}
\CommentTok{\#\textgreater{} [1] 0.9999989}
\end{Highlighting}
\end{Shaded}

Since \(p\)-value = 0.99, we do not reject the null hypothesis (i.e., the model is fitting well).

\hypertarget{probit-regression}{%
\section{Probit Regression}\label{probit-regression}}

\[
E(Y_i) = p_i = \Phi(\mathbf{x_i'\theta})
\]

where \(\Phi()\) is the CDF of a \(N(0,1)\) random variable.

Other models (e..g, t--distribution; log-log; I complimentary log-log)

We let \(Y_i = 1\) success, \(Y_i =0\) no success.

\begin{itemize}
\item
  assume \(Y \sim Ber\) and \(p_i = P(Y_i =1)\), the success probability.
\item
  consider a logistic regression with the response function \(logit(p_i) = x'_i \beta\)
\end{itemize}

\textbf{Confusion matrix}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
& Predicted & \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Truth & 1 & 0 \\
1 & True Positive (TP) & False Negative (FN) \\
0 & False Positive (FP) & True Negative (TN) \\
\end{longtable}

Sensitivity: ability to identify positive results

\[
\text{Sensitivity} = \frac{TP}{TP + FN}
\]

Specificity: ability to identify negative results

\[
\text{Specificity} = \frac{TN}{TN + FP}
\]

False positive rate: Type I error (1- specificity)

\[
\text{ False Positive Rate} = \frac{FP}{TN+ FP}
\]

False Negative Rate: Type II error (1-sensitivity)

\[
\text{False Negative Rate} = \frac{FN}{TP + FN}
\]

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
& Predicted & \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Truth & 1 & 0 \\
1 & Sensitivity & False Negative Rate \\
0 & False Positive Rate & Specificity \\
\end{longtable}

\hypertarget{binomial-regression}{%
\section{Binomial Regression}\label{binomial-regression}}

\textbf{Binomial}

Here, cancer case = successes, and control case = failures.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(}\StringTok{"esoph"}\NormalTok{)}
\FunctionTok{head}\NormalTok{(esoph, }\AttributeTok{n =} \DecValTok{3}\NormalTok{)}
\CommentTok{\#\textgreater{}   agegp     alcgp    tobgp ncases ncontrols}
\CommentTok{\#\textgreater{} 1 25{-}34 0{-}39g/day 0{-}9g/day      0        40}
\CommentTok{\#\textgreater{} 2 25{-}34 0{-}39g/day    10{-}19      0        10}
\CommentTok{\#\textgreater{} 3 25{-}34 0{-}39g/day    20{-}29      0         6}
\FunctionTok{plot}\NormalTok{(}
\NormalTok{  esoph}\SpecialCharTok{$}\NormalTok{ncases }\SpecialCharTok{/}\NormalTok{ (esoph}\SpecialCharTok{$}\NormalTok{ncases }\SpecialCharTok{+}\NormalTok{ esoph}\SpecialCharTok{$}\NormalTok{ncontrols) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ esoph}\SpecialCharTok{$}\NormalTok{alcgp,}
  \AttributeTok{ylab =} \StringTok{"Proportion"}\NormalTok{,}
  \AttributeTok{xlab =} \StringTok{\textquotesingle{}Alcohol consumption\textquotesingle{}}\NormalTok{,}
  \AttributeTok{main =} \StringTok{\textquotesingle{}Esophageal Cancer data\textquotesingle{}}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{07-generalized-linear-models_files/figure-latex/unnamed-chunk-11-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(esoph}\SpecialCharTok{$}\NormalTok{agegp) }\OtherTok{\textless{}{-}} \StringTok{"factor"}
\FunctionTok{class}\NormalTok{(esoph}\SpecialCharTok{$}\NormalTok{alcgp) }\OtherTok{\textless{}{-}} \StringTok{"factor"}
\FunctionTok{class}\NormalTok{(esoph}\SpecialCharTok{$}\NormalTok{tobgp) }\OtherTok{\textless{}{-}} \StringTok{"factor"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#  only the alcohol consumption as a predictor}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(ncases, ncontrols) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ alcgp, }\AttributeTok{data =}\NormalTok{ esoph, }\AttributeTok{family =}\NormalTok{ binomial)}
\FunctionTok{summary}\NormalTok{(model)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} glm(formula = cbind(ncases, ncontrols) \textasciitilde{} alcgp, family = binomial, }
\CommentTok{\#\textgreater{}     data = esoph)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Deviance Residuals: }
\CommentTok{\#\textgreater{}     Min       1Q   Median       3Q      Max  }
\CommentTok{\#\textgreater{} {-}4.0759  {-}1.2037  {-}0.0183   1.0928   3.7336  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}             Estimate Std. Error z value Pr(\textgreater{}|z|)    }
\CommentTok{\#\textgreater{} (Intercept)  {-}2.5885     0.1925 {-}13.444  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} alcgp40{-}79    1.2712     0.2323   5.472 4.46e{-}08 ***}
\CommentTok{\#\textgreater{} alcgp80{-}119   2.0545     0.2611   7.868 3.59e{-}15 ***}
\CommentTok{\#\textgreater{} alcgp120+     3.3042     0.3237  10.209  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} (Dispersion parameter for binomial family taken to be 1)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Null deviance: 367.95  on 87  degrees of freedom}
\CommentTok{\#\textgreater{} Residual deviance: 221.46  on 84  degrees of freedom}
\CommentTok{\#\textgreater{} AIC: 344.51}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Fisher Scoring iterations: 5}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Coefficient Odds}
\FunctionTok{coefficients}\NormalTok{(model) }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ exp}
\CommentTok{\#\textgreater{} (Intercept)  alcgp40{-}79 alcgp80{-}119   alcgp120+ }
\CommentTok{\#\textgreater{}  0.07512953  3.56527094  7.80261593 27.22570533}
\FunctionTok{deviance}\NormalTok{(model)}\SpecialCharTok{/}\FunctionTok{df.residual}\NormalTok{(model)}
\CommentTok{\#\textgreater{} [1] 2.63638}
\NormalTok{model}\SpecialCharTok{$}\NormalTok{aic}
\CommentTok{\#\textgreater{} [1] 344.5109}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# alcohol consumption and age as predictors}
\NormalTok{better\_model }\OtherTok{\textless{}{-}}
    \FunctionTok{glm}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(ncases, ncontrols) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ agegp }\SpecialCharTok{+}\NormalTok{ alcgp,}
        \AttributeTok{data =}\NormalTok{ esoph,}
        \AttributeTok{family =}\NormalTok{ binomial)}
\FunctionTok{summary}\NormalTok{(better\_model)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} glm(formula = cbind(ncases, ncontrols) \textasciitilde{} agegp + alcgp, family = binomial, }
\CommentTok{\#\textgreater{}     data = esoph)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Deviance Residuals: }
\CommentTok{\#\textgreater{}     Min       1Q   Median       3Q      Max  }
\CommentTok{\#\textgreater{} {-}2.2395  {-}0.7186  {-}0.2324   0.7930   3.3538  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}             Estimate Std. Error z value Pr(\textgreater{}|z|)    }
\CommentTok{\#\textgreater{} (Intercept)  {-}6.1472     1.0419  {-}5.900 3.63e{-}09 ***}
\CommentTok{\#\textgreater{} agegp35{-}44    1.6311     1.0800   1.510 0.130973    }
\CommentTok{\#\textgreater{} agegp45{-}54    3.4258     1.0389   3.297 0.000976 ***}
\CommentTok{\#\textgreater{} agegp55{-}64    3.9435     1.0346   3.811 0.000138 ***}
\CommentTok{\#\textgreater{} agegp65{-}74    4.3568     1.0413   4.184 2.87e{-}05 ***}
\CommentTok{\#\textgreater{} agegp75+      4.4242     1.0914   4.054 5.04e{-}05 ***}
\CommentTok{\#\textgreater{} alcgp40{-}79    1.4343     0.2448   5.859 4.64e{-}09 ***}
\CommentTok{\#\textgreater{} alcgp80{-}119   2.0071     0.2776   7.230 4.84e{-}13 ***}
\CommentTok{\#\textgreater{} alcgp120+     3.6800     0.3763   9.778  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} (Dispersion parameter for binomial family taken to be 1)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Null deviance: 367.95  on 87  degrees of freedom}
\CommentTok{\#\textgreater{} Residual deviance: 105.88  on 79  degrees of freedom}
\CommentTok{\#\textgreater{} AIC: 238.94}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Fisher Scoring iterations: 6}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{better\_model}\SpecialCharTok{$}\NormalTok{aic }\CommentTok{\#smaller AIC is better}
\CommentTok{\#\textgreater{} [1] 238.9361}
\FunctionTok{coefficients}\NormalTok{(better\_model) }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ exp}
\CommentTok{\#\textgreater{}  (Intercept)   agegp35{-}44   agegp45{-}54   agegp55{-}64   agegp65{-}74     agegp75+ }
\CommentTok{\#\textgreater{}  0.002139482  5.109601844 30.748594216 51.596634690 78.005283850 83.448437749 }
\CommentTok{\#\textgreater{}   alcgp40{-}79  alcgp80{-}119    alcgp120+ }
\CommentTok{\#\textgreater{}  4.196747169  7.441782227 39.646885126}
\FunctionTok{pchisq}\NormalTok{(}
    \AttributeTok{q =}\NormalTok{ model}\SpecialCharTok{$}\NormalTok{deviance }\SpecialCharTok{{-}}\NormalTok{ better\_model}\SpecialCharTok{$}\NormalTok{deviance,}
    \AttributeTok{df =}\NormalTok{ model}\SpecialCharTok{$}\NormalTok{df.residual }\SpecialCharTok{{-}}\NormalTok{ better\_model}\SpecialCharTok{$}\NormalTok{df.residual,}
    \AttributeTok{lower =} \ConstantTok{FALSE}
\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 2.713923e{-}23}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# specify link function as probit}
\NormalTok{Prob\_better\_model }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(}
    \FunctionTok{cbind}\NormalTok{(ncases, ncontrols) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ agegp }\SpecialCharTok{+}\NormalTok{ alcgp,}
    \AttributeTok{data =}\NormalTok{ esoph,}
    \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =}\NormalTok{ probit)}
\NormalTok{)}
\FunctionTok{summary}\NormalTok{(Prob\_better\_model)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} glm(formula = cbind(ncases, ncontrols) \textasciitilde{} agegp + alcgp, family = binomial(link = probit), }
\CommentTok{\#\textgreater{}     data = esoph)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Deviance Residuals: }
\CommentTok{\#\textgreater{}     Min       1Q   Median       3Q      Max  }
\CommentTok{\#\textgreater{} {-}2.1325  {-}0.6877  {-}0.1661   0.7654   3.3258  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}             Estimate Std. Error z value Pr(\textgreater{}|z|)    }
\CommentTok{\#\textgreater{} (Intercept)  {-}3.3741     0.4922  {-}6.855 7.13e{-}12 ***}
\CommentTok{\#\textgreater{} agegp35{-}44    0.8562     0.5081   1.685 0.092003 .  }
\CommentTok{\#\textgreater{} agegp45{-}54    1.7829     0.4904   3.636 0.000277 ***}
\CommentTok{\#\textgreater{} agegp55{-}64    2.1034     0.4876   4.314 1.61e{-}05 ***}
\CommentTok{\#\textgreater{} agegp65{-}74    2.3374     0.4930   4.741 2.13e{-}06 ***}
\CommentTok{\#\textgreater{} agegp75+      2.3694     0.5275   4.491 7.08e{-}06 ***}
\CommentTok{\#\textgreater{} alcgp40{-}79    0.8080     0.1330   6.076 1.23e{-}09 ***}
\CommentTok{\#\textgreater{} alcgp80{-}119   1.1399     0.1558   7.318 2.52e{-}13 ***}
\CommentTok{\#\textgreater{} alcgp120+     2.1204     0.2060  10.295  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} (Dispersion parameter for binomial family taken to be 1)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Null deviance: 367.95  on 87  degrees of freedom}
\CommentTok{\#\textgreater{} Residual deviance: 104.48  on 79  degrees of freedom}
\CommentTok{\#\textgreater{} AIC: 237.53}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Fisher Scoring iterations: 6}
\end{Highlighting}
\end{Shaded}

\hypertarget{poisson-regression}{%
\section{Poisson Regression}\label{poisson-regression}}

From the Poisson distribution

\[
\begin{aligned}
f(Y_i) &= \frac{\mu_i^{Y_i}exp(-\mu_i)}{Y_i!}, Y_i = 0,1,.. \\
E(Y_i) &= \mu_i  \\
var(Y_i) &= \mu_i
\end{aligned}
\]

which is a natural distribution for counts. We can see that the variance is a function of the mean. If we let \(\mu_i = f(\mathbf{x_i; \theta})\), it would be similar to \protect\hyperlink{logistic-regression}{Logistic Regression} since we can choose \(f()\) as \(\mu_i = \mathbf{x_i'\theta}, \mu_i = \exp(\mathbf{x_i'\theta}), \mu_i = \log(\mathbf{x_i'\theta})\)

\hypertarget{application-3}{%
\subsection{Application}\label{application-3}}

Count Data and Poisson regression

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(bioChemists, }\AttributeTok{package =} \StringTok{"pscl"}\NormalTok{)}
\NormalTok{bioChemists }\OtherTok{\textless{}{-}}\NormalTok{ bioChemists }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{rename}\NormalTok{(}
        \AttributeTok{Num\_Article =}\NormalTok{ art, }\CommentTok{\#articles in last 3 years of PhD}
        \AttributeTok{Sex =}\NormalTok{ fem, }\CommentTok{\#coded 1 if female}
        \AttributeTok{Married =}\NormalTok{ mar, }\CommentTok{\#coded 1 if married}
        \AttributeTok{Num\_Kid5 =}\NormalTok{ kid5, }\CommentTok{\#number of childeren under age 6}
        \AttributeTok{PhD\_Quality =}\NormalTok{ phd, }\CommentTok{\#prestige of PhD program}
        \AttributeTok{Num\_MentArticle =}\NormalTok{ ment }\CommentTok{\#articles by mentor in last 3 years}
\NormalTok{    )}
\FunctionTok{hist}\NormalTok{(bioChemists}\SpecialCharTok{$}\NormalTok{Num\_Article, }\AttributeTok{breaks =} \DecValTok{25}\NormalTok{, }\AttributeTok{main =} \StringTok{\textquotesingle{}Number of Articles\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{07-generalized-linear-models_files/figure-latex/unnamed-chunk-17-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Poisson\_Mod }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(Num\_Article }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{family=}\NormalTok{poisson, bioChemists)}
\FunctionTok{summary}\NormalTok{(Poisson\_Mod)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} glm(formula = Num\_Article \textasciitilde{} ., family = poisson, data = bioChemists)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Deviance Residuals: }
\CommentTok{\#\textgreater{}     Min       1Q   Median       3Q      Max  }
\CommentTok{\#\textgreater{} {-}3.5672  {-}1.5398  {-}0.3660   0.5722   5.4467  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}                  Estimate Std. Error z value Pr(\textgreater{}|z|)    }
\CommentTok{\#\textgreater{} (Intercept)      0.304617   0.102981   2.958   0.0031 ** }
\CommentTok{\#\textgreater{} SexWomen        {-}0.224594   0.054613  {-}4.112 3.92e{-}05 ***}
\CommentTok{\#\textgreater{} MarriedMarried   0.155243   0.061374   2.529   0.0114 *  }
\CommentTok{\#\textgreater{} Num\_Kid5        {-}0.184883   0.040127  {-}4.607 4.08e{-}06 ***}
\CommentTok{\#\textgreater{} PhD\_Quality      0.012823   0.026397   0.486   0.6271    }
\CommentTok{\#\textgreater{} Num\_MentArticle  0.025543   0.002006  12.733  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} (Dispersion parameter for poisson family taken to be 1)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Null deviance: 1817.4  on 914  degrees of freedom}
\CommentTok{\#\textgreater{} Residual deviance: 1634.4  on 909  degrees of freedom}
\CommentTok{\#\textgreater{} AIC: 3314.1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Fisher Scoring iterations: 5}
\end{Highlighting}
\end{Shaded}

Residual of 1634 with 909 df isn't great.

We see Pearson \(\chi^2\)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Predicted\_Means }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(Poisson\_Mod,}\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\NormalTok{X2 }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{((bioChemists}\SpecialCharTok{$}\NormalTok{Num\_Article }\SpecialCharTok{{-}}\NormalTok{ Predicted\_Means)}\SpecialCharTok{\^{}}\DecValTok{2}\SpecialCharTok{/}\NormalTok{Predicted\_Means)}
\NormalTok{X2}
\CommentTok{\#\textgreater{} [1] 1662.547}
\FunctionTok{pchisq}\NormalTok{(X2,Poisson\_Mod}\SpecialCharTok{$}\NormalTok{df.residual, }\AttributeTok{lower.tail =} \ConstantTok{FALSE}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 7.849882e{-}47}
\end{Highlighting}
\end{Shaded}

With interaction terms, there are some improvements

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Poisson\_Mod\_All2way }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(Num\_Article }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{, }\AttributeTok{family=}\NormalTok{poisson, bioChemists)}
\NormalTok{Poisson\_Mod\_All3way }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(Num\_Article }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .}\SpecialCharTok{\^{}}\DecValTok{3}\NormalTok{, }\AttributeTok{family=}\NormalTok{poisson, bioChemists)}
\end{Highlighting}
\end{Shaded}

Consider the \(\hat{\phi} = \frac{\text{deviance}}{df}\)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Poisson\_Mod}\SpecialCharTok{$}\NormalTok{deviance }\SpecialCharTok{/}\NormalTok{ Poisson\_Mod}\SpecialCharTok{$}\NormalTok{df.residual}
\CommentTok{\#\textgreater{} [1] 1.797988}
\end{Highlighting}
\end{Shaded}

This is evidence for over-dispersion. Likely cause is missing variables. And remedies could either be to include more variables or consider random effects.

A quick fix is to force the Poisson Regression to include this value of \(\phi\), and this model is called ``Quasi-Poisson''.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{phi\_hat }\OtherTok{=}\NormalTok{ Poisson\_Mod}\SpecialCharTok{$}\NormalTok{deviance}\SpecialCharTok{/}\NormalTok{Poisson\_Mod}\SpecialCharTok{$}\NormalTok{df.residual}
\FunctionTok{summary}\NormalTok{(Poisson\_Mod,}\AttributeTok{dispersion =}\NormalTok{ phi\_hat)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} glm(formula = Num\_Article \textasciitilde{} ., family = poisson, data = bioChemists)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Deviance Residuals: }
\CommentTok{\#\textgreater{}     Min       1Q   Median       3Q      Max  }
\CommentTok{\#\textgreater{} {-}3.5672  {-}1.5398  {-}0.3660   0.5722   5.4467  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}                 Estimate Std. Error z value Pr(\textgreater{}|z|)    }
\CommentTok{\#\textgreater{} (Intercept)      0.30462    0.13809   2.206  0.02739 *  }
\CommentTok{\#\textgreater{} SexWomen        {-}0.22459    0.07323  {-}3.067  0.00216 ** }
\CommentTok{\#\textgreater{} MarriedMarried   0.15524    0.08230   1.886  0.05924 .  }
\CommentTok{\#\textgreater{} Num\_Kid5        {-}0.18488    0.05381  {-}3.436  0.00059 ***}
\CommentTok{\#\textgreater{} PhD\_Quality      0.01282    0.03540   0.362  0.71715    }
\CommentTok{\#\textgreater{} Num\_MentArticle  0.02554    0.00269   9.496  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} (Dispersion parameter for poisson family taken to be 1.797988)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Null deviance: 1817.4  on 914  degrees of freedom}
\CommentTok{\#\textgreater{} Residual deviance: 1634.4  on 909  degrees of freedom}
\CommentTok{\#\textgreater{} AIC: 3314.1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Fisher Scoring iterations: 5}
\end{Highlighting}
\end{Shaded}

Or directly rerun the model as

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{quasiPoisson\_Mod }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(Num\_Article }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{family=}\NormalTok{quasipoisson, bioChemists)}
\end{Highlighting}
\end{Shaded}

Quasi-Poisson is not recommended, but \protect\hyperlink{negative-binomial-regression}{Negative Binomial Regression} that has an extra parameter to account for over-dispersion is.

\hypertarget{negative-binomial-regression}{%
\section{Negative Binomial Regression}\label{negative-binomial-regression}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(MASS)}
\NormalTok{NegBinom\_Mod }\OtherTok{\textless{}{-}}\NormalTok{ MASS}\SpecialCharTok{::}\FunctionTok{glm.nb}\NormalTok{(Num\_Article }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .,bioChemists)}
\FunctionTok{summary}\NormalTok{(NegBinom\_Mod)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} MASS::glm.nb(formula = Num\_Article \textasciitilde{} ., data = bioChemists, init.theta = 2.264387695, }
\CommentTok{\#\textgreater{}     link = log)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Deviance Residuals: }
\CommentTok{\#\textgreater{}     Min       1Q   Median       3Q      Max  }
\CommentTok{\#\textgreater{} {-}2.1678  {-}1.3617  {-}0.2806   0.4476   3.4524  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}                  Estimate Std. Error z value Pr(\textgreater{}|z|)    }
\CommentTok{\#\textgreater{} (Intercept)      0.256144   0.137348   1.865 0.062191 .  }
\CommentTok{\#\textgreater{} SexWomen        {-}0.216418   0.072636  {-}2.979 0.002887 ** }
\CommentTok{\#\textgreater{} MarriedMarried   0.150489   0.082097   1.833 0.066791 .  }
\CommentTok{\#\textgreater{} Num\_Kid5        {-}0.176415   0.052813  {-}3.340 0.000837 ***}
\CommentTok{\#\textgreater{} PhD\_Quality      0.015271   0.035873   0.426 0.670326    }
\CommentTok{\#\textgreater{} Num\_MentArticle  0.029082   0.003214   9.048  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} (Dispersion parameter for Negative Binomial(2.2644) family taken to be 1)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Null deviance: 1109.0  on 914  degrees of freedom}
\CommentTok{\#\textgreater{} Residual deviance: 1004.3  on 909  degrees of freedom}
\CommentTok{\#\textgreater{} AIC: 3135.9}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Fisher Scoring iterations: 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}               Theta:  2.264 }
\CommentTok{\#\textgreater{}           Std. Err.:  0.271 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  2 x log{-}likelihood:  {-}3121.917}
\end{Highlighting}
\end{Shaded}

We can see the dispersion is 2.264 with SE = 0.271, which is significantly different from 1, indicating over-dispersion. Check \protect\hyperlink{over-dispersion}{Over-Dispersion} for more detail

\hypertarget{multinomial}{%
\section{Multinomial}\label{multinomial}}

If we have more than two categories or groups that we want to model relative to covariates (e.g., we have observations \(i = 1,â€¦,n\) and groups/ covariates \(j = 1,2,â€¦,J\)), multinomial is our candidate model

Let

\begin{itemize}
\tightlist
\item
  \(p_{ij}\) be the probability that the i-th observation belongs to the j-th group
\item
  \(Y_{ij}\) be the number of observations for individual i in group j; An individual will have observations \(Y_{i1},Y_{i2},â€¦Y_{iJ}\)
\item
  assume the probability of observing this response is given by a multinomial distribution in terms of probabilities \(p_{ij}\), where \(\sum_{j = 1}^J p_{ij} = 1\) . For interpretation, we have a baseline category \(p_{i1} = 1 - \sum_{j = 2}^J p_{ij}\)
\end{itemize}

The link between the mean response (probability) \(p_{ij}\) and a linear function of the covariates

\[
\eta_{ij} = \mathbf{x'_i \beta_j} = \log \frac{p_{ij}}{p_{i1}}, j = 2,..,J
\]

We compare \(p_{ij}\) to the baseline \(p_{i1}\), suggesting

\[
p_{ij} = \frac{\exp(\eta_{ij})}{1 + \sum_{i=2}^J \exp(\eta_{ij})}
\]

which is known as \textbf{multinomial logistic} model.

Note:

\begin{itemize}
\tightlist
\item
  Softmax coding for multinomial logistic regression: rather than selecting a baseline class, we treat all \(K\) class symmetrically - equally important (no baseline).
\end{itemize}

\[
P(Y = k | X = x) = \frac{exp(\beta_{k1} + \dots + \beta_{k_p x_p})}{\sum_{l = 1}^K exp(\beta_{l0} + \dots + \beta_{l_p x_p})}
\]

then the log odds ratio between k-th and k'-th classes is

\[
\log (\frac{P(Y=k|X=x)}{P(Y = k' | X=x)}) = (\beta_{k0} - \beta_{k'0}) + \dots + (\beta_{kp} - \beta_{k'p}) x_p
\]

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(faraway)}
\FunctionTok{library}\NormalTok{(dplyr)}
\FunctionTok{data}\NormalTok{(nes96, }\AttributeTok{package=}\StringTok{"faraway"}\NormalTok{)}
\FunctionTok{head}\NormalTok{(nes96,}\DecValTok{3}\NormalTok{)}
\CommentTok{\#\textgreater{}   popul TVnews selfLR ClinLR DoleLR     PID age  educ   income    vote}
\CommentTok{\#\textgreater{} 1     0      7 extCon extLib    Con  strRep  36    HS $3Kminus    Dole}
\CommentTok{\#\textgreater{} 2   190      1 sliLib sliLib sliCon weakDem  20  Coll $3Kminus Clinton}
\CommentTok{\#\textgreater{} 3    31      7    Lib    Lib    Con weakDem  24 BAdeg $3Kminus Clinton}
\end{Highlighting}
\end{Shaded}

We try to understand their political strength

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(nes96}\SpecialCharTok{$}\NormalTok{PID)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  strDem weakDem  indDem  indind  indRep weakRep  strRep }
\CommentTok{\#\textgreater{}     200     180     108      37      94     150     175}
\NormalTok{nes96}\SpecialCharTok{$}\NormalTok{Political\_Strength }\OtherTok{\textless{}{-}} \ConstantTok{NA}
\NormalTok{nes96}\SpecialCharTok{$}\NormalTok{Political\_Strength[nes96}\SpecialCharTok{$}\NormalTok{PID }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"strDem"}\NormalTok{, }\StringTok{"strRep"}\NormalTok{)] }\OtherTok{\textless{}{-}}
    \StringTok{"Strong"}
\NormalTok{nes96}\SpecialCharTok{$}\NormalTok{Political\_Strength[nes96}\SpecialCharTok{$}\NormalTok{PID }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"weakDem"}\NormalTok{, }\StringTok{"weakRep"}\NormalTok{)] }\OtherTok{\textless{}{-}}
    \StringTok{"Weak"}
\NormalTok{nes96}\SpecialCharTok{$}\NormalTok{Political\_Strength[nes96}\SpecialCharTok{$}\NormalTok{PID }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"indDem"}\NormalTok{, }\StringTok{"indind"}\NormalTok{, }\StringTok{"indRep"}\NormalTok{)] }\OtherTok{\textless{}{-}}
    \StringTok{"Neutral"}
\NormalTok{nes96 }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{group\_by}\NormalTok{(Political\_Strength) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{summarise}\NormalTok{(}\AttributeTok{Count =} \FunctionTok{n}\NormalTok{())}
\CommentTok{\#\textgreater{} \# A tibble: 3 x 2}
\CommentTok{\#\textgreater{}   Political\_Strength Count}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}              \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 Neutral              239}
\CommentTok{\#\textgreater{} 2 Strong               375}
\CommentTok{\#\textgreater{} 3 Weak                 330}
\end{Highlighting}
\end{Shaded}

visualize the political strength variable

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\NormalTok{Plot\_DF }\OtherTok{\textless{}{-}}\NormalTok{ nes96 }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Age\_Grp =} \FunctionTok{cut\_number}\NormalTok{(age, }\DecValTok{4}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{group\_by}\NormalTok{(Age\_Grp, Political\_Strength) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{summarise}\NormalTok{(}\AttributeTok{count =} \FunctionTok{n}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{group\_by}\NormalTok{(Age\_Grp) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{etotal =} \FunctionTok{sum}\NormalTok{(count), }\AttributeTok{proportion =}\NormalTok{ count }\SpecialCharTok{/}\NormalTok{ etotal)}

\NormalTok{Age\_Plot }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(}
\NormalTok{    Plot\_DF,}
    \FunctionTok{aes}\NormalTok{(}
        \AttributeTok{x        =}\NormalTok{ Age\_Grp,}
        \AttributeTok{y        =}\NormalTok{ proportion,}
        \AttributeTok{group    =}\NormalTok{ Political\_Strength,}
        \AttributeTok{linetype =}\NormalTok{ Political\_Strength,}
        \AttributeTok{color    =}\NormalTok{ Political\_Strength}
\NormalTok{    )}
\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{size =} \DecValTok{2}\NormalTok{)}
\NormalTok{Age\_Plot}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{07-generalized-linear-models_files/figure-latex/unnamed-chunk-27-1} \end{center}

Fit the multinomial logistic model:

model political strength as a function of age and education

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(nnet)}
\NormalTok{Multinomial\_Model }\OtherTok{\textless{}{-}}
    \FunctionTok{multinom}\NormalTok{(Political\_Strength }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ educ, nes96, }\AttributeTok{trace =}\NormalTok{ F)}
\FunctionTok{summary}\NormalTok{(Multinomial\_Model)}
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} multinom(formula = Political\_Strength \textasciitilde{} age + educ, data = nes96, }
\CommentTok{\#\textgreater{}     trace = F)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}        (Intercept)          age     educ.L     educ.Q     educ.C      educ\^{}4}
\CommentTok{\#\textgreater{} Strong {-}0.08788729  0.010700364 {-}0.1098951 {-}0.2016197 {-}0.1757739 {-}0.02116307}
\CommentTok{\#\textgreater{} Weak    0.51976285 {-}0.004868771 {-}0.1431104 {-}0.2405395 {-}0.2411795  0.18353634}
\CommentTok{\#\textgreater{}            educ\^{}5     educ\^{}6}
\CommentTok{\#\textgreater{} Strong {-}0.1664377 {-}0.1359449}
\CommentTok{\#\textgreater{} Weak   {-}0.1489030 {-}0.2173144}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Std. Errors:}
\CommentTok{\#\textgreater{}        (Intercept)         age    educ.L    educ.Q    educ.C    educ\^{}4}
\CommentTok{\#\textgreater{} Strong   0.3017034 0.005280743 0.4586041 0.4318830 0.3628837 0.2964776}
\CommentTok{\#\textgreater{} Weak     0.3097923 0.005537561 0.4920736 0.4616446 0.3881003 0.3169149}
\CommentTok{\#\textgreater{}           educ\^{}5    educ\^{}6}
\CommentTok{\#\textgreater{} Strong 0.2515012 0.2166774}
\CommentTok{\#\textgreater{} Weak   0.2643747 0.2199186}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual Deviance: 2024.596 }
\CommentTok{\#\textgreater{} AIC: 2056.596}
\end{Highlighting}
\end{Shaded}

Alternatively, stepwise model selection based AIC

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Multinomial\_Step }\OtherTok{\textless{}{-}} \FunctionTok{step}\NormalTok{(Multinomial\_Model,}\AttributeTok{trace =} \DecValTok{0}\NormalTok{)}
\CommentTok{\#\textgreater{} trying {-} age }
\CommentTok{\#\textgreater{} trying {-} educ }
\CommentTok{\#\textgreater{} trying {-} age}
\NormalTok{Multinomial\_Step}
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} multinom(formula = Political\_Strength \textasciitilde{} age, data = nes96, trace = F)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}        (Intercept)          age}
\CommentTok{\#\textgreater{} Strong {-}0.01988977  0.009832916}
\CommentTok{\#\textgreater{} Weak    0.59497046 {-}0.005954348}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual Deviance: 2030.756 }
\CommentTok{\#\textgreater{} AIC: 2038.756}
\end{Highlighting}
\end{Shaded}

compare the best model to the full model based on deviance

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pchisq}\NormalTok{(}\AttributeTok{q =} \FunctionTok{deviance}\NormalTok{(Multinomial\_Step) }\SpecialCharTok{{-}} \FunctionTok{deviance}\NormalTok{(Multinomial\_Model),}
\AttributeTok{df =}\NormalTok{ Multinomial\_Model}\SpecialCharTok{$}\NormalTok{edf}\SpecialCharTok{{-}}\NormalTok{Multinomial\_Step}\SpecialCharTok{$}\NormalTok{edf,}\AttributeTok{lower=}\NormalTok{F)}
\CommentTok{\#\textgreater{} [1] 0.9078172}
\end{Highlighting}
\end{Shaded}

We see no significant difference

Plot of the fitted model

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PlotData }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{age =} \FunctionTok{seq}\NormalTok{(}\AttributeTok{from =} \DecValTok{19}\NormalTok{, }\AttributeTok{to =} \DecValTok{91}\NormalTok{))}
\NormalTok{Preds }\OtherTok{\textless{}{-}}
\NormalTok{  PlotData }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{bind\_cols}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(}\FunctionTok{predict}\NormalTok{(}
    \AttributeTok{object =}\NormalTok{ Multinomial\_Step,}
\NormalTok{    PlotData, }\AttributeTok{type =} \StringTok{"probs"}
\NormalTok{  )))}

\FunctionTok{plot}\NormalTok{(}
  \AttributeTok{x       =}\NormalTok{ Preds}\SpecialCharTok{$}\NormalTok{age,}
  \AttributeTok{y       =}\NormalTok{ Preds}\SpecialCharTok{$}\NormalTok{Neutral,}
  \AttributeTok{type    =} \StringTok{"l"}\NormalTok{,}
  \AttributeTok{ylim    =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\FloatTok{0.6}\NormalTok{),}
  \AttributeTok{col     =} \StringTok{"black"}\NormalTok{,}
  \AttributeTok{ylab    =} \StringTok{"Proportion"}\NormalTok{,}
  \AttributeTok{xlab    =} \StringTok{"Age"}
\NormalTok{)}

\FunctionTok{lines}\NormalTok{(}\AttributeTok{x   =}\NormalTok{ Preds}\SpecialCharTok{$}\NormalTok{age,}
      \AttributeTok{y   =}\NormalTok{ Preds}\SpecialCharTok{$}\NormalTok{Weak,}
      \AttributeTok{col =} \StringTok{"blue"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(}\AttributeTok{x   =}\NormalTok{ Preds}\SpecialCharTok{$}\NormalTok{age,}
      \AttributeTok{y   =}\NormalTok{ Preds}\SpecialCharTok{$}\NormalTok{Strong,}
      \AttributeTok{col =} \StringTok{"red"}\NormalTok{)}

\FunctionTok{legend}\NormalTok{(}
  \StringTok{\textquotesingle{}topleft\textquotesingle{}}\NormalTok{,}
  \AttributeTok{legend  =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Neutral\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Weak\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Strong\textquotesingle{}}\NormalTok{),}
  \AttributeTok{col     =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{),}
  \AttributeTok{lty     =} \DecValTok{1}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{07-generalized-linear-models_files/figure-latex/unnamed-chunk-31-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{predict}\NormalTok{(Multinomial\_Step,}\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{age =} \DecValTok{34}\NormalTok{)) }\CommentTok{\# predicted result (categoriy of political strength) of 34 year old}
\CommentTok{\#\textgreater{} [1] Weak}
\CommentTok{\#\textgreater{} Levels: Neutral Strong Weak}
\FunctionTok{predict}\NormalTok{(Multinomial\_Step,}\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{age =} \FunctionTok{c}\NormalTok{(}\DecValTok{34}\NormalTok{,}\DecValTok{35}\NormalTok{)),}\AttributeTok{type=}\StringTok{"probs"}\NormalTok{) }\CommentTok{\# predicted result of the probabilities of each level of political strength for a 34 and 35}
\CommentTok{\#\textgreater{}     Neutral    Strong      Weak}
\CommentTok{\#\textgreater{} 1 0.2597275 0.3556910 0.3845815}
\CommentTok{\#\textgreater{} 2 0.2594080 0.3587639 0.3818281}
\end{Highlighting}
\end{Shaded}

If categories are ordered (i.e., ordinal data), we must use another approach (still multinomial, but use cumulative probabilities).

Another example

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(agridat)}
\NormalTok{dat }\OtherTok{\textless{}{-}}\NormalTok{ agridat}\SpecialCharTok{::}\NormalTok{streibig.competition}
\CommentTok{\# See Schaberger and Pierce, pages 370+}
\CommentTok{\# Consider only the mono{-}species barley data (no competition from Sinapis)}
\NormalTok{gammaDat }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(dat, sseeds }\SpecialCharTok{\textless{}} \DecValTok{1}\NormalTok{)}
\NormalTok{gammaDat }\OtherTok{\textless{}{-}}
    \FunctionTok{transform}\NormalTok{(gammaDat,}
              \AttributeTok{x =}\NormalTok{ bseeds,}
              \AttributeTok{y =}\NormalTok{ bdwt,}
              \AttributeTok{block =} \FunctionTok{factor}\NormalTok{(block))}
\CommentTok{\# Inverse yield looks like it will be a good fit for Gamma\textquotesingle{}s inverse link}
\FunctionTok{ggplot}\NormalTok{(gammaDat, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =} \DecValTok{1} \SpecialCharTok{/}\NormalTok{ y)) }\SpecialCharTok{+} 
    \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color =}\NormalTok{ block, }\AttributeTok{shape =}\NormalTok{ block)) }\SpecialCharTok{+}
    \FunctionTok{xlab}\NormalTok{(}\StringTok{\textquotesingle{}Seeding Rate\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+} 
    \FunctionTok{ylab}\NormalTok{(}\StringTok{\textquotesingle{}Inverse yield\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+} 
    \FunctionTok{ggtitle}\NormalTok{(}\StringTok{\textquotesingle{}Streibig Competion {-} Barley only\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{07-generalized-linear-models_files/figure-latex/unnamed-chunk-33-1} \end{center}

\[
Y \sim Gamma
\]

because Gamma is non-negative as opposed to Normal. The canonical Gamma link function is the inverse (or reciprocal) link

\[
\begin{aligned}
\eta_{ij} &= \beta_{0j} + \beta_{1j}x_{ij} + \beta_2x_{ij}^2 \\
Y_{ij} &= \eta_{ij}^{-1}
\end{aligned}
\]

The linear predictor is a quadratic model fit to each of the j-th blocks. A different model (not fitted) could be one with common slopes: \texttt{glm(y\ \textasciitilde{}\ x\ +\ I(x\^{}2),â€¦)}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# linear predictor is quadratic, with separate intercept and slope per block}
\NormalTok{m1 }\OtherTok{\textless{}{-}}
    \FunctionTok{glm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ block }\SpecialCharTok{+}\NormalTok{ block }\SpecialCharTok{*}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ block }\SpecialCharTok{*} \FunctionTok{I}\NormalTok{(x }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{),}
        \AttributeTok{data =}\NormalTok{ gammaDat,}
        \AttributeTok{family =} \FunctionTok{Gamma}\NormalTok{(}\AttributeTok{link =} \StringTok{"inverse"}\NormalTok{))}
\FunctionTok{summary}\NormalTok{(m1)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} glm(formula = y \textasciitilde{} block + block * x + block * I(x\^{}2), family = Gamma(link = "inverse"), }
\CommentTok{\#\textgreater{}     data = gammaDat)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Deviance Residuals: }
\CommentTok{\#\textgreater{}      Min        1Q    Median        3Q       Max  }
\CommentTok{\#\textgreater{} {-}1.21708  {-}0.44148   0.02479   0.17999   0.80745  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}                  Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)     1.115e{-}01  2.870e{-}02   3.886 0.000854 ***}
\CommentTok{\#\textgreater{} blockB2        {-}1.208e{-}02  3.880e{-}02  {-}0.311 0.758630    }
\CommentTok{\#\textgreater{} blockB3        {-}2.386e{-}02  3.683e{-}02  {-}0.648 0.524029    }
\CommentTok{\#\textgreater{} x              {-}2.075e{-}03  1.099e{-}03  {-}1.888 0.072884 .  }
\CommentTok{\#\textgreater{} I(x\^{}2)          1.372e{-}05  9.109e{-}06   1.506 0.146849    }
\CommentTok{\#\textgreater{} blockB2:x       5.198e{-}04  1.468e{-}03   0.354 0.726814    }
\CommentTok{\#\textgreater{} blockB3:x       7.475e{-}04  1.393e{-}03   0.537 0.597103    }
\CommentTok{\#\textgreater{} blockB2:I(x\^{}2) {-}5.076e{-}06  1.184e{-}05  {-}0.429 0.672475    }
\CommentTok{\#\textgreater{} blockB3:I(x\^{}2) {-}6.651e{-}06  1.123e{-}05  {-}0.592 0.560012    }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} (Dispersion parameter for Gamma family taken to be 0.3232083)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Null deviance: 13.1677  on 29  degrees of freedom}
\CommentTok{\#\textgreater{} Residual deviance:  7.8605  on 21  degrees of freedom}
\CommentTok{\#\textgreater{} AIC: 225.32}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Fisher Scoring iterations: 5}
\end{Highlighting}
\end{Shaded}

For predict new value of \(x\)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{newdf }\OtherTok{\textless{}{-}}
    \FunctionTok{expand.grid}\NormalTok{(}\AttributeTok{x =} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{120}\NormalTok{, }\AttributeTok{length =} \DecValTok{50}\NormalTok{), }\AttributeTok{block =} \FunctionTok{factor}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}B1\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}B2\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}B3\textquotesingle{}}\NormalTok{)))}

\NormalTok{newdf}\SpecialCharTok{$}\NormalTok{pred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(m1, }\AttributeTok{new =}\NormalTok{ newdf, }\AttributeTok{type =} \StringTok{\textquotesingle{}response\textquotesingle{}}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(gammaDat, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ y)) }\SpecialCharTok{+} 
    \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color =}\NormalTok{ block, }\AttributeTok{shape =}\NormalTok{ block)) }\SpecialCharTok{+}
    \FunctionTok{xlab}\NormalTok{(}\StringTok{\textquotesingle{}Seeding Rate\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{ylab}\NormalTok{(}\StringTok{\textquotesingle{}Inverse yield\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+} 
    \FunctionTok{ggtitle}\NormalTok{(}\StringTok{\textquotesingle{}Streibig Competion {-} Barley only Predictions\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{data =}\NormalTok{ newdf, }\FunctionTok{aes}\NormalTok{(}
        \AttributeTok{x =}\NormalTok{ x,}
        \AttributeTok{y =}\NormalTok{ pred,}
        \AttributeTok{color =}\NormalTok{ block,}
        \AttributeTok{linetype =}\NormalTok{ block}
\NormalTok{    ))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{07-generalized-linear-models_files/figure-latex/unnamed-chunk-35-1} \end{center}

\hypertarget{generalization}{%
\section{Generalization}\label{generalization}}

We can see that Poisson regression looks similar to logistic regression. Hence, we can generalize to a class of modeling. Thanks to \citet{nelder1972generalized}, we have the \textbf{generalized linear models} (GLMs). Estimation is generalize in these models.

\textbf{Exponential Family}\\
The theory of GLMs is developed for data with distribution given y the \textbf{exponential family}.\\
The form of the data distribution that is useful for GLMs is

\[
f(y;\theta, \phi) = \exp(\frac{\theta y - b(\theta)}{a(\phi)} + c(y, \phi))
\]

where

\begin{itemize}
\tightlist
\item
  \(\theta\) is called the natural parameter
\item
  \(\phi\) is called the dispersion parameter
\end{itemize}

\textbf{Note}:

This family includes the \protect\hyperlink{gamma}{Gamma}, \protect\hyperlink{normal}{Normal}, \protect\hyperlink{poisson}{Poisson}, and other. For all parameterization of the exponential family, check this \href{https://www.stat.purdue.edu/~tlzhang/stat526/logistic.pdf}{link}

\textbf{Example}

if we have \(Y \sim N(\mu, \sigma^2)\)

\[
\begin{aligned}
f(y; \mu, \sigma^2) &= \frac{1}{(2\pi \sigma^2)^{1/2}}\exp(-\frac{1}{2\sigma^2}(y- \mu)^2) \\
&= \exp(-\frac{1}{2\sigma^2}(y^2 - 2y \mu +\mu^2)- \frac{1}{2}\log(2\pi \sigma^2)) \\
&= \exp(\frac{y \mu - \mu^2/2}{\sigma^2} - \frac{y^2}{2\sigma^2} - \frac{1}{2}\log(2\pi \sigma^2)) \\
&= \exp(\frac{\theta y - b(\theta)}{a(\phi)} + c(y , \phi))
\end{aligned}
\]

where

\begin{itemize}
\tightlist
\item
  \(\theta = \mu\)
\item
  \(b(\theta) = \frac{\mu^2}{2}\)
\item
  \(a(\phi) = \sigma^2 = \phi\)
\item
  \(c(y , \phi) = - \frac{1}{2}(\frac{y^2}{\phi}+\log(2\pi \sigma^2))\)
\end{itemize}

\textbf{Properties of GLM exponential families}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \(E(Y) = b' (\theta)\) where \(b'(\theta) = \frac{\partial b(\theta)}{\partial \theta}\) (here \texttt{\textquotesingle{}} is ``prime'', not transpose)
\item
  \(var(Y) = a(\phi)b''(\theta)= a(\phi)V(\mu)\).

  \begin{itemize}
  \tightlist
  \item
    \(V(\mu)\) is the \emph{variance function}; however, it is only the variance in the case that \(a(\phi) =1\)
  \end{itemize}
\item
  If \(a(), b(), c()\) are identifiable, we will derive expected value and variance of Y.
\end{enumerate}

Example

Normal distribution

\[
\begin{aligned}
b'(\theta) &= \frac{\partial b(\mu^2/2)}{\partial \mu} = \mu \\
V(\mu) &= \frac{\partial^2 (\mu^2/2)}{\partial \mu^2} = 1 \\
\to var(Y) &= a(\phi) = \sigma^2
\end{aligned}
\]

Poisson distribution

\[
\begin{aligned}
f(y, \theta, \phi) &= \frac{\mu^y \exp(-\mu)}{y!} \\
&= \exp(y\log(\mu) - \mu - \log(y!)) \\
&= \exp(y\theta - \exp(\theta) - \log(y!))
\end{aligned}
\]

where

\begin{itemize}
\tightlist
\item
  \(\theta = \log(\mu)\)
\item
  \(a(\phi) = 1\)
\item
  \(b(\theta) = \exp(\theta)\)
\item
  \(c(y, \phi) = \log(y!)\)
\end{itemize}

Hence,

\[
\begin{aligned}
E(Y) = \frac{\partial b(\theta)}{\partial \theta} = \exp(\theta) &= \mu \\
var(Y) = \frac{\partial^2 b(\theta)}{\partial \theta^2} &= \mu
\end{aligned}
\]

Since \(\mu = E(Y) = b'(\theta)\)

In GLM, we take some monotone function (typically nonlinear) of \(\mu\) to be linear in the set of covariates

\[
g(\mu) = g(b'(\theta)) = \mathbf{x'\beta}
\]

Equivalently,

\[
\mu = g^{-1}(\mathbf{x'\beta})
\]

where \(g(.)\) is the \textbf{link function} since it links mean response (\(\mu = E(Y)\)) and a linear expression of the covariates

Some people use \(\eta = \mathbf{x'\beta}\) where \(\eta\) = the ``linear predictor''

\textbf{GLM is composed of 2 components}

The \textbf{random component}:

\begin{itemize}
\item
  is the distribution chosen to model the response variables \(Y_1,...,Y_n\)
\item
  is specified by the choice fo \(a(), b(), c()\) in the exponential form
\item
  Notation:

  \begin{itemize}
  \tightlist
  \item
    Assume that there are n \textbf{independent} response variables \(Y_1,...,Y_n\) with densities\\
    \[
    f(y_i ; \theta_i, \phi) = \exp(\frac{\theta_i y_i - b(\theta_i)}{a(\phi)}+ c(y_i, \phi))
    \] notice each observation might have different densities
  \item
    Assume that \(\phi\) is constant for all \(i = 1,...,n\), but \(\theta_i\) will vary. \(\mu_i = E(Y_i)\) for all i.
  \end{itemize}
\end{itemize}

The \textbf{systematic component}

\begin{itemize}
\item
  is the portion of the model that gives the relation between \(\mu\) and the covariates \(\mathbf{x}\)
\item
  consists of 2 parts:

  \begin{itemize}
  \tightlist
  \item
    the \emph{link} function, \(g(.)\)
  \item
    the \emph{linear predictor}, \(\eta = \mathbf{x'\beta}\)
  \end{itemize}
\item
  Notation:

  \begin{itemize}
  \tightlist
  \item
    assume \(g(\mu_i) = \mathbf{x'\beta} = \eta_i\) where \(\mathbf{\beta} = (\beta_1,..., \beta_p)'\)
  \item
    The parameters to be estimated are \(\beta_1,...\beta_p , \phi\)
  \end{itemize}
\end{itemize}

\textbf{The Canonical Link}

To choose \(g(.)\), we can use \textbf{canonical link function} (Remember: Canonical link is just a special case of the link function)

If the link function \(g(.)\) is such \(g(\mu_i) = \eta_i = \theta_i\), the natural parameter, then \(g(.)\) is the canonical link.

\begin{center}\includegraphics[width=0.9\linewidth]{images/GLM} \end{center}

\begin{itemize}
\tightlist
\item
  \(b(\theta)\) = cumulant moment generating function
\item
  \(g(\mu)\) is the link function, which relates the linear predictor to the mean and is required to be monotone increasing, continuously differentiable and invertible.
\end{itemize}

Equivalently, we can think of canonical link function as

\[
\gamma^{-1} \circ g^{-1} = I
\] which is the identity. Hence,

\[
\theta = \eta
\]

\textbf{The inverse link}

\(g^{-1}(.)\) is also known as the mean function, take linear predictor output (ranging from \(-\infty\) to \(\infty\)) and transform it into a different scale.

\begin{itemize}
\item
  \textbf{Exponential}: converts \(\mathbf{\beta X}\) into a curve that is restricted between 0 and \(\infty\) (which you can see that is useful in case you want to convert a linear predictor into a non-negative value). \(\lambda = \exp(y) = \mathbf{\beta X}\)
\item
  \textbf{Inverse Logit} (also known as logistic): converts \(\mathbf{\beta X}\) into a curve that is restricted between 0 and 1, which is useful in case you want to convert a linear predictor to a probability. \(\theta = \frac{1}{1 + \exp(-y)} = \frac{1}{1 + \exp(- \mathbf{\beta X})}\)

  \begin{itemize}
  \tightlist
  \item
    \(y\) = linear predictor value
  \item
    \(\theta\) = transformed value
  \end{itemize}
\end{itemize}

The \textbf{identity link} is that

\[
\begin{aligned}
\eta_i &= g(\mu_i) = \mu_i \\
\mu_i &= g^{-1}(\eta_i) = \eta_i
\end{aligned}
\]

\begin{center}\includegraphics[width=0.9\linewidth]{images/2-Table15.1-1} \end{center}

Table 15.1 Generalized Linear Models 15.1 the Structure of Generalized Linear Models

More example on the link functions and their inverses can be found on \href{https://www.sagepub.com/sites/default/files/upm-binaries/21121_Chapter_15.pdf}{page 380}

Example

Normal random component

\begin{itemize}
\item
  Mean Response: \(\mu_i = \theta_i\)
\item
  Canonical Link: \(g( \mu_i) = \mu_i\) (the identity link)
\end{itemize}

Binomial random component

\begin{itemize}
\item
  Mean Response: \(\mu_i = \frac{n_i \exp( \theta)}{1+\exp (\theta_i)}\) and \(\theta(\mu_i) = \log(\frac{p_i }{1-p_i}) = \log (\frac{\mu_i} {n_i - \mu_i})\)
\item
  Canonical link: \(g(\mu_i) = \log(\frac{\mu_i} {n_i - \mu_i})\) (logit link)
\end{itemize}

Poisson random component

\begin{itemize}
\item
  Mean Response: \(\mu_i = \exp(\theta_i)\)
\item
  Canonical Link: \(g(\mu_i) = \log(\mu_i)\)
\end{itemize}

Gamma random component:

\begin{itemize}
\item
  Mean response: \(\mu_i = -\frac{1}{\theta_i}\) and \(\theta(\mu_i) = - \mu_i^{-1}\)
\item
  Canonical Link: \(g(\mu\_i) = - \frac{1}{\mu_i}\)
\end{itemize}

Inverse Gaussian random

\begin{itemize}
\tightlist
\item
  Canonical Link: \(g(\mu_i) = \frac{1}{\mu_i^2}\)
\end{itemize}

\hypertarget{estimation-1}{%
\subsection{Estimation}\label{estimation-1}}

\begin{itemize}
\tightlist
\item
  MLE for parameters of the \textbf{systematic component (}\(\beta\))
\item
  Unification of derivation and computation (thanks to the exponential forms)
\item
  No unification for estimation of the dispersion parameter (\(\phi\))
\end{itemize}

\hypertarget{estimation-of-beta}{%
\subsubsection{\texorpdfstring{Estimation of \(\beta\)}{Estimation of \textbackslash beta}}\label{estimation-of-beta}}

We have

\[
\begin{aligned}
f(y_i ; \theta_i, \phi) &= \exp(\frac{\theta_i y_i - b(\theta_i)}{a(\phi)}+ c(y_i, \phi)) \\
E(Y_i) &= \mu_i = b'(\theta) \\
var(Y_i) &= b''(\theta)a(\phi) = V(\mu_i)a(\phi) \\
g(\mu_i) &= \mathbf{x}_i'\beta = \eta_i
\end{aligned}
\]

If the log-likelihood for a single observation is \(l_i (\beta,\phi)\). The log-likelihood for all n observations is

\[
\begin{aligned}
l(\beta,\phi) &= \sum_{i=1}^n l_i (\beta,\phi) \\
&= \sum_{i=1}^n (\frac{\theta_i y_i - b(\theta_i)}{a(\phi)}+ c(y_i, \phi))
\end{aligned}
\]

Using MLE to find \(\beta\), we use the chain rule to get the derivatives

\[
\begin{aligned}
\frac{\partial l_i (\beta,\phi)}{\partial \beta_j} &=  \frac{\partial l_i (\beta, \phi)}{\partial \theta_i} \times \frac{\partial \theta_i}{\partial \mu_i} \times \frac{\partial \mu_i}{\partial \eta_i}\times \frac{\partial \eta_i}{\partial \beta_j} \\
&= \sum_{i=1}^{n}(\frac{ y_i - \mu_i}{a(\phi)} \times \frac{1}{V(\mu_i)} \times \frac{\partial \mu_i}{\partial \eta_i} \times x_{ij})
\end{aligned}
\]

If we let

\[
w_i \equiv ((\frac{\partial \eta_i}{\partial \mu_i})^2 V(\mu_i))^{-1}
\]

Then,

\[
\frac{\partial l_i (\beta,\phi)}{\partial \beta_j} = \sum_{i=1}^n (\frac{y_i \mu_i}{a(\phi)} \times w_i \times \frac{\partial \eta_i}{\partial \mu_i} \times x_{ij})
\]

We can also get the second derivatives using the chain rule.

Example:

For the \[Newton-Raphson\] algorithm, we need

\[
- E(\frac{\partial^2 l(\beta,\phi)}{\partial \beta_j \partial \beta_k})
\]

where \((j,k)\)-th element of the \textbf{Fisher information matrix} \(\mathbf{I}(\beta)\)

Hence,

\[
- E(\frac{\partial^2 l(\beta,\phi)}{\partial \beta_j \partial \beta_k}) = \sum_{i=1}^n \frac{w_i}{a(\phi)}x_{ij}x_{ik}
\]

for the (j,k)th element

If Bernoulli model with logit link function (which is the canonical link)

\[
\begin{aligned}
b(\theta) &= \log(1 + \exp(\theta)) = \log(1 + \exp(\mathbf{x'\beta})) \\
a(\phi) &= 1  \\
c(y_i, \phi) &= 0 \\
E(Y) = b'(\theta) &= \frac{\exp(\theta)}{1 + \exp(\theta)} = \mu = p \\
\eta = g(\mu) &= \log(\frac{\mu}{1-\mu}) = \theta = \log(\frac{p}{1-p}) = \mathbf{x'\beta} 
\end{aligned}
\]

For \(Y_i\), i = 1,.., the log-likelihood is

\[
l_i (\beta, \phi) = \frac{y_i \theta_i - b(\theta_i)}{a(\phi)} + c(y_i, \phi) = y_i \mathbf{x}'_i \beta - \log(1+ \exp(\mathbf{x'\beta}))
\]

Additionally,

\[
\begin{aligned}
V(\mu_i) &= \mu_i(1-\mu_i)= p_i (1-p_i) \\
\frac{\partial \mu_i}{\partial \eta_i} &= p_i(1-p_i)
\end{aligned}
\]

Hence,

\[
\begin{aligned}
\frac{\partial l(\beta, \phi)}{\partial \beta_j} &= \sum_{i=1}^n[\frac{y_i - \mu_i}{a(\phi)} \times \frac{1}{V(\mu_i)}\times \frac{\partial \mu_i}{\partial \eta_i} \times x_{ij}] \\
&= \sum_{i=1}^n (y_i - p_i) \times \frac{1}{p_i(1-p_i)} \times p_i(1-p_i) \times x_{ij} \\
&= \sum_{i=1}^n (y_i - p_i) x_{ij} \\
&= \sum_{i=1}^n (y_i - \frac{\exp(\mathbf{x'_i\beta})}{1+ \exp(\mathbf{x'_i\beta})})x_{ij}
\end{aligned}
\]

then

\[
w_i = ((\frac{\partial \eta_i}{\partial \mu_i})^2 V(\mu_i))^{-1} = p_i (1-p_i)
\]

\[
\mathbf{I}_{jk}(\mathbf{\beta}) = \sum_{i=1}^n \frac{w_i}{a(\phi)} x_{ij}x_{ik} = \sum_{i=1}^n p_i (1-p_i)x_{ij}x_{ik}
\]

The \textbf{Fisher-scoring} algorithm for the MLE of \(\mathbf{\beta}\) is

\[
\left(
\begin{array}
{c}
\beta_1 \\
\beta_2 \\
. \\
. \\
. \\
\beta_p \\
\end{array}
\right)^{(m+1)}
=
\left(
\begin{array}
{c}
\beta_1 \\
\beta_2 \\
. \\
. \\
. \\
\beta_p \\
\end{array}
\right)^{(m)} +
\mathbf{I}^{-1}(\mathbf{\beta})
\left(
\begin{array}
{c}
\frac{\partial l (\beta, \phi)}{\partial \beta_1} \\
\frac{\partial l (\beta, \phi)}{\partial \beta_2} \\
. \\
. \\
. \\
\frac{\partial l (\beta, \phi)}{\partial \beta_p} \\
\end{array}
\right)|_{\beta = \beta^{(m)}}
\]

Similar to \[Newton-Raphson\] expect the matrix of second derivatives by the expected value of the second derivative matrix.

In matrix notation,

\[
\begin{aligned}
\frac{\partial l }{\partial \beta} &= \frac{1}{a(\phi)}\mathbf{X'W\Delta(y - \mu)} \\
&= \frac{1}{a(\phi)}\mathbf{F'V^{-1}(y - \mu)} \\
\end{aligned}
\]

\[
\mathbf{I}(\beta) = \frac{1}{a(\phi)}\mathbf{X'WX} = \frac{1}{a(\phi)}\mathbf{F'V^{-1}F}
\]

where

\begin{itemize}
\tightlist
\item
  \(\mathbf{X}\) is an \(n \times p\) matrix of covariates
\item
  \(\mathbf{W}\) is an \(n \times n\) diagonal matrix with \((i,i)\)-th element given by \(w_i\)
\item
  \(\mathbf{\Delta}\) an \(n \times n\) diagonal matrix with \((i,i)\)-th element given by \(\frac{\partial \eta_i}{\partial \mu_i}\)
\item
  \(\mathbf{F} = \mathbf{\frac{\partial \mu}{\partial \beta}}\) an \(n \times p\) matrix with \(i\)-th row \(\frac{\partial \mu_i}{\partial \beta} = (\frac{\partial \mu_i}{\partial \eta_i})\mathbf{x}'_i\)
\item
  \(\mathbf{V}\) an \(n \times n\) diagonal matrix with \((i,i)\)-th element given by \(V(\mu_i)\)
\end{itemize}

Setting the derivative of the log-likelihood equal to 0, ML estimating equations are

\[
\mathbf{F'V^{-1}y= F'V^{-1}\mu}
\]

where all components of this equation expect y depends on the parameters \(\beta\)

\textbf{Special Cases}

If one has a canonical link, the estimating equations reduce to

\[
\mathbf{X'y= X'\mu}
\]

If one has an identity link, then

\[
\mathbf{X'V^{-1}y = X'V^{-1}X\hat{\beta}}
\]

which gives the generalized least squares estimator

Generally, we can rewrite the Fisher-scoring algorithm as

\[
\beta^{(m+1)} = \beta^{(m)} + \mathbf{(\hat{F}'\hat{V}^{-1}\hat{F})^{-1}\hat{F}'\hat{V}^{-1}(y- \hat{\mu})}
\]

Since \(\hat{F},\hat{V}, \hat{\mu}\) depend on \(\beta\), we evaluate at \(\beta^{(m)}\)

From starting values \(\beta^{(0)}\), we can iterate until convergence.

Notes:

\begin{itemize}
\tightlist
\item
  if \(a(\phi)\) is a constant or of the form \(m_i \phi\) with known \(m_i\), then \(\phi\) cancels.
\end{itemize}

\hypertarget{estimation-of-phi}{%
\subsubsection{\texorpdfstring{Estimation of \(\phi\)}{Estimation of \textbackslash phi}}\label{estimation-of-phi}}

2 approaches:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  MLE
\end{enumerate}

\[
\frac{\partial l_i}{\partial \phi} = \frac{(\theta_i y_i - b(\theta_i)a'(\phi))}{a^2(\phi)} + \frac{\partial c(y_i,\phi)}{\partial \phi}
\]

the MLE of \(\phi\) solves

\[
\frac{a^2(\phi)}{a'(\phi)}\sum_{i=1}^n \frac{\partial c(y_i, \phi)}{\partial \phi} = \sum_{i=1}^n(\theta_i y_i - b(\theta_i))
\]

\begin{itemize}
\item
  Situation others than normal error case, expression for \(\frac{\partial c(y,\phi)}{\partial \phi}\) are not simple
\item
  Even for the canonical link and \(a(\phi)\) constant, there is no nice general expression for \(-E(\frac{\partial^2 l}{\partial \phi^2})\), so the unification GLMs provide for estimation of \(\beta\) breaks down for \(\phi\)
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  Moment Estimation (``Bias Corrected \(\chi^2\)'')

  \begin{itemize}
  \tightlist
  \item
    The MLE is not conventional approach to estimation of \(\phi\) in GLMS.
  \item
    For the exponential family \(var(Y) =V(\mu)a(\phi)\). This implies\\
    \[
    \begin{aligned}
    a(\phi) &= \frac{var(Y)}{V(\mu)} = \frac{E(Y- \mu)^2}{V(\mu)} \\
    a(\hat{\phi})  &= \frac{1}{n-p} \sum_{i=1}^n \frac{(y_i -\hat{\mu}_i)^2}{V(\hat{\mu})}
    \end{aligned}
    \] where \(p\) is the dimension of \(\beta\)
  \item
    GLM with canonical link function \(g(.)= (b'(.))^{-1}\)\\
    \[
    \begin{aligned}
    g(\mu) &= \theta = \eta = \mathbf{x'\beta} \\
    \mu &= g^{-1}(\eta)= b'(\eta)
    \end{aligned}
    \]
  \item
    so the method estimator for \(a(\phi)=\phi\) is
  \end{itemize}
\end{enumerate}

\[
\hat{\phi} = \frac{1}{n-p} \sum_{i=1}^n \frac{(y_i - g^{-1}(\hat{\eta}_i))^2}{V(g^{-1}(\hat{\eta}_i))}
\]

\hypertarget{inference-2}{%
\subsection{Inference}\label{inference-2}}

We have

\[
\hat{var}(\beta) = a(\phi)(\mathbf{\hat{F}'\hat{V}\hat{F}})^{-1}
\]

where

\begin{itemize}
\tightlist
\item
  \(\mathbf{V}\) is an \(n \times n\) diagonal matrix with diagonal elements given by \(V(\mu_i)\)
\item
  \(\mathbf{F}\) is an \(n \times p\) matrix given by \(\mathbf{F} = \frac{\partial \mu}{\partial \beta}\)
\item
  Both \(\mathbf{V,F}\) are dependent on the mean \(\mu\), and thus \(\beta\). Hence, their estimates (\(\mathbf{\hat{V},\hat{F}}\)) depend on \(\hat{\beta}\).
\end{itemize}

\[
H_0: \mathbf{L\beta = d}
\]

where \(\mathbf{L}\) is a q x p matrix with a \textbf{Wald} test

\[
W = \mathbf{(L \hat{\beta}-d)'(a(\phi)L(\hat{F}'\hat{V}^{-1}\hat{F})L')^{-1}(L \hat{\beta}-d)}
\]

which follows \(\chi_q^2\) distribution (asymptotically), where \(q\) is the rank of \(\mathbf{L}\)

In the simple case \(H_0: \beta_j = 0\) gives \(W = \frac{\hat{\beta}^2_j}{\hat{var}(\hat{\beta}_j)} \sim \chi^2_1\) asymptotically

Likelihood ratio test

\[
\Lambda = 2 (l(\hat{\beta}_f)-l(\hat{\beta}_r)) \sim \chi^2_q
\]

where

\begin{itemize}
\tightlist
\item
  \(q\) is the number of constraints used to fit the reduced model \(\hat{\beta}_r\), and \(\hat{\beta}_r\) is the fit under the full model.
\end{itemize}

Wald test is easier to implement, but likelihood ratio test is better (especially for small samples).

\hypertarget{deviance}{%
\subsection{Deviance}\label{deviance}}

\protect\hyperlink{deviance}{Deviance} is necessary for goodness of fit, inference and for alternative estimation of the dispersion parameter. We define and consider \protect\hyperlink{deviance}{Deviance} from a likelihood ratio perspective.

\begin{itemize}
\item
  Assume that \(\phi\) is known. Let \(\tilde{\theta}\) denote the full and \(\hat{\theta}\) denote the reduced model MLEs. Then, the likelihood ratio (2 times the difference in log-likelihoods) is \[
  2\sum_{i=1}^{n} \frac{y_i (\tilde{\theta}_i- \hat{\theta}_i)-b(\tilde{\theta}_i) + b(\hat{\theta}_i)}{a_i(\phi)}
  \]
\item
  For exponential families, \(\mu = E(y) = b'(\theta)\), so the natural parameter is a function of \(\mu: \theta = \theta(\mu) = b'^{-1}(\mu)\), and the likelihood ratio turns into\\
  \[
  2 \sum_{i=1}^m \frac{y_i\{\theta(\tilde{\mu}_i - \theta(\hat{\mu}_i)\} - b(\theta(\tilde{\mu}_i)) + b(\theta(\hat{\mu}_i))}{a_i(\phi)}
  \]
\item
  Comparing a fitted model to ``the fullest possible model'', which is the \textbf{saturated model}: \(\tilde{\mu}_i = y_i\), i = 1,..,n.~If \(\tilde{\theta}_i^* = \theta(y_i), \hat{\theta}_i^* = \theta (\hat{\mu})\), the likelihood ratio is\\
  \[
  2 \sum_{i=1}^{n} \frac{y_i (\tilde{\theta}_i^* - \hat{\theta}_i^* + b(\hat{\theta}_i^*))}{a_i(\phi)}
  \]
\item
  \citep{McCullagh_2019} specify \(a(\phi) = \phi\), then the likelihood ratio can be written as\\
  \[
  D^*(\mathbf{y, \hat{\mu}}) = \frac{2}{\phi}\sum_{i=1}^n\{y_i (\tilde{\theta}_i^*- \hat{\theta}_i^*)- b(\tilde{\theta}_i^*) +b(\hat{\theta}_i^*)  \}  
  \] where
\item
  \(D^*(\mathbf{y, \hat{\mu}})\) = \textbf{scaled deviance}
\item
  \(D(\mathbf{y, \hat{\mu}}) = \phi D^*(\mathbf{y, \hat{\mu}})\) = \textbf{deviance}
\end{itemize}

\textbf{Note}:

\begin{itemize}
\item
  in some random component distributions, we can write \(a_i(\phi) = \phi m_i\), where

  \begin{itemize}
  \tightlist
  \item
    \(m_i\) is some known scalar that may change with the observations. Then, the scaled deviance components are divided by \(m_i\):\\
    \[
    D^*(\mathbf{y, \hat{\mu}}) \equiv 2\sum_{i=1}^n\{y_i (\tilde{\theta}_i^*- \hat{\theta}_i^*)- b(\tilde{\theta}_i^*) +b(\hat{\theta}_i^*)\} / (\phi m_i)  
    \]
  \end{itemize}
\item
  \(D^*(\mathbf{y, \hat{\mu}}) = \sum_{i=1}^n d_i\)m where \(d_i\) is the deviance contribution from the \(i\)-th observation.
\item
  \(D\) is used in model selection
\item
  \(D^*\) is used in goodness of fit tests (as it is a likelihood ratio statistic). \[
  D^*(\mathbf{y, \hat{\mu}}) = 2\{l(\mathbf{y,\tilde{\mu}})-l(\mathbf{y,\hat{\mu}})\}
  \]
\item
  \(d_i\) are used to form \textbf{deviance residuals}
\end{itemize}

\textbf{Normal}

We have

\[
\begin{aligned}
\theta &= \mu \\
\phi &= \sigma^2 \\
b(\theta) &= \frac{1}{2} \theta^2 \\
a(\phi) &= \phi
\end{aligned}
\]

Hence,

\[
\begin{aligned}
\tilde{\theta}_i &= y_i \\
\hat{\theta}_i &= \hat{\mu}_i = g^{-1}(\hat{\eta}_i) 
\end{aligned}
\]

And

\[
\begin{aligned}
D &= 2 \sum_{1=1}^n Y^2_i - y_i \hat{\mu}_i - \frac{1}{2}y^2_i + \frac{1}{2} \hat{\mu}_i^2 \\
&= \sum_{i=1}^n y_i^2 - 2y_i \hat{\mu}_i + \hat{\mu}_i^2 \\
&= \sum_{i=1}^n (y_i - \hat{\mu}_i)^2
\end{aligned}
\]

which is the \textbf{residual sum of squares}

\textbf{Poisson}

\[
\begin{aligned}
f(y) &= \exp\{y\log(\mu) - \mu - \log(y!)\} \\
\theta &= \log(\mu) \\
b(\theta) &= \exp(\theta) \\
a(\phi) &= 1 \\
\tilde{\theta}_i &= \log(y_i) \\
\hat{\theta}_i &= \log(\hat{\mu}_i) \\
\hat{\mu}_i &= g^{-1}(\hat{\eta}_i)
\end{aligned}
\]

Then,

\[
\begin{aligned}
D &= 2 \sum_{i = 1}^n y_i \log(y_i) - y_i \log(\hat{\mu}_i) - y_i + \hat{\mu}_i \\
&= 2 \sum_{i = 1}^n y_i \log(\frac{y_i}{\hat{\mu}_i}) - (y_i - \hat{\mu}_i)
\end{aligned}
\]

and

\[
d_i = 2\{y_i \log(\frac{y_i}{\hat{\mu}})- (y_i - \hat{\mu}_i)\}
\]

\hypertarget{analysis-of-deviance}{%
\subsubsection{Analysis of Deviance}\label{analysis-of-deviance}}

The difference in deviance between a reduced and full model, where q is the difference in the number of free parameters, has an asymptotic \(\chi^2_q\). The likelihood ratio test

\[
D^*(\mathbf{y;\hat{\mu}_r}) - D^*(\mathbf{y;\hat{\mu}_f}) = 2\{l(\mathbf{y;\hat{\mu}_f})-l(\mathbf{y;\hat{\mu}_r})\}
\]

this comparison of models is \textbf{Analysis of Deviance}. \protect\hyperlink{generalized-linear-models}{GLM} uses this analysis for model selection.

An estimation of \(\phi\) is

\[
\hat{\phi} = \frac{D(\mathbf{y, \hat{\mu}})}{n - p}
\]

where \(p\) = number of parameters fit.

Excessive use of \(\chi^2\) test could be problematic since it is asymptotic \citep{McCullagh_2019}

\hypertarget{deviance-residuals}{%
\subsubsection{Deviance Residuals}\label{deviance-residuals}}

We have \(D = \sum_{i=1}^{n}d_i\). Then, we define \textbf{deviance residuals}

\[
r_{D_i} = \text{sign}(y_i -\hat{\mu}_i)\sqrt{d_i}
\]

Standardized version of deviance residuals is

\[
r_{s,i} = \frac{y_i -\hat{\mu}}{\hat{\sigma}(1-h_{ii})^{1/2}}
\]

Let \(\mathbf{H^{GLM} = W^{1/2}X(X'WX)^{-1}X'W^{-1/2}}\), where \(\mathbf{W}\) is an \(n \times n\) diagonal matrix with \((i,i)\)-th element given by \(w_i\) (see \protect\hyperlink{estimation-of-beta}{Estimation of \(\beta\)}). Then Standardized deviance residuals is equivalently

\[
r_{s, D_i} = \frac{r_{D_i}}{\{\hat{\phi}(1-h_{ii}^{glm}\}^{1/2}}
\]

where \(h_{ii}^{glm}\) is the \(i\)-th diagonal of \(\mathbf{H}^{GLM}\)

\hypertarget{pearson-chi-square-residuals}{%
\subsubsection{Pearson Chi-square Residuals}\label{pearson-chi-square-residuals}}

Another \(\chi^2\) statistic is \textbf{Pearson} \(\chi^2\) statistics: (assume \(m_i = 1\))

\[
X^2 = \sum_{i=1}^{n} \frac{(y_i - \hat{\mu}_i)^2}{V(\hat{\mu}_i)}
\]

where \(\hat{\mu}_i\) is the fitted mean response fo the model of interest.

The \textbf{Scaled Pearson} \(\chi^2\) statistic is given by \(\frac{X^2}{\phi} \sim \chi^2_{n-p}\) where p is the number of parameters estimated. Hence, the \textbf{Pearson} \(\chi^2\) residuals are

\[
X^2_i = \frac{(y_i - \hat{\mu}_i)^2}{V(\hat{\mu}_i)}
\]

If we have the following assumptions:

\begin{itemize}
\tightlist
\item
  Independent samples\\
\item
  No over-dispersion: If \(\phi = 1\), \(\frac{D(\mathbf{y;\hat{\mu}})}{n-p}\) and \(\frac{X^2}{n-p}\) have a value substantially larger 1 indicates \textbf{improperly specified model} or \textbf{overdispersion}\\
\item
  Multiple groups
\end{itemize}

then \(\frac{X^2}{\phi}\) and \(D^*(\mathbf{y; \hat{\mu}})\) both follow \(\chi^2_{n-p}\)

\hypertarget{diagnostic-plots}{%
\subsection{Diagnostic Plots}\label{diagnostic-plots}}

\begin{itemize}
\item
  Standardized residual Plots:

  \begin{itemize}
  \tightlist
  \item
    plot(\(r_{s, D_i}\), \(\hat{\mu}_i\)) or plot(\(r_{s, D_i}\), \(T(\hat{\mu}_i)\)) where \(T(\hat{\mu}_i)\) is transformation(\(\hat{\mu}_i\)) called \textbf{constant information scale}:\\
  \item
    plot(\(r_{s, D_i}\), \(\hat{\eta}_i\))
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}cc@{}}
\toprule\noalign{}
Random Component & \(T(\hat{\mu}_i)\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Normal & \(\hat{\mu}\) \\
Poisson & \(2\sqrt{\mu}\) \\
Binomial & \(2 \sin^{-1}(\sqrt{\hat{\mu}})\) \\
Gamma & \(2 \log(\hat{\mu})\) \\
Inverse Gaussian & \(-2\hat{\mu}^{-1/2}\) \\
\end{longtable}

\begin{itemize}
\item
  If we see:

  \begin{itemize}
  \tightlist
  \item
    Trend, it means we might have a wrong link function, or choice of scale\\
  \item
    Systematic change in range of residuals with a change in \(T(\hat{\mu})\) (incorrect random component) (systematic \(\neq\) random)
  \end{itemize}
\item
  plot(\(|r_{D_i}|,\hat{\mu}_i\)) to check \textbf{Variance Function}.
\end{itemize}

\hypertarget{goodness-of-fit}{%
\subsection{Goodness of Fit}\label{goodness-of-fit}}

To assess goodness of fit, we can use

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{deviance}{Deviance}
\item
  \protect\hyperlink{pearson-chi-square-residuals}{Pearson Chi-square Residuals}
\end{itemize}

In nested model, we could use likelihood-based information measures:

\[
\begin{aligned}
AIC &= -2l(\mathbf{\hat{\mu}}) + 2p \\
AICC &= -2l(\mathbf{\hat{\mu}}) + 2p(\frac{n}{n-p-1}) \\
BIC &= 2l(\hat{\mu}) + p \log(n)
\end{aligned}
\]

where

\begin{itemize}
\tightlist
\item
  \(l(\hat{\mu})\) is the log-likelihood evaluated at the parameter estimates
\item
  \(p\) is the number of parameters
\item
  \(n\) is the number of observations.
\end{itemize}

Note: you have to use the same data with the same model (i.e., same link function, same random underlying random distribution). but you can have different number of parameters.

Even though statisticians try to come up with measures that are similar to \(R^2\), in practice, it is not so appropriate. For example, they compare the log-likelihood of the fitted model against the that of a model with just the intercept:

\[
R^2_p = 1 - \frac{l(\hat{\mu})}{l(\hat{\mu}_0)}
\]

For certain specific random components such as binary response model, we have rescaled generalized \(R^2\)

\[
\bar{R}^2 = \frac{R^2_*}{\max(R^2_*)} = \frac{1-\exp\{-\frac{2}{n}(l(\hat{\mu}) - l(\hat{\mu}_0) \}}{1 - \exp\{\frac{2}{n}l(\hat{\mu}_0)\}}
\]

\hypertarget{over-dispersion}{%
\subsection{Over-Dispersion}\label{over-dispersion}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2394}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2817}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4789}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Random Components
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(var(Y)\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(V(\mu)\)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Binomial & \(var(Y) = n \mu (1- \mu)\) & \(V(\mu) = \phi n \mu(1- \mu)\) where \(m_i =n\) \\
Poisson & \(var(Y) = \mu\) & \(V(\mu) = \phi \mu\) \\
\end{longtable}

In both cases \(\phi = 1\). Recall \(b''(\theta)= V(\mu)\) check \protect\hyperlink{estimation-of-phi}{Estimation of \(\phi\)}.

If we find

\begin{itemize}
\tightlist
\item
  \(\phi >1\): over-dispersion (i.e., too much variation for an independent binomial or Poisson distribution).
\item
  \(\phi<1\): under-dispersion (i.e., too little variation for an independent binomial or Poisson distribution).
\end{itemize}

If we have either over or under-dispersion, it means we might have unspecified random component, we could

\begin{itemize}
\tightlist
\item
  Select a different random component distribution that can accommodate over or under-dispersion (e.g., negative binomial, Conway-Maxwell Poisson)
\item
  use \protect\hyperlink{nonlinear-and-generalized-linear-mixed-models}{Nonlinear and Generalized Linear Mixed Models} to handle random effects in generalized linear models.
\end{itemize}

\hypertarget{linear-mixed-models}{%
\chapter{Linear Mixed Models}\label{linear-mixed-models}}

\hypertarget{dependent-data}{%
\section{Dependent Data}\label{dependent-data}}

Forms of dependent data:

\begin{itemize}
\tightlist
\item
  Multivariate measurements on different individuals: (e.g., a person's blood pressure, fat, etc are correlated)
\item
  Clustered measurements: (e.g., blood pressure measurements of people in the same family can be correlated).
\item
  Repeated measurements: (e.g., measurement of cholesterol over time can be correlated) ``If data are collected repeatedly on experimental material to which treatments were applied initially, the data is a repeated measure.'' \citep{Schabenberger_2001}
\item
  Longitudinal data: (e.g., individual's cholesterol tracked over time are correlated): ``data collected repeatedly over time in an observational study are termed longitudinal.'' \citep{Schabenberger_2001}
\item
  Spatial data: (e.g., measurement of individuals living in the same neighborhood are correlated)
\end{itemize}

Hence, we like to account for these correlations.

\textbf{Linear Mixed Model} (LMM), also known as \textbf{Mixed Linear Model} has 2 components:

\begin{itemize}
\item
  \textbf{Fixed effect} (e.g, gender, age, diet, time)
\item
  \textbf{Random effects} representing individual variation or auto correlation/spatial effects that imply \textbf{dependent (correlated) errors}
\end{itemize}

Review \protect\hyperlink{two-way-mixed-effects-anova}{Two-Way Mixed Effects ANOVA}

We choose to model the random subject-specific effect instead of including dummy subject covariates in our model because:

\begin{itemize}
\tightlist
\item
  reduction in the number of parameters to estimate
\item
  when you do inference, it would make more sense that you can infer from a population (i.e., random effect).
\end{itemize}

\textbf{LLM Motivation}

In a repeated measurements analysis where \(Y_{ij}\) is the response for the \(i\)-th individual measured at the \(j\)-th time,

\(i =1,â€¦,N\) ; \(j = 1,â€¦,n_i\)

\[
\mathbf{Y}_i = 
\left(
\begin{array}
{c}
Y_{i1} \\
. \\
.\\
.\\
Y_{in_i}
\end{array}
\right)
\]

is all measurements for subject \(i\).

\ul{\emph{Stage 1: (Regression Model)}} how the response changes over time for the \(i\)-th subject

\[
\mathbf{Y_i = Z_i \beta_i + \epsilon_i}
\]

where

\begin{itemize}
\tightlist
\item
  \(Z_i\) is an \(n_i \times q\) matrix of known covariates
\item
  \(\beta_i\) is an unknown \(q \times 1\) vector of subjective -specific coefficients (regression coefficients different for each subject)
\item
  \(\epsilon_i\) are the random errors (typically \(\sim N(0, \sigma^2 I)\))
\end{itemize}

We notice that there are two many \(\beta\) to estimate here. Hence, this is the motivation for the second stage

\ul{\emph{Stage 2: (Parameter Model)}}

\[
\mathbf{\beta_i = K_i \beta + b_i}
\]

where

\begin{itemize}
\tightlist
\item
  \(K_i\) is a \(q \times p\) matrix of known covariates
\item
  \(\beta\) is a \(p \times 1\) vector of unknown parameter
\item
  \(\mathbf{b}_i\) are independent \(N(0,D)\) random variables
\end{itemize}

This model explain the observed variability between subjects with respect to the subject-specific regression coefficients, \(\beta_i\). We model our different coefficient (\(\beta_i\)) with respect to \(\beta\).

Example:

Stage 1:

\[
Y_{ij} = \beta_{1i} + \beta_{2i}t_{ij} + \epsilon_{ij}
\]

where

\begin{itemize}
\tightlist
\item
  \(j = 1,..,n_i\)
\end{itemize}

In the matrix notation,

\[
\mathbf{Y_i} = 
\left(
\begin{array}
{c}
Y_{i1} \\
.\\
Y_{in_i}
\end{array}
\right); \mathbf{Z}_i = 
\left(
\begin{array}
{cc}
1 & t_{i1} \\
. & . \\
1 & t_{in_i} 
\end{array}
\right)
\]

\[
\beta_i =
\left(
\begin{array}
{c}
\beta_{1i} \\
\beta_{2i}
\end{array}
\right); \epsilon_i = 
\left(
\begin{array}
{c}
\epsilon_{i1} \\
. \\
\epsilon_{in_i}
\end{array}
\right)
\]

Thus,

\[
\mathbf{Y_i = Z_i \beta_i + \epsilon_i}
\]

Stage 2:

\[
\begin{aligned}
\beta_{1i} &= \beta_0 + b_{1i} \\
\beta_{2i} &= \beta_1 L_i + \beta_2 H_i + \beta_3 C_i + b_{2i}
\end{aligned}
\]

where \(L_i, H_i, C_i\) are indicator variables defined to 1 as the subject falls into different categories.

Subject specific intercepts do not depend upon treatment, with \(\beta_0\) (the average response at the start of treatment), and \(\beta_1 , \beta_2, \beta_3\) (the average time effects for each of three treatment groups).

\[
\begin{aligned}
\mathbf{K}_i &= \left(
\begin{array}
{cccc}
1 & 0 & 0 & 0 \\
0 & L_i & H_i & C_i 
\end{array}
\right) \\ 
\beta &= (\beta_0 , \beta_1, \beta_2, \beta_3)' \\ 
\mathbf{b}_i &= 
\left(
\begin{array}
{c}
b_{1i} \\
b_{2i} \\
\end{array}
\right) \\ 
\beta_i &= \mathbf{K_i \beta + b_i}
\end{aligned}
\]

To get \(\hat{\beta}\), we can fit the model sequentially:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Estimate \(\hat{\beta_i}\) in the first stage
\item
  Estimate \(\hat{\beta}\) in the second stage by replacing \(\beta_i\) with \(\hat{\beta}_i\)
\end{enumerate}

However, problems arise from this method:

\begin{itemize}
\tightlist
\item
  information is lost by summarizing the vector \(\mathbf{Y}_i\) solely by \(\hat{\beta}_i\)
\item
  we need to account for variability when replacing \(\beta_i\) with its estimate
\item
  different subjects might have different number of observations.
\end{itemize}

To address these problems, we can use \textbf{Linear Mixed Model} \citep{laird1982random}

Substituting stage 2 into stage 1:

\[
\mathbf{Y}_i = \mathbf{Z}_i \mathbf{K}_i \beta + \mathbf{Z}_i \mathbf{b}_i + \mathbf{\epsilon}_i
\]

Let \(\mathbf{X}_i = \mathbf{Z}_i \mathbf{K}_i\) be an \(n_i \times p\) matrix . Then, the LMM is

\[
\mathbf{Y}_i = \mathbf{X}_i \beta + \mathbf{Z}_i \mathbf{b}_i + \mathbf{\epsilon}_i
\]

where

\begin{itemize}
\tightlist
\item
  \(i = 1,..,N\)
\item
  \(\beta\) are the fixed effects, which are common to all subjects
\item
  \(\mathbf{b}_i\) are the subject specific random effects. \(\mathbf{b}_i \sim N_q (\mathbf{0,D})\)
\item
  \(\mathbf{\epsilon}_i \sim N_{n_i}(\mathbf{0,\Sigma_i})\)
\item
  \(\mathbf{b}_i\) and \(\epsilon_i\) are independent
\item
  \(\mathbf{Z}_{i(n_i \times q})\) and \(\mathbf{X}_{i(n_i \times p})\) are matrices of known covariates.
\end{itemize}

Equivalently, in the hierarchical form, we call \textbf{conditional} or \textbf{hierarchical} formulation of the linear mixed model

\[
\begin{aligned}
\mathbf{Y}_i | \mathbf{b}_i &\sim N(\mathbf{X}_i \beta+ \mathbf{Z}_i \mathbf{b}_i, \mathbf{\Sigma}_i) \\
\mathbf{b}_i &\sim N(\mathbf{0,D})
\end{aligned}
\]

for \(i = 1,..,N\). denote the respective functions by \(f(\mathbf{Y}_i |\mathbf{b}_i)\) and \(f(\mathbf{b}_i)\)

In general,

\[
\begin{aligned}
f(A,B) &= f(A|B)f(B) \\
f(A) &= \int f(A,B)dB = \int f(A|B) f(B) dB
\end{aligned}
\]

In the LMM, the marginal density of \(\mathbf{Y}_i\) is

\[
f(\mathbf{Y}_i) = \int f(\mathbf{Y}_i | \mathbf{b}_i) f(\mathbf{b}_i) d\mathbf{b}_i
\]

which can be shown

\[
\mathbf{Y}_i \sim N(\mathbf{X_i \beta, Z_i DZ'_i + \Sigma_i})
\]

This is the \textbf{marginal} formulation of the linear mixed model

Notes:

We no longer have \(Z_i b_i\) in the mean, but add error in the variance (marginal dependence in Y). kinda of averaging out the common effect. Technically, we shouldn't call it averaging the error b (adding it to the variance covariance matrix), it should be called adding random effect

Continue with our example

\[
Y_{ij} = (\beta_0 + b_{1i}) + (\beta_1L_i + \beta_2 H_i + \beta_3 C_i + b_{2i})t_{ij} + \epsilon_{ij}
\]

for each treatment group

\[
Y_{ik}= 
\begin{cases}
\beta_0 + b_{1i} + (\beta_1 + \ b_{2i})t_{ij} + \epsilon_{ij} & L \\
\beta_0 + b_{1i} + (\beta_2 + \ b_{2i})t_{ij} + \epsilon_{ij} & H\\
\beta_0 + b_{1i} + (\beta_3 + \ b_{2i})t_{ij} + \epsilon_{ij} & C
\end{cases}
\]

\begin{itemize}
\tightlist
\item
  Intercepts and slopes are all subject specific
\item
  Different treatment groups have different slops, but the same intercept.
\end{itemize}

\textbf{In the hierarchical model form}

\[
\begin{aligned}
\mathbf{Y}_i | \mathbf{b}_i &\sim N(\mathbf{X}_i \beta + \mathbf{Z}_i \mathbf{b}_i, \mathbf{\Sigma}_i)\\
\mathbf{b}_i &\sim N(\mathbf{0,D})
\end{aligned}
\]

X will be in the form of

\[
\beta = (\beta_0, \beta_1, \beta_2, \beta_3)'
\]

\[
\begin{aligned}
\mathbf{X}_i &= \mathbf{Z}_i \mathbf{K}_i \\
&= 
\left[
\begin{array}
{cc}
1 & t_{i1} \\
1 & t_{i2} \\
. & . \\
1 & t_{in_i}
\end{array}
\right]
\times
\left[
\begin{array}
{cccc}
1 & 0 & 0 & 0 \\
0 & L_i & H_i & C_i \\
\end{array}
\right] \\
&=
\left[ 
\begin{array}
{cccc}
1 & t_{i1}L_i & t_{i1}H_i & T_{i1}C_i \\
1 & t_{i2}L_i & t_{i2}H_i & T_{i2}C_i \\
. &. &. &. \\
1 & t_{in_i}L_i & t_{in_i}H_i & T_{in_i}C_i \\
\end{array}
\right]\end{aligned}
\]

\[
\mathbf{b}_i = 
\left(
\begin{array}
{c}
b_{1i} \\
b_{2i}
\end{array}
\right)
\]

\[
D = 
\left(
\begin{array}
{cc}
d_{11} & d_{12}\\
d_{12} & d_{22}
\end{array}
\right)
\]

Assuming \(\mathbf{\Sigma}_i = \sigma^2 \mathbf{I}_{n_i}\), which is called \textbf{conditional independence}, meaning the response on subject i are independent conditional on \(\mathbf{b}_i\) and \(\beta\)

\textbf{In the marginal model form}

\[
Y_{ij} = \beta_0 + \beta_1 L_i t_{ij} + \beta_2 H_i t_{ij} + \beta_3 C_i t_{ij} + \eta_{ij}
\]

where \(\eta_i \sim N(\mathbf{0},\mathbf{Z}_i\mathbf{DZ}_i'+ \mathbf{\Sigma}_i)\)

Equivalently,

\[
\mathbf{Y_i \sim N(X_i \beta, Z_i DZ_i' + \Sigma_i})
\]

In this case that \(n_i = 2\)

\[
\begin{aligned}
\mathbf{Z_iDZ_i'} &= 
\left(
\begin{array}
{cc}
1 & t_{i1} \\
1 & t_{i2} 
\end{array}
\right)
\left(
\begin{array}
{cc}
d_{11} & d_{12} \\
d_{12} & d_{22} 
\end{array}
\right)
\left(
\begin{array}
{cc}
1 & 1 \\
t_{i1} & t_{i2} 
\end{array}
\right) \\
&=
\left(
\begin{array}
{cc}
d_{11} + 2d_{12}t_{i1} + d_{22}t_{i1}^2 & d_{11} + d_{12}(t_{i1} + t_{i2}) + d_{22}t_{i1}t_{i2} \\
d_{11} + d_{12}(t_{i1} + t_{i2}) + d_{22} t_{i1} t_{i2} & d_{11} + 2d_{12}t_{i2} + d_{22}t_{i2}^2  
\end{array}
\right)
\end{aligned}
\]

\[
var(Y_{i1}) = d_{11} + 2d_{12}t_{i1} + d_{22} t_{i1}^2 + \sigma^2
\]

On top of correlation in the errors, the marginal implies that the variance function of the response is quadratic over time, with positive curvature \(d_{22}\)

\hypertarget{random-intercepts-model}{%
\subsection{Random-Intercepts Model}\label{random-intercepts-model}}

If we remove the random slopes,

\begin{itemize}
\tightlist
\item
  the assumption is that all variability in subject-specific slopes can be attributed to treatment differences
\item
  the model is random-intercepts model. This has subject specific intercepts, but the same slopes within each treatment group.
\end{itemize}

\[
\begin{aligned}
\mathbf{Y}_i | b_i &\sim N(\mathbf{X}_i \beta + 1 b_i , \Sigma_i) \\
b_i &\sim N(0,d_{11})
\end{aligned}
\]

The marginal model is then (\(\mathbf{\Sigma}_i = \sigma^2 \mathbf{I}\))

\[
\mathbf{Y}_i \sim N(\mathbf{X}_i \beta, 11'd_{11} + \sigma^2 \mathbf{I})
\]

The marginal covariance matrix is

\[
\begin{aligned}
cov(\mathbf{Y}_i)  &= 11'd_{11} + \sigma^2I \\
&=
\left(
\begin{array}
{cccc}
d_{11}+ \sigma^2 & d_{11} & ... & d_{11} \\
d_{11} & d_{11} + \sigma^2 & d_{11} & ... \\
. & . & . & . \\
d_{11} & ... & ... & d_{11} + \sigma^2
\end{array}
\right)
\end{aligned}
\]

the associated correlation matrix is

\[
corr(\mathbf{Y}_i) = 
\left(
\begin{array}
{cccc}
1 & \rho & ... & \rho \\
\rho & 1 & \rho & ... \\
. & . & . & . \\
\rho & ... & ... & 1 \\
\end{array}
\right)
\]

where \(\rho \equiv \frac{d_{11}}{d_{11} + \sigma^2}\)

Thu, we have

\begin{itemize}
\tightlist
\item
  constant variance over time
\item
  equal, positive correlation between any two measurements from the same subject
\item
  a covariance structure that is called \textbf{compound symmetry}, and \(\rho\) is called the \textbf{intra-class correlation}
\item
  that when \(\rho\) is large, the \textbf{inter-subject variability} (\(d_{11}\)) is large relative to the intra-subject variability (\(\sigma^2\))
\end{itemize}

\hypertarget{covariance-models}{%
\subsection{Covariance Models}\label{covariance-models}}

If the conditional independence assumption, (\(\mathbf{\Sigma_i= \sigma^2 I_{n_i}}\)). Consider, \(\epsilon_i = \epsilon_{(1)i} + \epsilon_{(2)i}\), where

\begin{itemize}
\tightlist
\item
  \(\epsilon_{(1)i}\) is a ``serial correlation'' component. That is, part of the individual's profile is a response to time-varying stochastic processes.
\item
  \(\epsilon_{(2)i}\) is the measurement error component, and is independent of \(\epsilon_{(1)i}\)
\end{itemize}

Then

\[
\mathbf{Y_i = X_i \beta + Z_i b_i + \epsilon_{(1)i} + \epsilon_{(2)i}}
\]

where

\begin{itemize}
\item
  \(\mathbf{b_i} \sim N(\mathbf{0,D})\)
\item
  \(\epsilon_{(2)i} \sim N(\mathbf{0,\sigma^2 I_{n_i}})\)
\item
  \(\epsilon_{(1)i} \sim N(\mathbf{0,\tau^2H_i})\)
\item
  \(\mathbf{b}_i\) and \(\epsilon_i\) are mutually independent
\end{itemize}

To model the structure of the \(n_i \times n_i\) correlation (or covariance ) matrix \(\mathbf{H}_i\). Let the (j,k)th element of \(\mathbf{H}_i\) be \(h_{ijk}= g(t_{ij}t_{ik})\). that is a function of the times \(t_{ij}\) and \(t_{ik}\) , which is assumed to be some function of the ``distance' between the times.

\[
h_{ijk} = g(|t_{ij}-t_{ik}|)
\]

for some decreasing function \(g(.)\) with \(g(0)=1\) (for correlation matrices).

Examples of this type of function:

\begin{itemize}
\tightlist
\item
  Exponential function: \(g(|t_{ij}-t_{ik}|) = \exp(-\phi|t_{ij} - t_{ik}|)\)
\item
  Gaussian function: \(g(|t_{ij} - t_{ik}|) = \exp(-\phi(t_{ij} - t_{ik})^2)\)
\end{itemize}

Similar structures could also be used for \(\mathbf{D}\) matrix (of \(\mathbf{b}\))

Example: Autoregressive Covariance Structure

A first order Autoregressive Model (AR(1)) has the form

\[
\alpha_t = \phi \alpha_{t-1} + \eta_t
\]

where \(\eta_t \sim iid N (0,\sigma^2_\eta)\)

Then, the covariance between two observations is

\[
cov(\alpha_t, \alpha_{t+h}) = \frac{\sigma^2_\eta \phi^{|h|}}{1- \phi^2}
\]

for \(h = 0, \pm 1, \pm 2, ...; |\phi|<1\)

Hence,

\[
corr(\alpha_t, \alpha_{t+h}) = \phi^{|h|}
\]

If we let \(\alpha_T = (\alpha_1,...\alpha_T)'\), then

\[
corr(\alpha_T) = 
\left[
\begin{array}
{ccccc}
1 & \phi^1 & \phi^2 & ... & \phi^2 \\
\phi^1 & 1 & \phi^1 & ... & \phi^{T-1} \\
\phi^2 & \phi^1 & 1 & ... & \phi^{T-2} \\
. & . & . & . &. \\
\phi^T & \phi^{T-1} & \phi^{T-2} & ... & 1
\end{array}
\right]
\]

Notes:

\begin{itemize}
\tightlist
\item
  The correlation decreases as time lag increases
\item
  This matrix structure is known as a \textbf{Toeplitz} structure
\item
  More complicated covariance structures are possible, which is critical component of spatial random effects models and time series models.
\item
  Often, we don't need both random effects \(\mathbf{b}\) and \(\epsilon_{(1)i}\)
\end{itemize}

More in the \protect\hyperlink{time-series-1}{Time Series} section

\hypertarget{estimation-2}{%
\section{Estimation}\label{estimation-2}}

\[
\mathbf{Y}_i = \mathbf{X}_i \beta + \mathbf{Z}_i \mathbf{b}_i + \epsilon_i
\]

where \(\beta, \mathbf{b}_i, \mathbf{D}, \mathbf{\Sigma}_i\) we must obtain estimation from the data

\begin{itemize}
\tightlist
\item
  \(\mathbf{\beta}, \mathbf{D}, \mathbf{\Sigma}_i\) are unknown, but fixed, parameters, and must be estimated from the data
\item
  \(\mathbf{b}_i\) is a random variable. Thus, we can't estimate these values, but we can predict them. (i.e., you can't estimate a random thing).
\end{itemize}

If we have

\begin{itemize}
\tightlist
\item
  \(\hat{\beta}\) as an estimator of \(\beta\)
\item
  \(\hat{\mathbf{b}}_i\) as a predictor of \(\mathbf{b}_i\)
\end{itemize}

Then,

\begin{itemize}
\tightlist
\item
  The population average estimate of \(\mathbf{Y}_i\) is \(\hat{\mathbf{Y}_i} = \mathbf{X}_i \hat{\beta}\)
\item
  The subject-specific prediction is \(\hat{\mathbf{Y}_i} = \mathbf{X}_i \hat{\beta} + \mathbf{Z}_i \hat{b}_i\)
\end{itemize}

According to \citep{henderson1975best}, estimating equations known as the mixed model equations:

\[
\left[
\begin{array}
{c}
\hat{\beta} \\
\hat{\mathbf{b}}
\end{array}
\right]
=
\left[
\begin{array}
{cc}
\mathbf{X'\Sigma^{-1}X} & \mathbf{X'\Sigma^{-1}Z} \\
\mathbf{Z'\Sigma^{-1}X} & \mathbf{Z'\Sigma^{-1}Z +B^{-1}}
\end{array}
\right]
\left[
\begin{array}
{cc}
\mathbf{X'\Sigma^{-1}Y} \\
\mathbf{Z'\Sigma^{-1}Y}
\end{array}
\right]
\]

where

\[
\begin{aligned}
\mathbf{Y}
&=
\left[
\begin{array}
{c}
\mathbf{y}_1 \\
. \\
\mathbf{y}_N
\end{array}
\right] ;
\mathbf{X}
=
\left[
\begin{array}
{c}
\mathbf{X}_1 \\
. \\
\mathbf{X}_N
\end{array}
\right];
\mathbf{b} = 
\left[
\begin{array}
{c}
\mathbf{b}_1 \\
. \\
\mathbf{b}_N
\end{array}
\right] ;
\epsilon = 
\left[
\begin{array}
{c}
\epsilon_1 \\
. \\
\epsilon_N
\end{array}
\right]
\\
cov(\epsilon) &= \mathbf{\Sigma},
\mathbf{Z} = 
\left[
\begin{array}
{cccc}
\mathbf{Z}_1 & 0 &  ... & 0 \\
0 & \mathbf{Z}_2 & ... & 0 \\
. & . & . & . \\
0 & 0 & ... & \mathbf{Z}_n
\end{array}
\right],
\mathbf{B} =
\left[
\begin{array}
{cccc}
\mathbf{D} & 0 & ... & 0 \\
0 & \mathbf{D} & ... & 0 \\
. & . & . & . \\
0 & 0 & ... & \mathbf{D}
\end{array}
\right]
\end{aligned}
\]

The model has the form

\[
\begin{aligned}
\mathbf{Y} &= \mathbf{X \beta + Z b + \epsilon} \\
\mathbf{Y} &\sim N(\mathbf{X \beta, ZBZ' + \Sigma})
\end{aligned}
\]

If \(\mathbf{V = ZBZ' + \Sigma}\), then the solutions to the estimating equations can be

\[
\begin{aligned}
\hat{\beta} &= \mathbf{(X'V^{-1}X)^{-1}X'V^{-1}Y} \\
\hat{\mathbf{b}} &= \mathbf{BZ'V^{-1}(Y-X\hat{\beta}})
\end{aligned}
\]

The estimate \(\hat{\beta}\) is a generalized least squares estimate.

The predictor, \(\hat{\mathbf{b}}\) is the best linear unbiased predictor (BLUP), for \(\mathbf{b}\)

\[
\begin{aligned}
E(\hat{\beta}) &= \beta \\
var(\hat{\beta}) &= (\mathbf{X'V^{-1}X})^{-1} \\
E(\hat{\mathbf{b}}) &= 0
\end{aligned}
\]

\[
var(\mathbf{\hat{b}-b}) = \mathbf{B-BZ'V^{-1}ZB + BZ'V^{-1}X(X'V^{-1}X)^{-1}X'V^{-1}B}
\]

The variance here is the variance of the prediction error (mean squared prediction error, MSPE), which is more meaningful than \(var(\hat{\mathbf{b}})\), since MSPE accounts for both variance and bias in the prediction.

To derive the mixed model equations, consider

\[
\mathbf{\epsilon = Y - X\beta - Zb}
\]

Let \(T = \sum_{i=1}^N n_i\) be the total number of observations (i.e., the length of \(\mathbf{Y},\epsilon\)) and \(Nq\) the length of \(\mathbf{b}\). The joint distribution of \(\mathbf{b, \epsilon}\) is

\[
f(\mathbf{b,\epsilon})= \frac{1}{(2\pi)^{(T+ Nq)/2}}
\left|
\begin{array}
{cc}
\mathbf{B} & 0 \\
0 & \mathbf{\Sigma}
\end{array}
\right| ^{-1/2}
\exp
\left(
-\frac{1}{2}
\left[
\begin{array}
{c}
\mathbf{b} \\
\mathbf{Y - X \beta - Zb}
\end{array}
\right]'
\left[
\begin{array}
{cc}
\mathbf{B} & 0 \\
0 & \mathbf{\Sigma}
\end{array}
\right]^{-1}
\left[
\begin{array}
{c}
\mathbf{b} \\
\mathbf{Y - X \beta - Zb}
\end{array}
\right]
\right)
\]

Maximization of \(f(\mathbf{b},\epsilon)\) with respect to \(\mathbf{b}\) and \(\beta\) requires minimization of

\[
\begin{aligned}
Q &= 
\left[
\begin{array}
{c}
\mathbf{b} \\
\mathbf{Y - X \beta - Zb}
\end{array}
\right]'
\left[
\begin{array}
{cc}
\mathbf{B} & 0 \\
0 & \mathbf{\Sigma}
\end{array}
\right]^{-1}
\left[
\begin{array}
{c}
\mathbf{b} \\
\mathbf{Y - X \beta - Zb}
\end{array}
\right] \\
&= \mathbf{b'B^{-1}b+(Y-X \beta-Zb)'\Sigma^{-1}(Y-X \beta-Zb)}
\end{aligned}
\]

Setting the derivatives of Q with respect to \(\mathbf{b}\) and \(\mathbf{\beta}\) to zero leads to the system of equations:

\[
\begin{aligned}
\mathbf{X'\Sigma^{-1}X\beta + X'\Sigma^{-1}Zb} &= \mathbf{X'\Sigma^{-1}Y}\\
\mathbf{(Z'\Sigma^{-1}Z + B^{-1})b + Z'\Sigma^{-1}X\beta} &= \mathbf{Z'\Sigma^{-1}Y}
\end{aligned}
\]

Rearranging

\[
\left[
\begin{array}
{cc}
\mathbf{X'\Sigma^{-1}X} & \mathbf{X'\Sigma^{-1}Z} \\
\mathbf{Z'\Sigma^{-1}X} & \mathbf{Z'\Sigma^{-1}Z + B^{-1}}
\end{array}
\right]
\left[
\begin{array}
{c}
\beta \\
\mathbf{b}
\end{array}
\right]
= 
\left[
\begin{array}
{c}
\mathbf{X'\Sigma^{-1}Y} \\
\mathbf{Z'\Sigma^{-1}Y}
\end{array}
\right]
\]

Thus, the solution to the mixed model equations give:

\[
\left[
\begin{array}
{c}
\hat{\beta} \\
\hat{\mathbf{b}}
\end{array}
\right]
= 
\left[
\begin{array}
{cc}
\mathbf{X'\Sigma^{-1}X} & \mathbf{X'\Sigma^{-1}Z} \\
\mathbf{Z'\Sigma^{-1}X} & \mathbf{Z'\Sigma^{-1}Z + B^{-1}}
\end{array}
\right] ^{-1}
\left[
\begin{array}
{c}
\mathbf{X'\Sigma^{-1}Y} \\
\mathbf{Z'\Sigma^{-1}Y}
\end{array}
\right]
\]

Equivalently,

Bayes' theorem

\[
f(\mathbf{b}| \mathbf{Y}) = \frac{f(\mathbf{Y}|\mathbf{b})f(\mathbf{b})}{\int f(\mathbf{Y}|\mathbf{b})f(\mathbf{b}) d\mathbf{b}}
\]

where

\begin{itemize}
\tightlist
\item
  \(f(\mathbf{Y}|\mathbf{b})\) is the ``likelihood''
\item
  \(f(\mathbf{b})\) is the prior
\item
  the denominator is the ``normalizing constant''
\item
  \(f(\mathbf{b}|\mathbf{Y})\) is the posterior distribution
\end{itemize}

In this case

\[
\begin{aligned}
\mathbf{Y} | \mathbf{b} &\sim N(\mathbf{X\beta+Zb,\Sigma}) \\
\mathbf{b} &\sim N(\mathbf{0,B})
\end{aligned}
\]

The posterior distribution has the form

\[
\mathbf{b}|\mathbf{Y} \sim N(\mathbf{BZ'V^{-1}(Y-X\beta),(Z'\Sigma^{-1}Z + B^{-1})^{-1}})
\]

Hence, the best predictor (based on squared error loss)

\[
E(\mathbf{b}|\mathbf{Y}) = \mathbf{BZ'V^{-1}(Y-X\beta)}
\]

\hypertarget{estimating-mathbfv}{%
\subsection{\texorpdfstring{Estimating \(\mathbf{V}\)}{Estimating \textbackslash mathbf\{V\}}}\label{estimating-mathbfv}}

If we have \(\tilde{\mathbf{V}}\) (estimate of \(\mathbf{V}\)), then we can estimate:

\[
\begin{aligned}
\hat{\beta} &= \mathbf{(X'\tilde{V}^{-1}X)^{-1}X'\tilde{V}^{-1}Y} \\
\hat{\mathbf{b}} &= \mathbf{BZ'\tilde{V}^{-1}(Y-X\hat{\beta})}
\end{aligned}
\]

where \({\mathbf{b}}\) is \textbf{EBLUP} (estimated BLUP) or \textbf{empirical Bayes estimate}

Note:

\begin{itemize}
\tightlist
\item
  \(\hat{var}(\hat{\beta})\) is a consistent estimator of \(var(\hat{\beta})\) if \(\tilde{\mathbf{V}}\) is a consistent estimator of \(\mathbf{V}\)
\item
  However, \(\hat{var}(\hat{\beta})\) is biased since the variability arises from estimating \(\mathbf{V}\) is not accounted for in the estimate.
\item
  Hence, \(\hat{var}(\hat{\beta})\) underestimates the true variability
\end{itemize}

Ways to estimate \(\mathbf{V}\)

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{maximum-likelihood-estimation-mle}{Maximum Likelihood Estimation (MLE)}
\item
  \protect\hyperlink{restricted-maximum-likelihood-reml}{Restricted Maximum Likelihood (REML)}
\item
  \protect\hyperlink{estimated-generalized-least-squares}{Estimated Generalized Least Squares}
\item
  \protect\hyperlink{bayesian-hierarchical-models-bhm}{Bayesian Hierarchical Models (BHM)}
\end{itemize}

\hypertarget{maximum-likelihood-estimation-mle}{%
\subsubsection{Maximum Likelihood Estimation (MLE)}\label{maximum-likelihood-estimation-mle}}

Grouping unknown parameters in \(\Sigma\) and \(B\) under a parameter vector \(\theta\). Under MLE, \(\hat{\theta}\) and \(\hat{\beta}\) maximize the likelihood \(\mathbf{y} \sim N(\mathbf{X\beta, V(\theta))}\). Synonymously, \(-2\log L(\mathbf{y;\theta,\beta})\):

\[
-2l(\mathbf{\beta,\theta,y}) = \log |\mathbf{V(\theta)}| + \mathbf{(y-X\beta)'V(\theta)^{-1}(y-X\beta)} + N \log(2\pi)
\]

\begin{itemize}
\tightlist
\item
  Step 1: Replace \(\beta\) with its maximum likelihood (where \(\theta\) is known \(\hat{\beta}= (\mathbf{X'V(\theta)^{-1}X)^{-1}X'V(\theta)^{-1}y}\)
\item
  Step 2: Minimize the above equation with respect to \(\theta\) to get the estimator \(\hat{\theta}_{MLE}\)
\item
  Step 3: Substitute \(\hat{\theta}_{MLE}\) back to get \(\hat{\beta}_{MLE} = (\mathbf{X'V(\theta_{MLE})^{-1}X)^{-1}X'V(\theta_{MLE})^{-1}y}\)
\item
  Step 4: Get \(\hat{\mathbf{b}}_{MLE} = \mathbf{B(\hat{\theta}_{MLE})Z'V(\hat{\theta}_{MLE})^{-1}(y-X\hat{\beta}_{MLE})}\)
\end{itemize}

Note:

\begin{itemize}
\tightlist
\item
  \(\hat{\theta}\) are typically negatively biased due to unaccounted fixed effects being estimated, which we could try to account for.
\end{itemize}

\hypertarget{restricted-maximum-likelihood-reml}{%
\subsubsection{Restricted Maximum Likelihood (REML)}\label{restricted-maximum-likelihood-reml}}

REML accounts for the number of estimated mean parameters by adjusting the objective function. Specifically, the likelihood of linear combination of the elements of \(\mathbf{y}\) is accounted for.

We have \(\mathbf{K'y}\), where \(\mathbf{K}\) is any \(N \times (N - p)\) full-rank contrast matrix, which has columns orthogonal to the \(\mathbf{X}\) matrix (that is \(\mathbf{K'X} = 0\)). Then,

\[
\mathbf{K'y} \sim N(0,\mathbf{K'V(\theta)K})
\]

where \(\beta\) is no longer in the distribution

We can proceed to maximize this likelihood for the contrasts to get \(\hat{\theta}_{REML}\), which does not depend on the choice of \(\mathbf{K}\). And \(\hat{\beta}\) are based on \(\hat{\theta}\)

Comparison REML and MLE

\begin{itemize}
\item
  Both methods are based upon the likelihood principle, and have desired properties for the estimates:

  \begin{itemize}
  \item
    consistency
  \item
    asymptotic normality
  \item
    efficiency
  \end{itemize}
\item
  ML estimation provides estimates for fixed effects, while REML can't
\item
  In balanced models, REML is identical to ANOVA
\item
  REML accounts for df for the fixed effects int eh model, which is important when \(\mathbf{X}\) is large relative to the sample size
\item
  Changing \(\mathbf{\beta}\) has no effect on the REML estimates of \(\theta\)
\item
  REML is less sensitive to outliers than MLE
\item
  MLE is better than REML regarding model comparisons (e.g., AIC or BIC)
\end{itemize}

\hypertarget{estimated-generalized-least-squares}{%
\subsubsection{Estimated Generalized Least Squares}\label{estimated-generalized-least-squares}}

MLE and REML rely upon the Gaussian assumption. To overcome this issue, EGLS uses the first and second moments.

\[
\mathbf{Y}_i = \mathbf{X}_i \beta + \mathbf{Z}_i \mathbf{b}_i + \epsilon_i
\]

where

\begin{itemize}
\tightlist
\item
  \(\epsilon_i \sim (\mathbf{0,\Sigma_i})\)
\item
  \(\mathbf{b}_i \sim (\mathbf{0,D})\)
\item
  \(cov(\epsilon_i, \mathbf{b}_i) = 0\)
\end{itemize}

Then the EGLS estimator is

\[
\begin{aligned}
\hat{\beta}_{GLS} &= \{\sum_{i=1}^n \mathbf{X'_iV_i(\theta)^{-1}X_i}  \}^{-1} \sum_{i=1}^n \mathbf{X'_iV_i(\theta)^{-1}Y_i} \\
&=\{\mathbf{X'V(\theta)^{-1}X} \}^{-1} \mathbf{X'V(\theta)^{-1}Y}
\end{aligned}
\]

depends on the first two moments

\begin{itemize}
\tightlist
\item
  \(E(\mathbf{Y}_i) = \mathbf{X}_i \beta\)
\item
  \(var(\mathbf{Y}_i)= \mathbf{V}_i\)
\end{itemize}

EGLS use \(\hat{\mathbf{V}}\) for \(\mathbf{V(\theta)}\)

\[
\hat{\beta}_{EGLS} = \{ \mathbf{X'\hat{V}^{-1}X} \}^{-1} \mathbf{X'\hat{V}^{-1}Y}
\]

Hence, the fixed effects estimators for the MLE, REML, and EGLS are of the same form, except for the estimate of \(\mathbf{V}\)

In case of non-iterative approach, EGLS can be appealing when \(\mathbf{V}\) can be estimated without much computational burden.

\hypertarget{bayesian-hierarchical-models-bhm}{%
\subsubsection{Bayesian Hierarchical Models (BHM)}\label{bayesian-hierarchical-models-bhm}}

Joint distribution cane be decomposed hierarchically in terms of the product of conditional distributions and a marginal distribution

\[
f(A,B,C) = f(A|B,C) f(B|C)f(C)
\]

Applying to estimate \(\mathbf{V}\)

\[
\begin{aligned}
f(\mathbf{Y, \beta, b, \theta}) &= f(\mathbf{Y|\beta,b, \theta})f(\mathbf{b|\theta,\beta})f(\mathbf{\beta|\theta})f(\mathbf{\theta}) & \text{based on probability decomposition} \\
&= f(\mathbf{Y|\beta,b, \theta})f(\mathbf{b|\theta})f(\mathbf{\beta})f(\mathbf{\theta}) & \text{based on simplifying modeling assumptions}
\end{aligned}
\]

elaborate on the second equality, if we assume conditional independence (e.g., given \(\theta\), no additional info about \(\mathbf{b}\) is given by knowing \(\beta\)), then we can simply from the first equality

Using Bayes' rule

\[
f(\mathbf{\beta, b, \theta|Y}) \propto f(\mathbf{Y|\beta,b, \theta})f(\mathbf{b|\theta})f(\mathbf{\beta})f(\mathbf{\theta})
\]

where

\[
\begin{aligned}
\mathbf{Y| \beta, b, \theta} &\sim \mathbf{N(X\beta+ Zb, \Sigma(\theta))} \\
\mathbf{b | \theta} &\sim \mathbf{N(0, B(\theta))}
\end{aligned}
\]

and we also have to have prior distributions for \(f(\beta), f(\theta)\)

With normalizing constant, we can obtain the posterior distribution. Typically, we can't get analytical solution right away. Hence, we can use Markov Chain Monte Carlo (MCMC) to obtain samples from the posterior distribution.

Bayesian Methods:

\begin{itemize}
\tightlist
\item
  account for the uncertainty in parameters estimates and accommodate the propagation of that uncertainty through the model
\item
  can adjust prior information (i.e., priori) in parameters
\item
  Can extend beyond Gaussian distributions
\item
  but hard to implement algorithms and might have problem converging
\end{itemize}

\hypertarget{inference-3}{%
\section{Inference}\label{inference-3}}

\hypertarget{parameters-beta}{%
\subsection{\texorpdfstring{Parameters \(\beta\)}{Parameters \textbackslash beta}}\label{parameters-beta}}

\hypertarget{wald-test-GLMM}{%
\subsubsection{Wald test}\label{wald-test-GLMM}}

We have

\[
\begin{aligned}
\mathbf{\hat{\beta}(\theta)} &= \mathbf{\{X'V^{-1}(\theta) X\}^{-1}X'V^{-1}(\theta) Y} \\
var(\hat{\beta}(\theta)) &= \mathbf{\{X'V^{-1}(\theta) X\}^{-1}}
\end{aligned}
\]

We can use \(\hat{\theta}\) in place of \(\theta\) to approximate Wald test

\[
H_0: \mathbf{A \beta =d} 
\]

With

\[
W = \mathbf{(A\hat{\beta} - d)'[A(X'\hat{V}^{-1}X)^{-1}A']^{-1}(A\hat{\beta} - d)}
\]

where \(W \sim \chi^2_{rank(A)}\) under \(H_0\) is true. However, it does not take into account variability from using \(\hat{\theta}\) in place of \(\theta\), hence the standard errors are underestimated

\hypertarget{f-test-1}{%
\subsubsection{F-test}\label{f-test-1}}

Alternatively, we can use the modified F-test, suppose we have \(var(\mathbf{Y}) = \sigma^2 \mathbf{V}(\theta)\), then

\[
F^* = \frac{\mathbf{(A\hat{\beta} - d)'[A(X'\hat{V}^{-1}X)^{-1}A']^{-1}(A\hat{\beta} - d)}}{\hat{\sigma}^2 \text{rank}(A)}
\]

where \(F^* \sim f_{rank(A), den(df)}\) under the null hypothesis. And den(df) needs to be approximated from the data by either:

\begin{itemize}
\tightlist
\item
  Satterthwaite method
\item
  Kenward-Roger approximation
\end{itemize}

Under balanced cases, the Wald and F tests are similar. But for small sample sizes, they can differ in p-values. And both can be reduced to t-test for a single \(\beta\)

\hypertarget{likelihood-ratio-test}{%
\subsubsection{Likelihood Ratio Test}\label{likelihood-ratio-test}}

\[
H_0: \beta \in \Theta_{\beta,0}
\]

where \(\Theta_{\beta, 0}\) is a subspace of the parameter space, \(\Theta_{\beta}\) of the fixed effects \(\beta\) . Then

\[
-2\log \lambda_N = -2\log\{\frac{\hat{L}_{ML,0}}{\hat{L}_{ML}}\}
\]

where

\begin{itemize}
\tightlist
\item
  \(\hat{L}_{ML,0}\) , \(\hat{L}_{ML}\) are the maximized likelihood obtained from maximizing over \(\Theta_{\beta,0}\) and \(\Theta_{\beta}\)
\item
  \(-2 \log \lambda_N \dot{\sim} \chi^2_{df}\) where df is the difference in the dimension (i.e., number of parameters) of \(\Theta_{\beta,0}\) and \(\Theta_{\beta}\)
\end{itemize}

This method is not applicable for REML. But REML can still be used to test for covariance parameters between nested models.

\hypertarget{variance-components}{%
\subsection{Variance Components}\label{variance-components}}

\begin{itemize}
\item
  For ML and REML estimator, \(\hat{\theta} \sim N(\theta, I(\theta))\) for large samples
\item
  Wald test in variance components is analogous to the fixed effects case (see \ref{wald-test-GLMM} )

  \begin{itemize}
  \item
    However, the normal approximation depends largely on the true value of \(\theta\). It will fail if the true value of \(\theta\) is close to the boundary of the parameter space \(\Theta_{\theta}\) (i.e., \(\sigma^2 \approx 0\))
  \item
    Typically works better for covariance parameter, than variance parameters.
  \end{itemize}
\item
  The likelihood ratio tests can also be used with ML or REML estimates. However, the same problem of parameters
\end{itemize}

\hypertarget{information-criteria}{%
\section{Information Criteria}\label{information-criteria}}

\begin{itemize}
\tightlist
\item
  account for the likelihood and the number of parameters to assess model comparison.
\end{itemize}

\hypertarget{akaikes-information-criteria-aic}{%
\subsection{Akaike's Information Criteria (AIC)}\label{akaikes-information-criteria-aic}}

Derived as an estimator of the expected Kullback discrepancy between the true model and a fitted candidate model

\[
AIC = -2l(\hat{\theta}, \hat{\beta}) + 2q
\]

where

\begin{itemize}
\tightlist
\item
  \(l(\hat{\theta}, \hat{\beta})\) is the log-likelihood
\item
  q = the effective number of parameters; total of fixed and those associated with random effects (variance/covariance; those not estimated to be on a boundary constraint)
\end{itemize}

Note:

\begin{itemize}
\tightlist
\item
  In comparing models that differ in their random effects, this method is not advised to due the inability to get the correct number of effective parameters).
\item
  We prefer smaller AIC values.
\item
  If your program uses \(l-q\) then we prefer larger AIC values (but rarely).
\item
  can be used for mixed model section, (e.g., selection of the covariance structure), but the sample size must be very large to have adequate comparison based on the criterion
\item
  Can have a large negative bias (e.g., when sample size is small but the number of parameters is large) due to the penalty term can't approximate the bias adjustment adequately
\end{itemize}

\hypertarget{corrected-aic-aicc}{%
\subsection{Corrected AIC (AICC)}\label{corrected-aic-aicc}}

\begin{itemize}
\tightlist
\item
  developed by \citep{hurvich1989regression}
\item
  correct small-sample adjustment
\item
  depends on the candidate model class
\item
  Only if you have fixed covariance structure, then AICC is justified, but not general covariance structure
\end{itemize}

\hypertarget{bayesian-information-criteria-bic}{%
\subsection{Bayesian Information Criteria (BIC)}\label{bayesian-information-criteria-bic}}

\[
BIC = -2l(\hat{\theta}, \hat{\beta}) + q \log n
\]

where n = number of observations.

\begin{itemize}
\tightlist
\item
  we prefer smaller BIC value
\item
  BIC and AIC are used for both REML and MLE if we have the same mean structure. Otherwise, in general, we should prefer MLE
\end{itemize}

With our example presented at the beginning of \protect\hyperlink{linear-mixed-models}{Linear Mixed Models},

\[
Y_{ik}= 
\begin{cases}
\beta_0 + b_{1i} + (\beta_1 + \ b_{2i})t_{ij} + \epsilon_{ij} & L \\
\beta_0 + b_{1i} + (\beta_2 + \ b_{2i})t_{ij} + \epsilon_{ij} & H\\
\beta_0 + b_{1i} + (\beta_3 + \ b_{2i})t_{ij} + \epsilon_{ij} & C
\end{cases}
\]

where

\begin{itemize}
\tightlist
\item
  \(i = 1,..,N\)
\item
  \(j = 1,..,n_i\) (measures at time \(t_{ij}\))
\end{itemize}

Note:

\begin{itemize}
\tightlist
\item
  we have subject-specific intercepts,
\end{itemize}

\[
\begin{aligned}
\mathbf{Y}_i |b_i &\sim N(\mathbf{X}_i \beta + 1 b_i, \sigma^2 \mathbf{I}) \\
b_i &\sim N(0,d_{11})
\end{aligned}
\]

here, we want to estimate \(\beta, \sigma^2, d_{11}\) and predict \(b_i\)

\hypertarget{split-plot-designs}{%
\section{Split-Plot Designs}\label{split-plot-designs}}

\begin{itemize}
\tightlist
\item
  Typically used in the case that you have two factors where one needs much larger units than the other.
\end{itemize}

Example:

A: 3 levels (large units)

B: 2 levels (small units)

\begin{itemize}
\tightlist
\item
  A and B levels are randomized into 4 blocks.
\item
  But it differs from \protect\hyperlink{randomized-block-designs}{Randomized Block Designs}. In each block, both have one of the 6 (3x2) treatment combinations. But \protect\hyperlink{randomized-block-designs}{Randomized Block Designs} assign in each block randomly, while split-plot does not randomize this step.
\item
  Moreover, because A needs to be applied in large units, factor A is applied only once in each block while B can be applied multiple times.
\end{itemize}

Hence, we have our model

If A is our factor of interest

\[
Y_{ij} = \mu + \rho_i + \alpha_j + e_{ij}
\]

where

\begin{itemize}
\tightlist
\item
  \(i\) = replication (block or subject)
\item
  \(j\) = level of Factor A
\item
  \(\mu\) = overall mean
\item
  \(\rho_i\) = variation due to the \(i\)-th block
\item
  \(e_{ij} \sim N(0, \sigma^2_e)\) = whole plot error
\end{itemize}

If B is our factor of interest

\[
Y_{ijk} = \mu + \phi_{ij} + \beta_k + \epsilon_{ijk}
\]

where

\begin{itemize}
\tightlist
\item
  \(\phi_{ij}\) = variation due to the \(ij\)-th main plot
\item
  \(\beta_k\) = Factor B effect
\item
  \(\epsilon_{ijk} \sim N(0, \sigma^2_\epsilon)\) = subplot error
\item
  \(\phi_{ij} = \rho_i + \alpha_j + e_{ij}\)
\end{itemize}

Together, the split-plot model

\[
Y_{ijk} = \mu + \rho_i + \alpha_j + e_{ij} + \beta_k + (\alpha \beta)_{jk} + \epsilon_{ijk}
\]

where

\begin{itemize}
\tightlist
\item
  \(i\) = replicate (blocks or subjects)
\item
  \(j\) = level of factor A
\item
  \(k\) = level of factor B
\item
  \(\mu\) = overall mean
\item
  \(\rho_i\) = effect of the block
\item
  \(\alpha_j\) = main effect of factor A (fixed)
\item
  \(e_{ij} = (\rho \alpha)_{ij}\) = block by factor A interaction (the whole plot error, random)
\item
  \(\beta_k\) = main effect of factor B (fixed)
\item
  \((\alpha \beta)_{jk}\) = interaction between factors A and B (fixed)
\item
  \(\epsilon_{ijk}\) = subplot error (random)
\end{itemize}

We can approach sub-plot analysis based on

\begin{itemize}
\item
  the ANOVA perspective

  \begin{itemize}
  \item
    Whole plot comparisons

    \begin{itemize}
    \item
      Compare factor A to the whole plot error (i.e., \(\alpha_j\) to \(e_{ij}\))
    \item
      Compare the block to the whole plot error (i.e., \(\rho_i\) to \(e_{ij}\))
    \end{itemize}
  \item
    Sub-plot comparisons:

    \begin{itemize}
    \item
      Compare factor B to the subplot error (\(\beta\) to \(\epsilon_{ijk}\))
    \item
      Compare the AB interaction to the subplot error (\((\alpha \beta)_{jk}\) to \(\epsilon_{ijk}\))
    \end{itemize}
  \end{itemize}
\item
  the mixed model perspective
\end{itemize}

\[
\mathbf{Y = X \beta + Zb + \epsilon}
\]

\hypertarget{application-4}{%
\subsection{Application}\label{application-4}}

\hypertarget{example-1}{%
\subsubsection{Example 1}\label{example-1}}

\[
y_{ijk} = \mu + i_i + v_j + (iv)_{ij} + f_k + \epsilon_{ijk}
\]

where

\begin{itemize}
\tightlist
\item
  \(y_{ijk}\) = observed yield
\item
  \(\mu\) = overall average yield
\item
  \(i_i\) = irrigation effect
\item
  \(v_j\) = variety effect
\item
  \((iv)_{ij}\) = irrigation by variety interaction
\item
  \(f_k\) = random field (block) effect
\item
  \(\epsilon_{ijk}\) = residual
\item
  because variety-field combination is only observed once, we can't have the random interaction effects between variety and field
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{data}\NormalTok{(irrigation, }\AttributeTok{package =} \StringTok{"faraway"}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(irrigation)}
\CommentTok{\#\textgreater{}      field   irrigation variety     yield      }
\CommentTok{\#\textgreater{}  f1     :2   i1:4       v1:8    Min.   :34.80  }
\CommentTok{\#\textgreater{}  f2     :2   i2:4       v2:8    1st Qu.:37.60  }
\CommentTok{\#\textgreater{}  f3     :2   i3:4               Median :40.15  }
\CommentTok{\#\textgreater{}  f4     :2   i4:4               Mean   :40.23  }
\CommentTok{\#\textgreater{}  f5     :2                      3rd Qu.:42.73  }
\CommentTok{\#\textgreater{}  f6     :2                      Max.   :47.60  }
\CommentTok{\#\textgreater{}  (Other):4}
\FunctionTok{head}\NormalTok{(irrigation, }\DecValTok{4}\NormalTok{)}
\CommentTok{\#\textgreater{}   field irrigation variety yield}
\CommentTok{\#\textgreater{} 1    f1         i1      v1  35.4}
\CommentTok{\#\textgreater{} 2    f1         i1      v2  37.9}
\CommentTok{\#\textgreater{} 3    f2         i2      v1  36.7}
\CommentTok{\#\textgreater{} 4    f2         i2      v2  38.2}
\FunctionTok{ggplot}\NormalTok{(irrigation,}
       \FunctionTok{aes}\NormalTok{(}
         \AttributeTok{x     =}\NormalTok{ field,}
         \AttributeTok{y     =}\NormalTok{ yield,}
         \AttributeTok{shape =}\NormalTok{ irrigation,}
         \AttributeTok{color =}\NormalTok{ variety}
\NormalTok{       )) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{size =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{08-linear-mixed-models_files/figure-latex/unnamed-chunk-1-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sp\_model }\OtherTok{\textless{}{-}}
\NormalTok{    lmerTest}\SpecialCharTok{::}\FunctionTok{lmer}\NormalTok{(yield }\SpecialCharTok{\textasciitilde{}}\NormalTok{ irrigation }\SpecialCharTok{*}\NormalTok{ variety }
                   \SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{field), irrigation)}
\FunctionTok{summary}\NormalTok{(sp\_model)}
\CommentTok{\#\textgreater{} Linear mixed model fit by REML. t{-}tests use Satterthwaite\textquotesingle{}s method [}
\CommentTok{\#\textgreater{} lmerModLmerTest]}
\CommentTok{\#\textgreater{} Formula: yield \textasciitilde{} irrigation * variety + (1 | field)}
\CommentTok{\#\textgreater{}    Data: irrigation}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} REML criterion at convergence: 45.4}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Scaled residuals: }
\CommentTok{\#\textgreater{}     Min      1Q  Median      3Q     Max }
\CommentTok{\#\textgreater{} {-}0.7448 {-}0.5509  0.0000  0.5509  0.7448 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Random effects:}
\CommentTok{\#\textgreater{}  Groups   Name        Variance Std.Dev.}
\CommentTok{\#\textgreater{}  field    (Intercept) 16.200   4.025   }
\CommentTok{\#\textgreater{}  Residual              2.107   1.452   }
\CommentTok{\#\textgreater{} Number of obs: 16, groups:  field, 8}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Fixed effects:}
\CommentTok{\#\textgreater{}                        Estimate Std. Error     df t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)              38.500      3.026  4.487  12.725 0.000109 ***}
\CommentTok{\#\textgreater{} irrigationi2              1.200      4.279  4.487   0.280 0.791591    }
\CommentTok{\#\textgreater{} irrigationi3              0.700      4.279  4.487   0.164 0.877156    }
\CommentTok{\#\textgreater{} irrigationi4              3.500      4.279  4.487   0.818 0.454584    }
\CommentTok{\#\textgreater{} varietyv2                 0.600      1.452  4.000   0.413 0.700582    }
\CommentTok{\#\textgreater{} irrigationi2:varietyv2   {-}0.400      2.053  4.000  {-}0.195 0.855020    }
\CommentTok{\#\textgreater{} irrigationi3:varietyv2   {-}0.200      2.053  4.000  {-}0.097 0.927082    }
\CommentTok{\#\textgreater{} irrigationi4:varietyv2    1.200      2.053  4.000   0.584 0.590265    }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Correlation of Fixed Effects:}
\CommentTok{\#\textgreater{}             (Intr) irrgt2 irrgt3 irrgt4 vrtyv2 irr2:2 irr3:2}
\CommentTok{\#\textgreater{} irrigation2 {-}0.707                                          }
\CommentTok{\#\textgreater{} irrigation3 {-}0.707  0.500                                   }
\CommentTok{\#\textgreater{} irrigation4 {-}0.707  0.500  0.500                            }
\CommentTok{\#\textgreater{} varietyv2   {-}0.240  0.170  0.170  0.170                     }
\CommentTok{\#\textgreater{} irrgtn2:vr2  0.170 {-}0.240 {-}0.120 {-}0.120 {-}0.707              }
\CommentTok{\#\textgreater{} irrgtn3:vr2  0.170 {-}0.120 {-}0.240 {-}0.120 {-}0.707  0.500       }
\CommentTok{\#\textgreater{} irrgtn4:vr2  0.170 {-}0.120 {-}0.120 {-}0.240 {-}0.707  0.500  0.500}

\FunctionTok{anova}\NormalTok{(sp\_model, }\AttributeTok{ddf =} \FunctionTok{c}\NormalTok{(}\StringTok{"Kenward{-}Roger"}\NormalTok{))}
\CommentTok{\#\textgreater{} Type III Analysis of Variance Table with Kenward{-}Roger\textquotesingle{}s method}
\CommentTok{\#\textgreater{}                    Sum Sq Mean Sq NumDF DenDF F value Pr(\textgreater{}F)}
\CommentTok{\#\textgreater{} irrigation         2.4545 0.81818     3     4  0.3882 0.7685}
\CommentTok{\#\textgreater{} variety            2.2500 2.25000     1     4  1.0676 0.3599}
\CommentTok{\#\textgreater{} irrigation:variety 1.5500 0.51667     3     4  0.2452 0.8612}
\end{Highlighting}
\end{Shaded}

Since p-value of the interaction term is insignificant, we consider fitting without it.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(lme4)}
\NormalTok{sp\_model\_additive }\OtherTok{\textless{}{-}} \FunctionTok{lmer}\NormalTok{(yield }\SpecialCharTok{\textasciitilde{}}\NormalTok{ irrigation }\SpecialCharTok{+}\NormalTok{ variety }
                          \SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ field), irrigation)}
\FunctionTok{anova}\NormalTok{(sp\_model\_additive,sp\_model,}\AttributeTok{ddf =} \StringTok{"Kenward{-}Roger"}\NormalTok{)}
\CommentTok{\#\textgreater{} Data: irrigation}
\CommentTok{\#\textgreater{} Models:}
\CommentTok{\#\textgreater{} sp\_model\_additive: yield \textasciitilde{} irrigation + variety + (1 | field)}
\CommentTok{\#\textgreater{} sp\_model: yield \textasciitilde{} irrigation * variety + (1 | field)}
\CommentTok{\#\textgreater{}                   npar    AIC    BIC  logLik deviance  Chisq Df Pr(\textgreater{}Chisq)}
\CommentTok{\#\textgreater{} sp\_model\_additive    7 83.959 89.368 {-}34.980   69.959                     }
\CommentTok{\#\textgreater{} sp\_model            10 88.609 96.335 {-}34.305   68.609 1.3503  3     0.7172}
\end{Highlighting}
\end{Shaded}

Since \(p\)-value of \(\chi^2\) test is insignificant, we can't reject the additive model is already sufficient. Looking at AIC and BIC, we can also see that we would prefer the additive model

\textbf{Random Effect Examination}

\texttt{exactRLRT} test

\begin{itemize}
\tightlist
\item
  \(H_0\): Var(random effect) (i.e., \(\sigma^2\))= 0
\item
  \(H_a\): Var(random effect) (i.e., \(\sigma^2\)) \textgreater{} 0
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sp\_model }\OtherTok{\textless{}{-}}\NormalTok{ lme4}\SpecialCharTok{::}\FunctionTok{lmer}\NormalTok{(yield }\SpecialCharTok{\textasciitilde{}}\NormalTok{ irrigation }\SpecialCharTok{*}\NormalTok{ variety }
                       \SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ field), irrigation)}
\FunctionTok{library}\NormalTok{(RLRsim)}
\FunctionTok{exactRLRT}\NormalTok{(sp\_model)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  simulated finite sample distribution of RLRT.}
\CommentTok{\#\textgreater{}  }
\CommentTok{\#\textgreater{}  (p{-}value based on 10000 simulated values)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  }
\CommentTok{\#\textgreater{} RLRT = 6.1118, p{-}value = 0.0075}
\end{Highlighting}
\end{Shaded}

Since the p-value is significant, we reject \(H_0\)

\hypertarget{repeated-measures-in-mixed-models}{%
\section{Repeated Measures in Mixed Models}\label{repeated-measures-in-mixed-models}}

\[
Y_{ijk} = \mu + \alpha_i + \beta_j + (\alpha \beta)_{ij} + \delta_{i(k)}+ \epsilon_{ijk}
\]

where

\begin{itemize}
\tightlist
\item
  \(i\)-th group (fixed)
\item
  \(j\)-th (repeated measure) time effect (fixed)
\item
  \(k\)-th subject
\item
  \(\delta_{i(k)} \sim N(0,\sigma^2_\delta)\) (k-th subject in the \(i\)-th group) and \(\epsilon_{ijk} \sim N(0,\sigma^2)\) (independent error) are random effects (\(i = 1,..,n_A, j = 1,..,n_B, k = 1,...,n_i\))
\end{itemize}

hence, the variance-covariance matrix of the repeated observations on the k-th subject of the i-th group, \(\mathbf{Y}_{ik} = (Y_{i1k},..,Y_{in_Bk})'\), will be

\[
\begin{aligned}
\mathbf{\Sigma}_{subject} &=
\left(
\begin{array}
{cccc}
\sigma^2_\delta + \sigma^2 & \sigma^2_\delta & ... & \sigma^2_\delta \\
\sigma^2_\delta & \sigma^2_\delta +\sigma^2 & ... & \sigma^2_\delta \\
. & . & . & . \\
\sigma^2_\delta & \sigma^2_\delta & ... & \sigma^2_\delta + \sigma^2 \\
\end{array}
\right) \\
&= (\sigma^2_\delta + \sigma^2)
\left(
\begin{array}
{cccc}
1 & \rho & ... & \rho \\
\rho & 1 & ... & \rho \\
. & . & . & . \\
\rho & \rho & ... & 1 \\
\end{array}
\right) 
& \text{product of a scalar and a correlation matrix}
\end{aligned}
\]

where \(\rho = \frac{\sigma^2_\delta}{\sigma^2_\delta + \sigma^2}\), which is the compound symmetry structure that we discussed in \protect\hyperlink{random-intercepts-model}{Random-Intercepts Model}

But if you only have repeated measurements on the subject over time, AR(1) structure might be more appropriate

Mixed model for a repeated measure

\[
Y_{ijk} = \mu + \alpha_i + \beta_j + (\alpha \beta)_{ij} + \epsilon_{ijk}
\]

where

\begin{itemize}
\tightlist
\item
  \(\epsilon_{ijk}\) combines random error of both the whole and subplots.
\end{itemize}

In general,

\[
\mathbf{Y = X \beta + \epsilon}
\]

where

\begin{itemize}
\tightlist
\item
  \(\epsilon \sim N(0, \sigma^2 \mathbf{\Sigma})\) where \(\mathbf{\Sigma}\) is block diagonal if the random error covariance is the same for each subject
\end{itemize}

The variance covariance matrix with AR(1) structure is

\[
\mathbf{\Sigma}_{subject} =
\sigma^2
\left(
\begin{array}
{ccccc}
1  & \rho & \rho^2 & ... & \rho^{n_B-1} \\
\rho & 1 & \rho & ... & \rho^{n_B-2} \\
. & . & . & . & . \\
\rho^{n_B-1} & \rho^{n_B-2} & \rho^{n_B-3} & ... & 1 \\
\end{array}
\right)
\]

Hence, the mixed model for a repeated measure can be written as

\[
Y_{ijk} = \mu + \alpha_i + \beta_j + (\alpha \beta)_{ij} + \epsilon_{ijk}
\]

where

\begin{itemize}
\tightlist
\item
  \(\epsilon_{ijk}\) = random error of whole and subplots
\end{itemize}

Generally,

\[
\mathbf{Y = X \beta + \epsilon} 
\]

where \(\epsilon \sim N(0, \mathbf{\sigma^2 \Sigma})\) and \(\Sigma\) = block diagonal if the random error covariance is the same for each subject.

\hypertarget{unbalanced-or-unequally-spaced-data}{%
\section{Unbalanced or Unequally Spaced Data}\label{unbalanced-or-unequally-spaced-data}}

Consider the model

\[
Y_{ikt} = \beta_0 + \beta_{0i} + \beta_{1}t + \beta_{1i}t + \beta_{2} t^2 + \beta_{2i} t^2 + \epsilon_{ikt}
\]

where

\begin{itemize}
\tightlist
\item
  \(i = 1,2\) (groups)
\item
  \(k = 1,â€¦, n_i\) ( individuals)
\item
  \(t = (t_1,t_2,t_3,t_4)\) (times)
\item
  \(\beta_{2i}\) = common quadratic term
\item
  \(\beta_{1i}\) = common linear time trends
\item
  \(\beta_{0i}\) = common intercepts
\end{itemize}

Then, we assume the variance-covariance matrix of the repeated measurements collected on a particular subject over time has the form

\[
\mathbf{\Sigma}_{ik} = \sigma^2
\left(
\begin{array}
{cccc}
1 & \rho^{t_2-t_1} & \rho^{t_3-t_1} & \rho^{t_4-t_1} \\
\rho^{t_2-t_1} & 1 & \rho^{t_3-t_2} & \rho^{t_4-t_2} \\
\rho^{t_3-t_1} & \rho^{t_3-t_2} & 1 & \rho^{t_4-t_3} \\
\rho^{t_4-t_1} & \rho^{t_4-t_2} & \rho^{t_4-t_3} & 1
\end{array}
\right)
\]

which is called ``power'' covariance model

We can consider \(\beta_{2i} , \beta_{1i}, \beta_{0i}\) accordingly to see whether these terms are needed in the final model

\hypertarget{application-5}{%
\section{Application}\label{application-5}}

R Packages for mixed models

\begin{itemize}
\item
  \texttt{nlme}

  \begin{itemize}
  \item
    has nested structure
  \item
    flexible for complex design
  \item
    not user-friendly
  \end{itemize}
\item
  \texttt{lme4}

  \begin{itemize}
  \item
    computationally efficient
  \item
    user-friendly
  \item
    can handle non-normal response
  \item
    for more detailed application, check \href{https://arxiv.org/abs/1406.5823}{Fitting Linear Mixed-Effects Models Using lme4}
  \end{itemize}
\item
  Others

  \begin{itemize}
  \item
    Bayesian setting: \texttt{MCMCglmm}, \texttt{brms}
  \item
    For genetics: \texttt{ASReml}
  \end{itemize}
\end{itemize}

\hypertarget{example-1-pulps}{%
\subsection{Example 1 (Pulps)}\label{example-1-pulps}}

Model:

\[
y_{ij} = \mu + \alpha_i + \epsilon_{ij}
\]

where

\begin{itemize}
\tightlist
\item
  \(i = 1,..,a\) groups for random effect \(\alpha_i\)
\item
  \(j = 1,...,n\) individuals in each group
\item
  \(\alpha_i \sim N(0, \sigma^2_\alpha)\) is random effects
\item
  \(\epsilon_{ij} \sim N(0, \sigma^2_\epsilon)\) is random effects
\item
  Imply compound symmetry model where the intraclass correlation coefficient is: \(\rho = \frac{\sigma^2_\alpha}{\sigma^2_\alpha + \sigma^2_\epsilon}\)
\item
  If factor \(a\) does not explain much variation, low correlation within the levels: \(\sigma^2_\alpha \to 0\) then \(\rho \to 0\)
\item
  If factor \(a\) explain much variation, high correlation within the levels \(\sigma^2_\alpha \to \infty\) hence, \(\rho \to 1\)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(pulp, }\AttributeTok{package =} \StringTok{"faraway"}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(}
    \AttributeTok{y    =}\NormalTok{ pulp}\SpecialCharTok{$}\NormalTok{bright,}
    \AttributeTok{x    =}\NormalTok{ pulp}\SpecialCharTok{$}\NormalTok{operator,}
    \AttributeTok{xlab =} \StringTok{"Operator"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"Brightness"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{08-linear-mixed-models_files/figure-latex/unnamed-chunk-5-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pulp }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{group\_by}\NormalTok{(operator) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{summarise}\NormalTok{(}\AttributeTok{average =} \FunctionTok{mean}\NormalTok{(bright))}
\CommentTok{\#\textgreater{} \# A tibble: 4 x 2}
\CommentTok{\#\textgreater{}   operator average}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{}      \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 a           60.2}
\CommentTok{\#\textgreater{} 2 b           60.1}
\CommentTok{\#\textgreater{} 3 c           60.6}
\CommentTok{\#\textgreater{} 4 d           60.7}
\end{Highlighting}
\end{Shaded}

\texttt{lmer} application

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(lme4)}
\NormalTok{mixed\_model }\OtherTok{\textless{}{-}}
    \FunctionTok{lmer}\NormalTok{(}
        \CommentTok{\# pipe (i..e, | ) denotes random{-}effect terms}
        \AttributeTok{formula =}\NormalTok{ bright }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{operator), }
         \AttributeTok{data =}\NormalTok{ pulp)}
\FunctionTok{summary}\NormalTok{(mixed\_model)}
\CommentTok{\#\textgreater{} Linear mixed model fit by REML [\textquotesingle{}lmerMod\textquotesingle{}]}
\CommentTok{\#\textgreater{} Formula: bright \textasciitilde{} 1 + (1 | operator)}
\CommentTok{\#\textgreater{}    Data: pulp}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} REML criterion at convergence: 18.6}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Scaled residuals: }
\CommentTok{\#\textgreater{}     Min      1Q  Median      3Q     Max }
\CommentTok{\#\textgreater{} {-}1.4666 {-}0.7595 {-}0.1244  0.6281  1.6012 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Random effects:}
\CommentTok{\#\textgreater{}  Groups   Name        Variance Std.Dev.}
\CommentTok{\#\textgreater{}  operator (Intercept) 0.06808  0.2609  }
\CommentTok{\#\textgreater{}  Residual             0.10625  0.3260  }
\CommentTok{\#\textgreater{} Number of obs: 20, groups:  operator, 4}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Fixed effects:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value}
\CommentTok{\#\textgreater{} (Intercept)  60.4000     0.1494   404.2}
\FunctionTok{coef}\NormalTok{(mixed\_model)}
\CommentTok{\#\textgreater{} $operator}
\CommentTok{\#\textgreater{}   (Intercept)}
\CommentTok{\#\textgreater{} a    60.27806}
\CommentTok{\#\textgreater{} b    60.14088}
\CommentTok{\#\textgreater{} c    60.56767}
\CommentTok{\#\textgreater{} d    60.61340}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} attr(,"class")}
\CommentTok{\#\textgreater{} [1] "coef.mer"}
\FunctionTok{fixef}\NormalTok{(mixed\_model)   }\CommentTok{\# fixed effects}
\CommentTok{\#\textgreater{} (Intercept) }
\CommentTok{\#\textgreater{}        60.4}
\FunctionTok{confint}\NormalTok{(mixed\_model) }\CommentTok{\# confidence interval}
\CommentTok{\#\textgreater{}                 2.5 \%     97.5 \%}
\CommentTok{\#\textgreater{} .sig01       0.000000  0.6178987}
\CommentTok{\#\textgreater{} .sigma       0.238912  0.4821845}
\CommentTok{\#\textgreater{} (Intercept) 60.071299 60.7287012}
\FunctionTok{ranef}\NormalTok{(mixed\_model)   }\CommentTok{\# random effects}
\CommentTok{\#\textgreater{} $operator}
\CommentTok{\#\textgreater{}   (Intercept)}
\CommentTok{\#\textgreater{} a  {-}0.1219403}
\CommentTok{\#\textgreater{} b  {-}0.2591231}
\CommentTok{\#\textgreater{} c   0.1676679}
\CommentTok{\#\textgreater{} d   0.2133955}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} with conditional variances for "operator"}
\FunctionTok{VarCorr}\NormalTok{(mixed\_model) }\CommentTok{\# random effects standard deviation}
\CommentTok{\#\textgreater{}  Groups   Name        Std.Dev.}
\CommentTok{\#\textgreater{}  operator (Intercept) 0.26093 }
\CommentTok{\#\textgreater{}  Residual             0.32596}
\NormalTok{re\_dat }\OtherTok{=} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{VarCorr}\NormalTok{(mixed\_model))}

\CommentTok{\# rho based on the above formula}
\NormalTok{rho }\OtherTok{=}\NormalTok{ re\_dat[}\DecValTok{1}\NormalTok{, }\StringTok{\textquotesingle{}vcov\textquotesingle{}}\NormalTok{] }\SpecialCharTok{/}\NormalTok{ (re\_dat[}\DecValTok{1}\NormalTok{, }\StringTok{\textquotesingle{}vcov\textquotesingle{}}\NormalTok{] }\SpecialCharTok{+}\NormalTok{ re\_dat[}\DecValTok{2}\NormalTok{, }\StringTok{\textquotesingle{}vcov\textquotesingle{}}\NormalTok{])}
\NormalTok{rho}
\CommentTok{\#\textgreater{} [1] 0.3905354}
\end{Highlighting}
\end{Shaded}

To Satterthwaite approximation for the denominator df, we use \texttt{lmerTest}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(lmerTest)}
\FunctionTok{summary}\NormalTok{(lmerTest}\SpecialCharTok{::}\FunctionTok{lmer}\NormalTok{(bright }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ operator), pulp))}\SpecialCharTok{$}\NormalTok{coefficients}
\CommentTok{\#\textgreater{}             Estimate Std. Error df  t value     Pr(\textgreater{}|t|)}
\CommentTok{\#\textgreater{} (Intercept)     60.4  0.1494434  3 404.1664 3.340265e{-}08}
\FunctionTok{confint}\NormalTok{(mixed\_model)[}\DecValTok{3}\NormalTok{, ]}
\CommentTok{\#\textgreater{}   2.5 \%  97.5 \% }
\CommentTok{\#\textgreater{} 60.0713 60.7287}
\end{Highlighting}
\end{Shaded}

In this example, we can see that the confidence interval computed by \texttt{confint} in \texttt{lmer} package is very close is \texttt{confint} in \texttt{lmerTest} model.

\texttt{MCMglmm} application

under the Bayesian framework

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(MCMCglmm)}
\NormalTok{mixed\_model\_bayes }\OtherTok{\textless{}{-}}
    \FunctionTok{MCMCglmm}\NormalTok{(}
\NormalTok{        bright }\SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{,}
        \AttributeTok{random =}  \SpecialCharTok{\textasciitilde{}}\NormalTok{ operator,}
        \AttributeTok{data =}\NormalTok{ pulp,}
        \AttributeTok{verbose =} \ConstantTok{FALSE}
\NormalTok{    )}
\FunctionTok{summary}\NormalTok{(mixed\_model\_bayes)}\SpecialCharTok{$}\NormalTok{solutions}
\CommentTok{\#\textgreater{}             post.mean l{-}95\% CI u{-}95\% CI eff.samp pMCMC}
\CommentTok{\#\textgreater{} (Intercept)  60.38821 60.08119 60.70809     1000 0.001}
\end{Highlighting}
\end{Shaded}

this method offers the confidence interval slightly more positive than \texttt{lmer} and \texttt{lmerTest}

\hypertarget{prediction}{%
\subsubsection{Prediction}\label{prediction}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# random effects prediction (BLUPs)}
\FunctionTok{ranef}\NormalTok{(mixed\_model)}\SpecialCharTok{$}\NormalTok{operator}
\CommentTok{\#\textgreater{}   (Intercept)}
\CommentTok{\#\textgreater{} a  {-}0.1219403}
\CommentTok{\#\textgreater{} b  {-}0.2591231}
\CommentTok{\#\textgreater{} c   0.1676679}
\CommentTok{\#\textgreater{} d   0.2133955}

\CommentTok{\# prediction for each categories}
\FunctionTok{fixef}\NormalTok{(mixed\_model) }\SpecialCharTok{+} \FunctionTok{ranef}\NormalTok{(mixed\_model)}\SpecialCharTok{$}\NormalTok{operator }
\CommentTok{\#\textgreater{}   (Intercept)}
\CommentTok{\#\textgreater{} a    60.27806}
\CommentTok{\#\textgreater{} b    60.14088}
\CommentTok{\#\textgreater{} c    60.56767}
\CommentTok{\#\textgreater{} d    60.61340}

\CommentTok{\# equivalent to the above method}
\FunctionTok{predict}\NormalTok{(mixed\_model, }\AttributeTok{newdata =} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{operator =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}a\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}b\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}c\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}d\textquotesingle{}}\NormalTok{))) }
\CommentTok{\#\textgreater{}        1        2        3        4 }
\CommentTok{\#\textgreater{} 60.27806 60.14088 60.56767 60.61340}
\end{Highlighting}
\end{Shaded}

use \texttt{bootMer()} to get bootstrap-based confidence intervals for predictions.

Another example using GLMM in the context of blocking

Penicillin data

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(penicillin, }\AttributeTok{package =} \StringTok{"faraway"}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(penicillin)}
\CommentTok{\#\textgreater{}  treat    blend       yield   }
\CommentTok{\#\textgreater{}  A:5   Blend1:4   Min.   :77  }
\CommentTok{\#\textgreater{}  B:5   Blend2:4   1st Qu.:81  }
\CommentTok{\#\textgreater{}  C:5   Blend3:4   Median :87  }
\CommentTok{\#\textgreater{}  D:5   Blend4:4   Mean   :86  }
\CommentTok{\#\textgreater{}        Blend5:4   3rd Qu.:89  }
\CommentTok{\#\textgreater{}                   Max.   :97}
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{ggplot}\NormalTok{(penicillin, }\FunctionTok{aes}\NormalTok{(}
    \AttributeTok{y     =}\NormalTok{ yield,}
    \AttributeTok{x     =}\NormalTok{ treat,}
    \AttributeTok{shape =}\NormalTok{ blend,}
    \AttributeTok{color =}\NormalTok{ blend}
\NormalTok{)) }\SpecialCharTok{+} 
    \CommentTok{\# treatment = fixed effect}
    \CommentTok{\# blend = random effects}
    \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{size =} \DecValTok{3}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{xlab}\NormalTok{(}\StringTok{"Treatment"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{08-linear-mixed-models_files/figure-latex/unnamed-chunk-10-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\FunctionTok{library}\NormalTok{(lmerTest) }\CommentTok{\# for p{-}values}
\NormalTok{mixed\_model }\OtherTok{\textless{}{-}}\NormalTok{ lmerTest}\SpecialCharTok{::}\FunctionTok{lmer}\NormalTok{(yield }\SpecialCharTok{\textasciitilde{}}\NormalTok{ treat }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ blend),}
                              \AttributeTok{data =}\NormalTok{ penicillin)}
\FunctionTok{summary}\NormalTok{(mixed\_model)}
\CommentTok{\#\textgreater{} Linear mixed model fit by REML. t{-}tests use Satterthwaite\textquotesingle{}s method [}
\CommentTok{\#\textgreater{} lmerModLmerTest]}
\CommentTok{\#\textgreater{} Formula: yield \textasciitilde{} treat + (1 | blend)}
\CommentTok{\#\textgreater{}    Data: penicillin}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} REML criterion at convergence: 103.8}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Scaled residuals: }
\CommentTok{\#\textgreater{}     Min      1Q  Median      3Q     Max }
\CommentTok{\#\textgreater{} {-}1.4152 {-}0.5017 {-}0.1644  0.6830  1.2836 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Random effects:}
\CommentTok{\#\textgreater{}  Groups   Name        Variance Std.Dev.}
\CommentTok{\#\textgreater{}  blend    (Intercept) 11.79    3.434   }
\CommentTok{\#\textgreater{}  Residual             18.83    4.340   }
\CommentTok{\#\textgreater{} Number of obs: 20, groups:  blend, 5}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Fixed effects:}
\CommentTok{\#\textgreater{}             Estimate Std. Error     df t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)   84.000      2.475 11.075  33.941 1.51e{-}12 ***}
\CommentTok{\#\textgreater{} treatB         1.000      2.745 12.000   0.364   0.7219    }
\CommentTok{\#\textgreater{} treatC         5.000      2.745 12.000   1.822   0.0935 .  }
\CommentTok{\#\textgreater{} treatD         2.000      2.745 12.000   0.729   0.4802    }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Correlation of Fixed Effects:}
\CommentTok{\#\textgreater{}        (Intr) treatB treatC}
\CommentTok{\#\textgreater{} treatB {-}0.555              }
\CommentTok{\#\textgreater{} treatC {-}0.555  0.500       }
\CommentTok{\#\textgreater{} treatD {-}0.555  0.500  0.500}

\CommentTok{\#The BLUPs for the each blend}
\FunctionTok{ranef}\NormalTok{(mixed\_model)}\SpecialCharTok{$}\NormalTok{blend}
\CommentTok{\#\textgreater{}        (Intercept)}
\CommentTok{\#\textgreater{} Blend1   4.2878788}
\CommentTok{\#\textgreater{} Blend2  {-}2.1439394}
\CommentTok{\#\textgreater{} Blend3  {-}0.7146465}
\CommentTok{\#\textgreater{} Blend4   1.4292929}
\CommentTok{\#\textgreater{} Blend5  {-}2.8585859}
\end{Highlighting}
\end{Shaded}

Examine treatment effect

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(mixed\_model) }\CommentTok{\# p{-}value based on lmerTest}
\CommentTok{\#\textgreater{} Type III Analysis of Variance Table with Satterthwaite\textquotesingle{}s method}
\CommentTok{\#\textgreater{}       Sum Sq Mean Sq NumDF DenDF F value Pr(\textgreater{}F)}
\CommentTok{\#\textgreater{} treat     70  23.333     3    12  1.2389 0.3387}
\end{Highlighting}
\end{Shaded}

Since the p-value is greater than 0.05, we can't reject the null hypothesis that there is no treatment effect.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(pbkrtest)}
\CommentTok{\# REML is not appropriate for testing fixed effects, it should be ML}
\NormalTok{full\_model }\OtherTok{\textless{}{-}}
    \FunctionTok{lmer}\NormalTok{(yield }\SpecialCharTok{\textasciitilde{}}\NormalTok{ treat }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ blend), }
\NormalTok{         penicillin, }
         \AttributeTok{REML =} \ConstantTok{FALSE}\NormalTok{) }
\NormalTok{null\_model }\OtherTok{\textless{}{-}} \FunctionTok{lmer}\NormalTok{(yield }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ blend), }
\NormalTok{                   penicillin, }
                   \AttributeTok{REML =} \ConstantTok{FALSE}\NormalTok{)}

\CommentTok{\# use  Kenward{-}Roger approximation for df}
\FunctionTok{KRmodcomp}\NormalTok{(full\_model, null\_model) }
\CommentTok{\#\textgreater{} large : yield \textasciitilde{} treat + (1 | blend)}
\CommentTok{\#\textgreater{} small : yield \textasciitilde{} 1 + (1 | blend)}
\CommentTok{\#\textgreater{}          stat     ndf     ddf F.scaling p.value}
\CommentTok{\#\textgreater{} Ftest  1.2389  3.0000 12.0000         1  0.3387}
\end{Highlighting}
\end{Shaded}

Since the p-value is greater than 0.05, and consistent with our previous observation, we conclude that we can't reject the null hypothesis that there is no treatment effect.

\hypertarget{example-2-rats}{%
\subsection{Example 2 (Rats)}\label{example-2-rats}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rats }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}
    \StringTok{"images/rats.dat"}\NormalTok{,}
    \AttributeTok{header =}\NormalTok{ F,}
    \AttributeTok{sep =} \StringTok{\textquotesingle{} \textquotesingle{}}\NormalTok{,}
    \AttributeTok{col.names =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Treatment\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}rat\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}y\textquotesingle{}}\NormalTok{)}
\NormalTok{)}

\CommentTok{\# log transformed age}
\NormalTok{rats}\SpecialCharTok{$}\NormalTok{t }\OtherTok{\textless{}{-}} \FunctionTok{log}\NormalTok{(}\DecValTok{1} \SpecialCharTok{+}\NormalTok{ (rats}\SpecialCharTok{$}\NormalTok{age }\SpecialCharTok{{-}} \DecValTok{45}\NormalTok{) }\SpecialCharTok{/} \DecValTok{10}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

We are interested in whether treatment effect induces changes over time.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rat\_model }\OtherTok{\textless{}{-}}
    \CommentTok{\# treatment = fixed effect, rat = random effects}
\NormalTok{    lmerTest}\SpecialCharTok{::}\FunctionTok{lmer}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ t}\SpecialCharTok{:}\NormalTok{Treatment }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ rat), }
                   \AttributeTok{data =}\NormalTok{ rats) }
\FunctionTok{summary}\NormalTok{(rat\_model)}
\CommentTok{\#\textgreater{} Linear mixed model fit by REML. t{-}tests use Satterthwaite\textquotesingle{}s method [}
\CommentTok{\#\textgreater{} lmerModLmerTest]}
\CommentTok{\#\textgreater{} Formula: y \textasciitilde{} t:Treatment + (1 | rat)}
\CommentTok{\#\textgreater{}    Data: rats}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} REML criterion at convergence: 932.4}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Scaled residuals: }
\CommentTok{\#\textgreater{}      Min       1Q   Median       3Q      Max }
\CommentTok{\#\textgreater{} {-}2.25574 {-}0.65898 {-}0.01163  0.58356  2.88309 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Random effects:}
\CommentTok{\#\textgreater{}  Groups   Name        Variance Std.Dev.}
\CommentTok{\#\textgreater{}  rat      (Intercept) 3.565    1.888   }
\CommentTok{\#\textgreater{}  Residual             1.445    1.202   }
\CommentTok{\#\textgreater{} Number of obs: 252, groups:  rat, 50}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Fixed effects:}
\CommentTok{\#\textgreater{}                Estimate Std. Error       df t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)     68.6074     0.3312  89.0275  207.13   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} t:Treatmentcon   7.3138     0.2808 247.2762   26.05   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} t:Treatmenthig   6.8711     0.2276 247.7097   30.19   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} t:Treatmentlow   7.5069     0.2252 247.5196   33.34   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Correlation of Fixed Effects:}
\CommentTok{\#\textgreater{}             (Intr) t:Trtmntc t:Trtmnth}
\CommentTok{\#\textgreater{} t:Tretmntcn {-}0.327                    }
\CommentTok{\#\textgreater{} t:Tretmnthg {-}0.340  0.111             }
\CommentTok{\#\textgreater{} t:Tretmntlw {-}0.351  0.115     0.119}
\FunctionTok{anova}\NormalTok{(rat\_model)}
\CommentTok{\#\textgreater{} Type III Analysis of Variance Table with Satterthwaite\textquotesingle{}s method}
\CommentTok{\#\textgreater{}             Sum Sq Mean Sq NumDF  DenDF F value    Pr(\textgreater{}F)    }
\CommentTok{\#\textgreater{} t:Treatment 3181.9  1060.6     3 223.21  734.11 \textless{} 2.2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

Since the p-value is significant, we can be confident concluding that there is a treatment effect

\hypertarget{example-3-agridat}{%
\subsection{Example 3 (Agridat)}\label{example-3-agridat}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(agridat)}
\FunctionTok{library}\NormalTok{(latticeExtra)}
\NormalTok{dat }\OtherTok{\textless{}{-}}\NormalTok{ harris.wateruse}
\CommentTok{\# Compare to Schabenberger \& Pierce, fig 7.23}
\FunctionTok{useOuterStrips}\NormalTok{(}
    \FunctionTok{xyplot}\NormalTok{(}
\NormalTok{        water }\SpecialCharTok{\textasciitilde{}}\NormalTok{ day }\SpecialCharTok{|}\NormalTok{ species }\SpecialCharTok{*}\NormalTok{ age,}
\NormalTok{        dat,}
        \AttributeTok{as.table =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{group =}\NormalTok{ tree,}
        \AttributeTok{type =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}p\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}smooth\textquotesingle{}}\NormalTok{),}
        \AttributeTok{main =} \StringTok{"harris.wateruse 2 species, 2 ages (10 trees each)"}
\NormalTok{    )}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{08-linear-mixed-models_files/figure-latex/unnamed-chunk-15-1} \end{center}

Remove outliers

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(dat, day}\SpecialCharTok{!=}\DecValTok{268}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Plot between age and species

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{xyplot}\NormalTok{(}
\NormalTok{    water }\SpecialCharTok{\textasciitilde{}}\NormalTok{ day }\SpecialCharTok{|}\NormalTok{ tree,}
\NormalTok{    dat,}
    \AttributeTok{subset   =}\NormalTok{ age }\SpecialCharTok{==} \StringTok{"A2"} \SpecialCharTok{\&}\NormalTok{ species }\SpecialCharTok{==} \StringTok{"S2"}\NormalTok{,}
    \AttributeTok{as.table =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{type     =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}p\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}smooth\textquotesingle{}}\NormalTok{),}
    \AttributeTok{ylab     =} \StringTok{"Water use profiles of individual trees"}\NormalTok{,}
    \AttributeTok{main     =} \StringTok{"harris.wateruse (Age 2, Species 2)"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{08-linear-mixed-models_files/figure-latex/unnamed-chunk-17-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Rescale day for nicer output, }
\CommentTok{\# and convergence issues, add quadratic term}
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{transform}\NormalTok{(dat, }\AttributeTok{ti =}\NormalTok{ day }\SpecialCharTok{/} \DecValTok{100}\NormalTok{)}
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{transform}\NormalTok{(dat, }\AttributeTok{ti2 =}\NormalTok{ ti }\SpecialCharTok{*}\NormalTok{ ti)}
\CommentTok{\# Start with a subgroup: age 2, species 2}
\NormalTok{d22 }\OtherTok{\textless{}{-}} \FunctionTok{droplevels}\NormalTok{(}\FunctionTok{subset}\NormalTok{(dat, age }\SpecialCharTok{==} \StringTok{"A2"} \SpecialCharTok{\&}\NormalTok{ species }\SpecialCharTok{==} \StringTok{"S2"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\texttt{lme} function from \texttt{nlme} package

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(nlme)}

\DocumentationTok{\#\# We use pdDiag() to get uncorrelated random effects}
\NormalTok{m1n }\OtherTok{\textless{}{-}} \FunctionTok{lme}\NormalTok{(}
\NormalTok{    water }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ ti }\SpecialCharTok{+}\NormalTok{ ti2,}
    \CommentTok{\#intercept, time and time{-}squared = fixed effects}
    \AttributeTok{data =}\NormalTok{ d22,}
    \AttributeTok{na.action =}\NormalTok{ na.omit,}
    \AttributeTok{random =} \FunctionTok{list}\NormalTok{(}\AttributeTok{tree =} \FunctionTok{pdDiag}\NormalTok{(}\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ ti }\SpecialCharTok{+}\NormalTok{ ti2)) }
    \CommentTok{\# random intercept, time }
    \CommentTok{\# and time squared per tree = random effects}
\NormalTok{)}
\FunctionTok{ranef}\NormalTok{(m1n)}
\CommentTok{\#\textgreater{}     (Intercept)            ti           ti2}
\CommentTok{\#\textgreater{} T04   0.1985796  1.609864e{-}09  4.990101e{-}10}
\CommentTok{\#\textgreater{} T05   0.3492827  2.487690e{-}10 {-}4.845287e{-}11}
\CommentTok{\#\textgreater{} T19  {-}0.1978989 {-}7.681202e{-}10 {-}1.961453e{-}10}
\CommentTok{\#\textgreater{} T23   0.4519003 {-}3.270426e{-}10 {-}2.413583e{-}10}
\CommentTok{\#\textgreater{} T38  {-}0.6457494 {-}1.608770e{-}09 {-}3.298010e{-}10}
\CommentTok{\#\textgreater{} T40   0.3739432  3.264705e{-}10 {-}2.543109e{-}11}
\CommentTok{\#\textgreater{} T49   0.8620648  9.021831e{-}10 {-}5.402247e{-}12}
\CommentTok{\#\textgreater{} T53  {-}0.5655049 {-}8.279040e{-}10 {-}4.579291e{-}11}
\CommentTok{\#\textgreater{} T67  {-}0.4394623 {-}3.485113e{-}10  2.147434e{-}11}
\CommentTok{\#\textgreater{} T71  {-}0.3871552  7.930610e{-}10  3.718993e{-}10}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{fixef}\NormalTok{(m1n)}
\CommentTok{\#\textgreater{} (Intercept)          ti         ti2 }
\CommentTok{\#\textgreater{}  {-}10.798799   12.346704   {-}2.838503}
\FunctionTok{summary}\NormalTok{(m1n)}
\CommentTok{\#\textgreater{} Linear mixed{-}effects model fit by REML}
\CommentTok{\#\textgreater{}   Data: d22 }
\CommentTok{\#\textgreater{}        AIC     BIC    logLik}
\CommentTok{\#\textgreater{}   276.5142 300.761 {-}131.2571}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Random effects:}
\CommentTok{\#\textgreater{}  Formula: \textasciitilde{}1 + ti + ti2 | tree}
\CommentTok{\#\textgreater{}  Structure: Diagonal}
\CommentTok{\#\textgreater{}         (Intercept)           ti          ti2  Residual}
\CommentTok{\#\textgreater{} StdDev:   0.5187869 1.438333e{-}05 3.864019e{-}06 0.3836614}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Fixed effects:  water \textasciitilde{} 1 + ti + ti2 }
\CommentTok{\#\textgreater{}                  Value Std.Error  DF   t{-}value p{-}value}
\CommentTok{\#\textgreater{} (Intercept) {-}10.798799 0.8814666 227 {-}12.25094       0}
\CommentTok{\#\textgreater{} ti           12.346704 0.7827112 227  15.77428       0}
\CommentTok{\#\textgreater{} ti2          {-}2.838503 0.1720614 227 {-}16.49704       0}
\CommentTok{\#\textgreater{}  Correlation: }
\CommentTok{\#\textgreater{}     (Intr) ti    }
\CommentTok{\#\textgreater{} ti  {-}0.979       }
\CommentTok{\#\textgreater{} ti2  0.970 {-}0.997}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Standardized Within{-}Group Residuals:}
\CommentTok{\#\textgreater{}         Min          Q1         Med          Q3         Max }
\CommentTok{\#\textgreater{} {-}3.07588246 {-}0.58531056  0.01210209  0.65402695  3.88777402 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Observations: 239}
\CommentTok{\#\textgreater{} Number of Groups: 10}
\end{Highlighting}
\end{Shaded}

\texttt{lmer} function from \texttt{lme4} package

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1lmer }\OtherTok{\textless{}{-}}
    \FunctionTok{lmer}\NormalTok{(water }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ ti }\SpecialCharTok{+}\NormalTok{ ti2 }\SpecialCharTok{+}\NormalTok{ (ti }\SpecialCharTok{+}\NormalTok{ ti2 }\SpecialCharTok{||}
\NormalTok{                                     tree),}
         \AttributeTok{data =}\NormalTok{ d22,}
         \AttributeTok{na.action =}\NormalTok{ na.omit)}
\FunctionTok{ranef}\NormalTok{(m1lmer)}
\CommentTok{\#\textgreater{} $tree}
\CommentTok{\#\textgreater{}     (Intercept) ti ti2}
\CommentTok{\#\textgreater{} T04   0.1985796  0   0}
\CommentTok{\#\textgreater{} T05   0.3492827  0   0}
\CommentTok{\#\textgreater{} T19  {-}0.1978989  0   0}
\CommentTok{\#\textgreater{} T23   0.4519003  0   0}
\CommentTok{\#\textgreater{} T38  {-}0.6457494  0   0}
\CommentTok{\#\textgreater{} T40   0.3739432  0   0}
\CommentTok{\#\textgreater{} T49   0.8620648  0   0}
\CommentTok{\#\textgreater{} T53  {-}0.5655049  0   0}
\CommentTok{\#\textgreater{} T67  {-}0.4394623  0   0}
\CommentTok{\#\textgreater{} T71  {-}0.3871552  0   0}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} with conditional variances for "tree"}
\end{Highlighting}
\end{Shaded}

Notes:

\begin{itemize}
\item
  \texttt{\textbar{}\textbar{}} double pipes= uncorrelated random effects
\item
  To remove the intercept term:

  \begin{itemize}
  \item
    \texttt{(0+ti\textbar{}tree)}
  \item
    \texttt{(ti-1\textbar{}tree)}
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{fixef}\NormalTok{(m1lmer)}
\CommentTok{\#\textgreater{} (Intercept)          ti         ti2 }
\CommentTok{\#\textgreater{}  {-}10.798799   12.346704   {-}2.838503}
\NormalTok{m1l }\OtherTok{\textless{}{-}}
    \FunctionTok{lmer}\NormalTok{(water }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ ti }\SpecialCharTok{+}\NormalTok{ ti2 }
         \SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ tree) }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{0} \SpecialCharTok{+}\NormalTok{ ti }\SpecialCharTok{|}\NormalTok{ tree) }
         \SpecialCharTok{+}\NormalTok{ (}\DecValTok{0} \SpecialCharTok{+}\NormalTok{ ti2 }\SpecialCharTok{|}\NormalTok{ tree), }\AttributeTok{data =}\NormalTok{ d22)}
\FunctionTok{ranef}\NormalTok{(m1l)}
\CommentTok{\#\textgreater{} $tree}
\CommentTok{\#\textgreater{}     (Intercept) ti ti2}
\CommentTok{\#\textgreater{} T04   0.1985796  0   0}
\CommentTok{\#\textgreater{} T05   0.3492827  0   0}
\CommentTok{\#\textgreater{} T19  {-}0.1978989  0   0}
\CommentTok{\#\textgreater{} T23   0.4519003  0   0}
\CommentTok{\#\textgreater{} T38  {-}0.6457494  0   0}
\CommentTok{\#\textgreater{} T40   0.3739432  0   0}
\CommentTok{\#\textgreater{} T49   0.8620648  0   0}
\CommentTok{\#\textgreater{} T53  {-}0.5655049  0   0}
\CommentTok{\#\textgreater{} T67  {-}0.4394623  0   0}
\CommentTok{\#\textgreater{} T71  {-}0.3871552  0   0}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} with conditional variances for "tree"}
\FunctionTok{fixef}\NormalTok{(m1l)}
\CommentTok{\#\textgreater{} (Intercept)          ti         ti2 }
\CommentTok{\#\textgreater{}  {-}10.798799   12.346704   {-}2.838503}
\end{Highlighting}
\end{Shaded}

To include structured covariance terms, we can use the following way

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m2n }\OtherTok{\textless{}{-}} \FunctionTok{lme}\NormalTok{(}
\NormalTok{    water }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ ti }\SpecialCharTok{+}\NormalTok{ ti2,}
    \AttributeTok{data =}\NormalTok{ d22,}
    \AttributeTok{random =} \SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{|}\NormalTok{ tree,}
    \AttributeTok{cor =} \FunctionTok{corExp}\NormalTok{(}\AttributeTok{form =}  \SpecialCharTok{\textasciitilde{}}\NormalTok{ day }\SpecialCharTok{|}\NormalTok{ tree),}
    \AttributeTok{na.action =}\NormalTok{ na.omit}
\NormalTok{)}
\FunctionTok{ranef}\NormalTok{(m2n)}
\CommentTok{\#\textgreater{}     (Intercept)}
\CommentTok{\#\textgreater{} T04   0.1929971}
\CommentTok{\#\textgreater{} T05   0.3424631}
\CommentTok{\#\textgreater{} T19  {-}0.1988495}
\CommentTok{\#\textgreater{} T23   0.4538660}
\CommentTok{\#\textgreater{} T38  {-}0.6413664}
\CommentTok{\#\textgreater{} T40   0.3769378}
\CommentTok{\#\textgreater{} T49   0.8410043}
\CommentTok{\#\textgreater{} T53  {-}0.5528236}
\CommentTok{\#\textgreater{} T67  {-}0.4452930}
\CommentTok{\#\textgreater{} T71  {-}0.3689358}
\FunctionTok{fixef}\NormalTok{(m2n)}
\CommentTok{\#\textgreater{} (Intercept)          ti         ti2 }
\CommentTok{\#\textgreater{}  {-}11.223310   12.712094   {-}2.913682}
\FunctionTok{summary}\NormalTok{(m2n)}
\CommentTok{\#\textgreater{} Linear mixed{-}effects model fit by REML}
\CommentTok{\#\textgreater{}   Data: d22 }
\CommentTok{\#\textgreater{}        AIC      BIC   logLik}
\CommentTok{\#\textgreater{}   263.3081 284.0911 {-}125.654}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Random effects:}
\CommentTok{\#\textgreater{}  Formula: \textasciitilde{}1 | tree}
\CommentTok{\#\textgreater{}         (Intercept)  Residual}
\CommentTok{\#\textgreater{} StdDev:   0.5154042 0.3925777}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Correlation Structure: Exponential spatial correlation}
\CommentTok{\#\textgreater{}  Formula: \textasciitilde{}day | tree }
\CommentTok{\#\textgreater{}  Parameter estimate(s):}
\CommentTok{\#\textgreater{}    range }
\CommentTok{\#\textgreater{} 3.794624 }
\CommentTok{\#\textgreater{} Fixed effects:  water \textasciitilde{} 1 + ti + ti2 }
\CommentTok{\#\textgreater{}                  Value Std.Error  DF   t{-}value p{-}value}
\CommentTok{\#\textgreater{} (Intercept) {-}11.223310 1.0988725 227 {-}10.21348       0}
\CommentTok{\#\textgreater{} ti           12.712094 0.9794235 227  12.97916       0}
\CommentTok{\#\textgreater{} ti2          {-}2.913682 0.2148551 227 {-}13.56115       0}
\CommentTok{\#\textgreater{}  Correlation: }
\CommentTok{\#\textgreater{}     (Intr) ti    }
\CommentTok{\#\textgreater{} ti  {-}0.985       }
\CommentTok{\#\textgreater{} ti2  0.976 {-}0.997}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Standardized Within{-}Group Residuals:}
\CommentTok{\#\textgreater{}         Min          Q1         Med          Q3         Max }
\CommentTok{\#\textgreater{} {-}3.04861039 {-}0.55703950  0.00278101  0.62558762  3.80676991 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Observations: 239}
\CommentTok{\#\textgreater{} Number of Groups: 10}
\end{Highlighting}
\end{Shaded}

\hypertarget{nonlinear-and-generalized-linear-mixed-models}{%
\chapter{Nonlinear and Generalized Linear Mixed Models}\label{nonlinear-and-generalized-linear-mixed-models}}

\begin{itemize}
\tightlist
\item
  NLMMs extend the nonlinear model to include both fixed effects and random effects
\item
  GLMMs extend the generalized linear model to include both fixed effects and random effects.
\end{itemize}

A nonlinear mixed model has the form of

\[
Y_{ij} = f(\mathbf{x_{ij} , \theta, \alpha_i}) + \epsilon_{ij}
\]

for the j-th response from cluster (or subject) i (\(i = 1,...,n\)), where

\begin{itemize}
\tightlist
\item
  \(j = 1,...,n_i\)
\item
  \(\mathbf{\theta}\) are the fixed effects
\item
  \(\mathbf{\alpha}_i\) are the random effects for cluster i
\item
  \(\mathbf{x}_{ij}\) are the regressors or design variables
\item
  \(f(.)\) is nonlinear mean response function
\end{itemize}

A GLMM can be written as:

we assume

\[
y_i |\alpha_i \sim \text{indep } f(y_i | \alpha)
\]

and \(f(y_i | \mathbf{\alpha})\) is an exponential family distribution,

\[
f(y_i | \alpha) = \exp [\frac{y_i \theta_i - b(\theta_i)}{a(\phi)} - c(y_i, \phi)]
\]

The conditional mean of \(y_i\) is related to \(\theta_i\)

\[
\mu_i = \frac{\partial b(\theta_i)}{\partial \theta_i}
\]

The transformation of this mean will give us the desired linear model to model both the fixed and random effects.

\[
\begin{aligned}
E(y_i |\alpha) &= \mu_i \\
g(\mu_i) &= \mathbf{x_i' \beta + z'_i \alpha}
\end{aligned}
\]

where \(g()\) is a known link function and \(\mu_i\) is the conditional mean. We can see similarity to \protect\hyperlink{generalized-linear-models}{GLM}

We also have to specify the random effects distribution

\[
\alpha \sim f(\alpha)
\]

which is similar to the specification for mixed models.

Moreover, law of large number applies to fixed effects so that you know it is a normal distribution. But here, you can specify \(\alpha\) subjectively.

Hence, we can show NLMM is a special case of the GLMM

\[
\begin{aligned}
\mathbf{Y}_i &= \mathbf{f}(\mathbf{x}_i, \mathbf{\theta, \alpha}_i) + \mathbf{\epsilon}_i \\
\mathbf{Y}_i &= \mathbf{g}^{-1} (\mathbf{x}_i' \beta + \mathbf{z}_i' \mathbf{\alpha}_i) + \mathbf{\epsilon}_i
\end{aligned}
\]

where the inverse link function corresponds to a nonlinear transformation of the fixed and random effects.

\textbf{Note}:

\begin{itemize}
\tightlist
\item
  we can't derive the analytical formulation of the marginal distribution because nonlinear combination of normal variables is not normally distributed, even in the case of additive error (\(e_i\)) and random effects (\(\alpha_i\)) are both normal.
\end{itemize}

\textbf{Consequences of having random effects}

The marginal mean of \(y_i\) is

\[
E(y_i) = E_\alpha(E(y_i | \alpha)) = E_\alpha (\mu_i) = E(g^{-1}(\mathbf{x_i' \beta + z_i' \alpha}))
\]

Because \(g^{-1}()\) is nonlinear, this is the most simplified version we can go for.

In special cases such as log link (\(g(\mu) = \log \mu\) or \(g^{-1}() = \exp()\)) then

\[
E(y_i) = E(\exp(\mathbf{x_i' \beta + z_i' \alpha})) = \exp(\mathbf{x'_i \beta})E(\exp(\mathbf{z}_i'\alpha))
\]

which is the moment generating function of \(\alpha\) evaluated at \(\mathbf{z}_i\)

\textbf{Marginal variance} of \(y_i\)

\[
\begin{aligned}
var(y_i) &= var_\alpha (E(y_i | \alpha)) + E_\alpha (var(y_i | \alpha)) \\
&= var(\mu_i) + E(a(\phi) V(\mu_i)) \\
&= var(g^{-1} (\mathbf{x'_i \beta + z'_i \alpha})) + E(a(\phi)V(g^{-1} (\mathbf{x'_i \beta + z'_i \alpha})))
\end{aligned}
\]

Without specific assumption about \(g()\) and/or the conditional distribution of \(\mathbf{y}\), this is the most simplified version.

\textbf{Marginal covariance of} \(\mathbf{y}\)

In a linear mixed model, random effects introduce a dependence among observations which share any random effect in common

\[
\begin{aligned}
cov(y_i, y_j) &= cov_{\alpha}(E(y_i | \mathbf{\alpha}),E(y_j | \mathbf{\alpha})) + E_{\alpha}(cov(y_i, y_j | \mathbf{\alpha})) \\
&= cov(\mu_i, \mu_j) + E(0) \\
&= cov(g^{-1}(\mathbf{x}_i' \beta + \mathbf{z}_i' \mathbf{\alpha}), g^{-1}(\mathbf{x}'_j \beta + \mathbf{z}_j' \mathbf{\alpha}))
\end{aligned}
\]

\begin{itemize}
\tightlist
\item
  Important: conditioning to induce the covariability
\end{itemize}

Example:

Repeated measurements on the subjects.

Let \(y_{ij}\) be the j-th count taken on the \(i\)-th subject.

then, the model is \(y_{ij} | \mathbf{\alpha} \sim \text{indep } Pois(\mu_{ij})\). Here

\[
\log(\mu_{ij}) = \mathbf{x}_{ij}' \beta + \alpha_i 
\]

where \(\alpha_i \sim iid N(0,\sigma^2_{\alpha})\)

which is a log-link with a random patient effect.

\hypertarget{estimation-3}{%
\section{Estimation}\label{estimation-3}}

In linear mixed models, the marginal likelihood for \(\mathbf{y}\) is the integration of the random effects from the hierarchical formulation

\[
f(\mathbf{y}) = \int f(\mathbf{y}| \alpha) f(\alpha) d \alpha
\]

For linear mixed models, we assumed that the 2 component distributions were Gaussian with linear relationships, which implied the marginal distribution was also linear and Gaussian and allows us to solve this integral analytically.

On the other hand, GLMMs, the distribution for \(f(\mathbf{y} | \alpha)\) is not Gaussian in general, and for NLMMs, the functional form between the mean response and the random (and fixed) effects is nonlinear. In both cases, we can't perform the integral analytically, which means we have to solve it

\begin{itemize}
\item
  \protect\hyperlink{estimation-by-numerical-integration}{numerically} and/or
\item
  \protect\hyperlink{estimation-by-linearization}{linearize the inverse link function}.
\end{itemize}

\hypertarget{estimation-by-numerical-integration}{%
\subsection{Estimation by Numerical Integration}\label{estimation-by-numerical-integration}}

The marginal likelihood is

\[
L(\beta; \mathbf{y}) = \int f(\mathbf{y} | \alpha) f(\alpha) d \alpha
\]

Estimation fo the fixed effects requires \(\frac{\partial l}{\partial \beta}\), where \(l\) is the log-likelihood

One way to obtain the marginal inference is to numerically integrate out the random effects through

\begin{itemize}
\item
  numerical quadrature
\item
  Laplace approximation
\item
  Monte Carlo methods
\end{itemize}

When the dimension of \(\mathbf{\alpha}\) is relatively low, this is easy. But when the dimension of \(\alpha\) is high, additional approximation is required.

\hypertarget{estimation-by-linearization}{%
\subsection{Estimation by Linearization}\label{estimation-by-linearization}}

Idea: Linearized version of the response (known as working response, or pseudo-response) called \(\tilde{y}_i\) and then the conditional mean is

\[
E(\tilde{y}_i | \alpha) = \mathbf{x}_i' \beta + \mathbf{z}_i' \alpha
\]

and also estimate \(var(\tilde{y}_i | \alpha)\). then, apply \protect\hyperlink{linear-mixed-models}{Linear Mixed Models} estimation as usual.

The difference is only in how the linearization is done (i.e., how to expand \(f(\mathbf{x, \theta, \alpha})\) or the inverse link function

\hypertarget{penalized-quasi-likelihood}{%
\subsubsection{Penalized quasi-likelihood}\label{penalized-quasi-likelihood}}

(PQL)

This is the more popular method

\[
\tilde{y}_i^{(k)} = \hat{\eta}_i^{(k-1)} + ( y_i - \hat{\mu}_i^{(k-1)})\frac{d \eta}{d \mu}| \hat{\eta}_i^{(k-1)}
\]

where

\begin{itemize}
\item
  \(\eta_i = g(\mu_i)\) is the linear predictor
\item
  \(k\) = iteration of the optimization algorithm
\end{itemize}

The algorithm updates \(\tilde{y}_i\) after each linear mixed model fit using \(E(\tilde{y}_i | \alpha)\) and \(var(\tilde{y}_i | \alpha)\)

Comments:

\begin{itemize}
\item
  Easy to implement
\item
  Inference is only asymptotically correct due to the linearizaton
\item
  Biased estimates are likely for binomial response with small groups and worst for Bernoulli response. Similarly for Poisson models with small counts. \citep{faraway2016extending}
\item
  Hypothesis testing and confidence intervals also have problems.
\end{itemize}

\hypertarget{generalized-estimating-equations}{%
\subsubsection{Generalized Estimating Equations}\label{generalized-estimating-equations}}

(GEE)

Let a marginal generalized linear model for the mean of y as a function of the predictors, which means we linearize the mean response function and assume a dependent error structure

Example\\
Binary data:

\[
logit (E(\mathbf{y})) = \mathbf{X} \beta
\]

If we assume a ``working covariance matrix'', \(\mathbf{V}\) the the elements of \(\mathbf{y}\), then the maximum likelihood equations for estimating \(\beta\) is

\[
\mathbf{X'V^{-1}y} = \mathbf{X'V^{-1}} E(\mathbf{y})
\]

If \(\mathbf{V}\) is correct, then unbiased estimating equations

We typically define \(\mathbf{V} = \mathbf{I}\). Solutions to unbiased estimating equation give consistent estimators.

In practice, we assume a covariance structure, and then do a logistic regression, and calculate its large sample variance

Let \(y_{ij} , j = 1,..,n_i, i = 1,..,K\) be the j-th measurement on the \(i\)-th subject.

\[
\mathbf{y}_i = 
\left(
\begin{array}
{c}
y_{i1} \\
. \\
y_{in_i}
\end{array}
\right)
\]

with mean

\[
\mathbf{\mu}_i =
\left(
\begin{array}
{c}
\mu_{i1} \\
. \\
\mu_{in_i}
\end{array}
\right)
\]

and

\[
\mathbf{x}_{ij} = 
\left(
\begin{array}
{c}
X_{ij1} \\
. \\
X_{ijp}
\end{array}
\right)
\]

Let \(\mathbf{V}_i = cov(\mathbf{y}_i)\), then based on\citep{liang1986longitudinal} GEE estimates for \(\beta\) can be obtained from solving the equation:

\[
S(\beta) = \sum_{i=1}^K \frac{\partial \mathbf{\mu}_i'}{\partial \beta} \mathbf{V}^{-1}(\mathbf{y}_i - \mathbf{\mu}_i) = 0
\]

Let \(\mathbf{R}_i (\mathbf{c})\) be an \(n_i \times n_i\) ``working'' correlation matrix specified up to some parameters \(\mathbf{c}\). Then, \(\mathbf{V}_i = a(\phi) \mathbf{B}_i^{1/2}\mathbf{R}(\mathbf{c}) \mathbf{B}_i^{1/2}\), where \(\mathbf{B}_i\) is an \(n_i \times n_i\) diagonal matrix with \(V(\mu_{ij})\) on the j-th diagonal

If \(\mathbf{R}(\mathbf{c})\) is the true correlation matrix of \(\mathbf{y}_i\), then \(\mathbf{V}_i\) is the true covariance matrix

The working correlation matrix must be estimated iteratively by a fitting algorithm:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Compute the initial estimate of \(\beta\) (using GLM under the independence assumption)
\item
  Compute the working correlation matrix \(\mathbf{R}\) based upon studentized residuals
\item
  Compute the estimate covariance \(\hat{\mathbf{V}}_i\)
\item
  Update \(\beta\) according to

  \[
  \beta_{r+1} = \beta_r + (\sum_{i=1}^K \frac{\partial \mathbf{\mu}'_i}{\partial \beta} \hat{\mathbf{V}}_i^{-1} \frac{\partial \mathbf{\mu}_i}{\partial \beta})
  \]
\item
  Iterate until the algorithm converges
\end{enumerate}

Note: Inference based on likelihoods is not appropriate because this is not a likelihood estimator

\hypertarget{estimation-by-bayesian-hierarchical-models}{%
\subsection{Estimation by Bayesian Hierarchical Models}\label{estimation-by-bayesian-hierarchical-models}}

Bayesian Estimation

\[
f(\mathbf{\alpha}, \mathbf{\beta} | \mathbf{y}) \propto f(\mathbf{y} | \mathbf{\alpha}, \mathbf{\beta}) f(\mathbf{\alpha})f(\mathbf{\beta})
\]

Numerical techniques (e.g., MCMC) can be used to find posterior distribution. This method is best in terms of not having to make simplifying approximation and fully accounting for uncertainty in estimation and prediction, but it could be complex, time-consuming, and computationally intensive.

Implementation Issues:

\begin{itemize}
\item
  No valid joint distribution can be constructed from the given conditional model and random parameters
\item
  The mean/ variance relationship and the random effects lead to constraints on the marginal covariance model
\item
  Difficult to fit computationally
\end{itemize}

2 types of estimation approaches:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Approximate the objective function (marginal likelihood) through integral approximation

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    Laplace methods
  \item
    Quadrature methods
  \item
    Monte Carlo integration
  \end{enumerate}
\item
  Approximate the model (based on Taylor series linearization)
\end{enumerate}

Packages in R

\begin{itemize}
\item
  GLMM: \texttt{MASS:glmmPQL} \texttt{lme4::glmer} \texttt{glmmTMB}
\item
  NLMM: \texttt{nlme::nlme}; \texttt{lme4::nlmer} \texttt{brms::brm}
\item
  Bayesian: \texttt{MCMCglmm} ; \texttt{brms:brm}
\end{itemize}

Example: Non-Gaussian Repeated measurements

\begin{itemize}
\item
  When the data are Gaussian, then \protect\hyperlink{linear-mixed-models}{Linear Mixed Models}
\item
  When the data are non-Gaussian, then \protect\hyperlink{nonlinear-and-generalized-linear-mixed-models}{Nonlinear and Generalized Linear Mixed Models}
\end{itemize}

\hypertarget{application-6}{%
\section{Application}\label{application-6}}

\hypertarget{binomial-cbpp-data}{%
\subsection{Binomial (CBPP Data)}\label{binomial-cbpp-data}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(cbpp,}\AttributeTok{package =} \StringTok{"lme4"}\NormalTok{)}
\FunctionTok{head}\NormalTok{(cbpp)}
\CommentTok{\#\textgreater{}   herd incidence size period}
\CommentTok{\#\textgreater{} 1    1         2   14      1}
\CommentTok{\#\textgreater{} 2    1         3   12      2}
\CommentTok{\#\textgreater{} 3    1         4    9      3}
\CommentTok{\#\textgreater{} 4    1         0    5      4}
\CommentTok{\#\textgreater{} 5    2         3   22      1}
\CommentTok{\#\textgreater{} 6    2         1   18      2}
\end{Highlighting}
\end{Shaded}

PQL

Pro:

\begin{itemize}
\item
  Linearizes the response to have a pseudo-response as the mean response (like LMM)
\item
  computationally efficient
\end{itemize}

Cons:

\begin{itemize}
\item
  biased for binary, Poisson data with small counts
\item
  random effects have to be interpreted on the link scale
\item
  can't interpret AIC/BIC value
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(MASS)}
\NormalTok{pql\_cbpp }\OtherTok{\textless{}{-}}
    \FunctionTok{glmmPQL}\NormalTok{(}
        \FunctionTok{cbind}\NormalTok{(incidence, size }\SpecialCharTok{{-}}\NormalTok{ incidence) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ period,}
        \AttributeTok{random  =} \SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{|}\NormalTok{ herd,}
        \AttributeTok{data    =}\NormalTok{ cbpp,}
        \AttributeTok{family  =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"logit"}\NormalTok{),}
        \AttributeTok{verbose =}\NormalTok{ F}
\NormalTok{    )}
\FunctionTok{summary}\NormalTok{(pql\_cbpp)}
\CommentTok{\#\textgreater{} Linear mixed{-}effects model fit by maximum likelihood}
\CommentTok{\#\textgreater{}   Data: cbpp }
\CommentTok{\#\textgreater{}   AIC BIC logLik}
\CommentTok{\#\textgreater{}    NA  NA     NA}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Random effects:}
\CommentTok{\#\textgreater{}  Formula: \textasciitilde{}1 | herd}
\CommentTok{\#\textgreater{}         (Intercept) Residual}
\CommentTok{\#\textgreater{} StdDev:   0.5563535 1.184527}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Variance function:}
\CommentTok{\#\textgreater{}  Structure: fixed weights}
\CommentTok{\#\textgreater{}  Formula: \textasciitilde{}invwt }
\CommentTok{\#\textgreater{} Fixed effects:  cbind(incidence, size {-} incidence) \textasciitilde{} period }
\CommentTok{\#\textgreater{}                 Value Std.Error DF   t{-}value p{-}value}
\CommentTok{\#\textgreater{} (Intercept) {-}1.327364 0.2390194 38 {-}5.553372  0.0000}
\CommentTok{\#\textgreater{} period2     {-}1.016126 0.3684079 38 {-}2.758156  0.0089}
\CommentTok{\#\textgreater{} period3     {-}1.149984 0.3937029 38 {-}2.920944  0.0058}
\CommentTok{\#\textgreater{} period4     {-}1.605217 0.5178388 38 {-}3.099839  0.0036}
\CommentTok{\#\textgreater{}  Correlation: }
\CommentTok{\#\textgreater{}         (Intr) perid2 perid3}
\CommentTok{\#\textgreater{} period2 {-}0.399              }
\CommentTok{\#\textgreater{} period3 {-}0.373  0.260       }
\CommentTok{\#\textgreater{} period4 {-}0.282  0.196  0.182}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Standardized Within{-}Group Residuals:}
\CommentTok{\#\textgreater{}        Min         Q1        Med         Q3        Max }
\CommentTok{\#\textgreater{} {-}2.0591168 {-}0.6493095 {-}0.2747620  0.5170492  2.6187632 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Observations: 56}
\CommentTok{\#\textgreater{} Number of Groups: 15}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{exp}\NormalTok{(}\FloatTok{0.556}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 1.743684}
\end{Highlighting}
\end{Shaded}

is how the herd specific outcome odds varies.

We can interpret the fixed effect coefficients just like in GLM. Because we use logit link function here, we can say that the log odds of the probability of having a case in period 2 is -1.016 less than period 1 (baseline).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(pql\_cbpp)}\SpecialCharTok{$}\NormalTok{tTable}
\CommentTok{\#\textgreater{}                 Value Std.Error DF   t{-}value      p{-}value}
\CommentTok{\#\textgreater{} (Intercept) {-}1.327364 0.2390194 38 {-}5.553372 2.333216e{-}06}
\CommentTok{\#\textgreater{} period2     {-}1.016126 0.3684079 38 {-}2.758156 8.888179e{-}03}
\CommentTok{\#\textgreater{} period3     {-}1.149984 0.3937029 38 {-}2.920944 5.843007e{-}03}
\CommentTok{\#\textgreater{} period4     {-}1.605217 0.5178388 38 {-}3.099839 3.637000e{-}03}
\end{Highlighting}
\end{Shaded}

Numerical Integration

Pro:

\begin{itemize}
\tightlist
\item
  more accurate
\end{itemize}

Con:

\begin{itemize}
\item
  computationally expensive
\item
  won't work for complex models.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(lme4)}
\NormalTok{numint\_cbpp }\OtherTok{\textless{}{-}}
    \FunctionTok{glmer}\NormalTok{(}
        \FunctionTok{cbind}\NormalTok{(incidence, size }\SpecialCharTok{{-}}\NormalTok{ incidence) }\SpecialCharTok{\textasciitilde{}} 
\NormalTok{            period }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ herd),}
        \AttributeTok{data =}\NormalTok{ cbpp,}
        \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"logit"}\NormalTok{)}
\NormalTok{    )}
\FunctionTok{summary}\NormalTok{(numint\_cbpp)}
\CommentTok{\#\textgreater{} Generalized linear mixed model fit by maximum likelihood (Laplace}
\CommentTok{\#\textgreater{}   Approximation) [glmerMod]}
\CommentTok{\#\textgreater{}  Family: binomial  ( logit )}
\CommentTok{\#\textgreater{} Formula: cbind(incidence, size {-} incidence) \textasciitilde{} period + (1 | herd)}
\CommentTok{\#\textgreater{}    Data: cbpp}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}      AIC      BIC   logLik deviance df.resid }
\CommentTok{\#\textgreater{}    194.1    204.2    {-}92.0    184.1       51 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Scaled residuals: }
\CommentTok{\#\textgreater{}     Min      1Q  Median      3Q     Max }
\CommentTok{\#\textgreater{} {-}2.3816 {-}0.7889 {-}0.2026  0.5142  2.8791 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Random effects:}
\CommentTok{\#\textgreater{}  Groups Name        Variance Std.Dev.}
\CommentTok{\#\textgreater{}  herd   (Intercept) 0.4123   0.6421  }
\CommentTok{\#\textgreater{} Number of obs: 56, groups:  herd, 15}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Fixed effects:}
\CommentTok{\#\textgreater{}             Estimate Std. Error z value Pr(\textgreater{}|z|)    }
\CommentTok{\#\textgreater{} (Intercept)  {-}1.3983     0.2312  {-}6.048 1.47e{-}09 ***}
\CommentTok{\#\textgreater{} period2      {-}0.9919     0.3032  {-}3.272 0.001068 ** }
\CommentTok{\#\textgreater{} period3      {-}1.1282     0.3228  {-}3.495 0.000474 ***}
\CommentTok{\#\textgreater{} period4      {-}1.5797     0.4220  {-}3.743 0.000182 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Correlation of Fixed Effects:}
\CommentTok{\#\textgreater{}         (Intr) perid2 perid3}
\CommentTok{\#\textgreater{} period2 {-}0.363              }
\CommentTok{\#\textgreater{} period3 {-}0.340  0.280       }
\CommentTok{\#\textgreater{} period4 {-}0.260  0.213  0.198}
\end{Highlighting}
\end{Shaded}

For small data set, the difference between two approaches are minimal

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rbenchmark)}
\FunctionTok{benchmark}\NormalTok{(}
    \StringTok{"MASS"} \OtherTok{=}\NormalTok{ \{}
\NormalTok{        pql\_cbpp }\OtherTok{\textless{}{-}}
            \FunctionTok{glmmPQL}\NormalTok{(}
                \FunctionTok{cbind}\NormalTok{(incidence, size }\SpecialCharTok{{-}}\NormalTok{ incidence) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ period,}
                \AttributeTok{random =} \SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{|}\NormalTok{ herd,}
                \AttributeTok{data =}\NormalTok{ cbpp,}
                \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"logit"}\NormalTok{),}
                \AttributeTok{verbose =}\NormalTok{ F}
\NormalTok{            )}
\NormalTok{    \},}
    \StringTok{"lme4"} \OtherTok{=}\NormalTok{ \{}
        \FunctionTok{glmer}\NormalTok{(}
            \FunctionTok{cbind}\NormalTok{(incidence, size }\SpecialCharTok{{-}}\NormalTok{ incidence) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ period }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ herd),}
            \AttributeTok{data =}\NormalTok{ cbpp,}
            \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"logit"}\NormalTok{)}
\NormalTok{        )}
\NormalTok{    \},}
    \AttributeTok{replications =} \DecValTok{50}\NormalTok{,}
    \AttributeTok{columns =} \FunctionTok{c}\NormalTok{(}\StringTok{"test"}\NormalTok{, }\StringTok{"replications"}\NormalTok{, }\StringTok{"elapsed"}\NormalTok{, }\StringTok{"relative"}\NormalTok{),}
    \AttributeTok{order =} \StringTok{"relative"}
\NormalTok{)}
\CommentTok{\#\textgreater{}   test replications elapsed relative}
\CommentTok{\#\textgreater{} 1 MASS           50    3.69    1.000}
\CommentTok{\#\textgreater{} 2 lme4           50    7.58    2.054}
\end{Highlighting}
\end{Shaded}

In numerical integration, we can set \texttt{nAGQ\ \textgreater{}\ 1} to switch the method of likelihood evaluation, which might increase accuracy

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(lme4)}
\NormalTok{numint\_cbpp\_GH }\OtherTok{\textless{}{-}}
    \FunctionTok{glmer}\NormalTok{(}
        \FunctionTok{cbind}\NormalTok{(incidence, size }\SpecialCharTok{{-}}\NormalTok{ incidence) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ period }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ herd),}
        \AttributeTok{data =}\NormalTok{ cbpp,}
        \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"logit"}\NormalTok{),}
        \AttributeTok{nAGQ =} \DecValTok{20}
\NormalTok{    )}
\FunctionTok{summary}\NormalTok{(numint\_cbpp\_GH)}\SpecialCharTok{$}\NormalTok{coefficients[, }\DecValTok{1}\NormalTok{] }\SpecialCharTok{{-}} 
    \FunctionTok{summary}\NormalTok{(numint\_cbpp)}\SpecialCharTok{$}\NormalTok{coefficients[, }\DecValTok{1}\NormalTok{]}
\CommentTok{\#\textgreater{}   (Intercept)       period2       period3       period4 }
\CommentTok{\#\textgreater{} {-}0.0008808634  0.0005160912  0.0004066218  0.0002644629}
\end{Highlighting}
\end{Shaded}

Bayesian approach to GLMMs

\begin{itemize}
\item
  assume the fixed effects parameters have distribution
\item
  can handle models with intractable result under traditional methods
\item
  computationally expensive
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(MCMCglmm)}
\NormalTok{Bayes\_cbpp }\OtherTok{\textless{}{-}}
    \FunctionTok{MCMCglmm}\NormalTok{(}
        \FunctionTok{cbind}\NormalTok{(incidence, size }\SpecialCharTok{{-}}\NormalTok{ incidence) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ period,}
        \AttributeTok{random  =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ herd,}
        \AttributeTok{data    =}\NormalTok{ cbpp,}
        \AttributeTok{family  =} \StringTok{"multinomial2"}\NormalTok{,}
        \AttributeTok{verbose =} \ConstantTok{FALSE}
\NormalTok{    )}
\FunctionTok{summary}\NormalTok{(Bayes\_cbpp)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Iterations = 3001:12991}
\CommentTok{\#\textgreater{}  Thinning interval  = 10}
\CommentTok{\#\textgreater{}  Sample size  = 1000 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  DIC: 538.3852 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  G{-}structure:  \textasciitilde{}herd}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}      post.mean  l{-}95\% CI u{-}95\% CI eff.samp}
\CommentTok{\#\textgreater{} herd   0.02602 1.164e{-}16   0.1944    53.97}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  R{-}structure:  \textasciitilde{}units}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}       post.mean l{-}95\% CI u{-}95\% CI eff.samp}
\CommentTok{\#\textgreater{} units     1.115    0.158    2.163    304.9}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Location effects: cbind(incidence, size {-} incidence) \textasciitilde{} period }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}             post.mean l{-}95\% CI u{-}95\% CI eff.samp  pMCMC    }
\CommentTok{\#\textgreater{} (Intercept)   {-}1.5298  {-}2.1566  {-}0.8140    875.6 \textless{}0.001 ***}
\CommentTok{\#\textgreater{} period2       {-}1.2500  {-}2.3851  {-}0.2880    780.4  0.020 *  }
\CommentTok{\#\textgreater{} period3       {-}1.4018  {-}2.5983  {-}0.3816    800.8  0.006 ** }
\CommentTok{\#\textgreater{} period4       {-}1.9360  {-}3.1710  {-}0.7034    432.9  0.004 ** }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \texttt{MCMCglmm} fits a residual variance component (useful with dispersion issues)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# explains less variability}
\FunctionTok{apply}\NormalTok{(Bayes\_cbpp}\SpecialCharTok{$}\NormalTok{VCV,}\DecValTok{2}\NormalTok{,sd)}
\CommentTok{\#\textgreater{}      herd     units }
\CommentTok{\#\textgreater{} 0.1074491 0.5582551}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(Bayes\_cbpp)}\SpecialCharTok{$}\NormalTok{solutions}
\CommentTok{\#\textgreater{}             post.mean  l{-}95\% CI   u{-}95\% CI eff.samp pMCMC}
\CommentTok{\#\textgreater{} (Intercept) {-}1.529801 {-}2.156645 {-}0.8139842 875.5644 0.001}
\CommentTok{\#\textgreater{} period2     {-}1.249981 {-}2.385050 {-}0.2879657 780.4308 0.020}
\CommentTok{\#\textgreater{} period3     {-}1.401787 {-}2.598287 {-}0.3815690 800.8027 0.006}
\CommentTok{\#\textgreater{} period4     {-}1.936025 {-}3.171037 {-}0.7033584 432.9481 0.004}
\end{Highlighting}
\end{Shaded}

interpret Bayesian ``credible intervals'' similarly to confidence intervals

Make sure you make post-hoc diagnoses

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(lattice)}
\FunctionTok{xyplot}\NormalTok{(}\FunctionTok{as.mcmc}\NormalTok{(Bayes\_cbpp}\SpecialCharTok{$}\NormalTok{Sol), }\AttributeTok{layout =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{09-nonlinear_generalized_linear_mixed_files/figure-latex/unnamed-chunk-11-1} \end{center}

There is no trend, well-mixed

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{xyplot}\NormalTok{(}\FunctionTok{as.mcmc}\NormalTok{(Bayes\_cbpp}\SpecialCharTok{$}\NormalTok{VCV),}\AttributeTok{layout=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{09-nonlinear_generalized_linear_mixed_files/figure-latex/unnamed-chunk-12-1} \end{center}

For the herd variable, a lot of them are 0, which suggests problem. To fix the instability in the herd effect sampling, we can either

\begin{itemize}
\item
  modify the prior distribution on the herd variation
\item
  increases the number of iteration
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(MCMCglmm)}
\NormalTok{Bayes\_cbpp2 }\OtherTok{\textless{}{-}}
    \FunctionTok{MCMCglmm}\NormalTok{(}
        \FunctionTok{cbind}\NormalTok{(incidence, size }\SpecialCharTok{{-}}\NormalTok{ incidence) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ period,}
        \AttributeTok{random =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ herd,}
        \AttributeTok{data   =}\NormalTok{ cbpp,}
        \AttributeTok{family =} \StringTok{"multinomial2"}\NormalTok{,}
        \AttributeTok{nitt   =} \DecValTok{20000}\NormalTok{,}
        \AttributeTok{burnin =} \DecValTok{10000}\NormalTok{,}
        \AttributeTok{prior  =} \FunctionTok{list}\NormalTok{(}\AttributeTok{G =} \FunctionTok{list}\NormalTok{(}\FunctionTok{list}\NormalTok{(}
            \AttributeTok{V  =} \DecValTok{1}\NormalTok{, }\AttributeTok{nu =}\NormalTok{ .}\DecValTok{1}
\NormalTok{        ))),}
        \AttributeTok{verbose =} \ConstantTok{FALSE}
\NormalTok{    )}
\FunctionTok{xyplot}\NormalTok{(}\FunctionTok{as.mcmc}\NormalTok{(Bayes\_cbpp2}\SpecialCharTok{$}\NormalTok{VCV), }\AttributeTok{layout =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{09-nonlinear_generalized_linear_mixed_files/figure-latex/unnamed-chunk-13-1} \end{center}

To change the shape of priors, in \texttt{MCMCglmm} use:

\begin{itemize}
\item
  \texttt{V} controls for the location of the distribution (default = 1)
\item
  \texttt{nu} controls for the concentration around V (default = 0)
\end{itemize}

\hypertarget{count-owl-data}{%
\subsection{Count (Owl Data)}\label{count-owl-data}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(glmmTMB)}
\FunctionTok{library}\NormalTok{(dplyr)}
\FunctionTok{data}\NormalTok{(Owls, }\AttributeTok{package =} \StringTok{"glmmTMB"}\NormalTok{)}
\NormalTok{Owls }\OtherTok{\textless{}{-}}\NormalTok{ Owls }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{rename}\NormalTok{(}\AttributeTok{Ncalls =}\NormalTok{ SiblingNegotiation)}
\end{Highlighting}
\end{Shaded}

In a typical Poisson model, \(\lambda\) (Poisson mean), is model as \(\log(\lambda) = \mathbf{x'\beta}\) But if the response is the rate (e.g., counts per BroodSize), we could model it as \(\log(\lambda / b) = \mathbf{x'\beta}\) , equivalently \(\log(\lambda) = \log(b) + \mathbf{x'\beta}\) where \(b\) is BroodSize. Hence, we ``offset'' the mean by the log of this variable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{owls\_glmer }\OtherTok{\textless{}{-}}
    \FunctionTok{glmer}\NormalTok{(}
\NormalTok{        Ncalls }\SpecialCharTok{\textasciitilde{}} \FunctionTok{offset}\NormalTok{(}\FunctionTok{log}\NormalTok{(BroodSize)) }
        \SpecialCharTok{+}\NormalTok{ FoodTreatment }\SpecialCharTok{*}\NormalTok{ SexParent }\SpecialCharTok{+}
\NormalTok{            (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ Nest),}
        \AttributeTok{family =}\NormalTok{ poisson,}
        \AttributeTok{data =}\NormalTok{ Owls}
\NormalTok{    )}
\FunctionTok{summary}\NormalTok{(owls\_glmer)}
\CommentTok{\#\textgreater{} Generalized linear mixed model fit by maximum likelihood (Laplace}
\CommentTok{\#\textgreater{}   Approximation) [glmerMod]}
\CommentTok{\#\textgreater{}  Family: poisson  ( log )}
\CommentTok{\#\textgreater{} Formula: Ncalls \textasciitilde{} offset(log(BroodSize)) + FoodTreatment * SexParent +  }
\CommentTok{\#\textgreater{}     (1 | Nest)}
\CommentTok{\#\textgreater{}    Data: Owls}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}      AIC      BIC   logLik deviance df.resid }
\CommentTok{\#\textgreater{}   5212.8   5234.8  {-}2601.4   5202.8      594 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Scaled residuals: }
\CommentTok{\#\textgreater{}     Min      1Q  Median      3Q     Max }
\CommentTok{\#\textgreater{} {-}3.5529 {-}1.7971 {-}0.6842  1.2689 11.4312 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Random effects:}
\CommentTok{\#\textgreater{}  Groups Name        Variance Std.Dev.}
\CommentTok{\#\textgreater{}  Nest   (Intercept) 0.2063   0.4542  }
\CommentTok{\#\textgreater{} Number of obs: 599, groups:  Nest, 27}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Fixed effects:}
\CommentTok{\#\textgreater{}                                     Estimate Std. Error z value Pr(\textgreater{}|z|)    }
\CommentTok{\#\textgreater{} (Intercept)                          0.65585    0.09567   6.855 7.12e{-}12 ***}
\CommentTok{\#\textgreater{} FoodTreatmentSatiated               {-}0.65612    0.05606 {-}11.705  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} SexParentMale                       {-}0.03705    0.04501  {-}0.823   0.4104    }
\CommentTok{\#\textgreater{} FoodTreatmentSatiated:SexParentMale  0.13135    0.07036   1.867   0.0619 .  }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Correlation of Fixed Effects:}
\CommentTok{\#\textgreater{}             (Intr) FdTrtS SxPrnM}
\CommentTok{\#\textgreater{} FdTrtmntStt {-}0.225              }
\CommentTok{\#\textgreater{} SexParentMl {-}0.292  0.491       }
\CommentTok{\#\textgreater{} FdTrtmS:SPM  0.170 {-}0.768 {-}0.605}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  nest explains a relatively large proportion of the variability (its standard deviation is larger than some coefficients)
\item
  the model fit isn't great (deviance of 5202 on 594 df)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Negative binomial model}
\NormalTok{owls\_glmerNB }\OtherTok{\textless{}{-}}
    \FunctionTok{glmer.nb}\NormalTok{(Ncalls }\SpecialCharTok{\textasciitilde{}} \FunctionTok{offset}\NormalTok{(}\FunctionTok{log}\NormalTok{(BroodSize)) }
             \SpecialCharTok{+}\NormalTok{ FoodTreatment }\SpecialCharTok{*}\NormalTok{ SexParent}
             \SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ Nest), }\AttributeTok{data =}\NormalTok{ Owls)}

\FunctionTok{c}\NormalTok{(}\AttributeTok{Deviance =} \FunctionTok{round}\NormalTok{(}\FunctionTok{summary}\NormalTok{(owls\_glmerNB)}\SpecialCharTok{$}\NormalTok{AICtab[}\StringTok{"deviance"}\NormalTok{], }\DecValTok{3}\NormalTok{),}
  \AttributeTok{df =} \FunctionTok{summary}\NormalTok{(owls\_glmerNB)}\SpecialCharTok{$}\NormalTok{AICtab[}\StringTok{"df.resid"}\NormalTok{])}
\CommentTok{\#\textgreater{} Deviance.deviance       df.df.resid }
\CommentTok{\#\textgreater{}          3483.616           593.000}
\end{Highlighting}
\end{Shaded}

There is an improvement using negative binomial considering over-dispersion

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(Owls}\SpecialCharTok{$}\NormalTok{Ncalls,}\AttributeTok{breaks=}\DecValTok{30}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{09-nonlinear_generalized_linear_mixed_files/figure-latex/unnamed-chunk-17-1} \end{center}

To account for too many 0s in these data, we can use zero-inflated Poisson (ZIP) model.

\begin{itemize}
\tightlist
\item
  \texttt{glmmTMB} can handle ZIP GLMMs since it adds automatic differentiation to existing estimation strategies.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(glmmTMB)}
\NormalTok{owls\_glmm }\OtherTok{\textless{}{-}}
    \FunctionTok{glmmTMB}\NormalTok{(}
\NormalTok{        Ncalls }\SpecialCharTok{\textasciitilde{}}\NormalTok{ FoodTreatment }\SpecialCharTok{*}\NormalTok{ SexParent }\SpecialCharTok{+} \FunctionTok{offset}\NormalTok{(}\FunctionTok{log}\NormalTok{(BroodSize)) }\SpecialCharTok{+}
\NormalTok{            (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ Nest),}
        \AttributeTok{ziformula =}  \SpecialCharTok{\textasciitilde{}} \DecValTok{0}\NormalTok{,}
        \AttributeTok{family =} \FunctionTok{nbinom2}\NormalTok{(}\AttributeTok{link =} \StringTok{"log"}\NormalTok{),}
        \AttributeTok{data =}\NormalTok{ Owls}
\NormalTok{    )}
\NormalTok{owls\_glmm\_zi }\OtherTok{\textless{}{-}}
    \FunctionTok{glmmTMB}\NormalTok{(}
\NormalTok{        Ncalls }\SpecialCharTok{\textasciitilde{}}\NormalTok{ FoodTreatment }\SpecialCharTok{*}\NormalTok{ SexParent }\SpecialCharTok{+} \FunctionTok{offset}\NormalTok{(}\FunctionTok{log}\NormalTok{(BroodSize)) }\SpecialCharTok{+}
\NormalTok{            (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ Nest),}
        \AttributeTok{ziformula =}  \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{,}
        \AttributeTok{family =} \FunctionTok{nbinom2}\NormalTok{(}\AttributeTok{link =} \StringTok{"log"}\NormalTok{),}
        \AttributeTok{data =}\NormalTok{ Owls}
\NormalTok{    )}
\CommentTok{\# Scale Arrival time to use as a covariate for zero{-}inflation parameter}
\NormalTok{Owls}\SpecialCharTok{$}\NormalTok{ArrivalTime }\OtherTok{\textless{}{-}} \FunctionTok{scale}\NormalTok{(Owls}\SpecialCharTok{$}\NormalTok{ArrivalTime)}
\NormalTok{owls\_glmm\_zi\_cov }\OtherTok{\textless{}{-}} \FunctionTok{glmmTMB}\NormalTok{(}
\NormalTok{    Ncalls }\SpecialCharTok{\textasciitilde{}}\NormalTok{ FoodTreatment }\SpecialCharTok{*}\NormalTok{ SexParent }\SpecialCharTok{+}
        \FunctionTok{offset}\NormalTok{(}\FunctionTok{log}\NormalTok{(BroodSize)) }\SpecialCharTok{+}
\NormalTok{        (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ Nest),}
    \AttributeTok{ziformula =}  \SpecialCharTok{\textasciitilde{}}\NormalTok{ ArrivalTime,}
    \AttributeTok{family =} \FunctionTok{nbinom2}\NormalTok{(}\AttributeTok{link =} \StringTok{"log"}\NormalTok{),}
    \AttributeTok{data =}\NormalTok{ Owls}
\NormalTok{)}
\FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{anova}\NormalTok{(owls\_glmm, owls\_glmm\_zi))}
\CommentTok{\#\textgreater{}              Df      AIC      BIC    logLik deviance    Chisq Chi Df}
\CommentTok{\#\textgreater{} owls\_glmm     6 3495.610 3521.981 {-}1741.805 3483.610       NA     NA}
\CommentTok{\#\textgreater{} owls\_glmm\_zi  7 3431.646 3462.413 {-}1708.823 3417.646 65.96373      1}
\CommentTok{\#\textgreater{}                Pr(\textgreater{}Chisq)}
\CommentTok{\#\textgreater{} owls\_glmm              NA}
\CommentTok{\#\textgreater{} owls\_glmm\_zi 4.592983e{-}16}
\FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{anova}\NormalTok{(owls\_glmm\_zi, owls\_glmm\_zi\_cov))}
\CommentTok{\#\textgreater{}                  Df      AIC      BIC    logLik deviance    Chisq Chi Df}
\CommentTok{\#\textgreater{} owls\_glmm\_zi      7 3431.646 3462.413 {-}1708.823 3417.646       NA     NA}
\CommentTok{\#\textgreater{} owls\_glmm\_zi\_cov  8 3422.532 3457.694 {-}1703.266 3406.532 11.11411      1}
\CommentTok{\#\textgreater{}                    Pr(\textgreater{}Chisq)}
\CommentTok{\#\textgreater{} owls\_glmm\_zi               NA}
\CommentTok{\#\textgreater{} owls\_glmm\_zi\_cov 0.0008567362}
\FunctionTok{summary}\NormalTok{(owls\_glmm\_zi\_cov)}
\CommentTok{\#\textgreater{}  Family: nbinom2  ( log )}
\CommentTok{\#\textgreater{} Formula:          }
\CommentTok{\#\textgreater{} Ncalls \textasciitilde{} FoodTreatment * SexParent + offset(log(BroodSize)) +      (1 | Nest)}
\CommentTok{\#\textgreater{} Zero inflation:          \textasciitilde{}ArrivalTime}
\CommentTok{\#\textgreater{} Data: Owls}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}      AIC      BIC   logLik deviance df.resid }
\CommentTok{\#\textgreater{}   3422.5   3457.7  {-}1703.3   3406.5      591 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Random effects:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Conditional model:}
\CommentTok{\#\textgreater{}  Groups Name        Variance Std.Dev.}
\CommentTok{\#\textgreater{}  Nest   (Intercept) 0.07487  0.2736  }
\CommentTok{\#\textgreater{} Number of obs: 599, groups:  Nest, 27}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Dispersion parameter for nbinom2 family (): 2.22 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Conditional model:}
\CommentTok{\#\textgreater{}                                     Estimate Std. Error z value Pr(\textgreater{}|z|)    }
\CommentTok{\#\textgreater{} (Intercept)                          0.84778    0.09961   8.511  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} FoodTreatmentSatiated               {-}0.39529    0.13742  {-}2.877  0.00402 ** }
\CommentTok{\#\textgreater{} SexParentMale                       {-}0.07025    0.10435  {-}0.673  0.50079    }
\CommentTok{\#\textgreater{} FoodTreatmentSatiated:SexParentMale  0.12388    0.16449   0.753  0.45138    }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Zero{-}inflation model:}
\CommentTok{\#\textgreater{}             Estimate Std. Error z value Pr(\textgreater{}|z|)    }
\CommentTok{\#\textgreater{} (Intercept)  {-}1.3018     0.1261  {-}10.32  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} ArrivalTime   0.3545     0.1074    3.30 0.000966 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

We can see ZIP GLMM with an arrival time covariate on the zero is best.

\begin{itemize}
\item
  arrival time has a positive effect on observing a nonzero number of calls
\item
  interactions are non significant, the food treatment is significant (fewer calls after eating)
\item
  nest variability is large in magnitude (without this, the parameter estimates change)
\end{itemize}

\hypertarget{binomial-1}{%
\subsection{Binomial}\label{binomial-1}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(agridat)}
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(lme4)}
\FunctionTok{library}\NormalTok{(spaMM)}
\FunctionTok{data}\NormalTok{(gotway.hessianfly)}
\NormalTok{dat }\OtherTok{\textless{}{-}}\NormalTok{ gotway.hessianfly}
\NormalTok{dat}\SpecialCharTok{$}\NormalTok{prop }\OtherTok{\textless{}{-}}\NormalTok{ dat}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{/}\NormalTok{ dat}\SpecialCharTok{$}\NormalTok{n}

\FunctionTok{ggplot}\NormalTok{(dat, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ lat, }\AttributeTok{y =}\NormalTok{ long, }\AttributeTok{fill =}\NormalTok{ prop)) }\SpecialCharTok{+}
    \FunctionTok{geom\_tile}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{scale\_fill\_gradient}\NormalTok{(}\AttributeTok{low =} \StringTok{\textquotesingle{}white\textquotesingle{}}\NormalTok{, }\AttributeTok{high =} \StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_text}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =}\NormalTok{ gen, }\AttributeTok{color =}\NormalTok{ block)) }\SpecialCharTok{+}
    \FunctionTok{ggtitle}\NormalTok{(}\StringTok{\textquotesingle{}Gotway Hessian Fly\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{09-nonlinear_generalized_linear_mixed_files/figure-latex/unnamed-chunk-19-1} \end{center}

\begin{itemize}
\item
  Fixed effects (\(\beta\)) = genotype
\item
  Random effects (\(\alpha\)) = block
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flymodel }\OtherTok{\textless{}{-}}
    \FunctionTok{glmer}\NormalTok{(}
        \FunctionTok{cbind}\NormalTok{(y, n }\SpecialCharTok{{-}}\NormalTok{ y) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gen }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ block),}
        \AttributeTok{data   =}\NormalTok{ dat,}
        \AttributeTok{family =}\NormalTok{ binomial,}
        \AttributeTok{nAGQ   =} \DecValTok{5}
\NormalTok{    )}
\FunctionTok{summary}\NormalTok{(flymodel)}
\CommentTok{\#\textgreater{} Generalized linear mixed model fit by maximum likelihood (Adaptive}
\CommentTok{\#\textgreater{}   Gauss{-}Hermite Quadrature, nAGQ = 5) [glmerMod]}
\CommentTok{\#\textgreater{}  Family: binomial  ( logit )}
\CommentTok{\#\textgreater{} Formula: cbind(y, n {-} y) \textasciitilde{} gen + (1 | block)}
\CommentTok{\#\textgreater{}    Data: dat}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}      AIC      BIC   logLik deviance df.resid }
\CommentTok{\#\textgreater{}    162.2    198.9    {-}64.1    128.2       47 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Scaled residuals: }
\CommentTok{\#\textgreater{}      Min       1Q   Median       3Q      Max }
\CommentTok{\#\textgreater{} {-}2.38644 {-}1.01188  0.09631  1.03468  2.75479 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Random effects:}
\CommentTok{\#\textgreater{}  Groups Name        Variance Std.Dev.}
\CommentTok{\#\textgreater{}  block  (Intercept) 0.001022 0.03196 }
\CommentTok{\#\textgreater{} Number of obs: 64, groups:  block, 4}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Fixed effects:}
\CommentTok{\#\textgreater{}             Estimate Std. Error z value Pr(\textgreater{}|z|)    }
\CommentTok{\#\textgreater{} (Intercept)   1.5035     0.3914   3.841 0.000122 ***}
\CommentTok{\#\textgreater{} genG02       {-}0.1939     0.5302  {-}0.366 0.714644    }
\CommentTok{\#\textgreater{} genG03       {-}0.5408     0.5103  {-}1.060 0.289260    }
\CommentTok{\#\textgreater{} genG04       {-}1.4342     0.4714  {-}3.043 0.002346 ** }
\CommentTok{\#\textgreater{} genG05       {-}0.2037     0.5429  {-}0.375 0.707486    }
\CommentTok{\#\textgreater{} genG06       {-}0.9783     0.5046  {-}1.939 0.052533 .  }
\CommentTok{\#\textgreater{} genG07       {-}0.6041     0.5111  {-}1.182 0.237235    }
\CommentTok{\#\textgreater{} genG08       {-}1.6774     0.4907  {-}3.418 0.000630 ***}
\CommentTok{\#\textgreater{} genG09       {-}1.3984     0.4725  {-}2.960 0.003078 ** }
\CommentTok{\#\textgreater{} genG10       {-}0.6817     0.5333  {-}1.278 0.201181    }
\CommentTok{\#\textgreater{} genG11       {-}1.4630     0.4843  {-}3.021 0.002522 ** }
\CommentTok{\#\textgreater{} genG12       {-}1.4591     0.4918  {-}2.967 0.003010 ** }
\CommentTok{\#\textgreater{} genG13       {-}3.5528     0.6600  {-}5.383 7.31e{-}08 ***}
\CommentTok{\#\textgreater{} genG14       {-}2.5073     0.5264  {-}4.763 1.90e{-}06 ***}
\CommentTok{\#\textgreater{} genG15       {-}2.0872     0.4851  {-}4.302 1.69e{-}05 ***}
\CommentTok{\#\textgreater{} genG16       {-}2.9697     0.5383  {-}5.517 3.46e{-}08 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

Equivalently, we can use \texttt{MCMCglmm} , for a Bayesian approach

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(coda)}
\NormalTok{Bayes\_flymodel }\OtherTok{\textless{}{-}} \FunctionTok{MCMCglmm}\NormalTok{(}
    \FunctionTok{cbind}\NormalTok{(y, n }\SpecialCharTok{{-}}\NormalTok{ y) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gen ,}
    \AttributeTok{random  =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ block,}
    \AttributeTok{data    =}\NormalTok{ dat,}
    \AttributeTok{family  =} \StringTok{"multinomial2"}\NormalTok{,}
    \AttributeTok{verbose =} \ConstantTok{FALSE}
\NormalTok{)}
\FunctionTok{plot}\NormalTok{(Bayes\_flymodel}\SpecialCharTok{$}\NormalTok{Sol[, }\DecValTok{1}\NormalTok{], }\AttributeTok{main =} \FunctionTok{dimnames}\NormalTok{(Bayes\_flymodel}\SpecialCharTok{$}\NormalTok{Sol)[[}\DecValTok{2}\NormalTok{]][}\DecValTok{1}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{09-nonlinear_generalized_linear_mixed_files/figure-latex/unnamed-chunk-21-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{autocorr.plot}\NormalTok{(Bayes\_flymodel}\SpecialCharTok{$}\NormalTok{Sol[, }\DecValTok{1}\NormalTok{], }\AttributeTok{main =} \FunctionTok{dimnames}\NormalTok{(Bayes\_flymodel}\SpecialCharTok{$}\NormalTok{Sol)[[}\DecValTok{2}\NormalTok{]][}\DecValTok{1}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{09-nonlinear_generalized_linear_mixed_files/figure-latex/unnamed-chunk-22-1} \end{center}

\hypertarget{example-from-schabenberger_2001-section-8.4.1}{%
\subsection{\texorpdfstring{Example from \citep{Schabenberger_2001} section 8.4.1}{Example from {[}@Schabenberger\_2001{]} section 8.4.1}}\label{example-from-schabenberger_2001-section-8.4.1}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat2 }\OtherTok{\textless{}{-}} \FunctionTok{read.table}\NormalTok{(}\StringTok{"images/YellowPoplarData\_r.txt"}\NormalTok{)}
\FunctionTok{names}\NormalTok{(dat2) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}tn\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}k\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}dbh\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}totht\textquotesingle{}}\NormalTok{,}
                 \StringTok{\textquotesingle{}dob\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}ht\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}maxd\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}cumv\textquotesingle{}}\NormalTok{)}
\NormalTok{dat2}\SpecialCharTok{$}\NormalTok{t }\OtherTok{\textless{}{-}}\NormalTok{ dat2}\SpecialCharTok{$}\NormalTok{dob }\SpecialCharTok{/}\NormalTok{ dat2}\SpecialCharTok{$}\NormalTok{dbh}
\NormalTok{dat2}\SpecialCharTok{$}\NormalTok{r }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ dat2}\SpecialCharTok{$}\NormalTok{dob }\SpecialCharTok{/}\NormalTok{ dat2}\SpecialCharTok{$}\NormalTok{totht}
\end{Highlighting}
\end{Shaded}

The cumulative volume relates to the complementary diameter (subplots were created based on total tree height)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(dplyr)}
\NormalTok{dat2 }\OtherTok{\textless{}{-}}\NormalTok{ dat2 }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{group\_by}\NormalTok{(tn) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{z =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{        totht }\SpecialCharTok{\textless{}} \DecValTok{74} \SpecialCharTok{\&}\NormalTok{ totht }\SpecialCharTok{\textgreater{}=} \DecValTok{0} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}a: 0{-}74ft\textquotesingle{}}\NormalTok{,}
\NormalTok{        totht }\SpecialCharTok{\textless{}} \DecValTok{88} \SpecialCharTok{\&}\NormalTok{ totht }\SpecialCharTok{\textgreater{}=} \DecValTok{74} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}b: 74{-}88\textquotesingle{}}\NormalTok{,}
\NormalTok{        totht }\SpecialCharTok{\textless{}} \DecValTok{95} \SpecialCharTok{\&}\NormalTok{ totht }\SpecialCharTok{\textgreater{}=} \DecValTok{88} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}c: 88{-}95\textquotesingle{}}\NormalTok{,}
\NormalTok{        totht }\SpecialCharTok{\textless{}} \DecValTok{99} \SpecialCharTok{\&}\NormalTok{ totht }\SpecialCharTok{\textgreater{}=} \DecValTok{95} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}d: 95{-}99\textquotesingle{}}\NormalTok{,}
\NormalTok{        totht }\SpecialCharTok{\textless{}} \DecValTok{104} \SpecialCharTok{\&}\NormalTok{ totht }\SpecialCharTok{\textgreater{}=} \DecValTok{99} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}e: 99{-}104\textquotesingle{}}\NormalTok{,}
\NormalTok{        totht }\SpecialCharTok{\textless{}} \DecValTok{109} \SpecialCharTok{\&}\NormalTok{ totht }\SpecialCharTok{\textgreater{}=} \DecValTok{104} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}f: 104{-}109\textquotesingle{}}\NormalTok{,}
\NormalTok{        totht }\SpecialCharTok{\textless{}} \DecValTok{115} \SpecialCharTok{\&}\NormalTok{ totht }\SpecialCharTok{\textgreater{}=} \DecValTok{109} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}g: 109{-}115\textquotesingle{}}\NormalTok{,}
\NormalTok{        totht }\SpecialCharTok{\textless{}} \DecValTok{120} \SpecialCharTok{\&}\NormalTok{ totht }\SpecialCharTok{\textgreater{}=} \DecValTok{115} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}h: 115{-}120\textquotesingle{}}\NormalTok{,}
\NormalTok{        totht }\SpecialCharTok{\textless{}} \DecValTok{140} \SpecialCharTok{\&}\NormalTok{ totht }\SpecialCharTok{\textgreater{}=} \DecValTok{120} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}i: 120{-}150\textquotesingle{}}\NormalTok{,}
\NormalTok{    )}
\NormalTok{)}
\FunctionTok{ggplot}\NormalTok{(dat2, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ r, }\AttributeTok{y =}\NormalTok{ cumv)) }\SpecialCharTok{+} 
    \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{size =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+} 
    \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(z))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{09-nonlinear_generalized_linear_mixed_files/figure-latex/unnamed-chunk-24-1} \end{center}

The proposed non-linear model:

\[
V_{id_j} = (\beta_0 + (\beta_1 + b_{1i})\frac{D^2_i H_i}{1000})(\exp[-(\beta_2 + b_{2i})t_{ij} \exp(\beta_3 t_{ij})]) + e_{ij}
\]

where

\begin{itemize}
\item
  \(b_{1i}, b_{2i}\) are random effects
\item
  \(e_{ij}\) are random errors
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(nlme)}
\NormalTok{tmp }\OtherTok{\textless{}{-}}
    \FunctionTok{nlme}\NormalTok{(}
\NormalTok{        cumv }\SpecialCharTok{\textasciitilde{}}\NormalTok{ (b0 }\SpecialCharTok{+}\NormalTok{ (b1 }\SpecialCharTok{+}\NormalTok{ u1) }\SpecialCharTok{*}
\NormalTok{                    (dbh }\SpecialCharTok{*}\NormalTok{ dbh }\SpecialCharTok{*}\NormalTok{ totht }\SpecialCharTok{/} \DecValTok{1000}\NormalTok{)) }\SpecialCharTok{*}
\NormalTok{            (}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{(b2 }\SpecialCharTok{+}\NormalTok{ u2) }\SpecialCharTok{*}\NormalTok{ (t }\SpecialCharTok{/} \DecValTok{1000}\NormalTok{) }\SpecialCharTok{*} \FunctionTok{exp}\NormalTok{(b3 }\SpecialCharTok{*}\NormalTok{ t))), }
        \AttributeTok{data =}\NormalTok{ dat2,}
        \AttributeTok{fixed =}\NormalTok{ b0 }\SpecialCharTok{+}\NormalTok{ b1 }\SpecialCharTok{+}\NormalTok{ b2 }\SpecialCharTok{+}\NormalTok{ b3 }\SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{,}
        \CommentTok{\# 1 on the right hand side of the formula indicates }
        \CommentTok{\# a single fixed effects for the corresponding parameters}
        \AttributeTok{random =} \FunctionTok{list}\NormalTok{(}\FunctionTok{pdDiag}\NormalTok{(u1 }\SpecialCharTok{+}\NormalTok{ u2 }\SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{)),}
        \CommentTok{\#uncorrelated random effects}
        \AttributeTok{groups =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ tn,}
        \CommentTok{\#group on trees so each tree w/ have u1 and u2}
        \AttributeTok{start =} \FunctionTok{list}\NormalTok{(}\AttributeTok{fixed =} \FunctionTok{c}\NormalTok{(}
            \AttributeTok{b0 =} \FloatTok{0.25}\NormalTok{,}
            \AttributeTok{b1 =} \FloatTok{2.3}\NormalTok{,}
            \AttributeTok{b2 =} \FloatTok{2.87}\NormalTok{,}
            \AttributeTok{b3 =} \FloatTok{6.7}
\NormalTok{        ))}
\NormalTok{    )}
\FunctionTok{summary}\NormalTok{(tmp)}
\CommentTok{\#\textgreater{} Nonlinear mixed{-}effects model fit by maximum likelihood}
\CommentTok{\#\textgreater{}   Model: cumv \textasciitilde{} (b0 + (b1 + u1) * (dbh * dbh * totht/1000)) * (exp({-}(b2 +      u2) * (t/1000) * exp(b3 * t))) }
\CommentTok{\#\textgreater{}   Data: dat2 }
\CommentTok{\#\textgreater{}        AIC      BIC    logLik}
\CommentTok{\#\textgreater{}   31103.73 31151.33 {-}15544.86}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Random effects:}
\CommentTok{\#\textgreater{}  Formula: list(u1 \textasciitilde{} 1, u2 \textasciitilde{} 1)}
\CommentTok{\#\textgreater{}  Level: tn}
\CommentTok{\#\textgreater{}  Structure: Diagonal}
\CommentTok{\#\textgreater{}                u1       u2 Residual}
\CommentTok{\#\textgreater{} StdDev: 0.1508094 0.447829 2.226361}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Fixed effects:  b0 + b1 + b2 + b3 \textasciitilde{} 1 }
\CommentTok{\#\textgreater{}       Value  Std.Error   DF  t{-}value p{-}value}
\CommentTok{\#\textgreater{} b0 0.249386 0.12894687 6297   1.9340  0.0532}
\CommentTok{\#\textgreater{} b1 2.288832 0.01266804 6297 180.6776  0.0000}
\CommentTok{\#\textgreater{} b2 2.500497 0.05606685 6297  44.5985  0.0000}
\CommentTok{\#\textgreater{} b3 6.848871 0.02140677 6297 319.9395  0.0000}
\CommentTok{\#\textgreater{}  Correlation: }
\CommentTok{\#\textgreater{}    b0     b1     b2    }
\CommentTok{\#\textgreater{} b1 {-}0.639              }
\CommentTok{\#\textgreater{} b2  0.054  0.056       }
\CommentTok{\#\textgreater{} b3 {-}0.011 {-}0.066 {-}0.850}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Standardized Within{-}Group Residuals:}
\CommentTok{\#\textgreater{}           Min            Q1           Med            Q3           Max }
\CommentTok{\#\textgreater{} {-}6.694575e+00 {-}3.081861e{-}01 {-}8.904304e{-}05  3.469469e{-}01  7.855665e+00 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Observations: 6636}
\CommentTok{\#\textgreater{} Number of Groups: 336}
\NormalTok{nlme}\SpecialCharTok{::}\FunctionTok{intervals}\NormalTok{(tmp)}
\CommentTok{\#\textgreater{} Approximate 95\% confidence intervals}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Fixed effects:}
\CommentTok{\#\textgreater{}           lower      est.     upper}
\CommentTok{\#\textgreater{} b0 {-}0.003318061 0.2493855 0.5020892}
\CommentTok{\#\textgreater{} b1  2.264006036 2.2888322 2.3136584}
\CommentTok{\#\textgreater{} b2  2.390620340 2.5004973 2.6103743}
\CommentTok{\#\textgreater{} b3  6.806919342 6.8488713 6.8908232}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Random Effects:}
\CommentTok{\#\textgreater{}   Level: tn }
\CommentTok{\#\textgreater{}            lower      est.     upper}
\CommentTok{\#\textgreater{} sd(u1) 0.1376084 0.1508094 0.1652768}
\CommentTok{\#\textgreater{} sd(u2) 0.4056209 0.4478290 0.4944291}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Within{-}group standard error:}
\CommentTok{\#\textgreater{}    lower     est.    upper }
\CommentTok{\#\textgreater{} 2.187258 2.226361 2.266162}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Little different from the book because of different implementation of nonlinear mixed models.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(cowplot)}
\NormalTok{nlmmfn }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(fixed,rand,dbh,totht,t)\{}
\NormalTok{  b0 }\OtherTok{\textless{}{-}}\NormalTok{ fixed[}\DecValTok{1}\NormalTok{]}
\NormalTok{  b1 }\OtherTok{\textless{}{-}}\NormalTok{ fixed[}\DecValTok{2}\NormalTok{]}
\NormalTok{  b2 }\OtherTok{\textless{}{-}}\NormalTok{ fixed[}\DecValTok{3}\NormalTok{]}
\NormalTok{  b3 }\OtherTok{\textless{}{-}}\NormalTok{ fixed[}\DecValTok{4}\NormalTok{]}
\NormalTok{  u1 }\OtherTok{\textless{}{-}}\NormalTok{ rand[}\DecValTok{1}\NormalTok{]}
\NormalTok{  u2 }\OtherTok{\textless{}{-}}\NormalTok{ rand[}\DecValTok{2}\NormalTok{]}
  \CommentTok{\#just made so we can predict w/o random effects}
  \FunctionTok{return}\NormalTok{((b0}\SpecialCharTok{+}\NormalTok{(b1}\SpecialCharTok{+}\NormalTok{u1)}\SpecialCharTok{*}\NormalTok{(dbh}\SpecialCharTok{*}\NormalTok{dbh}\SpecialCharTok{*}\NormalTok{totht}\SpecialCharTok{/}\DecValTok{1000}\NormalTok{))}\SpecialCharTok{*}\NormalTok{(}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{(b2}\SpecialCharTok{+}\NormalTok{u2)}\SpecialCharTok{*}\NormalTok{(t}\SpecialCharTok{/}\DecValTok{1000}\NormalTok{)}\SpecialCharTok{*}\FunctionTok{exp}\NormalTok{(b3}\SpecialCharTok{*}\NormalTok{t))))}
\NormalTok{\}}



\CommentTok{\#Tree 1}
\NormalTok{pred1 }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{24}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{100}\NormalTok{))}
\FunctionTok{names}\NormalTok{(pred1) }\OtherTok{\textless{}{-}} \StringTok{\textquotesingle{}dob\textquotesingle{}}
\NormalTok{pred1}\SpecialCharTok{$}\NormalTok{tn }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{pred1}\SpecialCharTok{$}\NormalTok{dbh }\OtherTok{\textless{}{-}} \FunctionTok{unique}\NormalTok{(dat2[dat2}\SpecialCharTok{$}\NormalTok{tn }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, ]}\SpecialCharTok{$}\NormalTok{dbh)}
\NormalTok{pred1}\SpecialCharTok{$}\NormalTok{t }\OtherTok{\textless{}{-}}\NormalTok{ pred1}\SpecialCharTok{$}\NormalTok{dob }\SpecialCharTok{/}\NormalTok{ pred1}\SpecialCharTok{$}\NormalTok{dbh}
\NormalTok{pred1}\SpecialCharTok{$}\NormalTok{totht }\OtherTok{\textless{}{-}} \FunctionTok{unique}\NormalTok{(dat2[dat2}\SpecialCharTok{$}\NormalTok{tn }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, ]}\SpecialCharTok{$}\NormalTok{totht)}
\NormalTok{pred1}\SpecialCharTok{$}\NormalTok{r }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ pred1}\SpecialCharTok{$}\NormalTok{dob }\SpecialCharTok{/}\NormalTok{ pred1}\SpecialCharTok{$}\NormalTok{totht}


\NormalTok{pred1}\SpecialCharTok{$}\NormalTok{test }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(tmp, pred1)}
\NormalTok{pred1}\SpecialCharTok{$}\NormalTok{testno }\OtherTok{\textless{}{-}}
    \FunctionTok{nlmmfn}\NormalTok{(}
        \AttributeTok{fixed =}\NormalTok{ tmp}\SpecialCharTok{$}\NormalTok{coefficients}\SpecialCharTok{$}\NormalTok{fixed,}
        \AttributeTok{rand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{),}
\NormalTok{        pred1}\SpecialCharTok{$}\NormalTok{dbh,}
\NormalTok{        pred1}\SpecialCharTok{$}\NormalTok{totht,}
\NormalTok{        pred1}\SpecialCharTok{$}\NormalTok{t}
\NormalTok{    )}

\NormalTok{p1 }\OtherTok{\textless{}{-}}
    \FunctionTok{ggplot}\NormalTok{(pred1) }\SpecialCharTok{+} 
    \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ r, }\AttributeTok{y =}\NormalTok{ test, }\AttributeTok{color =} \StringTok{\textquotesingle{}with random\textquotesingle{}}\NormalTok{)) }\SpecialCharTok{+}
    \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ r, }\AttributeTok{y =}\NormalTok{ testno, }\AttributeTok{color =} \StringTok{\textquotesingle{}No random\textquotesingle{}}\NormalTok{)) }\SpecialCharTok{+} 
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{colour =} \StringTok{""}\NormalTok{) }\SpecialCharTok{+} 
    \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{data =}\NormalTok{ dat2[dat2}\SpecialCharTok{$}\NormalTok{tn }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, ], }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ r, }\AttributeTok{y =}\NormalTok{ cumv)) }\SpecialCharTok{+} 
    \FunctionTok{ggtitle}\NormalTok{(}\StringTok{\textquotesingle{}Tree 1\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{)}


\CommentTok{\#Tree 151}
\NormalTok{pred151        }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{21}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{100}\NormalTok{))}
\FunctionTok{names}\NormalTok{(pred151) }\OtherTok{\textless{}{-}} \StringTok{\textquotesingle{}dob\textquotesingle{}}
\NormalTok{pred151}\SpecialCharTok{$}\NormalTok{tn     }\OtherTok{\textless{}{-}} \DecValTok{151}
\NormalTok{pred151}\SpecialCharTok{$}\NormalTok{dbh    }\OtherTok{\textless{}{-}} \FunctionTok{unique}\NormalTok{(dat2[dat2}\SpecialCharTok{$}\NormalTok{tn }\SpecialCharTok{==} \DecValTok{151}\NormalTok{, ]}\SpecialCharTok{$}\NormalTok{dbh)}
\NormalTok{pred151}\SpecialCharTok{$}\NormalTok{t      }\OtherTok{\textless{}{-}}\NormalTok{ pred151}\SpecialCharTok{$}\NormalTok{dob }\SpecialCharTok{/}\NormalTok{ pred151}\SpecialCharTok{$}\NormalTok{dbh}
\NormalTok{pred151}\SpecialCharTok{$}\NormalTok{totht  }\OtherTok{\textless{}{-}} \FunctionTok{unique}\NormalTok{(dat2[dat2}\SpecialCharTok{$}\NormalTok{tn }\SpecialCharTok{==} \DecValTok{151}\NormalTok{, ]}\SpecialCharTok{$}\NormalTok{totht)}
\NormalTok{pred151}\SpecialCharTok{$}\NormalTok{r      }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ pred151}\SpecialCharTok{$}\NormalTok{dob }\SpecialCharTok{/}\NormalTok{ pred151}\SpecialCharTok{$}\NormalTok{totht}


\NormalTok{pred151}\SpecialCharTok{$}\NormalTok{test }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(tmp, pred151)}
\NormalTok{pred151}\SpecialCharTok{$}\NormalTok{testno }\OtherTok{\textless{}{-}}
    \FunctionTok{nlmmfn}\NormalTok{(}
        \AttributeTok{fixed =}\NormalTok{ tmp}\SpecialCharTok{$}\NormalTok{coefficients}\SpecialCharTok{$}\NormalTok{fixed,}
        \AttributeTok{rand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{),}
\NormalTok{        pred151}\SpecialCharTok{$}\NormalTok{dbh,}
\NormalTok{        pred151}\SpecialCharTok{$}\NormalTok{totht,}
\NormalTok{        pred151}\SpecialCharTok{$}\NormalTok{t}
\NormalTok{    )}

\NormalTok{p2 }\OtherTok{\textless{}{-}}
    \FunctionTok{ggplot}\NormalTok{(pred151) }\SpecialCharTok{+} 
    \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ r, }\AttributeTok{y =}\NormalTok{ test, }\AttributeTok{color =} \StringTok{\textquotesingle{}with random\textquotesingle{}}\NormalTok{)) }\SpecialCharTok{+}
    \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ r, }\AttributeTok{y =}\NormalTok{ testno, }\AttributeTok{color =} \StringTok{\textquotesingle{}No random\textquotesingle{}}\NormalTok{)) }\SpecialCharTok{+} 
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{colour =} \StringTok{""}\NormalTok{) }\SpecialCharTok{+} 
    \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{data =}\NormalTok{ dat2[dat2}\SpecialCharTok{$}\NormalTok{tn }\SpecialCharTok{==} \DecValTok{151}\NormalTok{,], }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ r, }\AttributeTok{y =}\NormalTok{ cumv)) }\SpecialCharTok{+} 
    \FunctionTok{ggtitle}\NormalTok{(}\StringTok{\textquotesingle{}Tree 151\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+} 
    \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{)}


\CommentTok{\#Tree 279}
\NormalTok{pred279        }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{9}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{100}\NormalTok{))}
\FunctionTok{names}\NormalTok{(pred279) }\OtherTok{\textless{}{-}} \StringTok{\textquotesingle{}dob\textquotesingle{}}
\NormalTok{pred279}\SpecialCharTok{$}\NormalTok{tn     }\OtherTok{\textless{}{-}} \DecValTok{279}
\NormalTok{pred279}\SpecialCharTok{$}\NormalTok{dbh    }\OtherTok{\textless{}{-}} \FunctionTok{unique}\NormalTok{(dat2[dat2}\SpecialCharTok{$}\NormalTok{tn }\SpecialCharTok{==} \DecValTok{279}\NormalTok{, ]}\SpecialCharTok{$}\NormalTok{dbh)}
\NormalTok{pred279}\SpecialCharTok{$}\NormalTok{t      }\OtherTok{\textless{}{-}}\NormalTok{ pred279}\SpecialCharTok{$}\NormalTok{dob }\SpecialCharTok{/}\NormalTok{ pred279}\SpecialCharTok{$}\NormalTok{dbh}
\NormalTok{pred279}\SpecialCharTok{$}\NormalTok{totht  }\OtherTok{\textless{}{-}} \FunctionTok{unique}\NormalTok{(dat2[dat2}\SpecialCharTok{$}\NormalTok{tn }\SpecialCharTok{==} \DecValTok{279}\NormalTok{, ]}\SpecialCharTok{$}\NormalTok{totht)}
\NormalTok{pred279}\SpecialCharTok{$}\NormalTok{r      }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ pred279}\SpecialCharTok{$}\NormalTok{dob }\SpecialCharTok{/}\NormalTok{ pred279}\SpecialCharTok{$}\NormalTok{totht}


\NormalTok{pred279}\SpecialCharTok{$}\NormalTok{test }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(tmp, pred279)}
\NormalTok{pred279}\SpecialCharTok{$}\NormalTok{testno }\OtherTok{\textless{}{-}}
    \FunctionTok{nlmmfn}\NormalTok{(}
        \AttributeTok{fixed =}\NormalTok{ tmp}\SpecialCharTok{$}\NormalTok{coefficients}\SpecialCharTok{$}\NormalTok{fixed,}
        \AttributeTok{rand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{),}
\NormalTok{        pred279}\SpecialCharTok{$}\NormalTok{dbh,}
\NormalTok{        pred279}\SpecialCharTok{$}\NormalTok{totht,}
\NormalTok{        pred279}\SpecialCharTok{$}\NormalTok{t}
\NormalTok{    )}

\NormalTok{p3 }\OtherTok{\textless{}{-}}
    \FunctionTok{ggplot}\NormalTok{(pred279) }\SpecialCharTok{+} 
    \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ r, }\AttributeTok{y =}\NormalTok{ test, }\AttributeTok{color =} \StringTok{\textquotesingle{}with random\textquotesingle{}}\NormalTok{)) }\SpecialCharTok{+}
    \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ r, }\AttributeTok{y =}\NormalTok{ testno, }\AttributeTok{color =} \StringTok{\textquotesingle{}No random\textquotesingle{}}\NormalTok{)) }\SpecialCharTok{+} 
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{colour =} \StringTok{""}\NormalTok{) }\SpecialCharTok{+} 
    \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{data =}\NormalTok{ dat2[dat2}\SpecialCharTok{$}\NormalTok{tn }\SpecialCharTok{==} \DecValTok{279}\NormalTok{, ], }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ r, }\AttributeTok{y =}\NormalTok{ cumv)) }\SpecialCharTok{+} 
    \FunctionTok{ggtitle}\NormalTok{(}\StringTok{\textquotesingle{}Tree 279\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+} 
    \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{)}

\FunctionTok{plot\_grid}\NormalTok{(p1, p2, p3)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{09-nonlinear_generalized_linear_mixed_files/figure-latex/unnamed-chunk-26-1} \end{center}

red line = predicted observations based on the common fixed effects

teal line = tree-specific predictions with random effects

\hypertarget{summary-2}{%
\section{Summary}\label{summary-2}}

\includegraphics[width=4.6875in,height=3.125in]{images/umbrella_of_models.PNG}

\hypertarget{part-iii.-ramifications}{%
\part*{III. RAMIFICATIONS}\label{part-iii.-ramifications}}
\addcontentsline{toc}{part}{III. RAMIFICATIONS}

\hypertarget{model-specification}{%
\chapter{Model Specification}\label{model-specification}}

Test whether underlying assumptions hold true

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{nested-model}{Nested Model} (A1/A3)
\item
  \protect\hyperlink{non-nested-model}{Non-Nested Model} (A1/A3)
\item
  \protect\hyperlink{heteroskedasticity}{Heteroskedasticity} (A4)
\end{itemize}

\hypertarget{nested-model}{%
\section{Nested Model}\label{nested-model}}

\[
\begin{aligned}
y &= \beta_0 + x_1\beta_1 + x_2\beta-2 + x_3\beta_3 + \epsilon & \text{unrestricted model} \\
y &= \beta_0 + x_1\beta_1 + \epsilon & \text{restricted model}
\end{aligned}
\]

Unrestricted model is always longer than the restricted model\\
The restricted model is ``nested'' within the unrestricted model\\
To determine which variables should be included or exclude, we could use the same \protect\hyperlink{wald-test-GLMM}{Wald Test}

\textbf{Adjusted} \(R^2\)

\begin{itemize}
\tightlist
\item
  \(R^2\) will always increase with more variables included
\item
  Adjusted \(R^2\) tries to correct by penalizing inclusion of unnecessary variables.
\end{itemize}

\[
\begin{aligned}
{R}^2 &= 1 - \frac{SSR/n}{SST/n} \\
{R}^2_{adj} &= 1 - \frac{SSR/(n-k)}{SST/(n-1)} \\
&= 1 - \frac{(n-1)(1-R^2)}{(n-k)}
\end{aligned}
\]

\begin{itemize}
\tightlist
\item
  \({R}^2_{adj}\) increases if and only if the t-statistic on the additional variable is greater than 1 in absolute value.
\item
  \({R}^2_{adj}\) is valid in models where there is no heteroskedasticity
\item
  there fore it \textbf{should not} be used in determining which variables should be included in the model (the t or F-tests are more appropriate)
\end{itemize}

\hypertarget{chow-test}{%
\subsection{Chow test}\label{chow-test}}

Should we run two different regressions for two groups?

\hypertarget{non-nested-model}{%
\section{Non-Nested Model}\label{non-nested-model}}

compare models with different non-nested specifications

\hypertarget{davidson-mackinnon-test}{%
\subsection{Davidson-Mackinnon test}\label{davidson-mackinnon-test}}

\hypertarget{independent-variable}{%
\subsubsection{Independent Variable}\label{independent-variable}}

Should the independent variables be logged? (decide between non-nested alternatives)

\[
\begin{aligned}
y =  \beta_0 + x_1\beta_1 + x_2\beta_2 + \epsilon && \text{(level eq)} \\
y =  \beta_0 + ln(x_1)\beta_1 + x_2\beta_2 + \epsilon && \text{(log eq)}
\end{aligned}
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Obtain predict outcome when estimating the model in log equation \(\check{y}\) and then estimate the following auxiliary equation,
\end{enumerate}

\[
y = \beta_0 + x_1\beta_1 + x_2\beta_2 + \check{y}\gamma + error
\]

and evaluate the t-statistic for the null hypothesis \(H_0: \gamma = 0\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Obtain predict outcome when estimating the model in the level equation \(\hat{y}\), then estimate the following auxiliary equation,
\end{enumerate}

\[
y = \beta_0 + ln(x_1)\beta_1 + x_2\beta_2 + \check{y}\gamma + error
\]

and evaluate the t-statistic for the null hypothesis \(H_0: \gamma = 0\)

\begin{itemize}
\tightlist
\item
  If you reject the null in the (1) step but fail to reject the null in the second step, then the log equation is preferred.
\item
  If fail to reject the null in the (1) step but reject the null in the (2) step then, level equation is preferred.
\item
  If reject in both steps, then you have statistical evidence that neither model should be used and should re-evaluate the functional form of your model.
\item
  If fail to reject in both steps, you do not have sufficient evidence to prefer one model over the other. You can compare the \(R^2_{adj}\) to choose between the two models.
\end{itemize}

\[
\begin{aligned}
y &= \beta_0 + ln(x)\beta_1 + \epsilon \\
y &= \beta_0 + x(\beta_1) + x^2\beta_2 + \epsilon
\end{aligned}
\]

\begin{itemize}
\tightlist
\item
  Compare which better fits the data
\item
  Compare standard \(R^2\) is unfair because the second model is less parsimonious (more parameters to estimate)
\item
  The \(R_{adj}^2\) will penalize the second model for being less parsimonious + Only valid when there is no heteroskedasticity (\protect\hyperlink{a4-homoskedasticity}{A4} holds)
\item
  Should only compare after a \protect\hyperlink{davidson-mackinnon-test}{Davidson-Mackinnon test}
\end{itemize}

\hypertarget{dependent-variable}{%
\subsubsection{Dependent Variable}\label{dependent-variable}}

\[
\begin{aligned}
y &= \beta_0 + x_1\beta_1 + \epsilon & \text{level eq} \\
ln(y) &= \beta_0 + x_1\beta_1 + \epsilon & \text{log eq} \\
\end{aligned}
\]

\begin{itemize}
\tightlist
\item
  In the level model, regardless of how big y is, x has a constant effect (i.e., one unit change in \(x_1\) results in a \(\beta_1\) unit change in y)
\item
  In the log model, the larger in y is, the effect of x is stronger (i.e., one unit change in \(x_1\) could increase y from 1 to \(1+\beta_1\) or from 100 to 100+100x\(\beta_1\))
\item
  Cannot compare \(R^2\) or \(R^2_{adj}\) because the outcomes are complement different, the scaling is different (SST is different)
\end{itemize}

We need to ``un-transform'' the \(ln(y)\) back to the same scale as y and then compare,

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Estimate the model in the log equation to obtain the predicted outcome \(\hat{ln(y)}\)
\item
  ``Un-transform'' the predicted outcome
\end{enumerate}

\[
\hat{m} = exp(\hat{ln(y)})
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Estimate the following model (without an intercept)
\end{enumerate}

\[
y = \alpha\hat{m} + error
\]

and obtain predicted outcome \(\hat{y}\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Then take the square of the correlation between \(\hat{y}\) and y as a scaled version of the \(R^2\) from the log model that can now compare with the usual \(R^2\) in the level model.
\end{enumerate}

\hypertarget{heteroskedasticity-1}{%
\section{Heteroskedasticity}\label{heteroskedasticity-1}}

\begin{itemize}
\item
  Using roust standard errors are always valid
\item
  If there is significant evidence of heteroskedasticity implying \protect\hyperlink{a4-homoskedasticity}{A4} does not hold

  \begin{itemize}
  \tightlist
  \item
    \protect\hyperlink{gauss-markov-theorem}{Gauss-Markov Theorem} no longer holds, OLS is not BLUE.
  \item
    Should consider using a better linear unbiased estimator (\protect\hyperlink{weighted-least-squares}{Weighted Least Squares} or \protect\hyperlink{generalized-least-squares}{Generalized Least Squares})
  \end{itemize}
\end{itemize}

\hypertarget{breusch-pagan-test}{%
\subsection{Breusch-Pagan test}\label{breusch-pagan-test}}

\protect\hyperlink{a4-homoskedasticity}{A4} implies

\[
E(\epsilon_i^2|\mathbf{x_i})=\sigma^2
\]

\[
\epsilon_i^2 = \gamma_0 + x_{i1}\gamma_1 + ... + x_{ik -1}\gamma_{k-1} + error
\]

and determining whether or not \(\mathbf{x}_i\) has any predictive value

\begin{itemize}
\tightlist
\item
  if \(\mathbf{x}_i\) has predictive value, then the variance changes over the levels of \(\mathbf{x}_i\) which is evidence of heteroskedasticity
\item
  if \(\mathbf{x}_i\) does not have predictive value, the variance is constant for all levels of \(\mathbf{x}_i\)
\end{itemize}

The \protect\hyperlink{breusch-pagan-test}{Breusch-Pagan test} for heteroskedasticity would compute the F-test of total significance for the following model

\[
e_i^2 = \gamma_0 + x_{i1}\gamma_1 + ... + x_{ik -1}\gamma_{k-1} + error
\]

A low p-value means we reject the null of homoskedasticity

However, \protect\hyperlink{breusch-pagan-test}{Breusch-Pagan test} cannot detect heteroskedasticity in non-linear form

\hypertarget{white-test}{%
\subsection{White test}\label{white-test}}

test heteroskedasticity would allow for a non-linear relationship by computing the F-test of total significance for the following model (assume there are three independent random variables)

\[
\begin{aligned}
e_i^2 &= \gamma_0 + x_i \gamma_1 + x_{i2}\gamma_2 + x_{i3}\gamma_3 \\
&+ x_{i1}^2\gamma_4 + x_{i2}^2\gamma_5 + x_{i3}^2\gamma_6 \\
&+ (x_{i1} \times x_{i2})\gamma_7 + (x_{i1} \times x_{i3})\gamma_8 + (x_{i2} \times x_{i3})\gamma_9 + error
\end{aligned}
\]

A low p-value means we reject the null of homoskedasticity

Equivalently, we can compute \protect\hyperlink{lagrange-multiplier-score}{LM} as \(LM = nR^2_{e^2}\) where the \(R^2_{e^2}\) come from the regression with the squared residual as the outcome

\begin{itemize}
\tightlist
\item
  The \protect\hyperlink{lagrange-multiplier-score}{LM} statistic has a \protect\hyperlink{chi-squared}{\(\chi_k^2\)} distribution
\end{itemize}

\hypertarget{imputation-missing-data}{%
\chapter{Imputation (Missing Data)}\label{imputation-missing-data}}

Imputation is a statistical procedure where you replace missing data with \emph{some reasonable values}

\begin{itemize}
\item
  Unit imputation = single data point
\item
  Item imputation = single feature value
\end{itemize}

Imputation is usually seen as the illegitimate child of statistical analysis. Several reasons that contribute to these negative views could be:

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  Peopled hardly do imputation correctly (which will introduce bias to your estimates)
\item
  Imputation can only be applied to a small range of problems correctly
\end{enumerate}

If you have missing data on \(y\) (dependent variable), you probably would not be able to do any imputation appropriately. However, if you have certain type of missing data (e.g., non-random missing data) in the \(x\)'s variable (independent variables), then you can still salvage your collected data points with imputation.

We also need to talk why you would want to do imputation in the first place. If your purpose is inference/ explanation (valid statistical inference not optimal point prediction), then imputation would not offer much help \citep{Rubin_1996}. However, if your purpose is prediction, you would want your standard error to be reduced by including information (non-missing data) on other variables of a data point. Then imputation could be the tool that you're looking for.

For most software packages, it will use listwise deletion or casewise deletion to have complete case analysis (analysis with only observations with all information). Not until recently that statistician can propose some methods that are a bit better than listwise deletion which are maximum likelihood and multiple imputation.

``Judging the quality of missing data procedures by their ability to recreate the individual missing values (according to hit rate, mean square error, etc) does not lead to choosing procedures that result in valid inference'', \citep{Rubin_1996}

Missing data can make it more challenging to big datasets.

\hypertarget{assumptions-1}{%
\section{Assumptions}\label{assumptions-1}}

\hypertarget{missing-completely-at-random-mcar}{%
\subsection{Missing Completely at Random (MCAR)}\label{missing-completely-at-random-mcar}}

Missing Completely at Random, MCAR, means there is no relationship between the missingness of the data and any values, observed or missing. Those missing data points are a random subset of the data. There is nothing systematic going on that makes some data more likely to be missing than others.

The probability of missing data on a variable is unrelated to the value of it or to the values of any other variables in the data set.

\textbf{Note}: the ``missingness'' on Y can be correlated with the ``missingness'' on X We can compare the value of other variables for the observations with missing data, and observations without missing data. If we reject the t-test for mean difference, we can say there is evidence that the data are not MCAR. But we cannot say that our data are MCAR if we fail to reject the t-test.

\begin{itemize}
\tightlist
\item
  the propensity for a data point to be missing is completely random.
\item
  There's no relationship between whether a data point is missing and any values in the data set, missing or observed.
\item
  The missing data are just a random subset of the data.
\end{itemize}

\hypertarget{missing-at-random-mar}{%
\subsection{Missing at Random (MAR)}\label{missing-at-random-mar}}

Missing at Random, MAR, means there is a systematic relationship between the propensity of missing values and the observed data, but not the missing data. Whether an observation is missing has nothing to do with the missing values, but it does have to do with the values of an individual's observed variables. So, for example, if men are more likely to tell you their weight than women, weight is MAR.

MAR is weaker than MCAR

\[
P(Y_{missing}|Y,X)= P(Y_{missing}|X)
\]

The probability of Y missing given Y and X equal to the probability of of Y missing given X. However, it is impossible to provide evidence to the MAR condition.

\begin{itemize}
\item
  the propensity for a data point to be missing is not related to the missing data, but it is related to some of the observed data. In another word, there is a systematic relationship between the propensity of missing values and the observed data, but not the missing data.

  \begin{itemize}
  \tightlist
  \item
    For example, if men are more likely to tell you their weight than women, weight is MAR
  \end{itemize}
\item
  MAR requires that the cause of the missing data is unrelated to the missing values but may be related to the observed values of other variables.
\item
  MAR means that the missing values are related to observed values on other variables. As an example of CD missing data, missing income data may be unrelated to the actual income values but are related to education. Perhaps people with more education are less likely to reveal their income than those with less education
\end{itemize}

\hypertarget{ignorable}{%
\subsection{Ignorable}\label{ignorable}}

The missing data mechanism is ignorable when

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  The data are \protect\hyperlink{missing-at-random-mar}{MAR}
\item
  the parameters in the function of the missing data process are unrelated to the parameters (of interest) that need to be estimated.
\end{enumerate}

In this case, you actually don't need to model the missing data mechanisms unless you would like to improve on your accuracy, in which case you still need to be very rigorous about your approach to improve efficiency in your parameters.

\hypertarget{nonignorable}{%
\subsection{Nonignorable}\label{nonignorable}}

Missing Not at Random, MNAR, means there is a relationship between the propensity of a value to be missing and its values.

Example: people with the lowest education are missing on education or the sickest people are most likely to drop out of the study.

MNAR is called \protect\hyperlink{nonignorable}{Nonignorable} because the missing data mechanism itself has to be modeled as you deal with the missing data. You have to include some model for why the data are missing and what the likely values are.

Hence, in the case of nonignorable, the data are not MAR. Then, your parameters of interest will be biased if you do not model the missing data mechanism. One of the most widely used approach for nonignorable missing data is \citep{Heckman_1976}

\begin{itemize}
\item
  Another name: Missing Not at Random (MNAR): there is a relationship between the propensity of a value to be missing and its values

  \begin{itemize}
  \tightlist
  \item
    For example, people with low education will be less likely to report it
  \end{itemize}
\item
  We need to model why the data are missing and what the likely values are.
\item
  the missing data mechanism is related to the missing values
\item
  It commonly occurs when people do not want to reveal something very personal or unpopular about themselves
\item
  Complete case analysis can give highly biased results for NI missing data. If proportionally more low and moderate income individuals are left in the sample because high income people are missing, an estimate of the mean income will be lower than the actual population mean.
\end{itemize}

One can use instrument that can predict the nonresponse process in outcome variable, and unrelated to the outcome of the population to correct for this missingness (but you still have to use complete cases) \citep{sun2018semiparametric, tchetgen2017general}

\hypertarget{solutions-to-missing-data}{%
\section{Solutions to Missing data}\label{solutions-to-missing-data}}

\hypertarget{listwise-deletion}{%
\subsection{Listwise Deletion}\label{listwise-deletion}}

Also known as complete case deletion only where you only retain cases with complete data for all features.

Advantages:

\begin{itemize}
\item
  Can be applied to any statistical test (SEM, multi-level regression, etc.)
\item
  In the case of MCAR, both the parameters estimates and its standard errors are unbiased.
\item
  In the case of MAR among independent variables (not depend on the values of dependent variables), then listwise deletion parameter estimates can still be unbiased. \citep{Little_1992} For example, you have a model \(y=\beta_{0}+\beta_1X_1 + \beta_2X_2 +\epsilon\) if the probability of missing data on \(X1\) is independent of \(Y\), but dependent on the value of \(X1\) and \(X2\), then the model estimates are still unbiased.

  \begin{itemize}
  \tightlist
  \item
    The missing data mechanism the depends on the values of the independent variables are the same as stratified sampling. And stratified sampling does not bias your estimates
  \item
    In the case of logistic regression, if the probability of missing data on any variable depends on the value of the dependent variable, but independent of the value of the independent variables, then the listwise deletion will yield biased intercept estimate, but consistent estimates of the slope and their standard errors \citep{Vach_1994}. However, logistic regression will still fail if the probability of missing data is dependent on both the value of the dependent and independent variables.
  \item
    Under regression analysis, listwise deletion is more robust than maximum likelihood and multiple imputation when MAR assumption is violated.
  \end{itemize}
\end{itemize}

Disadvantages:

\begin{itemize}
\tightlist
\item
  It will yield a larger standard errors than other more sophisticated methods discussed later.
\item
  If the data are not MCAR, but MAR, then your listwise deletion can yield biased estimates.
\item
  In other cases than regression analysis, other sophisticated methods can yield better estimates compared to listwise deletion.
\end{itemize}

\hypertarget{pairwise-deletion}{%
\subsection{Pairwise Deletion}\label{pairwise-deletion}}

This method could only be used in the case of linear models such as linear regression, factor analysis, or SEM. The premise of this method based on that the coefficient estimates are calculated based on the means, standard deviations, and correlation matrix. Compared to listwise deletion, we still utilized as many correlation between variables as possible to compute the correlation matrix.

Advantages:

\begin{itemize}
\item
  If the true missing data mechanism is MCAR, pair wise deletion will yield consistent estimates, and unbiased in large samples
\item
  Compared to listwise deletion: \citep{Glasser_1964}

  \begin{itemize}
  \tightlist
  \item
    If the correlation among variables are low, pairwise deletion is more efficient estimates than listwise
  \item
    If the correlations among variables are high, listwise deletion is more efficient than pairwise.
  \end{itemize}
\end{itemize}

Disadvantages:

\begin{itemize}
\tightlist
\item
  If the data mechanism is MAR, pairwise deletion will yield biased estimates.
\item
  In small sample, sometimes covariance matrix might not be positive definite, which means coefficients estimates cannot be calculated.
\end{itemize}

\textbf{Note}: You need to read carefully on how your software specify the sample size because it will alter the standard errors.

\hypertarget{dummy-variable-adjustment}{%
\subsection{Dummy Variable Adjustment}\label{dummy-variable-adjustment}}

Also known as Missing Indicator Method or Proxy Variable

Add another variable in the database to indicate whether a value is missing.

Create 2 variables

\[
D=
\begin{cases}
1 & \text{data on X are missing} \\
0 & \text{otherwise}\\
\end{cases}
\]

\[
X^* = 
\begin{cases}
X & \text{data are available} \\
c & \text{data are missing}\\
\end{cases}
\]

\textbf{Note}: A typical choice for \(c\) is usually the mean of \(X\)

Interpretation:

\begin{itemize}
\tightlist
\item
  Coefficient of \(D\) is the the difference in the expected value of \(Y\) between the group with data and the group without data on \(X\).
\item
  Coefficient of \(X^*\) is the effect of the group with data on \(Y\)
\end{itemize}

Disadvantages:

\begin{itemize}
\tightlist
\item
  This method yields biased estimates of the coefficient even in the case of MCAR \citep{jones1996indicator}
\end{itemize}

\hypertarget{imputation}{%
\subsection{Imputation}\label{imputation}}

\hypertarget{mean-mode-median-imputation}{%
\subsubsection{Mean, Mode, Median Imputation}\label{mean-mode-median-imputation}}

\begin{itemize}
\item
  Bad:

  \begin{itemize}
  \tightlist
  \item
    Mean imputation does not preserve the relationships among variables
  \item
    Mean imputation leads to An Underestimate of Standard Errors â†’ you're making Type I errors without realizing it.
  \item
    Biased estimates of variances and covariances \citep{haitovsky1968missing}
  \item
    In high-dimensions, mean substitution cannot account for dependence structure among features.
  \end{itemize}
\end{itemize}

\hypertarget{maximum-likelihood}{%
\subsubsection{Maximum Likelihood}\label{maximum-likelihood}}

When missing data are MAR and monotonic (such as in the case of panel studies), ML can be adequately in estimating coefficients.

Monotonic means that if you are missing data on X1, then that observation also has missing data on all other variables that come after it.

ML can generally handle linear models, log-linear model, but beyond that, ML still lacks both theory and software to implement.

\hypertarget{expectation-maximization-algorithm-em-algorithm}{%
\paragraph{Expectation-Maximization Algorithm (EM Algorithm)}\label{expectation-maximization-algorithm-em-algorithm}}

An iterative process:

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  Other variables are used to impute a value (Expectation).
\item
  Check whether the value is most likely (Maximization).
\item
  If not, it re-imputes a more likely value.
\end{enumerate}

You start your regression with your estimates based on either listwise deletion or pairwise deletion. After regressing missing variables on available variables, you obtain a regression model. Plug the missing data back into the original model, with modified variances and covariances For example, if you have missing data on \(X_{ij}\) you would regress it on available data of \(X_{i(j)}\), then plug the expected value of \(X_{ij}\) back with its \(X_{ij}^2\) turn into \(X_{ij}^2 + s_{j(j)}^2\) where \(s_{j(j)}^2\) stands for the residual variance from regressing \(X_{ij}\) on \(X_{i(j)}\) With the new estimated model, you rerun the process until the estimates converge.

Advantages:

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  Easy to use
\item
  Preserves the relationship with other variables (important if you use Factor Analysis or Linear Regression later on), but best in the case of Factor Analysis, which doesn't require standard error of individuals item.
\end{enumerate}

Disadvantages:

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  Standard errors of the coefficients are incorrect (biased usually downward - underestimate)
\item
  Models with overidentification, the estimates will not be efficient
\end{enumerate}

\hypertarget{direct-ml-raw-maximum-likelihood}{%
\paragraph{Direct ML (raw maximum likelihood)}\label{direct-ml-raw-maximum-likelihood}}

Advantages

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  Efficient estimates and correct standard errors.
\end{enumerate}

Disadvantages:

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  Hard to implements
\end{enumerate}

\hypertarget{multiple-imputation}{%
\subsubsection{Multiple Imputation}\label{multiple-imputation}}

MI is designed to use ``the Bayesian model-based approach to \emph{create} procedures, and the frequentist (randomization-based approach) to \emph{evaluate} procedures''. \citep{Rubin_1996}

MI estimates have the same properties as \protect\hyperlink{maximum-likelihood}{ML} when the data is \protect\hyperlink{missing-at-random-mar}{MAR}

\begin{itemize}
\tightlist
\item
  Consistent
\item
  Asymptotically efficient
\item
  Asymptotically normal
\end{itemize}

MI can be applied to any type of model, unlike \protect\hyperlink{maximum-likelihood}{Maximum Likelihood} that is only limited to a small set of models.

A drawback of MI is that it will produce slightly different estimates every time you run it. To avoid such problem, you can set seed when doing your analysis to ensure its reproducibility.

\hypertarget{single-random-imputation}{%
\paragraph{Single Random Imputation}\label{single-random-imputation}}

Random draws form the residual distribution of each imputed variable and add those random numbers to the imputed values.

For example, if we have missing data on \(X\), and it's MCAR, then

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\item
  Regress \(X\) on \(Y\) (\protect\hyperlink{listwise-deletion}{Listwise Deletion} method) to get its residual distribution.
\item
  For every missing value on X, we substitute with \(\tilde{x_i}=\hat{x_i} + \rho u_i\) where

  \begin{itemize}
  \tightlist
  \item
    \(u_i\) is a random draw from a standard normal distribution
  \item
    \(x_i\) is the predicted value from the regression of X and Y
  \item
    \(\rho\) is the standard deviation of the residual distribution of X regressed on Y.
  \end{itemize}
\end{enumerate}

However, the model you run with the imputed data still thinks that your data are collected, not imputed, which leads your standard error estimates to be too low and test statistics too high.

To address this problem, we need to repeat the imputation process which leads us to repeated imputation or multiple random imputation.

\hypertarget{repeated-imputation}{%
\paragraph{Repeated Imputation}\label{repeated-imputation}}

``Repeated imputations are draws from the posterior predictive distribution of the missing values under a specific model , a particular Bayesian model for both the data and the missing mechanism''.\citep{Rubin_1996}

Repeated imputation, also known as, multiple random imputation, allows us to have multiple ``completed'' data sets. The variability across imputations will adjust the standard errors upward.

The estimate of the standard error of \(\bar{r}\) (mean correlation estimates between X and Y) is \[
SE(\bar{r})=\sqrt{\frac{1}{M}\sum_{k}s_k^2+ (1+\frac{1}{M})(\frac{1}{M-1})\sum_{k}(r_k-\bar{r})^2}
\] where M is the number of replications, \(r_k\) is the the correlation in replication k, \(s_k\) is the estimated standard error in replication k.

However, this method still considers the parameter in predicting \(\tilde{x}\) is still fixed, which means we assume that we are using the true parameters to predict \(\tilde{x}\). To overcome this challenge, we need to introduce variability into our model for \(\tilde{x}\) by treating the parameters as a random variables and use Bayesian posterior distribution of the parameters to predict the parameters.

However, if your sample is large and the proportion of missing data is small, the extra Bayesian step might not be necessary. If your sample is small or the proportion of missing data is large, the extra Bayesian step is necessary.

Two algorithms to get random draws of the regression parameters from its posterior distribution:

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{data-augmentation}{Data Augmentation}
\item
  Sampling importance/resampling (SIR)
\end{itemize}

Authors have argued for SIR superiority due to its computer time \citep{king2001analyzing}

\hypertarget{data-augmentation}{%
\subparagraph{Data Augmentation}\label{data-augmentation}}

Steps for data augmentation:

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  Choose starting values for the parameters (e.g., for multivariate normal, choose means and covariance matrix). These values can come from previous values, expert knowledge, or from listwise deletion or pairwise deletion or EM estimation.
\item
  Based on the current values of means and covariances calculate the coefficients estimates for the equation that variable with missing data is regressed on all other variables (or variables that you think will help predict the missing values, could also be variables that are not in the final estimation model)
\item
  Use the estimates in step (2) to predict values for missing values. For each predicted value, add a random error from the residual normal distribution for that variable.
\item
  From the ``complete'' data set, recalculate the means and covariance matrix. And take a random draw from the posterior distribution of the means and covariances with Jeffreys' prior.
\item
  Using the random draw from step (4), repeat step (2) to (4) until the means and covariances stabilize (converged).
\end{enumerate}

The iterative process allows us to get random draws from the joint posterior distribution of both data nd parameters, given the observed data.

Rules of thumb regarding convergence:

\begin{itemize}
\tightlist
\item
  The higher the proportion of missing, the more iterations
\item
  the rate of convergence for EM algorithm should be the minimum threshold for DA.
\item
  You can also check if your distribution has been converged by diagnostic statistics Can check \href{https://bookdown.org/mike/bayesian_analysis/diag.html}{Bayesian Diagnostics} for some introduction.
\end{itemize}

Types of chains

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Parallel}: Run a separate chain of iterations for each of data set. Different starting values are encouraged. For example, one could use bootstrap to generate different data set with replacement, and for each data set, calculate the starting values by EM estimates.

  \begin{itemize}
  \tightlist
  \item
    Pro: Run faster, and less likely to have dependence in the resulting data sets.
  \item
    Con: Sometimes it will not converge
  \end{itemize}
\item
  \textbf{Sequential} one long chain of data augmentation cycles. After burn-in and thinning, you will have to data sets

  \begin{itemize}
  \tightlist
  \item
    Pro: Converged to the true posterior distribution is more likely.
  \item
    Con: The resulting data sets are likely to be dependent. Remedies can be thinning and burn-in.
  \end{itemize}
\end{enumerate}

\textbf{Note on Non-normal or categorical data} The normal-based methods still work well, but you will need to do some transformation. For example,

\begin{itemize}
\tightlist
\item
  If the data is skewed, then log-transform, then impute the exponentiate to have the missing data back to its original metric.
\item
  If the data is proportion, logit-transform, impute, then de-transform the missing data.
\end{itemize}

If you want to impute non-linear relationship, such as interaction between 2 variables and 1 variable is categorical. You can do separate imputation for different levels of that variable separately, then combined for the final analysis.

\begin{itemize}
\tightlist
\item
  If all variables that have missing data are categorical, then \textbf{unrestricted multinomial model} or \textbf{log-linear model} is recommended.
\item
  If a single categorical variable, \textbf{logistic (logit) regression} would be sufficient.
\end{itemize}

\hypertarget{nonparametric-semiparametric-methods}{%
\subsubsection{Nonparametric/ Semiparametric Methods}\label{nonparametric-semiparametric-methods}}

\hypertarget{hot-deck-imputation}{%
\paragraph{Hot Deck Imputation}\label{hot-deck-imputation}}

\begin{itemize}
\tightlist
\item
  Used by U.S. Census Bureau for public datasets
\item
  approximate Bayesian bootstrap
\item
  A randomly chosen value from an individual in the sample who has similar values on other variables. In other words, find all the sample subjects who are similar on other variables, then randomly choose one of their values on the missing variable.
\end{itemize}

When we have \(n_1\) cases with complete data on \(Y\) and \(n_0\) cases with missing data on \(Y\)

\begin{itemize}
\tightlist
\item
  Step 1: From \(n_1\), take a random sample (with replacement) of \(n_1\) cases
\item
  Step 2: From the retrieved sample take a random sample (with replacement) of \(n_0\) cases
\item
  Step 3: Assign the \(n_0\) cases in step 2 to \(n_0\) missing data cases.
\item
  Step 4: Repeat the process for every variable.
\item
  Step 5: For multiple imputation, repeat the four steps multiple times.
\end{itemize}

Note:

\begin{itemize}
\item
  If we skip step 1, it reduce variability for estimating standard errors.
\item
  Good:

  \begin{itemize}
  \tightlist
  \item
    Constrained to only possible values.
  \item
    Since the value is picked at random, it adds some variability, which might come in handy when calculating standard errors.
  \end{itemize}
\item
  Challenge: how can you define ``similar'' here.
\end{itemize}

\hypertarget{cold-deck-imputation}{%
\paragraph{Cold Deck Imputation}\label{cold-deck-imputation}}

Contrary to Hot Deck, Cold Deck choose value systematically from an observation that has similar values on other variables, which remove the random variation that we want.

Donor samples of ``cold-deck'' imputation come from a different data set.

\hypertarget{predictive-mean-matching}{%
\paragraph{Predictive Mean Matching}\label{predictive-mean-matching}}

Steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Regress \(Y\) on \(X\) (matrix of covariates) for the \(n_1\) (i.e., non-missing cases) to get coefficients \(b\) (a \(k \times 1\) vector) and residual variance estimates \(s^2\)
\item
  Draw randomly from the posterior predictive distribution of the residual variance (assuming a noninformative prior) by calculating \(\frac{(n_1-k)s^2}{\chi^2}\), where \(\chi^2\) is a random draw from a \(\chi^2_{n_1-k}\) and let \(s^2_{[1]}\) be an i-th random draw
\item
  Randomly draw from the posterior distribution of the coefficients \(b\), by drawing from \(MVN(b, s^2_{[1]}(X'X)^{-1})\), where X is an \(n_1 \times k\) matrix of X values. Then we have \(b_{1}\)
\item
  Using step 1, we can calculate standardized residuals for \(n_1\) cases: \(e_i = \frac{y_i - bx_i}{\sqrt{s^2(1-k/n_1)}}\)
\item
  Randomly draw a sample (with replacement) of \(n_0\) from the \(n_1\) residuals in step 4
\item
  With \(n_0\) cases, we can calculate imputed values of \(Y\): \(y_i = b_{[1]}x_i + s_{[1]}e_i\) where \(e_i\) are taken from step 5, and \(b_{[1]}\) taken from step 3, and \(s_{[1]}\) taken from step 2.
\item
  Repeat steps 2 through 6 except for step 4.
\end{enumerate}

Notes:

\begin{itemize}
\tightlist
\item
  can be used for multiple variables where each variable is imputed using all other variables as predictor.
\item
  can also be used for heteroskedasticity in imputed values.
\end{itemize}

Example from \href{https://statisticsglobe.com/predictive-mean-matching-imputation-method/}{Statistics Globe}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{918273}\NormalTok{) }\CommentTok{\# Seed}
\NormalTok{N  }\OtherTok{\textless{}{-}} \DecValTok{3000}                                    \CommentTok{\# Sample size}
\NormalTok{y  }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(}\FunctionTok{runif}\NormalTok{(N,}\SpecialCharTok{{-}}\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{))                 }\CommentTok{\# Target variable Y}
\NormalTok{x1 }\OtherTok{\textless{}{-}}\NormalTok{ y }\SpecialCharTok{+} \FunctionTok{round}\NormalTok{(}\FunctionTok{runif}\NormalTok{(N, }\DecValTok{0}\NormalTok{, }\DecValTok{50}\NormalTok{))              }\CommentTok{\# Auxiliary variable 1}
\NormalTok{x2 }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(y }\SpecialCharTok{+} \FloatTok{0.25} \SpecialCharTok{*}\NormalTok{ x1 }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(N,}\SpecialCharTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{15}\NormalTok{))  }\CommentTok{\# Auxiliary variable 2}
\NormalTok{x3 }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(}\FloatTok{0.1} \SpecialCharTok{*}\NormalTok{ x1 }\SpecialCharTok{+} \FunctionTok{rpois}\NormalTok{(N, }\DecValTok{2}\NormalTok{))           }\CommentTok{\# Auxiliary variable 3}
\CommentTok{\# (categorical variable)}
\NormalTok{x4 }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(}\FunctionTok{round}\NormalTok{(}\FloatTok{0.02} \SpecialCharTok{*}\NormalTok{ y }\SpecialCharTok{+} \FunctionTok{runif}\NormalTok{(N)))   }\CommentTok{\# Auxiliary variable 4 }

\CommentTok{\# Insert 20\% missing data in Y}
\NormalTok{y[}\FunctionTok{rbinom}\NormalTok{(N, }\DecValTok{1}\NormalTok{, }\FloatTok{0.2}\NormalTok{) }\SpecialCharTok{==} \DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NA}               

\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(y, x1, x2, x3, x4)         }\CommentTok{\# Store data in dataset}
\FunctionTok{head}\NormalTok{(data) }\CommentTok{\# First 6 rows of our data}
\CommentTok{\#\textgreater{}    y x1  x2 x3 x4}
\CommentTok{\#\textgreater{} 1  8 38  {-}3  6  1}
\CommentTok{\#\textgreater{} 2  1 50  {-}9  5  0}
\CommentTok{\#\textgreater{} 3  5 43  20  5  1}
\CommentTok{\#\textgreater{} 4 NA  9  13  3  0}
\CommentTok{\#\textgreater{} 5 {-}4 40 {-}10  6  0}
\CommentTok{\#\textgreater{} 6 NA 29  {-}6  5  1}

\FunctionTok{library}\NormalTok{(}\StringTok{"mice"}\NormalTok{) }\CommentTok{\# Load mice package}

\DocumentationTok{\#\#\#\#\# Impute data via predictive mean matching (single imputation)\#\#\#\#\#}

\NormalTok{imp\_single }\OtherTok{\textless{}{-}} \FunctionTok{mice}\NormalTok{(data, }\AttributeTok{m =} \DecValTok{1}\NormalTok{, }\AttributeTok{method =} \StringTok{"pmm"}\NormalTok{) }\CommentTok{\# Impute missing values}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  iter imp variable}
\CommentTok{\#\textgreater{}   1   1  y}
\CommentTok{\#\textgreater{}   2   1  y}
\CommentTok{\#\textgreater{}   3   1  y}
\CommentTok{\#\textgreater{}   4   1  y}
\CommentTok{\#\textgreater{}   5   1  y}
\NormalTok{data\_imp\_single }\OtherTok{\textless{}{-}} \FunctionTok{complete}\NormalTok{(imp\_single)         }\CommentTok{\# Store imputed data}
\CommentTok{\# head(data\_imp\_single)}

\CommentTok{\# Since single imputation underestiamtes stnadard errors, }
\CommentTok{\# we use multiple imputaiton}

\DocumentationTok{\#\#\#\#\# Predictive mean matching (multiple imputation) \#\#\#\#\#}

\CommentTok{\# Impute missing values multiple times}
\NormalTok{imp\_multi }\OtherTok{\textless{}{-}} \FunctionTok{mice}\NormalTok{(data, }\AttributeTok{m =} \DecValTok{5}\NormalTok{, }\AttributeTok{method =} \StringTok{"pmm"}\NormalTok{)  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  iter imp variable}
\CommentTok{\#\textgreater{}   1   1  y}
\CommentTok{\#\textgreater{}   1   2  y}
\CommentTok{\#\textgreater{}   1   3  y}
\CommentTok{\#\textgreater{}   1   4  y}
\CommentTok{\#\textgreater{}   1   5  y}
\CommentTok{\#\textgreater{}   2   1  y}
\CommentTok{\#\textgreater{}   2   2  y}
\CommentTok{\#\textgreater{}   2   3  y}
\CommentTok{\#\textgreater{}   2   4  y}
\CommentTok{\#\textgreater{}   2   5  y}
\CommentTok{\#\textgreater{}   3   1  y}
\CommentTok{\#\textgreater{}   3   2  y}
\CommentTok{\#\textgreater{}   3   3  y}
\CommentTok{\#\textgreater{}   3   4  y}
\CommentTok{\#\textgreater{}   3   5  y}
\CommentTok{\#\textgreater{}   4   1  y}
\CommentTok{\#\textgreater{}   4   2  y}
\CommentTok{\#\textgreater{}   4   3  y}
\CommentTok{\#\textgreater{}   4   4  y}
\CommentTok{\#\textgreater{}   4   5  y}
\CommentTok{\#\textgreater{}   5   1  y}
\CommentTok{\#\textgreater{}   5   2  y}
\CommentTok{\#\textgreater{}   5   3  y}
\CommentTok{\#\textgreater{}   5   4  y}
\CommentTok{\#\textgreater{}   5   5  y}
\NormalTok{data\_imp\_multi\_all }\OtherTok{\textless{}{-}}
    \CommentTok{\# Store multiply imputed data}
    \FunctionTok{complete}\NormalTok{(imp\_multi,       }
             \StringTok{"repeated"}\NormalTok{,}
             \AttributeTok{include =} \ConstantTok{TRUE}\NormalTok{)}

\NormalTok{data\_imp\_multi }\OtherTok{\textless{}{-}}
    \CommentTok{\# Combine imputed Y and X1{-}X4 (for convenience)}
    \FunctionTok{data.frame}\NormalTok{(data\_imp\_multi\_all[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{], data[, }\DecValTok{2}\SpecialCharTok{:}\DecValTok{5}\NormalTok{])}

\FunctionTok{head}\NormalTok{(data\_imp\_multi)}
\CommentTok{\#\textgreater{}   y.0 y.1 y.2 y.3 y.4 y.5 x1  x2 x3 x4}
\CommentTok{\#\textgreater{} 1   8   8   8   8   8   8 38  {-}3  6  1}
\CommentTok{\#\textgreater{} 2   1   1   1   1   1   1 50  {-}9  5  0}
\CommentTok{\#\textgreater{} 3   5   5   5   5   5   5 43  20  5  1}
\CommentTok{\#\textgreater{} 4  NA   1  {-}2  {-}4   9  {-}8  9  13  3  0}
\CommentTok{\#\textgreater{} 5  {-}4  {-}4  {-}4  {-}4  {-}4  {-}4 40 {-}10  6  0}
\CommentTok{\#\textgreater{} 6  NA   4   7   7   6   0 29  {-}6  5  1}
\end{Highlighting}
\end{Shaded}

Example from \href{https://stats.idre.ucla.edu/r/faq/how-do-i-perform-multiple-imputation-using-predictive-mean-matching-in-r/}{UCLA Statistical Consulting}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(mice)}
\FunctionTok{library}\NormalTok{(VIM)}
\FunctionTok{library}\NormalTok{(lattice)}
\FunctionTok{library}\NormalTok{(ggplot2)}
\DocumentationTok{\#\# set observations to NA}
\NormalTok{anscombe }\OtherTok{\textless{}{-}} \FunctionTok{within}\NormalTok{(anscombe, \{}
\NormalTok{    y1[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NA}
\NormalTok{    y4[}\DecValTok{3}\SpecialCharTok{:}\DecValTok{5}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NA}
\NormalTok{\})}
\DocumentationTok{\#\# view}
\FunctionTok{head}\NormalTok{(anscombe)}
\CommentTok{\#\textgreater{}   x1 x2 x3 x4   y1   y2    y3   y4}
\CommentTok{\#\textgreater{} 1 10 10 10  8   NA 9.14  7.46 6.58}
\CommentTok{\#\textgreater{} 2  8  8  8  8   NA 8.14  6.77 5.76}
\CommentTok{\#\textgreater{} 3 13 13 13  8   NA 8.74 12.74   NA}
\CommentTok{\#\textgreater{} 4  9  9  9  8 8.81 8.77  7.11   NA}
\CommentTok{\#\textgreater{} 5 11 11 11  8 8.33 9.26  7.81   NA}
\CommentTok{\#\textgreater{} 6 14 14 14  8 9.96 8.10  8.84 7.04}

\DocumentationTok{\#\# check missing data patterns}
\FunctionTok{md.pattern}\NormalTok{(anscombe)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{11-imputation_files/figure-latex/unnamed-chunk-2-1} \end{center}

\begin{verbatim}
#>   x1 x2 x3 x4 y2 y3 y1 y4  
#> 6  1  1  1  1  1  1  1  1 0
#> 2  1  1  1  1  1  1  1  0 1
#> 2  1  1  1  1  1  1  0  1 1
#> 1  1  1  1  1  1  1  0  0 2
#>    0  0  0  0  0  0  3  3 6

## Number of observations per patterns for all pairs of variables
p <- md.pairs(anscombe)
p 
#> $rr
#>    x1 x2 x3 x4 y1 y2 y3 y4
#> x1 11 11 11 11  8 11 11  8
#> x2 11 11 11 11  8 11 11  8
#> x3 11 11 11 11  8 11 11  8
#> x4 11 11 11 11  8 11 11  8
#> y1  8  8  8  8  8  8  8  6
#> y2 11 11 11 11  8 11 11  8
#> y3 11 11 11 11  8 11 11  8
#> y4  8  8  8  8  6  8  8  8
#> 
#> $rm
#>    x1 x2 x3 x4 y1 y2 y3 y4
#> x1  0  0  0  0  3  0  0  3
#> x2  0  0  0  0  3  0  0  3
#> x3  0  0  0  0  3  0  0  3
#> x4  0  0  0  0  3  0  0  3
#> y1  0  0  0  0  0  0  0  2
#> y2  0  0  0  0  3  0  0  3
#> y3  0  0  0  0  3  0  0  3
#> y4  0  0  0  0  2  0  0  0
#> 
#> $mr
#>    x1 x2 x3 x4 y1 y2 y3 y4
#> x1  0  0  0  0  0  0  0  0
#> x2  0  0  0  0  0  0  0  0
#> x3  0  0  0  0  0  0  0  0
#> x4  0  0  0  0  0  0  0  0
#> y1  3  3  3  3  0  3  3  2
#> y2  0  0  0  0  0  0  0  0
#> y3  0  0  0  0  0  0  0  0
#> y4  3  3  3  3  2  3  3  0
#> 
#> $mm
#>    x1 x2 x3 x4 y1 y2 y3 y4
#> x1  0  0  0  0  0  0  0  0
#> x2  0  0  0  0  0  0  0  0
#> x3  0  0  0  0  0  0  0  0
#> x4  0  0  0  0  0  0  0  0
#> y1  0  0  0  0  3  0  0  1
#> y2  0  0  0  0  0  0  0  0
#> y3  0  0  0  0  0  0  0  0
#> y4  0  0  0  0  1  0  0  3
\end{verbatim}

\begin{itemize}
\tightlist
\item
  \texttt{rr} = number of observations where both pairs of values are observed
\item
  \texttt{rm} = the number of observations where both variables are missing values
\item
  \texttt{mr} = the number of observations where the first variable's value (e.g.~the row variable) is observed and second (or column) variable is missing
\item
  \texttt{mm} = the number of observations where the second variable's value (e.g.~the col variable) is observed and first (or row) variable is missing
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# Margin plot of y1 and y4}
\FunctionTok{marginplot}\NormalTok{(anscombe[}\FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{8}\NormalTok{)], }\AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\StringTok{"red"}\NormalTok{, }\StringTok{"orange"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{11-imputation_files/figure-latex/unnamed-chunk-3-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\DocumentationTok{\#\# 5 imputations for all missing values}
\NormalTok{imp1 }\OtherTok{\textless{}{-}} \FunctionTok{mice}\NormalTok{(anscombe, }\AttributeTok{m =} \DecValTok{5}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  iter imp variable}
\CommentTok{\#\textgreater{}   1   1  y1  y4}
\CommentTok{\#\textgreater{}   1   2  y1  y4}
\CommentTok{\#\textgreater{}   1   3  y1  y4}
\CommentTok{\#\textgreater{}   1   4  y1  y4}
\CommentTok{\#\textgreater{}   1   5  y1  y4}
\CommentTok{\#\textgreater{}   2   1  y1  y4}
\CommentTok{\#\textgreater{}   2   2  y1  y4}
\CommentTok{\#\textgreater{}   2   3  y1  y4}
\CommentTok{\#\textgreater{}   2   4  y1  y4}
\CommentTok{\#\textgreater{}   2   5  y1  y4}
\CommentTok{\#\textgreater{}   3   1  y1  y4}
\CommentTok{\#\textgreater{}   3   2  y1  y4}
\CommentTok{\#\textgreater{}   3   3  y1  y4}
\CommentTok{\#\textgreater{}   3   4  y1  y4}
\CommentTok{\#\textgreater{}   3   5  y1  y4}
\CommentTok{\#\textgreater{}   4   1  y1  y4}
\CommentTok{\#\textgreater{}   4   2  y1  y4}
\CommentTok{\#\textgreater{}   4   3  y1  y4}
\CommentTok{\#\textgreater{}   4   4  y1  y4}
\CommentTok{\#\textgreater{}   4   5  y1  y4}
\CommentTok{\#\textgreater{}   5   1  y1  y4}
\CommentTok{\#\textgreater{}   5   2  y1  y4}
\CommentTok{\#\textgreater{}   5   3  y1  y4}
\CommentTok{\#\textgreater{}   5   4  y1  y4}
\CommentTok{\#\textgreater{}   5   5  y1  y4}

\DocumentationTok{\#\# linear regression for each imputed data set {-} 5 regression are run}
\NormalTok{fitm }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(imp1, }\FunctionTok{lm}\NormalTok{(y1 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ y4 }\SpecialCharTok{+}\NormalTok{ x1))}
\FunctionTok{summary}\NormalTok{(fitm)}
\CommentTok{\#\textgreater{} \# A tibble: 15 x 6}
\CommentTok{\#\textgreater{}    term        estimate std.error statistic p.value  nobs}
\CommentTok{\#\textgreater{}    \textless{}chr\textgreater{}          \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{} \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{}  1 (Intercept)    8.60      2.67      3.23  0.0121     11}
\CommentTok{\#\textgreater{}  2 y4            {-}0.533     0.251    {-}2.12  0.0667     11}
\CommentTok{\#\textgreater{}  3 x1             0.334     0.155     2.16  0.0628     11}
\CommentTok{\#\textgreater{}  4 (Intercept)    4.19      2.93      1.43  0.190      11}
\CommentTok{\#\textgreater{}  5 y4            {-}0.213     0.273    {-}0.782 0.457      11}
\CommentTok{\#\textgreater{}  6 x1             0.510     0.167     3.05  0.0159     11}
\CommentTok{\#\textgreater{}  7 (Intercept)    6.51      2.35      2.77  0.0244     11}
\CommentTok{\#\textgreater{}  8 y4            {-}0.347     0.215    {-}1.62  0.145      11}
\CommentTok{\#\textgreater{}  9 x1             0.395     0.132     3.00  0.0169     11}
\CommentTok{\#\textgreater{} 10 (Intercept)    5.48      3.02      1.81  0.107      11}
\CommentTok{\#\textgreater{} 11 y4            {-}0.316     0.282    {-}1.12  0.295      11}
\CommentTok{\#\textgreater{} 12 x1             0.486     0.173     2.81  0.0230     11}
\CommentTok{\#\textgreater{} 13 (Intercept)    7.12      1.81      3.92  0.00439    11}
\CommentTok{\#\textgreater{} 14 y4            {-}0.436     0.173    {-}2.53  0.0355     11}
\CommentTok{\#\textgreater{} 15 x1             0.425     0.102     4.18  0.00308    11}

\DocumentationTok{\#\# pool coefficients and standard errors across all 5 regression models}
\FunctionTok{pool}\NormalTok{(fitm)}
\CommentTok{\#\textgreater{} Class: mipo    m = 5 }
\CommentTok{\#\textgreater{}          term m   estimate       ubar           b           t dfcom       df}
\CommentTok{\#\textgreater{} 1 (Intercept) 5  6.3808015 6.72703243 2.785088109 10.06913816     8 3.902859}
\CommentTok{\#\textgreater{} 2          y4 5 {-}0.3690455 0.05860053 0.014674911  0.07621042     8 4.716160}
\CommentTok{\#\textgreater{} 3          x1 5  0.4301588 0.02191260 0.004980516  0.02788922     8 4.856052}
\CommentTok{\#\textgreater{}         riv    lambda       fmi}
\CommentTok{\#\textgreater{} 1 0.4968172 0.3319158 0.5254832}
\CommentTok{\#\textgreater{} 2 0.3005074 0.2310693 0.4303733}
\CommentTok{\#\textgreater{} 3 0.2727480 0.2142985 0.4143230}

\DocumentationTok{\#\# output parameter estimates}
\FunctionTok{summary}\NormalTok{(}\FunctionTok{pool}\NormalTok{(fitm))}
\CommentTok{\#\textgreater{}          term   estimate std.error statistic       df    p.value}
\CommentTok{\#\textgreater{} 1 (Intercept)  6.3808015 3.1731905  2.010847 3.902859 0.11643863}
\CommentTok{\#\textgreater{} 2          y4 {-}0.3690455 0.2760624 {-}1.336819 4.716160 0.24213491}
\CommentTok{\#\textgreater{} 3          x1  0.4301588 0.1670007  2.575791 4.856052 0.05107581}
\end{Highlighting}
\end{Shaded}

\hypertarget{stochastic-imputation}{%
\paragraph{Stochastic Imputation}\label{stochastic-imputation}}

\texttt{Regression\ imputation\ +\ random\ residual\ =\ Stochastic\ Imputation}

Most multiple imputation is based off of some form of stochastic regression imputation.

Good:

\begin{itemize}
\tightlist
\item
  Has all the advantage of \protect\hyperlink{regression-imputation}{Regression Imputation}
\item
  and also has the random components
\end{itemize}

Bad:

\begin{itemize}
\tightlist
\item
  might lead to implausible values (e.g.~negative values)
\item
  can't handle heteroskadastic data
\end{itemize}

\textbf{Note}\\
Multiple Imputation usually based on some form of stochastic regression imputation.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Income data}
 
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{91919}\NormalTok{)                              }\CommentTok{\# Set seed}
\NormalTok{N }\OtherTok{\textless{}{-}} \DecValTok{1000}                                    \CommentTok{\# Sample size}
 
\NormalTok{income }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(N, }\DecValTok{0}\NormalTok{, }\DecValTok{500}\NormalTok{))            }\CommentTok{\# Create some synthetic income data}
\NormalTok{income[income }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ income[income }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ (}\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)}
 
\NormalTok{x1 }\OtherTok{\textless{}{-}}\NormalTok{ income }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(N, }\DecValTok{1000}\NormalTok{, }\DecValTok{1500}\NormalTok{)          }\CommentTok{\# Auxiliary variables}
\NormalTok{x2 }\OtherTok{\textless{}{-}}\NormalTok{ income }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(N, }\SpecialCharTok{{-}} \DecValTok{5000}\NormalTok{, }\DecValTok{2000}\NormalTok{)}
 
\NormalTok{income[}\FunctionTok{rbinom}\NormalTok{(N, }\DecValTok{1}\NormalTok{, }\FloatTok{0.1}\NormalTok{) }\SpecialCharTok{==} \DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NA}         \CommentTok{\# Create 10\% missingness in income}
 
\NormalTok{data\_inc\_miss }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(income, x1, x2)}
\end{Highlighting}
\end{Shaded}

Single stochastic regression imputation

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{imp\_inc\_sri  }\OtherTok{\textless{}{-}} \FunctionTok{mice}\NormalTok{(data\_inc\_miss, }\AttributeTok{method =} \StringTok{"norm.nob"}\NormalTok{, }\AttributeTok{m =} \DecValTok{1}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  iter imp variable}
\CommentTok{\#\textgreater{}   1   1  income}
\CommentTok{\#\textgreater{}   2   1  income}
\CommentTok{\#\textgreater{}   3   1  income}
\CommentTok{\#\textgreater{}   4   1  income}
\CommentTok{\#\textgreater{}   5   1  income}
\NormalTok{data\_inc\_sri }\OtherTok{\textless{}{-}} \FunctionTok{complete}\NormalTok{(imp\_inc\_sri)}
\end{Highlighting}
\end{Shaded}

Single predictive mean matching

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{imp\_inc\_pmm  }\OtherTok{\textless{}{-}} \FunctionTok{mice}\NormalTok{(data\_inc\_miss, }\AttributeTok{method =} \StringTok{"pmm"}\NormalTok{, }\AttributeTok{m =} \DecValTok{1}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  iter imp variable}
\CommentTok{\#\textgreater{}   1   1  income}
\CommentTok{\#\textgreater{}   2   1  income}
\CommentTok{\#\textgreater{}   3   1  income}
\CommentTok{\#\textgreater{}   4   1  income}
\CommentTok{\#\textgreater{}   5   1  income}
\NormalTok{data\_inc\_pmm }\OtherTok{\textless{}{-}} \FunctionTok{complete}\NormalTok{(imp\_inc\_pmm)}
\end{Highlighting}
\end{Shaded}

Stochastic regression imputation contains negative values

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_inc\_sri}\SpecialCharTok{$}\NormalTok{income[data\_inc\_sri}\SpecialCharTok{$}\NormalTok{income }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{]}
\CommentTok{\#\textgreater{} [1]  {-}66.055957  {-}96.980053  {-}28.921432   {-}4.175686  {-}54.480798  {-}27.207102}
\CommentTok{\#\textgreater{} [7] {-}143.603500  {-}80.960488}
\CommentTok{\# No values below 0}
\NormalTok{data\_inc\_pmm}\SpecialCharTok{$}\NormalTok{income[data\_inc\_pmm}\SpecialCharTok{$}\NormalTok{income }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{] }
\CommentTok{\#\textgreater{} numeric(0)}
\end{Highlighting}
\end{Shaded}

Evidence for heteroskadastic data

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Heteroscedastic data}
 
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{654654}\NormalTok{)                             }\CommentTok{\# Set seed}
\NormalTok{N }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\DecValTok{5000}                                  \CommentTok{\# Sample size}
 
\NormalTok{a }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{b }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{sigma2 }\OtherTok{\textless{}{-}}\NormalTok{ N}\SpecialCharTok{\^{}}\DecValTok{2}
\NormalTok{eps }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(N, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \FunctionTok{sqrt}\NormalTok{(sigma2))}
 
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ a }\SpecialCharTok{+}\NormalTok{ b }\SpecialCharTok{*}\NormalTok{ N }\SpecialCharTok{+}\NormalTok{ eps                         }\CommentTok{\# Heteroscedastic variable}
\NormalTok{x }\OtherTok{\textless{}{-}} \DecValTok{30} \SpecialCharTok{*}\NormalTok{ N }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(N[}\FunctionTok{length}\NormalTok{(N)], }\DecValTok{1000}\NormalTok{, }\DecValTok{200}\NormalTok{) }\CommentTok{\# Correlated variable}
 
\NormalTok{y[}\FunctionTok{rbinom}\NormalTok{(N[}\FunctionTok{length}\NormalTok{(N)], }\DecValTok{1}\NormalTok{, }\FloatTok{0.3}\NormalTok{) }\SpecialCharTok{==} \DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NA}   \CommentTok{\# 30\% missing}
 
\NormalTok{data\_het\_miss }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(y, x)}
\end{Highlighting}
\end{Shaded}

Single stochastic regression imputation

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{imp\_het\_sri  }\OtherTok{\textless{}{-}} \FunctionTok{mice}\NormalTok{(data\_het\_miss, }\AttributeTok{method =} \StringTok{"norm.nob"}\NormalTok{, }\AttributeTok{m =} \DecValTok{1}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  iter imp variable}
\CommentTok{\#\textgreater{}   1   1  y}
\CommentTok{\#\textgreater{}   2   1  y}
\CommentTok{\#\textgreater{}   3   1  y}
\CommentTok{\#\textgreater{}   4   1  y}
\CommentTok{\#\textgreater{}   5   1  y}
\NormalTok{data\_het\_sri }\OtherTok{\textless{}{-}} \FunctionTok{complete}\NormalTok{(imp\_het\_sri)}
\end{Highlighting}
\end{Shaded}

Single predictive mean matching

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{imp\_het\_pmm  }\OtherTok{\textless{}{-}} \FunctionTok{mice}\NormalTok{(data\_het\_miss, }\AttributeTok{method =} \StringTok{"pmm"}\NormalTok{, }\AttributeTok{m =} \DecValTok{1}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  iter imp variable}
\CommentTok{\#\textgreater{}   1   1  y}
\CommentTok{\#\textgreater{}   2   1  y}
\CommentTok{\#\textgreater{}   3   1  y}
\CommentTok{\#\textgreater{}   4   1  y}
\CommentTok{\#\textgreater{}   5   1  y}
\NormalTok{data\_het\_pmm }\OtherTok{\textless{}{-}} \FunctionTok{complete}\NormalTok{(imp\_het\_pmm)}
\end{Highlighting}
\end{Shaded}

Comparison between predictive mean matching and stochastic regression imputation

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))                              }\CommentTok{\# Both plots in one graphic}

\CommentTok{\# Plot of observed values}
\FunctionTok{plot}\NormalTok{(x[}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(data\_het\_sri}\SpecialCharTok{$}\NormalTok{y)],}
\NormalTok{     data\_het\_sri}\SpecialCharTok{$}\NormalTok{y[}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(data\_het\_sri}\SpecialCharTok{$}\NormalTok{y)],}
     \AttributeTok{main =} \StringTok{""}\NormalTok{,}
     \AttributeTok{xlab =} \StringTok{"X"}\NormalTok{,}
     \AttributeTok{ylab =} \StringTok{"Y"}\NormalTok{)}
\CommentTok{\# Plot of missing values}
\FunctionTok{points}\NormalTok{(x[}\FunctionTok{is.na}\NormalTok{(y)], data\_het\_sri}\SpecialCharTok{$}\NormalTok{y[}\FunctionTok{is.na}\NormalTok{(y)],}
       \AttributeTok{col =} \StringTok{"red"}\NormalTok{)}

\CommentTok{\# Title of plot}
\FunctionTok{title}\NormalTok{(}\StringTok{"Stochastic Regression Imputation"}\NormalTok{,        }
      \AttributeTok{line =} \FloatTok{0.5}\NormalTok{)}

\CommentTok{\# Regression line}
\FunctionTok{abline}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, data\_het\_sri),                   }
       \AttributeTok{col =} \StringTok{"\#1b98e0"}\NormalTok{, }\AttributeTok{lwd =} \FloatTok{2.5}\NormalTok{)}

\CommentTok{\# Legend}
\FunctionTok{legend}\NormalTok{(}
  \StringTok{"topleft"}\NormalTok{,}
  \FunctionTok{c}\NormalTok{(}\StringTok{"Observed Values"}\NormalTok{, }\StringTok{"Imputed Values"}\NormalTok{, }\StringTok{"Regression Y \textasciitilde{} X"}\NormalTok{),}
  \AttributeTok{pch =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\ConstantTok{NA}\NormalTok{),}
  \AttributeTok{lty =} \FunctionTok{c}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\DecValTok{1}\NormalTok{),}
  \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"red"}\NormalTok{, }\StringTok{"\#1b98e0"}\NormalTok{)}
\NormalTok{)}

\CommentTok{\# Plot of observed values}
\FunctionTok{plot}\NormalTok{(x[}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(data\_het\_pmm}\SpecialCharTok{$}\NormalTok{y)],}
\NormalTok{     data\_het\_pmm}\SpecialCharTok{$}\NormalTok{y[}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(data\_het\_pmm}\SpecialCharTok{$}\NormalTok{y)],}
     \AttributeTok{main =} \StringTok{""}\NormalTok{,}
     \AttributeTok{xlab =} \StringTok{"X"}\NormalTok{,}
     \AttributeTok{ylab =} \StringTok{"Y"}\NormalTok{)}


\CommentTok{\# Plot of missing values}
\FunctionTok{points}\NormalTok{(x[}\FunctionTok{is.na}\NormalTok{(y)], data\_het\_pmm}\SpecialCharTok{$}\NormalTok{y[}\FunctionTok{is.na}\NormalTok{(y)],}
       \AttributeTok{col =} \StringTok{"red"}\NormalTok{)}

\CommentTok{\# Title of plot}
\FunctionTok{title}\NormalTok{(}\StringTok{"Predictive Mean Matching"}\NormalTok{,}
      \AttributeTok{line =} \FloatTok{0.5}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, data\_het\_pmm),}
       \AttributeTok{col =} \StringTok{"\#1b98e0"}\NormalTok{, }\AttributeTok{lwd =} \FloatTok{2.5}\NormalTok{)}

\CommentTok{\# Legend}
\FunctionTok{legend}\NormalTok{(}
  \StringTok{"topleft"}\NormalTok{,}
  \FunctionTok{c}\NormalTok{(}\StringTok{"Observed Values"}\NormalTok{, }\StringTok{"Imputed Values"}\NormalTok{, }\StringTok{"Regression Y \textasciitilde{} X"}\NormalTok{),}
  \AttributeTok{pch =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\ConstantTok{NA}\NormalTok{),}
  \AttributeTok{lty =} \FunctionTok{c}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\DecValTok{1}\NormalTok{),}
  \AttributeTok{col =} \FunctionTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"red"}\NormalTok{, }\StringTok{"\#1b98e0"}\NormalTok{)}
\NormalTok{)}

\FunctionTok{mtext}\NormalTok{(}
  \StringTok{"Imputation of Heteroscedastic Data"}\NormalTok{,}
  \CommentTok{\# Main title of plot}
  \AttributeTok{side =} \DecValTok{3}\NormalTok{,}
  \AttributeTok{line =} \SpecialCharTok{{-}}\FloatTok{1.5}\NormalTok{,}
  \AttributeTok{outer =} \ConstantTok{TRUE}\NormalTok{,}
  \AttributeTok{cex =} \DecValTok{2}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{11-imputation_files/figure-latex/unnamed-chunk-11-1} \end{center}

\hypertarget{regression-imputation}{%
\subsubsection{Regression Imputation}\label{regression-imputation}}

Also known as conditional mean imputation Missing value is based (regress) on other variables.

\begin{itemize}
\item
  Good:

  \begin{itemize}
  \item
    Maintain the relationship with other variables (i.e., preserve dependence structure among features, unlike \ref{mean-mode-median-imputation}).
  \item
    If the data are MCAR, least-squares coefficients estimates will be consistent, and approximately unbiased in large samples \citep{Gourieroux_1981}

    \begin{itemize}
    \tightlist
    \item
      Can have improvement on efficiency by using weighted least squares \citep{Beale_1975} or generalized least squares \citep{Gourieroux_1981}.
    \end{itemize}
  \end{itemize}
\item
  Bad:

  \begin{itemize}
  \tightlist
  \item
    No variability left. treated data as if they were collected.
  \item
    Underestimate the standard errors and overestimate test statistics
  \end{itemize}
\end{itemize}

\hypertarget{interpolation-and-extrapolation}{%
\subsubsection{Interpolation and Extrapolation}\label{interpolation-and-extrapolation}}

An estimated value from other observations from the same individual. It usually only works in longitudinal data.

\hypertarget{k-nearest-neighbor-knn-imputation}{%
\subsubsection{K-nearest neighbor (KNN) imputation}\label{k-nearest-neighbor-knn-imputation}}

The above methods are model-based imputation (regression).\\
This is an example of neighbor-based imputation (K-nearest neighbor).

For every observation that needs to be imputed, the algorithm identifies `k' closest observations based on some types distance (e.g., Euclidean) and computes the weighted average (weighted based on distance) of these `k' obs.

For a discrete variable, it uses the most frequent value among the k nearest neighbors.

\begin{itemize}
\tightlist
\item
  Distance metrics: Hamming distance.
\end{itemize}

For a continuous variable, it uses the mean or mode.

\begin{itemize}
\item
  Distance metrics:

  \begin{itemize}
  \tightlist
  \item
    Euclidean
  \item
    Mahalanobis
  \item
    Manhattan
  \end{itemize}
\end{itemize}

\hypertarget{bayesian-ridge-regression-implementation}{%
\subsubsection{Bayesian Ridge regression implementation}\label{bayesian-ridge-regression-implementation}}

\hypertarget{matrix-completion}{%
\subsubsection{Matrix Completion}\label{matrix-completion}}

Impute items missing at random while accounting for dependence between features by using principal components, which is known as \textbf{matrix completion} \citep[Sec 12.3]{james2013}

Consider an \(n \times p\) feature matrix, \(\mathbf{X}\), with element \(x_{ij}\), some of which are missing.

Similar to \ref{principal-components}, we can approximate the matrix \(\mathbf{X}\) in terms of its leading PCs.

We consider the \(M\) principal components that optimize

\[
\underset{\mathbf{A} \in \mathbb{R}^{n \times M}, \mathbf{B} \in \mathbb{R}^{p \times M}}{\operatorname{min}} \left\{ \sum_{(i,j) \in \mathcal{O}} (x_{ij} - \sum_{m=1}^M a_{im}b_{jm})^2 \right\}
\]

where \(\mathcal{O}\) is the set of all observed pairs indices \((i,j)\), a subset of the possible \(n \times p\) pairs

Once this minimization is solved,

\begin{itemize}
\item
  One can impute a missing observation, \(x_{ij}\), with \(\hat{x}_{ij} = \sum_{m=1}^M \hat{a}_{im}\hat{b}_{jm}\) where \(\hat{a}_{im}, \hat{b}_{jm}\) are the \((i,m)\) and \((j.m)\) elements, respectively, of the matrices \(\hat{\mathbf{A}}\) and \(\hat{\mathbf{B}}\) from the minimization, and
\item
  One can approximately recover the \(M\) principal component scores and loadings, as we did when the data were complete
\end{itemize}

The challenge here is to solve this minimization problem: the eigen-decomposition non longer applies (as in \ref{principal-components}

Hence, we have to use iterative algorithm \citep[ Alg 12.1]{james2013}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create a complete data matrix \(\tilde{\mathbf{X}}\) of dimension \(n \times p\) of which the \((i,j)\) element equals
\end{enumerate}

\[
\tilde{x}_{ij} =
\begin{cases}
x_{ij} & \text{if } (i,j) \in \mathcal{O} \\
\bar{x}_{j} & \text{if } (i,j) \notin \mathcal{O}
\end{cases}
\]

where \(\bar{x}_j\) is the average of the observed values for the \(j\)th variable in the incomplete data matrix \(\mathbf{X}\)

\(\mathcal{O}\) indexes the observations that are observed in \(\mathbf{X}\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Repeat these 3 steps until some objectives are met
\end{enumerate}

a. Solve

\[
\underset{\mathbf{A} \in R^{n \times M}, \mathbf{B} \in R^{p \times M}}{\operatorname{min}} \{ \sum_{(i,j) \in \mathcal{O}} (x_{ij} - \sum_{m=1}^M a_{im}b_{jm})^2 \} 
\]

by computing the principal components of \(\tilde{\mathbf{X}}\)

b. For each element \((i,j) \notin \mathcal{O}\), set \(\tilde{x}_{ij} \leftarrow \sum_{m=1}^M \hat{a}_{im}\hat{b}_{jm}\)

c. Compute the objective

\[
\sum_{(i,j \in \mathcal{O})} (x_{ij} - \sum_{m=1}^M \hat{a}_{im} \hat{b}_{jm})^2
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Return the estimated missing entries \(\tilde{x}_{ij}, (i,j) \notin \mathcal{O}\)
\end{enumerate}

\hypertarget{other-methods}{%
\subsection{Other methods}\label{other-methods}}

\begin{itemize}
\tightlist
\item
  For panel data, or clustered data, use \texttt{pan} package by Schafer (1997)
\end{itemize}

\hypertarget{criteria-for-choosing-an-effective-approach}{%
\section{Criteria for Choosing an Effective Approach}\label{criteria-for-choosing-an-effective-approach}}

Criteria for an ideal technique in treating missing data:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Unbiased parameter estimates
\item
  Adequate power
\item
  Accurate standard errors (p-values, confidence intervals)
\end{enumerate}

The Multiple Imputation and Full Information Maximum Likelihood are the the most ideal candidate. Single imputation will generally lead to underestimation of standard errors.

\hypertarget{another-perspective}{%
\section{Another Perspective}\label{another-perspective}}

Model bias can arisen from various factors including:

\begin{itemize}
\tightlist
\item
  Imputation method
\item
  Missing data mechanism (\protect\hyperlink{missing-completely-at-random-mcar}{MCAR} vs.~\protect\hyperlink{missing-at-random-mar}{MAR})
\item
  Proportion of the missing data
\item
  Information available in the data set
\end{itemize}

Since the imputed observations are themselves estimates, their values have corresponding random error. But when you put in that estimate as a data point, your software doesn't know that. So it overlooks the extra source of error, resulting in too-small standard errors and too-small p-values. So multiple imputation comes up with multiple estimates.

Because multiple imputation have a random component, the multiple estimates are slightly different. This re-introduces some variation that your software can incorporate in order to give your model accurate estimates of standard error. Multiple imputation was a huge breakthrough in statistics about 20 years ago. It solves a lot of problems with missing data (though, unfortunately not all) and if done well, leads to unbiased parameter estimates and accurate standard errors. If your rate of missing data is very, very small (2-3\%) it doesn't matter what technique you use.

Remember that there are three goals of multiple imputation, or any missing data technique:

\begin{itemize}
\tightlist
\item
  Unbiased parameter estimates in the final analysis (regression coefficients, group means, odds ratios, etc.)
\item
  accurate standard errors of those parameter estimates, and therefore, accurate p-values in the analysis
\item
  adequate power to find meaningful parameter values significant.
\end{itemize}

Hence,

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Don't round off imputations for dummy variables. Many common imputation techniques, like MCMC, require normally distributed variables. Suggestions for imputing categorical variables were to dummy code them, impute them, then round off imputed values to 0 or 1. Recent research, however, has found that rounding off imputed values actually leads to biased parameter estimates in the analysis model. You actually get better results by leaving the imputed values at impossible values, even though it's counter-intuitive.
\item
  Don't transform skewed variables. Likewise, when you transform a variable to meet normality assumptions before imputing, you not only are changing the distribution of that variable but the relationship between that variable and the others you use to impute. Doing so can lead to imputing outliers, creating more bias than just imputing the skewed variable.
\item
  Use more imputations. The advice for years has been that 5-10 imputations are adequate. And while this is true for unbiasedness, you can get inconsistent results if you run the multiple imputation more than once. \citep{Bodner_2008} recommends having as many imputations as the percentage of missing data. Since running more imputations isn't any more work for the data analyst, there's no reason not to.
\item
  Create multiplicative terms before imputing. When the analysis model contains a multiplicative term, like an interaction term or a quadratic, create the multiplicative terms first, then impute. Imputing first, and then creating the multiplicative terms actually biases the regression parameters of the multiplicative term \citep{von_Hippel_2009}
\end{enumerate}

\hypertarget{diagnosing-the-mechanism}{%
\section{Diagnosing the Mechanism}\label{diagnosing-the-mechanism}}

\hypertarget{mar-vs.-mnar}{%
\subsection{MAR vs.~MNAR}\label{mar-vs.-mnar}}

The only true way to distinguish between MNAR and MAR is to measure some of that missing data.

It's a common practice among professional surveyors to, for example, follow-up on a paper survey with phone calls to a group of the non-respondents and ask a few key survey items. This allows you to compare respondents to non-respondents.

If their responses on those key items differ by very much, that's good evidence that the data are MNAR.

However in most missing data situations, we can't get a hold of the missing data. So while we can't test it directly, we can examine patterns in the data get an idea of what's the most likely mechanism.

The first thing in diagnosing randomness of the missing data is to use your substantive scientific knowledge of the data and your field. The more sensitive the issue, the less likely people are to tell you. They're not going to tell you as much about their cocaine usage as they are about their phone usage.

Likewise, many fields have common research situations in which non-ignorable data is common. Educate yourself in your field's literature.

\hypertarget{mcar-vs.-mar}{%
\subsection{MCAR vs.~MAR}\label{mcar-vs.-mar}}

There is a very useful test for MCAR, Little's test.

A second technique is to create dummy variables for whether a variable is missing.

1 = missing 0 = observed

You can then run t-tests and chi-square tests between this variable and other variables in the data set to see if the missingness on this variable is related to the values of other variables.

For example, if women really are less likely to tell you their weight than men, a chi-square test will tell you that the percentage of missing data on the weight variable is higher for women than men.

\hypertarget{application-7}{%
\section{Application}\label{application-7}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(visdat)}
\FunctionTok{library}\NormalTok{(naniar)}
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{vis\_miss}\NormalTok{()}



\FunctionTok{ggplot}\NormalTok{(data, }\FunctionTok{aes}\NormalTok{(x, y)) }\SpecialCharTok{+}
  \FunctionTok{geom\_miss\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{( }\SpecialCharTok{\textasciitilde{}}\NormalTok{ group)}

\FunctionTok{gg\_miss\_var}\NormalTok{(data, }\AttributeTok{facet =}\NormalTok{ group)}

\FunctionTok{gg\_miss\_upset}\NormalTok{(data)}

\FunctionTok{gg\_miss\_fct}\NormalTok{(}\AttributeTok{x =}\NormalTok{ variable1, }\AttributeTok{fct =}\NormalTok{ variable2)}
\end{Highlighting}
\end{Shaded}

Read more on \href{https://tmb.njtierney.com/}{The Missing Book by Nicholas Tierney \& Allison Horst}

How many imputation:

\textbf{Usually 5}. (unless you have extremely high portion of missing, in which case you probably need to check your data again)

According to Rubin, the relative efficiency of an estimate based on \(m\) imputations to infinity imputation is approximately

\[
(1+\frac{\lambda}{m})^{-1}
\]

where \(\lambda\) is the rate of missing data

Example 50\% of missing data means an estimate based on 5 imputation has standard deviation that is only 5\% wider compared to an estimate based on infinity imputation\\
(\(\sqrt{1+0.5/5}=1.049\))

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(missForest)}

\CommentTok{\#load data}
\NormalTok{data }\OtherTok{\textless{}{-}}\NormalTok{ iris}

\CommentTok{\#Generate 10\% missing values at Random}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{iris.mis }\OtherTok{\textless{}{-}} \FunctionTok{prodNA}\NormalTok{(iris, }\AttributeTok{noNA =} \FloatTok{0.1}\NormalTok{)}

\CommentTok{\#remove categorical variables}
\NormalTok{iris.mis.cat }\OtherTok{\textless{}{-}}\NormalTok{ iris.mis}
\NormalTok{iris.mis }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(iris.mis, }\AttributeTok{select =} \SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(Species))}
\end{Highlighting}
\end{Shaded}

\hypertarget{imputation-with-mean-median-mode}{%
\subsection{Imputation with mean / median / mode}\label{imputation-with-mean-median-mode}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# whole data set}
\NormalTok{e1071}\SpecialCharTok{::}\FunctionTok{impute}\NormalTok{(iris.mis, }\AttributeTok{what =} \StringTok{"mean"}\NormalTok{)        }\CommentTok{\# replace with mean}
\NormalTok{e1071}\SpecialCharTok{::}\FunctionTok{impute}\NormalTok{(iris.mis, }\AttributeTok{what =} \StringTok{"median"}\NormalTok{)      }\CommentTok{\# replace with median}

\CommentTok{\# by variables}
\NormalTok{Hmisc}\SpecialCharTok{::}\FunctionTok{impute}\NormalTok{(iris.mis}\SpecialCharTok{$}\NormalTok{Sepal.Length, mean)    }\CommentTok{\# mean}
\NormalTok{Hmisc}\SpecialCharTok{::}\FunctionTok{impute}\NormalTok{(iris.mis}\SpecialCharTok{$}\NormalTok{Sepal.Length, median)  }\CommentTok{\# median}
\NormalTok{Hmisc}\SpecialCharTok{::}\FunctionTok{impute}\NormalTok{(iris.mis}\SpecialCharTok{$}\NormalTok{Sepal.Length, }\DecValTok{0}\NormalTok{)       }\CommentTok{\# replace specific number}
\end{Highlighting}
\end{Shaded}

check accuracy

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# library(DMwR2)}
\CommentTok{\# actuals \textless{}{-} iris$Sepal.Width[is.na(iris.mis$Sepal.Width)]}
\CommentTok{\# predicteds \textless{}{-} rep(mean(iris$Sepal.Width, na.rm=T), length(actuals))}
\CommentTok{\# regr.eval(actuals, predicteds)}
\end{Highlighting}
\end{Shaded}

\hypertarget{knn}{%
\subsection{KNN}\label{knn}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# library(DMwR2)}
\CommentTok{\# \# iris.mis[,!names(iris.mis) \%in\% c("Sepal.Length")]}
\CommentTok{\# \# data should be this line. But since knn cant work with 3 or less variables, }
\CommentTok{\# \# we need to use at least 4 variables.}
\CommentTok{\# }
\CommentTok{\# \# knn is not appropriate for categorical variables}
\CommentTok{\# knnOutput \textless{}{-}}
\CommentTok{\#   knnImputation(data = iris.mis.cat,}
\CommentTok{\#                 \#k = 10,}
\CommentTok{\#                 meth = "median" \# could use "median" or "weighAvg")  }
\CommentTok{\#                 \# should exclude the dependent variable: Sepal.Length}
\CommentTok{\#                 anyNA(knnOutput)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# library(DMwR2)}
\CommentTok{\# actuals \textless{}{-} iris$Sepal.Width[is.na(iris.mis$Sepal.Width)]}
\CommentTok{\# predicteds \textless{}{-} knnOutput[is.na(iris.mis$Sepal.Width), "Sepal.Width"]}
\CommentTok{\# regr.eval(actuals, predicteds)}
\end{Highlighting}
\end{Shaded}

Compared to MAPE (mean absolute percentage error) of mean imputation, we see almost always see improvements.

\hypertarget{rpart}{%
\subsection{rpart}\label{rpart}}

For categorical (factor) variables, \texttt{rpart} can handle

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rpart)}
\NormalTok{class\_mod }\OtherTok{\textless{}{-}}
  \FunctionTok{rpart}\NormalTok{(}
\NormalTok{    Species }\SpecialCharTok{\textasciitilde{}}\NormalTok{ . }\SpecialCharTok{{-}}\NormalTok{ Sepal.Length,}
    \AttributeTok{data =}\NormalTok{ iris.mis.cat[}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(iris.mis.cat}\SpecialCharTok{$}\NormalTok{Species),],}
    \AttributeTok{method =} \StringTok{"class"}\NormalTok{,}
    \AttributeTok{na.action =}\NormalTok{ na.omit}
\NormalTok{  )  }\CommentTok{\# since Species is a factor, and exclude dependent variable "Sepal.Length"}

\NormalTok{anova\_mod }\OtherTok{\textless{}{-}}
  \FunctionTok{rpart}\NormalTok{(}
\NormalTok{    Sepal.Width }\SpecialCharTok{\textasciitilde{}}\NormalTok{ . }\SpecialCharTok{{-}}\NormalTok{ Sepal.Length,}
    \AttributeTok{data =}\NormalTok{ iris.mis[}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(iris.mis}\SpecialCharTok{$}\NormalTok{Sepal.Width),],}
    \AttributeTok{method =} \StringTok{"anova"}\NormalTok{,}
    \AttributeTok{na.action =}\NormalTok{ na.omit}
\NormalTok{  )  }\CommentTok{\# since Sepal.Width is numeric.}


\NormalTok{species\_pred }\OtherTok{\textless{}{-}}
  \FunctionTok{predict}\NormalTok{(class\_mod, iris.mis.cat[}\FunctionTok{is.na}\NormalTok{(iris.mis.cat}\SpecialCharTok{$}\NormalTok{Species),])}


\NormalTok{width\_pred }\OtherTok{\textless{}{-}}
  \FunctionTok{predict}\NormalTok{(anova\_mod, iris.mis[}\FunctionTok{is.na}\NormalTok{(iris.mis}\SpecialCharTok{$}\NormalTok{Sepal.Width),])}
\end{Highlighting}
\end{Shaded}

\hypertarget{mice-multivariate-imputation-via-chained-equations}{%
\subsection{MICE (Multivariate Imputation via Chained Equations)}\label{mice-multivariate-imputation-via-chained-equations}}

Assumption: data are \protect\hyperlink{missing-at-random-mar}{MAR}

It imputes data per variable by specifying an imputation model for each variable

\textbf{Example}

We have \(X_1, X_2,..,X_k\). If \(X_1\) has missing data, then it is regressed on the rest of the variables. Same procedure applies if \(X_2\) has missing data. Then, predicted values are used in place of missing values.

By default,

\begin{itemize}
\tightlist
\item
  \textbf{Continuous variables} use linear regression.
\item
  \textbf{Categorical Variables} use logistic regression.
\end{itemize}

Methods in \protect\hyperlink{mice-multivariate-imputation-via-chained-equations}{MICE}:

\begin{itemize}
\tightlist
\item
  PMM (Predictive Mean Matching) -- For numeric variables
\item
  \texttt{logreg}(Logistic Regression) -- For Binary Variables( with 2 levels)
\item
  \texttt{polyreg} (Bayesian polytomous regression) -- For Factor Variables (\textgreater= 2 levels)
\item
  Proportional odds model (ordered, \textgreater= 2 levels)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load package}
\FunctionTok{library}\NormalTok{(mice)}
\FunctionTok{library}\NormalTok{(VIM)}

\CommentTok{\# check missing values}
\FunctionTok{md.pattern}\NormalTok{(iris.mis)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{11-imputation_files/figure-latex/unnamed-chunk-19-1} \end{center}

\begin{verbatim}
#>     Sepal.Width Sepal.Length Petal.Length Petal.Width   
#> 100           1            1            1           1  0
#> 15            1            1            1           0  1
#> 8             1            1            0           1  1
#> 2             1            1            0           0  2
#> 11            1            0            1           1  1
#> 1             1            0            1           0  2
#> 1             1            0            0           1  2
#> 1             1            0            0           0  3
#> 7             0            1            1           1  1
#> 3             0            1            0           1  2
#> 1             0            0            1           1  2
#>              11           15           15          19 60

#plot the missing values
aggr(
  iris.mis,
  col = mdc(1:2),
  numbers = TRUE,
  sortVars = TRUE,
  labels = names(iris.mis),
  cex.axis = .7,
  gap = 3,
  ylab = c("Proportion of missingness", "Missingness Pattern")
)
\end{verbatim}

\begin{center}\includegraphics[width=0.9\linewidth]{11-imputation_files/figure-latex/unnamed-chunk-19-2} \end{center}

\begin{verbatim}
#> 
#>  Variables sorted by number of missings: 
#>      Variable      Count
#>   Petal.Width 0.12666667
#>  Sepal.Length 0.10000000
#>  Petal.Length 0.10000000
#>   Sepal.Width 0.07333333


mice_plot <- aggr(
  iris.mis,
  col = c('navyblue', 'yellow'),
  numbers = TRUE,
  sortVars = TRUE,
  labels = names(iris.mis),
  cex.axis = .7,
  gap = 3,
  ylab = c("Missing data", "Pattern")
)
\end{verbatim}

\begin{center}\includegraphics[width=0.9\linewidth]{11-imputation_files/figure-latex/unnamed-chunk-19-3} \end{center}

\begin{verbatim}
#> 
#>  Variables sorted by number of missings: 
#>      Variable      Count
#>   Petal.Width 0.12666667
#>  Sepal.Length 0.10000000
#>  Petal.Length 0.10000000
#>   Sepal.Width 0.07333333
\end{verbatim}

Impute Data

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{imputed\_Data }\OtherTok{\textless{}{-}}
    \FunctionTok{mice}\NormalTok{(}
\NormalTok{        iris.mis,}
        \AttributeTok{m =} \DecValTok{5}\NormalTok{, }\CommentTok{\# number of imputed datasets}
        \AttributeTok{maxit =} \DecValTok{50}\NormalTok{, }\CommentTok{\# number of iterations taken to impute missing values}
        \AttributeTok{method =} \StringTok{\textquotesingle{}pmm\textquotesingle{}}\NormalTok{, }\CommentTok{\# method used in imputation. }
        \CommentTok{\# Here, we used predictive mean matching}
        
        
        \CommentTok{\# other methods can be }
        \CommentTok{\# "pmm": Predictive mean matching}
        \CommentTok{\# "midastouch" : weighted predictive mean matching}
        \CommentTok{\# "sample": Random sample from observed values}
        \CommentTok{\# "cart": classification and regression trees}
        \CommentTok{\# "rf": random forest imputations.}
        \CommentTok{\# "2lonly.pmm": Level{-}2 class predictive mean matching}
        \CommentTok{\# Other methods based on whether variables are }
        \CommentTok{\# (1) numeric, (2) binary, (3) ordered, (4), unordered}
        \AttributeTok{seed =} \DecValTok{500}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(imputed\_Data)}
\CommentTok{\#\textgreater{} Class: mids}
\CommentTok{\#\textgreater{} Number of multiple imputations:  5 }
\CommentTok{\#\textgreater{} Imputation methods:}
\CommentTok{\#\textgreater{} Sepal.Length  Sepal.Width Petal.Length  Petal.Width }
\CommentTok{\#\textgreater{}        "pmm"        "pmm"        "pmm"        "pmm" }
\CommentTok{\#\textgreater{} PredictorMatrix:}
\CommentTok{\#\textgreater{}              Sepal.Length Sepal.Width Petal.Length Petal.Width}
\CommentTok{\#\textgreater{} Sepal.Length            0           1            1           1}
\CommentTok{\#\textgreater{} Sepal.Width             1           0            1           1}
\CommentTok{\#\textgreater{} Petal.Length            1           1            0           1}
\CommentTok{\#\textgreater{} Petal.Width             1           1            1           0}

\CommentTok{\#make a density plot}
\FunctionTok{densityplot}\NormalTok{(imputed\_Data)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{11-imputation_files/figure-latex/unnamed-chunk-21-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#the red (imputed values) should be similar to the blue (observed)}
\end{Highlighting}
\end{Shaded}

Check imputed dataset

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 1st dataset }
\NormalTok{completeData }\OtherTok{\textless{}{-}} \FunctionTok{complete}\NormalTok{(imputed\_Data,}\DecValTok{1}\NormalTok{)}

\CommentTok{\# 2nd dataset}
\FunctionTok{complete}\NormalTok{(imputed\_Data,}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Regression model using imputed datasets

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# regression model}
\NormalTok{fit }\OtherTok{\textless{}{-}}
  \FunctionTok{with}\NormalTok{(}\AttributeTok{data =}\NormalTok{ imputed\_Data,}
       \AttributeTok{exp =} \FunctionTok{lm}\NormalTok{(Sepal.Width }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Sepal.Length }\SpecialCharTok{+}\NormalTok{ Petal.Width))}

\CommentTok{\#combine results of all 5 models}
\NormalTok{combine }\OtherTok{\textless{}{-}} \FunctionTok{pool}\NormalTok{(fit)}
\FunctionTok{summary}\NormalTok{(combine)}
\CommentTok{\#\textgreater{}           term   estimate  std.error statistic       df      p.value}
\CommentTok{\#\textgreater{} 1  (Intercept)  1.8963130 0.32453912  5.843095 131.0856 3.838556e{-}08}
\CommentTok{\#\textgreater{} 2 Sepal.Length  0.2974293 0.06679204  4.453066 130.2103 1.802241e{-}05}
\CommentTok{\#\textgreater{} 3  Petal.Width {-}0.4811603 0.07376809 {-}6.522608 108.8253 2.243032e{-}09}
\end{Highlighting}
\end{Shaded}

\hypertarget{amelia}{%
\subsection{Amelia}\label{amelia}}

\begin{itemize}
\tightlist
\item
  Use bootstrap based EMB algorithm (faster and robust to impute many variables including cross sectional, time series data etc)
\item
  Use parallel imputation feature using multicore CPUs.
\end{itemize}

Assumptions

\begin{itemize}
\tightlist
\item
  All variables follow Multivariate Normal Distribution (MVN). Hence, this package works best when data is MVN, or transformation to normality.
\item
  Missing data is \protect\hyperlink{missing-at-random-mar}{Missing at Random (MAR)}
\end{itemize}

Steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  m bootstrap samples and applies EMB algorithm to each sample. Then we have m different estimates of mean and variances.
\item
  the first set of estimates are used to impute first set of missing values using regression, then second set of estimates are used for second set and so on.
\end{enumerate}

However, \protect\hyperlink{amelia}{Amelia} is different from \protect\hyperlink{mice-multivariate-imputation-via-chained-equations}{MICE}

\begin{itemize}
\tightlist
\item
  MICE imputes data on variable by variable basis whereas MVN uses a joint modeling approach based on multivariate normal distribution.
\item
  MICE can handle different types of variables while the variables in MVN need to be normally distributed or transformed to approximate normality.
\item
  MICE can manage imputation of variables defined on a subset of data whereas MVN cannot.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(Amelia)}
\FunctionTok{data}\NormalTok{(}\StringTok{"iris"}\NormalTok{)}
\CommentTok{\#seed 10\% missing values}
\NormalTok{iris.mis }\OtherTok{\textless{}{-}} \FunctionTok{prodNA}\NormalTok{(iris, }\AttributeTok{noNA =} \FloatTok{0.1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  idvars -- keep all ID variables and other variables which you don't want to impute
\item
  noms -- keep nominal variables here
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#specify columns and run amelia}
\NormalTok{amelia\_fit }\OtherTok{\textless{}{-}}
  \FunctionTok{amelia}\NormalTok{(iris.mis,}
         \AttributeTok{m =} \DecValTok{5}\NormalTok{,}
         \AttributeTok{parallel =} \StringTok{"multicore"}\NormalTok{,}
         \AttributeTok{noms =} \StringTok{"Species"}\NormalTok{)}
\CommentTok{\#\textgreater{} {-}{-} Imputation 1 {-}{-}}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   1  2  3  4  5  6  7  8}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} {-}{-} Imputation 2 {-}{-}}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   1  2  3  4  5  6  7  8}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} {-}{-} Imputation 3 {-}{-}}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   1  2  3  4  5}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} {-}{-} Imputation 4 {-}{-}}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   1  2  3  4  5  6  7}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} {-}{-} Imputation 5 {-}{-}}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   1  2  3  4  5  6  7}

\CommentTok{\# access imputed outputs}
\CommentTok{\# amelia\_fit$imputations[[1]]}
\end{Highlighting}
\end{Shaded}

\hypertarget{missforest}{%
\subsection{missForest}\label{missforest}}

\begin{itemize}
\tightlist
\item
  an implementation of random forest algorithm (a non parametric imputation method applicable to various variable types). Hence, no assumption about function form of f.~Instead, it tries to estimate f such that it can be as close to the data points as possible.
\item
  builds a random forest model for each variable. Then it uses the model to predict missing values in the variable with the help of observed values.
\item
  It yields out of bag imputation error estimate. Moreover, it provides high level of control on imputation process.
\item
  Since bagging works well on categorical variable too, we don't need to remove them here.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(missForest)}
\CommentTok{\#impute missing values, using all parameters as default values}
\NormalTok{iris.imp }\OtherTok{\textless{}{-}} \FunctionTok{missForest}\NormalTok{(iris.mis)}
\CommentTok{\# check imputed values}
\CommentTok{\# iris.imp$ximp}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  check imputation error
\item
  NRMSE is normalized mean squared error. It is used to represent error derived from imputing continuous values.
\item
  PFC (proportion of falsely classified) is used to represent error derived from imputing categorical values.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris.imp}\SpecialCharTok{$}\NormalTok{OOBerror}
\CommentTok{\#\textgreater{}      NRMSE        PFC }
\CommentTok{\#\textgreater{} 0.13631893 0.04477612}

\CommentTok{\#comparing actual data accuracy}
\NormalTok{iris.err }\OtherTok{\textless{}{-}} \FunctionTok{mixError}\NormalTok{(iris.imp}\SpecialCharTok{$}\NormalTok{ximp, iris.mis, iris)}
\NormalTok{iris.err}
\CommentTok{\#\textgreater{}     NRMSE       PFC }
\CommentTok{\#\textgreater{} 0.1501524 0.0625000}
\end{Highlighting}
\end{Shaded}

This means categorical variables are imputed with 5\% error and continuous variables are imputed with 14\% error.

This can be improved by tuning the values of \texttt{mtry} and \texttt{ntree} parameter.

\begin{itemize}
\tightlist
\item
  \texttt{mtry} refers to the number of variables being randomly sampled at each split.
\item
  \texttt{ntree} refers to number of trees to grow in the forest.
\end{itemize}

\hypertarget{hmisc}{%
\subsection{Hmisc}\label{hmisc}}

\begin{itemize}
\tightlist
\item
  \texttt{impute()} function imputes missing value using user defined statistical method (mean, max, mean). It's default is median.\\
\item
  \texttt{aregImpute()} allows mean imputation using additive regression, bootstrapping, and predictive mean matching.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In bootstrapping, different bootstrap resamples are used for each of multiple imputations. Then, a flexible additive model (non parametric regression method) is fitted on samples taken with replacements from original data and missing values (acts as dependent variable) are predicted using non-missing values (independent variable).\\
\item
  it uses predictive mean matching (default) to impute missing values. Predictive mean matching works well for continuous and categorical (binary \& multi-level) without the need for computing residuals and maximum likelihood fit.
\end{enumerate}

\textbf{Note}

\begin{itemize}
\tightlist
\item
  For predicting categorical variables, Fisher's optimum scoring method is used.
\item
  \texttt{Hmisc} automatically recognizes the variables types and uses bootstrap sample and predictive mean matching to impute missing values.
\item
  \texttt{missForest} can outperform \texttt{Hmisc} if the observed variables have sufficient information.
\end{itemize}

Assumption

\begin{itemize}
\tightlist
\item
  linearity in the variables being predicted.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(Hmisc)}
\CommentTok{\# impute with mean value}
\NormalTok{iris.mis}\SpecialCharTok{$}\NormalTok{imputed\_age }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(iris.mis, }\FunctionTok{impute}\NormalTok{(Sepal.Length, mean))}

\CommentTok{\# impute with random value}
\NormalTok{iris.mis}\SpecialCharTok{$}\NormalTok{imputed\_age2 }\OtherTok{\textless{}{-}}
  \FunctionTok{with}\NormalTok{(iris.mis, }\FunctionTok{impute}\NormalTok{(Sepal.Length, }\StringTok{\textquotesingle{}random\textquotesingle{}}\NormalTok{))}

\CommentTok{\# could also use min, max, median to impute missing value}

\CommentTok{\# using argImpute}
\NormalTok{impute\_arg }\OtherTok{\textless{}{-}}
  \CommentTok{\# argImpute() automatically identifies the variable type }
  \CommentTok{\# and treats them accordingly.}
  \FunctionTok{aregImpute}\NormalTok{(}
    \SpecialCharTok{\textasciitilde{}}\NormalTok{ Sepal.Length }\SpecialCharTok{+}\NormalTok{ Sepal.Width }\SpecialCharTok{+}\NormalTok{ Petal.Length }\SpecialCharTok{+}\NormalTok{ Petal.Width }\SpecialCharTok{+}
\NormalTok{      Species,}
    \AttributeTok{data =}\NormalTok{ iris.mis,}
    \AttributeTok{n.impute =} \DecValTok{5}
\NormalTok{  ) }
\CommentTok{\#\textgreater{} Iteration 1 }
\NormalTok{Iteration }\DecValTok{2} 
\NormalTok{Iteration }\DecValTok{3} 
\NormalTok{Iteration }\DecValTok{4} 
\NormalTok{Iteration }\DecValTok{5} 
\NormalTok{Iteration }\DecValTok{6} 
\NormalTok{Iteration }\DecValTok{7} 
\NormalTok{Iteration }\DecValTok{8} 

\NormalTok{impute\_arg }\CommentTok{\# R{-}squares are for predicted missing values.}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Multiple Imputation using Bootstrap and PMM}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} aregImpute(formula = \textasciitilde{}Sepal.Length + Sepal.Width + Petal.Length + }
\CommentTok{\#\textgreater{}     Petal.Width + Species, data = iris.mis, n.impute = 5)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} n: 150   p: 5    Imputations: 5      nk: 3 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of NAs:}
\CommentTok{\#\textgreater{} Sepal.Length  Sepal.Width Petal.Length  Petal.Width      Species }
\CommentTok{\#\textgreater{}           11           11           13           24           16 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}              type d.f.}
\CommentTok{\#\textgreater{} Sepal.Length    s    2}
\CommentTok{\#\textgreater{} Sepal.Width     s    2}
\CommentTok{\#\textgreater{} Petal.Length    s    2}
\CommentTok{\#\textgreater{} Petal.Width     s    2}
\CommentTok{\#\textgreater{} Species         c    2}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Transformation of Target Variables Forced to be Linear}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} R{-}squares for Predicting Non{-}Missing Values for Each Variable}
\CommentTok{\#\textgreater{} Using Last Imputations of Predictors}
\CommentTok{\#\textgreater{} Sepal.Length  Sepal.Width Petal.Length  Petal.Width      Species }
\CommentTok{\#\textgreater{}        0.907        0.660        0.978        0.963        0.993}

\CommentTok{\# check imputed variable Sepal.Length}
\NormalTok{impute\_arg}\SpecialCharTok{$}\NormalTok{imputed}\SpecialCharTok{$}\NormalTok{Sepal.Length}
\CommentTok{\#\textgreater{}     [,1] [,2] [,3] [,4] [,5]}
\CommentTok{\#\textgreater{} 19   5.2  5.2  5.2  5.8  5.7}
\CommentTok{\#\textgreater{} 21   5.1  5.0  5.1  5.7  5.4}
\CommentTok{\#\textgreater{} 31   4.8  5.0  5.2  5.0  4.8}
\CommentTok{\#\textgreater{} 35   4.6  4.9  4.9  4.9  4.8}
\CommentTok{\#\textgreater{} 49   5.0  5.1  5.1  5.1  5.1}
\CommentTok{\#\textgreater{} 62   6.2  5.7  6.0  6.4  5.6}
\CommentTok{\#\textgreater{} 65   5.5  5.5  5.2  5.8  5.5}
\CommentTok{\#\textgreater{} 67   6.5  5.8  5.8  6.3  6.5}
\CommentTok{\#\textgreater{} 82   5.2  5.1  5.7  5.8  5.5}
\CommentTok{\#\textgreater{} 113  6.4  6.5  7.4  7.2  6.3}
\CommentTok{\#\textgreater{} 122  6.2  5.8  5.5  5.8  6.7}
\end{Highlighting}
\end{Shaded}

\hypertarget{mi}{%
\subsection{mi}\label{mi}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  allows graphical diagnostics of imputation models and convergence of imputation process.
\item
  uses Bayesian version of regression models to handle issue of separation.
\item
  automatically detects irregularities in data (e.g., high collinearity among variables).
\item
  adds noise to imputation process to solve the problem of additive constraints.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(mi)}
\CommentTok{\# default values of parameters}
\CommentTok{\# 1. rand.imp.method as â€œbootstrapâ€}
\CommentTok{\# 2. n.imp (number of multiple imputations) as 3}
\CommentTok{\# 3. n.iter ( number of iterations) as 30}
\NormalTok{mi\_data }\OtherTok{\textless{}{-}} \FunctionTok{mi}\NormalTok{(iris.mis, }\AttributeTok{seed =} \DecValTok{335}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(mi\_data)}
\end{Highlighting}
\end{Shaded}

\hypertarget{data}{%
\chapter{Data}\label{data}}

There are multiple ways to categorize data. For example,

\begin{itemize}
\tightlist
\item
  Qualitative vs.~Quantitative:
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5556}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.4444}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Qualitative
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Quantitative
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
in-depth interviews, documents, focus groups, case study, ethnography. open-ended questions. observations in words & experiments, observation in words, survey with closed-end questions, structured interviews \\
language, descriptive & quantities, numbers \\
Text-based & Numbers-based \\
Subjective & Objectivity \\
\end{longtable}

\hypertarget{cross-sectional}{%
\section{Cross-Sectional}\label{cross-sectional}}

\hypertarget{time-series-1}{%
\section{Time Series}\label{time-series-1}}

\[
y_t = \beta_0 + x_{t1}\beta_1 + x_{t2}\beta_2 + ... + x_{t(k-1)}\beta_{k-1} + \epsilon_t
\]

Examples

\begin{itemize}
\item
  Static Model

  \begin{itemize}
  \tightlist
  \item
    \(y_t=\beta_0 + x_1\beta_1 + x_2\beta_2 - x_3\beta_3 - \epsilon_t\)
  \end{itemize}
\item
  Finite Distributed Lag model

  \begin{itemize}
  \tightlist
  \item
    \(y_t=\beta_0 + pe_t\delta_0 + pe_{t-1}\delta_1 +pe_{t-2}\delta_2 + \epsilon_t\)
  \item
    \textbf{Long Run Propensity (LRP)} is \(LRP = \delta_0 + \delta_1 + \delta_2\)
  \end{itemize}
\item
  Dynamic Model

  \begin{itemize}
  \tightlist
  \item
    \(GDP_t = \beta_0 + \beta_1GDP_{t-1} - \epsilon_t\)
  \end{itemize}
\end{itemize}

\protect\hyperlink{finite-sample-properties}{Finite Sample Properties} for \protect\hyperlink{time-series-1}{Time Series}:

\begin{itemize}
\tightlist
\item
  A1-A3: OLS is unbiased
\item
  A1-A4: usual standard errors are consistent and \protect\hyperlink{gauss-markov-theorem}{Gauss-Markov Theorem} holds (OLS is BLUE)
\item
  A1-A6, A6: Finite Sample \protect\hyperlink{wald-test-GLMM}{Wald Test} (t-test and F-test) are valid
\end{itemize}

\protect\hyperlink{a3-exogeneity-of-independent-variables}{A3} might not hold under time series setting

\begin{itemize}
\tightlist
\item
  Spurious Time Trend - solvable
\item
  \protect\hyperlink{a3-exogeneity-of-independent-variables}{Strict} vs Contemporaneous Exogeneity - not solvable
\end{itemize}

In time series data, there are many processes:

\begin{itemize}
\tightlist
\item
  Autoregressive model of order p: AR(p)
\item
  Moving average model of order q: MA(q)
\item
  Autoregressive model of order p and moving average model of order q: ARMA(p,q)
\item
  Autoregressive conditional heteroskedasticity model of order p: ARCH(p)
\item
  Generalized Autoregressive conditional heteroskedasticity of orders p and q; GARCH(p.q)
\end{itemize}

\hypertarget{deterministic-time-trend}{%
\subsection{Deterministic Time trend}\label{deterministic-time-trend}}

Both the dependent and independent variables are trending over time

\textbf{Spurious Time Series Regression}

\[
y_t = \alpha_0 + t\alpha_1 + v_t
\]

and x takes the form

\[
x_t = \lambda_0 + t\lambda_1 + u_t
\]

\begin{itemize}
\tightlist
\item
  \(\alpha_1 \neq 0\) and \(\lambda_1 \neq 0\)
\item
  \(v_t\) and \(u_t\) are independent
\item
  there is no relationship between \(y_t\) and \(x_t\)
\end{itemize}

If we estimate the regression,

\[
y_t = \beta_0 + x_t\beta_1 + \epsilon_t
\]

so the true \(\beta_1=0\)

\begin{itemize}
\tightlist
\item
  Inconsistent: \(plim(\hat{\beta}_1)=\frac{\alpha_1}{\lambda_1}\)
\item
  Invalid Inference: \(|t| \to^d \infty\) for \(H_0: \beta_1=0\), will always reject the null as \(n \to \infty\)
\item
  Uninformative \(R^2\): \(plim(R^2) = 1\) will be able to perfectly predict as \(n \to \infty\)
\end{itemize}

We can rewrite the equation as

\[
\begin{aligned}
y_t &=\beta_0 + \beta_1x_t+\epsilon_t \\
\epsilon_t &= \alpha_1t + v_t
\end{aligned}
\]

where \(\beta_0 = \alpha_0\) and \(\beta_1=0\). Since \(x_t\) is a deterministic function of time, \(\epsilon_t\) is correlated with \(x_t\) and we have the usual omitted variable bias.\\
Even when \(y_t\) and \(x_t\) are related (\(\beta_1 \neq 0\)) but they are both trending over time, we still get spurious results with the simple regression on \(y_t\) on \(x_t\)

\textbf{Solutions to Spurious Trend}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Include time trend \(t\) as an additional control

  \begin{itemize}
  \tightlist
  \item
    consistent parameter estimates and valid inference
  \end{itemize}
\item
  Detrend both dependent and independent variables and then regress the detrended outcome on detrended independent variables (i.e., regress residuals \(\hat{u}_t\) on residuals \(\hat{v}_t\))

  \begin{itemize}
  \item
    Detrending is the same as partialing out in the \protect\hyperlink{frisch-waugh-lovell-theorem}{Frisch-Waugh-Lovell Theorem}

    \begin{itemize}
    \tightlist
    \item
      Could allow for non-linear time trends by including \(t\) \(t^2\), and \(\exp(t)\)
    \item
      Allow for seasonality by including indicators for relevant ``seasons'' (quarters, months, weeks).
    \end{itemize}
  \end{itemize}
\end{enumerate}

\protect\hyperlink{a3-exogeneity-of-independent-variables}{A3} does not hold under:

\begin{itemize}
\item
  \protect\hyperlink{feedback-effect}{Feedback Effect}

  \begin{itemize}
  \tightlist
  \item
    \(\epsilon_t\) influences next period's independent variables
  \end{itemize}
\item
  \protect\hyperlink{dynamic-specification}{Dynamic Specification}

  \begin{itemize}
  \tightlist
  \item
    include last time period outcome as an explanatory variable
  \end{itemize}
\item
  \protect\hyperlink{dynamically-complete}{Dynamically Complete}

  \begin{itemize}
  \tightlist
  \item
    For finite distrusted lag model, the number of lags needs to be absolutely correct.
  \end{itemize}
\end{itemize}

\hypertarget{feedback-effect}{%
\subsection{Feedback Effect}\label{feedback-effect}}

\[
y_t = \beta_0 + x_t\beta_1 + \epsilon_t
\]

\protect\hyperlink{a3-exogeneity-of-independent-variables}{A3}

\[
E(\epsilon_t|\mathbf{X})= E(\epsilon_t| x_1,x_2, ...,x_t,x_{t+1},...,x_T)
\]

will not equal 0, because \(y_t\) will likely influence \(x_{t+1},..,x_T\)

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{a3-exogeneity-of-independent-variables}{A3} is violated because we require the error to be uncorrelated with all time observation of the independent regressors (\textbf{strict exogeneity})
\end{itemize}

\hypertarget{dynamic-specification}{%
\subsection{Dynamic Specification}\label{dynamic-specification}}

\[
y_t = \beta_0 + y_{t-1}\beta_1 + \epsilon_t
\]

\[
E(\epsilon_t|\mathbf{X})= E(\epsilon_t| y_1,y_2, ...,y_t,y_{t+1},...,y_T)
\]

will not equal 0, because \(y_t\) and \(\epsilon_t\) are inherently correlated

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{a3-exogeneity-of-independent-variables}{A3} is violated because we require the error to be uncorrelated with all time observation of the independent regressors (\textbf{strict exogeneity})
\item
  \protect\hyperlink{dynamic-specification}{Dynamic Specification} is not allowed under \protect\hyperlink{a3-exogeneity-of-independent-variables}{A3}
\end{itemize}

\hypertarget{dynamically-complete}{%
\subsection{Dynamically Complete}\label{dynamically-complete}}

\[
y_t = \beta_0 + x_t\delta_0 + x_{t-1}\delta_1 + \epsilon_t
\]

\[
E(\epsilon_t|\mathbf{X})= E(\epsilon_t| x_1,x_2, ...,x_t,x_{t+1},...,x_T)
\]

will not equal 0, because if we did not include enough lags, \(x_{t-2}\) and \(\epsilon_t\) are correlated

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{a3-exogeneity-of-independent-variables}{A3} is violated because we require the error to be uncorrelated with all time observation of the independent regressors (strict exogeneity)
\item
  Can be corrected by including more lags (but when stop? )
\end{itemize}

Without \protect\hyperlink{a3-exogeneity-of-independent-variables}{A3}

\begin{itemize}
\tightlist
\item
  OLS is biased
\item
  \protect\hyperlink{gauss-markov-theorem}{Gauss-Markov Theorem}
\item
  \protect\hyperlink{finite-sample-properties}{Finite Sample Properties} are invalid
\end{itemize}

then, we can

\begin{itemize}
\tightlist
\item
  Focus on \protect\hyperlink{large-sample-properties}{Large Sample Properties}
\item
  Can use \protect\hyperlink{a3a}{A3a} instead of \protect\hyperlink{a3-exogeneity-of-independent-variables}{A3}
\end{itemize}

\protect\hyperlink{a3a}{A3a} in time series become

\[
A3a: E(\mathbf{x}_t'\epsilon_t)= 0
\]

only the regressors in this time period need to be independent from the error in this time period (\textbf{Contemporaneous Exogeneity})

\begin{itemize}
\tightlist
\item
  \(\epsilon_t\) can be correlated with \(...,x_{t-2},x_{t-1},x_{t+1}, x_{t+2},...\)
\item
  can have a dynamic specification \(y_t = \beta_0 + y_{t-1}\beta_1 + \epsilon_t\)
\end{itemize}

Deriving \protect\hyperlink{large-sample-properties}{Large Sample Properties} for Time Series

\begin{itemize}
\item
  Assumptions \protect\hyperlink{a1-linearity}{A1}, \protect\hyperlink{a2-full-rank}{A2}, \protect\hyperlink{a3a}{A3a}
\item
  \protect\hyperlink{weak-law}{Weak Law} and \protect\hyperlink{central-limit-theorem}{Central Limit Theorem} depend on \protect\hyperlink{a5-data-generation-random-sampling}{A5}

  \begin{itemize}
  \tightlist
  \item
    \(x_t\) and \(\epsilon_t\) are dependent over t
  \item
    without \protect\hyperlink{weak-law}{Weak Law} or \protect\hyperlink{central-limit-theorem}{Central Limit Theorem} depend on \protect\hyperlink{a5-data-generation-random-sampling}{A5}, we cannot have \protect\hyperlink{large-sample-properties}{Large Sample Properties} for \protect\hyperlink{ordinary-least-squares}{OLS}
  \item
    Instead of \protect\hyperlink{a5-data-generation-random-sampling}{A5}, we consider \protect\hyperlink{a5a}{A5a}
  \end{itemize}
\item
  Derivation of the Asymptotic Variance depends on \protect\hyperlink{a4-homoskedasticity}{A4}

  \begin{itemize}
  \tightlist
  \item
    time series setting introduces \textbf{Serial Correlation}: \(Cov(\epsilon_t, \epsilon_s) \neq 0\)
  \end{itemize}
\end{itemize}

under \protect\hyperlink{a1-linearity}{A1}, \protect\hyperlink{a2-full-rank}{A2}, \protect\hyperlink{a3a}{A3a}, and \protect\hyperlink{a5a}{A5a}, \protect\hyperlink{ordinary-least-squares}{OLS estimator} is \textbf{consistent}, and \textbf{asymptotically normal}

\hypertarget{highly-persistent-data}{%
\subsection{Highly Persistent Data}\label{highly-persistent-data}}

If \(y_t, \mathbf{x}_t\) are not weakly dependent stationary process

\begin{itemize}
\item
  \(y_t\) and \(y_{t-h}\) are not almost independent for large h
\item
  \protect\hyperlink{a5a}{A5a} does not hold and OLS is not \textbf{consistent} and does not have a limiting distribution.
\item
  Example + Random Walk \(y_t = y_{t-1} + u_t\) + Random Walk with a drift: \(y_t = \alpha+ y_{t-1} + u_t\)
\end{itemize}

\textbf{Solution} First difference is a stationary process

\[
y_t - y_{t-1} = u_t
\]

\begin{itemize}
\tightlist
\item
  If \(u_t\) is a weakly dependent process (also called integrated of order 0) then \(y_t\) is said to be difference-stationary process (integrated of order 1)
\item
  For regression, if \(\{y_t, \mathbf{x}_t \}\) are random walks (integrated at order 1), can consistently estimate the first difference equation
\end{itemize}

\[
\begin{aligned}
y_t - y_{t-1} &= (\mathbf{x}_t - \mathbf{x}_{t-1}\beta + \epsilon_t - \epsilon_{t-1}) \\
\Delta y_t &= \Delta \mathbf{x}\beta + \Delta u_t
\end{aligned}
\]

\textbf{Unit Root Test}

\[
y_t = \alpha + \alpha y_{t-1} + u_t
\]

tests if \(\rho=1\) (integrated of order 1)\\

\begin{itemize}
\tightlist
\item
  Under the null \(H_0: \rho = 1\), OLS is not consistent or asymptotically normal.
\item
  Under the alternative \(H_a: \rho < 1\), OLS is consistent and asymptotically normal.
\item
  usual t-test is not valid, will need to use the transformed equation to produce a valid test.
\end{itemize}

\textbf{Dickey-Fuller Test} \[
\Delta y_t= \alpha + \theta y_{t-1} + v_t
\] where \(\theta = \rho -1\)\\

\begin{itemize}
\tightlist
\item
  \(H_0: \theta = 0\) and \(H_a: \theta < 0\)
\item
  Under the null, \(\Delta y_t\) is weakly dependent but \(y_{t-1}\) is not.
\item
  Dickey and Fuller derived the non-normal asymptotic distribution. If you reject the null then \(y_t\) is not a random walk.
\end{itemize}

Concerns with the standard Dickey Fuller Test\\
1. Only considers a fairly simplistic dynamic relationship

\[
\Delta y_t = \alpha + \theta y_{t-1} + \gamma_1 \Delta_{t-1} + ..+ \gamma_p \Delta_{t-p} +v_t
\]

\begin{itemize}
\tightlist
\item
  with one additional lag, under the null \(\Delta_{y_t}\) is an AR(1) process and under the alternative \(y_t\) is an AR(2) process.
\item
  Solution: include lags of \(\Delta_{y_t}\) as controls.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Does not allow for time trend \[
  \Delta y_t = \alpha + \theta y_{t-1} + \delta t + v_t
  \]
\end{enumerate}

\begin{itemize}
\tightlist
\item
  allows \(y_t\) to have a quadratic relationship with \(t\)
\item
  Solution: include time trend (changes the critical values).
\end{itemize}

\textbf{Adjusted Dickey-Fuller Test} \[
\Delta y_t = \alpha + \theta y_{t-1} + \delta t + \gamma_1 \Delta y_{t-1} + ... + \gamma_p \Delta y_{t-p} + v_t
\] where \(\theta = 1 - \rho\)\\

\begin{itemize}
\tightlist
\item
  \(H_0: \theta_1 = 0\) and \(H_a: \theta_1 < 0\)
\item
  Under the null, \(\Delta y_t\) is weakly dependent but \(y_{t-1}\) is not
\item
  Critical values are different with the time trend, if you reject the null then \(y_t\) is not a random walk.
\end{itemize}

\hypertarget{newey-west-standard-errors}{%
\paragraph{Newey West Standard Errors}\label{newey-west-standard-errors}}

If \protect\hyperlink{a4-homoskedasticity}{A4} does not hold, we can use Newey West Standard Errors (HAC - Heteroskedasticity Autocorrelation Consistent)

\[
\hat{B} = T^{-1} \sum_{t=1}^{T} e_t^2 \mathbf{x'_tx_t} + \sum_{h=1}^{g}(1-\frac{h}{g+1})T^{-1}\sum_{t=h+1}^{T} e_t e_{t-h}(\mathbf{x_t'x_{t-h}+ x_{t-h}'x_t})
\]

\begin{itemize}
\item
  estimates the covariances up to a distance g part
\item
  downweights to insure \(\hat{B}\) is PSD
\item
  How to choose g:

  \begin{itemize}
  \tightlist
  \item
    For yearly data: \(g = 1\) or 2 is likely to account for most of the correlation
  \item
    For quarterly or monthly data: g should be larger (\$g = 4\$ or 8 for quarterly and \(g = 12\) or 14 for monthly)
  \item
    can also take integer part of \(4(T/100)^{2/9}\) or integer part of \(T^{1/4}\)
  \end{itemize}
\end{itemize}

\textbf{Testing for Serial Correlation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Run OLS regression of \(y_t\) on \(\mathbf{x_t}\) and obtain residuals \(e_t\)
\item
  Run OLS regression of \(e_t\) on \(\mathbf{x}_t, e_{t-1}\) and test whether coefficient on \(e_{t-1}\) is significant.
\item
  Reject the null of no serial correlation if the coefficient is significant at the 5\% level.

  \begin{itemize}
  \tightlist
  \item
    Test using heteroskedastic robust standard errors
  \item
    can include \(e_{t-2},e_{t-3},..\) in step 2 to test for higher order serial correlation (t-test would now be an F-test of joint significance)
  \end{itemize}
\end{enumerate}

\hypertarget{repeated-cross-sections}{%
\section{Repeated Cross Sections}\label{repeated-cross-sections}}

For each time point (day, month, year, etc.), a set of data is sampled. This set of data can be different among different time points.

For example, you can sample different groups of students each time you survey.

Allowing structural change in pooled cross section

\[
y_i = \mathbf{x}_i \beta + \delta_1 y_1 + ... + \delta_T y_T + \epsilon_i
\]

Dummy variables for all but one time period

\begin{itemize}
\tightlist
\item
  allows different intercept for each time period
\item
  allows outcome to change on average for each time period
\end{itemize}

Allowing for structural change in pooled cross section

\[
y_i = \mathbf{x}_i \beta + \mathbf{x}_i y_1 \gamma_1 + ... + \mathbf{x}_i y_T \gamma_T + \delta_1 y_1 + ...+ \delta_T y_T + \epsilon_i
\]

Interact \(x_i\) with time period dummy variables

\begin{itemize}
\tightlist
\item
  allows different slopes for each time period
\item
  allows effects to change based on time period (\textbf{structural break})
\item
  Interacting all time period dummies with \(x_i\) can produce many variables - use hypothesis testing to determine which structural breaks are needed.
\end{itemize}

\hypertarget{pooled-cross-section}{%
\subsection{Pooled Cross Section}\label{pooled-cross-section}}

\[
y_i=\mathbf{x_i\beta +x_i \times y1\gamma_1 + ...+ x_i \times yT\gamma_T + \delta_1y_1+...+ \delta_Ty_T + \epsilon_i}
\]

Interact \(x_i\) with time period dummy variables

\begin{itemize}
\item
  allows different slopes for each time period
\item
  allows effect to change based on time period (structural break)

  \begin{itemize}
  \tightlist
  \item
    interacting all time period dummies with \(x_i\) can produce many variables - use hypothesis testing to determine which structural breaks are needed.
  \end{itemize}
\end{itemize}

\hypertarget{panel-data}{%
\section{Panel Data}\label{panel-data}}

Detail notes in R can be found \href{https://cran.r-project.org/web/packages/plm/vignettes/plmPackage.html\#robust}{here}

Follows an individual over T time periods.

Panel data structure is like having n samples of time series data

\textbf{Characteristics}

\begin{itemize}
\item
  Information both across individuals and over time (cross-sectional and time-series)
\item
  N individuals and T time periods
\item
  Data can be either

  \begin{itemize}
  \tightlist
  \item
    Balanced: all individuals are observed in all time periods
  \item
    Unbalanced: all individuals are not observed in all time periods.
  \end{itemize}
\item
  Assume correlation (clustering) over time for a given individual, with independence over individuals.
\end{itemize}

\textbf{Types}

\begin{itemize}
\tightlist
\item
  Short panel: many individuals and few time periods.
\item
  Long panel: many time periods and few individuals
\item
  Both: many time periods and many individuals
\end{itemize}

\textbf{Time Trends and Time Effects}

\begin{itemize}
\tightlist
\item
  Nonlinear
\item
  Seasonality
\item
  Discontinuous shocks
\end{itemize}

\textbf{Regressors}

\begin{itemize}
\tightlist
\item
  Time-invariant regressors \(x_{it}=x_i\) for all t (e.g., gender, race, education) have zero within variation
\item
  Individual-invariant regressors \(x_{it}=x_{t}\) for all i (e.g., time trend, economy trends) have zero between variation
\end{itemize}

\textbf{Variation for the dependent variable and regressors}

\begin{itemize}
\tightlist
\item
  Overall variation: variation over time and individuals.
\item
  Between variation: variation between individuals
\item
  Within variation: variation within individuals (over time).
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.1972}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.8028}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Estimate
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Formula
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Individual mean & \(\bar{x_i}= \frac{1}{T} \sum_{t}x_{it}\) \\
Overall mean & \(\bar{x}=\frac{1}{NT} \sum_{i} \sum_t x_{it}\) \\
Overall Variance & \(s _O^2 = \frac{1}{NT-1} \sum_i \sum_t (x_{it} - \bar{x})^2\) \\
Between variance & \(s_B^2 = \frac{1}{N-1} \sum_i (\bar{x_i} -\bar{x})^2\) \\
Within variance & \(s_W^2= \frac{1}{NT-1} \sum_i \sum_t (x_{it} - \bar{x_i})^2 = \frac{1}{NT-1} \sum_i \sum_t (x_{it} - \bar{x_i} +\bar{x})^2\) \\
\end{longtable}

\textbf{Note}: \(s_O^2 \approx s_B^2 + s_W^2\)

Since we have n observation for each time period t, we can control for each time effect separately by including time dummies (time effects)

\[
y_{it}=\mathbf{x_{it}\beta} + d_1\delta_1+...+d_{T-1}\delta_{T-1} + \epsilon_{it}
\]

\textbf{Note}: we cannot use these many time dummies in time series data because in time series data, our n is 1. Hence, there is no variation, and sometimes not enough data compared to variables to estimate coefficients.

\textbf{Unobserved Effects Model} Similar to group clustering, assume that there is a random effect that captures differences across individuals but is constant in time.

\[
y_it=\mathbf{x_{it}\beta} + d_1\delta_1+...+d_{T-1}\delta_{T-1} + c_i + u_{it}
\]

where

\begin{itemize}
\tightlist
\item
  \(c_i + u_{it} = \epsilon_{it}\)
\item
  \(c_i\) unobserved individual heterogeneity (effect)
\item
  \(u_{it}\) idiosyncratic shock
\item
  \(\epsilon_{it}\) unobserved error term.
\end{itemize}

\hypertarget{pooled-ols-estimator}{%
\subsection{Pooled OLS Estimator}\label{pooled-ols-estimator}}

If \(c_i\) is uncorrelated with \(x_{it}\)

\[
E(\mathbf{x_{it}'}(c_i+u_{it})) = 0
\]

then \protect\hyperlink{a3a}{A3a} still holds. And we have Pooled OLS consistent.

If \protect\hyperlink{a4-homoskedasticity}{A4} does not hold, OLS is still consistent, but not efficient, and we need cluster robust SE.

Sufficient for \protect\hyperlink{a3a}{A3a} to hold, we need

\begin{itemize}
\tightlist
\item
  \textbf{Exogeneity} for \(u_{it}\) \protect\hyperlink{a3a}{A3a} (contemporaneous exogeneity): \(E(\mathbf{x_{it}'}u_{it})=0\) time varying error
\item
  \textbf{Random Effect Assumption} (time constant error): \(E(\mathbf{x_{it}'}c_{i})=0\)
\end{itemize}

Pooled OLS will give you consistent coefficient estimates under \protect\hyperlink{a1-linearity}{A1}, \protect\hyperlink{a2-full-rank}{A2}, \protect\hyperlink{a3a}{A3a} (for both \(u_{it}\) and RE assumption), and \protect\hyperlink{a5-data-generation-random-sampling}{A5} (randomly sampling across i).

\hypertarget{individual-specific-effects-model}{%
\subsection{Individual-specific effects model}\label{individual-specific-effects-model}}

\begin{itemize}
\tightlist
\item
  If we believe that there is unobserved heterogeneity across individual (e.g., unobserved ability of an individual affects \(y\)), If the individual-specific effects are correlated with the regressors, then we have the \protect\hyperlink{fixed-effects-estimator}{Fixed Effects Estimator}. and if they are not correlated we have the \protect\hyperlink{random-effects-estimator}{Random Effects Estimator}.
\end{itemize}

\hypertarget{random-effects-estimator}{%
\subsubsection{Random Effects Estimator}\label{random-effects-estimator}}

Random Effects estimator is the Feasible GLS estimator that assumes \(u_{it}\) is serially uncorrelated and homoskedastic

\begin{itemize}
\item
  Under \protect\hyperlink{a1-linearity}{A1}, \protect\hyperlink{a2-full-rank}{A2}, \protect\hyperlink{a3a}{A3a} (for both \(u_{it}\) and RE assumption) and \protect\hyperlink{a5-data-generation-random-sampling}{A5} (randomly sampling across i), RE estimator is consistent.

  \begin{itemize}
  \tightlist
  \item
    If \protect\hyperlink{a4-homoskedasticity}{A4} holds for \(u_{it}\), RE is the most efficient estimator
  \item
    If \protect\hyperlink{a4-homoskedasticity}{A4} fails to hold (may be heteroskedasticity across i, and serial correlation over t), then RE is not the most efficient, but still more efficient than pooled OLS.
  \end{itemize}
\end{itemize}

\hypertarget{fixed-effects-estimator}{%
\subsubsection{Fixed Effects Estimator}\label{fixed-effects-estimator}}

also known as \textbf{Within Estimator} uses within variation (over time)

If the \textbf{RE assumption} is not hold (\(E(\mathbf{x_{it}'}c_i) \neq 0\)), then A3a does not hold (\(E(\mathbf{x_{it}'}\epsilon_i) \neq 0\)).

Hence, the OLS and RE are inconsistent/biased (because of omitted variable bias)

However, FE can only fix bias due to time-invariant factors (both observables and unobservables) correlated with treatment (not time-variant factors that correlated with the treatment).

The traditional FE technique is flawed when lagged dependent variables are included in the model. \citep{nickell1981biases} \citep{narayanan2013estimating}

With measurement error in the independent, FE will exacerbate the errors-in-the-variables bias.

\hypertarget{demean-approach}{%
\paragraph{Demean Approach}\label{demean-approach}}

To deal with violation in \(c_i\), we have

\[
y_{it}= \mathbf{x_{it} \beta} + c_i + u_{it}
\]

\[
\bar{y_i}=\bar{\mathbf{x_i}} \beta + c_i + \bar{u_i}
\]

where the second equation is the time averaged equation

using \textbf{within transformation}, we have

\[
y_{it} - \bar{y_i} = \mathbf{(x_{it} - \bar{x_i})}\beta + u_{it} - \bar{u_i}
\]

because \(c_i\) is time constant.

The Fixed Effects estimator uses POLS on the transformed equation

\[
y_{it} - \bar{y_i} = \mathbf{(x_{it} - \bar{x_i})} \beta + d_1\delta_1 + ... + d_{T-2}\delta_{T-2} + u_{it} - \bar{u_i}
\]

\begin{itemize}
\item
  we need \protect\hyperlink{a3-exogeneity-of-independent-variables}{A3} (strict exogeneity) (\(E((\mathbf{x_{it}-\bar{x_i}})'(u_{it}-\bar{u_i})=0\)) to have FE consistent.
\item
  Variables that are time constant will be absorbed into \(c_i\). Hence we cannot make inference on time constant independent variables.

  \begin{itemize}
  \tightlist
  \item
    If you are interested in the effects of time-invariant variables, you could consider the OLS or \textbf{between estimator}
  \end{itemize}
\item
  It's recommended that you should still use cluster robust standard errors.
\end{itemize}

\hypertarget{dummy-approach}{%
\paragraph{Dummy Approach}\label{dummy-approach}}

Equivalent to the within transformation (i.e., mathematically equivalent to \protect\hyperlink{demean-approach}{Demean Approach}), we can have the fixed effect estimator be the same with the dummy regression

\[
y_{it} = x_{it}\beta + d_1\delta_1 + ... + d_{T-2}\delta_{T-2} + c_1\gamma_1 + ... + c_{n-1}\gamma_{n-1} + u_{it}
\]

where

\[
c_i
=
\begin{cases}
1 &\text{if observation is i} \\
0 &\text{otherwise} \\
\end{cases}
\]

\begin{itemize}
\tightlist
\item
  The standard error is incorrectly calculated.
\item
  the FE within transformation is controlling for any difference across individual which is allowed to correlated with observables.
\end{itemize}

\hypertarget{first-difference-approach}{%
\paragraph{First-difference Approach}\label{first-difference-approach}}

Economists typically use this approach

\[
y_{it} - y_{i (t-1)} = (\mathbf{x}_{it} - \mathbf{x}_{i(t-1)}) \beta +  + (u_{it} - u_{i(t-1)})
\]

\hypertarget{fixed-effects-summary}{%
\paragraph{Fixed Effects Summary}\label{fixed-effects-summary}}

\begin{itemize}
\item
  The three approaches are \textbf{almost} equivalent.

  \begin{itemize}
  \item
    \protect\hyperlink{demean-approach}{Demean Approach} is mathematically equivalent to \protect\hyperlink{dummy-approach}{Dummy Approach}
  \item
    If you have only 1 period, all 3 are the same.
  \end{itemize}
\item
  Since fixed effect is a within estimator, only \textbf{status changes} can contribute to \(\beta\) variation.

  \begin{itemize}
  \tightlist
  \item
    Hence, with a small number of changes then the standard error for \(\beta\) will explode
  \end{itemize}
\item
  Status changes mean subjects change from (1) control to treatment group or (2) treatment to control group. Those who have status change, we call them \textbf{switchers}.

  \begin{itemize}
  \item
    Treatment effect is typically \textbf{non-directional}.
  \item
    You can give a parameter for the direction if needed.
  \end{itemize}
\item
  Issues:

  \begin{itemize}
  \item
    You could have fundamental difference between switchers and non-switchers. Even though we can't definitive test this, but providing descriptive statistics on switchers and non-switchers can give us confidence in our conclusion.
  \item
    Because fixed effects focus on bias reduction, you might have larger variance (typically, with fixed effects you will have less df)
  \end{itemize}
\item
  If the true model is \protect\hyperlink{random-effects-estimator}{random effect}, economists typically don't care, especially when \(c_i\) is the random effect and \(c_i \perp x_{it}\) (because RE assumption is that it is unrelated to \(x_{it}\)). The reason why economists don't care is because RE wouldn't correct bias, it only improves efficiency over OLS.
\item
  You can estimate FE for different units (not just individuals).
\item
  FE removes bias from time invariant factors but not without costs because it uses within variation, which imposes strict exogeneity assumption on \(u_{it}\): \(E[(x_{it} - \bar{x}_{i})(u_{it} - \bar{u}_{it})]=0\)
\end{itemize}

Recall

\[
Y_{it} = \beta_0 + X_{it}\beta_1 + \alpha_i + u_{it}
\]

where \(\epsilon_{it} = \alpha_i + u_{it}\)

\[
\hat{\sigma}^2_\epsilon = \frac{SSR_{OLS}}{NT - K}
\]

\[
\hat{\sigma}^2_u = \frac{SSR_{FE}}{NT - (N+K)} = \frac{SSR_{FE}}{N(T-1)-K}
\]

It's ambiguous whether your variance of error changes up or down because SSR can increase while the denominator decreases.

FE can be unbiased, but not consistent (i.e., not converging to the true effect)

\hypertarget{fe-examples}{%
\paragraph{FE Examples}\label{fe-examples}}

\hypertarget{blau1999}{%
\paragraph{\texorpdfstring{\citet{blau1999}}{@blau1999}}\label{blau1999}}

\begin{itemize}
\item
  Intergenerational mobility
\item
  If we transfer resources to low income family, can we generate upward mobility (increase ability)?
\end{itemize}

Mechanisms for intergenerational mobility

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Genetic (policy can't affect) (i.e., ability endowment)
\item
  Environmental indirect
\item
  Environmental direct
\end{enumerate}

\[
\frac{\% \Delta \text{Human capital}}{\% \Delta \text{income}}
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Financial transfer
\end{enumerate}

Income measures:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Total household income
\item
  Wage income
\item
  Non-wage income
\item
  Annual versus permanent income
\end{enumerate}

Core control variables:

\textbf{Bad controls are those jointly determined with dependent variable}

Control by mother = choice by mother

Uncontrolled by mothers:

\begin{itemize}
\item
  mother race
\item
  location of birth
\item
  education of parents
\item
  household structure at age 14
\end{itemize}

\[
Y_{ijt} = X_{jt} \beta_i + I_{jt} \alpha_i + \epsilon_{ijt}
\]

where

\begin{itemize}
\item
  \(i\) = test
\item
  \(j\) = individual (child)
\item
  \(t\) = time
\end{itemize}

Grandmother's model

Since child is nested within mother and mother nested within grandmother, the fixed effect of child is included in the fixed effect of mother, which is included in the fixed-effect of grandmother

\[
Y_{ijgmt} = X_{it} \beta_{i} + I_{jt} \alpha_i + \gamma_g + u_{ijgmt}
\]

where

\begin{itemize}
\item
  \(i\) = test, \(j\) = kid, \(m\) = mother, \(g\) = grandmother
\item
  where \(\gamma_g\) includes \(\gamma_m\) includes \(\gamma_j\)
\end{itemize}

Grandma fixed-effect

Pros:

\begin{itemize}
\item
  control for some genetics + fixed characteristics of how mother are raised
\item
  can estimate effect of parameter income
\end{itemize}

Con:

\begin{itemize}
\tightlist
\item
  Might not be a sufficient control
\end{itemize}

Common to cluster a the fixed-effect level (common correlated component)

\textbf{Fixed effect exaggerates attenuation bias}

Error rate on survey can help you fix this (plug in the number only , but not the uncertainty associated with that number).

\hypertarget{babcock2010}{%
\paragraph{\texorpdfstring{\citet{babcock2010}}{@babcock2010}}\label{babcock2010}}

\[
T_{ijct} = \alpha_0 + S_{jct} \alpha_1 + X_{ijct} \alpha_2 + u_{ijct}
\]

where

\begin{itemize}
\item
  \(S_{jct}\) is the average class expectation
\item
  \(X_{ijct}\alpha_2\) is the individual characteristics
\item
  \(i\) student
\item
  \(j\) instructor
\item
  \(c\) course
\item
  \(t\) time
\end{itemize}

\[
T_{ijct} = \beta_0+ S_{jct} \beta_1+ X_{ijct} \beta_2 +\mu_{jc} + \epsilon_{ijct}
\]

where \(\mu_{jc}\) is instructor by course fixed effect (unique id), which is different from \((\theta_j + \delta_c)\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Decrease course shopping because conditioned on available information (\(\mu_{jc}\)) (class grade and instructor's info).
\item
  Grade expectation change even though class materials stay the same
\end{enumerate}

Identification strategy is

\begin{itemize}
\tightlist
\item
  Under (fixed) time-varying factor that could bias my coefficient (simultaneity)
\end{itemize}

\[
Y_{ijt} = X_{it} \beta_1 + \text{Teacher Experience}_{jt} \beta_2 + \text{Teacher education}_{jt} \beta_3 + \text{Teacher score}_{it}\beta_4 + \dots + \epsilon_{ijt}
\]

Drop teacher characteristics, and include teacher dummy effect

\[
Y_{ijt} = X_{it} \alpha + \Gamma_{it} \theta_j + u_{ijt}
\]

where \(\alpha\) is the within teacher (conditional on teacher fixed effect) and \(j = 1 \to (J-1)\)

Nuisance in the sense that we don't about the interpretation of \(\alpha\)

The least we can say about \(\theta_j\) is the teacher effect conditional on student test score.

\[
Y_{ijt} = X_{it} \gamma + \epsilon_{ijt}
\]

\(\gamma\) is between within (unconditional) and \(e_{ijt}\) is the prediction error

\[
e_{ijt} = T_{it} \delta_j + \tilde{e}_{ijt}
\]

where \(\delta_j\) is the mean for each group

\[
Y_{ijkt} = Y_{ijkt-1} + X_{it} \beta + T_{it} \tau_j + (W_i + P_k + \epsilon_{ijkt})
\]

where

\begin{itemize}
\item
  \(Y_{ijkt-1}\) = lag control
\item
  \(\tau_j\) = teacher fixed time
\item
  \(W_i\) is the student fixed effect
\item
  \(P_k\) is the school fixed effect
\item
  \(u_{ijkt} = W_i + P_k + \epsilon_{ijkt}\)
\end{itemize}

And we worry about selection on class and school

Bias in \(\tau\) (for 1 teacher) is

\[
\frac{1}{N_j} \sum_{i = 1}^N (W_i + P_k + \epsilon_{ijkt})
\]

where \(N_j\) = the number of student in class with teacher \(j\)

then we can \(P_k + \frac{1}{N_j} \sum_{i = 1}^{N_j} (W_i + \epsilon_{ijkt})\)

Shocks from small class can bias \(\tau\)

\[
\frac{1}{N_j} \sum_{i = 1}^{N_j} \epsilon_{ijkt} \neq 0
\]

which will inflate the teacher fixed effect

Even if we create random teacher fixed effect and put it in the model, it still contains bias mentioned above which can still \(\tau\) (but we do not know the way it will affect - whether more positive or negative).

If teachers switch schools, then we can estimate both teacher and school fixed effect (\textbf{mobility web} thin vs.~thick)

Mobility web refers to the web of switchers (i.e., from one status to another).

\[
Y_{ijkt} = Y_{ijk(t-1)} \alpha + X_{it}\beta + T_{it} \tau + P_k + \epsilon_{ijkt}
\]

If we demean (fixed-effect), \(\tau\) (teacher fixed effect) will go away

If you want to examine teacher fixed effect, we have to include teacher fixed effect

Control for school, the article argues that there is no selection bias

For \(\frac{1}{N_j} \sum_{i =1}^{N_j} \epsilon_{ijkt}\) (teacher-level average residuals), \(var(\tau)\) does not change with \(N_j\) (Figure 2 in the paper). In words, the quality of teachers is not a function of the number of students

If \(var(\tau) =0\) it means that teacher quality does not matter

Spin-off of \protect\hyperlink{measurement-error}{Measurement Error}: Sampling error or estimation error

\[
\hat{\tau}_j = \tau_j + \lambda_j
\]

\[
var(\hat{\tau}) = var(\tau + \lambda)
\]

Assume \(cov(\tau_j, \lambda_j)=0\) (reasonable) In words, your randomness in getting children does not correlation with teacher quality.

Hence,

\[
\begin{aligned}
var(\hat{\tau}) &= var(\tau) + var(\lambda) \\
var(\tau) &= var(\hat{\tau}) - var(\lambda) \\
\end{aligned}
\]

We have \(var(\hat{\tau})\) and we need to estimate \(var(\lambda)\)

\[
var(\lambda) = \frac{1}{J} \sum_{j=1}^J \hat{\sigma}^2_j
\] where \(\hat{\sigma}^2_j\) is the squared standard error of the teacher \(j\) (a function of \(n\))

Hence,

\[
\frac{var(\tau)}{var(\hat{\tau})} = \text{reliability} = \text{true variance signal}
\] also known as how much noise in \(\hat{\tau}\) and

\[
1 - \frac{var(\tau)}{var(\hat{\tau})} = \text{noise}
\]

Even in cases where the true relationship is that \(\tau\) is a function of \(N_j\), then our recovery method for \(\lambda\) is still not affected

To examine our assumption

\[
\hat{\tau}_j = \beta_0 + X_j \beta_1 + \epsilon_j
\]

Regressing teacher fixed-effect on teacher characteristics should give us \(R^2\) close to 0, because teacher characteristics cannot predict sampling error (\(\hat{\tau}\) contain sampling error)

\hypertarget{tests-for-assumptions}{%
\subsection{Tests for Assumptions}\label{tests-for-assumptions}}

We typically don't test heteroskedasticity because we will use robust covariance matrix estimation anyway.

Dataset

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"plm"}\NormalTok{)}
\FunctionTok{data}\NormalTok{(}\StringTok{"EmplUK"}\NormalTok{, }\AttributeTok{package=}\StringTok{"plm"}\NormalTok{)}
\FunctionTok{data}\NormalTok{(}\StringTok{"Produc"}\NormalTok{, }\AttributeTok{package=}\StringTok{"plm"}\NormalTok{)}
\FunctionTok{data}\NormalTok{(}\StringTok{"Grunfeld"}\NormalTok{, }\AttributeTok{package=}\StringTok{"plm"}\NormalTok{)}
\FunctionTok{data}\NormalTok{(}\StringTok{"Wages"}\NormalTok{, }\AttributeTok{package=}\StringTok{"plm"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{poolability}{%
\subsubsection{Poolability}\label{poolability}}

also known as an F test of stability (or Chow test) for the coefficients

\(H_0\): All individuals have the same coefficients (i.e., equal coefficients for all individuals).

\(H_a\) Different individuals have different coefficients.

Notes:

\begin{itemize}
\tightlist
\item
  Under a within (i.e., fixed) model, different intercepts for each individual are assumed
\item
  Under random model, same intercept is assumed
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(plm)}
\NormalTok{plm}\SpecialCharTok{::}\FunctionTok{pooltest}\NormalTok{(inv}\SpecialCharTok{\textasciitilde{}}\NormalTok{value}\SpecialCharTok{+}\NormalTok{capital, }\AttributeTok{data=}\NormalTok{Grunfeld, }\AttributeTok{model=}\StringTok{"within"}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  F statistic}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  inv \textasciitilde{} value + capital}
\CommentTok{\#\textgreater{} F = 5.7805, df1 = 18, df2 = 170, p{-}value = 1.219e{-}10}
\CommentTok{\#\textgreater{} alternative hypothesis: unstability}
\end{Highlighting}
\end{Shaded}

Hence, we reject the null hypothesis that coefficients are stable. Then, we should use the random model.

\hypertarget{individual-and-time-effects}{%
\subsubsection{Individual and time effects}\label{individual-and-time-effects}}

use the Lagrange multiplier test to test the presence of individual or time or both (i.e., individual and time).

Types:

\begin{itemize}
\tightlist
\item
  \texttt{honda}: \citep{honda1985testing} Default
\item
  \texttt{bp}: \citep{Breusch_1980} for unbalanced panels
\item
  \texttt{kw}: \citep{King_1997} unbalanced panels, and two-way effects
\item
  \texttt{ghm}: \citep{gourieroux1982likelihood}: two-way effects
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pFtest}\NormalTok{(inv}\SpecialCharTok{\textasciitilde{}}\NormalTok{value}\SpecialCharTok{+}\NormalTok{capital, }\AttributeTok{data=}\NormalTok{Grunfeld, }\AttributeTok{effect=}\StringTok{"twoways"}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  F test for twoways effects}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  inv \textasciitilde{} value + capital}
\CommentTok{\#\textgreater{} F = 17.403, df1 = 28, df2 = 169, p{-}value \textless{} 2.2e{-}16}
\CommentTok{\#\textgreater{} alternative hypothesis: significant effects}
\FunctionTok{pFtest}\NormalTok{(inv}\SpecialCharTok{\textasciitilde{}}\NormalTok{value}\SpecialCharTok{+}\NormalTok{capital, }\AttributeTok{data=}\NormalTok{Grunfeld, }\AttributeTok{effect=}\StringTok{"individual"}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  F test for individual effects}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  inv \textasciitilde{} value + capital}
\CommentTok{\#\textgreater{} F = 49.177, df1 = 9, df2 = 188, p{-}value \textless{} 2.2e{-}16}
\CommentTok{\#\textgreater{} alternative hypothesis: significant effects}
\FunctionTok{pFtest}\NormalTok{(inv}\SpecialCharTok{\textasciitilde{}}\NormalTok{value}\SpecialCharTok{+}\NormalTok{capital, }\AttributeTok{data=}\NormalTok{Grunfeld, }\AttributeTok{effect=}\StringTok{"time"}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  F test for time effects}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  inv \textasciitilde{} value + capital}
\CommentTok{\#\textgreater{} F = 0.23451, df1 = 19, df2 = 178, p{-}value = 0.9997}
\CommentTok{\#\textgreater{} alternative hypothesis: significant effects}
\end{Highlighting}
\end{Shaded}

\hypertarget{cross-sectional-dependencecontemporaneous-correlation}{%
\subsubsection{Cross-sectional dependence/contemporaneous correlation}\label{cross-sectional-dependencecontemporaneous-correlation}}

\begin{itemize}
\tightlist
\item
  Null hypothesis: residuals across entities are not correlated.
\end{itemize}

\hypertarget{global-cross-sectional-dependence}{%
\paragraph{Global cross-sectional dependence}\label{global-cross-sectional-dependence}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pcdtest}\NormalTok{(inv}\SpecialCharTok{\textasciitilde{}}\NormalTok{value}\SpecialCharTok{+}\NormalTok{capital, }\AttributeTok{data=}\NormalTok{Grunfeld, }\AttributeTok{model=}\StringTok{"within"}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Pesaran CD test for cross{-}sectional dependence in panels}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  inv \textasciitilde{} value + capital}
\CommentTok{\#\textgreater{} z = 4.6612, p{-}value = 3.144e{-}06}
\CommentTok{\#\textgreater{} alternative hypothesis: cross{-}sectional dependence}
\end{Highlighting}
\end{Shaded}

\hypertarget{local-cross-sectional-dependence}{%
\paragraph{Local cross-sectional dependence}\label{local-cross-sectional-dependence}}

use the same command, but supply matrix \texttt{w} to the argument.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pcdtest}\NormalTok{(inv}\SpecialCharTok{\textasciitilde{}}\NormalTok{value}\SpecialCharTok{+}\NormalTok{capital, }\AttributeTok{data=}\NormalTok{Grunfeld, }\AttributeTok{model=}\StringTok{"within"}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Pesaran CD test for cross{-}sectional dependence in panels}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  inv \textasciitilde{} value + capital}
\CommentTok{\#\textgreater{} z = 4.6612, p{-}value = 3.144e{-}06}
\CommentTok{\#\textgreater{} alternative hypothesis: cross{-}sectional dependence}
\end{Highlighting}
\end{Shaded}

\hypertarget{serial-correlation-1}{%
\subsubsection{Serial Correlation}\label{serial-correlation-1}}

\begin{itemize}
\item
  Null hypothesis: there is no serial correlation
\item
  usually seen in macro panels with long time series (large N and T), not seen in micro panels (small T and large N)
\item
  Serial correlation can arise from individual effects(i.e., time-invariant error component), or idiosyncratic error terms (e..g, in the case of AR(1) process). But typically, when we refer to serial correlation, we refer to the second one.
\item
  Can be

  \begin{itemize}
  \item
    \textbf{marginal} test: only 1 of the two above dependence (but can be biased towards rejection)
  \item
    \textbf{joint} test: both dependencies (but don't know which one is causing the problem)
  \item
    \textbf{conditional} test: assume you correctly specify one dependence structure, test whether the other departure is present.
  \end{itemize}
\end{itemize}

\hypertarget{unobserved-effect-test}{%
\paragraph{Unobserved effect test}\label{unobserved-effect-test}}

\begin{itemize}
\item
  semi-parametric test (the test statistic \(W \dot{\sim} N\) regardless of the distribution of the errors) with \(H_0: \sigma^2_\mu = 0\) (i.e., no unobserved effects in the residuals), favors pooled OLS.

  \begin{itemize}
  \tightlist
  \item
    Under the null, covariance matrix of the residuals = its diagonal (off-diagonal = 0)
  \end{itemize}
\item
  It is robust against both \textbf{unobserved effects} that are constant within every group, and any kind of \textbf{serial correlation}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pwtest}\NormalTok{(}\FunctionTok{log}\NormalTok{(gsp) }\SpecialCharTok{\textasciitilde{}} \FunctionTok{log}\NormalTok{(pcap) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{(pc) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{(emp) }\SpecialCharTok{+}\NormalTok{ unemp, }\AttributeTok{data =}\NormalTok{ Produc)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Wooldridge\textquotesingle{}s test for unobserved individual effects}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  formula}
\CommentTok{\#\textgreater{} z = 3.9383, p{-}value = 8.207e{-}05}
\CommentTok{\#\textgreater{} alternative hypothesis: unobserved effect}
\end{Highlighting}
\end{Shaded}

Here, we reject the null hypothesis that the no unobserved effects in the residuals. Hence, we will exclude using pooled OLS.

\hypertarget{locally-robust-tests-for-random-effects-and-serial-correlation}{%
\paragraph{Locally robust tests for random effects and serial correlation}\label{locally-robust-tests-for-random-effects-and-serial-correlation}}

\begin{itemize}
\tightlist
\item
  A joint LM test for \textbf{random effects} and \textbf{serial correlation} assuming normality and homoskedasticity of the idiosyncratic errors {[}\citet{baltagi1991joint}{]}\citep{baltagi1995testing}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pbsytest}\NormalTok{(}\FunctionTok{log}\NormalTok{(gsp) }\SpecialCharTok{\textasciitilde{}} \FunctionTok{log}\NormalTok{(pcap) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{(pc) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{(emp) }\SpecialCharTok{+}\NormalTok{ unemp,}
         \AttributeTok{data =}\NormalTok{ Produc,}
         \AttributeTok{test =} \StringTok{"j"}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Baltagi and Li AR{-}RE joint test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  formula}
\CommentTok{\#\textgreater{} chisq = 4187.6, df = 2, p{-}value \textless{} 2.2e{-}16}
\CommentTok{\#\textgreater{} alternative hypothesis: AR(1) errors or random effects}
\end{Highlighting}
\end{Shaded}

Here, we reject the null hypothesis that there is no presence of \textbf{serial correlation,} and \textbf{random effects}. But we still do not know whether it is because of serial correlation, of random effects or of both

To know the departure from the null assumption, we can use \citet{bera2001tests}'s test for first-order serial correlation or random effects (both under normality and homoskedasticity assumption of the error).

BSY for serial correlation

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pbsytest}\NormalTok{(}\FunctionTok{log}\NormalTok{(gsp) }\SpecialCharTok{\textasciitilde{}} \FunctionTok{log}\NormalTok{(pcap) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{(pc) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{(emp) }\SpecialCharTok{+}\NormalTok{ unemp,}
         \AttributeTok{data =}\NormalTok{ Produc)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Bera, Sosa{-}Escudero and Yoon locally robust test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  formula}
\CommentTok{\#\textgreater{} chisq = 52.636, df = 1, p{-}value = 4.015e{-}13}
\CommentTok{\#\textgreater{} alternative hypothesis: AR(1) errors sub random effects}
\end{Highlighting}
\end{Shaded}

BSY for random effects

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pbsytest}\NormalTok{(}\FunctionTok{log}\NormalTok{(gsp)}\SpecialCharTok{\textasciitilde{}}\FunctionTok{log}\NormalTok{(pcap)}\SpecialCharTok{+}\FunctionTok{log}\NormalTok{(pc)}\SpecialCharTok{+}\FunctionTok{log}\NormalTok{(emp)}\SpecialCharTok{+}\NormalTok{unemp, }
         \AttributeTok{data=}\NormalTok{Produc, }
         \AttributeTok{test=}\StringTok{"re"}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Bera, Sosa{-}Escudero and Yoon locally robust test (one{-}sided)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  formula}
\CommentTok{\#\textgreater{} z = 57.914, p{-}value \textless{} 2.2e{-}16}
\CommentTok{\#\textgreater{} alternative hypothesis: random effects sub AR(1) errors}
\end{Highlighting}
\end{Shaded}

Since BSY is only locally robust, if you ``know'' there is no serial correlation, then this test is based on LM test is more superior:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plmtest}\NormalTok{(inv }\SpecialCharTok{\textasciitilde{}}\NormalTok{ value }\SpecialCharTok{+}\NormalTok{ capital, }\AttributeTok{data =}\NormalTok{ Grunfeld, }
        \AttributeTok{type =} \StringTok{"honda"}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Lagrange Multiplier Test {-} (Honda)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  inv \textasciitilde{} value + capital}
\CommentTok{\#\textgreater{} normal = 28.252, p{-}value \textless{} 2.2e{-}16}
\CommentTok{\#\textgreater{} alternative hypothesis: significant effects}
\end{Highlighting}
\end{Shaded}

On the other hand, if you know there is no random effects, to test for serial correlation, use \citep{breusch1978testing}-\citep{godfrey1978testing}'s test

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lmtest}\SpecialCharTok{::}\FunctionTok{bgtest}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

If you ``know'' there are random effects, use \citep{baltagi1995testing}'s. to test for serial correlation in both AR(1) and MA(1) processes.

\(H_0\): Uncorrelated errors.

Note:

\begin{itemize}
\tightlist
\item
  one-sided only has power against positive serial correlation.
\item
  applicable to only balanced panels.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pbltest}\NormalTok{(}
    \FunctionTok{log}\NormalTok{(gsp) }\SpecialCharTok{\textasciitilde{}} \FunctionTok{log}\NormalTok{(pcap) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{(pc) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{(emp) }\SpecialCharTok{+}\NormalTok{ unemp,}
    \AttributeTok{data =}\NormalTok{ Produc,}
    \AttributeTok{alternative =} \StringTok{"onesided"}
\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Baltagi and Li one{-}sided LM test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  log(gsp) \textasciitilde{} log(pcap) + log(pc) + log(emp) + unemp}
\CommentTok{\#\textgreater{} z = 21.69, p{-}value \textless{} 2.2e{-}16}
\CommentTok{\#\textgreater{} alternative hypothesis: AR(1)/MA(1) errors in RE panel model}
\end{Highlighting}
\end{Shaded}

General serial correlation tests

\begin{itemize}
\tightlist
\item
  applicable to random effects model, OLS, and FE (with large T, also known as long panel).
\item
  can also test higher-order serial correlation
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plm}\SpecialCharTok{::}\FunctionTok{pbgtest}\NormalTok{(plm}\SpecialCharTok{::}\FunctionTok{plm}\NormalTok{(inv }\SpecialCharTok{\textasciitilde{}}\NormalTok{ value }\SpecialCharTok{+}\NormalTok{ capital,}
                      \AttributeTok{data =}\NormalTok{ Grunfeld,}
                      \AttributeTok{model =} \StringTok{"within"}\NormalTok{),}
             \AttributeTok{order =} \DecValTok{2}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Breusch{-}Godfrey/Wooldridge test for serial correlation in panel models}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  inv \textasciitilde{} value + capital}
\CommentTok{\#\textgreater{} chisq = 42.587, df = 2, p{-}value = 5.655e{-}10}
\CommentTok{\#\textgreater{} alternative hypothesis: serial correlation in idiosyncratic errors}
\end{Highlighting}
\end{Shaded}

in the case of short panels (small T and large n), we can use

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pwartest}\NormalTok{(}\FunctionTok{log}\NormalTok{(emp) }\SpecialCharTok{\textasciitilde{}} \FunctionTok{log}\NormalTok{(wage) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{(capital), }\AttributeTok{data=}\NormalTok{EmplUK)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Wooldridge\textquotesingle{}s test for serial correlation in FE panels}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  plm.model}
\CommentTok{\#\textgreater{} F = 312.3, df1 = 1, df2 = 889, p{-}value \textless{} 2.2e{-}16}
\CommentTok{\#\textgreater{} alternative hypothesis: serial correlation}
\end{Highlighting}
\end{Shaded}

\hypertarget{unit-rootsstationarity}{%
\subsubsection{Unit roots/stationarity}\label{unit-rootsstationarity}}

\begin{itemize}
\tightlist
\item
  Dickey-Fuller test for stochastic trends.
\item
  Null hypothesis: the series is non-stationary (unit root)
\item
  You would want your test to be less than the critical value (p\textless.5) so that there is evidence there is not unit roots.
\end{itemize}

\hypertarget{heteroskedasticity-2}{%
\subsubsection{Heteroskedasticity}\label{heteroskedasticity-2}}

\begin{itemize}
\item
  Breusch-Pagan test
\item
  Null hypothesis: the data is homoskedastic
\item
  If there is evidence for heteroskedasticity, robust covariance matrix is advised.
\item
  To control for heteroskedasticity: Robust covariance matrix estimation (Sandwich estimator)

  \begin{itemize}
  \tightlist
  \item
    ``white1'' - for general heteroskedasticity but no serial correlation (check serial correlation first). Recommended for random effects.
  \item
    ``white2'' - is ``white1'' restricted to a common variance within groups. Recommended for random effects.
  \item
    ``arellano'' - both heteroskedasticity and serial correlation. Recommended for fixed effects
  \end{itemize}
\end{itemize}

\hypertarget{model-selection}{%
\subsection{Model Selection}\label{model-selection}}

\hypertarget{pols-vs.-re}{%
\subsubsection{POLS vs.~RE}\label{pols-vs.-re}}

The continuum between RE (used FGLS which more assumption ) and POLS check back on the section of FGLS

\textbf{Breusch-Pagan LM} test

\begin{itemize}
\tightlist
\item
  Test for the random effect model based on the OLS residual
\item
  Null hypothesis: variances across entities is zero. In another word, no panel effect.
\item
  If the test is significant, RE is preferable compared to POLS
\end{itemize}

\hypertarget{fe-vs.-re}{%
\subsubsection{FE vs.~RE}\label{fe-vs.-re}}

\begin{itemize}
\tightlist
\item
  RE does not require strict exogeneity for consistency (feedback effect between residual and covariates)
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.3194}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.6806}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Hypothesis
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
If true
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(H_0: Cov(c_i,\mathbf{x_{it}})=0\) & \(\hat{\beta}_{RE}\) is consistent and efficient, while \(\hat{\beta}_{FE}\) is consistent \\
\(H_0: Cov(c_i,\mathbf{x_{it}}) \neq 0\) & \(\hat{\beta}_{RE}\) is inconsistent, while \(\hat{\beta}_{FE}\) is consistent \\
\end{longtable}

\textbf{Hausman Test}

For the Hausman test to run, you need to assume that

\begin{itemize}
\tightlist
\item
  strict exogeneity hold
\item
  A4 to hold for \(u_{it}\)
\end{itemize}

Then,

\begin{itemize}
\tightlist
\item
  Hausman test statistic: \(H=(\hat{\beta}_{RE}-\hat{\beta}_{FE})'(V(\hat{\beta}_{RE})- V(\hat{\beta}_{FE}))(\hat{\beta}_{RE}-\hat{\beta}_{FE}) \sim \chi_{n(X)}^2\) where \(n(X)\) is the number of parameters for the time-varying regressors.
\item
  A low p-value means that we would reject the null hypothesis and prefer FE
\item
  A high p-value means that we would not reject the null hypothesis and consider RE estimator.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gw }\OtherTok{\textless{}{-}} \FunctionTok{plm}\NormalTok{(inv }\SpecialCharTok{\textasciitilde{}}\NormalTok{ value }\SpecialCharTok{+}\NormalTok{ capital, }\AttributeTok{data =}\NormalTok{ Grunfeld, }\AttributeTok{model =} \StringTok{"within"}\NormalTok{)}
\NormalTok{gr }\OtherTok{\textless{}{-}} \FunctionTok{plm}\NormalTok{(inv }\SpecialCharTok{\textasciitilde{}}\NormalTok{ value }\SpecialCharTok{+}\NormalTok{ capital, }\AttributeTok{data =}\NormalTok{ Grunfeld, }\AttributeTok{model =} \StringTok{"random"}\NormalTok{)}
\FunctionTok{phtest}\NormalTok{(gw, gr)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Hausman Test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  inv \textasciitilde{} value + capital}
\CommentTok{\#\textgreater{} chisq = 2.3304, df = 2, p{-}value = 0.3119}
\CommentTok{\#\textgreater{} alternative hypothesis: one model is inconsistent}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Violation Estimator
\item
  Basic Estimator
\item
  Instrumental variable Estimator
\item
  Variable Coefficients estimator
\item
  Generalized Method of Moments estimator
\item
  General FGLS estimator
\item
  Means groups estimator
\item
  CCEMG
\item
  Estimator for limited dependent variables
\end{itemize}

\hypertarget{summary-3}{%
\subsection{Summary}\label{summary-3}}

\begin{itemize}
\item
  All three estimators (POLS, RE, FE) require \protect\hyperlink{a1-linearity}{A1}, \protect\hyperlink{a2-full-rank}{A2}, \protect\hyperlink{a5-data-generation-random-sampling}{A5} (for individuals) to be consistent. Additionally,
\item
  POLS is consistent under A3a(for \(u_{it}\)): \(E(\mathbf{x}_{it}'u_{it})=0\), and RE Assumption \(E(\mathbf{x}_{it}'c_{i})=0\)

  \begin{itemize}
  \tightlist
  \item
    If \protect\hyperlink{a4-homoskedasticity}{A4} does not hold, use cluster robust SE but POLS is not efficient
  \end{itemize}
\item
  RE is consistent under A3a(for \(u_{it}\)): \(E(\mathbf{x}_{it}'u_{it})=0\), and RE Assumption \(E(\mathbf{x}_{it}'c_{i})=0\)

  \begin{itemize}
  \tightlist
  \item
    If \protect\hyperlink{a4-homoskedasticity}{A4} (for \(u_{it}\)) holds then usual SE are valid and RE is most efficient
  \item
    If \protect\hyperlink{a4-homoskedasticity}{A4} (for \(u_{it}\)) does not hold, use cluster robust SE ,and RE is no longer most efficient (but still more efficient than POLS)
  \end{itemize}
\item
  FE is consistent under \protect\hyperlink{a3-exogeneity-of-independent-variables}{A3} \(E((\mathbf{x}_{it}-\bar{\mathbf{x}}_{it})'(u_{it} -\bar{u}_{it}))=0\)

  \begin{itemize}
  \tightlist
  \item
    Cannot estimate effects of time constant variables
  \item
    A4 generally does not hold for \(u_{it} -\bar{u}_{it}\) so cluster robust SE are needed
  \end{itemize}
\end{itemize}

\textbf{Note}: \protect\hyperlink{a5-data-generation-random-sampling}{A5} for individual (not for time dimension) implies that you have \protect\hyperlink{a5a}{A5a} for the entire data set.

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Estimator / True Model & POLS & RE & FE \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
POLS & Consistent & Consistent & Inconsistent \\
FE & Consistent & Consistent & Consistent \\
RE & Consistent & Consistent & Inconsistent \\
\end{longtable}

Based on table provided by \href{https://sites.google.com/site/econometricsacademy/econometrics-models/panel-data-models}{Ani Katchova}

\hypertarget{application-8}{%
\subsection{Application}\label{application-8}}

\hypertarget{plm-package}{%
\subsubsection{\texorpdfstring{\texttt{plm} package}{plm package}}\label{plm-package}}

Recommended application of \texttt{plm} can be found \href{https://cran.r-project.org/web/packages/plm/vignettes/B_plmFunction.html}{here} and \href{https://cran.r-project.org/web/packages/plm/vignettes/C_plmModelComponents.html}{here} by Yves Croissant

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#install.packages("plm")}
\FunctionTok{library}\NormalTok{(}\StringTok{"plm"}\NormalTok{)}

\FunctionTok{library}\NormalTok{(foreign)}
\NormalTok{Panel }\OtherTok{\textless{}{-}} \FunctionTok{read.dta}\NormalTok{(}\StringTok{"http://dss.princeton.edu/training/Panel101.dta"}\NormalTok{)}

\FunctionTok{attach}\NormalTok{(Panel)}
\NormalTok{Y }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(y)}
\NormalTok{X }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(x1, x2, x3)}

\CommentTok{\# Set data as panel data}
\NormalTok{pdata }\OtherTok{\textless{}{-}} \FunctionTok{pdata.frame}\NormalTok{(Panel, }\AttributeTok{index =} \FunctionTok{c}\NormalTok{(}\StringTok{"country"}\NormalTok{, }\StringTok{"year"}\NormalTok{))}

\CommentTok{\# Pooled OLS estimator}
\NormalTok{pooling }\OtherTok{\textless{}{-}} \FunctionTok{plm}\NormalTok{(Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X, }\AttributeTok{data =}\NormalTok{ pdata, }\AttributeTok{model =} \StringTok{"pooling"}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(pooling)}

\CommentTok{\# Between estimator}
\NormalTok{between }\OtherTok{\textless{}{-}} \FunctionTok{plm}\NormalTok{(Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X, }\AttributeTok{data =}\NormalTok{ pdata, }\AttributeTok{model =} \StringTok{"between"}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(between)}

\CommentTok{\# First differences estimator}
\NormalTok{firstdiff }\OtherTok{\textless{}{-}} \FunctionTok{plm}\NormalTok{(Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X, }\AttributeTok{data =}\NormalTok{ pdata, }\AttributeTok{model =} \StringTok{"fd"}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(firstdiff)}

\CommentTok{\# Fixed effects or within estimator}
\NormalTok{fixed }\OtherTok{\textless{}{-}} \FunctionTok{plm}\NormalTok{(Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X, }\AttributeTok{data =}\NormalTok{ pdata, }\AttributeTok{model =} \StringTok{"within"}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(fixed)}

\CommentTok{\# Random effects estimator}
\NormalTok{random }\OtherTok{\textless{}{-}} \FunctionTok{plm}\NormalTok{(Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X, }\AttributeTok{data =}\NormalTok{ pdata, }\AttributeTok{model =} \StringTok{"random"}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(random)}

\CommentTok{\# LM test for random effects versus OLS}
\CommentTok{\# Accept Null, then OLS, Reject Null then RE}
\FunctionTok{plmtest}\NormalTok{(pooling, }\AttributeTok{effect =} \StringTok{"individual"}\NormalTok{, }\AttributeTok{type =} \FunctionTok{c}\NormalTok{(}\StringTok{"bp"}\NormalTok{)) }
\CommentTok{\# other type: "honda", "kw"," "ghm"; other effect : "time" "twoways"}


\CommentTok{\# B{-}P/LM and Pesaran CD (cross{-}sectional dependence) test}
\CommentTok{\# Breusch and Pagan\textquotesingle{}s original LM statistic}
\FunctionTok{pcdtest}\NormalTok{(fixed, }\AttributeTok{test =} \FunctionTok{c}\NormalTok{(}\StringTok{"lm"}\NormalTok{)) }
\CommentTok{\# Pesaran\textquotesingle{}s CD statistic}
\FunctionTok{pcdtest}\NormalTok{(fixed, }\AttributeTok{test =} \FunctionTok{c}\NormalTok{(}\StringTok{"cd"}\NormalTok{)) }

\CommentTok{\# Serial Correlation}
\FunctionTok{pbgtest}\NormalTok{(fixed)}

\CommentTok{\# stationary}
\FunctionTok{library}\NormalTok{(}\StringTok{"tseries"}\NormalTok{)}
\FunctionTok{adf.test}\NormalTok{(pdata}\SpecialCharTok{$}\NormalTok{y, }\AttributeTok{k =} \DecValTok{2}\NormalTok{)}

\CommentTok{\# LM test for fixed effects versus OLS}
\FunctionTok{pFtest}\NormalTok{(fixed, pooling)}

\CommentTok{\# Hausman test for fixed versus random effects model}
\FunctionTok{phtest}\NormalTok{(random, fixed)}

\CommentTok{\# Breusch{-}Pagan heteroskedasticity}
\FunctionTok{library}\NormalTok{(lmtest)}
\FunctionTok{bptest}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x1 }\SpecialCharTok{+} \FunctionTok{factor}\NormalTok{(country), }\AttributeTok{data =}\NormalTok{ pdata)}

\CommentTok{\# If there is presence of heteroskedasticity}
\DocumentationTok{\#\# For RE model}
\FunctionTok{coeftest}\NormalTok{(random) }\CommentTok{\#orginal coef}

\CommentTok{\# Heteroskedasticity consistent coefficients}
\FunctionTok{coeftest}\NormalTok{(random, vcovHC) }

\FunctionTok{t}\NormalTok{(}\FunctionTok{sapply}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"HC0"}\NormalTok{, }\StringTok{"HC1"}\NormalTok{, }\StringTok{"HC2"}\NormalTok{, }\StringTok{"HC3"}\NormalTok{, }\StringTok{"HC4"}\NormalTok{), }\ControlFlowTok{function}\NormalTok{(x)}
    \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{diag}\NormalTok{(}
        \FunctionTok{vcovHC}\NormalTok{(random, }\AttributeTok{type =}\NormalTok{ x)}
\NormalTok{    )))) }\CommentTok{\#show HC SE of the coef}
\CommentTok{\# HC0 {-} heteroskedasticity consistent. The default.}
\CommentTok{\# HC1,HC2, HC3 â€“ Recommended for small samples. }
\CommentTok{\# HC3 gives less weight to influential observations.}
\CommentTok{\# HC4 {-} small samples with influential observations}
\CommentTok{\# HAC {-} heteroskedasticity and autocorrelation consistent}

\DocumentationTok{\#\# For FE model}
\FunctionTok{coeftest}\NormalTok{(fixed) }\CommentTok{\# Original coefficients}
\FunctionTok{coeftest}\NormalTok{(fixed, vcovHC) }\CommentTok{\# Heteroskedasticity consistent coefficients}

\CommentTok{\# Heteroskedasticity consistent coefficients (Arellano)}
\FunctionTok{coeftest}\NormalTok{(fixed, }\FunctionTok{vcovHC}\NormalTok{(fixed, }\AttributeTok{method =} \StringTok{"arellano"}\NormalTok{)) }

\FunctionTok{t}\NormalTok{(}\FunctionTok{sapply}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"HC0"}\NormalTok{, }\StringTok{"HC1"}\NormalTok{, }\StringTok{"HC2"}\NormalTok{, }\StringTok{"HC3"}\NormalTok{, }\StringTok{"HC4"}\NormalTok{), }\ControlFlowTok{function}\NormalTok{(x)}
    \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{diag}\NormalTok{(}
        \FunctionTok{vcovHC}\NormalTok{(fixed, }\AttributeTok{type =}\NormalTok{ x)}
\NormalTok{    )))) }\CommentTok{\#show HC SE of the coef}
\end{Highlighting}
\end{Shaded}

\textbf{Advanced}

Other methods to estimate the random model:

\begin{itemize}
\tightlist
\item
  \texttt{"swar"}: \emph{default} \citep{swamy1972exact}
\item
  \texttt{"walhus"}: \citep{wallace1969use}
\item
  \texttt{"amemiya"}: \citep{amemiya1971estimation}
\item
  \texttt{"nerlove"}'' \citep{nerlove1971further}
\end{itemize}

Other effects:

\begin{itemize}
\tightlist
\item
  Individual effects: \emph{default}
\item
  Time effects: \texttt{"time"}
\item
  Individual and time effects: \texttt{"twoways"}
\end{itemize}

\textbf{Note}: no random two-ways effect model for \texttt{random.method\ =\ "nerlove"}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{amemiya }\OtherTok{\textless{}{-}}
    \FunctionTok{plm}\NormalTok{(}
\NormalTok{        Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X,}
        \AttributeTok{data =}\NormalTok{ pdata,}
        \AttributeTok{model =} \StringTok{"random"}\NormalTok{,}
        \AttributeTok{random.method =} \StringTok{"amemiya"}\NormalTok{,}
        \AttributeTok{effect =} \StringTok{"twoways"}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

To call the estimation of the variance of the error components

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ercomp}\NormalTok{(Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X,}
       \AttributeTok{data =}\NormalTok{ pdata,}
       \AttributeTok{method =} \StringTok{"amemiya"}\NormalTok{,}
       \AttributeTok{effect =} \StringTok{"twoways"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Check for the unbalancedness. Closer to 1 indicates balanced data \citep{ahrens1981two}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{punbalancedness}\NormalTok{(random)}
\end{Highlighting}
\end{Shaded}

\textbf{Instrumental variable}

\begin{itemize}
\tightlist
\item
  \texttt{"bvk"}: default \citep{balestra1987full}
\item
  \texttt{"baltagi"}: \citep{baltagi1981simultaneous}
\item
  \texttt{"am"} \citep{amemiya1986instrumental}
\item
  \texttt{"bms"}: \citep{breusch1989efficient}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{instr }\OtherTok{\textless{}{-}}
    \FunctionTok{plm}\NormalTok{(}
\NormalTok{        Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X }\SpecialCharTok{|}\NormalTok{ X\_ins,}
        \AttributeTok{data =}\NormalTok{ pdata,}
        \AttributeTok{random.method =} \StringTok{"ht"}\NormalTok{,}
        \AttributeTok{model =} \StringTok{"random"}\NormalTok{,}
        \AttributeTok{inst.method =} \StringTok{"baltagi"}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\hypertarget{other-estimators}{%
\paragraph{Other Estimators}\label{other-estimators}}

\hypertarget{variable-coefficients-model}{%
\subparagraph{Variable Coefficients Model}\label{variable-coefficients-model}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fixed\_pvcm  }\OtherTok{\textless{}{-}} \FunctionTok{pvcm}\NormalTok{(Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X, }\AttributeTok{data =}\NormalTok{ pdata, }\AttributeTok{model =} \StringTok{"within"}\NormalTok{)}
\NormalTok{random\_pvcm }\OtherTok{\textless{}{-}} \FunctionTok{pvcm}\NormalTok{(Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X, }\AttributeTok{data =}\NormalTok{ pdata, }\AttributeTok{model =} \StringTok{"random"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

More details can be found \href{https://cran.r-project.org/web/packages/plm/vignettes/plmPackage.html}{here}

\hypertarget{generalized-method-of-moments-estimator}{%
\subparagraph{Generalized Method of Moments Estimator}\label{generalized-method-of-moments-estimator}}

Typically use in dynamic models. Example is from \href{https://cran.r-project.org/web/packages/plm/vignettes/plmPackage.html}{plm package}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{z2 }\OtherTok{\textless{}{-}} \FunctionTok{pgmm}\NormalTok{(}
    \FunctionTok{log}\NormalTok{(emp) }\SpecialCharTok{\textasciitilde{}} \FunctionTok{lag}\NormalTok{(}\FunctionTok{log}\NormalTok{(emp), }\DecValTok{1}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{lag}\NormalTok{(}\FunctionTok{log}\NormalTok{(wage), }\DecValTok{0}\SpecialCharTok{:}\DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
        \FunctionTok{lag}\NormalTok{(}\FunctionTok{log}\NormalTok{(capital), }\DecValTok{0}\SpecialCharTok{:}\DecValTok{1}\NormalTok{) }\SpecialCharTok{|} \FunctionTok{lag}\NormalTok{(}\FunctionTok{log}\NormalTok{(emp), }\DecValTok{2}\SpecialCharTok{:}\DecValTok{99}\NormalTok{) }\SpecialCharTok{+}
        \FunctionTok{lag}\NormalTok{(}\FunctionTok{log}\NormalTok{(wage), }\DecValTok{2}\SpecialCharTok{:}\DecValTok{99}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{lag}\NormalTok{(}\FunctionTok{log}\NormalTok{(capital), }\DecValTok{2}\SpecialCharTok{:}\DecValTok{99}\NormalTok{),}
    \AttributeTok{data =}\NormalTok{ EmplUK,}
    \AttributeTok{effect =} \StringTok{"twoways"}\NormalTok{,}
    \AttributeTok{model =} \StringTok{"onestep"}\NormalTok{,}
    \AttributeTok{transformation =} \StringTok{"ld"}
\NormalTok{)}
\FunctionTok{summary}\NormalTok{(z2, }\AttributeTok{robust =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{general-feasible-generalized-least-squares-models}{%
\subparagraph{General Feasible Generalized Least Squares Models}\label{general-feasible-generalized-least-squares-models}}

Assume there is no cross-sectional correlation Robust against intragroup heteroskedasticity and serial correlation. Suited when n is much larger than T (long panel) However, inefficient under group-wise heteorskedasticity.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Random Effects}
\NormalTok{zz }\OtherTok{\textless{}{-}}
    \FunctionTok{pggls}\NormalTok{(}\FunctionTok{log}\NormalTok{(emp) }\SpecialCharTok{\textasciitilde{}} \FunctionTok{log}\NormalTok{(wage) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{(capital),}
          \AttributeTok{data =}\NormalTok{ EmplUK,}
          \AttributeTok{model =} \StringTok{"pooling"}\NormalTok{)}

\CommentTok{\# Fixed}
\NormalTok{zz }\OtherTok{\textless{}{-}}
    \FunctionTok{pggls}\NormalTok{(}\FunctionTok{log}\NormalTok{(emp) }\SpecialCharTok{\textasciitilde{}} \FunctionTok{log}\NormalTok{(wage) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{(capital),}
          \AttributeTok{data =}\NormalTok{ EmplUK,}
          \AttributeTok{model =} \StringTok{"within"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{fixest-package}{%
\subsubsection{\texorpdfstring{\texttt{fixest} package}{fixest package}}\label{fixest-package}}

Available functions

\begin{itemize}
\item
  \texttt{feols}: linear models
\item
  \texttt{feglm}: generalized linear models
\item
  \texttt{femlm}: maximum likelihood estimation
\item
  \texttt{feNmlm}: non-linear in RHS parameters
\item
  \texttt{fepois}: Poisson fixed-effect
\item
  \texttt{fenegbin}: negative binomial fixed-effect
\end{itemize}

Notes

\begin{itemize}
\tightlist
\item
  can only work for \texttt{fixest} object
\end{itemize}

Examples by the package's \href{https://cran.r-project.org/web/packages/fixest/vignettes/exporting_tables.html}{authors}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(fixest)}
\FunctionTok{data}\NormalTok{(airquality)}

\CommentTok{\# Setting a dictionary}
\FunctionTok{setFixest\_dict}\NormalTok{(}
    \FunctionTok{c}\NormalTok{(}
        \AttributeTok{Ozone   =} \StringTok{"Ozone (ppb)"}\NormalTok{,}
        \AttributeTok{Solar.R =} \StringTok{"Solar Radiation (Langleys)"}\NormalTok{,}
        \AttributeTok{Wind    =} \StringTok{"Wind Speed (mph)"}\NormalTok{,}
        \AttributeTok{Temp    =} \StringTok{"Temperature"}
\NormalTok{    )}
\NormalTok{)}


\CommentTok{\# On multiple estimations: see the dedicated vignette}
\NormalTok{est }\OtherTok{=} \FunctionTok{feols}\NormalTok{(}
\NormalTok{    Ozone }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Solar.R }\SpecialCharTok{+} \FunctionTok{sw0}\NormalTok{(Wind }\SpecialCharTok{+}\NormalTok{ Temp) }\SpecialCharTok{|} \FunctionTok{csw}\NormalTok{(Month, Day),}
    \AttributeTok{data =}\NormalTok{ airquality,}
    \AttributeTok{cluster =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ Day}
\NormalTok{)}

\FunctionTok{etable}\NormalTok{(est)}
\CommentTok{\#\textgreater{}                                         est.1              est.2}
\CommentTok{\#\textgreater{} Dependent Var.:                   Ozone (ppb)        Ozone (ppb)}
\CommentTok{\#\textgreater{}                                                                 }
\CommentTok{\#\textgreater{} Solar Radiation (Langleys) 0.1148*** (0.0234)   0.0522* (0.0202)}
\CommentTok{\#\textgreater{} Wind Speed (mph)                              {-}3.109*** (0.7986)}
\CommentTok{\#\textgreater{} Temperature                                    1.875*** (0.3671)}
\CommentTok{\#\textgreater{} Fixed{-}Effects:             {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} Month                                     Yes                Yes}
\CommentTok{\#\textgreater{} Day                                        No                 No}
\CommentTok{\#\textgreater{} \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}
\CommentTok{\#\textgreater{} S.E.: Clustered                       by: Day            by: Day}
\CommentTok{\#\textgreater{} Observations                              111                111}
\CommentTok{\#\textgreater{} R2                                    0.31974            0.63686}
\CommentTok{\#\textgreater{} Within R2                             0.12245            0.53154}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}                                        est.3              est.4}
\CommentTok{\#\textgreater{} Dependent Var.:                  Ozone (ppb)        Ozone (ppb)}
\CommentTok{\#\textgreater{}                                                                }
\CommentTok{\#\textgreater{} Solar Radiation (Langleys) 0.1078** (0.0329)   0.0509* (0.0236)}
\CommentTok{\#\textgreater{} Wind Speed (mph)                             {-}3.289*** (0.7777)}
\CommentTok{\#\textgreater{} Temperature                                   2.052*** (0.2415)}
\CommentTok{\#\textgreater{} Fixed{-}Effects:             {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} Month                                    Yes                Yes}
\CommentTok{\#\textgreater{} Day                                      Yes                Yes}
\CommentTok{\#\textgreater{} \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}
\CommentTok{\#\textgreater{} S.E.: Clustered                      by: Day            by: Day}
\CommentTok{\#\textgreater{} Observations                             111                111}
\CommentTok{\#\textgreater{} R2                                   0.58018            0.81604}
\CommentTok{\#\textgreater{} Within R2                            0.12074            0.61471}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes: 0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}

\CommentTok{\# in latex}
\FunctionTok{etable}\NormalTok{(est, }\AttributeTok{tex =}\NormalTok{ T)}
\CommentTok{\#\textgreater{} \textbackslash{}begingroup}
\CommentTok{\#\textgreater{} \textbackslash{}centering}
\CommentTok{\#\textgreater{} \textbackslash{}begin\{tabular\}\{lcccc\}}
\CommentTok{\#\textgreater{}    \textbackslash{}tabularnewline \textbackslash{}midrule \textbackslash{}midrule}
\CommentTok{\#\textgreater{}    Dependent Variable: \& \textbackslash{}multicolumn\{4\}\{c\}\{Ozone (ppb)\}\textbackslash{}\textbackslash{}}
\CommentTok{\#\textgreater{}    Model:                     \& (1)            \& (2)            \& (3)            \& (4)\textbackslash{}\textbackslash{}  }
\CommentTok{\#\textgreater{}    \textbackslash{}midrule}
\CommentTok{\#\textgreater{}    \textbackslash{}emph\{Variables\}\textbackslash{}\textbackslash{}}
\CommentTok{\#\textgreater{}    Solar Radiation (Langleys) \& 0.1148$\^{}\{***\}$ \& 0.0522$\^{}\{**\}$  \& 0.1078$\^{}\{***\}$ \& 0.0509$\^{}\{**\}$\textbackslash{}\textbackslash{}   }
\CommentTok{\#\textgreater{}                               \& (0.0234)       \& (0.0202)       \& (0.0329)       \& (0.0236)\textbackslash{}\textbackslash{}   }
\CommentTok{\#\textgreater{}    Wind Speed (mph)           \&                \& {-}3.109$\^{}\{***\}$ \&                \& {-}3.289$\^{}\{***\}$\textbackslash{}\textbackslash{}   }
\CommentTok{\#\textgreater{}                               \&                \& (0.7986)       \&                \& (0.7777)\textbackslash{}\textbackslash{}   }
\CommentTok{\#\textgreater{}    Temperature                \&                \& 1.875$\^{}\{***\}$  \&                \& 2.052$\^{}\{***\}$\textbackslash{}\textbackslash{}   }
\CommentTok{\#\textgreater{}                               \&                \& (0.3671)       \&                \& (0.2415)\textbackslash{}\textbackslash{}   }
\CommentTok{\#\textgreater{}    \textbackslash{}midrule}
\CommentTok{\#\textgreater{}    \textbackslash{}emph\{Fixed{-}effects\}\textbackslash{}\textbackslash{}}
\CommentTok{\#\textgreater{}    Month                      \& Yes            \& Yes            \& Yes            \& Yes\textbackslash{}\textbackslash{}  }
\CommentTok{\#\textgreater{}    Day                        \&                \&                \& Yes            \& Yes\textbackslash{}\textbackslash{}  }
\CommentTok{\#\textgreater{}    \textbackslash{}midrule}
\CommentTok{\#\textgreater{}    \textbackslash{}emph\{Fit statistics\}\textbackslash{}\textbackslash{}}
\CommentTok{\#\textgreater{}    Observations               \& 111            \& 111            \& 111            \& 111\textbackslash{}\textbackslash{}  }
\CommentTok{\#\textgreater{}    R$\^{}2$                      \& 0.31974        \& 0.63686        \& 0.58018        \& 0.81604\textbackslash{}\textbackslash{}  }
\CommentTok{\#\textgreater{}    Within R$\^{}2$               \& 0.12245        \& 0.53154        \& 0.12074        \& 0.61471\textbackslash{}\textbackslash{}  }
\CommentTok{\#\textgreater{}    \textbackslash{}midrule \textbackslash{}midrule}
\CommentTok{\#\textgreater{}    \textbackslash{}multicolumn\{5\}\{l\}\{\textbackslash{}emph\{Clustered (Day) standard{-}errors in parentheses\}\}\textbackslash{}\textbackslash{}}
\CommentTok{\#\textgreater{}    \textbackslash{}multicolumn\{5\}\{l\}\{\textbackslash{}emph\{Signif. Codes: ***: 0.01, **: 0.05, *: 0.1\}\}\textbackslash{}\textbackslash{}}
\CommentTok{\#\textgreater{} \textbackslash{}end\{tabular\}}
\CommentTok{\#\textgreater{} \textbackslash{}par\textbackslash{}endgroup}


\CommentTok{\# get the fixed{-}effects coefficients for 1 model}
\NormalTok{fixedEffects }\OtherTok{=} \FunctionTok{fixef}\NormalTok{(est[[}\DecValTok{1}\NormalTok{]])}
\FunctionTok{summary}\NormalTok{(fixedEffects)}
\CommentTok{\#\textgreater{} Fixed\_effects coefficients}
\CommentTok{\#\textgreater{} Number of fixed{-}effects for variable Month is 5.}
\CommentTok{\#\textgreater{}  Mean = 19.6 Variance = 272}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} COEFFICIENTS:}
\CommentTok{\#\textgreater{}   Month:     5     6     7     8     9}
\CommentTok{\#\textgreater{}          3.219 8.288 34.26 40.12 12.13}

\CommentTok{\# see the fixed effects for one dimension}
\NormalTok{fixedEffects}\SpecialCharTok{$}\NormalTok{Month}
\CommentTok{\#\textgreater{}         5         6         7         8         9 }
\CommentTok{\#\textgreater{}  3.218876  8.287899 34.260812 40.122257 12.130971}

\FunctionTok{plot}\NormalTok{(fixedEffects)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{12-data_files/figure-latex/unnamed-chunk-24-1} \end{center}

For \href{https://cran.r-project.org/web/packages/fixest/vignettes/multiple_estimations.html}{multiple estimation}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# set up}
\FunctionTok{library}\NormalTok{(fixest)}

\CommentTok{\# let R know the base dataset (the biggest/ultimate }
\CommentTok{\# dataset that includes everything in your analysis)}
\NormalTok{base }\OtherTok{=}\NormalTok{ iris}

\CommentTok{\# rename variables}
\FunctionTok{names}\NormalTok{(base) }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"y1"}\NormalTok{, }\StringTok{"y2"}\NormalTok{, }\StringTok{"x1"}\NormalTok{, }\StringTok{"x2"}\NormalTok{, }\StringTok{"species"}\NormalTok{)}

\NormalTok{res\_multi }\OtherTok{=} \FunctionTok{feols}\NormalTok{(}
    \FunctionTok{c}\NormalTok{(y1, y2) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x1 }\SpecialCharTok{+} \FunctionTok{csw}\NormalTok{(x2, x2 }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{) }\SpecialCharTok{|}
        \FunctionTok{sw0}\NormalTok{(species),}
    \AttributeTok{data =}\NormalTok{ base,}
    \AttributeTok{fsplit =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ species,}
    \AttributeTok{lean =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{vcov =} \StringTok{"hc1"} \CommentTok{\# can also clustered at the fixed effect level}
\NormalTok{)}
\CommentTok{\# it\textquotesingle{}s recommended to use vcov at }
\CommentTok{\# estimation stage, not summary stage}

\FunctionTok{summary}\NormalTok{(res\_multi, }\StringTok{"compact"}\NormalTok{)}
\CommentTok{\#\textgreater{}         sample   fixef lhs               rhs     (Intercept)                x1}
\CommentTok{\#\textgreater{} 1  Full sample 1        y1 x1 + x2           4.19*** (0.104)  0.542*** (0.076)}
\CommentTok{\#\textgreater{} 2  Full sample 1        y1 x1 + x2 + I(x2\^{}2) 4.27*** (0.101)  0.719*** (0.082)}
\CommentTok{\#\textgreater{} 3  Full sample 1        y2 x1 + x2           3.59*** (0.103) {-}0.257*** (0.066)}
\CommentTok{\#\textgreater{} 4  Full sample 1        y2 x1 + x2 + I(x2\^{}2) 3.68*** (0.097)    {-}0.030 (0.078)}
\CommentTok{\#\textgreater{} 5  Full sample species  y1 x1 + x2                            0.906*** (0.076)}
\CommentTok{\#\textgreater{} 6  Full sample species  y1 x1 + x2 + I(x2\^{}2)                  0.900*** (0.077)}
\CommentTok{\#\textgreater{} 7  Full sample species  y2 x1 + x2                              0.155* (0.073)}
\CommentTok{\#\textgreater{} 8  Full sample species  y2 x1 + x2 + I(x2\^{}2)                    0.148. (0.075)}
\CommentTok{\#\textgreater{} 9  setosa      1        y1 x1 + x2           4.25*** (0.474)     0.399 (0.325)}
\CommentTok{\#\textgreater{} 10 setosa      1        y1 x1 + x2 + I(x2\^{}2) 4.00*** (0.504)     0.405 (0.325)}
\CommentTok{\#\textgreater{} 11 setosa      1        y2 x1 + x2           2.89*** (0.416)     0.247 (0.305)}
\CommentTok{\#\textgreater{} 12 setosa      1        y2 x1 + x2 + I(x2\^{}2) 2.82*** (0.423)     0.248 (0.304)}
\CommentTok{\#\textgreater{} 13 setosa      species  y1 x1 + x2                               0.399 (0.325)}
\CommentTok{\#\textgreater{} 14 setosa      species  y1 x1 + x2 + I(x2\^{}2)                     0.405 (0.325)}
\CommentTok{\#\textgreater{} 15 setosa      species  y2 x1 + x2                               0.247 (0.305)}
\CommentTok{\#\textgreater{} 16 setosa      species  y2 x1 + x2 + I(x2\^{}2)                     0.248 (0.304)}
\CommentTok{\#\textgreater{} 17 versicolor  1        y1 x1 + x2           2.38*** (0.423)  0.934*** (0.166)}
\CommentTok{\#\textgreater{} 18 versicolor  1        y1 x1 + x2 + I(x2\^{}2)   0.323 (1.44)   0.901*** (0.164)}
\CommentTok{\#\textgreater{} 19 versicolor  1        y2 x1 + x2           1.25*** (0.275)     0.067 (0.095)}
\CommentTok{\#\textgreater{} 20 versicolor  1        y2 x1 + x2 + I(x2\^{}2)   0.097 (1.01)      0.048 (0.099)}
\CommentTok{\#\textgreater{} 21 versicolor  species  y1 x1 + x2                            0.934*** (0.166)}
\CommentTok{\#\textgreater{} 22 versicolor  species  y1 x1 + x2 + I(x2\^{}2)                  0.901*** (0.164)}
\CommentTok{\#\textgreater{} 23 versicolor  species  y2 x1 + x2                               0.067 (0.095)}
\CommentTok{\#\textgreater{} 24 versicolor  species  y2 x1 + x2 + I(x2\^{}2)                     0.048 (0.099)}
\CommentTok{\#\textgreater{} 25 virginica   1        y1 x1 + x2             1.05. (0.539)  0.995*** (0.090)}
\CommentTok{\#\textgreater{} 26 virginica   1        y1 x1 + x2 + I(x2\^{}2)   {-}2.39 (2.04)   0.994*** (0.088)}
\CommentTok{\#\textgreater{} 27 virginica   1        y2 x1 + x2             1.06. (0.572)     0.149 (0.107)}
\CommentTok{\#\textgreater{} 28 virginica   1        y2 x1 + x2 + I(x2\^{}2)    1.10 (1.76)      0.149 (0.108)}
\CommentTok{\#\textgreater{} 29 virginica   species  y1 x1 + x2                            0.995*** (0.090)}
\CommentTok{\#\textgreater{} 30 virginica   species  y1 x1 + x2 + I(x2\^{}2)                  0.994*** (0.088)}
\CommentTok{\#\textgreater{} 31 virginica   species  y2 x1 + x2                               0.149 (0.107)}
\CommentTok{\#\textgreater{} 32 virginica   species  y2 x1 + x2 + I(x2\^{}2)                     0.149 (0.108)}
\CommentTok{\#\textgreater{}                  x2          I(x2\^{}2)}
\CommentTok{\#\textgreater{} 1   {-}0.320. (0.170)                 }
\CommentTok{\#\textgreater{} 2  {-}1.52*** (0.307) 0.348*** (0.075)}
\CommentTok{\#\textgreater{} 3    0.364* (0.142)                 }
\CommentTok{\#\textgreater{} 4  {-}1.18*** (0.313) 0.446*** (0.074)}
\CommentTok{\#\textgreater{} 5    {-}0.006 (0.163)                 }
\CommentTok{\#\textgreater{} 6     0.290 (0.408)   {-}0.088 (0.117)}
\CommentTok{\#\textgreater{} 7  0.623*** (0.114)                 }
\CommentTok{\#\textgreater{} 8    0.951* (0.472)   {-}0.097 (0.125)}
\CommentTok{\#\textgreater{} 9    0.712. (0.418)                 }
\CommentTok{\#\textgreater{} 10    2.51. (1.47)     {-}2.91 (2.10) }
\CommentTok{\#\textgreater{} 11    0.702 (0.560)                 }
\CommentTok{\#\textgreater{} 12     1.27 (2.39)    {-}0.911 (3.28) }
\CommentTok{\#\textgreater{} 13   0.712. (0.418)                 }
\CommentTok{\#\textgreater{} 14    2.51. (1.47)     {-}2.91 (2.10) }
\CommentTok{\#\textgreater{} 15    0.702 (0.560)                 }
\CommentTok{\#\textgreater{} 16     1.27 (2.39)    {-}0.911 (3.28) }
\CommentTok{\#\textgreater{} 17   {-}0.320 (0.364)                 }
\CommentTok{\#\textgreater{} 18     3.01 (2.31)     {-}1.24 (0.841)}
\CommentTok{\#\textgreater{} 19 0.929*** (0.244)                 }
\CommentTok{\#\textgreater{} 20    2.80. (1.65)    {-}0.695 (0.583)}
\CommentTok{\#\textgreater{} 21   {-}0.320 (0.364)                 }
\CommentTok{\#\textgreater{} 22     3.01 (2.31)     {-}1.24 (0.841)}
\CommentTok{\#\textgreater{} 23 0.929*** (0.244)                 }
\CommentTok{\#\textgreater{} 24    2.80. (1.65)    {-}0.695 (0.583)}
\CommentTok{\#\textgreater{} 25    0.007 (0.205)                 }
\CommentTok{\#\textgreater{} 26    3.50. (2.09)    {-}0.870 (0.519)}
\CommentTok{\#\textgreater{} 27 0.535*** (0.122)                 }
\CommentTok{\#\textgreater{} 28    0.503 (1.56)     0.008 (0.388)}
\CommentTok{\#\textgreater{} 29    0.007 (0.205)                 }
\CommentTok{\#\textgreater{} 30    3.50. (2.09)    {-}0.870 (0.519)}
\CommentTok{\#\textgreater{} 31 0.535*** (0.122)                 }
\CommentTok{\#\textgreater{} 32    0.503 (1.56)     0.008 (0.388)}

\CommentTok{\# call the first 3 estimated models only}
\FunctionTok{etable}\NormalTok{(res\_multi[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{],}
       
       \CommentTok{\# customize the headers}
       \AttributeTok{headers =} \FunctionTok{c}\NormalTok{(}\StringTok{"mod1"}\NormalTok{, }\StringTok{"mod2"}\NormalTok{, }\StringTok{"mod3"}\NormalTok{)) }
\CommentTok{\#\textgreater{}                   res\_multi[1:3].1   res\_multi[1:3].2    res\_multi[1:3].3}
\CommentTok{\#\textgreater{}                               mod1               mod2                mod3}
\CommentTok{\#\textgreater{} Dependent Var.:                 y1                 y1                  y2}
\CommentTok{\#\textgreater{}                                                                          }
\CommentTok{\#\textgreater{} Constant         4.191*** (0.1037)  4.266*** (0.1007)   3.587*** (0.1031)}
\CommentTok{\#\textgreater{} x1              0.5418*** (0.0761) 0.7189*** (0.0815) {-}0.2571*** (0.0664)}
\CommentTok{\#\textgreater{} x2               {-}0.3196. (0.1700) {-}1.522*** (0.3072)    0.3640* (0.1419)}
\CommentTok{\#\textgreater{} x2 square                          0.3479*** (0.0748)                    }
\CommentTok{\#\textgreater{} \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}
\CommentTok{\#\textgreater{} S.E. type       Heteroskedas.{-}rob. Heteroskedas.{-}rob. Heteroskedast.{-}rob.}
\CommentTok{\#\textgreater{} Observations                   150                150                 150}
\CommentTok{\#\textgreater{} R2                         0.76626            0.79456             0.21310}
\CommentTok{\#\textgreater{} Adj. R2                    0.76308            0.79034             0.20240}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes: 0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

\hypertarget{multiple-estimation-left-hand-side}{%
\paragraph{Multiple estimation (Left-hand side)}\label{multiple-estimation-left-hand-side}}

\begin{itemize}
\tightlist
\item
  When you have multiple interested dependent variables
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{etable}\NormalTok{(}\FunctionTok{feols}\NormalTok{(}\FunctionTok{c}\NormalTok{(y1, y2) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x1 }\SpecialCharTok{+}\NormalTok{ x2, base))}
\CommentTok{\#\textgreater{}                 feols(c(y1, y2)..1 feols(c(y1, y2) ..2}
\CommentTok{\#\textgreater{} Dependent Var.:                 y1                  y2}
\CommentTok{\#\textgreater{}                                                       }
\CommentTok{\#\textgreater{} Constant         4.191*** (0.0970)   3.587*** (0.0937)}
\CommentTok{\#\textgreater{} x1              0.5418*** (0.0693) {-}0.2571*** (0.0669)}
\CommentTok{\#\textgreater{} x2               {-}0.3196* (0.1605)    0.3640* (0.1550)}
\CommentTok{\#\textgreater{} \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}
\CommentTok{\#\textgreater{} S.E. type                      IID                 IID}
\CommentTok{\#\textgreater{} Observations                   150                 150}
\CommentTok{\#\textgreater{} R2                         0.76626             0.21310}
\CommentTok{\#\textgreater{} Adj. R2                    0.76308             0.20240}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes: 0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

To input a list of dependent variable

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{depvars }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"y1"}\NormalTok{, }\StringTok{"y2"}\NormalTok{)}

\NormalTok{res }\OtherTok{\textless{}{-}} \FunctionTok{lapply}\NormalTok{(depvars, }\ControlFlowTok{function}\NormalTok{(var) \{}
\NormalTok{    res }\OtherTok{\textless{}{-}} \FunctionTok{feols}\NormalTok{(}\FunctionTok{xpd}\NormalTok{(..lhs }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x1 }\SpecialCharTok{+}\NormalTok{ x2, }\AttributeTok{..lhs =}\NormalTok{ var), }\AttributeTok{data =}\NormalTok{ base)}
    \CommentTok{\# summary(res)}
\NormalTok{\})}
\FunctionTok{etable}\NormalTok{(res)}
\CommentTok{\#\textgreater{}                            model 1             model 2}
\CommentTok{\#\textgreater{} Dependent Var.:                 y1                  y2}
\CommentTok{\#\textgreater{}                                                       }
\CommentTok{\#\textgreater{} Constant         4.191*** (0.0970)   3.587*** (0.0937)}
\CommentTok{\#\textgreater{} x1              0.5418*** (0.0693) {-}0.2571*** (0.0669)}
\CommentTok{\#\textgreater{} x2               {-}0.3196* (0.1605)    0.3640* (0.1550)}
\CommentTok{\#\textgreater{} \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}
\CommentTok{\#\textgreater{} S.E. type                      IID                 IID}
\CommentTok{\#\textgreater{} Observations                   150                 150}
\CommentTok{\#\textgreater{} R2                         0.76626             0.21310}
\CommentTok{\#\textgreater{} Adj. R2                    0.76308             0.20240}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes: 0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

\hypertarget{multiple-estimation-right-hand-side}{%
\paragraph{Multiple estimation (Right-hand side)}\label{multiple-estimation-right-hand-side}}

Options to write the functions

\begin{itemize}
\item
  \texttt{sw} (stepwise): sequentially analyze each elements

  \begin{itemize}
  \tightlist
  \item
    \texttt{y\ \textasciitilde{}\ sw(x1,\ x2)} will be estimated as \texttt{y\ \textasciitilde{}\ x1} and \texttt{y\ \textasciitilde{}\ x2}
  \end{itemize}
\item
  \texttt{sw0} (stepwise 0): similar to \texttt{sw} but also estimate a model without the elements in the set first

  \begin{itemize}
  \tightlist
  \item
    \texttt{y\ \textasciitilde{}\ sw(x1,\ x2)} will be estimated as \texttt{y\ \textasciitilde{}\ 1} and \texttt{y\ \textasciitilde{}\ x1} and \texttt{y\ \textasciitilde{}\ x2}
  \end{itemize}
\item
  \texttt{csw} (cumulative stepwise): sequentially add each element of the set to the formula

  \begin{itemize}
  \tightlist
  \item
    \texttt{y\ \textasciitilde{}\ csw(x1,\ x2)} will be estimated as \texttt{y\ \textasciitilde{}\ x1} and \texttt{y\ \textasciitilde{}\ x1\ +\ x2}
  \end{itemize}
\item
  \texttt{csw0} (cumulative stepwise 0): similar to \texttt{csw} but also estimate a model without the elements in the set first

  \begin{itemize}
  \tightlist
  \item
    \texttt{y\ \textasciitilde{}\ csw(x1,\ x2)} will be estimated as \texttt{y\textasciitilde{}\ 1} \texttt{y\ \textasciitilde{}\ x1} and \texttt{y\ \textasciitilde{}\ x1\ +\ x2}
  \end{itemize}
\item
  \texttt{mvsw} (multiverse stepwise): all possible combination of the elements in the set (it will get large very quick).

  \begin{itemize}
  \tightlist
  \item
    \texttt{mvsw(x1,\ x2,\ x3)} will be \texttt{sw0(x1,\ x2,\ x3,\ x1\ +\ x2,\ x1\ +\ x3,\ x2\ +\ x3,\ x1\ +\ x2\ +\ x3)}
  \end{itemize}
\end{itemize}

\hypertarget{split-sample-estimation}{%
\paragraph{Split sample estimation}\label{split-sample-estimation}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{etable}\NormalTok{(}\FunctionTok{feols}\NormalTok{(y1 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x1 }\SpecialCharTok{+}\NormalTok{ x2, }\AttributeTok{fsplit =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ species, }\AttributeTok{data =}\NormalTok{ base))}
\CommentTok{\#\textgreater{}                  feols(y1 \textasciitilde{} x1 +..1 feols(y1 \textasciitilde{} x1 ..2 feols(y1 \textasciitilde{} x1 +..3}
\CommentTok{\#\textgreater{} Sample (species)        Full sample            setosa         versicolor}
\CommentTok{\#\textgreater{} Dependent Var.:                  y1                y1                 y1}
\CommentTok{\#\textgreater{}                                                                         }
\CommentTok{\#\textgreater{} Constant          4.191*** (0.0970) 4.248*** (0.4114)  2.381*** (0.4493)}
\CommentTok{\#\textgreater{} x1               0.5418*** (0.0693)   0.3990 (0.2958) 0.9342*** (0.1693)}
\CommentTok{\#\textgreater{} x2                {-}0.3196* (0.1605)   0.7121 (0.4874)   {-}0.3200 (0.4024)}
\CommentTok{\#\textgreater{} \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}
\CommentTok{\#\textgreater{} S.E. type                       IID               IID                IID}
\CommentTok{\#\textgreater{} Observations                    150                50                 50}
\CommentTok{\#\textgreater{} R2                          0.76626           0.11173            0.57432}
\CommentTok{\#\textgreater{} Adj. R2                     0.76308           0.07393            0.55620}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}                  feols(y1 \textasciitilde{} x1 +..4}
\CommentTok{\#\textgreater{} Sample (species)          virginica}
\CommentTok{\#\textgreater{} Dependent Var.:                  y1}
\CommentTok{\#\textgreater{}                                    }
\CommentTok{\#\textgreater{} Constant            1.052* (0.5139)}
\CommentTok{\#\textgreater{} x1               0.9946*** (0.0893)}
\CommentTok{\#\textgreater{} x2                  0.0071 (0.1795)}
\CommentTok{\#\textgreater{} \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}
\CommentTok{\#\textgreater{} S.E. type                       IID}
\CommentTok{\#\textgreater{} Observations                     50}
\CommentTok{\#\textgreater{} R2                          0.74689}
\CommentTok{\#\textgreater{} Adj. R2                     0.73612}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes: 0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

\hypertarget{standard-errors-1}{%
\paragraph{Standard Errors}\label{standard-errors-1}}

\begin{itemize}
\item
  \texttt{iid}: errors are homoskedastic and independent and identically distributed
\item
  \texttt{hetero}: errors are heteroskedastic using White correction
\item
  \texttt{cluster}: errors are correlated within the cluster groups
\item
  \texttt{newey\_west}: \citep{newey1986simple} use for time series or panel data. Errors are heteroskedastic and serially correlated.

  \begin{itemize}
  \item
    \texttt{vcov\ =\ newey\_west\ \textasciitilde{}\ id\ +\ period} where \texttt{id} is the subject id and \texttt{period} is time period of the panel.
  \item
    to specify lag period to consider \texttt{vcov\ =\ newey\_west(2)\ \textasciitilde{}\ id\ +\ period} where we're considering 2 lag periods.
  \end{itemize}
\item
  \texttt{driscoll\_kraay} \citep{driscoll1998consistent} use for panel data. Errors are cross-sectionally and serially correlated.

  \begin{itemize}
  \tightlist
  \item
    \texttt{vcov\ =\ discoll\_kraay\ \textasciitilde{}\ period}
  \end{itemize}
\item
  \texttt{conley}: \citep{conley1999gmm} for cross-section data. Errors are spatially correlated

  \begin{itemize}
  \item
    \texttt{vcov\ =\ conley\ \textasciitilde{}\ latitude\ +\ longitude}
  \item
    to specify the distance cutoff, \texttt{vcov\ =\ vcov\_conley(lat\ =\ "lat",\ lon\ =\ "long",\ cutoff\ =\ 100,\ distance\ =\ "spherical")}, which will use the \texttt{conley()} helper function.
  \end{itemize}
\item
  \texttt{hc}: from the \texttt{sandwich} package

  \begin{itemize}
  \tightlist
  \item
    \texttt{vcov\ =\ function(x)\ sandwich::vcovHC(x,\ type\ =\ "HC1"))}
  \end{itemize}
\end{itemize}

To let R know which SE estimation you want to use, insert \texttt{vcov\ =\ vcov\_type\ \textasciitilde{}\ variables}

\hypertarget{small-sample-correction}{%
\paragraph{Small sample correction}\label{small-sample-correction}}

To specify that R needs to use small sample correction add

\texttt{ssc\ =\ ssc(adj\ =\ T,\ cluster.adj\ =\ T)}

\hypertarget{variable-transformation}{%
\chapter{Variable Transformation}\label{variable-transformation}}

\texttt{trafo} \href{https://cran.microsoft.com/snapshot/2018-08-09/web/packages/trafo/vignettes/vignette_trafo.pdf}{vignette}

\hypertarget{continuous-variables}{%
\section{Continuous Variables}\label{continuous-variables}}

Purposes:

\begin{itemize}
\item
  To change the scale of the variables
\item
  To transform skewed data distribution to normal distribution
\end{itemize}

\hypertarget{standardization}{%
\subsection{Standardization}\label{standardization}}

\[
x_i' = \frac{x_i - \bar{x}}{s}
\]

when you have a few large numbers

\hypertarget{min-max-scaling}{%
\subsection{Min-max scaling}\label{min-max-scaling}}

\[
x_i' = \frac{x_i - x_{max}}{x_{max} - x_{min}}
\]

dependent on the min and max values, which makes it sensitive to outliers.

best to use when you have values in a fixed interval.

\hypertarget{square-rootcube-root}{%
\subsection{Square Root/Cube Root}\label{square-rootcube-root}}

\begin{itemize}
\item
  When variables have positive skewness or residuals have positive heteroskasticity.
\item
  Frequency counts variable
\item
  Data have many 0 or extremely small values.
\end{itemize}

\hypertarget{logarithmic}{%
\subsection{Logarithmic}\label{logarithmic}}

\begin{itemize}
\tightlist
\item
  Variables have positively skewed distribution
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5556}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.4444}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Formula
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
In case
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(x_i' = \log(x_i)\) & cannot work zero because \texttt{log(0)\ =\ -Inf} \\
\(x_i' = \log(x_i + 1)\) & variables with 0 \\
\(x_i' = \log(x_i +c)\) & \\
\(x_i' = \frac{x_i}{|x_i|}\log|x_i|\) & variables with negative values \\
\(x_i'^\lambda = \log(x_i + \sqrt{x_i^2 + \lambda})\) & generalized log transformation \\
\end{longtable}

For the general case of \(\log(x_i + c)\), choosing a constant is rather tricky.

The choice of the constant is critically important, especially when you want to do inference. It can dramatically change your model fit (see \citep{ekwaru2018overlooked} for the independent variable case).

However, assuming that you do not have 0s because of

\begin{itemize}
\item
  Censoring
\item
  No measurement errors (stemming from measurement tools)
\end{itemize}

We can proceed choosing \texttt{c} (it's okay if your 0's are represent really small values).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(cars)}
\NormalTok{cars}\SpecialCharTok{$}\NormalTok{speed }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{} [1] 4 4 7 7 8 9}

\FunctionTok{log}\NormalTok{(cars}\SpecialCharTok{$}\NormalTok{speed) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{} [1] 1.386294 1.386294 1.945910 1.945910 2.079442 2.197225}

\CommentTok{\# log(x+1)}
\FunctionTok{log1p}\NormalTok{(cars}\SpecialCharTok{$}\NormalTok{speed) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{} [1] 1.609438 1.609438 2.079442 2.079442 2.197225 2.302585}
\end{Highlighting}
\end{Shaded}

\hypertarget{exponential-1}{%
\subsection{Exponential}\label{exponential-1}}

\begin{itemize}
\item
  Negatively skewed data
\item
  Underlying logarithmic trend (e.g., survival, decay)
\end{itemize}

\hypertarget{power-2}{%
\subsection{Power}\label{power-2}}

\begin{itemize}
\tightlist
\item
  Variables have negatively skewed distribution
\end{itemize}

\hypertarget{inversereciprocal}{%
\subsection{Inverse/Reciprocal}\label{inversereciprocal}}

\begin{itemize}
\item
  Variables have platykurtic distribution
\item
  Data are positively skewed
\item
  Ratio data
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(cars)}
\FunctionTok{head}\NormalTok{(cars}\SpecialCharTok{$}\NormalTok{dist)}
\CommentTok{\#\textgreater{} [1]  2 10  4 22 16 10}
\FunctionTok{plot}\NormalTok{(cars}\SpecialCharTok{$}\NormalTok{dist)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-2-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(}\DecValTok{1}\SpecialCharTok{/}\NormalTok{(cars}\SpecialCharTok{$}\NormalTok{dist))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-2-2} \end{center}

\hypertarget{hyperbolic-arcsine}{%
\subsection{Hyperbolic arcsine}\label{hyperbolic-arcsine}}

\begin{itemize}
\tightlist
\item
  Variables with positively skewed distribution
\end{itemize}

\hypertarget{ordered-quantile-norm}{%
\subsection{Ordered Quantile Norm}\label{ordered-quantile-norm}}

\begin{itemize}
\tightlist
\item
  \citep{bartlett1947use}
\end{itemize}

\[
x_i' = \Phi^{-1} (\frac{rank(x_i) - 1/2}{length(x)})
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ord\_dist }\OtherTok{\textless{}{-}}\NormalTok{ bestNormalize}\SpecialCharTok{::}\FunctionTok{orderNorm}\NormalTok{(cars}\SpecialCharTok{$}\NormalTok{dist)}
\NormalTok{ord\_dist}
\CommentTok{\#\textgreater{} orderNorm Transformation with 50 nonmissing obs and ties}
\CommentTok{\#\textgreater{}  {-} 35 unique values }
\CommentTok{\#\textgreater{}  {-} Original quantiles:}
\CommentTok{\#\textgreater{}   0\%  25\%  50\%  75\% 100\% }
\CommentTok{\#\textgreater{}    2   26   36   56  120}
\NormalTok{ord\_dist}\SpecialCharTok{$}\NormalTok{x.t }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{hist}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-3-1} \end{center}

\hypertarget{arcsinh}{%
\subsection{Arcsinh}\label{arcsinh}}

\begin{itemize}
\tightlist
\item
  Proportion variable (0-1)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cars}\SpecialCharTok{$}\NormalTok{dist }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{hist}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-4-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cars$dist \%\textgreater{}\% MASS::truehist()}

\NormalTok{as\_dist }\OtherTok{\textless{}{-}}\NormalTok{ bestNormalize}\SpecialCharTok{::}\FunctionTok{arcsinh\_x}\NormalTok{(cars}\SpecialCharTok{$}\NormalTok{dist)}
\NormalTok{as\_dist}
\CommentTok{\#\textgreater{} Standardized asinh(x) Transformation with 50 nonmissing obs.:}
\CommentTok{\#\textgreater{}  Relevant statistics:}
\CommentTok{\#\textgreater{}  {-} mean (before standardization) = 4.230843 }
\CommentTok{\#\textgreater{}  {-} sd (before standardization) = 0.7710887}
\NormalTok{as\_dist}\SpecialCharTok{$}\NormalTok{x.t }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{hist}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-4-2} \end{center}

\hypertarget{lambert-w-x-f-transformation}{%
\subsection{Lambert W x F Transformation}\label{lambert-w-x-f-transformation}}

\texttt{LambertW} package

\begin{itemize}
\item
  Using moments to normalize data.
\item
  Usually need to compare with the \protect\hyperlink{box-cox-transformation}{Box-Cox Transformation} and \protect\hyperlink{yeo-johnson-transformation}{Yeo-Johnson Transformation}
\item
  Can handle skewness, heavy-tailed.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(cars)}
\FunctionTok{head}\NormalTok{(cars}\SpecialCharTok{$}\NormalTok{dist)}
\CommentTok{\#\textgreater{} [1]  2 10  4 22 16 10}
\NormalTok{cars}\SpecialCharTok{$}\NormalTok{dist }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{hist}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-5-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]


\NormalTok{l\_dist }\OtherTok{\textless{}{-}}\NormalTok{ LambertW}\SpecialCharTok{::}\FunctionTok{Gaussianize}\NormalTok{(cars}\SpecialCharTok{$}\NormalTok{dist)}
\CommentTok{\# small fix}
\NormalTok{l\_dist }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{hist}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-5-2} \end{center}

\hypertarget{inverse-hyperbolic-sine-ihs-transformation}{%
\subsection{Inverse Hyperbolic Sine (IHS) transformation}\label{inverse-hyperbolic-sine-ihs-transformation}}

\begin{itemize}
\item
  Proposed by \citep{johnson1949}
\item
  Can be applied to real numbers.
\end{itemize}

\[
\begin{aligned}
f(x,\theta) &= \frac{\sinh^{-1} (\theta x)}{\theta} \\
&= \frac{\log(\theta x + (\theta^2 x^2 + 1)^{1/2})}{\theta}
\end{aligned}
\]

\hypertarget{box-cox-transformation}{%
\subsection{Box-Cox Transformation}\label{box-cox-transformation}}

\[
y^\lambda = \beta x+ \epsilon
\]

to fix non-linearity in the error terms

work well between (-3,3) (i.e., small transformation).

or with independent variables

\[
x_i'^\lambda = 
\begin{cases}
\frac{x_i^\lambda-1}{\lambda} & \text{if } \lambda \neq 0\\
\log(x_i) & \text{if } \lambda = 0
\end{cases}
\]

And the two-parameter version is

\[
x_i' (\lambda_1, \lambda_2) = 
\begin{cases}
\frac{(x_i + \lambda_2)^{\lambda_1}-1}{} & \text{if } \lambda_1 \neq 0 \\
\log(x_i + \lambda_2) & \text{if } \lambda_1 = 0
\end{cases}
\]

More advances

\begin{itemize}
\item
  \citep{manly1976exponential}
\item
  \citep{bickel1981analysis, box1981analysis}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(MASS)}
\FunctionTok{data}\NormalTok{(cars)}
\NormalTok{mod }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(cars}\SpecialCharTok{$}\NormalTok{speed }\SpecialCharTok{\textasciitilde{}}\NormalTok{ cars}\SpecialCharTok{$}\NormalTok{dist, }\AttributeTok{data =}\NormalTok{ cars)}
\CommentTok{\# check residuals}
\FunctionTok{plot}\NormalTok{(mod)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-6-1} \end{center}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-6-2} \end{center}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-6-3} \end{center}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-6-4} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{bc }\OtherTok{\textless{}{-}} \FunctionTok{boxcox}\NormalTok{(mod, }\AttributeTok{lambda =} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-6-5} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# best lambda}
\NormalTok{bc}\SpecialCharTok{$}\NormalTok{x[}\FunctionTok{which}\NormalTok{(bc}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{==} \FunctionTok{max}\NormalTok{(bc}\SpecialCharTok{$}\NormalTok{y))]}
\CommentTok{\#\textgreater{} [1] 1.242424}

\CommentTok{\# model with best lambda}
\NormalTok{mod\_lambda }\OtherTok{=} \FunctionTok{lm}\NormalTok{(cars}\SpecialCharTok{$}\NormalTok{speed }\SpecialCharTok{\^{}}\NormalTok{ (bc}\SpecialCharTok{$}\NormalTok{x[}\FunctionTok{which}\NormalTok{(bc}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{==} \FunctionTok{max}\NormalTok{(bc}\SpecialCharTok{$}\NormalTok{y))]) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ cars}\SpecialCharTok{$}\NormalTok{dist, }
                \AttributeTok{data =}\NormalTok{ cars)}
\FunctionTok{plot}\NormalTok{(mod\_lambda)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-6-6} \end{center}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-6-7} \end{center}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-6-8} \end{center}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-6-9} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# 2{-}parameter version}
\NormalTok{two\_bc }\OtherTok{=}\NormalTok{ geoR}\SpecialCharTok{::}\FunctionTok{boxcoxfit}\NormalTok{(cars}\SpecialCharTok{$}\NormalTok{speed)}
\NormalTok{two\_bc}
\CommentTok{\#\textgreater{} Fitted parameters:}
\CommentTok{\#\textgreater{}    lambda      beta   sigmasq }
\CommentTok{\#\textgreater{}  1.028798 15.253008 31.935297 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Convergence code returned by optim: 0}
\FunctionTok{plot}\NormalTok{(two\_bc)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-6-10} \end{center}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-6-11} \end{center}

\begin{Shaded}
\begin{Highlighting}[]


\CommentTok{\# bestNormalize}
\NormalTok{bc\_dist }\OtherTok{\textless{}{-}}\NormalTok{ bestNormalize}\SpecialCharTok{::}\FunctionTok{boxcox}\NormalTok{(cars}\SpecialCharTok{$}\NormalTok{dist)}
\NormalTok{bc\_dist}
\CommentTok{\#\textgreater{} Standardized Box Cox Transformation with 50 nonmissing obs.:}
\CommentTok{\#\textgreater{}  Estimated statistics:}
\CommentTok{\#\textgreater{}  {-} lambda = 0.4950628 }
\CommentTok{\#\textgreater{}  {-} mean (before standardization) = 10.35636 }
\CommentTok{\#\textgreater{}  {-} sd (before standardization) = 3.978036}
\NormalTok{bc\_dist}\SpecialCharTok{$}\NormalTok{x.t }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{hist}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-6-12} \end{center}

\hypertarget{yeo-johnson-transformation}{%
\subsection{Yeo-Johnson Transformation}\label{yeo-johnson-transformation}}

Similar to \protect\hyperlink{box-cox-transformation}{Box-Cox Transformation} (when \(\lambda = 1\)), but allows for negative value

\[
x_i'^\lambda = 
\begin{cases}
\frac{(x_i+1)^\lambda -1}{\lambda} & \text{if } \lambda \neq0, x_i \ge 0 \\
\log(x_i + 1) & \text{if } \lambda = 0, x_i \ge 0 \\
\frac{-[(-x_i+1)^{2-\lambda}-1]}{2 - \lambda} & \text{if } \lambda \neq 2, x_i <0 \\
-\log(-x_i + 1) & \text{if } \lambda = 2, x_i <0 
\end{cases}
\]

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(cars)}
\NormalTok{yj\_speed }\OtherTok{\textless{}{-}}\NormalTok{ bestNormalize}\SpecialCharTok{::}\FunctionTok{yeojohnson}\NormalTok{(cars}\SpecialCharTok{$}\NormalTok{speed)}
\NormalTok{yj\_speed}\SpecialCharTok{$}\NormalTok{x.t }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{hist}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-7-1} \end{center}

\hypertarget{rankgauss}{%
\subsection{RankGauss}\label{rankgauss}}

\begin{itemize}
\tightlist
\item
  Turn values into ranks, then ranks to values under normal distribution.
\end{itemize}

\hypertarget{summary-4}{%
\subsection{Summary}\label{summary-4}}

Automatically choose the best method to normalize data (\href{https://cran.r-project.org/web/packages/bestNormalize/vignettes/bestNormalize.html}{code} by \texttt{bestNormalize})

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bestdist }\OtherTok{\textless{}{-}}\NormalTok{ bestNormalize}\SpecialCharTok{::}\FunctionTok{bestNormalize}\NormalTok{(cars}\SpecialCharTok{$}\NormalTok{dist)}
\NormalTok{bestdist}\SpecialCharTok{$}\NormalTok{x.t }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{hist}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-8-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\FunctionTok{boxplot}\NormalTok{(}\FunctionTok{log10}\NormalTok{(bestdist}\SpecialCharTok{$}\NormalTok{oos\_preds), }\AttributeTok{yaxt =} \StringTok{"n"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{13-variable_transformation_files/figure-latex/unnamed-chunk-8-2} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# axis(2, at = log10(c(.1, .5, 1, 2, 5, 10)), }
\CommentTok{\#      labels = c(.1, .5, 1, 2, 5, 10))}
\end{Highlighting}
\end{Shaded}

\hypertarget{categorical-variables}{%
\section{Categorical Variables}\label{categorical-variables}}

Purposes

\begin{itemize}
\tightlist
\item
  To transform to continuous variable (for machine learning models) (e.g., encoding/ embedding in text mining)
\end{itemize}

Approaches:

\begin{itemize}
\item
  One-hot encoding
\item
  Label encoding
\item
  Feature hashing
\item
  Binary encoding
\item
  Base N encoding
\item
  Frequency encoding
\item
  Target encoding
\item
  Ordinal encoding
\item
  Helmert encoding
\item
  Mean encoding
\item
  Weight of evidence encoding
\item
  Probability ratio encoding
\item
  Backward difference encoding
\item
  Leave one out encoding
\item
  James-Stein encoding
\item
  M-estimator encoding
\item
  Thermometer encoding
\end{itemize}

\hypertarget{hypothesis-testing}{%
\chapter{Hypothesis Testing}\label{hypothesis-testing}}

Error types:

\begin{itemize}
\item
  Type I Error (False Positive):

  \begin{itemize}
  \tightlist
  \item
    Reality: nope
  \item
    Diagnosis/Analysis: yes
  \end{itemize}
\item
  Type II Error (False Negative):

  \begin{itemize}
  \tightlist
  \item
    Reality: yes
  \item
    Diagnosis/Analysis: nope
  \end{itemize}
\end{itemize}

Power: The probability of rejecting the null hypothesis when it is actually false

\textbf{Note:}

\begin{itemize}
\item
  Always written in terms of the population parameter (\(\beta\)) not the estimator/estimate (\(\hat{\beta}\))
\item
  Sometimes, different disciplines prefer to use \(\beta\) (i.e., standardized coefficient), or \(\mathbf{b}\) (i.e., unstandardized coefficient)

  \begin{itemize}
  \item
    \(\beta\) and \(\mathbf{b}\) are similar in interpretation; however, \(\beta\) is scale free. Hence, you can see the relative contribution of \(\beta\) to the dependent variable. On the other hand, \(\mathbf{b}\) can be more easily used in policy decisions.
  \item
    \[
    \beta_j = \mathbf{b} \frac{s_{x_j}}{s_y}
    \]
  \end{itemize}
\item
  Assuming the null hypothesis is true, what is the (asymptotic) distribution of the estimator
\item
  Two-sided
\end{itemize}

\[
\begin{aligned}
&H_0: \beta_j = 0 \\
&H_1: \beta_j \neq 0 
\end{aligned}
\]

then under the null, the OLS estimator has the following distribution

\[
A1-A3a, A5: \sqrt{n} \hat{\beta_j}  \sim  N(0,Avar(\sqrt{n}\hat{\beta}_j))
\]

\begin{itemize}
\tightlist
\item
  For the one-sided test, the null is a set of values, so now you choose the worst case single value that is hardest to prove and derive the distribution under the null
\item
  One-sided
\end{itemize}

\[
\begin{aligned}
&H_0: \beta_j\ge 0 \\
&H_1: \beta_j < 0 
\end{aligned}
\]

then the hardest null value to prove is \(H_0: \beta_j=0\). Then under this specific null, the OLS estimator has the following asymptotic distribution

\[
A1-A3a, A5: \sqrt{n}\hat{\beta_j} \sim N(0,Avar(\sqrt{n}\hat{\beta}_j))
\]

\hypertarget{types-of-hypothesis-testing}{%
\section{Types of hypothesis testing}\label{types-of-hypothesis-testing}}

\(H_0 : \theta = \theta_0\)

\(H_1 : \theta \neq \theta_0\)

How far away / extreme \(\theta\) can be if our null hypothesis is true

Assume that our likelihood function for q is \(L(q) = q^{30}(1-q)^{70}\) \textbf{Likelihood function}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{q }\OtherTok{=} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{length =} \DecValTok{100}\NormalTok{)}
\NormalTok{L }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(q) \{}
\NormalTok{    q }\SpecialCharTok{\^{}} \DecValTok{30} \SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ q) }\SpecialCharTok{\^{}} \DecValTok{70}
\NormalTok{\}}

\FunctionTok{plot}\NormalTok{(q,}
     \FunctionTok{L}\NormalTok{(q),}
     \AttributeTok{ylab =} \StringTok{"L(q)"}\NormalTok{,}
     \AttributeTok{xlab =} \StringTok{"q"}\NormalTok{,}
     \AttributeTok{type =} \StringTok{"l"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{14-hypothesis_files/figure-latex/unnamed-chunk-1-1} \end{center}

\textbf{Log-Likelihood function}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{q }\OtherTok{=} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{length =} \DecValTok{100}\NormalTok{)}
\NormalTok{l }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(q) \{}
    \DecValTok{30} \SpecialCharTok{*} \FunctionTok{log}\NormalTok{(q) }\SpecialCharTok{+} \DecValTok{70} \SpecialCharTok{*} \FunctionTok{log}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ q)}
\NormalTok{\}}
\FunctionTok{plot}\NormalTok{(q,}
     \FunctionTok{l}\NormalTok{(q) }\SpecialCharTok{{-}} \FunctionTok{l}\NormalTok{(}\FloatTok{0.3}\NormalTok{),}
     \AttributeTok{ylab =} \StringTok{"l(q) {-} l(qhat)"}\NormalTok{,}
     \AttributeTok{xlab =} \StringTok{"q"}\NormalTok{,}
     \AttributeTok{type =} \StringTok{"l"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{v =} \FloatTok{0.2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{14-hypothesis_files/figure-latex/unnamed-chunk-2-1} \end{center}

\includegraphics[width=6.25in,height=4.16667in]{images/nested_tests.jpg}

Figure from\citep{fox1997applied}

typically, \protect\hyperlink{the-likelihood-ratio-test}{The likelihood ratio test} (and \protect\hyperlink{lagrange-multiplier-score}{Lagrange Multiplier (Score)}) performs better with small to moderate sample sizes, but the \protect\hyperlink{wald-test-GLMM}{Wald test} only requires one maximization (under the full model).

\hypertarget{wald-test}{%
\section{Wald test}\label{wald-test}}

\[
\begin{aligned}
W &= (\hat{\theta}-\theta_0)'[cov(\hat{\theta})]^{-1}(\hat{\theta}-\theta_0) \\
W &\sim \chi_q^2
\end{aligned}
\]

where \(cov(\hat{\theta})\) is given by the inverse Fisher Information matrix evaluated at \(\hat{\theta}\) and q is the rank of \(cov(\hat{\theta})\), which is the number of non-redundant parameters in \(\theta\)

Alternatively,

\[
t_W=\frac{(\hat{\theta}-\theta_0)^2}{I(\theta_0)^{-1}} \sim \chi^2_{(v)}
\]

where v is the degree of freedom.

Equivalently,

\[
s_W= \frac{\hat{\theta}-\theta_0}{\sqrt{I(\hat{\theta})^{-1}}} \sim Z
\]

How far away in the distribution your sample estimate is from the hypothesized population parameter.

For a null value, what is the probability you would have obtained a realization ``more extreme'' or ``worse'' than the estimate you actually obtained?

Significance Level (\(\alpha\)) and Confidence Level (\(1-\alpha\))

\begin{itemize}
\tightlist
\item
  The significance level is the benchmark in which the probability is so low that we would have to reject the null
\item
  The confidence level is the probability that sets the bounds on how far away the realization of the estimator would have to be to reject the null.
\end{itemize}

\textbf{Test Statistics}

\begin{itemize}
\tightlist
\item
  Standardized (transform) the estimator and null value to a test statistic that always has the same distribution
\item
  Test Statistic for the OLS estimator for a single hypothesis
\end{itemize}

\[
T = \frac{\sqrt{n}(\hat{\beta}_j-\beta_{j0})}{\sqrt{n}SE(\hat{\beta_j})} \sim^a N(0,1)
\]

Equivalently,

\[
T = \frac{(\hat{\beta}_j-\beta_{j0})}{SE(\hat{\beta_j})} \sim^a N(0,1)
\]

the test statistic is another random variable that is a function of the data and null hypothesis.

\begin{itemize}
\tightlist
\item
  T denotes the random variable test statistic
\item
  t denotes the single realization of the test statistic
\end{itemize}

Evaluating Test Statistic: determine whether or not we reject or fail to reject the null hypothesis at a given significance / confidence level

Three equivalent ways

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Critical Value
\item
  P-value
\item
  Confidence Interval
\item
  Critical Value
\end{enumerate}

For a given significance level, will determine the critical value \((c)\)

\begin{itemize}
\tightlist
\item
  One-sided: \(H_0: \beta_j \ge \beta_{j0}\)
\end{itemize}

\[
P(T<c|H_0)=\alpha
\]

Reject the null if \(t<c\)

\begin{itemize}
\tightlist
\item
  One-sided: \(H_0: \beta_j \le \beta_{j0}\)
\end{itemize}

\[
P(T>c|H_0)=\alpha
\]

Reject the null if \(t>c\)

\begin{itemize}
\tightlist
\item
  Two-sided: \(H_0: \beta_j \neq \beta_{j0}\)
\end{itemize}

\[
P(|T|>c|H_0)=\alpha
\]

Reject the null if \(|t|>c\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  p-value
\end{enumerate}

Calculate the probability that the test statistic was worse than the realization you have

\begin{itemize}
\tightlist
\item
  One-sided: \(H_0: \beta_j \ge \beta_{j0}\)
\end{itemize}

\[
\text{p-value} = P(T<t|H_0)
\]

\begin{itemize}
\tightlist
\item
  One-sided: \(H_0: \beta_j \le \beta_{j0}\)
\end{itemize}

\[
\text{p-value} = P(T>t|H_0)
\]

\begin{itemize}
\tightlist
\item
  Two-sided: \(H_0: \beta_j \neq \beta_{j0}\)
\end{itemize}

\[
\text{p-value} = P(|T|<t|H_0)
\]

reject the null if p-value \(< \alpha\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Confidence Interval
\end{enumerate}

Using the critical value associated with a null hypothesis and significance level, create an interval

\[
CI(\hat{\beta}_j)_{\alpha} = [\hat{\beta}_j-(c \times SE(\hat{\beta}_j)),\hat{\beta}_j+(c \times SE(\hat{\beta}_j))]
\]

If the null set lies outside the interval then we reject the null.

\begin{itemize}
\tightlist
\item
  We are not testing whether the true population value is close to the estimate, we are testing that given a field true population value of the parameter, how like it is that we observed this estimate.
\item
  Can be interpreted as we believe with \((1-\alpha)\times 100 \%\) probability that the confidence interval captures the true parameter value.
\end{itemize}

With stronger assumption (A1-A6), we could consider \protect\hyperlink{finite-sample-properties}{Finite Sample Properties}

\[
T = \frac{\hat{\beta}_j-\beta_{j0}}{SE(\hat{\beta}_j)} \sim T(n-k)
\]

\begin{itemize}
\tightlist
\item
  This above distributional derivation is strongly dependent on \protect\hyperlink{a4-homoskedasticity}{A4} and \protect\hyperlink{a5-data-generation-random-sampling}{A5}
\item
  T has a student t-distribution because the numerator is normal and the denominator is \(\chi^2\).
\item
  Critical value and p-values will be calculated from the student t-distribution rather than the standard normal distribution.
\item
  \(n \to \infty\), \(T(n-k)\) is asymptotically standard normal.
\end{itemize}

\textbf{Rule of thumb}

\begin{itemize}
\item
  if \(n-k>120\): the critical values and p-values from the t-distribution are (almost) the same as the critical values and p-values from the standard normal distribution.
\item
  if \(n-k<120\)

  \begin{itemize}
  \tightlist
  \item
    if (A1-A6) hold then the t-test is an exact finite distribution test
  \item
    if (A1-A3a, A5) hold, because the t-distribution is asymptotically normal, computing the critical values from a t-distribution is still a valid asymptotic test (i.e., not quite the right critical values and p0values, the difference goes away as \(n \to \infty\))
  \end{itemize}
\end{itemize}

\hypertarget{multiple-hypothesis}{%
\subsection{Multiple Hypothesis}\label{multiple-hypothesis}}

\begin{itemize}
\item
  test multiple parameters as the same time

  \begin{itemize}
  \tightlist
  \item
    \(H_0: \beta_1 = 0\ \& \ \beta_2 = 0\)
  \item
    \(H_0: \beta_1 = 1\ \& \ \beta_2 = 0\)
  \end{itemize}
\item
  perform a series of simply hypothesis does not answer the question (joint distribution vs.~two marginal distributions).
\item
  The test statistic is based on a restriction written in matrix form.
\end{itemize}

\[
y=\beta_0+x_1\beta_1 + x_2\beta_2 + x_3\beta_3 + \epsilon
\]

Null hypothesis is \(H_0: \beta_1 = 0\) \& \(\beta_2=0\) can be rewritten as \(H_0: \mathbf{R}\beta -\mathbf{q}=0\) where

\begin{itemize}
\tightlist
\item
  \(\mathbf{R}\) is a \(m \times k\) matrix where m is the number of restrictions and \(k\) is the number of parameters. \(\mathbf{q}\) is a \(k \times 1\) vector
\item
  \(\mathbf{R}\) ``picks up'' the relevant parameters while \(\mathbf{q}\) is a the null value of the parameter
\end{itemize}

\[
\mathbf{R}= 
\left(
\begin{array}{cccc}
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
\end{array}
\right),
\mathbf{q} = 
\left(
\begin{array}{c}
0 \\
0 \\
\end{array}
\right)
\]

Test Statistic for OLS estimator for a multiple hypothesis

\[
F = \frac{(\mathbf{R\hat{\beta}-q})\hat{\Sigma}^{-1}(\mathbf{R\hat{\beta}-q})}{m} \sim^a F(m,n-k)
\]

\begin{itemize}
\item
  \(\hat{\Sigma}^{-1}\) is the estimator for the asymptotic variance-covariance matrix

  \begin{itemize}
  \tightlist
  \item
    if \protect\hyperlink{a4-homoskedasticity}{A4} holds, both the homoskedastic and heteroskedastic versions produce valid estimator
  \item
    If \protect\hyperlink{a4-homoskedasticity}{A4} does not hold, only the heteroskedastic version produces valid estimators.
  \end{itemize}
\item
  When \(m = 1\), there is only a single restriction, then the \(F\)-statistic is the \(t\)-statistic squared.
\item
  \(F\) distribution is strictly positive, check \protect\hyperlink{f-distribution}{F-Distribution} for more details.
\end{itemize}

\hypertarget{linear-combination}{%
\subsection{Linear Combination}\label{linear-combination}}

Testing multiple parameters as the same time

\[
\begin{aligned}
H_0&: \beta_1 -\beta_2 = 0 \\
H_0&: \beta_1 - \beta_2 > 0 \\
H_0&: \beta_1 - 2\times\beta_2 =0
\end{aligned}
\]

Each is a single restriction on a function of the parameters.

Null hypothesis:

\[
H_0: \beta_1 -\beta_2 = 0
\]

can be rewritten as

\[
H_0: \mathbf{R}\beta -\mathbf{q}=0
\]

where \(\mathbf{R}\)=(0 1 -1 0 0) and \(\mathbf{q}=0\)

\hypertarget{estimate-difference-in-coefficients}{%
\subsection{Estimate Difference in Coefficients}\label{estimate-difference-in-coefficients}}

There is no package to estimate for the difference between two coefficients and its CI, but a simple function created by \href{https://kzee.github.io/CoeffDiff_Demo.html}{Katherine Zee} can be used to calculate this difference. Some modifications might be needed if you don't use standard \texttt{lm} model in R.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{difftest\_lm }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x1, x2, model) \{}
\NormalTok{    diffest }\OtherTok{\textless{}{-}}
        \FunctionTok{summary}\NormalTok{(model)}\SpecialCharTok{$}\NormalTok{coef[x1, }\StringTok{"Estimate"}\NormalTok{] }\SpecialCharTok{{-}} \FunctionTok{summary}\NormalTok{(model)}\SpecialCharTok{$}\NormalTok{coef[x2, }\StringTok{"Estimate"}\NormalTok{]}
    
\NormalTok{    vardiff }\OtherTok{\textless{}{-}}\NormalTok{ (}\FunctionTok{summary}\NormalTok{(model)}\SpecialCharTok{$}\NormalTok{coef[x1, }\StringTok{"Std. Error"}\NormalTok{] }\SpecialCharTok{\^{}} \DecValTok{2} \SpecialCharTok{+}
                    \FunctionTok{summary}\NormalTok{(model)}\SpecialCharTok{$}\NormalTok{coef[x2, }\StringTok{"Std. Error"}\NormalTok{] }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{) }\SpecialCharTok{{-}}\NormalTok{ (}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ (}\FunctionTok{vcov}\NormalTok{(model)[x1, x2]))}
    \CommentTok{\# variance of x1 + variance of x2 {-} 2*covariance of x1 and x2}
\NormalTok{    diffse }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(vardiff)}
\NormalTok{    tdiff }\OtherTok{\textless{}{-}}\NormalTok{ (diffest) }\SpecialCharTok{/}\NormalTok{ (diffse)}
\NormalTok{    ptdiff }\OtherTok{\textless{}{-}} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{pt}\NormalTok{(}\FunctionTok{abs}\NormalTok{(tdiff), model}\SpecialCharTok{$}\NormalTok{df, }\AttributeTok{lower.tail =}\NormalTok{ T))}
\NormalTok{    upr }\OtherTok{\textless{}{-}}
        \CommentTok{\# will usually be very close to 1.96}
\NormalTok{        diffest }\SpecialCharTok{+} \FunctionTok{qt}\NormalTok{(.}\DecValTok{975}\NormalTok{, }\AttributeTok{df =}\NormalTok{ model}\SpecialCharTok{$}\NormalTok{df) }\SpecialCharTok{*}\NormalTok{ diffse }
\NormalTok{    lwr }\OtherTok{\textless{}{-}}\NormalTok{ diffest }\SpecialCharTok{+} \FunctionTok{qt}\NormalTok{(.}\DecValTok{025}\NormalTok{, }\AttributeTok{df =}\NormalTok{ model}\SpecialCharTok{$}\NormalTok{df) }\SpecialCharTok{*}\NormalTok{ diffse}
\NormalTok{    df }\OtherTok{\textless{}{-}}\NormalTok{ model}\SpecialCharTok{$}\NormalTok{df}
    \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}
        \AttributeTok{est =} \FunctionTok{round}\NormalTok{(diffest, }\AttributeTok{digits =} \DecValTok{2}\NormalTok{),}
        \AttributeTok{t =} \FunctionTok{round}\NormalTok{(tdiff, }\AttributeTok{digits =} \DecValTok{2}\NormalTok{),}
        \AttributeTok{p =} \FunctionTok{round}\NormalTok{(ptdiff, }\AttributeTok{digits =} \DecValTok{4}\NormalTok{),}
        \AttributeTok{lwr =} \FunctionTok{round}\NormalTok{(lwr, }\AttributeTok{digits =} \DecValTok{2}\NormalTok{),}
        \AttributeTok{upr =} \FunctionTok{round}\NormalTok{(upr, }\AttributeTok{digits =} \DecValTok{2}\NormalTok{),}
        \AttributeTok{df =}\NormalTok{ df}
\NormalTok{    ))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{application-9}{%
\subsection{Application}\label{application-9}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"car"}\NormalTok{)}

\CommentTok{\# Multiple hypothesis}
\NormalTok{mod.davis }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(weight }\SpecialCharTok{\textasciitilde{}}\NormalTok{ repwt, }\AttributeTok{data=}\NormalTok{Davis)}
\FunctionTok{linearHypothesis}\NormalTok{(mod.davis, }\FunctionTok{c}\NormalTok{(}\StringTok{"(Intercept) = 0"}\NormalTok{, }\StringTok{"repwt = 1"}\NormalTok{),}\AttributeTok{white.adjust =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\#\textgreater{} Linear hypothesis test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Hypothesis:}
\CommentTok{\#\textgreater{} (Intercept) = 0}
\CommentTok{\#\textgreater{} repwt = 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Model 1: restricted model}
\CommentTok{\#\textgreater{} Model 2: weight \textasciitilde{} repwt}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Note: Coefficient covariance matrix supplied.}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   Res.Df Df      F  Pr(\textgreater{}F)  }
\CommentTok{\#\textgreater{} 1    183                    }
\CommentTok{\#\textgreater{} 2    181  2 3.3896 0.03588 *}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}

\CommentTok{\# Linear Combination}
\NormalTok{mod.duncan }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(prestige }\SpecialCharTok{\textasciitilde{}}\NormalTok{ income }\SpecialCharTok{+}\NormalTok{ education, }\AttributeTok{data=}\NormalTok{Duncan)}
\FunctionTok{linearHypothesis}\NormalTok{(mod.duncan, }\StringTok{"1*income {-} 1*education = 0"}\NormalTok{)}
\CommentTok{\#\textgreater{} Linear hypothesis test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Hypothesis:}
\CommentTok{\#\textgreater{} income {-} education = 0}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Model 1: restricted model}
\CommentTok{\#\textgreater{} Model 2: prestige \textasciitilde{} income + education}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   Res.Df    RSS Df Sum of Sq      F Pr(\textgreater{}F)}
\CommentTok{\#\textgreater{} 1     43 7518.9                           }
\CommentTok{\#\textgreater{} 2     42 7506.7  1    12.195 0.0682 0.7952}
\end{Highlighting}
\end{Shaded}

\hypertarget{nonlinear-1}{%
\subsection{Nonlinear}\label{nonlinear-1}}

Suppose that we have q nonlinear functions of the parameters\\
\[
\mathbf{h}(\theta) = \{ h_1 (\theta), ..., h_q (\theta)\}'
\]

The,n, the Jacobian matrix (\(\mathbf{H}(\theta)\)), of rank q is

\[
\mathbf{H}_{q \times p}(\theta) = 
\left(
\begin{array}
{ccc}
\frac{\partial h_1(\theta)}{\partial \theta_1} & ... & \frac{\partial h_1(\theta)}{\partial \theta_p} \\
. & . & . \\
\frac{\partial h_q(\theta)}{\partial \theta_1} & ... & \frac{\partial h_q(\theta)}{\partial \theta_p}
\end{array}
\right)
\]

where the null hypothesis \(H_0: \mathbf{h} (\theta) = 0\) can be tested against the 2-sided alternative with the Wald statistic

\[
W = \frac{\mathbf{h(\hat{\theta})'\{H(\hat{\theta})[F(\hat{\theta})'F(\hat{\theta})]^{-1}H(\hat{\theta})'\}^{-1}h(\hat{\theta})}}{s^2q} \sim F_{q,n-p}
\]

\hypertarget{the-likelihood-ratio-test}{%
\section{The likelihood ratio test}\label{the-likelihood-ratio-test}}

\[
t_{LR} = 2[l(\hat{\theta})-l(\theta_0)] \sim \chi^2_v
\]

where v is the degree of freedom.

Compare the height of the log-likelihood of the sample estimate in relation to the height of log-likelihood of the hypothesized population parameter

Alternatively,

This test considers a ratio of two maximizations,

\[
\begin{aligned}
L_r &= \text{maximized value of the likelihood under $H_0$ (the reduced model)} \\
L_f &= \text{maximized value of the likelihood under $H_0 \cup H_a$ (the full model)}
\end{aligned}
\]

Then, the likelihood ratio is:

\[
\Lambda = \frac{L_r}{L_f}
\]

which can't exceed 1 (since \(L_f\) is always at least as large as \(L-r\) because \(L_r\) is the result of a maximization under a restricted set of the parameter values).

The likelihood ratio statistic is:

\[
\begin{aligned}
-2ln(\Lambda) &= -2ln(L_r/L_f) = -2(l_r - l_f) \\
\lim_{n \to \infty}(-2ln(\Lambda)) &\sim \chi^2_v
\end{aligned}
\]

where \(v\) is the number of parameters in the full model minus the number of parameters in the reduced model.

If \(L_r\) is much smaller than \(L_f\) (the likelihood ratio exceeds \(\chi_{\alpha,v}^2\)), then we reject he reduced model and accept the full model at \(\alpha \times 100 \%\) significance level

\hypertarget{lagrange-multiplier-score}{%
\section{Lagrange Multiplier (Score)}\label{lagrange-multiplier-score}}

\[
t_S= \frac{S(\theta_0)^2}{I(\theta_0)} \sim \chi^2_v
\]

where \(v\) is the degree of freedom.

Compare the slope of the log-likelihood of the sample estimate in relation to the slope of the log-likelihood of the hypothesized population parameter

\hypertarget{two-one-sided-tests-tost-equivalence-testing}{%
\section{Two One-Sided Tests (TOST) Equivalence Testing}\label{two-one-sided-tests-tost-equivalence-testing}}

This is a good way to test whether your population effect size is within a range of practical interest (e.g., if the effect size is 0).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(TOSTER)}
\end{Highlighting}
\end{Shaded}

\hypertarget{marginal-effects}{%
\chapter{Marginal Effects}\label{marginal-effects}}

In cases without polynomials or interactions, it can be easy to interpret the marginal effect.

For example,

\[
Y = \beta_1 X_1 + \beta_2 X_2
\]

where \(\beta\) are the marginal effects.

Numerical derivation is easier than analytical derivation.

\begin{itemize}
\tightlist
\item
  We need to choose values for all the variables to calculate the marginal effect of \(X\)
\end{itemize}

Analytical derivation

\[
f'(x) \equiv \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}
\]

E.g., \(f(x) = X^2\)

\[
\begin{aligned}
f'(x) &= \lim_{h \to 0} \frac{(x+h)^2 - x^2}{h} \\
&= \frac{x^2 + 2xh + h^2 - x^2}{h} \\
&= \frac{2xh + h^2}{h} \\
&= 2x + h \\
&= 2x
\end{aligned}
\]

For numerically approach, we ``just'' need to find a small \(h\) to plug in our function. However, you also need a large enough \(h\) to have numerically accurate computation \citep[chapter 1]{gould2010maximum}

Numerically approach

One-sided derivative

\[
\begin{aligned}
f'(x) &= \lim_{h \to 0} \frac{(x+h)^2 - x^2}{h}  \\
& \approx \frac{f(x+h) -f(x)}{h}
\end{aligned}
\]

Alternatively, two-sided derivative

\[
f'_2(x) \approx \frac{f(x+h) - f(x- h)}{2h}
\]

Marginal effects for

\begin{itemize}
\item
  discrete variables (also known as incremental effects) are the change in \(E[Y|X]\) for a one unit change in \(X\)
\item
  continuous variables are the change in \(E[Y|X]\) for very small changes in \(X\) (not unit changes), because it's a derivative, which is a limit when \(h \to 0\)
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2329}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2329}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5342}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Analytical derivation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Numerical derivation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Marginal Effects & Rules of expectations & Approximate analytical solution \\
Standard Errors & Rules of variances & Delta method using the asymptotic errors (vcov matrix) \\
\end{longtable}

\hypertarget{delta-method}{%
\section{Delta Method}\label{delta-method}}

\begin{itemize}
\tightlist
\item
  approximate the mean and variance of a function of random variables using a first-order Taylor approximation
\item
  A semi-parametric method
\item
  Alternative approaches:

  \begin{itemize}
  \item
    Analytically derive a probability function for the margin
  \item
    Simulation/Bootstrapping
  \end{itemize}
\item
  Resources:

  \begin{itemize}
  \item
    Advanced: \href{https://cran.r-project.org/web/packages/modmarg/vignettes/delta-method.html}{modmarg}
  \item
    Intermediate: \href{https://stats.oarc.ucla.edu/r/faq/how-can-i-estimate-the-standard-error-of-transformed-regression-parameters-in-r-using-the-delta-method/}{UCLA stat}
  \item
    Simple: \href{https://www.alexstephenson.me/post/2022-04-02-standard-errors-and-the-delta-method/}{Another one}
  \end{itemize}
\end{itemize}

Let \(G(\beta)\) be a function of the parameters \(\beta\), then

\[
var(G(\beta)) \approx \nabla G(\beta) cov (\beta) \nabla G(\beta)'
\]

where

\begin{itemize}
\tightlist
\item
  \(\nabla G(\beta)\) = the gradient of the partial derivatives of \(G(\beta)\) (also known as the Jacobian)
\end{itemize}

\hypertarget{average-marginal-effect-algorithm}{%
\section{Average Marginal Effect Algorithm}\label{average-marginal-effect-algorithm}}

For one-sided derivative \(\frac{\partial p(\mathbf{X},\beta)}{\partial X}\) in the probability scale

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Estimate your model
\item
  For each observation \(i\)

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    Calculate \(\hat{Y}_{i0}\) which is the prediction in the probability scale using observed values
  \item
    Increase \(X\) (variable of interest) by a ``small'' amount \(h\) (\(X_{new} = X + h\))

    \begin{itemize}
    \item
      When \(X\) is continuous, \(h = (|\bar{X}| + 0.001) \times 0.001\) where \(\bar{X}\) is the mean value of \(X\)
    \item
      When \(X\) is discrete, \(h = 1\)
    \end{itemize}
  \item
    Calculate \(\hat{Y}_{i1}\) which is the prediction in the probability scale using new \(X\) and other variables' observed vales.
  \item
    Calculate the difference between the two predictions as fraction of \(h\): \(\frac{\bar{Y}_{i1} - \bar{Y}_{i0}}{h}\)
  \end{enumerate}
\item
  Average numerical derivative is \(E[\frac{\bar{Y}_{i1} - \bar{Y}_{i0}}{h}] \approx \frac{\partial p (Y|\mathbf{X}, \beta)}{\partial X}\)
\end{enumerate}

Two-sided derivatives

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Estimate your model
\item
  For each observation \(i\)

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    Calculate \(\hat{Y}_{i0}\) which is the prediction in the probability scale using observed values
  \item
    Increase \(X\) (variable of interest) by a ``small'' amount \(h\) (\(X_{1} = X + h\)) and decrease \(X\) (variable of interest) by a ``small'' amount \(h\) (\(X_{2} = X - h\))

    \begin{itemize}
    \item
      When \(X\) is continuous, \(h = (|\bar{X}| + 0.001) \times 0.001\) where \(\bar{X}\) is the mean value of \(X\)
    \item
      When \(X\) is discrete, \(h = 1\)
    \end{itemize}
  \item
    Calculate \(\hat{Y}_{i1}\) which is the prediction in the probability scale using new \(X_1\) and other variables' observed vales.
  \item
    Calculate \(\hat{Y}_{i2}\) which is the prediction in the probability scale using new \(X_2\) and other variables' observed vales.
  \item
    Calculate the difference between the two predictions as fraction of \(h\): \(\frac{\bar{Y}_{i1} - \bar{Y}_{i2}}{2h}\)
  \end{enumerate}
\item
  Average numerical derivative is \(E[\frac{\bar{Y}_{i1} - \bar{Y}_{i2}}{2h}] \approx \frac{\partial p (Y|\mathbf{X}, \beta)}{\partial X}\)
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(margins)}
\FunctionTok{library}\NormalTok{(tidyverse)}

\FunctionTok{data}\NormalTok{(}\StringTok{"mtcars"}\NormalTok{)}
\NormalTok{mod }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ cyl }\SpecialCharTok{*}\NormalTok{ disp }\SpecialCharTok{*}\NormalTok{ hp, }\AttributeTok{data =}\NormalTok{ mtcars)}
\NormalTok{margins}\SpecialCharTok{::}\FunctionTok{margins}\NormalTok{(mod) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{summary}\NormalTok{()}
\CommentTok{\#\textgreater{}  factor     AME     SE       z      p    lower   upper}
\CommentTok{\#\textgreater{}     cyl {-}4.0592 3.7614 {-}1.0792 0.2805 {-}11.4313  3.3130}
\CommentTok{\#\textgreater{}    disp {-}0.0350 0.0132 {-}2.6473 0.0081  {-}0.0610 {-}0.0091}
\CommentTok{\#\textgreater{}      hp {-}0.0284 0.0185 {-}1.5348 0.1248  {-}0.0647  0.0079}

\CommentTok{\# function for variable}
\NormalTok{get\_mae }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(mod, }\AttributeTok{var =} \StringTok{"disp"}\NormalTok{) \{}
\NormalTok{    data }\OtherTok{=}\NormalTok{ mod}\SpecialCharTok{$}\NormalTok{model}
    
\NormalTok{    pred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(mod)}
    
    \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{class}\NormalTok{(mod}\SpecialCharTok{$}\NormalTok{model[[\{}
\NormalTok{        \{}
\NormalTok{            var}
\NormalTok{        \}}
\NormalTok{    \}]]) }\SpecialCharTok{==} \StringTok{"numeric"}\NormalTok{) \{}
\NormalTok{        h }\OtherTok{=}\NormalTok{ (}\FunctionTok{abs}\NormalTok{(}\FunctionTok{mean}\NormalTok{(mod}\SpecialCharTok{$}\NormalTok{model[[var]])) }\SpecialCharTok{+} \FloatTok{0.01}\NormalTok{) }\SpecialCharTok{*} \FloatTok{0.01}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{        h }\OtherTok{=} \DecValTok{1}
\NormalTok{    \}}
    
\NormalTok{    data[[\{}
\NormalTok{        \{}
\NormalTok{            var}
\NormalTok{        \}}
\NormalTok{    \}]] }\OtherTok{\textless{}{-}}\NormalTok{ data[[\{}
\NormalTok{\{}
\NormalTok{var}
\NormalTok{\}}
\NormalTok{\}]] }\SpecialCharTok{{-}}\NormalTok{ h}

\NormalTok{    pred\_new }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(mod, }\AttributeTok{newdata =}\NormalTok{ data)}

    \FunctionTok{mean}\NormalTok{(pred }\SpecialCharTok{{-}}\NormalTok{ pred\_new) }\SpecialCharTok{/}\NormalTok{ h}
\NormalTok{\}}

\FunctionTok{get\_mae}\NormalTok{(mod, }\AttributeTok{var =} \StringTok{"disp"}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] {-}0.03504546}
\end{Highlighting}
\end{Shaded}

\hypertarget{packages}{%
\section{Packages}\label{packages}}

\hypertarget{marginaleffects}{%
\subsection{MarginalEffects}\label{marginaleffects}}

\texttt{MarginalEffects} package is a successor of \texttt{margins} and \texttt{emtrends} (faster, more efficient, more adaptable). Hence, this is advocated to be used.

\begin{itemize}
\tightlist
\item
  A limitation is that there is no readily function to correct for multiple comparisons. Hence, one can use the \texttt{p.adjust} function to overcome this disadvantage.
\end{itemize}

Definitions from the package:

\begin{itemize}
\item
  \textbf{Marginal effects} are slopes or derivatives (i.e., effect of changes in a variable on the outcome)

  \begin{itemize}
  \tightlist
  \item
    \texttt{margins} package defines marginal effects as ``partial derivatives of the regression equation with respect to each variable in the model for each unit in the data.''
  \end{itemize}
\item
  \textbf{Marginal means} are averages or integrals (i.e., marginalizing across rows of a prediction grid)
\end{itemize}

To customize your plot using \texttt{plot\_cme} (which is a \texttt{ggplot} class), you can check this \href{https://stackoverflow.com/questions/72463092/estimate-marginal-effect-in-triple-interaction}{post} by the author of the \texttt{MarginalEffects} package

Causal inference with the parametric g-formula

\begin{itemize}
\tightlist
\item
  Because the plug-in g estimator is equivalent to the average contrast in the \texttt{marginaleffects} package.
\end{itemize}

To get predicted values

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(marginaleffects)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{data}\NormalTok{(mtcars)}

\NormalTok{mod }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ hp }\SpecialCharTok{*}\NormalTok{ wt }\SpecialCharTok{*}\NormalTok{ am, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{predictions}\NormalTok{(mod) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Estimate Std. Error    z Pr(\textgreater{}|z|)     S 2.5 \% 97.5 \%}
\CommentTok{\#\textgreater{}      22.5      0.884 25.4   \textless{}0.001 471.7  20.8   24.2}
\CommentTok{\#\textgreater{}      20.8      1.194 17.4   \textless{}0.001 223.3  18.5   23.1}
\CommentTok{\#\textgreater{}      25.3      0.709 35.7   \textless{}0.001 922.7  23.9   26.7}
\CommentTok{\#\textgreater{}      20.3      0.704 28.8   \textless{}0.001 601.5  18.9   21.6}
\CommentTok{\#\textgreater{}      17.0      0.712 23.9   \textless{}0.001 416.2  15.6   18.4}
\CommentTok{\#\textgreater{}      19.7      0.875 22.5   \textless{}0.001 368.8  17.9   21.4}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, wt, am}
\CommentTok{\# for specific reference values}
\FunctionTok{predictions}\NormalTok{(mod, }\AttributeTok{newdata =} \FunctionTok{datagrid}\NormalTok{(}\AttributeTok{am =} \DecValTok{0}\NormalTok{, }\AttributeTok{wt =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{)))}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  am wt Estimate Std. Error    z Pr(\textgreater{}|z|)     S 2.5 \% 97.5 \%  hp}
\CommentTok{\#\textgreater{}   0  2     22.0       2.04 10.8   \textless{}0.001  87.4  18.0   26.0 147}
\CommentTok{\#\textgreater{}   0  4     16.6       1.08 15.3   \textless{}0.001 173.8  14.5   18.7 147}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, am, wt}
\FunctionTok{plot\_cap}\NormalTok{(mod, }\AttributeTok{condition =} \FunctionTok{c}\NormalTok{(}\StringTok{"hp"}\NormalTok{,}\StringTok{"wt"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{15-marginal-effect_files/figure-latex/unnamed-chunk-2-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Average Margianl Effects}
\NormalTok{mfx }\OtherTok{\textless{}{-}} \FunctionTok{marginaleffects}\NormalTok{(mod, }\AttributeTok{variables =} \FunctionTok{c}\NormalTok{(}\StringTok{"hp"}\NormalTok{, }\StringTok{"wt"}\NormalTok{))}
\FunctionTok{summary}\NormalTok{(mfx)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Term    Contrast Estimate Std. Error     z Pr(\textgreater{}|z|)   2.5 \% 97.5 \%}
\CommentTok{\#\textgreater{}    hp mean(dY/dX)  {-}0.0381     0.0128 {-}2.98  0.00291 {-}0.0631 {-}0.013}
\CommentTok{\#\textgreater{}    wt mean(dY/dX)  {-}3.9391     1.0858 {-}3.63  \textless{} 0.001 {-}6.0672 {-}1.811}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Columns: term, contrast, estimate, std.error, statistic, p.value, conf.low, conf.high}

\CommentTok{\# Group{-}Average Marginal Effects}
\NormalTok{marginaleffects}\SpecialCharTok{::}\FunctionTok{marginaleffects}\NormalTok{(mod, }\AttributeTok{by =} \StringTok{"hp"}\NormalTok{, }\AttributeTok{variables =} \StringTok{"am"}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Term          Contrast  hp Estimate Std. Error      z Pr(\textgreater{}|z|)   S  2.5 \%}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0)  52    3.976       5.20  0.764    0.445 1.2  {-}6.22}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0)  62   {-}2.774       2.51 {-}1.107    0.268 1.9  {-}7.68}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0)  65    2.999       4.13  0.725    0.468 1.1  {-}5.10}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0)  66    2.025       3.48  0.582    0.561 0.8  {-}4.80}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0)  91    1.858       2.76  0.674    0.500 1.0  {-}3.54}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0)  93    1.201       2.35  0.511    0.609 0.7  {-}3.40}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0)  95   {-}1.832       1.97 {-}0.931    0.352 1.5  {-}5.69}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0)  97    0.708       2.04  0.347    0.728 0.5  {-}3.28}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0) 105   {-}2.682       2.37 {-}1.132    0.258 2.0  {-}7.32}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0) 109   {-}0.237       1.59 {-}0.149    0.881 0.2  {-}3.35}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0) 110   {-}0.640       1.57 {-}0.407    0.684 0.5  {-}3.73}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0) 113    4.081       3.94  1.037    0.300 1.7  {-}3.63}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0) 123   {-}2.098       2.10 {-}0.998    0.318 1.7  {-}6.22}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0) 150   {-}1.429       1.90 {-}0.753    0.452 1.1  {-}5.15}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0) 175   {-}0.416       1.56 {-}0.266    0.790 0.3  {-}3.48}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0) 180   {-}1.381       2.47 {-}0.560    0.576 0.8  {-}6.22}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0) 205   {-}2.873       6.24 {-}0.460    0.645 0.6 {-}15.11}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0) 215   {-}2.534       6.95 {-}0.364    0.716 0.5 {-}16.16}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0) 230   {-}1.477       7.07 {-}0.209    0.835 0.3 {-}15.34}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0) 245    1.115       2.28  0.488    0.625 0.7  {-}3.36}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0) 264    2.106       2.29  0.920    0.358 1.5  {-}2.38}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0) 335    4.027       3.24  1.243    0.214 2.2  {-}2.32}
\CommentTok{\#\textgreater{}  97.5 \%}
\CommentTok{\#\textgreater{}   14.18}
\CommentTok{\#\textgreater{}    2.14}
\CommentTok{\#\textgreater{}   11.10}
\CommentTok{\#\textgreater{}    8.85}
\CommentTok{\#\textgreater{}    7.26}
\CommentTok{\#\textgreater{}    5.80}
\CommentTok{\#\textgreater{}    2.02}
\CommentTok{\#\textgreater{}    4.70}
\CommentTok{\#\textgreater{}    1.96}
\CommentTok{\#\textgreater{}    2.87}
\CommentTok{\#\textgreater{}    2.45}
\CommentTok{\#\textgreater{}   11.79}
\CommentTok{\#\textgreater{}    2.02}
\CommentTok{\#\textgreater{}    2.29}
\CommentTok{\#\textgreater{}    2.64}
\CommentTok{\#\textgreater{}    3.46}
\CommentTok{\#\textgreater{}    9.36}
\CommentTok{\#\textgreater{}   11.09}
\CommentTok{\#\textgreater{}   12.39}
\CommentTok{\#\textgreater{}    5.59}
\CommentTok{\#\textgreater{}    6.59}
\CommentTok{\#\textgreater{}   10.38}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Columns: term, contrast, hp, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted\_lo, predicted\_hi, predicted}

\CommentTok{\# Marginal effects at representative values}
\NormalTok{marginaleffects}\SpecialCharTok{::}\FunctionTok{marginaleffects}\NormalTok{(mod, }
                                 \AttributeTok{newdata =} \FunctionTok{datagrid}\NormalTok{(}\AttributeTok{am =} \DecValTok{0}\NormalTok{, }
                                                    \AttributeTok{wt =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{)))}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Term Contrast am wt Estimate Std. Error      z Pr(\textgreater{}|z|)   S   2.5 \%   97.5 \%}
\CommentTok{\#\textgreater{}    am    1 {-} 0  0  2   2.5465     2.7860  0.914   0.3607 1.5 {-}2.9139  8.00694}
\CommentTok{\#\textgreater{}    am    1 {-} 0  0  4  {-}2.9661     3.0381 {-}0.976   0.3289 1.6 {-}8.9207  2.98852}
\CommentTok{\#\textgreater{}    hp    dY/dX  0  2  {-}0.0598     0.0283 {-}2.115   0.0344 4.9 {-}0.1153 {-}0.00439}
\CommentTok{\#\textgreater{}    hp    dY/dX  0  4  {-}0.0309     0.0187 {-}1.654   0.0981 3.3 {-}0.0676  0.00572}
\CommentTok{\#\textgreater{}    wt    dY/dX  0  2  {-}2.6762     1.4194 {-}1.885   0.0594 4.1 {-}5.4582  0.10587}
\CommentTok{\#\textgreater{}    wt    dY/dX  0  4  {-}2.6762     1.4199 {-}1.885   0.0595 4.1 {-}5.4591  0.10676}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, am, wt, predicted\_lo, predicted\_hi, predicted, mpg, hp}

\CommentTok{\# Marginal Effects at the Mean}
\NormalTok{marginaleffects}\SpecialCharTok{::}\FunctionTok{marginaleffects}\NormalTok{(mod, }\AttributeTok{newdata =} \StringTok{"mean"}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Term Contrast Estimate Std. Error      z Pr(\textgreater{}|z|)    S  2.5 \%  97.5 \%}
\CommentTok{\#\textgreater{}    am    1 {-} 0  {-}0.8086    1.52383 {-}0.531  0.59568  0.7 {-}3.795  2.1781}
\CommentTok{\#\textgreater{}    hp    dY/dX  {-}0.0323    0.00956 {-}3.375  \textless{} 0.001 10.4 {-}0.051 {-}0.0135}
\CommentTok{\#\textgreater{}    wt    dY/dX  {-}3.7959    1.21310 {-}3.129  0.00175  9.2 {-}6.174 {-}1.4183}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted\_lo, predicted\_hi, predicted, mpg, hp, wt, am}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# counterfactual}
\FunctionTok{comparisons}\NormalTok{(mod, }\AttributeTok{variables =} \FunctionTok{list}\NormalTok{(}\AttributeTok{am =} \DecValTok{0}\SpecialCharTok{:}\DecValTok{1}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{summary}\NormalTok{()}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Term          Contrast Estimate Std. Error      z Pr(\textgreater{}|z|) 2.5 \% 97.5 \%}
\CommentTok{\#\textgreater{}    am mean(1) {-} mean(0)  {-}0.0481       1.85 {-}0.026    0.979 {-}3.68   3.58}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Columns: term, contrast, estimate, std.error, statistic, p.value, conf.low, conf.high}
\end{Highlighting}
\end{Shaded}

\hypertarget{margins}{%
\subsection{margins}\label{margins}}

\begin{itemize}
\tightlist
\item
  Marginal effects are partial derivative of the regression equation with respect to each variable in the model for each unit in the data
\end{itemize}

\begin{itemize}
\item
  Average Partial Effects: the contribution of each variable the outcome scale, conditional on the other variables involved in the link function transformation of the linear predictor
\item
  Average Marginal Effects: the marginal contribution of each variable on the scale of the linear predictor.
\item
  Average marginal effects are the mean of these unit-specific partial derivatives over some sample
\end{itemize}

\texttt{margins} package gives the marginal effects of models (a replication of the \texttt{margins} command in Stata).

\texttt{prediction} package gives the unit-specific and sample average predictions of models (similar to the predictive margins in Stata).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(margins)}

\CommentTok{\# examples by the package\textquotesingle{}s authors}
\NormalTok{mod }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ cyl }\SpecialCharTok{*}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ wt, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{summary}\NormalTok{(mod)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = mpg \textasciitilde{} cyl * hp + wt, data = mtcars)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}     Min      1Q  Median      3Q     Max }
\CommentTok{\#\textgreater{} {-}3.3440 {-}1.4144 {-}0.6166  1.2160  4.2815 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}              Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept) 52.017520   4.916935  10.579 4.18e{-}11 ***}
\CommentTok{\#\textgreater{} cyl         {-}2.742125   0.800228  {-}3.427  0.00197 ** }
\CommentTok{\#\textgreater{} hp          {-}0.163594   0.052122  {-}3.139  0.00408 ** }
\CommentTok{\#\textgreater{} wt          {-}3.119815   0.661322  {-}4.718 6.51e{-}05 ***}
\CommentTok{\#\textgreater{} cyl:hp       0.018954   0.006645   2.852  0.00823 ** }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 2.242 on 27 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.8795, Adjusted R{-}squared:  0.8616 }
\CommentTok{\#\textgreater{} F{-}statistic: 49.25 on 4 and 27 DF,  p{-}value: 5.065e{-}12}
\end{Highlighting}
\end{Shaded}

In cases where you have interaction or polynomial terms, the coefficient estimates cannot be interpreted as the marginal effects of X on Y. Hence, if you want to know the average marginal effects of each variable then

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(}\FunctionTok{margins}\NormalTok{(mod))}
\CommentTok{\#\textgreater{}  factor     AME     SE       z      p   lower   upper}
\CommentTok{\#\textgreater{}     cyl  0.0381 0.5999  0.0636 0.9493 {-}1.1376  1.2139}
\CommentTok{\#\textgreater{}      hp {-}0.0463 0.0145 {-}3.1909 0.0014 {-}0.0748 {-}0.0179}
\CommentTok{\#\textgreater{}      wt {-}3.1198 0.6613 {-}4.7176 0.0000 {-}4.4160 {-}1.8236}

\CommentTok{\# equivalently }
\FunctionTok{margins\_summary}\NormalTok{(mod)}
\CommentTok{\#\textgreater{}  factor     AME     SE       z      p   lower   upper}
\CommentTok{\#\textgreater{}     cyl  0.0381 0.5999  0.0636 0.9493 {-}1.1376  1.2139}
\CommentTok{\#\textgreater{}      hp {-}0.0463 0.0145 {-}3.1909 0.0014 {-}0.0748 {-}0.0179}
\CommentTok{\#\textgreater{}      wt {-}3.1198 0.6613 {-}4.7176 0.0000 {-}4.4160 {-}1.8236}

\FunctionTok{plot}\NormalTok{(}\FunctionTok{margins}\NormalTok{(mod))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{15-marginal-effect_files/figure-latex/unnamed-chunk-6-1} \end{center}

Marginal effects at the mean (\textbf{MEM}):

\begin{itemize}
\tightlist
\item
  Marginal effects at the mean values of the sample
\item
  For discrete variables, it's called average discrete change (\textbf{ADC})
\end{itemize}

Average Marginal Effect (\textbf{AME})

\begin{itemize}
\tightlist
\item
  An average of the marginal effects at each value of the sample
\end{itemize}

Marginal Effects at representative values (\textbf{MER})

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{margins}\NormalTok{(mod, }\AttributeTok{at =} \FunctionTok{list}\NormalTok{(}\AttributeTok{hp =} \DecValTok{150}\NormalTok{))}
\CommentTok{\#\textgreater{}  at(hp)    cyl       hp    wt}
\CommentTok{\#\textgreater{}     150 0.1009 {-}0.04632 {-}3.12}

\FunctionTok{margins}\NormalTok{(mod, }\AttributeTok{at =} \FunctionTok{list}\NormalTok{(}\AttributeTok{hp =} \DecValTok{150}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{summary}\NormalTok{()}
\CommentTok{\#\textgreater{}  factor       hp     AME     SE       z      p   lower   upper}
\CommentTok{\#\textgreater{}     cyl 150.0000  0.1009 0.6128  0.1647 0.8692 {-}1.1001  1.3019}
\CommentTok{\#\textgreater{}      hp 150.0000 {-}0.0463 0.0145 {-}3.1909 0.0014 {-}0.0748 {-}0.0179}
\CommentTok{\#\textgreater{}      wt 150.0000 {-}3.1198 0.6613 {-}4.7175 0.0000 {-}4.4160 {-}1.8236}
\end{Highlighting}
\end{Shaded}

\hypertarget{mfx}{%
\subsection{mfx}\label{mfx}}

Works well with \protect\hyperlink{generalized-linear-models}{Generalized Linear Models}/\texttt{glm} package

For technical details, see the package \href{https://cran.rstudio.com/web/packages/mfx/vignettes/mfxarticle.pdf}{vignette}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Model & Dependent Variable & Syntax \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Probit & Binary & \texttt{probitmfx} \\
Logit & Binary & \texttt{logitmfx} \\
Poisson & Count & \texttt{poissonmfx} \\
Negative Binomial & Count & \texttt{negbinmfx} \\
Beta & Rate & \texttt{betamfx} \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(mfx)}
\FunctionTok{data}\NormalTok{(}\StringTok{"mtcars"}\NormalTok{)}
\FunctionTok{poissonmfx}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ vs }\SpecialCharTok{\textasciitilde{}}\NormalTok{ mpg }\SpecialCharTok{*}\NormalTok{ cyl }\SpecialCharTok{*}\NormalTok{ disp, }\AttributeTok{data =}\NormalTok{ mtcars)}
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} poissonmfx(formula = vs \textasciitilde{} mpg * cyl * disp, data = mtcars)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Marginal Effects:}
\CommentTok{\#\textgreater{}                    dF/dx   Std. Err.       z  P\textgreater{}|z|}
\CommentTok{\#\textgreater{} mpg           1.4722e{-}03  8.7531e{-}03  0.1682 0.8664}
\CommentTok{\#\textgreater{} cyl           6.6420e{-}03  3.9263e{-}02  0.1692 0.8657}
\CommentTok{\#\textgreater{} disp          1.5899e{-}04  9.4555e{-}04  0.1681 0.8665}
\CommentTok{\#\textgreater{} mpg:cyl      {-}3.4698e{-}04  2.0564e{-}03 {-}0.1687 0.8660}
\CommentTok{\#\textgreater{} mpg:disp     {-}7.6794e{-}06  4.5545e{-}05 {-}0.1686 0.8661}
\CommentTok{\#\textgreater{} cyl:disp     {-}3.3837e{-}05  1.9919e{-}04 {-}0.1699 0.8651}
\CommentTok{\#\textgreater{} mpg:cyl:disp  1.6812e{-}06  9.8919e{-}06  0.1700 0.8650}
\end{Highlighting}
\end{Shaded}

This package can only give the marginal effect for each variable in the \texttt{glm} model, but not the average marginal effect that we might look for.

\hypertarget{prediction-and-estimation}{%
\chapter{Prediction and Estimation}\label{prediction-and-estimation}}

Prediction and Estimation (Inference) have been the two fundamental pillars in statistics.

\begin{itemize}
\item
  You cannot have both. You can either have high prediction or high estimation.

  \begin{itemize}
  \item
    In prediction, you minimize the loss function.
  \item
    In estimation, you try to best fit the data. Because the goal of estimation is to best fit the data, you always run the risk of not predicting well.
  \end{itemize}
\end{itemize}

In high dimension, you always have weak to strong collinearity. Hence, your estimation can be undesirable. And you can't pick which one variable to stay in the model, but all these troubles would not affect your prediction. In Plateau problem

\begin{itemize}
\tightlist
\item
  If two functions are similar in output space, you can still do prediction, but you can't do estimation because of exploded standard errors.
\end{itemize}

\includegraphics[width=6.25in,height=3.64583in]{images/prediction_causation.PNG}

(SICSS 2018 - Sendhil Mullainathan's presentation slide)

Selective Labels Problem (\href{https://cs.stanford.edu/~jure/pubs/contraction-kdd17.pdf}{The Selective Labels Problem: Evaluating Algorithmic Predictions in the Presence of Unobservables})

Recall Linear Regression \ref{linear-regression} OLS estimates

\[
\begin{aligned}
\hat{\beta}_{OLS} &= (\mathbf{X}'\mathbf{X})^{-1}(\mathbf{X}'\mathbf{Y}) \\
&= (\mathbf{X}'\mathbf{X})^{-1}(\mathbf{X}'(\mathbf{X \beta}+ \epsilon)) \\
&= (\mathbf{X}'\mathbf{X})^{-1} (\mathbf{X}'\mathbf{X}) \beta + (\mathbf{X}'\mathbf{X})^{-1}(\mathbf{X}'\epsilon) \\
\hat{\beta}_{OLS} & \to \beta + (\mathbf{X}'\mathbf{X})^{-1}(\mathbf{X}'\epsilon)
\end{aligned}
\]

Hence, OLS estimates will be unbiased (i.e., get rid of \((\mathbf{X}'\mathbf{X})^{-1}(\mathbf{X}'\epsilon)\)) if we have the following 2 conditions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(E(\epsilon|\mathbf{X}) = 0\) With an intercept, we can usually solve this problem
\item
  \(Cov(\mathbf{X}, \epsilon) = 0\)
\end{enumerate}

Problem with estimation usually stems from the second condition.

Tools to combat this problem can be found in causal inference \ref{causal-inference}

\hypertarget{moderation}{%
\chapter{Moderation}\label{moderation}}

\begin{itemize}
\tightlist
\item
  Spotlight Analysis: Compare the mean of the dependent of the two groups (treatment and control) at every value (\protect\hyperlink{simple-slopes-analysis}{Simple Slopes Analysis})
\item
  Floodlight Analysis: is spotlight analysis on the whole range of the moderator (\protect\hyperlink{johnson-neyman-intervals}{Johnson-Neyman intervals})
\end{itemize}

Other Resources:

\begin{itemize}
\item
  \texttt{BANOVAL} : floodlight analysis on Bayesian ANOVA models
\item
  \texttt{cSEM} : \texttt{doFloodlightAnalysis} in SEM model
\item
  \citep{spiller2013}
\end{itemize}

Terminology:

\begin{itemize}
\item
  Main effects (slopes): coefficients that do no involve interaction terms
\item
  Simple slope: when a continuous independent variable interact with a moderating variable, its slope at a particular level of the moderating variable
\item
  Simple effect: when a categorical independent variable interacts with a moderating variable, its effect at a particular level of the moderating variable.
\end{itemize}

Example:

\[
Y = \beta_0 + \beta_1 X + \beta_2 M + \beta_3 X \times M
\]

where

\begin{itemize}
\item
  \(\beta_0\) = intercept
\item
  \(\beta_1\) = simple effect (slope) of \(X\) (independent variable)
\item
  \(\beta_2\) = simple effect (slope) of \(M\) (moderating variable)
\item
  \(\beta_3\) = interaction of \(X\) and \(M\)
\end{itemize}

Three types of interactions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \protect\hyperlink{continuous-by-continuous}{Continuous by continuous}
\item
  \protect\hyperlink{continuous-by-categorical}{Continuous by categorical}
\item
  \protect\hyperlink{categorical-by-categorical}{Categorical by categorical}
\end{enumerate}

\hypertarget{emmeans-package}{%
\section{emmeans package}\label{emmeans-package}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"emmeans"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(emmeans)}
\end{Highlighting}
\end{Shaded}

Data set is from \href{https://stats.oarc.ucla.edu/r/seminars/interactions-r/}{UCLA seminar} where \texttt{gender} and \texttt{prog} are categorical

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{readRDS}\NormalTok{(}\StringTok{"data/exercise.rds"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{prog =} \FunctionTok{factor}\NormalTok{(prog, }\AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"jog"}\NormalTok{, }\StringTok{"swim"}\NormalTok{, }\StringTok{"read"}\NormalTok{))) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{gender =} \FunctionTok{factor}\NormalTok{(gender, }\AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"male"}\NormalTok{, }\StringTok{"female"}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\hypertarget{continuous-by-continuous}{%
\subsection{Continuous by continuous}\label{continuous-by-continuous}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{contcont }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(loss}\SpecialCharTok{\textasciitilde{}}\NormalTok{hours}\SpecialCharTok{*}\NormalTok{effort,}\AttributeTok{data=}\NormalTok{dat)}
\FunctionTok{summary}\NormalTok{(contcont)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = loss \textasciitilde{} hours * effort, data = dat)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}    Min     1Q Median     3Q    Max }
\CommentTok{\#\textgreater{} {-}29.52 {-}10.60  {-}1.78  11.13  34.51 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}              Estimate Std. Error t value Pr(\textgreater{}|t|)  }
\CommentTok{\#\textgreater{} (Intercept)   7.79864   11.60362   0.672   0.5017  }
\CommentTok{\#\textgreater{} hours        {-}9.37568    5.66392  {-}1.655   0.0982 .}
\CommentTok{\#\textgreater{} effort       {-}0.08028    0.38465  {-}0.209   0.8347  }
\CommentTok{\#\textgreater{} hours:effort  0.39335    0.18750   2.098   0.0362 *}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 13.56 on 896 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.07818,    Adjusted R{-}squared:  0.07509 }
\CommentTok{\#\textgreater{} F{-}statistic: 25.33 on 3 and 896 DF,  p{-}value: 9.826e{-}16}
\end{Highlighting}
\end{Shaded}

Simple slopes for a continuous by continuous model

Spotlight analysis \citep{aiken2005interaction}: usually pick 3 values of moderating variable:

\begin{itemize}
\item
  Mean Moderating Variable + \(\sigma \times\) (Moderating variable)
\item
  Mean Moderating Variable
\item
  Mean Moderating Variable - \(\sigma \times\) (Moderating variable)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{effar }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(}\FunctionTok{mean}\NormalTok{(dat}\SpecialCharTok{$}\NormalTok{effort) }\SpecialCharTok{+} \FunctionTok{sd}\NormalTok{(dat}\SpecialCharTok{$}\NormalTok{effort), }\DecValTok{1}\NormalTok{)}
\NormalTok{effr  }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(}\FunctionTok{mean}\NormalTok{(dat}\SpecialCharTok{$}\NormalTok{effort), }\DecValTok{1}\NormalTok{)}
\NormalTok{effbr }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(}\FunctionTok{mean}\NormalTok{(dat}\SpecialCharTok{$}\NormalTok{effort) }\SpecialCharTok{{-}} \FunctionTok{sd}\NormalTok{(dat}\SpecialCharTok{$}\NormalTok{effort), }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# specify list of points}
\NormalTok{mylist }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{effort =} \FunctionTok{c}\NormalTok{(effbr, effr, effar))}

\CommentTok{\# get the estimates}
\FunctionTok{emtrends}\NormalTok{(contcont, }\SpecialCharTok{\textasciitilde{}}\NormalTok{ effort, }\AttributeTok{var =} \StringTok{"hours"}\NormalTok{, }\AttributeTok{at =}\NormalTok{ mylist)}
\CommentTok{\#\textgreater{}  effort hours.trend    SE  df lower.CL upper.CL}
\CommentTok{\#\textgreater{}    24.5       0.261 1.352 896   {-}2.392     2.91}
\CommentTok{\#\textgreater{}    29.7       2.307 0.915 896    0.511     4.10}
\CommentTok{\#\textgreater{}    34.8       4.313 1.308 896    1.745     6.88}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Confidence level used: 0.95}

\CommentTok{\# plot}
\NormalTok{mylist }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{hours =} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{4}\NormalTok{, }\AttributeTok{by =} \FloatTok{0.4}\NormalTok{),}
               \AttributeTok{effort =} \FunctionTok{c}\NormalTok{(effbr, effr, effar))}
\FunctionTok{emmip}\NormalTok{(contcont, effort }\SpecialCharTok{\textasciitilde{}}\NormalTok{ hours, }\AttributeTok{at =}\NormalTok{ mylist, }\AttributeTok{CIs =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{17-moderation_files/figure-latex/unnamed-chunk-6-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# statistical test for slope difference}
\FunctionTok{emtrends}\NormalTok{(}
\NormalTok{    contcont,}
\NormalTok{    pairwise }\SpecialCharTok{\textasciitilde{}}\NormalTok{ effort,}
    \AttributeTok{var =} \StringTok{"hours"}\NormalTok{,}
    \AttributeTok{at =}\NormalTok{ mylist,}
    \AttributeTok{adjust =} \StringTok{"none"}
\NormalTok{)}
\CommentTok{\#\textgreater{} $emtrends}
\CommentTok{\#\textgreater{}  effort hours.trend    SE  df lower.CL upper.CL}
\CommentTok{\#\textgreater{}    24.5       0.261 1.352 896   {-}2.392     2.91}
\CommentTok{\#\textgreater{}    29.7       2.307 0.915 896    0.511     4.10}
\CommentTok{\#\textgreater{}    34.8       4.313 1.308 896    1.745     6.88}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Results are averaged over the levels of: hours }
\CommentTok{\#\textgreater{} Confidence level used: 0.95 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $contrasts}
\CommentTok{\#\textgreater{}  contrast                estimate    SE  df t.ratio p.value}
\CommentTok{\#\textgreater{}  effort24.5 {-} effort29.7    {-}2.05 0.975 896  {-}2.098  0.0362}
\CommentTok{\#\textgreater{}  effort24.5 {-} effort34.8    {-}4.05 1.931 896  {-}2.098  0.0362}
\CommentTok{\#\textgreater{}  effort29.7 {-} effort34.8    {-}2.01 0.956 896  {-}2.098  0.0362}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Results are averaged over the levels of: hours}
\end{Highlighting}
\end{Shaded}

The 3 p-values are the same as the interaction term.

For publication, we use

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}

\CommentTok{\# data}
\NormalTok{mylist }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{hours =} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{4}\NormalTok{, }\AttributeTok{by =} \FloatTok{0.4}\NormalTok{),}
               \AttributeTok{effort =} \FunctionTok{c}\NormalTok{(effbr, effr, effar))}
\NormalTok{contcontdat }\OtherTok{\textless{}{-}}
    \FunctionTok{emmip}\NormalTok{(contcont,}
\NormalTok{          effort }\SpecialCharTok{\textasciitilde{}}\NormalTok{ hours,}
          \AttributeTok{at =}\NormalTok{ mylist,}
          \AttributeTok{CIs =} \ConstantTok{TRUE}\NormalTok{,}
          \AttributeTok{plotit =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{contcontdat}\SpecialCharTok{$}\NormalTok{feffort }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(contcontdat}\SpecialCharTok{$}\NormalTok{effort)}
\FunctionTok{levels}\NormalTok{(contcontdat}\SpecialCharTok{$}\NormalTok{feffort) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"low"}\NormalTok{, }\StringTok{"med"}\NormalTok{, }\StringTok{"high"}\NormalTok{)}

\CommentTok{\# plot}
\NormalTok{p  }\OtherTok{\textless{}{-}}
    \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ contcontdat, }
           \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ hours, }\AttributeTok{y =}\NormalTok{ yvar, }\AttributeTok{color =}\NormalTok{ feffort)) }\SpecialCharTok{+}  
    \FunctionTok{geom\_line}\NormalTok{()}
\NormalTok{p1 }\OtherTok{\textless{}{-}}
\NormalTok{    p }\SpecialCharTok{+} 
    \FunctionTok{geom\_ribbon}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{ymax =}\NormalTok{ UCL, }\AttributeTok{ymin =}\NormalTok{ LCL, }\AttributeTok{fill =}\NormalTok{ feffort), }
                    \AttributeTok{alpha =} \FloatTok{0.4}\NormalTok{)}
\NormalTok{p1  }\SpecialCharTok{+} \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Hours"}\NormalTok{,}
           \AttributeTok{y =} \StringTok{"Weight Loss"}\NormalTok{,}
           \AttributeTok{color =} \StringTok{"Effort"}\NormalTok{,}
           \AttributeTok{fill =} \StringTok{"Effort"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{17-moderation_files/figure-latex/unnamed-chunk-7-1} \end{center}

\hypertarget{continuous-by-categorical}{%
\subsection{Continuous by categorical}\label{continuous-by-categorical}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# use Female as basline}
\NormalTok{dat}\SpecialCharTok{$}\NormalTok{gender }\OtherTok{\textless{}{-}} \FunctionTok{relevel}\NormalTok{(dat}\SpecialCharTok{$}\NormalTok{gender, }\AttributeTok{ref =} \StringTok{"female"}\NormalTok{)}

\NormalTok{contcat }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(loss }\SpecialCharTok{\textasciitilde{}}\NormalTok{ hours }\SpecialCharTok{*}\NormalTok{ gender, }\AttributeTok{data =}\NormalTok{ dat)}
\FunctionTok{summary}\NormalTok{(contcat)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = loss \textasciitilde{} hours * gender, data = dat)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}     Min      1Q  Median      3Q     Max }
\CommentTok{\#\textgreater{} {-}27.118 {-}11.350  {-}1.963  10.001  42.376 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}                  Estimate Std. Error t value Pr(\textgreater{}|t|)  }
\CommentTok{\#\textgreater{} (Intercept)         3.335      2.731   1.221    0.222  }
\CommentTok{\#\textgreater{} hours               3.315      1.332   2.489    0.013 *}
\CommentTok{\#\textgreater{} gendermale          3.571      3.915   0.912    0.362  }
\CommentTok{\#\textgreater{} hours:gendermale   {-}1.724      1.898  {-}0.908    0.364  }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 14.06 on 896 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.008433,   Adjusted R{-}squared:  0.005113 }
\CommentTok{\#\textgreater{} F{-}statistic:  2.54 on 3 and 896 DF,  p{-}value: 0.05523}
\end{Highlighting}
\end{Shaded}

Get simple slopes by each level of the categorical moderator

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{emtrends}\NormalTok{(contcat, }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gender, }\AttributeTok{var =} \StringTok{"hours"}\NormalTok{)}
\CommentTok{\#\textgreater{}  gender hours.trend   SE  df lower.CL upper.CL}
\CommentTok{\#\textgreater{}  female        3.32 1.33 896    0.702     5.93}
\CommentTok{\#\textgreater{}  male          1.59 1.35 896   {-}1.063     4.25}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Confidence level used: 0.95}

\CommentTok{\# test difference in slopes}
\FunctionTok{emtrends}\NormalTok{(contcat, pairwise }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gender, }\AttributeTok{var =} \StringTok{"hours"}\NormalTok{)}
\CommentTok{\#\textgreater{} $emtrends}
\CommentTok{\#\textgreater{}  gender hours.trend   SE  df lower.CL upper.CL}
\CommentTok{\#\textgreater{}  female        3.32 1.33 896    0.702     5.93}
\CommentTok{\#\textgreater{}  male          1.59 1.35 896   {-}1.063     4.25}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Confidence level used: 0.95 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $contrasts}
\CommentTok{\#\textgreater{}  contrast      estimate  SE  df t.ratio p.value}
\CommentTok{\#\textgreater{}  female {-} male     1.72 1.9 896   0.908  0.3639}
\CommentTok{\# which is the same as the interaction term}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# plot}
\NormalTok{(mylist }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
    \AttributeTok{hours =} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{4}\NormalTok{, }\AttributeTok{by =} \FloatTok{0.4}\NormalTok{),}
    \AttributeTok{gender =} \FunctionTok{c}\NormalTok{(}\StringTok{"female"}\NormalTok{, }\StringTok{"male"}\NormalTok{)}
\NormalTok{))}
\CommentTok{\#\textgreater{} $hours}
\CommentTok{\#\textgreater{}  [1] 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 3.6 4.0}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $gender}
\CommentTok{\#\textgreater{} [1] "female" "male"}
\FunctionTok{emmip}\NormalTok{(contcat, gender }\SpecialCharTok{\textasciitilde{}}\NormalTok{ hours, }\AttributeTok{at =}\NormalTok{ mylist, }\AttributeTok{CIs =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{17-moderation_files/figure-latex/unnamed-chunk-10-1} \end{center}

\hypertarget{categorical-by-categorical}{%
\subsection{Categorical by categorical}\label{categorical-by-categorical}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# relevel baseline}
\NormalTok{dat}\SpecialCharTok{$}\NormalTok{prog   }\OtherTok{\textless{}{-}} \FunctionTok{relevel}\NormalTok{(dat}\SpecialCharTok{$}\NormalTok{prog, }\AttributeTok{ref =} \StringTok{"read"}\NormalTok{)}
\NormalTok{dat}\SpecialCharTok{$}\NormalTok{gender }\OtherTok{\textless{}{-}} \FunctionTok{relevel}\NormalTok{(dat}\SpecialCharTok{$}\NormalTok{gender, }\AttributeTok{ref =} \StringTok{"female"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{catcat }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(loss }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gender }\SpecialCharTok{*}\NormalTok{ prog, }\AttributeTok{data =}\NormalTok{ dat)}
\FunctionTok{summary}\NormalTok{(catcat)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = loss \textasciitilde{} gender * prog, data = dat)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}      Min       1Q   Median       3Q      Max }
\CommentTok{\#\textgreater{} {-}19.1723  {-}4.1894  {-}0.0994   3.7506  27.6939 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}                     Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)          {-}3.6201     0.5322  {-}6.802 1.89e{-}11 ***}
\CommentTok{\#\textgreater{} gendermale           {-}0.3355     0.7527  {-}0.446    0.656    }
\CommentTok{\#\textgreater{} progjog               7.9088     0.7527  10.507  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} progswim             32.7378     0.7527  43.494  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} gendermale:progjog    7.8188     1.0645   7.345 4.63e{-}13 ***}
\CommentTok{\#\textgreater{} gendermale:progswim  {-}6.2599     1.0645  {-}5.881 5.77e{-}09 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 6.519 on 894 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.7875, Adjusted R{-}squared:  0.7863 }
\CommentTok{\#\textgreater{} F{-}statistic: 662.5 on 5 and 894 DF,  p{-}value: \textless{} 2.2e{-}16}
\end{Highlighting}
\end{Shaded}

Simple effects

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{emcatcat }\OtherTok{\textless{}{-}} \FunctionTok{emmeans}\NormalTok{(catcat, }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gender}\SpecialCharTok{*}\NormalTok{prog)}

\CommentTok{\# differences in predicted values}
\FunctionTok{contrast}\NormalTok{(emcatcat, }
         \StringTok{"revpairwise"}\NormalTok{, }
         \AttributeTok{by =} \StringTok{"prog"}\NormalTok{, }
         \AttributeTok{adjust =} \StringTok{"bonferroni"}\NormalTok{)}
\CommentTok{\#\textgreater{} prog = read:}
\CommentTok{\#\textgreater{}  contrast      estimate    SE  df t.ratio p.value}
\CommentTok{\#\textgreater{}  male {-} female   {-}0.335 0.753 894  {-}0.446  0.6559}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} prog = jog:}
\CommentTok{\#\textgreater{}  contrast      estimate    SE  df t.ratio p.value}
\CommentTok{\#\textgreater{}  male {-} female    7.483 0.753 894   9.942  \textless{}.0001}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} prog = swim:}
\CommentTok{\#\textgreater{}  contrast      estimate    SE  df t.ratio p.value}
\CommentTok{\#\textgreater{}  male {-} female   {-}6.595 0.753 894  {-}8.762  \textless{}.0001}
\end{Highlighting}
\end{Shaded}

Plot

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{emmip}\NormalTok{(catcat, prog }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gender,}\AttributeTok{CIs=}\ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{17-moderation_files/figure-latex/unnamed-chunk-14-1} \end{center}

Bar graph

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{catcatdat }\OtherTok{\textless{}{-}} \FunctionTok{emmip}\NormalTok{(catcat,}
\NormalTok{                   gender }\SpecialCharTok{\textasciitilde{}}\NormalTok{ prog,}
                   \AttributeTok{CIs =} \ConstantTok{TRUE}\NormalTok{,}
                   \AttributeTok{plotit =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{p }\OtherTok{\textless{}{-}}
    \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ catcatdat,}
           \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ prog, }\AttributeTok{y =}\NormalTok{ yvar, }\AttributeTok{fill =}\NormalTok{ gender)) }\SpecialCharTok{+}
    \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat =} \StringTok{"identity"}\NormalTok{, }\AttributeTok{position =} \StringTok{"dodge"}\NormalTok{)}

\NormalTok{p1 }\OtherTok{\textless{}{-}}
\NormalTok{    p }\SpecialCharTok{+} \FunctionTok{geom\_errorbar}\NormalTok{(}
        \AttributeTok{position =} \FunctionTok{position\_dodge}\NormalTok{(.}\DecValTok{9}\NormalTok{),}
        \AttributeTok{width =}\NormalTok{ .}\DecValTok{25}\NormalTok{,}
        \FunctionTok{aes}\NormalTok{(}\AttributeTok{ymax =}\NormalTok{ UCL, }\AttributeTok{ymin =}\NormalTok{ LCL),}
        \AttributeTok{alpha =} \FloatTok{0.3}
\NormalTok{    )}
\NormalTok{p1  }\SpecialCharTok{+} \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Program"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Weight Loss"}\NormalTok{, }\AttributeTok{fill =} \StringTok{"Gender"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{17-moderation_files/figure-latex/unnamed-chunk-15-1} \end{center}

\hypertarget{probmod-package}{%
\section{probmod package}\label{probmod-package}}

\begin{itemize}
\tightlist
\item
  Not recommend: package has serious problem with subscript.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"probemod"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(probemod)}

\NormalTok{myModel }\OtherTok{\textless{}{-}}
    \FunctionTok{lm}\NormalTok{(loss }\SpecialCharTok{\textasciitilde{}}\NormalTok{ hours }\SpecialCharTok{*}\NormalTok{ gender, }\AttributeTok{data =}\NormalTok{ dat }\SpecialCharTok{\%\textgreater{}\%} 
           \FunctionTok{select}\NormalTok{(loss, hours, gender))}
\NormalTok{jnresults }\OtherTok{\textless{}{-}} \FunctionTok{jn}\NormalTok{(myModel,}
                \AttributeTok{dv =} \StringTok{\textquotesingle{}loss\textquotesingle{}}\NormalTok{,}
                \AttributeTok{iv =} \StringTok{\textquotesingle{}hours\textquotesingle{}}\NormalTok{,}
                \AttributeTok{mod =} \StringTok{\textquotesingle{}gender\textquotesingle{}}\NormalTok{)}


\FunctionTok{pickapoint}\NormalTok{(}
\NormalTok{    myModel,}
    \AttributeTok{dv =} \StringTok{\textquotesingle{}loss\textquotesingle{}}\NormalTok{,}
    \AttributeTok{iv =} \StringTok{\textquotesingle{}hours\textquotesingle{}}\NormalTok{,}
    \AttributeTok{mod =} \StringTok{\textquotesingle{}gender\textquotesingle{}}\NormalTok{,}
    \AttributeTok{alpha =}\NormalTok{ .}\DecValTok{01}
\NormalTok{)}

\FunctionTok{plot}\NormalTok{(jnresults)}
\end{Highlighting}
\end{Shaded}

\hypertarget{interactions-package}{%
\section{interactions package}\label{interactions-package}}

\begin{itemize}
\tightlist
\item
  Recommend
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"interactions"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{continuous-interaction}{%
\subsection{Continuous interaction}\label{continuous-interaction}}

\begin{itemize}
\tightlist
\item
  (at least one of the two variables is continuous)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(interactions)}
\FunctionTok{library}\NormalTok{(jtools) }\CommentTok{\# for summ()}
\NormalTok{states }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(state.x77)}
\NormalTok{fiti }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Income }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Illiteracy }\SpecialCharTok{*}\NormalTok{ Murder }\SpecialCharTok{+} \StringTok{\textasciigrave{}}\AttributeTok{HS Grad}\StringTok{\textasciigrave{}}\NormalTok{, }\AttributeTok{data =}\NormalTok{ states)}
\FunctionTok{summ}\NormalTok{(fiti)}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]
\centering
\begin{tabular}{lr}
\toprule
\cellcolor{gray!6}{Observations} & \cellcolor{gray!6}{50}\\
Dependent variable & Income\\
\cellcolor{gray!6}{Type} & \cellcolor{gray!6}{OLS linear regression}\\
\bottomrule
\end{tabular}
\end{table} \begin{table}[!h]
\centering
\begin{tabular}{lr}
\toprule
\cellcolor{gray!6}{F(4,45)} & \cellcolor{gray!6}{10.65}\\
RÂ² & 0.49\\
\cellcolor{gray!6}{Adj. RÂ²} & \cellcolor{gray!6}{0.44}\\
\bottomrule
\end{tabular}
\end{table} \begin{table}[!h]
\centering
\begin{threeparttable}
\begin{tabular}{lrrrr}
\toprule
  & Est. & S.E. & t val. & p\\
\midrule
\cellcolor{gray!6}{(Intercept)} & \cellcolor{gray!6}{1414.46} & \cellcolor{gray!6}{737.84} & \cellcolor{gray!6}{1.92} & \cellcolor{gray!6}{0.06}\\
Illiteracy & 753.07 & 385.90 & 1.95 & 0.06\\
\cellcolor{gray!6}{Murder} & \cellcolor{gray!6}{130.60} & \cellcolor{gray!6}{44.67} & \cellcolor{gray!6}{2.92} & \cellcolor{gray!6}{0.01}\\
`HS Grad` & 40.76 & 10.92 & 3.73 & 0.00\\
\cellcolor{gray!6}{Illiteracy:Murder} & \cellcolor{gray!6}{-97.04} & \cellcolor{gray!6}{35.86} & \cellcolor{gray!6}{-2.71} & \cellcolor{gray!6}{0.01}\\
\bottomrule
\end{tabular}
\begin{tablenotes}
\item Standard errors: OLS
\end{tablenotes}
\end{threeparttable}
\end{table}

For continuous moderator, the three values chosen are:

\begin{itemize}
\item
  -1 SD above the mean
\item
  The mean
\item
  -1 SD below the mean
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{interact\_plot}\NormalTok{(fiti,}
              \AttributeTok{pred =}\NormalTok{ Illiteracy,}
              \AttributeTok{modx =}\NormalTok{ Murder,}
              
              \CommentTok{\# if you don\textquotesingle{}t want the plot to mean{-}center}
              \CommentTok{\# centered = "none", }
              
              \CommentTok{\# exclude the mean value of the moderator}
              \CommentTok{\# modx.values = "plus{-}minus", }
              
              \CommentTok{\# split moderator\textquotesingle{}s distribution into 3 groups}
              \CommentTok{\# modx.values = "terciles" }
              
              \AttributeTok{plot.points =}\NormalTok{ T, }\CommentTok{\# overlay data}
              
              
              \CommentTok{\# different shape for differennt levels of the moderator}
              \AttributeTok{point.shape =}\NormalTok{ T, }
              
              \CommentTok{\# if two data points are on top one another, }
              \CommentTok{\# this moves them apart by little}
              \AttributeTok{jitter =} \FloatTok{0.1}\NormalTok{, }
              
              \CommentTok{\# other appearance option}
              \AttributeTok{x.label =} \StringTok{"X label"}\NormalTok{, }
              \AttributeTok{y.label =} \StringTok{"Y label"}\NormalTok{,}
              \AttributeTok{main.title =} \StringTok{"Title"}\NormalTok{,}
              \AttributeTok{legend.main =} \StringTok{"Legend Title"}\NormalTok{,}
              \AttributeTok{colors =} \StringTok{"blue"}\NormalTok{,}
              
              \CommentTok{\# include confidence band}
              \AttributeTok{interval =} \ConstantTok{TRUE}\NormalTok{, }
              \AttributeTok{int.width =} \FloatTok{0.9}\NormalTok{, }
              \AttributeTok{robust =} \ConstantTok{TRUE} \CommentTok{\# use robust SE}
\NormalTok{              ) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{17-moderation_files/figure-latex/unnamed-chunk-20-1} \end{center}

To include weights from the regression inn the plot

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fiti }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Income }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Illiteracy }\SpecialCharTok{*}\NormalTok{ Murder,}
           \AttributeTok{data =}\NormalTok{ states,}
           \AttributeTok{weights =}\NormalTok{ Population)}

\FunctionTok{interact\_plot}\NormalTok{(fiti,}
              \AttributeTok{pred =}\NormalTok{ Illiteracy,}
              \AttributeTok{modx =}\NormalTok{ Murder,}
              \AttributeTok{plot.points =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{17-moderation_files/figure-latex/unnamed-chunk-21-1} \end{center}

Partial Effect Plot

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{data}\NormalTok{(cars)}
\NormalTok{fitc }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(cty }\SpecialCharTok{\textasciitilde{}}\NormalTok{ year }\SpecialCharTok{+}\NormalTok{ cyl }\SpecialCharTok{*}\NormalTok{ displ }\SpecialCharTok{+}\NormalTok{ class }\SpecialCharTok{+}\NormalTok{ fl }\SpecialCharTok{+}\NormalTok{ drv, }
           \AttributeTok{data =}\NormalTok{ mpg)}
\FunctionTok{summ}\NormalTok{(fitc)}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]
\centering
\begin{tabular}{lr}
\toprule
\cellcolor{gray!6}{Observations} & \cellcolor{gray!6}{234}\\
Dependent variable & cty\\
\cellcolor{gray!6}{Type} & \cellcolor{gray!6}{OLS linear regression}\\
\bottomrule
\end{tabular}
\end{table} \begin{table}[!h]
\centering
\begin{tabular}{lr}
\toprule
\cellcolor{gray!6}{F(16,217)} & \cellcolor{gray!6}{99.73}\\
RÂ² & 0.88\\
\cellcolor{gray!6}{Adj. RÂ²} & \cellcolor{gray!6}{0.87}\\
\bottomrule
\end{tabular}
\end{table} \begin{table}[!h]
\centering
\begin{threeparttable}
\begin{tabular}{lrrrr}
\toprule
  & Est. & S.E. & t val. & p\\
\midrule
\cellcolor{gray!6}{(Intercept)} & \cellcolor{gray!6}{-200.98} & \cellcolor{gray!6}{47.01} & \cellcolor{gray!6}{-4.28} & \cellcolor{gray!6}{0.00}\\
year & 0.12 & 0.02 & 5.03 & 0.00\\
\cellcolor{gray!6}{cyl} & \cellcolor{gray!6}{-1.86} & \cellcolor{gray!6}{0.28} & \cellcolor{gray!6}{-6.69} & \cellcolor{gray!6}{0.00}\\
displ & -3.56 & 0.66 & -5.41 & 0.00\\
\cellcolor{gray!6}{classcompact} & \cellcolor{gray!6}{-2.60} & \cellcolor{gray!6}{0.93} & \cellcolor{gray!6}{-2.80} & \cellcolor{gray!6}{0.01}\\
\addlinespace
classmidsize & -2.63 & 0.93 & -2.82 & 0.01\\
\cellcolor{gray!6}{classminivan} & \cellcolor{gray!6}{-4.41} & \cellcolor{gray!6}{1.04} & \cellcolor{gray!6}{-4.24} & \cellcolor{gray!6}{0.00}\\
classpickup & -4.37 & 0.93 & -4.68 & 0.00\\
\cellcolor{gray!6}{classsubcompact} & \cellcolor{gray!6}{-2.38} & \cellcolor{gray!6}{0.93} & \cellcolor{gray!6}{-2.56} & \cellcolor{gray!6}{0.01}\\
classsuv & -4.27 & 0.87 & -4.92 & 0.00\\
\addlinespace
\cellcolor{gray!6}{fld} & \cellcolor{gray!6}{6.34} & \cellcolor{gray!6}{1.69} & \cellcolor{gray!6}{3.74} & \cellcolor{gray!6}{0.00}\\
fle & -4.57 & 1.66 & -2.75 & 0.01\\
\cellcolor{gray!6}{flp} & \cellcolor{gray!6}{-1.92} & \cellcolor{gray!6}{1.59} & \cellcolor{gray!6}{-1.21} & \cellcolor{gray!6}{0.23}\\
flr & -0.79 & 1.57 & -0.50 & 0.61\\
\cellcolor{gray!6}{drvf} & \cellcolor{gray!6}{1.40} & \cellcolor{gray!6}{0.40} & \cellcolor{gray!6}{3.52} & \cellcolor{gray!6}{0.00}\\
\addlinespace
drvr & 0.49 & 0.46 & 1.06 & 0.29\\
\cellcolor{gray!6}{cyl:displ} & \cellcolor{gray!6}{0.36} & \cellcolor{gray!6}{0.08} & \cellcolor{gray!6}{4.56} & \cellcolor{gray!6}{0.00}\\
\bottomrule
\end{tabular}
\begin{tablenotes}
\item Standard errors: OLS
\end{tablenotes}
\end{threeparttable}
\end{table}

\begin{Shaded}
\begin{Highlighting}[]

\FunctionTok{interact\_plot}\NormalTok{(}
\NormalTok{    fitc,}
    \AttributeTok{pred =}\NormalTok{ displ,}
    \AttributeTok{modx =}\NormalTok{ cyl,}
    \CommentTok{\# the observed data is based on displ, cyl, and model error}
    \AttributeTok{partial.residuals =} \ConstantTok{TRUE}\NormalTok{, }
    \AttributeTok{modx.values =} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{8}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{17-moderation_files/figure-latex/unnamed-chunk-22-1} \end{center}

Check linearity assumption in the model

Plot the lines based on the subsample (red line), and whole sample (black line)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x\_2 }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\AttributeTok{n =} \DecValTok{200}\NormalTok{, }\AttributeTok{min =} \SpecialCharTok{{-}}\DecValTok{3}\NormalTok{, }\AttributeTok{max =} \DecValTok{3}\NormalTok{)}
\NormalTok{w   }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(}\AttributeTok{n =} \DecValTok{200}\NormalTok{, }\AttributeTok{size =} \DecValTok{1}\NormalTok{, }\AttributeTok{prob =} \FloatTok{0.5}\NormalTok{)}
\NormalTok{err }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{200}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{4}\NormalTok{)}
\NormalTok{y\_2 }\OtherTok{\textless{}{-}} \FloatTok{2.5} \SpecialCharTok{{-}}\NormalTok{ x\_2 }\SpecialCharTok{\^{}} \DecValTok{2} \SpecialCharTok{{-}} \DecValTok{5} \SpecialCharTok{*}\NormalTok{ w }\SpecialCharTok{+} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ w }\SpecialCharTok{*}\NormalTok{ (x\_2 }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ err}

\NormalTok{data\_2 }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(x\_2, y\_2, w))}

\NormalTok{model\_2 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y\_2 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x\_2 }\SpecialCharTok{*}\NormalTok{ w, }\AttributeTok{data =}\NormalTok{ data\_2)}
\FunctionTok{summ}\NormalTok{(model\_2)}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]
\centering
\begin{tabular}{lr}
\toprule
\cellcolor{gray!6}{Observations} & \cellcolor{gray!6}{200}\\
Dependent variable & y\_2\\
\cellcolor{gray!6}{Type} & \cellcolor{gray!6}{OLS linear regression}\\
\bottomrule
\end{tabular}
\end{table} \begin{table}[!h]
\centering
\begin{tabular}{lr}
\toprule
\cellcolor{gray!6}{F(3,196)} & \cellcolor{gray!6}{1.40}\\
RÂ² & 0.02\\
\cellcolor{gray!6}{Adj. RÂ²} & \cellcolor{gray!6}{0.01}\\
\bottomrule
\end{tabular}
\end{table} \begin{table}[!h]
\centering
\begin{threeparttable}
\begin{tabular}{lrrrr}
\toprule
  & Est. & S.E. & t val. & p\\
\midrule
\cellcolor{gray!6}{(Intercept)} & \cellcolor{gray!6}{0.06} & \cellcolor{gray!6}{0.53} & \cellcolor{gray!6}{0.12} & \cellcolor{gray!6}{0.91}\\
x\_2 & 0.55 & 0.31 & 1.79 & 0.07\\
\cellcolor{gray!6}{w} & \cellcolor{gray!6}{0.54} & \cellcolor{gray!6}{0.75} & \cellcolor{gray!6}{0.72} & \cellcolor{gray!6}{0.47}\\
x\_2:w & -0.68 & 0.44 & -1.53 & 0.13\\
\bottomrule
\end{tabular}
\begin{tablenotes}
\item Standard errors: OLS
\end{tablenotes}
\end{threeparttable}
\end{table}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{interact\_plot}\NormalTok{(}
\NormalTok{    model\_2,}
    \AttributeTok{pred =}\NormalTok{ x\_2,}
    \AttributeTok{modx =}\NormalTok{ w,}
    \AttributeTok{linearity.check =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{plot.points =} \ConstantTok{TRUE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{17-moderation_files/figure-latex/unnamed-chunk-23-1} \end{center}

\hypertarget{simple-slopes-analysis}{%
\subsubsection{Simple Slopes Analysis}\label{simple-slopes-analysis}}

\begin{itemize}
\item
  continuous by continuous variable interaction (still work for binary)
\item
  conditional slope of the variable of interest (i.e., the slope of \(X\) when we hold \(M\) constant at a value)
\end{itemize}

Using \texttt{sim\_slopes} it will

\begin{itemize}
\item
  mean-center all variables except the variable of interest
\item
  For moderator that is

  \begin{itemize}
  \item
    Continuous, it will pick mean, and plus/minus 1 SD
  \item
    Categorical, it will use all factor
  \end{itemize}
\end{itemize}

\texttt{sim\_slopes} requires

\begin{itemize}
\item
  A regression model with an interaction term)
\item
  Variable of interest (\texttt{pred\ =})
\item
  Moderator: (\texttt{modx\ =})
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sim\_slopes}\NormalTok{(fiti,}
           \AttributeTok{pred =}\NormalTok{ Illiteracy,}
           \AttributeTok{modx =}\NormalTok{ Murder,}
           \AttributeTok{johnson\_neyman =} \ConstantTok{FALSE}\NormalTok{)}
\CommentTok{\#\textgreater{} SIMPLE SLOPES ANALYSIS }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Slope of Illiteracy when Murder =  5.420973 ({-} 1 SD): }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Est.     S.E.   t val.      p}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}   {-}71.59   268.65    {-}0.27   0.79}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Slope of Illiteracy when Murder =  8.685043 (Mean): }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}      Est.     S.E.   t val.      p}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}   {-}437.12   175.82    {-}2.49   0.02}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Slope of Illiteracy when Murder = 11.949113 (+ 1 SD): }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}      Est.     S.E.   t val.      p}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}   {-}802.66   145.72    {-}5.51   0.00}

\CommentTok{\# plot the coefficients}
\NormalTok{ss }\OtherTok{\textless{}{-}} \FunctionTok{sim\_slopes}\NormalTok{(fiti,}
                 \AttributeTok{pred =}\NormalTok{ Illiteracy,}
                 \AttributeTok{modx =}\NormalTok{ Murder,}
                 \AttributeTok{modx.values =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(ss)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{17-moderation_files/figure-latex/unnamed-chunk-24-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# table }
\NormalTok{ss }\OtherTok{\textless{}{-}} \FunctionTok{sim\_slopes}\NormalTok{(fiti,}
                 \AttributeTok{pred =}\NormalTok{ Illiteracy,}
                 \AttributeTok{modx =}\NormalTok{ Murder,}
                 \AttributeTok{modx.values =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{))}
\FunctionTok{library}\NormalTok{(huxtable)}
\FunctionTok{as\_huxtable}\NormalTok{(ss)}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-24} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l}


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} Value of Murder \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} Slope of Illiteracy \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{1}}->{\huxb{0, 0, 0}{1}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} Value of Murder \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} slope \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} 0.00 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} 535.50 (458.77) \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} 5.00 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} -24.44 (282.48) \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} 10.00 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} -584.38 (152.37)*** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

\hypertarget{johnson-neyman-intervals}{%
\subsubsection{Johnson-Neyman intervals}\label{johnson-neyman-intervals}}

To know all the values of the moderator for which the slope of the variable of interest will be statistically significant, we can use the Johnson-Neyman interval \citep{johnson1936tests}

Even though we kind of know that the alpha level when implementing the Johnson-Neyman interval is not correct \citep{bauer2005probing}, not until recently that there is a correction for the type I and II errors \citep{esarey2018marginal}.

Since Johnson-Neyman inflates the type I error (comparisons across all values of the moderator)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sim\_slopes}\NormalTok{(}
\NormalTok{    fiti,}
    \AttributeTok{pred =}\NormalTok{ Illiteracy,}
    \AttributeTok{modx =}\NormalTok{ Murder,}
    \AttributeTok{johnson\_neyman =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{control.fdr =} \ConstantTok{TRUE}\NormalTok{,}
    \CommentTok{\# correction for type I and II}
    
    \CommentTok{\# include conditional intecepts}
    \CommentTok{\# cond.int = TRUE, }
    
    \AttributeTok{robust =} \StringTok{"HC3"}\NormalTok{,}
    \CommentTok{\# rubust SE}
    
    \CommentTok{\# don\textquotesingle{}t mean{-}centered non{-}focal variables}
    \CommentTok{\# centered = "none",}
    \AttributeTok{jnalpha =} \FloatTok{0.05}
\NormalTok{)}
\CommentTok{\#\textgreater{} JOHNSON{-}NEYMAN INTERVAL }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} When Murder is OUTSIDE the interval [{-}11.70, 8.75], the slope of Illiteracy}
\CommentTok{\#\textgreater{} is p \textless{} .05.}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Note: The range of observed values of Murder is [1.40, 15.10]}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Interval calculated using false discovery rate adjusted t = 2.33 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} SIMPLE SLOPES ANALYSIS }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Slope of Illiteracy when Murder =  5.420973 ({-} 1 SD): }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Est.     S.E.   t val.      p}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}   {-}71.59   256.60    {-}0.28   0.78}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Slope of Illiteracy when Murder =  8.685043 (Mean): }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}      Est.     S.E.   t val.      p}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}   {-}437.12   191.07    {-}2.29   0.03}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Slope of Illiteracy when Murder = 11.949113 (+ 1 SD): }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}      Est.     S.E.   t val.      p}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}   {-}802.66   178.75    {-}4.49   0.00}
\end{Highlighting}
\end{Shaded}

For plotting, we can use \texttt{johnson\_neyman}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{johnson\_neyman}\NormalTok{(fiti,}
               \AttributeTok{pred =}\NormalTok{ Illiteracy,}
               \AttributeTok{modx =}\NormalTok{ Murder,}
               
               \CommentTok{\# correction for type I and II}
               \AttributeTok{control.fdr =} \ConstantTok{TRUE}\NormalTok{, }
               \AttributeTok{alpha =}\NormalTok{ .}\DecValTok{05}\NormalTok{)}
\CommentTok{\#\textgreater{} JOHNSON{-}NEYMAN INTERVAL }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} When Murder is OUTSIDE the interval [{-}22.57, 8.52], the slope of Illiteracy}
\CommentTok{\#\textgreater{} is p \textless{} .05.}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Note: The range of observed values of Murder is [1.40, 15.10]}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Interval calculated using false discovery rate adjusted t = 2.33}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{17-moderation_files/figure-latex/unnamed-chunk-26-1} \end{center}

Note:

\begin{itemize}
\tightlist
\item
  y-axis is the \textbf{conditional slope} of the variable of interest
\end{itemize}

\hypertarget{way-interaction}{%
\subsubsection{3-way interaction}\label{way-interaction}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# fita3 \textless{}{-}}
\CommentTok{\#     lm(rating \textasciitilde{} privileges * critical * learning, }
\CommentTok{\#        data = attitude)}
\CommentTok{\# }
\CommentTok{\# probe\_interaction(}
\CommentTok{\#     fita3,}
\CommentTok{\#     pred = critical,}
\CommentTok{\#     modx = learning,}
\CommentTok{\#     mod2 = privileges,}
\CommentTok{\#     alpha = .1}
\CommentTok{\# )}


\NormalTok{mtcars}\SpecialCharTok{$}\NormalTok{cyl }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{cyl,}
                     \AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"4 cylinder"}\NormalTok{, }\StringTok{"6 cylinder"}\NormalTok{, }\StringTok{"8 cylinder"}\NormalTok{))}
\NormalTok{fitc3 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ hp }\SpecialCharTok{*}\NormalTok{ wt }\SpecialCharTok{*}\NormalTok{ cyl, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{interact\_plot}\NormalTok{(fitc3,}
              \AttributeTok{pred =}\NormalTok{ hp,}
              \AttributeTok{modx =}\NormalTok{ wt,}
              \AttributeTok{mod2 =}\NormalTok{ cyl) }\SpecialCharTok{+}
    \FunctionTok{theme\_apa}\NormalTok{(}\AttributeTok{legend.pos =} \StringTok{"bottomright"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{17-moderation_files/figure-latex/unnamed-chunk-27-1} \end{center}

Johnson-Neyman 3-way interaction

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(survey)}
\FunctionTok{data}\NormalTok{(api)}

\NormalTok{dstrat }\OtherTok{\textless{}{-}} \FunctionTok{svydesign}\NormalTok{(}
    \AttributeTok{id =} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{,}
    \AttributeTok{strata =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ stype,}
    \AttributeTok{weights =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ pw,}
    \AttributeTok{data =}\NormalTok{ apistrat,}
    \AttributeTok{fpc =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ fpc}
\NormalTok{)}

\NormalTok{regmodel3 }\OtherTok{\textless{}{-}}
\NormalTok{    survey}\SpecialCharTok{::}\FunctionTok{svyglm}\NormalTok{(api00 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ avg.ed }\SpecialCharTok{*}\NormalTok{ growth }\SpecialCharTok{*}\NormalTok{ enroll, }\AttributeTok{design =}\NormalTok{ dstrat)}

\FunctionTok{sim\_slopes}\NormalTok{(}
\NormalTok{    regmodel3,}
    \AttributeTok{pred =}\NormalTok{ growth,}
    \AttributeTok{modx =}\NormalTok{ avg.ed,}
    \AttributeTok{mod2 =}\NormalTok{ enroll,}
    \AttributeTok{jnplot =} \ConstantTok{TRUE}
\NormalTok{)}
\CommentTok{\#\textgreater{} \#\#\#\#\#\#\#\#\#\#\#\#\#\#\# While enroll (2nd moderator) =  153.0518 ({-} 1 SD) \#\#\#\#\#\#\#\#\#\#\#\#\#\# }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} JOHNSON{-}NEYMAN INTERVAL }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} When avg.ed is OUTSIDE the interval [2.75, 3.82], the slope of growth is p}
\CommentTok{\#\textgreater{} \textless{} .05.}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Note: The range of observed values of avg.ed is [1.38, 4.44]}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} SIMPLE SLOPES ANALYSIS }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Slope of growth when avg.ed = 2.085002 ({-} 1 SD): }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   Est.   S.E.   t val.      p}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}   1.25   0.32     3.86   0.00}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Slope of growth when avg.ed = 2.787381 (Mean): }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   Est.   S.E.   t val.      p}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}   0.39   0.22     1.75   0.08}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Slope of growth when avg.ed = 3.489761 (+ 1 SD): }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}    Est.   S.E.   t val.      p}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}   {-}0.48   0.35    {-}1.37   0.17}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} \#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\# While enroll (2nd moderator) =  595.2821 (Mean) \#\#\#\#\#\#\#\#\#\#\#\#\#\#\# }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} JOHNSON{-}NEYMAN INTERVAL }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} When avg.ed is OUTSIDE the interval [2.84, 7.83], the slope of growth is p}
\CommentTok{\#\textgreater{} \textless{} .05.}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Note: The range of observed values of avg.ed is [1.38, 4.44]}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} SIMPLE SLOPES ANALYSIS }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Slope of growth when avg.ed = 2.085002 ({-} 1 SD): }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   Est.   S.E.   t val.      p}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}   0.72   0.22     3.29   0.00}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Slope of growth when avg.ed = 2.787381 (Mean): }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   Est.   S.E.   t val.      p}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}   0.34   0.16     2.16   0.03}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Slope of growth when avg.ed = 3.489761 (+ 1 SD): }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}    Est.   S.E.   t val.      p}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}   {-}0.04   0.24    {-}0.16   0.87}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} \#\#\#\#\#\#\#\#\#\#\#\#\#\#\# While enroll (2nd moderator) = 1037.5125 (+ 1 SD) \#\#\#\#\#\#\#\#\#\#\#\#\#\# }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} JOHNSON{-}NEYMAN INTERVAL }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} The Johnson{-}Neyman interval could not be found. Is the p value for your}
\CommentTok{\#\textgreater{} interaction term below the specified alpha?}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} SIMPLE SLOPES ANALYSIS }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Slope of growth when avg.ed = 2.085002 ({-} 1 SD): }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   Est.   S.E.   t val.      p}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}   0.18   0.31     0.58   0.56}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Slope of growth when avg.ed = 2.787381 (Mean): }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   Est.   S.E.   t val.      p}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}   0.29   0.20     1.49   0.14}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Slope of growth when avg.ed = 3.489761 (+ 1 SD): }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   Est.   S.E.   t val.      p}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}   0.40   0.27     1.49   0.14}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{17-moderation_files/figure-latex/unnamed-chunk-28-1} \end{center}

Report

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ss3 }\OtherTok{\textless{}{-}}
    \FunctionTok{sim\_slopes}\NormalTok{(regmodel3,}
               \AttributeTok{pred =}\NormalTok{ growth,}
               \AttributeTok{modx =}\NormalTok{ avg.ed,}
               \AttributeTok{mod2 =}\NormalTok{ enroll)}
\FunctionTok{plot}\NormalTok{(ss3)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{17-moderation_files/figure-latex/unnamed-chunk-29-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{as\_huxtable}\NormalTok{(ss3)}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-29} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l}


\hhline{>{\huxb{0, 0, 0}{1}}->{\huxb{0, 0, 0}{1}}-}
\arrayrulecolor{black}

\multicolumn{2}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textit{enroll = 153} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} Value of avg.ed \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} Slope of growth \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{1}}->{\huxb{0, 0, 0}{1}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} Value of avg.ed \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} slope \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} 2.09 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} 1.25 (0.32)*** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} 2.79 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} 0.39 (0.22)\# \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{1}}->{\huxb{0, 0, 0}{1}}-}
\arrayrulecolor{black}

\multicolumn{2}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textit{enroll = 595.28} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} Value of avg.ed \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} Slope of growth \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{1}}->{\huxb{0, 0, 0}{1}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} 3.49 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} -0.48 (0.35) \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} 2.09 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} 0.72 (0.22)** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} 2.79 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} 0.34 (0.16)* \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{1}}->{\huxb{0, 0, 0}{1}}-}
\arrayrulecolor{black}

\multicolumn{2}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textit{enroll = 1037.51} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} Value of avg.ed \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} Slope of growth \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{1}}->{\huxb{0, 0, 0}{1}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} 3.49 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} -0.04 (0.24) \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} 2.09 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} 0.18 (0.31) \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} 2.79 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} 0.29 (0.20) \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} 3.49 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} 0.40 (0.27) \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

\hypertarget{categorical-interaction}{%
\subsection{Categorical interaction}\label{categorical-interaction}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\NormalTok{mpg2 }\OtherTok{\textless{}{-}}\NormalTok{ mpg }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{cyl =} \FunctionTok{factor}\NormalTok{(cyl))}

\NormalTok{mpg2[}\StringTok{"auto"}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"auto"}
\NormalTok{mpg2}\SpecialCharTok{$}\NormalTok{auto[mpg2}\SpecialCharTok{$}\NormalTok{trans }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"manual(m5)"}\NormalTok{, }\StringTok{"manual(m6)"}\NormalTok{)] }\OtherTok{\textless{}{-}} \StringTok{"manual"}
\NormalTok{mpg2}\SpecialCharTok{$}\NormalTok{auto }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(mpg2}\SpecialCharTok{$}\NormalTok{auto)}
\NormalTok{mpg2[}\StringTok{"fwd"}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"2wd"}
\NormalTok{mpg2}\SpecialCharTok{$}\NormalTok{fwd[mpg2}\SpecialCharTok{$}\NormalTok{drv }\SpecialCharTok{==} \StringTok{"4"}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"4wd"}
\NormalTok{mpg2}\SpecialCharTok{$}\NormalTok{fwd }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(mpg2}\SpecialCharTok{$}\NormalTok{fwd)}
\DocumentationTok{\#\# Drop the two cars with 5 cylinders (rest are 4, 6, or 8)}
\NormalTok{mpg2 }\OtherTok{\textless{}{-}}\NormalTok{ mpg2[mpg2}\SpecialCharTok{$}\NormalTok{cyl }\SpecialCharTok{!=} \StringTok{"5"}\NormalTok{, ]}
\DocumentationTok{\#\# Fit the model}
\NormalTok{fit3 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(cty }\SpecialCharTok{\textasciitilde{}}\NormalTok{ cyl }\SpecialCharTok{*}\NormalTok{ fwd }\SpecialCharTok{*}\NormalTok{ auto, }\AttributeTok{data =}\NormalTok{ mpg2)}

\FunctionTok{library}\NormalTok{(jtools) }\CommentTok{\# for summ()}
\FunctionTok{summ}\NormalTok{(fit3)}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]
\centering
\begin{tabular}{lr}
\toprule
\cellcolor{gray!6}{Observations} & \cellcolor{gray!6}{230}\\
Dependent variable & cty\\
\cellcolor{gray!6}{Type} & \cellcolor{gray!6}{OLS linear regression}\\
\bottomrule
\end{tabular}
\end{table} \begin{table}[!h]
\centering
\begin{tabular}{lr}
\toprule
\cellcolor{gray!6}{F(11,218)} & \cellcolor{gray!6}{61.37}\\
RÂ² & 0.76\\
\cellcolor{gray!6}{Adj. RÂ²} & \cellcolor{gray!6}{0.74}\\
\bottomrule
\end{tabular}
\end{table} \begin{table}[!h]
\centering
\begin{threeparttable}
\begin{tabular}{lrrrr}
\toprule
  & Est. & S.E. & t val. & p\\
\midrule
\cellcolor{gray!6}{(Intercept)} & \cellcolor{gray!6}{21.37} & \cellcolor{gray!6}{0.39} & \cellcolor{gray!6}{54.19} & \cellcolor{gray!6}{0.00}\\
cyl6 & -4.37 & 0.54 & -8.07 & 0.00\\
\cellcolor{gray!6}{cyl8} & \cellcolor{gray!6}{-8.37} & \cellcolor{gray!6}{0.67} & \cellcolor{gray!6}{-12.51} & \cellcolor{gray!6}{0.00}\\
fwd4wd & -2.91 & 0.76 & -3.83 & 0.00\\
\cellcolor{gray!6}{automanual} & \cellcolor{gray!6}{1.45} & \cellcolor{gray!6}{0.57} & \cellcolor{gray!6}{2.56} & \cellcolor{gray!6}{0.01}\\
\addlinespace
cyl6:fwd4wd & 0.59 & 0.96 & 0.62 & 0.54\\
\cellcolor{gray!6}{cyl8:fwd4wd} & \cellcolor{gray!6}{2.13} & \cellcolor{gray!6}{0.99} & \cellcolor{gray!6}{2.15} & \cellcolor{gray!6}{0.03}\\
cyl6:automanual & -0.76 & 0.90 & -0.84 & 0.40\\
\cellcolor{gray!6}{cyl8:automanual} & \cellcolor{gray!6}{0.71} & \cellcolor{gray!6}{1.18} & \cellcolor{gray!6}{0.60} & \cellcolor{gray!6}{0.55}\\
fwd4wd:automanual & -1.66 & 1.07 & -1.56 & 0.12\\
\addlinespace
\cellcolor{gray!6}{cyl6:fwd4wd:automanual} & \cellcolor{gray!6}{1.29} & \cellcolor{gray!6}{1.52} & \cellcolor{gray!6}{0.85} & \cellcolor{gray!6}{0.40}\\
cyl8:fwd4wd:automanual & -1.39 & 1.76 & -0.79 & 0.43\\
\bottomrule
\end{tabular}
\begin{tablenotes}
\item Standard errors: OLS
\end{tablenotes}
\end{threeparttable}
\end{table}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat\_plot}\NormalTok{(fit3,}
         \AttributeTok{pred =}\NormalTok{ cyl,}
         \AttributeTok{modx =}\NormalTok{ fwd,}
         \AttributeTok{plot.points =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{17-moderation_files/figure-latex/unnamed-chunk-31-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#line plots}
\FunctionTok{cat\_plot}\NormalTok{(}
\NormalTok{    fit3,}
    \AttributeTok{pred =}\NormalTok{ cyl,}
    \AttributeTok{modx =}\NormalTok{ fwd,}
    \AttributeTok{geom =} \StringTok{"line"}\NormalTok{,}
    \AttributeTok{point.shape =} \ConstantTok{TRUE}\NormalTok{,}
    \CommentTok{\# colors = "Set2", \# choose color}
    \AttributeTok{vary.lty =} \ConstantTok{TRUE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{17-moderation_files/figure-latex/unnamed-chunk-31-2} \end{center}

\begin{Shaded}
\begin{Highlighting}[]


\CommentTok{\# bar plot}
\FunctionTok{cat\_plot}\NormalTok{(}
\NormalTok{    fit3,}
    \AttributeTok{pred =}\NormalTok{ cyl,}
    \AttributeTok{modx =}\NormalTok{ fwd,}
    \AttributeTok{geom =} \StringTok{"bar"}\NormalTok{,}
    \AttributeTok{interval =}\NormalTok{ T,}
    \AttributeTok{plot.points =} \ConstantTok{TRUE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{17-moderation_files/figure-latex/unnamed-chunk-31-3} \end{center}

\hypertarget{interactionr-package}{%
\section{interactionR package}\label{interactionr-package}}

\begin{itemize}
\tightlist
\item
  For publication purposes
\item
  Following

  \begin{itemize}
  \item
    \citep{knol2012recommendations} for presentation
  \item
    \citep{hosmer1992confidence} for confidence intervals based on the delta method
  \item
    \citep{zou2008estimation} for variance recovery ``mover'' method
  \item
    \citep{assmann1996confidence} for bootstrapping
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"interactionR"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{sjplot-package}{%
\section{sjPlot package}\label{sjplot-package}}

\begin{itemize}
\item
  For publication purposes (recommend, but more advanced)
\item
  \href{https://strengejacke.github.io/sjPlot/articles/plot_interactions.html}{link}
\end{itemize}

\hypertarget{part-iv.-causal-inference}{%
\part*{IV. CAUSAL INFERENCE}\label{part-iv.-causal-inference}}
\addcontentsline{toc}{part}{IV. CAUSAL INFERENCE}

\hypertarget{causal-inference}{%
\chapter{Causal Inference}\label{causal-inference}}

After all of the mambo jumbo that we have learned so far, I want to now talk about the concept of causality. We usually say that correlation is not causation. Then, what is causation?\\
One of my favorite books has explained this concept beautifully \citep{Pearl_2018}. And I am just going to quickly summarize the gist of it from my understanding. I hope that it can give you an initial grasp on the concept so that later you can continue to read up and develop a deeper understanding.

It's important to have a deep understanding regarding the method research. However, one needs to be aware of its limitation. As mentioned in various sections throughout the book, we see that we need to ask experts for number as our baseline or visit literature to gain insight from past research.

Here, we dive in a more conceptual side statistical analysis as a whole, regardless of particular approach.

You probably heard scientists say correlation doesn't mean causation. There are ridiculous \href{http://www.tylervigen.com/spurious-correlations}{spurious correlations} that give a firm grip on what the previous phrase means. The pioneer who tried to use regression to infer causation in social science was \citet{yule1899} (but it was a fatal attempt where he found relief policy increases poverty). To make a causal inference from statistics, \textbf{the equation (function form) must be stable} under intervention (i.e., variables are manipulated). Statistics is used to be a causality-free enterprise in the past.

Not until the development of path analysis by Sewall Wright in the 1920s that the discipline started to pay attention to causation. Then, it remained dormant until the Causal Revolution (quoted Judea Pearl's words). This revolution introduced the calculus of causation which includes (1) causal diagrams), and (2) a symbolic language

The world has been using \(P(Y|X)\) (statistics use to derive this), but what we want is to compare the difference between

\begin{itemize}
\item
  \(P(Y|do(X))\): treatment group
\item
  \(P(Y|do(not-X))\): control group
\end{itemize}

Hence, we can see a clear difference between \(P(Y|X) \neq P(Y|do(X))\)

The conclusion we want to make from data is counterfactuals: \textbf{What would have happened had we not do X?}

To teach a robot to make inference, we need inference engine

\begin{figure}
\centering
\includegraphics[width=6.25in,height=4.16667in]{images/Figure I.png}
\caption{p.~12 \citep{Pearl_2018}}
\end{figure}

Levels of cognitive ability to be a causal learner:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Seeing
\item
  Doing
\item
  Imagining
\end{enumerate}

Ladder of causation (associated with levels of cognitive ability as well):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Association: conditional probability, correlation, regression
\item
  Intervention
\item
  Counterfactuals
\end{enumerate}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1295}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1007}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3165}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.4388}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Level
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Activity
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Questions
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Examples
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Association

\(P(y|x)\) & Seeing & What is?

How would seeing X change my belief in Y? & What does a symptom tell me about a disease? \\
Intervention

\(P(y|do(x),z)\) & Doing

Intervening & What if?

What if I do X? & What if I spend more time learning, will my result change? \\
Counterfactuals

\(P(y_x|x',y')\) & Imagining & \begin{minipage}[t]{\linewidth}\raggedright
Why?\\
was it X that caused Y?

What if I had acted differently\strut
\end{minipage} & What if I stopped smoking a year ago? \\
\end{longtable}

Table by \citep[p.~57]{pearl2019seven}

You cannot define causation from probability alone

If you say X causes Y if X raises the probability of Y.'' On the surface, it might sound intuitively right. But when we translate it to probability notation: \(P(Y|X) >P(Y)\) , it can't be more wrong. Just because you are seeing X (1st level), it \textbf{doesn't mean} the probability of Y increases.

It could be either that (1) X causes Y, or (2) Z affects both X and Y. Hence, people might use \textbf{control variables}, which translate: \(P(Y|X, Z=z) > P(Y|Z=z)\), then you can be more confident in your probabilistic observation. However, the question is how can you choose \(Z\)

With the invention of the do-operator, now you can represent X causes Y as

\[
P(Y|do(X)) > P(Y)
\]

and with the help of causal diagram, now you can answer questions at the 2nd level (Intervention)

Note: people under econometrics might still use ``Granger causality'' and ``vector autoregression'' to use the probability language to represent causality (but it's not).

The 7 tools for Structural Causal Model framework \citep{pearl2019seven}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Encoding Causal Assumptions - transparency and testability (with graphical representation)
\item
  Do-calculus and the control of confounding: ``back-door''
\item
  The algorithmization of Counterfactuals
\item
  Mediation Analysis and the Assessment of Direct and Indirect Effects
\item
  Adaptability, External validity and Sample Selection Bias: are still researched under ``domain adaptation'', ``transfer learning''
\item
  Recovering from missing data
\item
  Causal Discovery:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    d-separation
  \item
    Functional decomposition \citep{hoyer2008nonlinear}
  \item
    Spontaneous local changes \citep{pearl2014graphical}
  \end{enumerate}
\end{enumerate}

\href{https://cran.r-project.org/web/views/CausalInference.html}{List of packages to do causal inference} in R

Simpson's Paradox:

\begin{itemize}
\tightlist
\item
  A statistical association seen in an entire population is reversed in sub-population.
\end{itemize}

Structural Causal Model accompanies graphical causal model to create more efficient language to represent causality

Structural Causal Model is the solution to the curse of dimensionality (i.e., large numbers of variable \(p\), and small dataset \(n\)) thanks to product decomposition. It allows us to solve problems without knowing the function, parameters, or distributions of the error terms.

Suppose you have a causal chain \(X \to Y \to Z\):

\[
P(X=x,Y=y, Z=z) = P(X=x)P(Y=y|X=x)P(Z=z|Y=y)
\]

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.4949}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5051}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Experimental Design
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Quasi-experimental Design
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Experimentalist & Observationalist \\
Experimental Data & Observational Data \\
Random Assignment (reduce treatment imbalance) & Random Sampling (reduce sample selection error) \\
\end{longtable}

Tools in a hierarchical order

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \protect\hyperlink{experimental-design}{Experimental Design}: Randomized Control Trials (Gold standard): Tier 1
\item
  \protect\hyperlink{quasi-experimental}{Quasi-experimental}

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    \protect\hyperlink{regression-discontinuity}{Regression Discontinuity} Tier 1A
  \item
  \item
    \protect\hyperlink{difference-in-differences}{Difference-In-Differences} Tier 2A
  \item
    \protect\hyperlink{synthetic-control}{Synthetic Control} Tier 2A
  \item
    \protect\hyperlink{event-studies}{Event Studies} Tier 2B
  \item
    Fixed Effects Estimator \ref{fixed-effects-estimator}: Tier 3
  \item
    \protect\hyperlink{endogenous-treatment}{Endogenous Treatment}: mostly \protect\hyperlink{instrumental-variable}{Instrumental Variable}: Tier 3A
  \item
    \protect\hyperlink{matching-methods}{Matching Methods} Tier 4
  \item
    \protect\hyperlink{interrupted-time-series}{Interrupted Time Series} Tier 4A
  \item
    Endogenous Sample Selection \ref{endogenous-sample-selection}: mostly Heckman's correction
  \end{enumerate}
\end{enumerate}

Internal vs.~External Validity

\begin{itemize}
\item
  Internal Validity: Economists and applied scientists mostly care about
\item
  External Validity: Localness might affect your external validity
\end{itemize}

For many economic policies, there is a difference between \textbf{treatment} and \textbf{intention to treat}.

For example, we might have an effective vaccine (i.e., intention to treat), but it does not mean that everybody will take it (i.e., treatment).

There are four types of subjects that we deal with:

\begin{itemize}
\item
  \textbf{Non-switchers}: we don't care about non-switchers because even if we introduce or don't introduce the intervention, it won't affect them.

  \begin{itemize}
  \item
    \textbf{Always takers}
  \item
    \textbf{Never takers}
  \end{itemize}
\item
  \textbf{Switchers}

  \begin{itemize}
  \item
    \textbf{Compliers}: defined as those who respect the intervention.

    \begin{itemize}
    \item
      We only care about compliers because when we introduce the intervention, they will do something. When we don't have any interventions, they won't do it.
    \item
      Tools above are used to identify the causal impact of an intervention on compliers
    \item
      If we have only \textbf{compliers} in our dataset, then \textbf{intention to treatment = treatment effect}.
    \end{itemize}
  \item
    \textbf{Defiers}: those who will go to the opposite direction of your treatment.

    \begin{itemize}
    \tightlist
    \item
      We typically aren't interested in defiers because they will do the opposite of what we want them to do. And they are typically a small group; hence, we just assume they don't exist.
    \end{itemize}
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
& Treatment Assignment & Control Assignment \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Compliers & Treated & No Treated \\
Always-takers & Treated & Treated \\
Never-takers & Not treated & No treated \\
Defiers & Not treated & Treated \\
\end{longtable}

Directional Bias due to selection into treatment comes from 2 general opposite sources

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Mitigation-based}: select into treatment to combat a problem
\item
  \textbf{Preference-based}: select into treatment because units like that kind of treatment.
\end{enumerate}

\hypertarget{treatment-effect-types}{%
\section{Treatment effect types}\label{treatment-effect-types}}

This section is based on \href{https://egap.org/resource/10-types-of-treatment-effect-you-should-know-about/}{Paul Testa's note}

Terminology:

\begin{itemize}
\item
  Quantities of causal interest (i.e., treatment effect types)
\item
  Estimands: parameters of interest
\item
  Estimators: procedures to calculate hesitates for the parameters of interest
\end{itemize}

Sources of bias (\href{https://www.youtube.com/watch?v=CjZnQ3ToJjg}{according to prof. Luke Keele})

\[
\begin{aligned}
&\text{Estimator - True Causal Effect} \\
&= \text{Hidden bias + Misspecification bias + Statistical Noise} \\
&= \text{Due to design + Due to modeling + Due to finite sample}
\end{aligned}
\]

\hypertarget{average-treatment-effects}{%
\subsection{Average Treatment Effects}\label{average-treatment-effects}}

Average treatment effect (ATE) is the difference in means of the treated and control groups

\textbf{Randomization} under \protect\hyperlink{experimental-design}{Experimental Design} can provide an unbiased estimate of ATE.

Let \(Y_i(1)\) denote the outcome of individual \(i\) under treatment and

\(Y_i(0)\) denote the outcome of individual \(i\) under control

Then, the treatment effect for individual \(i\) is the difference between her outcome under treatment and control

\[
\tau_i = Y_i(1) - Y_i(0)
\]

Without a time machine or dimension portal, we can only observe one of the two event: either individual \(i\) experiences the treatment or she doesn't.

Then, the ATE as a quantity of interest can come in handy since we can observe across all individuals

\[
ATE = \frac{1}{N} \sum_{i=1}^N \tau_i = \frac{\sum_1^N Y_i(1)}{N} - \frac{\sum_i^N Y_i(0)}{N}
\]

With random assignment (i.e., treatment assignment is independent of potential outcome and observables and unobservables), the observed means difference between the two groups is an unbiased estimator of the average treatment effect

\[
E(Y_i (1) |D = 1) = E(Y_i(1)|D=0) = E(Y_i(1)) \\
E(Y_i(0) |D = 1) = E(Y_i(0)|D = 0 ) = E(Y_i(0))
\]

\[
ATE = E(Y_i(1)) - E(Y_i(0))
\]

Alternatively, we can write the potential outcomes model in a regression form

\[
Y_i = Y_i(0)  + [Y_i (1) - Y_i(0)] D_i
\]

Let \(\beta_{0i} = Y_i (0) ; \beta_{1i} = Y_i(1) - Y_i(0)\), we have

\[
Y_i = \beta_{0i} + \beta_{1i} D_i
\]

where

\begin{itemize}
\item
  \(\beta_{0i}\) = outcome if the unit did not receive any treatment
\item
  \(\beta_{1i}\) = treatment effect (i.e., random coefficients for each unit \(i\))
\end{itemize}

To understand endogeneity (i.e., nonrandom treatment assignment), we can examine a standard linear model

\[
\begin{aligned}
Y_i &= \beta_{0i} + \beta_{1i} D_i \\
&= ( \bar{\beta}_{0} + \epsilon_{0i} ) + (\bar{\beta}_{1} + \epsilon_{1i} )D_i \\
&=  \bar{\beta}_{0} + \epsilon_{0i} + \bar{\beta}_{1} D_i + \epsilon_{1i} D_i
\end{aligned}
\]

When you have random assignment, \(E(\epsilon_{0i}) = E(\epsilon_{1i}) = 0\)

\begin{itemize}
\item
  No selection bias: \(D_i \perp e_{0i}\)
\item
  Treatment effect is independent of treatment assignment: \(D_i \perp e_{1i}\)
\end{itemize}

But otherwise, residuals can correlate with \(D_i\)

For estimation,

\begin{itemize}
\item
  \(\hat{\beta}_1^{OLS}\) is identical to difference in means (i.e., \(Y_i(1) - Y_i(0)\))
\item
  In case of heteroskedasticity (i.e., \(\epsilon_{0i} + D_i \epsilon_{1i} \neq 0\) ), this residual's variance depends on \(X\) when you have heterogeneous treatment effects (i.e., \(\epsilon_{1i} \neq 0\))

  \begin{itemize}
  \item
    Robust SE should still give consistent estimate of \(\hat{\beta}_1\) in this case
  \item
    Alternatively, one can use two-sample t-test on difference in means with unequal variances.
  \end{itemize}
\end{itemize}

\hypertarget{conditional-average-treatment-effects}{%
\subsection{Conditional Average Treatment Effects}\label{conditional-average-treatment-effects}}

Treatment effects can be different for different groups of people. In words, treatment effects can vary across subgroups.

To examine the heterogeneity across groups (e.g., men vs.~women), we can estimate the conditional average treatment effects (CATE) for each subgroup

\[
CATE = E(Y_i(1) - Y_i(0) |D_i, X_i))
\]

\hypertarget{intent-to-treat-effects}{%
\subsection{Intent-to-treat Effects}\label{intent-to-treat-effects}}

When we encounter non-compliance (either people suppose to receive treatment don't receive it, or people suppose to be in the control group receive the treatment), treatment receipt is not independent of potential outcomes and confounders.

In this case, the difference in observed means between the treatment and control groups is not \protect\hyperlink{average-treatment-effects}{Average Treatment Effects}, but \protect\hyperlink{intent-to-treat-effects}{Intent-to-treat Effects} (ITT). In words, ITT is the treatment effect on those who \textbf{receive} the treatment

\hypertarget{local-average-treatment-effects}{%
\subsection{Local Average Treatment Effects}\label{local-average-treatment-effects}}

Instead of estimating the treatment effects of those who \textbf{receive} the treatment (i.e., \protect\hyperlink{intent-to-treat-effects}{Intent-to-treat Effects}), you want to estimate the treatment effect of those who actually \textbf{comply} with the treatment. This is the local average treatment effects (LATE) or complier average causal effects (CACE). I assume we don't use CATE to denote complier average treatment effect because it was reserved for conditional average treatment effects.

\begin{itemize}
\tightlist
\item
  Using random treatment assignment as an instrument, we can recover the effect of treatment on compliers.
\end{itemize}

\includegraphics[width=6.25in,height=3.75in]{images/iv_late.PNG}

\begin{itemize}
\item
  As the percent of compliers increases, \protect\hyperlink{intent-to-treat-effects}{Intent-to-treat Effects} and \protect\hyperlink{local-average-treatment-effects}{Local Average Treatment Effects} converge
\item
  Rule of thumb: SE(LATE) = SE(ITT)/(share of compliers)
\item
  LATE estimate is always greater than the ITT estimate
\item
  LATE can also be estimated using a pure placebo group \citep{gerber2010}.
\item
  Partial compliance is hard to study, and IV/2SLS estimator is biased, we have to use Bayesian \citep{long2010, jin2009, jin2008}.
\end{itemize}

\hypertarget{one-sided-noncompliance}{%
\subsubsection{One-sided noncompliance}\label{one-sided-noncompliance}}

\begin{itemize}
\item
  One-sided noncompliance is when in the sample, we only have compliers and never-takers
\item
  With the exclusion restriction (i.e., excludability), never-takers have the same results in the treatment or control group (i.e., never treated)
\item
  With random assignment, we can have the same number of never-takers in the treatment and control groups
\item
  Hence,
\end{itemize}

\[
LATE = \frac{ITT}{\text{share of compliers}}
\]

\hypertarget{two-sided-noncompliance}{%
\subsubsection{Two-sided noncompliance}\label{two-sided-noncompliance}}

\begin{itemize}
\item
  Two-sided noncompliance is when in the sample, we have compliers, never-takers, and always-takers
\item
  To estimate LATE, beyond excludability like in the \protect\hyperlink{one-sided-noncompliance}{One-sided noncompliance} case, we need to assume that there is no defiers (i.e., monotonicity assumption) (this is excusable in practical studies)
\end{itemize}

\[
LATE = \frac{ITT}{\text{share of compliers}}
\]

\hypertarget{population-vs.-sample-average-treatment-effects}{%
\subsection{Population vs.~Sample Average Treatment Effects}\label{population-vs.-sample-average-treatment-effects}}

See \citep{imai2008} for when the sample average treatment effect (SATE) diverges from the population average treatment effect (PATE).

To stay consistent, this section uses notations from \citep{imai2008}'s paper.

In a finite population \(N\), we observe \(n\) observations (\(N>>n\)), where half is in the control and half is in the treatment group.

With unknown data generating process, we have

\[
I_i = 
\begin{cases}
1 \text{ if unit i is in the sample} \\
0 \text{ otherwise}
\end{cases}
\]

\[
T_i = 
\begin{cases}
1 \text{ if unit i is in the treatment group} \\
0 \text{ if unit i is in the control group}
\end{cases}
\]

\[
\text{potential outcome} = 
\begin{cases}
Y_i(1) \text{ if } T_i = 1 \\
Y_i(0) \text{ if } T_i = 0
\end{cases}
\]

Observed outcome is

\[
Y_i | I_i = 1= T_i Y_i(1) + (1-T_i)Y_i(0)
\]

Since we can never observed both outcome for the same individual, the treatment effect is always unobserved for unit \(i\)

\[
TE_i = Y_i(1) - Y_i(0)
\]

Sample average treatment effect is

\[
SATE = \frac{1}{n}\sum_{i \in \{I_i = 1\}} TE_i
\]

Population average treatment effect is

\[
PATE = \frac{1}{N}\sum_{i=1}^N TE_i
\]

Let \(X_i\) be observables and \(U_i\) be unobservables for unit \(i\)

The baseline estimator for SATE and PATE is

\[
\begin{aligned}
D &= \frac{1}{n/2} \sum_{i \in (I_i = 1, T_i = 1)} Y_i - \frac{1}{n/2} \sum_{i \in (I_i = 1 , T_i = 0)} Y_i \\
&= \text{observed sample mean of the treatment group} \\
&- \text{observed sample mean of the control group}
\end{aligned}
\]

Let \(\Delta\) be the estimation error (deviation from the truth), under an additive model

\[
Y_i(t) = g_t(X_i) + h_t(U_i)
\]

The decomposition of the estimation error is

\[
\begin{aligned}
PATE - D = \Delta &= \Delta_S + \Delta_T \\
&= (PATE - SATE) + (SATE - D)\\
&= \text{sample selection}+ \text{treatment imbalance} \\
&= (\Delta_{S_X} + \Delta_{S_U}) + (\Delta_{T_X} + \Delta_{T_U}) \\
&= \text{(selection on observed + selection on unobserved)} \\
&+ (\text{treatment imbalance in observed + unobserved})
\end{aligned}
\]

\hypertarget{estimation-error-from-sample-selection}{%
\subsubsection{Estimation Error from Sample Selection}\label{estimation-error-from-sample-selection}}

Also known as sample selection error

\[
\Delta_S = PATE - SATE = \frac{N - n}{N}(NATE - SATE)
\]

where NATE is the non-sample average treatment effect (i.e., average treatment effect for those in the population but not in your sample:

\[
NATE = \sum_{i\in (I_i = 0)} \frac{TE_i}{N-n}
\]

From the equation, to have zero sample selection error (i.e., \(\Delta_S = 0\)), we can either

\begin{itemize}
\item
  Get \(N = n\) by redefining your sample as the population of interest
\item
  \(NATE = SATE\) (e.g., \(TE_i\) is constant over \(i\) in both your selected sample, and those in the population that you did not select)
\end{itemize}

Note

\begin{itemize}
\item
  When you have heterogeneous treatment effects, \textbf{random sampling} can only warrant \textbf{sample selection bias}, not \textbf{sample selection error}.
\item
  Since we can rarely know the true underlying distributions of the observables (\(X\)) and unobservables (\(U\)), we cannot verify whether the empirical distributions of your observables and unobservables for those in your sample is identical to that of your population (to reduce \(\Delta_S\)). For special case,

  \begin{itemize}
  \item
    Say you have census of your population, you can adjust for the observables \(X\) to reduce \(\Delta_{S_X}\), but still you cannot adjust your unobservables (\(U\))
  \item
    Say you are willing to assume \(TE_i\) is constant over

    \begin{itemize}
    \item
      \(X_i\), then \(\Delta_{S_X} = 0\)
    \item
      \(U_i\), then \(\Delta_{U}=0\)
    \end{itemize}
  \end{itemize}
\end{itemize}

\hypertarget{estimation-error-from-treatment-imbalance}{%
\subsubsection{Estimation Error from Treatment Imbalance}\label{estimation-error-from-treatment-imbalance}}

Also known as treatment imbalance error

\[
\Delta_T = SATE - D
\]

\(\Delta_T \to 0\) when treatment and control groups are balanced (i.e., identical empirical distributions) for both observables (\(X\)) and unobservables (\(U\))

However, in reality, we can only readjust for observables, not unobservables.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1289}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3701}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4990}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\protect\hyperlink{randomized-block-designs}{Blocking}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
{[}Matching{]}\textbf{\protect\hyperlink{matching-methods}{Matching Methods}}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Definition & Random assignment within strata based on pre-treatment observables & Dropping, repeating or grouping observations to balance covariates between the treatment and control group \citep{rubin1973use} \\
Time & Before randomization of treatments & After randomization of treatments \\
What if the set of covariates used to adjust is irrelevant? & Nothing happens & In the worst case scenario (e.g., these variables are uncorrelated with the treatment assignment, but correlated with the post-treatment variables), matching induces bias that is greater than just using the unadjusted difference in means \\
Benefits & \(\Delta_{T_X}=0\) (no imbalance on observables). But we don't know its effect on unobservables imbalance (might reduce if the unobservables are correlated with the observables) & Reduce model dependence, bias, variance, mean-square error \\
\end{longtable}

\hypertarget{average-treatment-effects-on-the-treated-and-control}{%
\subsection{Average Treatment Effects on the Treated and Control}\label{average-treatment-effects-on-the-treated-and-control}}

Average Effect of treatment on the Treated (ATT) is

\[
\begin{aligned}
ATT &= E(Y_i(1) - Y_i(0)|D_i = 1) \\
&= E(Y_i(1)|D_i = 1) - E(Y_i(0) |D_i = 1)
\end{aligned}
\]

Average Effect of treatment on the Control (ATC) (i.e., the effect \textbf{would be} for those weren't treated) is

\[
\begin{aligned}
ATC &= E(Y_i(1) - Y_i (0) |D_i =0) \\
&= E(Y_i(1)|D_i = 0) - E(Y_i(0)|D_i = 0)
\end{aligned}
\]

Under random assignment and full compliance,

\[
ATE = ATT = ATC
\]

\textbf{Sample average treatment effect on the treated} is

\[
SATT = \frac{1}{n} \sum_i TE_i
\]

where

\begin{itemize}
\item
  \(TE_i\) is the treatment effect for unit \(i\)
\item
  \(n\) is the number of treated units in the sample
\item
  \(i\) belongs the subset (i.e., sample) of the population of interest that is treated.
\end{itemize}

\textbf{Population average treatment effect on the treated} is

\[
PATT = \frac{1}{N} \sum_i TE_i
\]

where

\begin{itemize}
\item
  \(TE_i\) is the treatment effect for unit \(i\)
\item
  \(N\) is the number of treated units in the population
\item
  \(i\) belongs to the population of interest that is treated.
\end{itemize}

\hypertarget{quantile-average-treatment-effects}{%
\subsection{Quantile Average Treatment Effects}\label{quantile-average-treatment-effects}}

Instead of the middle point estimate (ATE), we can also understand the changes in the distribution the outcome variable due to the treatment.

Using quantile regression and more assumptions \citep{abadie2002instrumental, chernozhukov2005iv}, we can have consistent estimate of quantile treatment effects (QTE), with which we can make inference regarding a given quantile.

\hypertarget{mediation-effects}{%
\subsection{Mediation Effects}\label{mediation-effects}}

With additional assumptions (i.e., sequential ignorability \citep{imai2010general, bullock2011mediation}), we can examine the mechanism of the treatment on the outcome.

Under the causal framework,

\begin{itemize}
\item
  the indirect effect of treatment via a mediator is called average causal mediation effect (ACME)
\item
  the direct effect of treatment on outcome is the average direct effect (ADE)
\end{itemize}

More in the \protect\hyperlink{mediation}{Mediation} Section \ref{mediation}

\hypertarget{log-odds-treatment-effects}{%
\subsection{Log-odds Treatment Effects}\label{log-odds-treatment-effects}}

For binary outcome variable, we might be interested in the log-odds of success. See \citep{freedman2008randomization} on how to estimate a consistent causal effect.

Alternatively, attributable effects \citep{rosenbaum2002attributing} can also be appropriate for binary outcome.

\hypertarget{part-a.-experimental-design}{%
\part*{A. EXPERIMENTAL DESIGN}\label{part-a.-experimental-design}}
\addcontentsline{toc}{part}{A. EXPERIMENTAL DESIGN}

\hypertarget{experimental-design}{%
\chapter{Experimental Design}\label{experimental-design}}

\begin{itemize}
\tightlist
\item
  Randomized Control Trials (RCT) or Experiments have always been and are likely to continue in the future to be the holy grail of causal inference, because of

  \begin{itemize}
  \item
    unbiased estimates
  \item
    elimination of confounding factors on average (covariate imbalance is always possible. Hence, you want to do \protect\hyperlink{rerandomization}{Rerandomization} to achieve platinum standard set by \citep{tukey1993tightening})
  \end{itemize}
\item
  RCT means you have two group treatment (or experimental) gorp and control group. Hence, as you introduce the treatment (your exogenous variable) to the treatment group, the only expected difference in the outcomes of the two group should be due to the treatment.
\item
  Subjects from the same population will be \textbf{randomly assigned} to either treatment or control group. This random assignment give us the confidence that changes in the outcome variable will be due only the treatment, not any other source (variable).
\item
  It can be easier for hard science to have RCT because they can introduce the treatment, and have control environments. But it's hard for social scientists because their subjects are usually human, and some treatment can be hard to introduce, or environments are uncontrollable. Hence, social scientists have to develop different tools (\protect\hyperlink{quasi-experimental}{Quasi-experimental}) to recover causal inference or to recreate the treatment and control group environment.
\item
  With RCT, you can easily establish internal validity
\item
  Even though random assignment is not the same thing as \emph{ceteris paribus} (i.e., holding everything else constant), it should have the same effect (i.e., under random manipulation, \emph{other things equal} can be observed, on average, across treatment and control groups).
\end{itemize}

\textbf{Selection Problem}

Assume we have

\begin{itemize}
\item
  binary treatment \(D_i =(0,1)\)
\item
  outcome of interest \(Y_i\) for individual \(i\)

  \begin{itemize}
  \item
    \(Y_{0i}\) are those were \textbf{not treated}
  \item
    \(Y_{1i}\) are those were \textbf{treated}
  \end{itemize}
\end{itemize}

\[
\text{Potential Outcome} =
\begin{cases}
Y_{1i} \text{ if } D_i = 1 \\
Y_{0i} \text{ if } D_i = 0
\end{cases}
\]

Then, what we observe in the outcome variable is

\[
Y_i = Y_{0i} + (Y_{1i} - Y_{0i})D_i
\]

It's likely that \(Y_{1i}\) and \(Y_{0i}\) both have their own distributions (i.e., different treatment effect for different people). Since we can't see both outcomes for the same individual (unless we have a time machine), then we can only make inference regarding the average outcome of those who were treated and those who were not.

\[
\begin{aligned}
E[Y_i | D_i = 1] - E[Y_i | D_i = 0] &= (E[Y_{1i} | D_i = 1] - E[Y_{0i}|D_i = 1] ) + (E[Y_{0i} |D_i = 1] - E[Y_{0i} |D_i = 0]) \\
&= (E[Y_{1i}-Y_{0i}|D_1 = 1] ) + (E[Y_{0i} |D_i = 1] - E[Y_{0i} |D_i = 0]) \\
\text{Observed difference in treatment} &= \text{Average treatment effect on the treated} + \text{Selection bias}
\end{aligned}
\]

\begin{itemize}
\item
  \textbf{The average treatment effect} is the average between between a person who is treated and the same person (in another parallel universe) who is not treated
\item
  \textbf{The selection bias} is the difference between those who were treated and those who weren't treated
\end{itemize}

With \textbf{random assignment} of treatment (\(D_i\)) under \protect\hyperlink{experimental-design}{Experimental Design}, we can have \(D_i\) independent of potential outcomes

\[
\begin{aligned}
E[Y_i | D_i = 1] - E[Y_i|D_i = 0] &= E[Y_{1i}|D_i = 1]-E[Y_{0i}|D_i = 0)]\\
&= E[Y_{1i}|D_i = 1]-E[Y_{0i}|D_i = 0)] && D_i \perp Y_i \\
&= E[Y_{1i} - Y_{0i}|D_i = 1] \\
&= E[Y_{1i} - Y_{0i}]
\end{aligned}
\]

\textbf{Another representation under regression}

Suppose that you know the effect is

\[
Y_{1i} - Y_{0i} = \rho
\]

The observed outcome variable (for an individual) can be rewritten as

\[
\begin{aligned}
Y_i &= E(Y_{0i}) + (Y_{1i}-Y_{0i})D_i + [Y_{0i} - E(Y_{0i})]\\
&= \alpha + \rho D_i + \eta_i
\end{aligned}
\]

where \(\eta_i\) = random variation of \(Y_{0i}\)

Hence, the conditional expectation of an individual outcome on treatment status is

\[
\begin{aligned}
E[Y_i |D_i = 1] &= \alpha + \rho &+ E[\eta_i |D_i = 1] \\
E[Y_i |D_i = 0] &= \alpha &+ E[\eta_i |D_i = 0]
\end{aligned}
\]

Thus,

\[
E[Y_i |D_i = 1] - E[Y_i |D_i = 0] = \rho + E[\eta_i |D_i = 1] -E[\eta_i |D_i = 0]
\]

where \(E[\eta_i |D_i = 1] -E[\eta_i |D_i = 0]\) is the selection bias - correlation between the regression error term (\(\eta_i\)), and the regressor (\(D_i\))

Under regression, we have

\[
E[\eta_i |D_i = 1] -E[\eta_i |D_i = 0] = E[Y_{0i} |D_i = 1] -E[Y_{0i}|D_i = 0]
\]

which is the difference in outcomes between \textbf{those who weren't treated get treated} and \textbf{those who weren't treated stay untreated}

Say you have control variables (\(X_i\)), that is \textbf{uncorrelated with the treatment} (\(D_i\)), then you can include in your model, and it won't (in principle) affect your estimate of the treatment effect (\(\rho\)) with an added benefit of reducing the residual variance, which subsequently reduces the standard error of other estimates.

\[
Y_i = \alpha + \rho D_i + X_i'\gamma + \eta_i
\]

\hypertarget{semi-random-experiment}{%
\section{Semi-random Experiment}\label{semi-random-experiment}}

Chicago Open Enrollment Program \citep{cullen2005impact}

\begin{itemize}
\item
  Students can apply to ``choice'' schools
\item
  Many schools are oversubscribed (Demand \textgreater{} Supply)
\item
  Resolve scarcity via random lotteries
\item
  Non-random enrollment, we only have random lottery which mean the above
\end{itemize}

Let

\[
\delta_j = E(Y_i | Enroll_{ij} = 1; Apply_{ij} = 1) - E(Y_i | Enroll_{ij} = 0; Apply_{ij} = 1)
\]

and

\[
\theta_j = E(Y_i | Win_{ij} = 1; Apply_{ij} = 1) - E(Y_i | Win_{ij} = 0; Apply_{ij} = 1)
\]

Hence, we can clearly see that \(\delta_j \neq \theta_j\) because you can only enroll, but you cannot ensure that you will win. Thus, \textbf{intention to treat is different from treatment effect}.

Non-random enrollment, we only have random lottery which means we can only estimate \(\theta_j\)

To recover the true treatment effect, we can use

\[
\delta_j = \frac{E(Y_i|W_{ij} = 1; A_{ij} = 1) - E(Y_i | W_{ij}=0; A_{ij} = 1)}{P(Enroll_{ij} = 1| W_{ij}= 1; A_{ij}=1) - P(Enroll_{ij} = 1| W_{ij}=0; A_{ij}=1)}
\]

where

\begin{itemize}
\item
  \(\delta_j\) = treatment effect
\item
  \(W\) = Whether students win the lottery
\item
  \(A\) = Whether student apply for the lottery
\item
  i = application
\item
  j = school
\end{itemize}

Say that we have

\textbf{10 win}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Number students
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Selection effect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Treatment effect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Total effect
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & Always Takers & +0.2 & +1 & +1.2 \\
2 & Compliers & 0 & +1 & +1 \\
7 & Never Takers & -0.1 & 0 & -0.1 \\
\end{longtable}

\textbf{10 lose}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Number students
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Selection effect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Treatment effect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Total effect
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & Always Takers & +0.2 & +1 & +1.2 \\
2 & Compliers & 0 & 0 & 0 \\
7 & Never Takers & -0.1 & 0 & -0.1 \\
\end{longtable}

Intent to treatment = Average effect of who you give option to choose

\[
\begin{aligned}
E(Y_i | W_{ij}=1; A_{ij} = 1) &= \frac{1*(1.2)+ 2*(1) + 7 * (-0.1)}{10}\\
&= 0.25
\end{aligned}
\]

\[
\begin{aligned}
E(Y_i | W_{ij}=0; A_{ij} = 1) &= \frac{1*(1.2)+ 2*(0) + 7 * (-0.1)}{10}\\
&= 0.05
\end{aligned}
\]

Hence,

\[
\begin{aligned}
\text{Intent to treatment} &= 0.25 - 0.05 = 0.2 \\
\text{Treatment effect} &= 1
\end{aligned}
\]

\[
\begin{aligned}
P(Enroll_{ij} = 1 | W_{ij} = 1; A_{ij}=1 ) &= \frac{1+2}{10} = 0.3 \\
P(Enroll_{ij} = 1 | W_{ij} = 0; A_{ij}=1 ) &= \frac{1}{10} = 0.1
\end{aligned}
\]

\[
\text{Treatment effect} = \frac{0.2}{0.3-0.1} = 1
\]

After knowing how to recover the treatment effect, we turn our attention to the main model

\[
Y_{ia} = \delta W_{ia} + \lambda L_{ia} + e_{ia}
\]

where

\begin{itemize}
\item
  \(W\) = whether a student wins a lottery
\item
  \(L\) = whether student enrolls in the lottery
\item
  \(\delta\) = intent to treat
\end{itemize}

Hence,

\begin{itemize}
\item
  Conditional on lottery, the \(\delta\) is valid
\item
  But without lottery, your \(\delta\) is not random
\item
  Winning and losing are only identified within lottery
\item
  Each lottery has multiple entries. Thus, we can have within estimator
\end{itemize}

We can also include other control variables (\(X_i \theta\))

\[
Y_{ia} = \delta_1 W_{ia} + \lambda_1 L_{ia} + X_i \theta + u_{ia}
\]

\[
\begin{aligned}
E(\delta) &= E(\delta_1) \\
E(\lambda) &\neq E(\lambda_1) && \text{because choosing a lottery is not random}
\end{aligned}
\]

Including \((X_i \theta)\) just shifts around control variables (i.e., reweighting of lottery), which would not affect your treatment effect \(E(\delta)\)

\hypertarget{rerandomization}{%
\section{Rerandomization}\label{rerandomization}}

\begin{itemize}
\item
  Since randomization only balances baseline covariates on average, imbalance in variables due to random chance can still happen.
\item
  In case that you have a ``bad'' randomization (i.e., imbalance for important baseline covariates), \citep{morgan2012rerandomization} introduce the idea of rerandomization.
\item
  Rerandomization is checking balance during the randomization process (before the experiment), to eliminate bad allocation (i.e., those with unacceptable balance).
\item
  The greater the number of variables, the greater the likelihood that at least one covariate would be imbalanced across treatment groups.

  \begin{itemize}
  \tightlist
  \item
    Example: For 10 covariates, the probability of a significant difference at \(\alpha = .05\) for at least one covariate is \(1 - (1-.05)^{10} = 0.4 = 40\%\)
  \end{itemize}
\item
  Rerandomization increase treatment effect estimate precision if the covariates are correlated with the outcome.

  \begin{itemize}
  \tightlist
  \item
    Improvement in precision for treatment effect estimate depends on (1) improvement in covariate balance and (2) correlation between covariates and the outcome.
  \end{itemize}
\item
  You also need to take into account rerandomization into your analysis when making inference.
\item
  Rerandomization is equivalent to increasing our sample size.
\item
  Alternatives include

  \begin{itemize}
  \item
    Stratified randomization \citep{johansson2022rerandomization}
  \item
    Matched randomization \citep{greevy2004optimal, kapelner2014matching}
  \item
    Minimization \citep{pocock1975sequential}
  \end{itemize}
\end{itemize}

\href{https://healthpolicy.usc.edu/evidence-base/rerandomization-what-is-it-and-why-should-you-use-it-for-random-assignment/}{\includegraphics[width=6.25in,height=5.20833in]{images/The-Randomization-Procedure.png}}

Rerandomization Criterion

\begin{itemize}
\tightlist
\item
  Acceptable randomization is based on a function of covariate matrix \(\mathbf{X}\) and vector of treatment assignments \(\mathbf{W}\)
\end{itemize}

\[
W_i = 
\begin{cases}
1 \text{ if treated} \\
0 \text{ if control} 
\end{cases}
\]

\begin{itemize}
\tightlist
\item
  Mahalanobis Distance, \(M\), can be used as criteria for acceptable balance
\end{itemize}

Let \(M\) be the multivariate distance between groups means

\[
\begin{aligned}
M &= (\bar{\mathbf{X}}_T - \bar{\mathbf{X}}_C)' cov(\bar{\mathbf{X}}_T - \bar{\mathbf{X}}_C)^{-1} (\bar{\mathbf{X}}_T - \bar{\mathbf{X}}_C) \\
&= (\frac{1}{n_T}+ \frac{1}{n_C})^{-1}(\bar{\mathbf{X}}_T - \bar{\mathbf{X}}_C)' cov(\mathbf{X})^{-1}(\bar{\mathbf{X}}_T - \bar{\mathbf{X}}_C)
\end{aligned}
\]

With large sample size and ``pure'' randomization \(M \sim \chi^2_k\) where \(k\) is the number of covariates to be balanced

Then let \(p_a\) be the probability of accepting a randomization. Choosing appropriate \(p_a\) is a tradeoff between balance and time.

Then the rule of thumb is re-randomize when \(M > a\)

\hypertarget{two-stage-randomized-experiments-with-interference-and-noncompliance}{%
\section{Two-Stage Randomized Experiments with Interference and Noncompliance}\label{two-stage-randomized-experiments-with-interference-and-noncompliance}}

\citep{imai2021causal}

\hypertarget{sampling}{%
\chapter{Sampling}\label{sampling}}

\hypertarget{simple-sampling}{%
\section{Simple Sampling}\label{simple-sampling}}

Simple (random) Sampling

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr)}
\NormalTok{iris\_df }\OtherTok{\textless{}{-}}\NormalTok{ iris}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\FunctionTok{sample\_n}\NormalTok{(iris\_df, }\DecValTok{10}\NormalTok{)}
\CommentTok{\#\textgreater{}    Sepal.Length Sepal.Width Petal.Length Petal.Width    Species}
\CommentTok{\#\textgreater{} 1           5.8         2.7          4.1         1.0 versicolor}
\CommentTok{\#\textgreater{} 2           6.4         2.8          5.6         2.1  virginica}
\CommentTok{\#\textgreater{} 3           4.4         3.2          1.3         0.2     setosa}
\CommentTok{\#\textgreater{} 4           4.3         3.0          1.1         0.1     setosa}
\CommentTok{\#\textgreater{} 5           7.0         3.2          4.7         1.4 versicolor}
\CommentTok{\#\textgreater{} 6           5.4         3.0          4.5         1.5 versicolor}
\CommentTok{\#\textgreater{} 7           5.4         3.4          1.7         0.2     setosa}
\CommentTok{\#\textgreater{} 8           7.6         3.0          6.6         2.1  virginica}
\CommentTok{\#\textgreater{} 9           6.1         2.8          4.7         1.2 versicolor}
\CommentTok{\#\textgreater{} 10          4.6         3.4          1.4         0.3     setosa}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(sampling)}
\CommentTok{\# set unique id number for each row}
\NormalTok{iris\_df}\SpecialCharTok{$}\NormalTok{id }\OtherTok{=} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(iris\_df)}

\CommentTok{\# Simple random sampling with replacement}
\FunctionTok{srswor}\NormalTok{(}\DecValTok{10}\NormalTok{, }\FunctionTok{length}\NormalTok{(iris\_df}\SpecialCharTok{$}\NormalTok{id))}
\CommentTok{\#\textgreater{}   [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1}
\CommentTok{\#\textgreater{}  [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0}
\CommentTok{\#\textgreater{}  [75] 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0}
\CommentTok{\#\textgreater{} [112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0}
\CommentTok{\#\textgreater{} [149] 0 0}

\CommentTok{\# Simple random sampling without replacement (sequential method)}
\FunctionTok{srswor1}\NormalTok{(}\DecValTok{10}\NormalTok{, }\FunctionTok{length}\NormalTok{(iris\_df}\SpecialCharTok{$}\NormalTok{id))}
\CommentTok{\#\textgreater{}   [1] 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0}
\CommentTok{\#\textgreater{}  [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0}
\CommentTok{\#\textgreater{}  [75] 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0}
\CommentTok{\#\textgreater{} [112] 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0}
\CommentTok{\#\textgreater{} [149] 0 0}

\CommentTok{\# Simple random sampling with replacement}
\FunctionTok{srswr}\NormalTok{(}\DecValTok{10}\NormalTok{, }\FunctionTok{length}\NormalTok{(iris\_df}\SpecialCharTok{$}\NormalTok{id))}
\CommentTok{\#\textgreater{}   [1] 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0}
\CommentTok{\#\textgreater{}  [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0}
\CommentTok{\#\textgreater{}  [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0}
\CommentTok{\#\textgreater{} [112] 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0}
\CommentTok{\#\textgreater{} [149] 0 0}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(survey)}
\FunctionTok{data}\NormalTok{(}\StringTok{"api"}\NormalTok{)}
\NormalTok{srs\_design }\OtherTok{\textless{}{-}} \FunctionTok{svydesign}\NormalTok{(}\AttributeTok{data =}\NormalTok{ apistrat,}
                        \AttributeTok{weights =} \SpecialCharTok{\textasciitilde{}}\NormalTok{pw, }
                        \AttributeTok{fpc =} \SpecialCharTok{\textasciitilde{}}\NormalTok{fpc, }
                        \AttributeTok{id =} \SpecialCharTok{\textasciitilde{}}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(sampler)}
\FunctionTok{rsamp}\NormalTok{(albania,}
      \AttributeTok{n =} \DecValTok{260}\NormalTok{,}
      \AttributeTok{over =} \FloatTok{0.1}\NormalTok{, }\CommentTok{\# desired oversampling proportion}
      \AttributeTok{rep =}\NormalTok{ F)}
\end{Highlighting}
\end{Shaded}

Identify missing points between sample and collected data

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alsample }\OtherTok{\textless{}{-}} \FunctionTok{rsamp}\NormalTok{(}\AttributeTok{df =}\NormalTok{ albania, }\DecValTok{544}\NormalTok{)}
\NormalTok{alreceived }\OtherTok{\textless{}{-}} \FunctionTok{rsamp}\NormalTok{(}\AttributeTok{df =}\NormalTok{ alsample, }\DecValTok{390}\NormalTok{)}
\FunctionTok{rmissing}\NormalTok{(}\AttributeTok{sampdf =}\NormalTok{ alsample,}
         \AttributeTok{colldf =}\NormalTok{ alreceived,}
         \AttributeTok{col\_name =}\NormalTok{ qvKod)}
\end{Highlighting}
\end{Shaded}

\hypertarget{stratified-sampling}{%
\section{Stratified Sampling}\label{stratified-sampling}}

A stratum is a subset of the population that has at least one common characteristic.

Steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Identify relevant stratums and their representation in the population.
\item
  Randomly sample to select a sufficient number of subjects from each stratum.
\end{enumerate}

Stratified sampling reduces sampling error.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr)}
\CommentTok{\# by number of rows}
\NormalTok{sample\_iris }\OtherTok{\textless{}{-}}\NormalTok{ iris }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{group\_by}\NormalTok{(Species) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{sample\_n}\NormalTok{(}\DecValTok{5}\NormalTok{)}
\NormalTok{sample\_iris}
\CommentTok{\#\textgreater{} \# A tibble: 15 x 5}
\CommentTok{\#\textgreater{} \# Groups:   Species [3]}
\CommentTok{\#\textgreater{}    Sepal.Length Sepal.Width Petal.Length Petal.Width Species   }
\CommentTok{\#\textgreater{}           \textless{}dbl\textgreater{}       \textless{}dbl\textgreater{}        \textless{}dbl\textgreater{}       \textless{}dbl\textgreater{} \textless{}fct\textgreater{}     }
\CommentTok{\#\textgreater{}  1          4.4         3            1.3         0.2 setosa    }
\CommentTok{\#\textgreater{}  2          5.2         3.5          1.5         0.2 setosa    }
\CommentTok{\#\textgreater{}  3          5.1         3.8          1.5         0.3 setosa    }
\CommentTok{\#\textgreater{}  4          5.2         3.4          1.4         0.2 setosa    }
\CommentTok{\#\textgreater{}  5          4.5         2.3          1.3         0.3 setosa    }
\CommentTok{\#\textgreater{}  6          5.5         2.5          4           1.3 versicolor}
\CommentTok{\#\textgreater{}  7          7           3.2          4.7         1.4 versicolor}
\CommentTok{\#\textgreater{}  8          6.7         3            5           1.7 versicolor}
\CommentTok{\#\textgreater{}  9          6.1         2.9          4.7         1.4 versicolor}
\CommentTok{\#\textgreater{} 10          5.5         2.4          3.8         1.1 versicolor}
\CommentTok{\#\textgreater{} 11          6.4         2.7          5.3         1.9 virginica }
\CommentTok{\#\textgreater{} 12          6.4         2.8          5.6         2.1 virginica }
\CommentTok{\#\textgreater{} 13          6.4         3.2          5.3         2.3 virginica }
\CommentTok{\#\textgreater{} 14          6.8         3.2          5.9         2.3 virginica }
\CommentTok{\#\textgreater{} 15          7.2         3.6          6.1         2.5 virginica}

\CommentTok{\# by fraction}
\NormalTok{sample\_iris }\OtherTok{\textless{}{-}}\NormalTok{ iris }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{group\_by}\NormalTok{(Species) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{sample\_frac}\NormalTok{(}\AttributeTok{size =}\NormalTok{ .}\DecValTok{15}\NormalTok{)}
\NormalTok{sample\_iris}
\CommentTok{\#\textgreater{} \# A tibble: 24 x 5}
\CommentTok{\#\textgreater{} \# Groups:   Species [3]}
\CommentTok{\#\textgreater{}    Sepal.Length Sepal.Width Petal.Length Petal.Width Species   }
\CommentTok{\#\textgreater{}           \textless{}dbl\textgreater{}       \textless{}dbl\textgreater{}        \textless{}dbl\textgreater{}       \textless{}dbl\textgreater{} \textless{}fct\textgreater{}     }
\CommentTok{\#\textgreater{}  1          5.5         4.2          1.4         0.2 setosa    }
\CommentTok{\#\textgreater{}  2          5           3            1.6         0.2 setosa    }
\CommentTok{\#\textgreater{}  3          5.2         4.1          1.5         0.1 setosa    }
\CommentTok{\#\textgreater{}  4          4.6         3.1          1.5         0.2 setosa    }
\CommentTok{\#\textgreater{}  5          5.1         3.7          1.5         0.4 setosa    }
\CommentTok{\#\textgreater{}  6          4.8         3.4          1.9         0.2 setosa    }
\CommentTok{\#\textgreater{}  7          5.1         3.3          1.7         0.5 setosa    }
\CommentTok{\#\textgreater{}  8          5.5         3.5          1.3         0.2 setosa    }
\CommentTok{\#\textgreater{}  9          5           2.3          3.3         1   versicolor}
\CommentTok{\#\textgreater{} 10          5.6         2.9          3.6         1.3 versicolor}
\CommentTok{\#\textgreater{} \# i 14 more rows}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(sampler)}
\CommentTok{\# Stratified sample using proportional allocation without replacement}
\FunctionTok{ssamp}\NormalTok{(}\AttributeTok{df=}\NormalTok{albania, }\AttributeTok{n=}\DecValTok{360}\NormalTok{, }\AttributeTok{strata=}\NormalTok{qarku, }\AttributeTok{over=}\FloatTok{0.1}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 395 x 45}
\CommentTok{\#\textgreater{}    qarku  Q\_ID bashkia   BAS\_ID zaz   njesiaAdministrative COM\_ID qvKod zgjedhes}
\CommentTok{\#\textgreater{}    \textless{}fct\textgreater{} \textless{}int\textgreater{} \textless{}fct\textgreater{}      \textless{}int\textgreater{} \textless{}fct\textgreater{} \textless{}fct\textgreater{}                 \textless{}int\textgreater{} \textless{}fct\textgreater{}    \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{}  1 Berat     1 Berat         11 ZAZ \textasciitilde{} "Berat "               1101 "\textbackslash{}"3\textasciitilde{}      558}
\CommentTok{\#\textgreater{}  2 Berat     1 Berat         11 ZAZ \textasciitilde{} "Berat "               1101 "\textbackslash{}"3\textasciitilde{}      815}
\CommentTok{\#\textgreater{}  3 Berat     1 Berat         11 ZAZ \textasciitilde{} "Sinje"                1108 "\textbackslash{}"3\textasciitilde{}      419}
\CommentTok{\#\textgreater{}  4 Berat     1 Kucove        13 ZAZ \textasciitilde{} "Lumas"                1104 "\textbackslash{}"3\textasciitilde{}      237}
\CommentTok{\#\textgreater{}  5 Berat     1 Kucove        13 ZAZ \textasciitilde{} "Kucove"               1201 "\textbackslash{}"3\textasciitilde{}      562}
\CommentTok{\#\textgreater{}  6 Berat     1 Skrapar       17 ZAZ \textasciitilde{} "Corovode"             1303 "\textbackslash{}"3\textasciitilde{}      829}
\CommentTok{\#\textgreater{}  7 Berat     1 Berat         11 ZAZ \textasciitilde{} "Roshnik"              1107 "\textbackslash{}"3\textasciitilde{}      410}
\CommentTok{\#\textgreater{}  8 Berat     1 Ura Vajg\textasciitilde{}     19 ZAZ \textasciitilde{} "Ura Vajgurore"        1110 "\textbackslash{}"3\textasciitilde{}      708}
\CommentTok{\#\textgreater{}  9 Berat     1 Kucove        13 ZAZ \textasciitilde{} "Perondi"              1203 "\textbackslash{}"3\textasciitilde{}      835}
\CommentTok{\#\textgreater{} 10 Berat     1 Kucove        13 ZAZ \textasciitilde{} "Kucove"               1201 "\textbackslash{}"3\textasciitilde{}      907}
\CommentTok{\#\textgreater{} \# i 385 more rows}
\CommentTok{\#\textgreater{} \# i 36 more variables: meshkuj \textless{}int\textgreater{}, femra \textless{}int\textgreater{}, totalSeats \textless{}int\textgreater{},}
\CommentTok{\#\textgreater{} \#   vendndodhja \textless{}fct\textgreater{}, ambienti \textless{}fct\textgreater{}, totalVoters \textless{}int\textgreater{}, femVoters \textless{}int\textgreater{},}
\CommentTok{\#\textgreater{} \#   maleVoters \textless{}int\textgreater{}, unusedBallots \textless{}int\textgreater{}, damagedBallots \textless{}int\textgreater{},}
\CommentTok{\#\textgreater{} \#   ballotsCast \textless{}int\textgreater{}, invalidVotes \textless{}int\textgreater{}, validVotes \textless{}int\textgreater{}, lsi \textless{}int\textgreater{},}
\CommentTok{\#\textgreater{} \#   ps \textless{}int\textgreater{}, pkd \textless{}int\textgreater{}, sfida \textless{}int\textgreater{}, pr \textless{}int\textgreater{}, pd \textless{}int\textgreater{}, pbdksh \textless{}int\textgreater{},}
\CommentTok{\#\textgreater{} \#   adk \textless{}int\textgreater{}, psd \textless{}int\textgreater{}, ad \textless{}int\textgreater{}, frd \textless{}int\textgreater{}, pds \textless{}int\textgreater{}, pdiu \textless{}int\textgreater{}, ...}
\end{Highlighting}
\end{Shaded}

Identify number of missing points by strata between sample and collected data

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alsample }\OtherTok{\textless{}{-}} \FunctionTok{rsamp}\NormalTok{(}\AttributeTok{df =}\NormalTok{ albania, }\DecValTok{544}\NormalTok{)}
\NormalTok{alreceived }\OtherTok{\textless{}{-}} \FunctionTok{rsamp}\NormalTok{(}\AttributeTok{df =}\NormalTok{ alsample, }\DecValTok{390}\NormalTok{)}
\FunctionTok{smissing}\NormalTok{(}
    \AttributeTok{sampdf =}\NormalTok{ alsample,}
    \AttributeTok{colldf =}\NormalTok{ alreceived,}
    \AttributeTok{strata =}\NormalTok{ qarku,}
    \AttributeTok{col\_name =}\NormalTok{ qvKod}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{unequal-probability-sampling}{%
\section{Unequal Probability Sampling}\label{unequal-probability-sampling}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{UPbrewer}\NormalTok{()}
\FunctionTok{UPmaxentropy}\NormalTok{()}
\FunctionTok{UPmidzuno}\NormalTok{()}
\FunctionTok{UPmidzunopi2}\NormalTok{()}
\FunctionTok{UPmultinomial}\NormalTok{()}
\FunctionTok{UPpivotal}\NormalTok{()}
\FunctionTok{UPrandompivotal}\NormalTok{()}
\FunctionTok{UPpoisson}\NormalTok{()}
\FunctionTok{UPsampford}\NormalTok{()}
\FunctionTok{UPsystematic}\NormalTok{()}
\FunctionTok{UPrandomsystematic}\NormalTok{()}
\FunctionTok{UPsystematicpi2}\NormalTok{()}
\FunctionTok{UPtille}\NormalTok{()}
\FunctionTok{UPtillepi2}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\hypertarget{balanced-sampling}{%
\section{Balanced Sampling}\label{balanced-sampling}}

\begin{itemize}
\item
  Purpose: to get the same means in the population and the sample for all the auxiliary variables
\item
  Balanced sampling is different from purposive selection
\end{itemize}

Balancing equations

\[
\sum_{k \in S} \frac{\mathbf{x}_k}{\pi_k} = \sum_{k \in U} \mathbf{x}_k
\]

where \(\mathbf{x}_k\) is a vector of auxiliary variables

\hypertarget{cube}{%
\subsection{Cube}\label{cube}}

\begin{itemize}
\item
  flight phase
\item
  landing phase
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{samplecube}\NormalTok{()}
\FunctionTok{fastflightcube}\NormalTok{()}
\FunctionTok{landingcube}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\hypertarget{stratification}{%
\subsection{Stratification}\label{stratification}}

\begin{itemize}
\tightlist
\item
  Try to replicate the population based on the original multivariate histogram
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(survey)}
\FunctionTok{data}\NormalTok{(}\StringTok{"api"}\NormalTok{)}
\NormalTok{srs\_design }\OtherTok{\textless{}{-}} \FunctionTok{svydesign}\NormalTok{(}\AttributeTok{data =}\NormalTok{ apistrat,}
                        \AttributeTok{weights =} \SpecialCharTok{\textasciitilde{}}\NormalTok{pw, }
                        \AttributeTok{fpc =} \SpecialCharTok{\textasciitilde{}}\NormalTok{fpc, }
                        \AttributeTok{strata =} \SpecialCharTok{\textasciitilde{}}\NormalTok{stype,}
                        \AttributeTok{id =} \SpecialCharTok{\textasciitilde{}}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{balancedstratification}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\hypertarget{cluster-1}{%
\subsection{Cluster}\label{cluster-1}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(survey)}
\FunctionTok{data}\NormalTok{(}\StringTok{"api"}\NormalTok{)}
\NormalTok{srs\_design }\OtherTok{\textless{}{-}} \FunctionTok{svydesign}\NormalTok{(}\AttributeTok{data =}\NormalTok{ apiclus1,}
                        \AttributeTok{weights =} \SpecialCharTok{\textasciitilde{}}\NormalTok{pw, }
                        \AttributeTok{fpc =} \SpecialCharTok{\textasciitilde{}}\NormalTok{fpc, }
                        \AttributeTok{id =} \SpecialCharTok{\textasciitilde{}}\NormalTok{dnum)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{balancedcluster}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\hypertarget{two-stage}{%
\subsection{Two-stage}\label{two-stage}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(survey)}
\FunctionTok{data}\NormalTok{(}\StringTok{"api"}\NormalTok{)}
\NormalTok{srs\_design }\OtherTok{\textless{}{-}} \FunctionTok{svydesign}\NormalTok{(}\AttributeTok{data =}\NormalTok{ apiclus2, }
                        \AttributeTok{fpc =} \SpecialCharTok{\textasciitilde{}}\NormalTok{fpc1 }\SpecialCharTok{+}\NormalTok{ fpc2, }
                        \AttributeTok{id =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ dnum }\SpecialCharTok{+}\NormalTok{ snum)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{balancedtwostage}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\hypertarget{analysis-of-variance-anova}{%
\chapter{Analysis of Variance (ANOVA)}\label{analysis-of-variance-anova}}

ANOVA is using the same underlying mechanism as linear regression. However, the angle that ANOVA chooses to look at is slightly different from the traditional linear regression. It can be more useful in the case with \textbf{qualitative variables} and \textbf{designed experiments}.

Experimental Design

\begin{itemize}
\tightlist
\item
  \textbf{Factor}: explanatory or predictor variable to be studied in an investigation
\item
  \textbf{Treatment} (or Factor Level): ``value'' of a factor applied to the experimental unit
\item
  \textbf{Experimental Unit}: person, animal, piece of material, etc. that is subjected to treatment(s) and provides a response
\item
  \textbf{Single Factor Experiment}: one explanatory variable considered
\item
  \textbf{Multifactor Experiment}: more than one explanatory variable
\item
  \textbf{Classification Factor}: A factor that is not under the control of the experimenter (observational data)
\item
  \textbf{Experimental Factor}: assigned by the experimenter
\end{itemize}

Basics of experimental design:

\begin{itemize}
\item
  Choices that a statistician has to make:

  \begin{itemize}
  \tightlist
  \item
    set of treatments
  \item
    set of experimental units
  \item
    treatment assignment (selection bias)
  \item
    measurement (measurement bias, blind experiments)
  \end{itemize}
\item
  Advancements in experimental design:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \item
    \textbf{Factorial Experiments}:\\
    consider multiple factors at the same time (interaction)
  \item
    \textbf{Replication}: repetition of experiment

    \begin{itemize}
    \tightlist
    \item
      assess mean squared error
    \item
      control over precision of experiment (power)
    \end{itemize}
  \item
    \textbf{Randomization}

    \begin{itemize}
    \tightlist
    \item
      Before R.A. Fisher (1900s), treatments were assigned systematically or subjectively
    \item
      randomization: assign treatments to experimental units at random, which averages out systematic effects that cannot be control by the investigator
    \end{itemize}
  \item
    \textbf{Local control}: Blocking or Stratification

    \begin{itemize}
    \tightlist
    \item
      Reduce experimental errors and increase power by placing restrictions on the randomization of treatments to experimental units.
    \end{itemize}
  \end{enumerate}
\end{itemize}

Randomization may also eliminate correlations due to time and space.

\hypertarget{completely-randomized-design-crd}{%
\section{Completely Randomized Design (CRD)}\label{completely-randomized-design-crd}}

Treatment factor A with \(a\ge2\) treatments levels. Experimental units are randomly assigned to each treatment. The number of experimental units in each group can be

\begin{itemize}
\tightlist
\item
  equal (balanced): n
\item
  unequal (unbalanced): \(n_i\) for the i-th group (i = 1,\ldots,a).
\end{itemize}

The total sample size is \(N=\sum_{i=1}^{a}n_i\)

Possible assignments of units to treatments are \(k=\frac{N!}{n_1!n_2!...n_a!}\)

Each has probability 1/k of being selected. Each experimental unit is measured with a response \(Y_{ij}\), in which j denotes unit and i denotes treatment.

Treatment

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1728}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2099}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2099}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1605}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2099}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
1
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
2
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\ldots{}
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
a
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
& \(Y_{11}\) & \(Y_{21}\) & \ldots{} & \(Y_{a1}\) \\
& \(Y_{12}\) & \ldots{} & \ldots{} & \ldots{} \\
& \ldots{} & \ldots{} & \ldots{} & \ldots{} \\
Sample Mean & \(\bar{Y_{1.}}\) & \(\bar{Y_{2.}}\) & \ldots{} & \(\bar{Y_{a.}}\) \\
Sample SD & \(s_1\) & \(s_2\) & \ldots{} & \(s_a\) \\
\end{longtable}

where \(\bar{Y_{i.}}=\frac{1}{n_i}\sum_{j=1}^{n_i}Y_{ij}\)

\(s_i^2=\frac{1}{n_i-1}\sum_{j=1}^{n_i}(Y_{ij}-\bar{Y_i})^2\)

And the grand mean is \(\bar{Y_{..}}=\frac{1}{N}\sum_{i}\sum_{j}Y_{ij}\)

\hypertarget{single-factor-fixed-effects-model}{%
\subsection{Single Factor Fixed Effects Model}\label{single-factor-fixed-effects-model}}

also known as Single Factor (One-Way) ANOVA or ANOVA Type I model.

Partitioning the Variance

The total variability of the \(Y_{ij}\) observation can be measured as the deviation of \(Y_{ij}\) around the overall mean \(\bar{Y_{..}}\): \(Y_{ij} - \bar{Y_{..}}\)

This can be rewritten as:

\[
\begin{aligned}
Y_{ij} - \bar{Y_{..}}&=Y_{ij} - \bar{Y_{..}} + \bar{Y_{i.}} - \bar{Y_{i.}} \\
&= (\bar{Y_{i.}}-\bar{Y_{..}})+(Y_{ij}-\bar{Y_{i.}})
\end{aligned}
\]

where

\begin{itemize}
\tightlist
\item
  the first term is the \emph{between} treatment differences (i.e., the deviation of the treatment mean from the overall mean)
\item
  the second term is \emph{within} treatment differences (i.e., the deviation of the observation around its treatment mean)
\end{itemize}

\[
\begin{aligned}
\sum_{i}\sum_{j}(Y_{ij} - \bar{Y_{..}})^2 &=  \sum_{i}n_i(\bar{Y_{i.}}-\bar{Y_{..}})^2+\sum_{i}\sum_{j}(Y_{ij}-\bar{Y_{i.}})^2 \\
SSTO &= SSTR + SSE \\
total~SS &= treatment~SS + error~SS \\
(N-1)~d.f. &= (a-1)~d.f. + (N - a) ~ d.f.
\end{aligned}
\]

we lose a d.f. for the total corrected SSTO because of the estimation of the mean (\(\sum_{i}\sum_{j}(Y_{ij} - \bar{Y_{..}})=0\))\\
And, for the SSTR \(\sum_{i}n_i(\bar{Y_{i.}}-\bar{Y_{..}})=0\)

Accordingly, \(MSTR= \frac{SST}{a-1}\) and \(MSR=\frac{SSE}{N-a}\)

\textbf{ANOVA Table}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2745}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.4510}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1275}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1275}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Source of Variation
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
SS
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
df
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
MS
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Between Treatments & \(\sum_{i}n_i (\bar{Y_{i.}}-\bar{Y_{..}})^2\) & a-1 & SSTR/(a-1) \\
Error (within treatments) & \(\sum_{i}\sum_{j}(Y_{ij}-\bar{Y_{i.}})^2\) & N-a & SSE/(N-a) \\
Total (corrected) & \(\sum_{i}n_i (\bar{Y_{i.}}-\bar{Y_{..}})^2\) & N-1 & \\
\end{longtable}

Linear Model Explanation of ANOVA

\hypertarget{cell-means-model}{%
\subsubsection{Cell means model}\label{cell-means-model}}

\[
Y_{ij}=\mu_i+\epsilon\_{ij}
\]

where

\begin{itemize}
\item
  \(Y_{ij}\) response variable in \(j\)-th subject for the \(i\)-th treatment
\item
  \(\mu_i\): parameters (fixed) representing the unknown population mean for the i-th treatment
\item
  \(\epsilon_{ij}\) independent \(N(0,\sigma^2)\) errors
\item
  \(E(Y_{ij})=\mu_i\) \(var(Y_{ij})=var(\epsilon_{ij})=\sigma^2\)
\item
  All observations have the same variance
\end{itemize}

Example:

\(a = 3\) (3 treatments) \(n_1=n_2=n_3=2\)

\[
\begin{aligned}
\left(\begin{array}{c} 
Y_{11}\\
Y_{12}\\
Y_{21}\\
Y_{22}\\
Y_{31}\\
Y_{32}\\
\end{array}\right) &= 
\left(\begin{array}{ccc} 
1 & 0 & 0 \\ 
1 & 0 & 0 \\ 
0 & 1 & 0 \\ 
0 & 1 & 0 \\ 
0 & 0 & 1 \\ 
0 & 0 & 1 \\ 
\end{array}\right)
\left(\begin{array}{c}
\mu_1 \\
\mu_2 \\
\mu_3 \\
\end{array}\right) + \left(\begin{array}{c}
\epsilon_{11} \\
\epsilon_{12} \\
\epsilon_{21} \\
\epsilon_{22} \\
\epsilon_{31} \\
\epsilon_{32} \\
\end{array}\right)\\
\mathbf{y} &= \mathbf{X\beta} +\mathbf{\epsilon}
\end{aligned}
\]

\(X_{k,ij}=1\) if the \(k\)-th treatment is used

\(X_{k,ij}=0\) Otherwise

\textbf{Note}: no intercept term.

\begin{equation}
\begin{aligned}
\mathbf{b}= \left[\begin{array}{c}
\mu_1 \\
\mu_2 \\
\mu_3 \\
\end{array}\right] &= 
(\mathbf{x}'\mathbf{x})^{-1}\mathbf{x}'\mathbf{y} \\
& = 
\left[\begin{array}{ccc}
n_1 & 0 & 0\\
0 & n_2 & 0\\
0 & 0 & n_3 \\
\end{array}\right]^{-1}
\left[\begin{array}{c}
Y_1\\
Y_2\\
Y_3\\
\end{array}\right] \\
& = 
\left[\begin{array}{c}
\bar{Y_1}\\
\bar{Y_2}\\
\bar{Y_3}\\
\end{array}\right] 
\end{aligned} 
\label{eq:betaorigin}
\end{equation}

is the BLUE (best linear unbiased estimator) for \(\beta=[\mu_1 \mu_2\mu_3]'\)

\[
E(\mathbf{b})=\beta
\]

\[
var(\mathbf{b})=\sigma^2(\mathbf{X'X})^{-1}=\sigma^2
\left[\begin{array}{ccc}
1/n_1 & 0 & 0\\
0 & 1/n_2 & 0\\
0 & 0 & 1/n_3\\
\end{array}\right]
\]

\(var(b_i)=var(\hat{\mu_i})=\sigma^2/n_i\) where \(\mathbf{b} \sim N(\beta,\sigma^2(\mathbf{X'X})^{-1})\)

\[
\begin{aligned}
MSE &= \frac{1}{N-a} \sum_{i}\sum_{j}(Y_{ij}-\bar{Y_{i.}})^2 \\
    &= \frac{1}{N-a} \sum_{i}[(n_i-1)\frac{\sum_{i}(Y_{ij}-\bar{Y_{i.}})^2}{n_i-1}] \\
    &= \frac{1}{N-a} \sum_{i}(n_i-1)s_1^2
\end{aligned}
\]

We have \(E(s_i^2)=\sigma^2\)

\(E(MSE)=\frac{1}{N-a}\sum_{i}(n_i-1)\sigma^2=\sigma^2\)

Hence, MSE is an unbiased estimator of \(\sigma^2\), regardless of whether the treatment means are equal or not.

\(E(MSTR)=\sigma^2+\frac{\sum_{i}n_i(\mu_i-\mu_.)^2}{a-1}\)\\
where \(\mu_.=\frac{\sum_{i=1}^{a}n_i\mu_i}{\sum_{i=1}^{a}n_i}\)\\
If all treatment means are equals (=\(\mu_.\)), \(E(MSTR)=\sigma^2\).

Then we can use an \(F\)-test for the equality of all treatment means:

\[H_0:\mu_1=\mu_2=..=\mu_a\]

\[H_a: not~al l~ \mu_i ~ are ~ equal \]

\(F=\frac{MSTR}{MSE}\)\\
where large values of F support \(H_a\) (since MSTR will tend to exceed MSE when \(H_a\) holds)\\
and F near 1 support \(H_0\) (upper tail test)

\textbf{Equivalently}, when \(H_0\) is true, \(F \sim f_{(a-1,N-a)}\)

\begin{itemize}
\tightlist
\item
  If \(F \leq f_{(a-1,N-a;1-\alpha)}\), we cannot reject \(H_0\)
\item
  If \(F \geq f_{(a-1,N-a;1-\alpha)}\), we reject \(H_0\)
\end{itemize}

Note: If \(a = 2\) (2 treatments), \(F\)-test = two sample \(t\)-test

\hypertarget{treatment-effects-factor-effects}{%
\subsubsection{Treatment Effects (Factor Effects)}\label{treatment-effects-factor-effects}}

Besides Cell means model, we have another way to formalize one-way ANOVA: \[Y_{ij} = \mu + \tau_i + \epsilon_{ij}\] where

\begin{itemize}
\tightlist
\item
  \(Y_{ij}\) is the \(j\)-th response for the \(i\)-th treatment
\item
  \(\tau_i\) is \(i\)-th treatment effect
\item
  \(\mu\) constant component, common to all observations
\item
  \(\epsilon_{ij}\) independent random errors \textasciitilde{} \(N(0,\sigma^2)\)
\end{itemize}

For example, \(a = 3\), \(n_1=n_2=n_3=2\)

\begin{equation} 
\begin{aligned}
\left(\begin{array}{c} 
Y_{11}\\
Y_{12}\\
Y_{21}\\
Y_{22}\\
Y_{31}\\
Y_{32}\\
\end{array}\right) &= 
\left(\begin{array}{cccc} 
1 & 1 & 0 & 0 \\ 
1 & 1 & 0 & 0 \\ 
1 & 0 & 1 & 0 \\ 
1 & 0 & 1 & 0 \\ 
1 & 0 & 0 & 1 \\ 
1 & 0 & 0 & 1 \\ 
\end{array}\right)
\left(\begin{array}{c}
\mu \\
\tau_1 \\
\tau_2 \\
\tau_3\\
\end{array}\right) + \left(\begin{array}{c}
\epsilon_{11} \\
\epsilon_{12} \\
\epsilon_{21} \\
\epsilon_{22} \\
\epsilon_{31} \\
\epsilon_{32} \\
\end{array}\right)\\
\mathbf{y} &= \mathbf{X\beta} +\mathbf{\epsilon} 
\end{aligned}
\label{eq:unsolvable}
\end{equation}

However,

\[
\mathbf{X'X} = 
\left(
\begin{array}
{cccc}
\sum_{i}n_i & n_1 & n_2 & n_3 \\
n_1 & n_1 & 0 & 0 \\
n_2 & 0 & n_2 & 0 \\
n_3 & 0 & 0 & n_3 \\
\end{array}
\right)
\]

is \textbf{singular} thus does not exist, \(\mathbf{b}\) is insolvable (infinite solutions)

Hence, we have to impose restrictions on the parameters to a model matrix \(\mathbf{X}\) of full rank.

Whatever restriction we use, we still have:

\(E(Y_{ij})=\mu + \tau_i = \mu_i = mean ~ response ~ for ~ i-th ~ treatment\)

\hypertarget{restriction-on-sum-of-tau}{%
\paragraph{Restriction on sum of tau}\label{restriction-on-sum-of-tau}}

\(\sum_{i=1}^{a}\tau_i=0\)

implies

\[
\mu= \mu +\frac{1}{a}\sum_{i=1}^{a}(\mu+\tau_i)
\]

is the average of the treatment mean (grand mean) (overall mean)

\[
\begin{aligned}
\tau_i  &=(\mu+\tau_i) -\mu = \mu_i-\mu \\
        &= \text{treatment  mean} - \text{grand~mean} \\
        &= \text{treatment  effect}
\end{aligned}
\]

\[
\tau_a=-\tau_1-\tau_2-...-\tau_{a-1}
\]

Hence, the mean for the a-th treatment is

\[
\mu_a=\mu+\tau_a=\mu-\tau_1-\tau_2-...-\tau_{a-1}
\]

Hence, the model need only ``a'' parameters:

\[
\mu,\tau_1,\tau_2,..,\tau_{a-1}
\]

Equation \eqref{eq:unsolvable} becomes

\begin{equation}
\begin{aligned}
\left(\begin{array}{c} 
Y_{11}\\
Y_{12}\\
Y_{21}\\
Y_{22}\\
Y_{31}\\
Y_{32}\\
\end{array}\right) &= 
\left(\begin{array}{ccc} 
1 & 1 & 0 \\ 
1 & 1 & 0 \\ 
1 & 0 & 1 \\ 
1 & 0 & 1 \\ 
1 & -1 & -1 \\ 
1 & -1 & -1 \\ 
\end{array}\right)
\left(\begin{array}{c}
\mu \\
\tau_1 \\
\tau_2 \\
\end{array}\right) + \left(\begin{array}{c}
\epsilon_{11} \\
\epsilon_{12} \\
\epsilon_{21} \\
\epsilon_{22} \\
\epsilon_{31} \\
\epsilon_{32} \\
\end{array}\right)\\
\mathbf{y} &= \mathbf{X\beta} +\mathbf{\epsilon}
\end{aligned}
\end{equation}

where \(\beta\equiv[\mu,\tau_1,\tau_2]'\)

Equation \eqref{eq:betaorigin} with \(\sum_{i}\tau_i=0\) becomes

\[
\begin{aligned}
\mathbf{b}= \left[\begin{array}{c}
\hat{\mu} \\
\hat{\tau_1} \\
\hat{\tau_2} \\
\end{array}\right] &= 
(\mathbf{x}'\mathbf{x})^{-1}\mathbf{x}'\mathbf{y} \\
& = 
\left[\begin{array}{ccc}
\sum_{i}n_i & n_1-n_3 & n_2-n_3\\
n_1-n_3 & n_1+n_3 & n_3\\
n_2-n_3 & n_3 & n_2-n_3 \\
\end{array}\right]^{-1}
\left[\begin{array}{c}
Y_{..}\\
Y_{1.}-Y_{3.}\\
Y_{2.}-Y_{3.}\\
\end{array}\right] \\
& =
\left[\begin{array}{c}
\frac{1}{3}\sum_{i=1}^{3}\bar{Y_{i.}}\\
\bar{Y_{1.}}-\frac{1}{3}\sum_{i=1}^{3}\bar{Y_{i.}}\\
\bar{Y_{2.}}-\frac{1}{3}\sum_{i=1}^{3}\bar{Y_{i.}}\\
\end{array}\right]\\
& = 
\left[\begin{array}{c}
\hat{\mu}\\
\hat{\tau_1}\\
\hat{\tau_2}\\
\end{array}\right]
\end{aligned}
\]

and \(\hat{\tau_3}=-\hat{\tau_1}-\hat{\tau_2}=\bar{Y_3}-\frac{1}{3} \sum_{i}\bar{Y_{i.}}\)

\hypertarget{restriction-on-first-tau}{%
\paragraph{Restriction on first tau}\label{restriction-on-first-tau}}

In R, \texttt{lm()} uses the restriction \(\tau_1=0\)

For the previous example, for \(n_1=n_2=n_3=2\), and \(\tau_1=0\).

Then the treatment means can be written as:

\[
\begin{aligned}
\mu_1 &= \mu + \tau_1 = \mu + 0 = \mu  \\
\mu_2 &= \mu + \tau_2 \\
\mu_3 &= \mu + \tau_3
\end{aligned}
\]

Hence, \(\mu\) is the mean response for the first treatment

In the matrix form,

\[
\begin{aligned}
\left(\begin{array}{c} 
Y_{11}\\
Y_{12}\\
Y_{21}\\
Y_{22}\\
Y_{31}\\
Y_{32}\\
\end{array}\right) &= 
\left(\begin{array}{ccc} 
1 & 0 & 0 \\ 
1 & 0 & 0 \\ 
1 & 1 & 0 \\ 
1 & 1 & 0 \\ 
1 & 0 & 1 \\ 
1 & 0 & 1 \\ 
\end{array}\right)
\left(\begin{array}{c}
\mu \\
\tau_2 \\
\tau_3 \\
\end{array}\right) + \left(\begin{array}{c}
\epsilon_{11} \\
\epsilon_{12} \\
\epsilon_{21} \\
\epsilon_{22} \\
\epsilon_{31} \\
\epsilon_{32} \\
\end{array}\right)\\
\mathbf{y} &= \mathbf{X\beta} +\mathbf{\epsilon}
\end{aligned}
\]

\(\beta = [\mu,\tau_2,\tau_3]'\)

\[
\begin{aligned}
\mathbf{b}= \left[\begin{array}{c}
\hat{\mu} \\
\hat{\tau_2} \\
\hat{\tau_3} \\
\end{array}\right] &= 
(\mathbf{x}'\mathbf{x})^{-1}\mathbf{x}'\mathbf{y} \\
& = 
\left[\begin{array}{ccc}
\sum_{i}n_i & n_2 & n_3\\
n_2 & n_2 & 0\\
n_3 & 0 & n_3 \\
\end{array}\right]^{-1}
\left[\begin{array}{c}
Y_{..}\\
Y_{2.}\\
Y_{3.}\\
\end{array}\right] \\
& = 
\left[
\begin{array}{c}
\bar{Y_{1.}} \\
\bar{Y_{2.}} - \bar{Y_{1.}} \\
\bar{Y_{3.}} - \bar{Y_{1.}}\\
\end{array}\right]
\end{aligned}
\]

\[
E(\mathbf{b})= \beta = 
\left[\begin{array}{c}
{\mu}\\
{\tau_2}\\
{\tau_3}\\
\end{array}\right]
=
\left[\begin{array}{c}
\mu_1\\
\mu_2-\mu_1\\
\mu_3-\mu_1\\
\end{array}\right]
\]

\[
\begin{aligned}
var(\mathbf{b}) &= \sigma^2(\mathbf{X'X})^{-1} \\
var(\hat{\mu}) &= var(\bar{Y_{1.}})=\sigma^2/n_1 \\
var(\hat{\tau_2}) &= var(\bar{Y_{2.}}-\bar{Y_{1.}}) = \sigma^2/n_2 + \sigma^2/n_1 \\
var(\hat{\tau_3}) &= var(\bar{Y_{3.}}-\bar{Y_{1.}}) = \sigma^2/n_3 + \sigma^2/n_1
\end{aligned}
\]

\textbf{Note} For all three parameterization, the ANOVA table is the same

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{cell-means-model-1}{Model 1}: \(Y_{ij} = \mu_i + \epsilon_{ij}\)
\item
  \protect\hyperlink{restriction-on-sum-of-tau}{Model 2}: \(Y_{ij} = \mu + \tau_i + \epsilon_{ij}\) where \(\sum_{i} \tau_i=0\)
\item
  \protect\hyperlink{restriction-on-first-tau}{Model 3}: \(Y_{ij}= \mu + \tau_i + \epsilon_{ij}\) where \(\tau_1=0\)
\end{itemize}

All models have the same calculation for \(\hat{Y}\) as

\[
\mathbf{\hat{Y} = X(X'X)^{-1}X'Y=PY = Xb}
\]

\textbf{ANOVA Table}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1410}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.5064}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.0641}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1346}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1346}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Source of Variation
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
SS
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
df
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
MS
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
F
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Between Treatments & \(\sum_{i} n _ i (\bar { Y_ {i .} } -\bar{Y_{..}})^2 = \mathbf{Y ' (P-P_1)Y}\) & a-1 & \(\frac{SSTR}{a-1}\) & \(\frac{MSTR}{MSE}\) \\
Error

(within treatments) & \(\sum_{i}\sum_{j}(Y_{ij} -\bar{Y_{i.}})^2=\mathbf{e'e}\) & N-a & \(\frac{SSE}{N-a}\) & \\
Total (corrected) & \(\sum_{i } n_i(\bar{Y_{i.}}-\bar{Y_{..}})^2=\mathbf{Y'Y - Y'P_1Y}\) & N-1 & & \\
\end{longtable}

where \(\mathbf{P_1} = \frac{1}{n}\mathbf{J}\)

The \(F\)-statistic here has \((a-1,N-a)\) degrees of freedom, which gives the same value for all three parameterization, but the hypothesis test is written a bit different:

\[
\begin{aligned}
&H_0 : \mu_1 = \mu_2 = ... = \mu_a \\
&H_0 : \mu + \tau_1 = \mu + \tau_2 = ... = \mu + \tau_a \\
&H_0 : \tau_1 = \tau_2 = ...= \tau_a 
\end{aligned}
\]

The \(F\)-test here serves as a preliminary analysis, to see if there is any difference at different factors. For more in-depth analysis, we consider different testing of treatment effects.

\hypertarget{testing-of-treatment-effects}{%
\subsubsection{Testing of Treatment Effects}\label{testing-of-treatment-effects}}

\begin{itemize}
\tightlist
\item
  A \protect\hyperlink{single-treatment-mean}{Single Treatment Mean} \(\mu_i\)
\item
  A \protect\hyperlink{differences-between-treatment-means}{Differences Between Treatment Means}
\item
  A \protect\hyperlink{contrast-among-treatment-means}{Contrast Among Treatment Means}
\item
  A \protect\hyperlink{linear-combination-of-treatment-means}{Linear Combination of Treatment Means}
\end{itemize}

\hypertarget{single-treatment-mean}{%
\paragraph{Single Treatment Mean}\label{single-treatment-mean}}

We have \(\hat{\mu_i}=\bar{Y_{i.}}\) where

\begin{itemize}
\tightlist
\item
  \(E(\bar{Y_{i.}})=\mu_i\)
\item
  \(var(\bar{Y_{i}})=\sigma^2/n_i\) estimated by \(s^2(\bar{Y_{i.}})=MSE / n_i\)
\end{itemize}

Since \(\frac{\bar{Y_{i.}}-\mu_i}{s(\bar{Y_{i.}})} \sim t_{N-a}\) and the confidence interval for \(\mu_i\) is \(\bar{Y_{i.}} \pm t_{1-\alpha/2;N-a}s(\bar{Y_{i.}})\),\\
then we can do a t-test for the means difference with some constant \(c\)

\[
\begin{aligned}
&H_0: \mu_i = c \\
&H_1: \mu_i \neq c
\end{aligned}
\]

where

\[
T =\frac{\bar{Y_{i.}}-c}{s(\bar{Y_{i.}})}
\]

follows \(t_{N-a}\) when \(H_0\) is true.\\
If \(|T| > t_{1-\alpha/2;N-a}\), we can reject \(H_0\)

\hypertarget{differences-between-treatment-means}{%
\paragraph{Differences Between Treatment Means}\label{differences-between-treatment-means}}

Let \(D=\mu_i - \mu_i'\), also known as \textbf{pairwise comparison}\\
\(D\) can be estimated by \(\hat{D}=\bar{Y_{i}}-\bar{Y_{i}}'\) is unbiased (\(E(\hat{D})=\mu_i-\mu_i'\))

Since \(\bar{Y_{i}}\) and \(\bar{Y_{i}}'\) are independent, then

\[
var(\hat{D})=var(\bar{Y_{i}}) + var(\bar{Y_{i'}}) = \sigma^2(1/n_i + 1/n_i')
\]

can be estimated with

\[
s^2(\hat{D}) = MSE(1/n_i + 1/n_i')
\]

With the single treatment inference,

\[
\frac{\hat{D}-D}{s(\hat{D})} \sim t_{N-a}
\]

hence,

\[
\hat{D} \pm t_{(1-\alpha/2;N-a)}s(\hat{D})
\]

Hypothesis tests:

\[
\begin{aligned}
&H_0: \mu_i = \mu_i' \\
&H_a: \mu_i \neq \mu_i'
\end{aligned}
\]

can be tested by the following statistic

\[
T = \frac{\hat{D}}{s(\hat{D})} \sim t_{1-\alpha/2;N-a}
\]

reject \(H_0\) if \(|T| > t_{1-\alpha/2;N-a}\)

\hypertarget{contrast-among-treatment-means}{%
\paragraph{Contrast Among Treatment Means}\label{contrast-among-treatment-means}}

generalize the comparison of two means, we have \textbf{contrasts}

A contrast is a linear combination of treatment means:

\[
L = \sum_{i=1}^{a}c_i \mu_i
\]

where each \(c_i\) is non-random constant and sum to 0:

\[
\sum_{i=1}^{a} c_i = 0
\]

An unbiased estimator of a contrast L is

\[
\hat{L} = \sum_{i=1}^{a}c_i \bar{Y}_{i.}
\]

and \(E(\hat{L}) = L\). Since the \(\bar{Y}_{i.}\), i = 1,\ldots, a are independent.

\[
\begin{aligned}
var(\hat{L}) &= var(\sum_{i=1}^a c_i \bar{Y}_{i.}) = \sum_{i=1}^a var(c_i \bar{Y}_i)  \\
&= \sum_{i=1}^a c_i^2 var(\bar{Y}_i) = \sum_{i=1}^a c_i^2 \sigma^2 /n_i \\
&= \sigma^2 \sum_{i=1}^{a} c_i^2 /n_i
\end{aligned}
\]

Estimation of the variance:

\[
s^2(\hat{L}) = MSE \sum_{i=1}^{a} \frac{c_i^2}{n_i}
\]

\(\hat{L}\) is normally distributed (since it is a linear combination of independent normal random variables).

Then, since \(SSE/\sigma^2\) is \(\chi_{N-a}^2\)

\[
\frac{\hat{L}-L}{s(\hat{L})} \sim t_{N-a}
\]

A \(1-\alpha\) confidence limits are given by

\[
\hat{L} \pm t_{1-\alpha/2; N-a}s(\hat{L})
\]

Hypothesis testing

\[
\begin{aligned}
&H_0: L = 0 \\
&H_a: L \neq 0
\end{aligned}
\]

with

\[
T = \frac{\hat{L}}{s(\hat{L})}
\]

reject \(H_0\) if \(|T| > t_{1-\alpha/2;N-a}\)

\hypertarget{linear-combination-of-treatment-means}{%
\paragraph{Linear Combination of Treatment Means}\label{linear-combination-of-treatment-means}}

just like contrast \(L = \sum_{i=1}^a c_i \mu_i\) but no restrictions on the \(c_i\) coefficients.

Tests on a single treatment mean, two treatment means, and contrasts can all be considered form the same perspective.

\[
\begin{aligned}
&H_0: \sum c_i \mu_i = c \\
&H_a: \sum c_i \mu_i \neq c 
\end{aligned}
\]

The test statistics ( \(t\)-stat) can be considered equivalently as \(F\)-tests; \(F = (T)^2\) where \(F \sim F_{1,N-a}\). Since the numerator degrees of freedom is always 1 in these cases, we refer to them as single-degree-of-freedom tests.

\textbf{Multiple Contrasts}

To test simultaneously \(k \ge 2\) contrasts, let \(T_1,...,T_k\) be the t-stat. The joint distribution of these random variables is a multivariate t-distribution (the tests are dependent since they re based on the same data).

Limitations for comparing multiple contrasts:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The confidence coefficient \(1-\alpha\) only applies to a particular estimate, not a series of estimates; similarly, the Type I error rate, \(\alpha\), applies to a particular test, not a series of tests. Example: 3 \(t\)-tests at \(\alpha = 0.05\), if tests are independent (which they are not), \(0.95^3 = 0.857\) (thus \(\alpha - 0.143\) not 0.05)\\
\item
  The confidence coefficient \(1-\alpha\) and significance level \(\alpha\) are appropriate only if the test was not suggest by the data.

  \begin{itemize}
  \tightlist
  \item
    often, the results of an experiment suggest important (i.e.,..g, potential significant) relationships.
  \item
    the process of studying effects suggests by the data is called \textbf{data snooping}
  \end{itemize}
\end{enumerate}

Multiple Comparison Procedures:

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{tukey}{Tukey}
\item
  \protect\hyperlink{scheffe}{Scheffe}
\item
  \protect\hyperlink{bonferroni}{Bonferroni}
\end{itemize}

\hypertarget{tukey}{%
\subparagraph{Tukey}\label{tukey}}

All pairwise comparisons of factor level means. All pairs \(D = \mu_i - \mu_i'\) or all tests of the form:

\[
\begin{aligned}
&H_0: \mu_i -\mu_i' = 0 \\
&H_a: \mu_i - \mu_i' \neq 0
\end{aligned}
\]

\begin{itemize}
\tightlist
\item
  When all sample sizes are equal (\(n_1 = n_2 = ... = n_a\)) then the Tukey method family confidence coefficient is exactly \(1-\alpha\) and the significance level is exactly \(\alpha\)\\
\item
  When the sample sizes are not equal, the family confidence coefficient is greater than \(1-\alpha\) (i.e., the significance level is less than \(\alpha\)) so the test \textbf{conservative}\\
\item
  Tukey considers the \textbf{studentized range distribution}. If we have \(Y_1,..,Y_r\), observations from a normal distribution with mean \(\alpha\) and variance \(\sigma^2\). Define: \[
  w = max(Y_i) - min(Y_i)
  \] as the range of the observations. Let \(s^2\) be an estimate of \(\sigma^2\) with v degrees of freedom. Then, \[
  q(r,v) = \frac{w}{s}
  \] is called the studentized range. The distribution of q uses a special table.
\end{itemize}

\textbf{Notes}

\begin{itemize}
\tightlist
\item
  when we are not interested in testing all pairwise comparison,s the confidence coefficient for the family of comparisons under consideration will be greater than \(1-\alpha\) (with the significance level less than \(\alpha\))
\item
  Tukey can be used for ``data snooping'' as long as the effects to be studied on the basis of preliminary data analysis are pairwise comparisons.
\end{itemize}

\hypertarget{scheffe}{%
\subparagraph{Scheffe}\label{scheffe}}

This method applies when the family of interest is the set of possible contrasts among the treatment means:

\[
L = \sum_{i=1}^a c_i \mu_i
\]

where \(\sum_{i=1}^a c_i =0\)

That is, the family of all possible contrasts \(L\) or

\[
\begin{aligned}
&H_0: L = 0 \\
&H_a: L \neq 0
\end{aligned}
\]

The family confidence level for the Scheffe procedure is exactly \(1-\alpha\) (i.e., significance level = \(\alpha\)) whether the sample sizes are equal or not.

For simultaneous confidence intervals,

\[
\hat{L} \pm Ss(\hat{L})
\]

where \(\hat{L}=\sum c_i \bar{Y}_{i.},s^2(\hat{L}) = MSE \sum c_i^2/n_i\) and \(S^2 = (a-1)f_{1-\alpha;a-1,N-a}\)

The Scheffe procedure considers

\[
F = \frac{\hat{L}^2}{(a-1)s^2(\hat{L})}
\]

where we reject \(H_0\) at the family significance level \(\alpha\) if \(F > f_{(1-\alpha;a-1,N-a)}\)

\textbf{Note}

\begin{itemize}
\tightlist
\item
  Since applications of the Scheffe never involve all conceivable contrasts, the \textbf{finite family} confidence coefficient will be larger than \(1-\alpha\), so \(1-\alpha\) is a lower bound. Thus, people often consider a larger \(\alpha\) (e.g., 90\% confidence interval)
\item
  Scheffe can be used for ``data scooping'' since the family of statements contains all possible contrasts.
\item
  If only pairwise comparisons are to be considered, The Tukey procedure gives narrower confidence limits.
\end{itemize}

\hypertarget{bonferroni}{%
\subparagraph{Bonferroni}\label{bonferroni}}

Applicable whether the sample sizes are equal or unequal.

For the confidence intervals,

\[
\hat{L} \pm B s(\hat{L})
\]

where \(B= t_{(1-\alpha/(2g);N-a)}\) and g is the number of comparisons in the family.

Hypothesis testing

\[
\begin{aligned}
&H_0: L = 0 \\
&H_a: L \neq 0 
\end{aligned}
\]

Let \(T= \frac{\hat{L}}{s(\hat{L})}\) and reject \(H_0\) if \(|T|>t_{1-\alpha/(2g),N-a}\)

\textbf{Notes}

\begin{itemize}
\tightlist
\item
  If all pairwise comparisons are of interest, the Tukey procedure is superior (narrower confidence intervals). If not, Bonferroni may be better.
\item
  Bonferroni is better than Scheffe when the number of contrasts is about the same as the treatment levels (or less).
\item
  Recommendation: compute all threes and pick the smallest.
\item
  Bonferroni can't be used for \textbf{data snooping}
\end{itemize}

\hypertarget{fishers-lsd}{%
\subparagraph{Fisher's LSD}\label{fishers-lsd}}

does not control for family error rate

use \(t\)-stat for testing

\[
H_0: \mu_i = \mu_j
\]

t-stat

\[
t = \frac{\bar{y}_i - \bar{y}_j}{\sqrt{MSE(\frac{1}{n_i}+ \frac{1}{n_j})}}
\]

\hypertarget{newman-keuls}{%
\subparagraph{Newman-Keuls}\label{newman-keuls}}

Do not recommend using this test since it has less power than ANOVA.

\hypertarget{multiple-comparisons-with-a-control}{%
\paragraph{Multiple comparisons with a control}\label{multiple-comparisons-with-a-control}}

\hypertarget{dunnett}{%
\subparagraph{Dunnett}\label{dunnett}}

We have \(a\) groups where the last group is the control group, and the \(a-1\) treatment groups.

Then, we compare treatment groups to the control group. Hence, we have \(a-1\) contrasts (i.e., \(a-1\) pairwise comparisons)

\hypertarget{summary-5}{%
\paragraph{Summary}\label{summary-5}}

When choosing a multiple contrast method:

\begin{itemize}
\item
  Pairwise

  \begin{itemize}
  \tightlist
  \item
    Equal groups sizes: \protect\hyperlink{tukey}{Tukey}
  \item
    Unequal groups sizes: \protect\hyperlink{tukey}{Tukey}, \protect\hyperlink{scheffe}{Scheffe}
  \end{itemize}
\item
  Not pairwise

  \begin{itemize}
  \tightlist
  \item
    with control: \protect\hyperlink{dunnett}{Dunnett}
  \item
    general: \protect\hyperlink{bonferroni}{Bonferroni}, \protect\hyperlink{scheffe}{Scheffe}
  \end{itemize}
\end{itemize}

\hypertarget{single-factor-random-effects-model}{%
\subsection{Single Factor Random Effects Model}\label{single-factor-random-effects-model}}

Also known as ANOVA Type II models.

Treatments are chosen at from from larger population. We extend inference to all treatments in the population and not restrict our inference to those treatments that happened to be selected for the study.

\hypertarget{random-cell-means}{%
\subsubsection{Random Cell Means}\label{random-cell-means}}

\[
Y_{ij} = \mu_i + \epsilon_{ij}
\]

where

\begin{itemize}
\tightlist
\item
  \(\mu_i \sim N(\mu, \sigma^2_{\mu})\) and independent
\item
  \(\epsilon_{ij} \sim N(0,\sigma^2)\) and independent
\end{itemize}

\(\mu_i\) and \(\epsilon_{ij}\) are mutually independent for \(i =1,...,a; j = 1,...,n\)

With all treatment sample sizes are equal

\[
\begin{aligned}
E(Y_{ij}) &= E(\mu_i) = \mu \\
var(Y_{ij}) &= var(\mu_i) + var(\epsilon_i) = \sigma^2_{\mu} + \sigma^2
\end{aligned}
\]

Since \(Y_{ij}\) are not independent

\[
\begin{aligned}
cov(Y_{ij},Y_{ij'}) &= E(Y_{ij}Y_{ij'}) - E(Y_{ij})E(Y_{ij'})  \\
&= E(\mu_i^2 + \mu_i \epsilon_{ij'} + \mu_i \epsilon_{ij} + \epsilon_{ij}\epsilon_{ij'}) - \mu^2 \\
&= \sigma^2_{\mu} + \mu^2 - \mu^2 & \text{if} j \neq j' \\
&= \sigma^2_{\mu} & \text{if} j \neq j' 
\end{aligned}
\]

\[
\begin{aligned}
cov(Y_{ij},Y_{i'j'}) &= E(\mu_i \mu_{i'} + \mu_i \epsilon_{i'j'}+ \mu_{i'}\epsilon_{ij}+ \epsilon_{ij}\epsilon_{i'j'}) - \mu^2 \\
&= \mu^2 - \mu^2 & \text{if } i \neq i' \\
&= 0 \\
\end{aligned}
\]

Hence,

\begin{itemize}
\tightlist
\item
  all observations have the same variance
\item
  any two observations from the same treatment have covariance \(\sigma^2_{\mu}\)
\item
  The correlation between any two responses from the same treatment:\\
  \[
  \begin{aligned}
  \rho(Y_{ij},Y_{ij'}) &= \frac{\sigma^2_{\mu}}{\sigma^2_{\mu}+ \sigma^2} && \text{$j \neq j'$}
  \end{aligned}
  \]
\end{itemize}

\textbf{Inference}

\textbf{Intraclass Correlation Coefficient}

\[
\frac{\sigma^2_{\mu}}{\sigma^2 + \sigma^2_{\mu}}
\]

which measures the proportion of total variability of \(Y_{ij}\) accounted for by the variance of \(\mu_i\)

\[
\begin{aligned}
&H_0: \sigma_{\mu}^2 = 0 \\
&H_a: \sigma_{\mu}^2 \neq 0
\end{aligned}
\]

\(H_0\) implies \(\mu_i = \mu\) for all i, which can be tested by the F-test in ANOVA.

The understandings of the \protect\hyperlink{single-factor-fixed-effects-model}{Single Factor Fixed Effects Model} and the \protect\hyperlink{single-factor-random-effects-model}{Single Factor Random Effects Model} are different, the ANOVA is same for the one factor model. The difference is in the expected mean squares

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.3868}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.6132}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Random Effects} Model
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Fixed Effects} Model
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(E(MSE) = \sigma^2\) & \(E(MSE) = \sigma^2\) \\
\(E(M STR) = \sigma^2 - n \sigma^2_\mu\) & \(E(MSTR) = \sigma^2 + \frac{ \sum_i n_i (\mu_i - \mu)^2}{a-1}\) \\
\end{longtable}

If \(\sigma^2_\mu\), then MSE and MSTR have the same expectation (\(\sigma^2\)). Otherwise, \(E(MSTR) >E(MSE)\). Large values of the statistic

\[
F = \frac{MSTR}{MSE}
\]

suggest we reject \(H_0\).

Since \(F \sim F_{(a-1,a(n-1))}\) when \(H_0\) holds. If \(F > f_{(1-\alpha;a-1,a(n-1))}\) we reject \(H_0\).

If sample sizes are not equal, \(F\)-test can still be used, but the df are \(a-1\) and \(N-a\).

\hypertarget{estimation-of-mu}{%
\paragraph{\texorpdfstring{Estimation of \(\mu\)}{Estimation of \textbackslash mu}}\label{estimation-of-mu}}

An unbiased estimator of \(E(Y_{ij})=\mu\) is the grand mean: \(\hat{\mu} = \hat{Y}_{..}\)

The variance of this estimator is

\[
\begin{aligned}
var(\bar{Y}_{..}) &= var(\sum_i \bar{Y}_{i.}/a) \\
&= \frac{1}{a^2}\sum_ivar(\bar{Y}_{i.}) \\
&= \frac{1}{a^2}\sum_i(\sigma^2_\mu+\sigma^2/n) \\
&= \frac{1}{a^2}(\sigma^2_{\mu}+\sigma^2/n) \\
&= \frac{n\sigma^2_{\mu}+ \sigma^2}{an}
\end{aligned}
\]

An unbiased estimator of this variance is \(s^2(\bar{Y})=\frac{MSTR}{an}\). Thus \(\frac{\bar{Y}_{..}-\mu}{s(\bar{Y}_{..})} \sim t_{a-1}\)

A \(1-\alpha\) confidence interval is \(\bar{Y}_{..} \pm t_{(1-\alpha/2;a-1)}s(\bar{Y}_{..})\)

\hypertarget{estimation-of-sigma2_musigma2_musigma2}{%
\paragraph{\texorpdfstring{Estimation of \(\sigma^2_\mu/(\sigma^2_{\mu}+\sigma^2)\)}{Estimation of \textbackslash sigma\^{}2\_\textbackslash mu/(\textbackslash sigma\^{}2\_\{\textbackslash mu\}+\textbackslash sigma\^{}2)}}\label{estimation-of-sigma2_musigma2_musigma2}}

In the random and fixed effects model, MSTR and MSE are independent. When the sample sizes are equal (\(n_i = n\) for all i),

\[
\frac{\frac{MSTR}{n\sigma^2_\mu+ \sigma^2}}{\frac{MSE}{\sigma^2}} \sim f_{(a-1,a(n-1))}
\]

\[
P(f_{(\alpha/2;a-1,a(n-1))}\le \frac{\frac{MSTR}{n\sigma^2_\mu+ \sigma^2}}{\frac{MSE}{\sigma^2}} \le f_{(1-\alpha/2;a-1,a(n-1))}) = 1-\alpha
\]

\[
\begin{aligned}
L &= \frac{1}{n}(\frac{MSTR}{MSE}(\frac{1}{f_{(1-\alpha/2;a-1,a(n-1))}})-1) \\
U &= \frac{1}{n}(\frac{MSTR}{MSE}(\frac{1}{f_{(\alpha/2;a-1,a(n-1))}})-1)
\end{aligned}
\]

The lower and upper \((L^*,U^*)\) confidence limits for \(\frac{\sigma^2_\mu}{\sigma^2_\mu + \sigma^2}\)

\[
\begin{aligned}
L^* &= \frac{L}{1+L} \\
U^* &= \frac{U}{1+U}
\end{aligned}
\]

If the lower limit for \(\frac{\sigma^2_\mu}{\sigma^2}\) is negative, it is customary to set \(L = 0\).

\hypertarget{estimation-of-sigma2}{%
\paragraph{\texorpdfstring{Estimation of \(\sigma^2\)}{Estimation of \textbackslash sigma\^{}2}}\label{estimation-of-sigma2}}

\(a(n-1)MSE/\sigma^2 \sim \chi^2_{a(n-1)}\), the \((1-\alpha)\) confidence interval for \(\sigma^2\):

\[
\frac{a(n-1)MSE}{\chi^2_{1-\alpha/2;a(n-1)}} \le \sigma^2 \le \frac{a(n-1)MSE}{\chi^2_{\alpha/2;a(n-1)}}
\]

can also be used in case sample sizes are not equal - then df is N-a.

\hypertarget{estimation-of-sigma2_mu}{%
\paragraph{\texorpdfstring{Estimation of \(\sigma^2_\mu\)}{Estimation of \textbackslash sigma\^{}2\_\textbackslash mu}}\label{estimation-of-sigma2_mu}}

\(E(MSE) = \sigma^2\) \(E(MSTR) = \sigma^2 + n\sigma^2_\mu\). Hence,

\[
\sigma^2_{\mu} = \frac{E(MSTR)- E(MSE)}{n}
\]

An unbiased estimator of \(\sigma^2_\mu\) is given by

\[
s^2_\mu =\frac{MSTR-MSE}{n}
\]

if \(s^2_\mu < 0\), set \(s^2_\mu = 0\)

If sample sizes are not equal,

\[
s^2_\mu = \frac{MSTR - MSE}{n'}
\]

where \(n' = \frac{1}{a-1}(\sum_i n_i- \frac{\sum_i n^2_i}{\sum_i n_i})\)

no exact confidence intervals for \(\sigma^2_\mu\), but we can approximate intervals.

\textbf{Satterthewaite Procedure} can be used to construct approximate confidence intervals for linear combination of expected mean squares\\
A linear combination:

\[
\sigma^2_\mu = \frac{1}{n} E(MSTR) + (-\frac{1}{n}) E(MSE)
\]

\[
S = d_1 E(MS_1) + ..+ d_h E(MS_h)
\]

where \(d_i\) are coefficients.

An unbiased estimator of S is

\[
\hat{S} = d_1 MS_1 + ...+ d_h  MS_h 
\]

Let \(df_i\) be the degrees of freedom associated with the mean square \(MS_i\). The \textbf{Satterthwaite} approximation:

\[
\frac{(df)\hat{S}}{S} \sim \chi^2_{df}
\]

where

\[
df = \frac{(d_1MS_1+...+d_hMS_h)^2}{(d_1MS_1)^2/df_1 + ...+ (d_hMS_h)^2/df_h}
\]

An approximate \(1-\alpha\) confidence interval for S:

\[
\frac{(df)\hat{S}}{\chi^2_{1-\alpha/2;df}} \le S \le \frac{(df)\hat{S}}{\chi^2_{\alpha/2;df}}
\]

For the single factor random effects model

\[
\frac{(df)s^2_\mu}{\chi^2_{1-\alpha/2;df}} \le \sigma^2_\mu \le \frac{(df)s^2_\mu}{\chi^2_{\alpha/2;df}}
\]

where

\[
df = \frac{(sn^2_\mu)^2}{\frac{(MSTR)^2}{a-1}+ \frac{(MSE)^2}{a(n-1)}}
\]

\hypertarget{random-treatment-effects-model}{%
\subsubsection{Random Treatment Effects Model}\label{random-treatment-effects-model}}

\[
\tau_i = \mu_i - E(\mu_i) = \mu_i - \mu
\]

we have \(\mu_i = \mu + \tau_i\) and

\[
Y_{ij} = \mu + \tau_i + \epsilon_{ij}
\]

where

\begin{itemize}
\tightlist
\item
  \(\mu\) = constant, common to all observations
\item
  \(\tau_i \sim N(0,\sigma^2_\tau)\) independent (random variables)
\item
  \(\epsilon_{ij} \sim N(0,\sigma^2)\) independent.
\item
  \(\tau_{i}, \epsilon_{ij}\) are independent (i=1,\ldots,a; j =1,..,n)
\item
  our model is concerned with only balanced single factor ANOVA.
\end{itemize}

\textbf{Diagnostics Measures}

\begin{itemize}
\tightlist
\item
  Non-constant error variance (plots, Levene test, Hartley test).
\item
  Non-independence of errors (plots, Durban-Watson test).
\item
  Outliers (plots, regression methods).
\item
  Non-normality of error terms (plots, Shapiro-Wilk, Anderson-Darling).
\item
  Omitted Variable Bias (plots)
\end{itemize}

\textbf{Remedial}

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{weighted-least-squares}{Weighted Least Squares}
\item
  \protect\hyperlink{transformations}{Transformations}
\item
  Non-parametric Procedures.
\end{itemize}

\textbf{Note}

\begin{itemize}
\item
  Fixed effect ANOVA is relatively robust to

  \begin{itemize}
  \tightlist
  \item
    non-normality
  \item
    unequal variances when sample sizes are approximately equal; at least the F-test and multiple comparisons. However, single comparisons of treatment means are sensitive to unequal variances.
  \end{itemize}
\item
  Lack of independence can seriously affect both fixed and random effect ANVOA.
\end{itemize}

\hypertarget{two-factor-fixed-effect-anova}{%
\subsection{Two Factor Fixed Effect ANOVA}\label{two-factor-fixed-effect-anova}}

The multi-factor experiment is

\begin{itemize}
\tightlist
\item
  more efficient
\item
  provides more info
\item
  gives more validity to the findings.
\end{itemize}

\hypertarget{balanced}{%
\subsubsection{Balanced}\label{balanced}}

Assumption:

\begin{itemize}
\tightlist
\item
  All treatment sample sizes are equal
\item
  All treatment means are of equal importance
\end{itemize}

Assume:

\begin{itemize}
\tightlist
\item
  Factor \(A\) has \texttt{a} levels and Factor \(B\) has \texttt{b} levels. All \(a \times b\) factor levels are considered.
\item
  The number of treatments for each level is n.~\(N = abn\) observations in the study.
\end{itemize}

\hypertarget{cell-means-model-1}{%
\paragraph{Cell Means Model}\label{cell-means-model-1}}

\[
Y_{ijk} = \mu_{ij} + \epsilon_{ijk}
\]

where

\begin{itemize}
\tightlist
\item
  \(\mu_{ij}\) are fixed parameters (cell means)
\item
  \(i = 1,...,a\) = the levels of Factor A
\item
  \(j = 1,...,b\) = the levels of Factor B.
\item
  \(\epsilon_{ijk} \sim \text{indep } N(0,\sigma^2)\) for \(i = 1,...,a\), \(j = 1,..,b\) and \(k = 1,..,n\)
\end{itemize}

And

\[
\begin{aligned}
E(Y_{ijk}) &= \mu_{ij} \\
var(Y_{ijk}) &= var(\epsilon_{ijk}) = \sigma^2
\end{aligned}
\]

Hence,

\[
Y_{ijk} \sim \text{indep } N(\mu_{ij},\sigma^2)
\]

And the model is\\

\[
\mathbf{Y} = \mathbf{X} \beta + \epsilon
\]

Thus,

\[
\begin{aligned}
E(\mathbf{Y}) &= \mathbf{X}\beta \\
var(\mathbf{Y}) &= \sigma^2 \mathbf{I}
\end{aligned}
\]

\textbf{Interaction}

\[
(\alpha \beta)_{ij} = \mu_{ij} - (\mu_{..}+ \alpha_i + \beta_j)
\]

where

\begin{itemize}
\tightlist
\item
  \(\mu_{..} = \sum_i \sum_j \mu_{ij}/ab\) is the grand mean
\item
  \(\alpha_i = \mu_{i.}-\mu_{..}\) is the main effect for factor \(A\) at the \(i\)-th level
\item
  \(\beta_j = \mu_{.j} - \mu_{..}\) is the main effect for factor \(B\) at the \(j\)-th level
\item
  \((\alpha \beta)_{ij}\) is the interaction effect when factor \(A\) is at the \(i\)-th level and factor \(B\) is at the \(j\)-th level.
\item
  \((\alpha \beta)_{ij} = \mu_{ij} - \mu_{i.}-\mu_{.j}+ \mu_{..}\)
\end{itemize}

Examine interactions:

\begin{itemize}
\tightlist
\item
  Examine whether all \(\mu_{ij}\) can be expressed as the sums \(\mu_{..} + \alpha_i + \beta_j\)
\item
  Examine whether the difference between the mean responses for any two levels of factor \(B\) is the same for all levels of factor \(A\).
\item
  Examine whether the difference between the mean response for any two levels of factor \(A\) is the same for all levels of factor \(B\)
\item
  Examine whether the treatment mean curves for the different factor levels in a treatment plot are parallel.
\end{itemize}

For \(j = 1,...,b\)

\[
\begin{aligned}
\sum_i(\alpha \beta)_{ij} &= \sum_i (\mu_{ij} - \mu_{..} - \alpha_i - \beta_j) \\
&= \sum_i \mu_{ij} - a \mu_{..} - \sum_i \alpha_i - a \beta_j \\
&= a \mu_{.j} - a \mu_{..}- \sum_i (\mu_{i.} - \mu_{..}) - a(\mu_{.j}-\mu_{..}) \\
&= a \mu_{.j} - a \mu_{..} - a \mu_{..}+ a \mu_{..} - a (\mu_{.j} - \mu_{..}) \\
&= 0
\end{aligned}
\]

Similarly, \(\sum_j (\alpha \beta) = 0, i = 1,...,a\) and \(\sum_i \sum_j (\alpha \beta)_{ij} =0\), \(\sum_i \alpha_i = 0\), \(\sum_j \beta_j = 0\)

\hypertarget{factor-effects-model}{%
\paragraph{Factor Effects Model}\label{factor-effects-model}}

\[
\begin{aligned}
\mu_{ij} &= \mu_{..} + \alpha_i + \beta_j + (\alpha \beta)_{ij} \\
Y_{ijk} &= \mu_{..} + \alpha_i + \beta_j + (\alpha \beta)_{ij} + \epsilon_{ijk}
\end{aligned}
\]

where

\begin{itemize}
\tightlist
\item
  \(\mu_{..}\) is a constant
\item
  \(\alpha_i\) are constants subject to the restriction \(\sum_i \alpha_i=0\)
\item
  \(\beta_j\) are constants subject to the restriction \(\sum_j \beta_j = 0\)
\item
  \((\alpha \beta)_{ij}\) are constants subject to the restriction \(\sum_i(\alpha \beta)_{ij} = 0\) for \(j=1,...,b\) and \(\sum_j(\alpha \beta)_{ij} = 0\) for \(i = 1,...,a\)
\item
  \(\epsilon_{ijk} \sim \text{indep } N(0,\sigma^2)\) for \(k = 1,..,n\)
\end{itemize}

We have

\[
\begin{aligned}
E(Y_{ijk}) &= \mu_{..} + \alpha_i + \beta_j + (\alpha \beta)_{ij}\\
var(Y_{ijk}) &= \sigma^2 \\
Y_{ijk} &\sim N (\mu_{..} + \alpha_i + \beta_j + (\alpha \beta)_{ij}, \sigma^2)
\end{aligned}
\]

We have \(1+a+b+ab\) parameters. But there are \(ab\) parameters in the \protect\hyperlink{cell-means-model-1}{Cell Means Model}. In the \protect\hyperlink{factor-effects-model}{Factor Effects Model}, the restrictions limit the number of parameters that can be estimated:

\[
\begin{aligned}
1 &\text{ for } \mu_{..} \\
(a-1) &\text{ for } \alpha_i \\
(b-1) &\text{ for } \beta_j \\
(a-1)(b-1) &\text{ for } (\alpha \beta)_{ij}
\end{aligned}
\]

Hence, there are

\[
1 + a - 1 + b - 1 + ab - a- b + 1 = ab
\]

parameters in the model.

We can have several restrictions when considering the model in the form \(\mathbf{Y} = \mathbf{X} \beta + \epsilon\)

One way:

\[
\begin{aligned}
\alpha_a  &= \alpha_1 - \alpha_2 - ... - \alpha_{a-1} \\
\beta_b &= -\beta_1 - \beta_2 - ... - \beta_{b-1} \\
(\alpha \beta)_{ib} &= -(\alpha \beta)_{i1} -(\alpha \beta)_{i2} -...-(\alpha \beta)_{i,b-1} ; i = 1,..,a \\
(\alpha \beta)_{aj}& = -(\alpha \beta)_{1j}-(\alpha \beta)_{2j} - ... -(\alpha \beta)_{a-1,j}; j = 1,..,b
\end{aligned}
\]

We can fit the model by least squares or maximum likelihood

\textbf{Cell Means Model}\\
minimize\\

\[
Q = \sum_i \sum_j \sum_k (Y_{ijk}-\mu_{ij})^2
\]

estimators

\[
\begin{aligned}
\hat{\mu}_{ij} &= \bar{Y}_{ij} \\
\hat{Y}_{ijk} &= \bar{Y}_{ij} \\
e_{ijk} = Y_{ijk} - \hat{Y}_{ijk} &= Y_{ijk} - \bar{Y}_{ij}
\end{aligned}
\]

\textbf{Factor Effects Model}

\[
Q = \sum_i \sum_j \sum_k (Y_{ijk} - \mu_{..}-\alpha_i = \beta_j - (\alpha \beta)_{ij})^2
\]

subject to the restrictions

\[
\begin{aligned}
\sum_i \alpha_i &= 0 \\
\sum_j \beta_j &= 0 \\
\sum_i (\alpha \beta)_{ij} &= 0 \\
\sum_j (\alpha \beta)_{ij} &= 0
\end{aligned}
\]

estimators

\[
\begin{aligned}
\hat{\mu}_{..} &= \bar{Y}_{...} \\
\hat{\alpha}_i &= \bar{Y}_{i..} - \bar{Y}_{...} \\
\hat{\beta}_j &= \bar{Y}_{.j.}-\bar{Y}_{...} \\
(\hat{\alpha \beta})_{ij} &= \bar{Y}_{ij.} - \bar{Y}_{i..} - \bar{Y}_{.j.}+ \bar{Y}_{...}
\end{aligned}
\]

The fitted values

\[
\hat{Y}_{ijk} = \bar{Y}_{...}+ (\bar{Y}_{i..}- \bar{Y}_{...})+ (\bar{Y}_{.j.}- \bar{Y}_{...}) + (\bar{Y}_{ij.} - \bar{Y}_{i..}-\bar{Y}_{.j.}+\bar{Y}_{...}) = \bar{Y}_{ij.}
\]

where

\[
\begin{aligned}
e_{ijk} &= Y_{ijk} - \bar{Y}_{ij.} \\
e_{ijk} &\sim \text{ indep } (0,\sigma^2)
\end{aligned}
\]

and

\[
\begin{aligned}
s^2_{\hat{\mu}..} &= \frac{MSE}{nab} \\
s^2_{\hat{\alpha}_i} &= MSE(\frac{1}{nb} - \frac{1}{nab}) \\
s^2_{\hat{\beta}_j} &= MSE(\frac{1}{na} - \frac{1}{nab}) \\
s^2_{(\hat{\alpha\beta})_{ij}} &= MSE (\frac{1}{n} - \frac{1}{na}- \frac{1}{nb} + \frac{1}{nab})
\end{aligned}
\]

\hypertarget{partitioning-the-total-sum-of-squares}{%
\subparagraph{Partitioning the Total Sum of Squares}\label{partitioning-the-total-sum-of-squares}}

\[
Y_{ijk} - \bar{Y}_{...} = \bar{Y}_{ij.} - \bar{Y}_{...} + Y_{ijk} - \bar{Y}_{ij.}
\]

\(Y_{ijk} - \bar{Y}_{...}\): Total deviation\\
\(\bar{Y}_{ij.} - \bar{Y}_{...}\): Deviation of treatment mean from overall mean\\
\(Y_{ijk} - \bar{Y}_{ij.}\): Deviation of observation around treatment mean (residual).

\[
\begin{aligned}
\sum_i \sum_j \sum_k (Y_{ijk} - \bar{Y}_{...})^2 &= n \sum_i \sum_j (\bar{Y}_{ij.}- \bar{Y}_{...})^2+ \sum_i \sum_j sum_k (Y_{ijk} - \bar{ij.})^2 \\
SSTO &= SSTR + SSE
\end{aligned}
\]

(cross product terms are 0)

\[
\bar{Y}_{ij.}- \bar{Y}_{...} = \bar{Y}_{i..}-\bar{Y}_{...} + \bar{Y}_{.j.}-\bar{Y}_{...} + \bar{Y}_{ij.} - \bar{Y}_{i..} - \bar{Y}_{.j.} + \bar{Y}_{...}
\]

squaring and summing:\\

\[
\begin{aligned}
n\sum_i \sum_j (\bar{Y}_{ij.}-\bar{Y}_{...})^2 &= nb\sum_i (\bar{Y}_{i..}-\bar{Y}_{...})^2 + na \sum_j (\bar{Y}_{.j.}-\bar{Y}_{...})^2 \\
&+ n \sum_i \sum_j (\bar{Y}_{ij.}-\bar{Y}_{i..}- \bar{Y}_{.j.}+ \bar{Y}_{...})^2 \\
SSTR &= SSA + SSB + SSAB
\end{aligned}
\]

The interaction term from

\[
\begin{aligned}
SSAB &= SSTO - SSE - SSA - SSB \\
SSAB &= SSTR - SSA - SSB 
\end{aligned}
\]

where

\begin{itemize}
\tightlist
\item
  \(SSA\) is the factor \(A\) sum of squares (measures the variability of the estimated factor \(A\) level means \(\bar{Y}_{i..}\))- the more variable, the larger \(SSA\)
\item
  \(SSB\) is the factor \(B\) sum of squares
\item
  \(SSAB\) is the interaction sum of squares, measuring the variability of the estimated interactions.
\end{itemize}

\hypertarget{partitioning-the-df}{%
\subparagraph{Partitioning the df}\label{partitioning-the-df}}

\(N = abn\) cases and \(ab\) treatments.

For one-way ANOVA and regression, the partition has df:

\[
SS: SSTO = SSTR + SSE
\]

\[
df: N-1 = (ab-1) + (N-ab) 
\]

we must further partition the \(ab-1\) df with SSTR

\[
SSTR = SSA + SSB + SSAB
\]

\[
ab-1 = (a-1) + (b-1) + (a-1)(b-1) 
\]

\begin{itemize}
\tightlist
\item
  \(df_{SSA} = a-1\): a treatment deviations but 1 df is lost due to the restriction \(\sum (\bar{Y}_{i..}- \bar{Y}_{...})=0\)\\
\item
  \(df_{SSB} = b-1\): b treatment deviations but 1 df is lost due to the restriction \(\sum (\bar{Y}_{.j.}- \bar{Y}_{...})=0\)\\
\item
  \(df_{SSAB} = (a-1)(b-1)= (ab-1)-(a-1)-(b-1)\): ab interactions, there are (a+b-1) restrictions, so df = ab-a-(b-1)= (a-1)(b-1)
\end{itemize}

\hypertarget{mean-squares}{%
\subparagraph{Mean Squares}\label{mean-squares}}

\[
\begin{aligned}
MSA &= \frac{SSA}{a-1}\\
MSB &= \frac{SSB}{b-1}\\
MSAB &= \frac{SSAB}{(a-1)(b-1)}
\end{aligned}
\]

The expected mean squares are

\[
\begin{aligned}
E(MSE) &= \sigma^2 \\
E(MSA) &= \sigma^2 + nb \frac{\sum \alpha_i^2}{a-1} = \sigma^2 + nb \frac{\sum(\sum_{i.}-\mu_{..})^2}{a-1}  \\
E(MSB) &= \sigma^2 + na \frac{\sum \beta_i^2}{b-1} = \sigma^2 + na \frac{\sum(\sum_{.j}-\mu_{..})^2}{b-1} \\
E(MSAB) &= \sigma^2 + n \frac{\sum \sum (\alpha \beta)_{ij}^2}{(a-1)(b-1)} = \sigma^2 + n \frac{\sum (\mu_{ij}- \mu_{i.}- \mu_{.j}+ \mu_{..} )^2}{(a-1)(b-1)}
\end{aligned}
\]

If there are no factor A main effects (all \(\mu_{i.} = 0\) or \(\alpha_i = 0\)) the MSA and MSE have the same expectation; otherwise MSA \textgreater{} MSE. Same for factor B, and interaction effects. which case we can examine F-statistics.

\textbf{Interaction}

\[
\begin{aligned}
H_0: \mu_{ij}- \mu_{i.} - \mu_{.j} + \mu_{..} = 0 && \text{for all i,j} \\
H_a: \mu_{ij}- \mu_{i.} - \mu_{.j} + \mu_{..} \neq 0 && \text{for some i,j}
\end{aligned}
\]

or

\[
\begin{aligned}
&H_0: \text{All}(\alpha \beta)_{ij} = 0 \\
&H_a: \text{Not all} (\alpha \beta) = 0
\end{aligned}
\]

Let \(F = \frac{MSAB}{MSE}\). When \(H_0\) is true \(F \sim f_{((a-1)(b-1),ab(n-1))}\). So reject \(H_0\) when \(F > f_{((a-1)(b-1),ab(n-1))}\)

Factor A main effects:\\

\[
\begin{aligned}
&H_0: \mu_{1.} = \mu_{2.} = ... = \mu_{a.} \\
&H_a: \text{Not all $\mu_{i.}$ are equal}
\end{aligned}
\]

or

\[
\begin{aligned}
&H_0: \alpha_1 = ... = \alpha_a = 0 \\
&H_a: \text{Not all $\alpha_i$ are equal to 0}
\end{aligned}
\]

\(F= \frac{MSA}{MSE}\) and reject \(H_0\) if \(F>f_{(1-\alpha;a-1,ab(n-1))}\)

\hypertarget{two-way-anova}{%
\subparagraph{Two-way ANOVA}\label{two-way-anova}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1477}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1705}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1477}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Source of Variation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SS
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
df
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MS
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
F
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Factor A & \(SSA\) & \(a-1\) & \(MSA = SSA/(a-1)\) & \(MSA/MSE\) \\
Factor B & \(SSB\) & \(b-1\) & \(MSB = SSB/(b-1)\) & \(MSB/MSE\) \\
AB interactions & \(SSAB\) & \((a-1)(b-1)\) & \(MSAB = SSAB /MSE\) & \\
Error & \(SSE\) & \(ab(n-1)\) & \(MSE = SSE/ab(n-1)\) & \\
Total (corrected) & \(SSTO\) & \(abn - 1\) & & \\
\end{longtable}

Doing 2-way ANOVA means you always check interaction first, because if there are significant interactions, checking the significance of the main effects becomes moot.

The main effects concern the mean responses for levels of one factor averaged over the levels of the other factor. When interaction is present, we can't conclude that a given factor has no effect, even if these averages are the same. It means that the effect of the factor depends on the level of the other factor.

On the other hand, if you can establish that there is no interaction, then you can consider inference on the factor main effects, which are then said to be \textbf{additive}.\\
And we can also compare factor means like the \protect\hyperlink{single-factor-fixed-effects-model}{Single Factor Fixed Effects Model} using \protect\hyperlink{tukey}{Tukey}, \protect\hyperlink{scheffe}{Scheffe}, \protect\hyperlink{bonferroni}{Bonferroni}.

We can also consider contrasts in the 2-way model

\[
L = \sum c_i \mu_i
\]

where \(\sum c_i =0\)\\
which is estimated by

\[
\hat{L} = \sum c_i \bar{Y}_{i..}
\]

with variance

\[
\sigma^2(\hat{L}) = \frac{\sigma^2}{bn} \sum c_i^2
\]

and variance estimate

\[
\frac{MSE}{bn} \sum c_i^2
\]

\textbf{Orthogonal Contrasts}

\[
\begin{aligned}
L_1 &= \sum c_i \mu_i, \sum c_i = 0 \\
L_2 &= \sum d_i \mu_i , \sum d_i = 0
\end{aligned}
\]

these contrasts are said to be \textbf{orthogonal} if

\[
\sum \frac{c_i d_i}{n_i} = 0
\]

in balanced case \(\sum c_i d_i =0\)

\[
\begin{aligned}
cov(\hat{L}_1, \hat{L}_2) &= cov(\sum_i c_i \bar{Y}_{i..}, \sum_l d_l \bar{Y}_{l..}) \\
&= \sum_i \sum_l c_i d_l cov(\bar{Y}_{i..},\bar{Y}_{l..}) \\
&= \sum_i c_i d_i \frac{\sigma^2}{bn} = 0
\end{aligned}
\]

Orthogonal contrasts can be used to further partition the model sum of squares. There are many sets of orthogonal contrasts and thus, many ways to partition the sum of squares.

A special set of orthogonal contrasts that are used when the levels of a factor can be assigned values on a metric scale are called \textbf{orthogonal polynomials}

Coefficients can be found for the special case of

\begin{itemize}
\tightlist
\item
  equal spaced levels (e.g., (0 15 30 45 60))\\
\item
  equal sample sizes (\(n_1 = n_2 = ... = n_{ab}\))
\end{itemize}

We can define the SS for a given contrast:

\[
SS_L = \frac{\hat{L}^2}{\sum_{i=1}^a (c^2_i/bn_i)}
\]

\[
T = \frac{\hat{L}}{\sqrt{MSE\sum_{i=1}^a(c_i^2/bn_i)}} \sim t
\]

Moreover,

\[
t^2_{(1-\alpha/2;df)}=F_{(1-\alpha;1,df)}
\]

So,

\[
\frac{SS_L}{MSE} \sim F_{(1-\alpha;1,df_{MSE})}
\]

all contrasts have d.f = 1

\hypertarget{unbalanced}{%
\subsubsection{Unbalanced}\label{unbalanced}}

We could have unequal numbers of replications for all treatment combinations:

\begin{itemize}
\tightlist
\item
  Observational studies
\item
  Dropouts in designed studies
\item
  Larger sample sizes for inexpensive treatments
\item
  Sample sizes to match population makeup.
\end{itemize}

Assume that each factor combination has at least 1 observation (no empty cells)

Consider the same model as:

\[
Y_{ijk} = \mu_{..} + \alpha_i + \beta_j + (\alpha \beta)_{ij} + \epsilon_{ijk}
\]

where sample sizes are: \(n_{ij}\):

\[
\begin{aligned}
n_{i.} &= \sum_j n_{ij} \\
n_{.j} &= \sum_i n_{ij} \\
n_T &= \sum_i \sum_j n_{ij}
\end{aligned}
\]

Problem here is that

\[
SSTO \neq SSA + SSB + SSAB + SSE
\]

(the design is \textbf{non-orthogonal})

\begin{itemize}
\tightlist
\item
  For \(i = 1,...,a-1,\)
\end{itemize}

\[
u_i = \begin{cases} +1 & \text{if the obs is from the i-th level of Factor 1} \\ -1 & \text{if the obs is from the a-th level of Factor 1} \\ 0 & \text{otherwise} \\ \end{cases}
\]

\begin{itemize}
\tightlist
\item
  For \(j=1,...,b-1\)
\end{itemize}

\[
v_i = 
\begin{cases} +1 & \text{if the obs is from the j-th level of Factor 1} \\ -1 & \text{if the obs is from the b-th level of Factor 1} \\ 0 & \text{otherwise} \\ 
\end{cases}
\]

We can use these indicator variables as predictor variables and \(\mu_{..}, \alpha_i ,\beta_j, (\alpha \beta)_{ij}\) as unknown parameters.

\[
Y = \mu_{..} + \sum_{i=1}^{a-1} \alpha_i u_i + \sum_{j=1}^{b-1} \beta_j v_j + \sum_{i=1}^{a-1} \sum_{j=1}^{b-1}(\alpha \beta)_{ij} u_i v_j + \epsilon
\]

To test hypotheses, we use the extra sum of squares idea.

For interaction effects

\[
\begin{aligned}
&H_0: all (\alpha \beta)_{ij} = 0 \\
&H_a: \text{not all }(\alpha \beta)_{ij} =0
\end{aligned}
\]

Or to test

\[
\begin{aligned}
&H_0: \beta_1 = \beta_2 = \beta_3 = 0 \\
&H_a: \text{not all } \beta_j = 0
\end{aligned}
\]

\textbf{Analysis of Factor Means}

(e.g., contrasts) is analogous to the balanced case, with modifications in the formulas for means and standard errors to account for unequal sample sizes.

Or , we can fit the cell means model and consider it from a regression perspective

If you have empty cells (i.e., some factor combinations have no observation), then the equivalent regression approach can't be used. But you can still do partial analyses

\hypertarget{two-way-random-effects-anova}{%
\subsection{Two-Way Random Effects ANOVA}\label{two-way-random-effects-anova}}

\[
Y_{ijk} = \mu_{..} + \alpha_i + \beta_j + (\alpha \beta)_{ij} + \epsilon_{ij}
\]

where

\begin{itemize}
\tightlist
\item
  \(\mu_{..}\): constant
\item
  \(\alpha_i \sim N(0,\sigma^2_{\alpha}), i = 1,..,a\) (independent)
\item
  \(\beta_j \sim N(0,\sigma^2_{\beta}), j = 1,..,b\) (independent)
\item
  \((\alpha \beta)_{ij} \sim N(0,\sigma^2_{\alpha \beta}),i=1,...,a,j=1,..,b\) (independent)
\item
  \(\epsilon_{ijk} \sim N(0,\sigma^2)\) (independent)
\end{itemize}

All \(\alpha_i, \beta_j, (\alpha \beta)_{ij}\) are pairwise independent

Theoretical means, variances, and covariances are

\[
\begin{aligned}
E(Y_{ijk}) &= \mu_{..} \\
var(Y_{ijk}) &= \sigma^2_Y= \sigma^2_\alpha + \sigma^2_\beta +  \sigma^2_{\alpha \beta} + \sigma^2 
\end{aligned}
\]

So

\(Y_{ijk} \sim N(\mu_{..},\sigma^2_\alpha + \sigma^2_\beta + \sigma^2_{\alpha \beta} + \sigma^2)\)

\[
\begin{aligned}
cov(Y_{ijk},Y_{ij'k'}) &= \sigma^2_{\alpha}, j \neq j' \\
cov(Y_{ijk},Y_{i'jk'}) &= \sigma^2_{\beta}, i \neq i'\\
cov(Y_{ijk},Y_{ijk'}) &= \sigma^2_\alpha + \sigma^2_{\beta} + \sigma^2_{\alpha \beta}, k \neq k' \\
cov(Y_{ijk},Y_{i'j'k'}) &= , i \neq i', j \neq j'
\end{aligned}
\]

\hypertarget{two-way-mixed-effects-anova}{%
\subsection{Two-Way Mixed Effects ANOVA}\label{two-way-mixed-effects-anova}}

\hypertarget{balanced-1}{%
\subsubsection{Balanced}\label{balanced-1}}

One fixed factor, while other is random treatment levels, we have a \textbf{mixed effects model} or a \textbf{mixed model}

\textbf{Restricted mixed model} for 2-way ANOVA:

\[
Y_{ijk} = \mu_{..} + \alpha_i + \beta_j + (\alpha \beta)_{ij} + \epsilon_{ijk}
\]

where

\begin{itemize}
\tightlist
\item
  \(\mu_{..}\): constant
\item
  \(\alpha_i\): fixed effects with constraints subject to restriction \(\sum \alpha_i = 0\)
\item
  \(\beta_j \sim indep N(0,\sigma^2_\beta)\)
\item
  \((\alpha \beta)_{ij} \sim N(0,\frac{a-1}{a}\sigma^2_{\alpha \beta})\) subject to restriction \(\sum_i (\alpha \beta)_{ij} = 0\) for all j, the variance here is written as the proportion for convenience; it makes the expected mean squares simpler (other assumed \(var((\alpha \beta)_{ij}= \sigma^2_{\alpha \beta}\))
\item
  \(cov((\alpha \beta)_{ij},(\alpha \beta)_{i'j'}) = - \frac{1}{a} \sigma^2_{\alpha \beta}, i \neq i'\)
\item
  \(\epsilon_{ijk}\sim indepN(0,\sigma^2)\)
\item
  \(\beta_j, (\alpha \beta)_{ij}, \epsilon_{ijk}\) are pairwise independent
\end{itemize}

Two-way mixed models are written in an ``unrestricted'' form, with no restrictions on the interaction effects \((\alpha \beta)_{ij}\), they are pairwise independent.

Let \(\beta^*, (\alpha \beta)^*_{ij}\) be the unrestricted random effects, and \((\bar{\alpha \beta})_{ij}^*\) the means averaged over the fixed factor for each level of random factor B.

\[
\begin{aligned}
\beta_j &= \beta_j^* + (\bar{\alpha \beta})_{ij}^* \\
(\alpha \beta)_{ij} &= (\alpha \beta)_{ij}^* - (\bar{\alpha \beta})_{ij}^*
\end{aligned}
\]

Some consider the restricted model to be more general. but here we consider the restricted form.

\[
\begin{aligned}
E(Y_{ijk}) &= \mu_{..} + \alpha_i \\
var(Y_{ijk}) &= \sigma^2_\beta + \frac{a-1}{a} \sigma^2_{\alpha \beta} + \sigma^2
\end{aligned}
\]

Responses from the same random factor \((B)\) level are correlated

\[
\begin{aligned}
cov(Y_{ijk},Y_{ijk'}) &= E(Y_{ijk}Y_{ijk'}) - E(Y_{ijk})E(Y_{ijk'}) \\
&= \sigma^2_\beta + \frac{a-1}{a} \sigma^2_{\alpha \beta} , k \neq k'
\end{aligned}
\]

Similarly,

\[
\begin{aligned}
cov(Y_{ijk},Y_{i'jk'}) &= \sigma^2_\beta - \frac{1}{a} \sigma^2_{\alpha\ \beta}, i \neq i' \\
cov(Y_{ijk},Y_{i'j'k'}) &= 0,  j \neq j'
\end{aligned}
\]

Hence, you can see that the only way you don't have dependence in the \(Y\) is when they don't share the same random effect.

An advantage of the \textbf{restricted mixed model} is that 2 observations from the same random factor b level can be positively or negatively correlated. In the \textbf{unrestricted model}, they can only be positively correlated.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.0843}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.0904}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.4337}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3795}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Mean Square
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Fixed ANOVA

(A, B Fixed)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Random ANOVA

(A,B random)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Mixed ANVOA

(A fixed, B random)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
MSA & a - 1 & \(\sigma ^2+ n b \frac{\sum\alpha_i^2}{a-1}\) & \(\sigma^2 + nb\sigma^ 2_ \alpha +n \sigma^ 2_{\alpha \beta}\) \\
MSB & b-1 & \(\sigma^2 + n a \frac{\sum\beta ^2_j}{b-1}\) & \(\sigma^ 2 + na\sigma^2_ \beta +n \sigma^ 2_{\alpha \beta}\) \\
MSAB & ( a-1)(b-1) & \(\sigma^2 + n \frac{\sum \sum(\alpha \beta )^2_ {ij}} { ( a-1)(b-1)}\) & \(\sigma^2+n \sigma^2_{\alpha \beta}\) \\
MSE & (n-1)ab & \(\sigma^2\) & \(\sigma^2\) \\
\end{longtable}

For fixed, random, and mixed models (balanced), the ANOVA table sums of squares calculations are identical. (also true for df and mean squares). The only difference is with the expected mean squares, thus the test statistics.

In Random ANOVA, we test

\[
\begin{aligned}
&H_0: \sigma^2 = 0 \\
&H_a: \sigma^2 > 0 
\end{aligned}
\]

by considering \(F= \frac{MSA}{MSAB} \sim F_{a-1;(a-1)(b-1)}\)

The same test statistic is used for mixed models, but in that case we are testing null hypothesis that all of the \(\alpha_i = 0\)

The test statistic different for the same null hypothesis under the fixed effects model.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2386}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2386}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Test for effects of
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Fixed ANOVA

(A\&B fixed)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Random ANOVA

(A\&B random)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Mixed ANOVA

(A fixed, B random)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Factor A & \(\frac{MSA}{MSE}\) & \(\frac{MSA}{MSAB}\) & \(\frac{MSA}{MSAB}\) \\
Factor B & \(\frac{MSB}{MSE}\) & \(\frac{MSB}{MSAB}\) & \(\frac{MSB}{MSE}\) \\
AB interactions & \(\frac{MSAB}{MSE}\) & \(\frac{MSAB}{MSE}\) & \(\frac{MSAB}{MSE}\) \\
\end{longtable}

\textbf{Estimation Of Variance Components}

In random and mixed effects models, we are interested in estimating the \textbf{variance components}\\
Variance component \(\sigma^2_\beta\) in the mixed ANOVA.

\[
E(\sigma^2_\beta) = \frac{E(MSB)-E(MSE)}{na} = \frac{\sigma^2 + na \sigma^2_\beta - \sigma^2}{na} = \sigma^2_\beta
\]

which can be estimated with

\[
\hat{\sigma}^2_\beta = \frac{MSB - MSE}{na}
\]

Confidence intervals for variance components can be constructed (approximately) by using the \textbf{Satterthwaite} procedure or the MLS procedure (like the 1-way random effects)

\textbf{Estimation of Fixed Effects in Mixed Models}

\[
\begin{aligned}
\hat{\alpha}_i &= \bar{Y}_{i..} - \bar{Y}_{...} \\
\hat{\mu}_{i.} &= \bar{Y}_{...} + (\bar{Y}_{i..}- \bar{Y}_{...}) = \bar{Y}_{i..}  \\
\sigma^2(\hat{\alpha}_i) &= \frac{\sigma^2 + n \sigma^2_{\alpha \beta}}{bn} = \frac{E(MSAB)}{bn} \\
s^2(\hat{\alpha}_i) &= \frac{MSAB}{bn}
\end{aligned}
\]

Contrasts on the \textbf{Fixed Effects}

\[
\begin{aligned}
L &= \sum c_i \alpha_i \\
\sum c_i &= 0 \\
\hat{L} &= \sum c_i \hat{\alpha}_i \\
\sigma^2(\hat{L}) &= \sum c^2_i \sigma^2 (\hat{\alpha}_i) \\
s^2(\hat{L}) &= \frac{MSAB}{bn} \sum c^2_i
\end{aligned}
\]

Confidence intervals and tests can be constructed as usual

\hypertarget{unbalanced-1}{%
\subsubsection{Unbalanced}\label{unbalanced-1}}

For a mixed model with a = 2, b = 4

\[
\begin{aligned}
Y_{ijk} &= \mu_{..} + \alpha_i + \beta_j + (\alpha \beta)_{ij} + \epsilon_{ijk} \\
var(\beta_j)&= \sigma^2_\beta \\
var((\alpha \beta)_{ij})&= \frac{2-1}{2}\sigma^2_{\alpha \beta} = \frac{\sigma^2_{\alpha \beta}}{2} \\
var(\epsilon_{ijk}) &= \sigma^2 \\
E(Y_{ijk}) &= \mu_{..} + \alpha_i \\
var(Y_{ijk}) &= \sigma^2_{\beta} + \frac{\sigma^2_{\alpha \beta}}{2} + \sigma^2 \\
cov(Y_{ijk},Y_{ijk'}) &= \sigma^2 + \frac{\sigma^2_{\alpha \beta}}{2}, k \neq k' \\
cov(Y_{ijk},Y_{i'jk'}) &= \sigma^2_{\beta} - \frac{\sigma^2_{\alpha \beta}}{2}, i \neq i' \\
cov(Y_{ijk},Y_{i'j'k'}) &= 0, j \neq j' 
\end{aligned}
\]

assume

\[
\mathbf{Y} \sim N(\mathbf{X}\beta, M)
\]

where \(M\) is block diagonal

density function

\[
f(\mathbf{Y}) = \frac{1}{(2\pi)^{N/2}|M|^{1/2}}exp(-\frac{1}{2}\mathbf{(Y - X \beta)' M^{-1}(Y-X\beta)})
\]

if we knew the variance components, we could use GLS:

\[
\hat{\beta}_{GLS} = \mathbf{(X'M^{-1}X)^{-1}X'M^{-1}Y}
\]

but we usually don't know the variance components \(\sigma^2, \sigma^2_\beta, \sigma^2_{\alpha \beta}\) that make up \(M\)\\
Another way to get estimates is by \textbf{Maximum likelihood estimation}

we try to maximize its log

\[
\ln L = - \frac{N}{2} \ln (2\pi) - \frac{1}{2}\ln|M| - \frac{1}{2} \mathbf{(Y-X \beta)'\Sigma^{-1}(Y-X\beta)}
\]

\hypertarget{nonparametric-anova}{%
\section{Nonparametric ANOVA}\label{nonparametric-anova}}

\hypertarget{kruskal-wallis}{%
\subsection{Kruskal-Wallis}\label{kruskal-wallis}}

Generalization of independent samples Wilcoxon Rank sum test for 2 independent samples (like F-test of one-way ANOVA is a generalization to several independent samples of the two sample t-test)

Consider the one-way case:

We have

\begin{itemize}
\tightlist
\item
  \(a\ge2\) treatments
\item
  \(n_i\) is the sample size for the \(i\)-th treatment
\item
  \(Y_{ij}\) is the \(j\)-th observation from the \(i\)-th treatment.
\item
  we make \textbf{no} assumption of normality
\item
  We only assume that observations on the \(i\)-th treatment are a random sample from the continuous CDF \(F_i\), i = 1,..,n, and are mutually independent.
\end{itemize}

\[
\begin{aligned}
&H_0: F_1 = F_2 = ... = F_a \\
&H_a: F_i < F_j \text{ for some } i \neq j
\end{aligned}
\]

or if distribution is from the location-scale family, \(H_0: \theta_1 = \theta_2 = ... = \theta_a\))

\textbf{Procedure}

\begin{itemize}
\tightlist
\item
  Rank all \(N = \sum_{i=1}^a n_i\) observations in ascending order. Let \(r_{ij} = rank(Y_{ij})\), note \(\sum_i \sum_j r_{ij} = 1 + 2 .. + N = \frac{N(N+1)}{2}\)\\
\item
  Calculate the rank sums and averages:\\
  \[
  r_{i.} = \sum_{j=1}^{n_i} r_{ij}
  \] and \[
  \bar{r}_{i.} = \frac{r_{i.}}{n_i}, i = 1,..,a
  \]
\item
  Calculate the test statistic on the ranks: \[
  \chi_{KW}^2 = \frac{SSTR}{\frac{SSTO}{N-1}}
  \] where \(SSTR = \sum n_i (\bar{r}_{i.}- \bar{r}_{..})^2\) and \(SSTO = \sum \sum (\bar{r}_{ij}- \bar{r}_{..})^2\)
\item
  For large \(n_i\) (\(\ge 5\) observations) the Kruskal-Wallis statistic is approximated by a \(\chi^2_{a-1}\) distribution when all the treatment means are equal. Hence, reject \(H_0\) if \(\chi^2_{KW} > \chi^2_{(1-\alpha;a-1)}\).\\
\item
  If sample sizes are small, one can exhaustively work out all possible distinct ways of assigning N ranks to the observations from a treatments and calculate the value of the KW statistic in each case (\(\frac{N!}{n_1!..n_a!}\) possible combinations). Under \(H_0\) all of these assignments are equally likely.
\end{itemize}

\hypertarget{friedman-test}{%
\subsection{Friedman Test}\label{friedman-test}}

When the responses \(Y_{ij} = 1,..,n, j = 1,..,r\) in a randomized complete block design are not normally distributed (or do not have constant variance), a nonparametric test is more helpful.

A distribution-free rank-based test for comparing the treatments in this setting is the Friedman test. Let \(F_{ij}\) be the CDF of random \(Y_{ij}\), corresponding to the observed value \(y_{ij}\)

Under the null hypothesis, \(F_{ij}\) are identical for all treatments j separately for each block i.

\[
\begin{aligned}
&H_0: F_{i1} = F_{i2} = ... = F_{ir}  \text{ for all i} \\
&H_a: F_{ij} < F_{ij'} \text{ for some } j \neq j' \text{ for all } i
\end{aligned}
\]

For location parameter distributions, treatment effects can be tested:

\[
\begin{aligned}
&H_0: \tau_1 = \tau_2 = ... = \tau_r \\
&H_a: \tau_j > \tau_{j'} \text{ for some } j \neq j'
\end{aligned}
\]

\textbf{Procedure}

\begin{itemize}
\tightlist
\item
  Rank observations from the r treatments separately within each block (in ascending order; if ties, each tied observation is given the mean of ranks involved). Let the ranks be called \(r_{ij}\)\\
\item
  Calculate the Friedman test statistic\\
  \[
  \chi^2_F = \frac{SSTR}{\frac{SSTR + SSE}{n(r-1)}}
  \] where \[
  \begin{aligned}
  SSTR &= n \sum (\bar{r}_{.j}-\bar{r}_{..})^2 \\
  SSE &= \sum \sum (r_{ij} - \bar{r}_{.j})^2 \\
  \bar{r}_{.j} &= \frac{\sum_i r_{ij}}{n}\\
  \bar{r}_{..} &= \frac{r+1}{2}
  \end{aligned}
  \]
\end{itemize}

If there is no ties, it can be rewritten as

\[
\chi^2_{F} = [\frac{12}{nr(n+1)}\sum_j r_{.j}^2] - 3n(r+1)
\]

with large number of blocks, \(\chi^2_F\) is approximately \(\chi^2_{r-1}\) under \(H_0\). Hence, we reject \(H_0\) if \(\chi^2_F > \chi^2_{(1-\alpha;r-1)}\)\\
The exact null distribution for \(\chi^2_F\) can be derived since there are r! possible ways of assigning ranks 1,2,\ldots,r to the r observations within each block. There are n blocks and thus \((r!)^n\) possible assignments to the ranks, which are equally likely when \(H_0\) is true.

\hypertarget{sample-size-planning-for-anova}{%
\section{Sample Size Planning for ANOVA}\label{sample-size-planning-for-anova}}

\hypertarget{balanced-designs}{%
\subsection{Balanced Designs}\label{balanced-designs}}

\hypertarget{single-factor-studies}{%
\subsubsection{Single Factor Studies}\label{single-factor-studies}}

\hypertarget{fixed-cell-means}{%
\paragraph{Fixed cell means}\label{fixed-cell-means}}

\[
P(F>f_{(1-\alpha;a-1,N-a)}|\phi) = 1 - \beta
\]

where \(\phi\) is the non-centrality \textbf{parameter} (measures how unequal the treatment means \(\mu_i\) are)

\[
\phi = \frac{1}{\sigma}\sqrt{\frac{n}{a}\sum_i (\mu_i - \mu_.)^2} , (n_i \equiv n)
\]

and

\[
\mu_. = \frac{\sum \mu_i}{a}
\]

To decide on the power probabilities we use the non-central F distribution.

We could use the power table directly when effects are fixed and design is balanced by using \textbf{minimum range} of factor level means for your desired differences

\[
\Delta = \max(\mu_i) - \min(\mu_i)
\]

Hence, we need

\begin{itemize}
\tightlist
\item
  \(\alpha\) level
\item
  \(\Delta\)
\item
  \(\sigma\)
\item
  \(\beta\)
\end{itemize}

Notes:

\begin{itemize}
\tightlist
\item
  When \(\Delta/\sigma\) is small greatly affects sample size, but if \(\Delta/\sigma\) is large.
\item
  Reducing \(\alpha\) or \(\beta\) increases the required sample sizes.
\item
  Error in estimating \(\sigma\) can make a large difference.
\end{itemize}

\hypertarget{multi-factor-studies}{%
\subsubsection{Multi-factor Studies}\label{multi-factor-studies}}

The same noncentral \(F\) tables can be used here

For two-factor fixed effect model

Test for interactions:

\[
\begin{aligned}
\phi &= \frac{1}{\sigma} \sqrt{\frac{n \sum \sum (\alpha \beta_{ij})^2}{(a-1)(b-1)+1}} = \frac{1}{\sigma} \sqrt{\frac{n \sum \sum (\mu_{ij}- \mu_{i.} - \mu_{.j} + \mu_{..})^2}{(a-1)(b-1)+1}} \\
\upsilon_1 &= (a-1)(b-1) \\
\upsilon_2 &= ab(n-1)
\end{aligned}
\]

Test for Factor \(A\) main effects:

\[
\begin{aligned}
\phi &= \frac{1}{\sigma} \sqrt{\frac{nb \sum \alpha_i^2}{a}} = \frac{1}{\sigma}\sqrt{\frac{nb \sum (\mu_{i.}- \mu_{..})^2}{a}} \\
\upsilon_1 &= a-1 \\
\upsilon_2 &= ab(n-1)
\end{aligned}
\]

Test for Factor \(B\) main effects:

\[
\begin{aligned}
\phi &= \frac{1}{\sigma} \sqrt{\frac{na \sum \beta_j^2}{b}} = \frac{1}{\sigma}\sqrt{\frac{na \sum (\mu_{.j}- \mu_{..})^2}{b}} \\
\upsilon_1 &= b-1 \\
\upsilon_2 &= ab(n-1)
\end{aligned}
\]

Procedure:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Specify the minimum range of Factor \(A\) means
\item
  Obtain sample sizes with \(r = a\). The resulting sample size is \(bn\), from which \(n\) can be obtained.
\item
  Repeat the first 2 steps for Factor \(B\) minimum range.
\item
  Choose the greater number of sample size between \(A\) and \(B\).
\end{enumerate}

\hypertarget{randomized-block-experiments}{%
\subsection{Randomized Block Experiments}\label{randomized-block-experiments}}

Analogous to completely randomized designs . The power of the F-test for treatment effects for randomized block design uses the same non-centrality parameter as completely randomized design:

\[
\phi = \frac{1}{\sigma} \sqrt{\frac{n}{r} \sum (\mu_i - \mu_.)^2}
\]

However, the power level is different from the randomized block design because

\begin{itemize}
\tightlist
\item
  error variance \(\sigma^2\) is different
\item
  df(MSE) is different.
\end{itemize}

\hypertarget{randomized-block-designs}{%
\section{Randomized Block Designs}\label{randomized-block-designs}}

To improve the precision of treatment comparisons, we can reduce variability among the experimental units. We can group experimental units into \textbf{blocks} so that each block contains relatively homogeneous units.

\begin{itemize}
\tightlist
\item
  Within each block, random assignment treatments to units (separate random assignment for each block)
\item
  The number of units per block is a multiple of the number of factor combinations.
\item
  Commonly, use each treatment once in each block.
\end{itemize}

Benefits of \textbf{Blocking}

\begin{itemize}
\item
  Reduction in variability of estimators for treatment means

  \begin{itemize}
  \tightlist
  \item
    Improved power for t-tests and F-tests
  \item
    Narrower confidence intervals
  \item
    Smaller MSE
  \end{itemize}
\item
  Compare treatments under different conditions (related to different blocks).
\end{itemize}

Loss from \textbf{Blocking} (little to lose)

\begin{itemize}
\tightlist
\item
  If you don't do blocking well, you waste df on negligible block effects that could have been used to estimate \(\sigma^2\)
\item
  Hence, the df for \(t\)-tests and denominator df for \(F\)-tests will be reduced without reducing MSE and small loss of power for both tests.
\end{itemize}

Consider

\[
Y_{ij} = \mu_{..} + \rho_i + \tau_j + \epsilon_{ij}
\]

where

\begin{itemize}
\tightlist
\item
  \(i = 1, 2, \dots, n\)
\item
  \(j = 1, 2, \dots, r\)
\item
  \(\mu_{..}\): overall mean response, averaging across all blocks and treatments
\item
  \(\rho_i\): block effect, average difference in response for i-th block (\(\sum \rho_i =0\))
\item
  \(\tau_j\) treatment effect, average across blocks (\(\sum \tau_j = 0\))
\item
  \(\epsilon_{ij} \sim iid N(0,\sigma^2)\): random experimental error.
\end{itemize}

Here, we assume that the block and treatment effects are additive. The difference in average response for any pair of treatments i the same \textbf{within} each block

\[
(\mu_{..} +  \rho_i + \tau_j) - (\mu_{..} + \rho_i + \tau_j') = \tau_j - \tau_j'
\]

for all \(i=1,..,n\) blocks

\[
\begin{aligned}
\hat{\mu} &= \bar{Y}_{..} \\
\hat{\rho}_i &= \bar{Y}_{i.} - \bar{Y}_{..} \\
\hat{\tau}_j &= \bar{Y}_{.j} - \bar{Y}_{..}
\end{aligned}
\]

Hence,

\[
\begin{aligned}
\hat{Y}_{ij} &= \bar{Y}_{..} + (\bar{Y}_{i.} - \bar{Y}_{..}) + (\bar{Y}_{.j}- \bar{Y}_{..}) = \bar{Y}_{i.} + \bar{Y}_{.j} - \bar{Y}_{..} \\
e_{ij} &= Y_{ij} - \hat{Y}_{ij} = Y_{ij}- \bar{Y}_{i.} - \bar{Y}_{.j} + \bar{Y}_{..}
\end{aligned}
\]

\textbf{ANOVA table}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1063}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.3913}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.0725}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2077}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2077}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Source of Variation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SS
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
df
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Fixed Treatments

E(MS)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Random Treatments

E(MS)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Blocks & \(r \sum_i(\bar{Y}_{i.}-\bar{Y}_{..})^2\) & \(n - 1\) & \(\sigma^2 +r \frac{\sum \rho^2_i}{n-1}\) & \(\sigma^2 + r \frac{\sum \rho^2_i}{n-1}\) \\
Treatments & \(n\sum_ j (\bar{Y} _ {.j}-\bar{ Y}_{..})^2\) & \(r - 1\) & \(\sigma^2 + n \frac{\sum \tau^2_j}{r-1}\) & \(\sigma^2 + n \sigma^2_\tau\) \\
Error & \(\sum_i \sum _j ( Y_{ ij } - \bar { Y}_{i.} - \bar{Y}_{.j} + \bar{ Y}_{..})^2\) & \((n-1)(r-1)\) & \(\sigma^2\) & \(\sigma^2\) \\
Total & \(SSTO\) & \(nr-1\) & & \\
\end{longtable}

\textbf{F-tests}

\[
\begin{aligned}
H_0: \tau_1 = \tau_2 = ... = \tau_r = 0 && \text{Fixed Treatment Effects} \\
H_a: \text{not all } \tau_j = 0 \\
\\
H_0: \sigma^2_{\tau} = 0 && \text{Random Treatment Effects} \\
H_a: \sigma^2_{\tau} \neq 0 
\end{aligned}
\]

In both cases \(F = \frac{MSTR}{MSE}\), reject \(H_0\) if \(F > f_{(1-\alpha; r-1,(n-1)(r-1))}\)

we don't use F-test to compare blocks, because

\begin{itemize}
\tightlist
\item
  We have a priori that blocs are different\\
\item
  Randomization is done ``within'' block.
\end{itemize}

To estimate the efficiency that was gained by blocking (relative to completely randomized design).

\[
\begin{aligned}
\hat{\sigma}^2_{CR} &= \frac{(n-1)MSBL + n(r-1)MSE}{nr-1} \\
\hat{\sigma}^2_{RB} &= MSE \\
\frac{\hat{\sigma}^2_{CR}}{\hat{\sigma}^2_{RB}} &= \text{above 1} \\
\end{aligned}
\]

then a completely randomized experiment would

\[
(\frac{\hat{\sigma}^2_{CR}}{\hat{\sigma}^2_{RB}}-1)\%%
\]

more observations than the randomized block design to get the same MSE

If batches are randomly selected then they are random effects. That is , if the experiment was repeated, a new sample of i batches would be selected,d yielding new values for \(\rho_1, \rho_2,...,\rho_i\) then.

\[
\rho_1, \rho_2,...,\rho_j \sim N(0,\sigma^2_\rho)
\]

Then,

\[
Y_{ij} = \mu_{..} + \rho_i + \tau_j + \epsilon_{ij}
\]

where

\begin{itemize}
\tightlist
\item
  \(\mu_{..}\) fixed
\item
  \(\rho_i\): random iid \(N(0,\sigma^2_p)\)
\item
  \(\tau_j\) fixed (or random) \(\sum \tau_j = 0\)
\item
  \(\epsilon_{ij} \sim iid N(0,\sigma^2)\)
\end{itemize}

\textbf{Fixed Treatment}

\[
\begin{aligned}
E(Y_{ij}) &= \mu_{..} + \tau_j \\
var(Y_{ij}) &= \sigma^2_{\rho} + \sigma^2
\end{aligned}
\]

\[
\begin{aligned}
cov(Y_{ij},Y_{ij'}) &= \sigma^2 , j \neq j' \text{ treatments within same block are correlated} \\
cov(Y_{ij},Y_{i'j'}) &= 0 , i \neq i' , j \neq j'
\end{aligned}
\]

Correlation between 2 observations in the same block

\[
\frac{\sigma^2_{\rho}}{\sigma^2 + \sigma^2_{\rho}}
\]

The expected MS for the additive fixed treatment effect, random block effect is

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Source & SS & E(MS) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Blocks & SSBL & \(\sigma^2 + r \sigma^2_\rho\) \\
Treatment & SSTR & \(\sigma^2 + n \frac{\sum \tau^2_j}{r-1}\) \\
Error & SSE & \(\sigma^2\) \\
\end{longtable}

\textbf{Interactions and Blocks}\\
without replications within each block for each treatment, we can't consider interaction between block and treatment when the block effect is fixed. Hence, only in the random block effect, we have

\[
Y_{ij} = \mu_{..} + \rho_i + \tau_j + (\rho \tau)_{ij} + \epsilon_{ij}
\]

where

\begin{itemize}
\tightlist
\item
  \(\mu_{..}\) constant
\item
  \(\rho_i \sim idd N(0,\sigma^2_{\rho})\) random
\item
  \(\tau_j\) fixed (\(\sum \tau_j = 0\))
\item
  \((\rho \tau)_{ij} \sim N(0,\frac{r-1}{r}\sigma^2_{\rho \tau})\) with \(\sum_j (\rho \tau)_{ij}=0\) for all i
\item
  \(cov((\rho \tau)_{ij},(\rho \tau)_{ij'})= -\frac{1}{r} \sigma^2_{\rho \tau}\) for \(j \neq j'\)
\item
  \(\epsilon_{ij} \sim iid N(0,\sigma^2)\) random
\end{itemize}

Note: a special case of mixed 2-factor model with 1 observation per ``cell''

\[
\begin{aligned}
E(Y_{ij}) &= \mu_{..} + \tau_j \\
var(Y_{ij}) &= \sigma^2_\rho + \frac{r-1}{r} \sigma^2_{\rho \tau} + \sigma^2
\end{aligned}
\]

\[
\begin{aligned}
cov(Y_{ij},Y_{ij'}) &= \sigma^2_\rho - \frac{1}{r} \sigma^2_{\rho \tau}, j \neq j' \text{ obs from the same block are correlated} \\
cov(Y_{ij},Y_{i'j'}) &= 0, i \neq i', j \neq j' \text{ obs from different blocks are independent}
\end{aligned}
\]

The sum of squares and degrees of freedom for interaction model are the same as those for the additive model. The difference exists only with the expected mean squares

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1132}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.0943}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1415}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.6321}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Source
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SS
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
df
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
E(MS)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Blocks & \(SSBL\) & \(n-1\) & \(\sigma^2 + r \sigma^2_\rho\) \\
Treatment & \(SSTR\) & \(r -1\) & \(\sigma^2 + \sigma ^2_{\rho \tau} + n \frac{\sum \tau_j^2}{r-1}\) \\
Error & \(SSE\) & \((n-1)(r-1)\) & \(\sigma^2 + \sigma ^2_{\rho \tau}\) \\
\end{longtable}

\begin{itemize}
\tightlist
\item
  No exact test is possible for block effects when interaction is present (Not important if blocks are used primarily to reduce experimental error variability)\\
\item
  \(E(MSE) = \sigma^2 + \sigma^2_{\rho \tau}\) the error term variance and interaction variance \(\sigma^2_{\rho \tau}\). We can't estimate these components separately with this model. The two are \textbf{confounded}.\\
\item
  If more than 1 observation per treatment block combination, one can consider interaction with fixed block effects, which is called \textbf{generalized randomized block designs} (multifactor analysis).
\end{itemize}

\hypertarget{tukey-test-of-additivity}{%
\subsection{Tukey Test of Additivity}\label{tukey-test-of-additivity}}

(Tukey's 1 df test for additivity)

formal test of interaction effects between blocks and treatments for a randomized block design. can also considered for testing additivity in 2-way analyses when there is only one observation per cell.

we consider a less restricted interaction term

\[
(\rho \tau)_{ij} = D\rho_i \tau_j \text{(D: Constant)}
\]

So,

\[
Y_{ij} = \mu_{..} + \rho_i + \tau_j + D\rho_i \tau_j + \epsilon_{ij}
\]

the least square estimate or MLE for D

\[
\hat{D} = \frac{\sum_i \sum_j \rho_i \tau_j Y_{ij}}{\sum_i \rho_i^2 \sum_j \tau^2_j}
\]

replacing the parameters by their estimates

\[
\hat{D} = \frac{\sum_i \sum_j (\bar{Y}_{i.}- \bar{Y}_{..})(\bar{Y}_{.j}- \bar{Y}_{..})Y_{ij}}{\sum_i (\bar{Y}_{i.}- \bar{Y}_{..})^2 \sum_j(\bar{Y}_{.j}- \bar{Y}_{..})^2}
\]

Thus, the interaction sum of squares

\[
SSint = \sum_i \sum_j \hat{D}^2(\bar{Y}_{i.}- \bar{Y}_{..})^2(\bar{Y}_{.j}- \bar{Y}_{..})^2
\]

The ANOVA decomposition

\[
SSTO = SSBL + SSTR + SSint + SSRem
\]

where \(SSRem\): remainder sum of squares

\[
SSRem = SSTO - SSBL - SSTR - SSint
\]

if \(D = 0\) (i.e., no interactions of the type \(D \rho_i \tau_j\)). \(SSint\) and \(SSRem\) are independent \(\chi^2_{1,rn-r-n}\).

If \(D = 0\),

\[
F = \frac{SSint/1}{SSRem/(rn-r-n)} \sim f_{(1-\alpha;rn-r-n)}
\]

if

\[
\begin{aligned}
&H_0: D = 0 \text{ no interaction present} \\
&H_a: D \neq 0 \text{ interaction of form $D \rho_i \tau_j$ present}
\end{aligned}
\]

we reject \(H_0\) if \(F > f_{(1-\alpha;1,nr-r-n)}\)

\hypertarget{nested-designs}{%
\section{Nested Designs}\label{nested-designs}}

Let \(\mu_{ij}\) be the mean response when factor A is at the i-th level and factor B is at the j-th level.\\
If the factors are crossed, the \(j\)-th level of B is the same for all levels of A.\\
If factor B is nested within A, the j-th level of B when A is at level 1 has nothing in common with the j-th level of B when A is at level 2.

Factors that can't be manipulated are designated as \textbf{classification factors}, as opposed to \textbf{experimental factors} (i.e., you assign to the experimental units).

\hypertarget{two-factor-nested-designs}{%
\subsection{Two-Factor Nested Designs}\label{two-factor-nested-designs}}

\begin{itemize}
\tightlist
\item
  Consider B is nested within A.
\item
  both factors are fixed
\item
  All treatment means are equally important.
\end{itemize}

\textbf{Mean responses}

\[
\mu_{i.} = \sum_j \mu_{ij}/b
\]

Main effect factor A

\[
\alpha_i = \mu_{i.} - \mu_{..}
\]

where \(\mu_{..} = \frac{\mu_{ij}}{ab} = \frac{\sum_i \mu_{i.}}{a}\) and \(\sum_i \alpha_i = 0\)

Individual effects of \(B\) is denoted as \(\beta_{j(i)}\) where \(j(i)\) indicates the \(j\)-th level of factor \(B\) is nested within the it-h level of factor A

\[
\begin{aligned}
\beta_{j(i)} &= \mu_{ij} - \mu_{i.} \\
&= \mu_{ij} - \alpha_i - \mu_{..} \\
\sum_j \beta_{j(i)}&=0 , i = 1,...,a
\end{aligned}
\]

\(\beta_{j(i)}\) is the \textbf{specific effect} of the \(j\)-th level of factor \(B\) nested within the \(i\)-th level of factor \(A\). Hence,

\[
\mu_{ij} \equiv \mu_{..} + \alpha_i + \beta_{j(i)} \equiv \mu_{..} + (\mu_{i.} - \mu_{..}) + (\mu_{ij} - \mu_{i.})
\]

\textbf{Model}

\[
Y_{ijk} = \mu_{..} + \alpha_i + \beta_{j(i)} + \epsilon_{ijk}
\]

where

\begin{itemize}
\tightlist
\item
  \(Y_{ijk}\) response for the \(k\)-th treatment when factor \(A\) is at the \(i\)-th level and factor \(B\) is at the \(j\)-th level \((i = 1,..,a; j = 1,..,b; k = 1,..n)\)
\item
  \(\mu_{..}\) constant
\item
  \(\alpha_i\) constants subject to restriction \(\sum_i \alpha_i = 0\)
\item
  \(\beta_{j(i)}\) constants subject to restriction \(\sum_j \beta_{j(i)} = 0\) for all \(i\)
\item
  \(\epsilon_{ijk} \sim iid N(0,\sigma^2)\)
\end{itemize}

\[
\begin{aligned}
E(Y_{ijk}) &= \mu_{..} + \alpha_i + \beta_{j(i)} \\
var(Y_{ijk}) &= \sigma^2
\end{aligned}
\]

there is no interaction term in a nested model

\textbf{ANOVA for Two-Factor Nested Designs}

Least Squares and MLE estimates

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.3194}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.4722}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Parameter
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Estimator
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(\mu_{..}\) & \(\bar{Y}_{...}\) \\
\(\alpha_i\) & \(\bar{Y}_{i..} - \bar{Y}_{...}\) \\
\(\beta_{j(i)}\) & \(\bar{Y}_{ij.} - \bar{Y}_{i..}\) \\
\(\hat{Y}_{ijk}\) & \(\bar{Y}_{ij.}\) \\
\end{longtable}

residual \(e_{ijk} = Y_{ijk} - \bar{Y}_{ijk}\)

\[
\begin{aligned}
SSTO &= SSA + SSB(A) + SSE \\
\sum_i \sum_j \sum_k (Y_{ijk}- \bar{Y}_{...})^2 &= bn \sum_i (\bar{Y}_{i..}- \bar{Y}_{...})^2 + n \sum_i \sum_j (\bar{Y}_{ij.}- \bar{Y}_{i..})^2  \\
&+ \sum_i \sum_j \sum_k (Y_{ijk} -\bar{Y}_{ij.})^2
\end{aligned}
\]

ANOVA Table

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1789}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.0894}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.0976}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.0894}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.5203}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Source of Variation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SS
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
df
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MS
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
E(MS)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Factor A & \(SSA\) & \(a-1\) & \(MSA\) & \(\sigma^2 + bn \frac{\sum \alpha_i^2}{a-1}\) \\
Factor B & \(SSB(A)\) & \(a(b-1)\) & \(MSB(A)\) & \(\sigma^2 + n \frac{\  | | | | um \sum e ta_{i)}^ 2}{a(b-1)}\) \\
Error & \(SSE\) & \(ab(n-1)\) & \(MSE\) & \(\sigma^2\) \\
Total & \(SSTO\) & \(abn -1\) & & \\
\end{longtable}

\textbf{Tests For Factor Effects}

\[
\begin{aligned}
&H_0: \text{ All } \alpha_i =0 \\
&H_a: \text{ not all } \alpha_i = 0
\end{aligned}
\]

\(F = \frac{MSA}{MSE} \sim f_{(1-\alpha;a-1,(n-1)ab)}\) reject if \(F > f\)

\[
\begin{aligned}
&H_0: \text{ All } \beta_{j(i)} =0 \\
&H_a: \text{ not all } \beta_{j(i)} = 0
\end{aligned}
\]

\(F = \frac{MSB(A)}{MSE} \sim f_{(1-\alpha;a(b-1),(n-1)ab)}\) reject \(F>f\)

\textbf{Testing Factor Effect Contrasts}

\(L = \sum c_i \mu_i\) where \(\sum c_i =0\)

\[
\begin{aligned}
\hat{L} &= \sum c_i \bar{Y}_{i..} \\
\hat{L} &\pm t_{(1-\alpha/2;df)}s(\hat{L})
\end{aligned}
\]

where \(s^2(\hat{L}) = \sum c_i^2 s^2(\bar{Y}_{i..})\), where \(s^2(\bar{Y}_{i..}) = \frac{MSE}{bn}, df = ab(n-1)\)

\textbf{Testing Treatment Means}

\(L = \sum c_i \mu_{.j}\) estimated by \(\hat{L} = \sum c_i \bar{Y}_{ij}\) with confidence limits:

\[
\hat{L} \pm t_{(1-\alpha/2;(n-1)ab)}s(\hat{L})
\]

where

\[
s^2(\hat{L}) = \frac{MSE}{n}\sum c^2_i
\]

\textbf{Unbalanced Nested Two-Factor Designs}

If there are different number of levels of factor \(B\) for different levels of factor \(A\), then the design is called \textbf{unbalanced}

The model

\[
\begin{aligned}
Y_{ijk} &= \mu_{..} + \alpha_i + \beta_{j(i)} + \epsilon_{ijk} \\
\sum_{i=1}^2 \alpha_i &=0 \\
\sum_{j=1}^3 \beta_{j(1)} &= 0 \\
\sum_{j=1}^2 \beta_{j(2)}&=0
\end{aligned}
\]

where

\begin{itemize}
\item
  \(i = 1,2;j =1,..,b_i;k=1,..,n_{ij}\)
\item
  \(b_1 = 3, b_2= 2, n_{11} = n_{13} =2, n_{12}=1,n_{21} = n_{22} = 2\)
\item
  \(\alpha_1,\beta_{1(1)}, \beta_{2(1)}, \beta_{1(2)}\) are parameters.
\end{itemize}

And constraints: \(\alpha_2 = - \alpha_1, \beta_{3(1)}= - \beta_{1(1)}-\beta_{2(1)}, \beta_{2(2)}=-\beta_{1(2)}\)

4 indicator variables

\begin{equation}
X_1 = 
\begin{cases}
1&\text{if obs from school 1}\\
-1&\text{if obs from school 2}\\
\end{cases}
\end{equation}

\begin{equation}
X_2 = 
\begin{cases}
1&\text{if obs from instructor 1 in school 1}\\
-1&\text{if obs from instructor 3 in school 1}\\
0&\text{otherwise}\\
\end{cases}
\end{equation}

\begin{equation}
X_3 = 
\begin{cases}
1&\text{if obs from instructor 2 in school 1}\\
-1&\text{if obs from instructor 3 in school 1}\\
0&\text{otherwise}\\
\end{cases}
\end{equation}

\begin{equation}
X_4 = 
\begin{cases}
1&\text{if obs from instructor 1 in school 1}\\
-1&\text{if obs from instructor 2 in school 1}\\
0&\text{otherwise}\\
\end{cases}
\end{equation}

Regression Full Model

\[
Y_{ijk} = \mu_{..} + \alpha_1 X_{ijk1} + \beta_{1(1)}X_{ijk2} + \beta_{2(1)}X_{ijk3} + \beta_{1(2)}X_{ijk4} + \epsilon_{ijk}
\]

\textbf{Random Factor Effects}

If

\[
\begin{aligned}
\alpha_1 &\sim iid N(0,\sigma^2_\alpha) \\
\beta_{j(i)} &\sim iid N(0,\sigma^2_\beta)
\end{aligned}
\]

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1029}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4853}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4044}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Mean Square
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Expected Mean Squares

A fixed, B random
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Expected Mean Squares

A random, B random
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
MSA & \(\sigma^ 2 + n \sigma^2_\beta + bn \frac{\sum \alpha_i^2}{a-1}\) & \(\sigma^2 + bn \sigma^2_{\alpha} + n \sigma^2_\beta\) \\
MSB(A) & \(\sigma^2 + n \sigma^2_\beta\) & \(\sigma^2 + n \sigma^2_\beta\) \\
MSE & \(\sigma^2\) & \(\sigma^2\) \\
\end{longtable}

Test Statistics

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Factor A & \(\frac{MSA}{MSB(A)}\) & \(\frac{MSA}{MSB(A)}\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Factor B(A) & \(\frac{MSB(A)}{MSE}\) & \(\frac{MSB(A)}{MSE}\) \\
\end{longtable}

Another way to increase the precision of treatment comparisons by reducing variability is to use regression models to adjust for differences among experimental units (also known as \textbf{analysis of covariance}).

\hypertarget{single-factor-covariance-model}{%
\section{Single Factor Covariance Model}\label{single-factor-covariance-model}}

\[
Y_{ij} = \mu_{.} + \tau_i + \gamma(X_{ij} - \bar{X}_{..}) + \epsilon_{ij} 
\]

for \(i = 1,...,r;j=1,..,n_i\)

where

\begin{itemize}
\tightlist
\item
  \(\mu_.\) overall mean
\item
  \(\tau_i\): fixed treatment effects (\(\sum \tau_i =0\))
\item
  \(\gamma\): fixed regression coefficient effect between X and Y
\item
  \(X_{ij}\) covariate (not random)
\item
  \(\epsilon_{ij} \sim iid N(0,\sigma^2)\): random errors
\end{itemize}

If we just use \(\gamma X_{ij}\) as the regression term (rather than \(\gamma(X_{ij}-\bar{X}_{..})\)), then \(\mu_.\) is no longer the overall mean; thus we need to centered mean.

\[
\begin{aligned}
E(Y_{ij}) &= \mu_. + \tau_i + \gamma(X_{ij}-\bar{X}_{..}) \\
var(Y_{ij}) &= \sigma^2
\end{aligned}
\]

\(Y_{ij} \sim N(\mu_{ij},\sigma^2)\),

where

\[
\begin{aligned}
\mu_{ij} &= \mu_. + \tau_i + \gamma(X_{ij} - \bar{X}_{..}) \\
\sum \tau_i &=0 
\end{aligned}
\]

Thus, the mean response (\(\mu_{ij}\)) is a regression line with intercept \(\mu_. + \tau_i\) and slope \(\gamma\) for each treatment \$\$i.

\textbf{Assumption}:

\begin{itemize}
\tightlist
\item
  All treatment regression lines have the same slope\\
\item
  when treatment interact with covariate \(X\) (non-parallel slopes), covariance analysis is \textbf{not} appropriate. in which case we should use separate regression lines.
\end{itemize}

More complicated regression features (e.g., quadratic, cubic) or additional covariates e.g.,

\[
Y_{ij} = \mu_. + \tau_i + \gamma_1(X_{ij1}-\bar{X}_{..2}) + \gamma_2(X_{ij2}-\bar{X}_{..2}) + \epsilon_{ij}
\]

\textbf{Regression Formulation}

We can use indicator variables for treatments

\[
l_1 =
\begin{cases}
1 & \text{if case is from treatment 1}\\
-1 & \text{if case is from treatment r}\\
0 &\text{otherwise}\\
\end{cases}
\]

\[
.
\]

\[
.
\]

\[
l_{r-1} =
\begin{cases}
1 & \text{if case is from treatment r-1}\\
-1 & \text{if case is from treatment r}\\
0 &\text{otherwise}\\
\end{cases}
\]

Let \(x_{ij} = X_{ij}- \bar{X}_{..}\). the regression model is

\[
Y_{ij} = \mu_. + \tau_1l_{ij,1} + .. + \tau_{r-1}l_{ij,r-1} + \gamma x_{ij}+\epsilon_{ij}
\]

where \(I_{ij,1}\) is the indicator variable \(l_1\) for the j-th case from treatment i. The treatment effect \(\tau_1,..\tau_{r-1}\) are just regression coefficients for the indicator variables.

We could use the same diagnostic tools for this case.

\textbf{Inference}

Treatment effects

\[
\begin{aligned}
&H_0: \tau_1 = \tau_2 = ...= 0 \\
&H_a: \text{not all } \tau_i =0
\end{aligned}
\]

\[
\begin{aligned}
&\text{Full Model}: Y_{ij} = \mu_. + \tau_i + \gamma X_{ij} +\epsilon_{ij}  \\
&\text{Reduced Model}: Y_{ij} = \mu_. + \gamma X_{ij} + \epsilon_{ij}
\end{aligned}
\]

\[
F = \frac{SSE(R) - SSE(F)}{(N-2)-(N-(r+1))} / \frac{SSE(F)}{N-(r+1)} \sim F_{(r-1,N-(r+1))}
\]

If we are interested in comparisons of treatment effects.\\
For example, r - 3. We estimate \(\tau_1,\tau_2, \tau_3 = -\tau_1 - \tau_2\)

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1504}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2556}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5865}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Comparison
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Estimate
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Variance of Estimator
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(\tau_1 - \tau_2\) & \(\hat{\tau}_1 - \hat{\tau}_2\) & \(var(\hat {\tau}_1) + var(\hat{\tau}_2) - 2cov(\hat{ \tau}_1\hat{\tau}_2)\) \\
\(\tau_1 - \tau_3\) & \(2 \hat{\tau}_1 + \hat{\tau}_2\) & \(4var(\hat {\tau}_1) + var(\hat{\tau}_2) - 4cov(\hat{ \tau}_1\hat{\tau}_2)\) \\
\(\tau_2 - \tau_3\) & \(\hat{\tau}_1 + 2 \hat{\tau}_2\) & \(var(\hat{\tau}_1) + 4var(\hat{\tau}_2) - 4cov(\hat{\tau}_1\hat{\tau}_2)\) \\
\end{longtable}

Testing for Parallel Slopes

Example:

r = 3

\[
Y_{ij} = \mu_{.} + \tau_1 I_{ij,1} + \tau_2 I_{ij,2} + \gamma X_{ij} + \beta_1 I_{ij,1}X_{ij} + \beta_2 I_{ij,2}X_{ij} + \epsilon_{ij}
\]

where \(\beta_1,\beta_2\): interaction coefficients.

\[
\begin{aligned}
&H_0: \beta_1 = \beta_2 = 0 \\
&H_a: \text{at least one} \beta \neq 0 
\end{aligned}
\]

If we can't reject \(H_0\) using F-test then we have evidence that the slopes are parallel

\textbf{Adjusted Means}

The means in response after adjusting for the covariate effect

\[
Y_{i.}(adj) = \bar{Y}_{i.} - \hat{\gamma}(\bar{X}_{i.} - \bar{X}_{..})
\]

\hypertarget{multivariate-methods}{%
\chapter{Multivariate Methods}\label{multivariate-methods}}

\(y_1,...,y_p\) are possibly correlated random variables with means \(\mu_1,...,\mu_p\)

\[
\mathbf{y} = 
\left(
\begin{array}
{c}
y_1 \\
. \\
y_p \\
\end{array}
\right)
\]

\[
E(\mathbf{y}) = 
\left(
\begin{array}
{c}
\mu_1 \\
. \\
\mu_p \\
\end{array}
\right)
\]

Let \(\sigma_{ij} = cov(y_i, y_j)\) for \(i,j = 1,â€¦,p\)

\[
\mathbf{\Sigma} = (\sigma_{ij}) = 
\left(
\begin{array}
{cccc}
\sigma_{11} & \sigma_{22} & ... &  \sigma_{1p} \\
\sigma_{21} & \sigma_{22} & ... & \sigma_{2p} \\
. & . & . & . \\
\sigma_{p1} & \sigma_{p2} & ... & \sigma_{pp}
\end{array}
\right)
\]

where \(\mathbf{\Sigma}\) (symmetric) is the variance-covariance or dispersion matrix

Let \(\mathbf{u}_{p \times 1}\) and \(\mathbf{v}_{q \times 1}\) be random vectors with means \(\mu_u\) and \(\mu_v\) . Then

\[
\mathbf{\Sigma}_{uv} = cov(\mathbf{u,v}) = E[(\mathbf{u} - \mu_u)(\mathbf{v} - \mu_v)']
\]

in which \(\mathbf{\Sigma}_{uv} \neq \mathbf{\Sigma}_{vu}\) and \(\mathbf{\Sigma}_{uv} = \mathbf{\Sigma}_{vu}'\)

\hfill\break
\textbf{Properties of Covariance Matrices}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Symmetric \(\mathbf{\Sigma}' = \mathbf{\Sigma}\)
\item
  Non-negative definite \(\mathbf{a'\Sigma a} \ge 0\) for any \(\mathbf{a} \in R^p\), which is equivalent to eigenvalues of \(\mathbf{\Sigma}\), \(\lambda_1 \ge \lambda_2 \ge ... \ge \lambda_p \ge 0\)
\item
  \(|\mathbf{\Sigma}| = \lambda_1 \lambda_2 ... \lambda_p \ge 0\) (\textbf{generalized variance}) (the bigger this number is, the more variation there is
\item
  \(trace(\mathbf{\Sigma}) = tr(\mathbf{\Sigma}) = \lambda_1 + ... + \lambda_p = \sigma_{11} + ... + \sigma_{pp} =\) sum of variance (\textbf{total variance})
\end{enumerate}

Note:

\begin{itemize}
\tightlist
\item
  \(\mathbf{\Sigma}\) is typically required to be positive definite, which means all eigenvalues are positive, and \(\mathbf{\Sigma}\) has an inverse \(\mathbf{\Sigma}^{-1}\) such that \(\mathbf{\Sigma}^{-1}\mathbf{\Sigma} = \mathbf{I}_{p \times p} = \mathbf{\Sigma \Sigma}^{-1}\)
\end{itemize}

\textbf{Correlation Matrices}

\[
\rho_{ij} = \frac{\sigma_{ij}}{\sqrt{\sigma_{ii} \sigma_{jj}}}
\]

\[
\mathbf{R} = 
\left(
\begin{array}
{cccc}
\rho_{11} & \rho_{12} & ... & \rho_{1p} \\
\rho_{21} & \rho_{22} & ... & \rho_{2p} \\
. & . & . &. \\
\rho_{p1} & \rho_{p2} & ... & \rho_{pp} \\
\end{array}
\right)
\]

where \(\rho_{ij}\) is the correlation, and \(\rho_{ii} = 1\) for all i

Alternatively,

\[
\mathbf{R} = [diag(\mathbf{\Sigma})]^{-1/2}\mathbf{\Sigma}[diag(\mathbf{\Sigma})]^{-1/2}
\]

where \(diag(\mathbf{\Sigma})\) is the matrix which has the \(\sigma_{ii}\)'s on the diagonal and 0's elsewhere

and \(\mathbf{A}^{1/2}\) (the square root of a symmetric matrix) is a symmetric matrix such as \(\mathbf{A} = \mathbf{A}^{1/2}\mathbf{A}^{1/2}\)

\textbf{Equalities}

Let

\begin{itemize}
\item
  \(\mathbf{x}\) and \(\mathbf{y}\) be random vectors with means \(\mu_x\) and \(\mu_y\) and variance -variance matrices \(\mathbf{\Sigma}_x\) and \(\mathbf{\Sigma}_y\).
\item
  \(\mathbf{A}\) and \(\mathbf{B}\) be matrices of constants and \(\mathbf{c}\) and \(\mathbf{d}\) be vectors of constants
\end{itemize}

Then

\begin{itemize}
\item
  \(E(\mathbf{Ay + c} ) = \mathbf{A} \mu_y + c\)
\item
  \(var(\mathbf{Ay + c}) = \mathbf{A} var(\mathbf{y})\mathbf{A}' = \mathbf{A \Sigma_y A}'\)
\item
  \(cov(\mathbf{Ay + c, By+ d}) = \mathbf{A\Sigma_y B}'\)
\item
  \(E(\mathbf{Ay + Bx + c}) = \mathbf{A \mu_y + B \mu_x + c}\)
\item
  \(var(\mathbf{Ay + Bx + c}) = \mathbf{A \Sigma_y A' + B \Sigma_x B' + A \Sigma_{yx}B' + B\Sigma'_{yx}A'}\)
\end{itemize}

\textbf{Multivariate Normal Distribution}

Let \(\mathbf{y}\) be a multivariate normal (MVN) random variable with mean \(\mu\) and variance \(\mathbf{\Sigma}\). Then the density of \(\mathbf{y}\) is

\[
f(\mathbf{y}) = \frac{1}{(2\pi)^{p/2}|\mathbf{\Sigma}|^{1/2}} \exp(-\frac{1}{2} \mathbf{(y-\mu)'\Sigma^{-1}(y-\mu)} )
\]

\(\mathbf{y} \sim N_p(\mu, \mathbf{\Sigma})\)

\hypertarget{properties-of-mvn}{%
\subsection{Properties of MVN}\label{properties-of-mvn}}

\begin{itemize}
\item
  Let \(\mathbf{A}_{r \times p}\) be a fixed matrix. Then \(\mathbf{Ay} \sim N_r (\mathbf{A \mu, A \Sigma A'})\) . \(r \le p\) and all rows of \(\mathbf{A}\) must be linearly independent to guarantee that \(\mathbf{A \Sigma A}'\) is non-singular.
\item
  Let \(\mathbf{G}\) be a matrix such that \(\mathbf{\Sigma}^{-1} = \mathbf{GG}'\). Then \(\mathbf{G'y} \sim N_p(\mathbf{G' \mu, I})\) and \(\mathbf{G'(y-\mu)} \sim N_p (0,\mathbf{I})\)
\item
  Any fixed linear combination of \(y_1,...,y_p\) (say \(\mathbf{c'y}\)) follows \(\mathbf{c'y} \sim N_1 (\mathbf{c' \mu, c' \Sigma c})\)
\item
  Define a partition, \([\mathbf{y}'_1,\mathbf{y}_2']'\) where

  \begin{itemize}
  \item
    \(\mathbf{y}_1\) is \(p_1 \times 1\)
  \item
    \(\mathbf{y}_2\) is \(p_2 \times 1\),
  \item
    \(p_1 + p_2 = p\)
  \item
    \(p_1,p_2 \ge 1\) Then
  \end{itemize}
\end{itemize}

\[
\left(
\begin{array}
{c}
\mathbf{y}_1 \\
\mathbf{y}_2 \\
\end{array}
\right)
\sim
N
\left(
\left(
\begin{array}
{c}
\mu_1 \\
\mu_2 \\
\end{array}
\right),
\left(
\begin{array}
{cc}
\mathbf{\Sigma}_{11} & \mathbf{\Sigma}_{12} \\
\mathbf{\Sigma}_{21} & \mathbf{\Sigma}_{22}\\
\end{array}
\right)
\right)
\]

\begin{itemize}
\item
  The marginal distributions of \(\mathbf{y}_1\) and \(\mathbf{y}_2\) are \(\mathbf{y}_1 \sim N_{p1}(\mathbf{\mu_1, \Sigma_{11}})\) and \(\mathbf{y}_2 \sim N_{p2}(\mathbf{\mu_2, \Sigma_{22}})\)
\item
  Individual components \(y_1,...,y_p\) are all normally distributed \(y_i \sim N_1(\mu_i, \sigma_{ii})\)
\item
  The conditional distribution of \(\mathbf{y}_1\) and \(\mathbf{y}_2\) is normal

  \begin{itemize}
  \item
    \(\mathbf{y}_1 | \mathbf{y}_2 \sim N_{p1}(\mathbf{\mu_1 + \Sigma_{12} \Sigma_{22}^{-1}(y_2 - \mu_2),\Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \sigma_{21}})\)

    \begin{itemize}
    \tightlist
    \item
      In this formula, we see if we know (have info about) \(\mathbf{y}_2\), we can re-weight \(\mathbf{y}_1\) 's mean, and the variance is reduced because we know more about \(\mathbf{y}_1\) because we know \(\mathbf{y}_2\)
    \end{itemize}
  \item
    which is analogous to \(\mathbf{y}_2 | \mathbf{y}_1\). And \(\mathbf{y}_1\) and \(\mathbf{y}_2\) are independently distrusted only if \(\mathbf{\Sigma}_{12} = 0\)
  \end{itemize}
\item
  If \(\mathbf{y} \sim N(\mathbf{\mu, \Sigma})\) and \(\mathbf{\Sigma}\) is positive definite, then \(\mathbf{(y-\mu)' \Sigma^{-1} (y - \mu)} \sim \chi^2_{(p)}\)
\item
  If \(\mathbf{y}_i\) are independent \(N_p (\mathbf{\mu}_i , \mathbf{\Sigma}_i)\) random variables, then for fixed matrices \(\mathbf{A}_{i(m \times p)}\), \(\sum_{i=1}^k \mathbf{A}_i \mathbf{y}_i \sim N_m (\sum_{i=1}^{k} \mathbf{A}_i \mathbf{\mu}_i, \sum_{i=1}^k \mathbf{A}_i \mathbf{\Sigma}_i \mathbf{A}_i)\)
\end{itemize}

\textbf{Multiple Regression}

\[
\left(
\begin{array}
{c}
Y \\
\mathbf{x}
\end{array}
\right)
\sim 
N_{p+1}
\left(
\left[
\begin{array}
{c}
\mu_y \\
\mathbf{\mu}_x
\end{array}
\right]
,
\left[
\begin{array}
{cc}
\sigma^2_Y & \mathbf{\Sigma}_{yx} \\
\mathbf{\Sigma}_{yx} & \mathbf{\Sigma}_{xx}
\end{array}
\right]
\right)
\]

The conditional distribution of Y given x follows a univariate normal distribution with

\[
\begin{aligned}
E(Y| \mathbf{x}) &= \mu_y + \mathbf{\Sigma}_{yx} \Sigma_{xx}^{-1} (\mathbf{x}- \mu_x) \\
&= \mu_y - \Sigma_{yx} \Sigma_{xx}^{-1}\mu_x + \Sigma_{yx} \Sigma_{xx}^{-1}\mathbf{x} \\
&= \beta_0 + \mathbf{\beta'x}
\end{aligned} 
\]

where \(\beta = (\beta_1,...,\beta_p)' = \mathbf{\Sigma}_{xx}^{-1} \mathbf{\Sigma}_{yx}'\) (e.g., analogous to \(\mathbf{(x'x)^{-1}x'y}\) but not the same if we consider \(Y_i\) and \(\mathbf{x}_i\), \(i = 1,..,n\) and use the empirical covariance formula: \(var(Y|\mathbf{x}) = \sigma^2_Y - \mathbf{\Sigma_{yx}\Sigma^{-1}_{xx} \Sigma'_{yx}}\))

\textbf{Samples from Multivariate Normal Populations}

A random sample of size n, \(\mathbf{y}_1,.., \mathbf{y}_n\) from \(N_p (\mathbf{\mu}, \mathbf{\Sigma})\). Then

\begin{itemize}
\item
  Since \(\mathbf{y}_1,..., \mathbf{y}_n\) are iid, their sample mean, \(\bar{\mathbf{y}} = \sum_{i=1}^n \mathbf{y}_i/n \sim N_p (\mathbf{\mu}, \mathbf{\Sigma}/n)\). that is, \(\bar{\mathbf{y}}\) is an unbiased estimator of \(\mathbf{\mu}\)
\item
  The \(p \times p\) sample variance-covariance matrix, \(\mathbf{S}\) is \(\mathbf{S} = \frac{1}{n-1}\sum_{i=1}^n (\mathbf{y}_i - \bar{\mathbf{y}})(\mathbf{y}_i - \bar{\mathbf{y}})' = \frac{1}{n-1} (\sum_{i=1}^n \mathbf{y}_i \mathbf{y}_i' - n \bar{\mathbf{y}}\bar{\mathbf{y}}')\)

  \begin{itemize}
  \tightlist
  \item
    where \(\mathbf{S}\) is symmetric, unbiased estimator of \(\mathbf{\Sigma}\) and has \(p(p+1)/2\) random variables.
  \end{itemize}
\item
  \((n-1)\mathbf{S} \sim W_p (n-1, \mathbf{\Sigma})\) is a Wishart distribution with n-1 degrees of freedom and expectation \((n-1) \mathbf{\Sigma}\). The Wishart distribution is a multivariate extension of the Chi-squared distribution.
\item
  \(\bar{\mathbf{y}}\) and \(\mathbf{S}\) are independent
\item
  \(\bar{\mathbf{y}}\) and \(\mathbf{S}\) are sufficient statistics. (All of the info in the data about \(\mathbf{\mu}\) and \(\mathbf{\Sigma}\) is contained in \(\bar{\mathbf{y}}\) and \(\mathbf{S}\) , regardless of sample size).
\end{itemize}

\textbf{Large Sample Properties}

\(\mathbf{y}_1,..., \mathbf{y}_n\) are a random sample from some population with mean \(\mathbf{\mu}\) and variance-covariance matrix \(\mathbf{\Sigma}\)

\begin{itemize}
\item
  \(\bar{\mathbf{y}}\) is a consistent estimator for \(\mu\)
\item
  \(\mathbf{S}\) is a consistent estimator for \(\mathbf{\Sigma}\)
\item
  \textbf{Multivariate Central Limit Theorem}: Similar to the univariate case, \(\sqrt{n}(\bar{\mathbf{y}} - \mu) \dot{\sim} N_p (\mathbf{0,\Sigma})\) where n is large relative to p (\(n \ge 25p\)), which is equivalent to \(\bar{\mathbf{y}} \dot{\sim} N_p (\mu, \mathbf{\Sigma}/n)\)
\item
  \textbf{Wald's Theorem}: \(n(\bar{\mathbf{y}} - \mu)' \mathbf{S}^{-1} (\bar{\mathbf{y}} - \mu)\) when n is large relative to p.
\end{itemize}

Maximum Likelihood Estimation for MVN

Suppose iid \(\mathbf{y}_1 ,... \mathbf{y}_n \sim N_p (\mu, \mathbf{\Sigma})\), the likelihood function for the data is

\[
\begin{aligned}
L(\mu, \mathbf{\Sigma}) &= \prod_{j=1}^n (\frac{1}{(2\pi)^{p/2}|\mathbf{\Sigma}|^{1/2}} \exp(-\frac{1}{2}(\mathbf{y}_j -\mu)'\mathbf{\Sigma}^{-1})(\mathbf{y}_j -\mu)) \\
&= \frac{1}{(2\pi)^{np/2}|\mathbf{\Sigma}|^{n/2}} \exp(-\frac{1}{2} \sum_{j=1}^n(\mathbf{y}_j -\mu)'\mathbf{\Sigma}^{-1})(\mathbf{y}_j -\mu)
\end{aligned}
\]

Then, the MLEs are

\[
\hat{\mu} = \bar{\mathbf{y}}
\]

\[
\hat{\mathbf{\Sigma}} = \frac{n-1}{n} \mathbf{S}
\]

using derivatives of the log of the likelihood function with respect to \(\mu\) and \(\mathbf{\Sigma}\)

\textbf{Properties of MLEs}

\begin{itemize}
\item
  Invariance: If \(\hat{\theta}\) is the MLE of \(\theta\), then the MLE of \(h(\theta)\) is \(h(\hat{\theta})\) for any function h(.)
\item
  Consistency: MLEs are consistent estimators, but they are usually biased
\item
  Efficiency: MLEs are efficient estimators (no other estimator has a smaller variance for large samples)
\item
  Asymptotic normality: Suppose that \(\hat{\theta}_n\) is the MLE for \(\theta\) based upon n independent observations. Then \(\hat{\theta}_n \dot{\sim} N(\theta, \mathbf{H}^{-1})\)

  \begin{itemize}
  \item
    \(\mathbf{H}\) is the Fisher Information Matrix, which contains the expected values of the second partial derivatives fo the log-likelihood function. the (i,j)th element of \(\mathbf{H}\) is \(-E(\frac{\partial^2 l(\mathbf{\theta})}{\partial \theta_i \partial \theta_j})\)
  \item
    we can estimate \(\mathbf{H}\) by finding the form determined above, and evaluate it at \(\theta = \hat{\theta}_n\)
  \end{itemize}
\item
  Likelihood ratio testing: for some null hypothesis, \(H_0\) we can form a likelihood ratio test

  \begin{itemize}
  \item
    The statistic is: \(\Lambda = \frac{\max_{H_0}l(\mathbf{\mu}, \mathbf{\Sigma|Y})}{\max l(\mu, \mathbf{\Sigma | Y})}\)
  \item
    For large n, \(-2 \log \Lambda \sim \chi^2_{(v)}\) where v is the number of parameters in the unrestricted space minus the number of parameters under \(H_0\)
  \end{itemize}
\end{itemize}

\textbf{Test of Multivariate Normality}

\begin{itemize}
\item
  Check univariate normality for each trait (X) separately

  \begin{itemize}
  \item
    Can check \[Normality Assessment\]
  \item
    The good thing is that if any of the univariate trait is not normal, then the joint distribution is not normal (see again {[}m{]}). If a joint multivariate distribution is normal, then the marginal distribution has to be normal.
  \item
    However, marginal normality of all traits does not imply joint MVN
  \item
    Easily rule out multivariate normality, but not easy to prove it
  \end{itemize}
\item
  Mardia's tests for multivariate normality

  \begin{itemize}
  \item
    Multivariate skewness is\[
    \beta_{1,p} = E[(\mathbf{y}- \mathbf{\mu})' \mathbf{\Sigma}^{-1} (\mathbf{x} - \mathbf{\mu})]^3
    \]
  \item
    where \(\mathbf{x}\) and \(\mathbf{y}\) are independent, but have the same distribution (note: \(\beta\) here is not regression coefficient)
  \item
    Multivariate kurtosis is defined as
  \item
    \[
    \beta_{2,p} - E[(\mathbf{y}- \mathbf{\mu})' \mathbf{\Sigma}^{-1} (\mathbf{x} - \mathbf{\mu})]^2
    \]
  \item
    For the MVN distribution, we have \(\beta_{1,p} = 0\) and \(\beta_{2,p} = p(p+2)\)
  \item
    For a sample of size n, we can estimate

    \[
    \hat{\beta}_{1,p} = \frac{1}{n^2}\sum_{i=1}^n \sum_{j=1}^n g^2_{ij}
    \]

    \[
    \hat{\beta}_{2,p} = \frac{1}{n} \sum_{i=1}^n g^2_{ii}
    \]

    \begin{itemize}
    \tightlist
    \item
      where \(g_{ij} = (\mathbf{y}_i - \bar{\mathbf{y}})' \mathbf{S}^{-1} (\mathbf{y}_j - \bar{\mathbf{y}})\). Note: \(g_{ii} = d^2_i\) where \(d^2_i\) is the Mahalanobis distance
    \end{itemize}
  \item
    \citep{mardia1970measures} shows for large n

    \[
    \kappa_1 = \frac{n \hat{\beta}_{1,p}}{6} \dot{\sim} \chi^2_{p(p+1)(p+2)/6}
    \]

    \[
    \kappa_2 = \frac{\hat{\beta}_{2,p} - p(p+2)}{\sqrt{8p(p+2)/n}} \sim N(0,1)
    \]

    \begin{itemize}
    \item
      Hence, we can use \(\kappa_1\) and \(\kappa_2\) to test the null hypothesis of MVN.
    \item
      When the data are non-normal, normal theory tests on the mean are sensitive to \(\beta_{1,p}\) , while tests on the covariance are sensitive to \(\beta_{2,p}\)
    \end{itemize}
  \end{itemize}
\item
  Alternatively, Doornik-Hansen test for multivariate normality \citep{doornik2008omnibus}
\item
  Chi-square Q-Q plot

  \begin{itemize}
  \item
    Let \(\mathbf{y}_i, i = 1,...,n\) be a random sample sample from \(N_p(\mathbf{\mu}, \mathbf{\Sigma})\)
  \item
    Then \(\mathbf{z}_i = \mathbf{\Sigma}^{-1/2}(\mathbf{y}_i - \mathbf{\mu}), i = 1,...,n\) are iid \(N_p (\mathbf{0}, \mathbf{I})\). Thus, \(d_i^2 = \mathbf{z}_i' \mathbf{z}_i \sim \chi^2_p , i = 1,...,n\)
  \item
    plot the ordered \(d_i^2\) values against the qualities of the \(\chi^2_p\) distribution. When normality holds, the plot should approximately resemble a straight lien passing through the origin at a 45 degree
  \item
    it requires large sample size (i.e., sensitive to sample size). Even if we generate data from a MVN, the tail of the Chi-square Q-Q plot can still be out of line.
  \end{itemize}
\item
  If the data are not normal, we can

  \begin{itemize}
  \item
    ignore it
  \item
    use nonparametric methods
  \item
    use models based upon an approximate distribution (e.g., GLMM)
  \item
    try performing a transformation
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(heplots)}
\FunctionTok{library}\NormalTok{(ICSNP)}
\FunctionTok{library}\NormalTok{(MVN)}
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{trees }\OtherTok{=} \FunctionTok{read.table}\NormalTok{(}\StringTok{"images/trees.dat"}\NormalTok{)}
\FunctionTok{names}\NormalTok{(trees) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Nitrogen"}\NormalTok{,}\StringTok{"Phosphorous"}\NormalTok{,}\StringTok{"Potassium"}\NormalTok{,}\StringTok{"Ash"}\NormalTok{,}\StringTok{"Height"}\NormalTok{)}
\FunctionTok{str}\NormalTok{(trees)}
\CommentTok{\#\textgreater{} \textquotesingle{}data.frame\textquotesingle{}:    26 obs. of  5 variables:}
\CommentTok{\#\textgreater{}  $ Nitrogen   : num  2.2 2.1 1.52 2.88 2.18 1.87 1.52 2.37 2.06 1.84 ...}
\CommentTok{\#\textgreater{}  $ Phosphorous: num  0.417 0.354 0.208 0.335 0.314 0.271 0.164 0.302 0.373 0.265 ...}
\CommentTok{\#\textgreater{}  $ Potassium  : num  1.35 0.9 0.71 0.9 1.26 1.15 0.83 0.89 0.79 0.72 ...}
\CommentTok{\#\textgreater{}  $ Ash        : num  1.79 1.08 0.47 1.48 1.09 0.99 0.85 0.94 0.8 0.77 ...}
\CommentTok{\#\textgreater{}  $ Height     : int  351 249 171 373 321 191 225 291 284 213 ...}

\FunctionTok{summary}\NormalTok{(trees)}
\CommentTok{\#\textgreater{}     Nitrogen      Phosphorous       Potassium           Ash        }
\CommentTok{\#\textgreater{}  Min.   :1.130   Min.   :0.1570   Min.   :0.3800   Min.   :0.4500  }
\CommentTok{\#\textgreater{}  1st Qu.:1.532   1st Qu.:0.1963   1st Qu.:0.6050   1st Qu.:0.6375  }
\CommentTok{\#\textgreater{}  Median :1.855   Median :0.2250   Median :0.7150   Median :0.9300  }
\CommentTok{\#\textgreater{}  Mean   :1.896   Mean   :0.2506   Mean   :0.7619   Mean   :0.8873  }
\CommentTok{\#\textgreater{}  3rd Qu.:2.160   3rd Qu.:0.2975   3rd Qu.:0.8975   3rd Qu.:0.9825  }
\CommentTok{\#\textgreater{}  Max.   :2.880   Max.   :0.4170   Max.   :1.3500   Max.   :1.7900  }
\CommentTok{\#\textgreater{}      Height     }
\CommentTok{\#\textgreater{}  Min.   : 65.0  }
\CommentTok{\#\textgreater{}  1st Qu.:122.5  }
\CommentTok{\#\textgreater{}  Median :181.0  }
\CommentTok{\#\textgreater{}  Mean   :196.6  }
\CommentTok{\#\textgreater{}  3rd Qu.:276.0  }
\CommentTok{\#\textgreater{}  Max.   :373.0}
\FunctionTok{cor}\NormalTok{(trees, }\AttributeTok{method =} \StringTok{"pearson"}\NormalTok{) }\CommentTok{\# correlation matrix}
\CommentTok{\#\textgreater{}              Nitrogen Phosphorous Potassium       Ash    Height}
\CommentTok{\#\textgreater{} Nitrogen    1.0000000   0.6023902 0.5462456 0.6509771 0.8181641}
\CommentTok{\#\textgreater{} Phosphorous 0.6023902   1.0000000 0.7037469 0.6707871 0.7739656}
\CommentTok{\#\textgreater{} Potassium   0.5462456   0.7037469 1.0000000 0.6710548 0.7915683}
\CommentTok{\#\textgreater{} Ash         0.6509771   0.6707871 0.6710548 1.0000000 0.7676771}
\CommentTok{\#\textgreater{} Height      0.8181641   0.7739656 0.7915683 0.7676771 1.0000000}

\CommentTok{\# qq{-}plot }
\NormalTok{gg }\OtherTok{\textless{}{-}}\NormalTok{ trees }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{pivot\_longer}\NormalTok{(}\FunctionTok{everything}\NormalTok{(), }\AttributeTok{names\_to =} \StringTok{"Var"}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{"Value"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{sample =}\NormalTok{ Value)) }\SpecialCharTok{+}
    \FunctionTok{geom\_qq}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{geom\_qq\_line}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{facet\_wrap}\NormalTok{(}\StringTok{"Var"}\NormalTok{, }\AttributeTok{scales =} \StringTok{"free"}\NormalTok{)}
\NormalTok{gg}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{22-multivariate_files/figure-latex/unnamed-chunk-1-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Univariate normality}
\NormalTok{sw\_tests }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(trees, }\AttributeTok{MARGIN =} \DecValTok{2}\NormalTok{, }\AttributeTok{FUN =}\NormalTok{ shapiro.test)}
\NormalTok{sw\_tests}
\CommentTok{\#\textgreater{} $Nitrogen}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Shapiro{-}Wilk normality test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  newX[, i]}
\CommentTok{\#\textgreater{} W = 0.96829, p{-}value = 0.5794}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Phosphorous}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Shapiro{-}Wilk normality test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  newX[, i]}
\CommentTok{\#\textgreater{} W = 0.93644, p{-}value = 0.1104}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Potassium}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Shapiro{-}Wilk normality test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  newX[, i]}
\CommentTok{\#\textgreater{} W = 0.95709, p{-}value = 0.3375}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Ash}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Shapiro{-}Wilk normality test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  newX[, i]}
\CommentTok{\#\textgreater{} W = 0.92071, p{-}value = 0.04671}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Height}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Shapiro{-}Wilk normality test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  newX[, i]}
\CommentTok{\#\textgreater{} W = 0.94107, p{-}value = 0.1424}
\CommentTok{\# Kolmogorov{-}Smirnov test }
\NormalTok{ks\_tests }\OtherTok{\textless{}{-}} \FunctionTok{map}\NormalTok{(trees, }\SpecialCharTok{\textasciitilde{}} \FunctionTok{ks.test}\NormalTok{(}\FunctionTok{scale}\NormalTok{(.x),}\StringTok{"pnorm"}\NormalTok{))}
\NormalTok{ks\_tests}
\CommentTok{\#\textgreater{} $Nitrogen}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Asymptotic one{-}sample Kolmogorov{-}Smirnov test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  scale(.x)}
\CommentTok{\#\textgreater{} D = 0.12182, p{-}value = 0.8351}
\CommentTok{\#\textgreater{} alternative hypothesis: two{-}sided}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Phosphorous}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Asymptotic one{-}sample Kolmogorov{-}Smirnov test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  scale(.x)}
\CommentTok{\#\textgreater{} D = 0.17627, p{-}value = 0.3944}
\CommentTok{\#\textgreater{} alternative hypothesis: two{-}sided}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Potassium}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Asymptotic one{-}sample Kolmogorov{-}Smirnov test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  scale(.x)}
\CommentTok{\#\textgreater{} D = 0.10542, p{-}value = 0.9348}
\CommentTok{\#\textgreater{} alternative hypothesis: two{-}sided}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Ash}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Asymptotic one{-}sample Kolmogorov{-}Smirnov test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  scale(.x)}
\CommentTok{\#\textgreater{} D = 0.14503, p{-}value = 0.6449}
\CommentTok{\#\textgreater{} alternative hypothesis: two{-}sided}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Height}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Asymptotic one{-}sample Kolmogorov{-}Smirnov test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  scale(.x)}
\CommentTok{\#\textgreater{} D = 0.1107, p{-}value = 0.9076}
\CommentTok{\#\textgreater{} alternative hypothesis: two{-}sided}

\CommentTok{\# Mardia\textquotesingle{}s test, need large sample size for power}
\NormalTok{mardia\_test }\OtherTok{\textless{}{-}}
    \FunctionTok{mvn}\NormalTok{(}
\NormalTok{        trees,}
        \AttributeTok{mvnTest =} \StringTok{"mardia"}\NormalTok{,}
        \AttributeTok{covariance =} \ConstantTok{FALSE}\NormalTok{,}
        \AttributeTok{multivariatePlot =} \StringTok{"qq"}
\NormalTok{    )}

\NormalTok{mardia\_test}\SpecialCharTok{$}\NormalTok{multivariateNormality}
\CommentTok{\#\textgreater{}              Test         Statistic            p value Result}
\CommentTok{\#\textgreater{} 1 Mardia Skewness  29.7248528871795   0.72054426745778    YES}
\CommentTok{\#\textgreater{} 2 Mardia Kurtosis {-}1.67743173185383 0.0934580886477281    YES}
\CommentTok{\#\textgreater{} 3             MVN              \textless{}NA\textgreater{}               \textless{}NA\textgreater{}    YES}

\CommentTok{\# Doornik{-}Hansen\textquotesingle{}s test }
\NormalTok{dh\_test }\OtherTok{\textless{}{-}}
    \FunctionTok{mvn}\NormalTok{(}
\NormalTok{        trees,}
        \AttributeTok{mvnTest =} \StringTok{"dh"}\NormalTok{,}
        \AttributeTok{covariance =} \ConstantTok{FALSE}\NormalTok{,}
        \AttributeTok{multivariatePlot =} \StringTok{"qq"}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{22-multivariate_files/figure-latex/unnamed-chunk-1-2} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dh\_test}\SpecialCharTok{$}\NormalTok{multivariateNormality}
\CommentTok{\#\textgreater{}             Test        E df      p value MVN}
\CommentTok{\#\textgreater{} 1 Doornik{-}Hansen 161.9446 10 1.285352e{-}29  NO}

\CommentTok{\# Henze{-}Zirkler\textquotesingle{}s test }
\NormalTok{hz\_test }\OtherTok{\textless{}{-}}
    \FunctionTok{mvn}\NormalTok{(}
\NormalTok{        trees,}
        \AttributeTok{mvnTest =} \StringTok{"hz"}\NormalTok{,}
        \AttributeTok{covariance =} \ConstantTok{FALSE}\NormalTok{,}
        \AttributeTok{multivariatePlot =} \StringTok{"qq"}
\NormalTok{    )}
\NormalTok{hz\_test}\SpecialCharTok{$}\NormalTok{multivariateNormality}
\CommentTok{\#\textgreater{}            Test        HZ   p value MVN}
\CommentTok{\#\textgreater{} 1 Henze{-}Zirkler 0.7591525 0.6398905 YES}
\CommentTok{\# The last column indicates whether dataset follows a multivariate normality or not (i.e, YES or NO) at significance level 0.05.}

\CommentTok{\# Royston\textquotesingle{}s test}
\CommentTok{\# can only apply for 3 \textless{} obs \textless{} 5000 (because of Shapiro{-}Wilk\textquotesingle{}s test)}
\NormalTok{royston\_test }\OtherTok{\textless{}{-}}
    \FunctionTok{mvn}\NormalTok{(}
\NormalTok{        trees,}
        \AttributeTok{mvnTest =} \StringTok{"royston"}\NormalTok{,}
        \AttributeTok{covariance =} \ConstantTok{FALSE}\NormalTok{,}
        \AttributeTok{multivariatePlot =} \StringTok{"qq"}
\NormalTok{    )}
\NormalTok{royston\_test}\SpecialCharTok{$}\NormalTok{multivariateNormality}
\CommentTok{\#\textgreater{}      Test        H    p value MVN}
\CommentTok{\#\textgreater{} 1 Royston 9.064631 0.08199215 YES}


\CommentTok{\# E{-}statistic}
\NormalTok{estat\_test }\OtherTok{\textless{}{-}}
    \FunctionTok{mvn}\NormalTok{(}
\NormalTok{        trees,}
        \AttributeTok{mvnTest =} \StringTok{"energy"}\NormalTok{,}
        \AttributeTok{covariance =} \ConstantTok{FALSE}\NormalTok{,}
        \AttributeTok{multivariatePlot =} \StringTok{"qq"}
\NormalTok{    )}
\NormalTok{estat\_test}\SpecialCharTok{$}\NormalTok{multivariateNormality}
\CommentTok{\#\textgreater{}          Test Statistic p value MVN}
\CommentTok{\#\textgreater{} 1 E{-}statistic  1.091101   0.526 YES}
\end{Highlighting}
\end{Shaded}

\hypertarget{mean-vector-inference}{%
\subsection{Mean Vector Inference}\label{mean-vector-inference}}

In the univariate normal distribution, we test \(H_0: \mu =\mu_0\) by using

\[
T = \frac{\bar{y}- \mu_0}{s/\sqrt{n}} \sim t_{n-1}
\]

under the null hypothesis. And reject the null if \(|T|\) is large relative to \(t_{(1-\alpha/2,n-1)}\) because it means that seeing a value as large as what we observed is rare if the null is true

Equivalently,

\[
T^2 = \frac{(\bar{y}- \mu_0)^2}{s^2/n} = n(\bar{y}- \mu_0)(s^2)^{-1}(\bar{y}- \mu_0) \sim f_{(1,n-1)}
\]

\hypertarget{natural-multivariate-generalization}{%
\subsubsection{\texorpdfstring{\textbf{Natural Multivariate Generalization}}{Natural Multivariate Generalization}}\label{natural-multivariate-generalization}}

\[
\begin{aligned}
&H_0: \mathbf{\mu} = \mathbf{\mu}_0 \\
&H_a: \mathbf{\mu} \neq \mathbf{\mu}_0
\end{aligned}
\]

Define \textbf{Hotelling's} \(T^2\) by

\[
T^2 = n(\bar{\mathbf{y}} - \mathbf{\mu}_0)'\mathbf{S}^{-1}(\bar{\mathbf{y}} - \mathbf{\mu}_0)
\]

which can be viewed as a generalized distance between \(\bar{\mathbf{y}}\) and \(\mathbf{\mu}_0\)

Under the assumption of normality,

\[
F = \frac{n-p}{(n-1)p} T^2 \sim f_{(p,n-p)}
\]

and reject the null hypothesis when \(F > f_{(1-\alpha, p, n-p)}\)

\begin{itemize}
\item
  The \(T^2\) test is invariant to changes in measurement units.

  \begin{itemize}
  \tightlist
  \item
    If \(\mathbf{z = Cy + d}\) where \(\mathbf{C}\) and \(\mathbf{d}\) do not depend on \(\mathbf{y}\), then \(T^2(\mathbf{z}) - T^2(\mathbf{y})\)
  \end{itemize}
\item
  The \(T^2\) test can be derived as a \textbf{likelihood ratio} test of \(H_0: \mu = \mu_0\)
\end{itemize}

\hypertarget{confidence-intervals}{%
\subsubsection{Confidence Intervals}\label{confidence-intervals}}

\hypertarget{confidence-region}{%
\paragraph{Confidence Region}\label{confidence-region}}

An ``exact'' \(100(1-\alpha)\%\) confidence region for \(\mathbf{\mu}\) is the set of all vectors, \(\mathbf{v}\), which are ``close enough'' to the observed mean vector, \(\bar{\mathbf{y}}\) to satisfy

\[
n(\bar{\mathbf{y}} - \mathbf{\mu}_0)'\mathbf{S}^{-1}(\bar{\mathbf{y}} - \mathbf{\mu}_0) \le \frac{(n-1)p}{n-p} f_{(1-\alpha, p, n-p)}
\]

\begin{itemize}
\tightlist
\item
  \(\mathbf{v}\) are just the mean vectors that are not rejected by the \(T^2\) test when \(\mathbf{\bar{y}}\) is observed.
\end{itemize}

In case that you have 2 parameters, the confidence region is a ``hyper-ellipsoid''.

In this region, it consists of all \(\mathbf{\mu}_0\) vectors for which the \(T^2\) test would not reject \(H_0\) at significance level \(\alpha\)

Even though the confidence region better assesses the joint knowledge concerning plausible values of \(\mathbf{\mu}\) , people typically include confidence statement about the individual component means. We'd like all of the separate confidence statements to hold \textbf{simultaneously} with a specified high probability. Simultaneous confidence intervals: intervals \textbf{against} any statement being incorrect

\hypertarget{simultaneous-confidence-statements}{%
\subparagraph{Simultaneous Confidence Statements}\label{simultaneous-confidence-statements}}

\begin{itemize}
\tightlist
\item
  Intervals based on a rectangular confidence region by projecting the previous region onto the coordinate axes:
\end{itemize}

\[
\bar{y}_{i} \pm \sqrt{\frac{(n-1)p}{n-p}f_{(1-\alpha, p,n-p)}\frac{s_{ii}}{n}}
\]

for all \(i = 1,..,p\)

which implied confidence region is conservative; it has at least \(100(1- \alpha)\%\)

Generally, simultaneous \(100(1-\alpha) \%\) confidence intervals for all linear combinations , \(\mathbf{a}\) of the elements of the mean vector are given by

\[
\mathbf{a'\bar{y}} \pm \sqrt{\frac{(n-1)p}{n-p}f_{(1-\alpha, p,n-p)}\frac{\mathbf{a'Sa}}{n}}
\]

\begin{itemize}
\item
  works for any arbitrary linear combination \(\mathbf{a'\mu} = a_1 \mu_1 + ... + a_p \mu_p\), which is a projection onto the axis in the direction of \(\mathbf{a}\)
\item
  These intervals have the property that the probability that at least one such interval does not contain the appropriate \(\mathbf{a' \mu}\) is no more than \(\alpha\)
\item
  These types of intervals can be used for ``data snooping'' (like \[Scheffe\])
\end{itemize}

\hypertarget{one-mu-at-a-time}{%
\subparagraph{\texorpdfstring{One \(\mu\) at a time}{One \textbackslash mu at a time}}\label{one-mu-at-a-time}}

\begin{itemize}
\tightlist
\item
  One at a time confidence intervals:
\end{itemize}

\[
\bar{y}_i \pm t_{(1 - \alpha/2, n-1} \sqrt{\frac{s_{ii}}{n}}
\]

\begin{itemize}
\item
  Each of these intervals has a probability of \(1-\alpha\) of covering the appropriate \(\mu_i\)
\item
  But they ignore the covariance structure of the \(p\) variables
\item
  If we only care about \(k\) simultaneous intervals, we can use ``one at a time'' method with the \[Bonferroni\] correction.
\item
  This method gets more conservative as the number of intervals \(k\) increases.
\end{itemize}

\hypertarget{general-hypothesis-testing}{%
\subsection{General Hypothesis Testing}\label{general-hypothesis-testing}}

\hypertarget{one-sample-tests}{%
\subsubsection{One-sample Tests}\label{one-sample-tests}}

\[
H_0: \mathbf{C \mu= 0} 
\]

where

\begin{itemize}
\tightlist
\item
  \(\mathbf{C}\) is a \(c \times p\) matrix of rank c where \(c \le p\)
\end{itemize}

We can test this hypothesis using the following statistic

\[
F = \frac{n - c}{(n-1)c} T^2
\]

where \(T^2 = n(\mathbf{C\bar{y}})' (\mathbf{CSC'})^{-1} (\mathbf{C\bar{y}})\)

Example:

\[
H_0: \mu_1 = \mu_2 = ... = \mu_p
\]

Equivalently,

\[
\begin{aligned}
\mu_1 - \mu_2 &= 0 \\
&\vdots \\
\mu_{p-1} - \mu_p &= 0
\end{aligned}
\]

a total of \(p-1\) tests. Hence, we have \(\mathbf{C}\) as the \(p - 1 \times p\) matrix

\[
\mathbf{C} = 
\left(
\begin{array}
{ccccc}
1 & -1 & 0 & \ldots & 0 \\
0 & 1 & -1 & \ldots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & \ldots & 1 & -1 
\end{array}
\right)
\]

number of rows = \(c = p -1\)

Equivalently, we can also compare all of the other means to the first mean. Then, we test \(\mu_1 - \mu_2 = 0, \mu_1 - \mu_3 = 0,..., \mu_1 - \mu_p = 0\), the \((p-1) \times p\) matrix \(\mathbf{C}\) is

\[
\mathbf{C} = 
\left(
\begin{array}
{ccccc}
-1 & 1 & 0 & \ldots & 0 \\
-1 & 0 & 1 & \ldots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
-1 & 0 & \ldots & 0 & 1 
\end{array}
\right)
\]

The value of \(T^2\) is invariant to these equivalent choices of \(\mathbf{C}\)

This is often used for \textbf{repeated measures designs}, where each subject receives each treatment once over successive periods of time (all treatments are administered to each unit).

Example:

Let \(y_{ij}\) be the response from subject i at time j for \(i = 1,..,n, j = 1,...,T\). In this case, \(\mathbf{y}_i = (y_{i1}, ..., y_{iT})', i = 1,...,n\) are a random sample from \(N_T (\mathbf{\mu}, \mathbf{\Sigma})\)

Let \(n=8\) subjects, \(T = 6\). We are interested in \(\mu_1, .., \mu_6\)

\[
H_0: \mu_1 = \mu_2 = ... = \mu_6
\]

Equivalently,

\[
\begin{aligned}
\mu_1 - \mu_2 &= 0 \\
\mu_2 - \mu_3 &= 0 \\
&... \\
\mu_5  - \mu_6 &= 0
\end{aligned}
\]

We can test orthogonal polynomials for 4 equally spaced time points. To test for example the null hypothesis that quadratic and cubic effects are jointly equal to 0, we would define \(\mathbf{C}\)

\[
\mathbf{C} = 
\left(
\begin{array}
{cccc}
1 & -1 & -1 & 1 \\
-1 & 3 & -3 & 1
\end{array}
\right)
\]

\hypertarget{two-sample-tests}{%
\subsubsection{Two-Sample Tests}\label{two-sample-tests}}

Consider the analogous two sample multivariate tests.

Example: we have data on two independent random samples, one sample from each of two populations

\[
\begin{aligned}
\mathbf{y}_{1i} &\sim N_p (\mathbf{\mu_1, \Sigma}) \\
\mathbf{y}_{2j} &\sim N_p (\mathbf{\mu_2, \Sigma})
\end{aligned}
\]

We \textbf{assume}

\begin{itemize}
\item
  normality
\item
  equal variance-covariance matrices
\item
  independent random samples
\end{itemize}

We can summarize our data using the \textbf{sufficient statistics} \(\mathbf{\bar{y}}_1, \mathbf{S}_1, \mathbf{\bar{y}}_2, \mathbf{S}_2\) with respective sample sizes, \(n_1,n_2\)

Since we assume that \(\mathbf{\Sigma}_1 = \mathbf{\Sigma}_2 = \mathbf{\Sigma}\), compute a pooled estimate of the variance-covariance matrix on \(n_1 + n_2 - 2\) df

\[
\mathbf{S} = \frac{(n_1 - 1)\mathbf{S}_1 + (n_2-1) \mathbf{S}_2}{(n_1 -1) + (n_2 - 1)}
\]

\[
\begin{aligned}
&H_0: \mathbf{\mu}_1 = \mathbf{\mu}_2 \\
&H_a: \mathbf{\mu}_1 \neq \mathbf{\mu}_2
\end{aligned}
\]

At least one element of the mean vectors is different

We use

\begin{itemize}
\item
  \(\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2\) to estimate \(\mu_1 - \mu_2\)
\item
  \(\mathbf{S}\) to estimate \(\mathbf{\Sigma}\)

  Note: because we assume the two populations are independent, there is no covariance

  \(cov(\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2) = var(\mathbf{\bar{y}}_1) + var(\mathbf{\bar{y}}_2) = \frac{\mathbf{\Sigma_1}}{n_1} + \frac{\mathbf{\Sigma_2}}{n_2} = \mathbf{\Sigma}(\frac{1}{n_1} + \frac{1}{n_2})\)
\end{itemize}

Reject \(H_0\) if

\[
\begin{aligned}
T^2 &= (\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2)'\{ \mathbf{S} (\frac{1}{n_1} + \frac{1}{n_2})\}^{-1} (\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2)\\
&= \frac{n_1 n_2}{n_1 +n_2} (\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2)'\{ \mathbf{S} \}^{-1} (\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2)\\
& \ge \frac{(n_1 + n_2 -2)p}{n_1 + n_2 - p - 1} f_{(1- \alpha,n_1 + n_2 - p -1)}
\end{aligned}
\]

or equivalently, if

\[
F = \frac{n_1 + n_2 - p -1}{(n_1 + n_2 -2)p} T^2 \ge f_{(1- \alpha, p , n_1 + n_2 -p -1)}
\]

A \(100(1-\alpha) \%\) confidence region for \(\mu_1 - \mu_2\) consists of all vector \(\delta\) which satisfy

\[
\frac{n_1 n_2}{n_1 + n_2} (\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2 - \mathbf{\delta})' \mathbf{S}^{-1}(\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2 - \mathbf{\delta}) \le \frac{(n_1 + n_2 - 2)p}{n_1 + n_2 -p - 1}f_{(1-\alpha, p , n_1 + n_2 - p -1)}
\]

The simultaneous confidence intervals for all linear combinations of \(\mu_1 - \mu_2\) have the form

\[
\mathbf{a'}(\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2) \pm \sqrt{\frac{(n_1 + n_2 -2)p}{n_1 + n_2 - p -1}}f_{(1-\alpha, p, n_1 + n_2 -p -1)} \times \sqrt{\mathbf{a'Sa}(\frac{1}{n_1} + \frac{1}{n_2})}
\]

Bonferroni intervals, for k combinations

\[
(\bar{y}_{1i} - \bar{y}_{2i}) \pm t_{(1-\alpha/2k, n_1 + n_2 - 2)}\sqrt{(\frac{1}{n_1}  + \frac{1}{n_2})s_{ii}}
\]

\hypertarget{model-assumptions}{%
\subsubsection{Model Assumptions}\label{model-assumptions}}

If model assumption are not met

\begin{itemize}
\item
  Unequal Covariance Matrices

  \begin{itemize}
  \item
    If \(n_1 = n_2\) (large samples) there is little effect on the Type I error rate and power fo the two sample test
  \item
    If \(n_1 > n_2\) and the eigenvalues of \(\mathbf{\Sigma}_1 \mathbf{\Sigma}^{-1}_2\) are less than 1, the Type I error level is inflated
  \item
    If \(n_1 > n_2\) and some eigenvalues of \(\mathbf{\Sigma}_1 \mathbf{\Sigma}_2^{-1}\) are greater than 1, the Type I error rate is too small, leading to a reduction in power
  \end{itemize}
\item
  Sample Not Normal

  \begin{itemize}
  \item
    Type I error level of the two sample \(T^2\) test isn't much affect by moderate departures from normality if the two populations being sampled have similar distributions
  \item
    One sample \(T^2\) test is much more sensitive to lack of normality, especially when the distribution is skewed.
  \item
    Intuitively, you can think that in one sample your distribution will be sensitive, but the distribution of the difference between two similar distributions will not be as sensitive.
  \item
    Solutions:

    \begin{itemize}
    \item
      Transform to make the data more normal
    \item
      Large large samples, use the \(\chi^2\) (Wald) test, in which populations don't need to be normal, or equal sample sizes, or equal variance-covariance matrices

      \begin{itemize}
      \tightlist
      \item
        \(H_0: \mu_1 - \mu_2 =0\) use \((\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2)'( \frac{1}{n_1} \mathbf{S}_1 + \frac{1}{n_2}\mathbf{S}_2)^{-1}(\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2) \dot{\sim} \chi^2_{(p)}\)
      \end{itemize}
    \end{itemize}
  \end{itemize}
\end{itemize}

\hypertarget{equal-covariance-matrices-tests}{%
\paragraph{Equal Covariance Matrices Tests}\label{equal-covariance-matrices-tests}}

With independent random samples from k populations of \(p\)-dimensional vectors. We compute the sample covariance matrix for each, \(\mathbf{S}_i\), where \(i = 1,...,k\)

\[
\begin{aligned}
&H_0: \mathbf{\Sigma}_1 = \mathbf{\Sigma}_2 = \ldots = \mathbf{\Sigma}_k = \mathbf{\Sigma} \\
&H_a: \text{at least 2 are different}
\end{aligned}
\]

Assume \(H_0\) is true, we would use a pooled estimate of the common covariance matrix, \(\mathbf{\Sigma}\)

\[
\mathbf{S} = \frac{\sum_{i=1}^k (n_i -1)\mathbf{S}_i}{\sum_{i=1}^k (n_i - 1)}
\]

with \(\sum_{i=1}^k (n_i -1)\)

\hypertarget{bartletts-test}{%
\subparagraph{Bartlett's Test}\label{bartletts-test}}

(a modification of the likelihood ratio test). Define

\[
N = \sum_{i=1}^k n_i
\]

and (note: \(| |\) are determinants here, not absolute value)

\[
M = (N - k) \log|\mathbf{S}| - \sum_{i=1}^k (n_i - 1)  \log|\mathbf{S}_i|
\]

\[
C^{-1} = 1 - \frac{2p^2 + 3p - 1}{6(p+1)(k-1)} \{\sum_{i=1}^k (\frac{1}{n_i - 1}) - \frac{1}{N-k} \}
\]

\begin{itemize}
\item
  Reject \(H_0\) when \(MC^{-1} > \chi^2_{1- \alpha, (k-1)p(p+1)/2}\)
\item
  If not all samples are from normal populations, \(MC^{-1}\) has a distribution which is often shifted to the right of the nominal \(\chi^2\) distribution, which means \(H_0\) is often rejected even when it is true (the Type I error level is inflated). Hence, it is better to test individual normality first, or then multivariate normality before you do Bartlett's test.
\end{itemize}

\hypertarget{two-sample-repeated-measurements}{%
\subsubsection{Two-Sample Repeated Measurements}\label{two-sample-repeated-measurements}}

\begin{itemize}
\item
  Define \(\mathbf{y}_{hi} = (y_{hi1}, ..., y_{hit})'\) to be the observations from the i-th subject in the h-th group for times 1 through T
\item
  Assume that \(\mathbf{y}_{11}, ..., \mathbf{y}_{1n_1}\) are iid \(N_t(\mathbf{\mu}_1, \mathbf{\Sigma})\) and that \(\mathbf{y}_{21},...,\mathbf{y}_{2n_2}\) are iid \(N_t(\mathbf{\mu}_2, \mathbf{\Sigma})\)
\item
  \(H_0: \mathbf{C}(\mathbf{\mu}_1 - \mathbf{\mu}_2) = \mathbf{0}_c\) where \(\mathbf{C}\) is a \(c \times t\) matrix of rank \(c\) where \(c \le t\)
\item
  The test statistic has the form
\end{itemize}

\[
T^2 = \frac{n_1 n_2}{n_1 + n_2} (\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2)' \mathbf{C}'(\mathbf{CSC}')^{-1}\mathbf{C} (\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2)
\]

where \(\mathbf{S}\) is the pooled covariance estimate. Then,

\[
F = \frac{n_1 + n_2 - c -1}{(n_1 + n_2-2)c} T^2 \sim f_{(c, n_1 + n_2 - c-1)}
\]

when \(H_0\) is true

If the null hypothesis~\(H_0: \mu_1 = \mu_2\) is rejected. A weaker hypothesis is that the profiles for the two groups are parallel.

\[
\begin{aligned}
\mu_{11} - \mu_{21} &= \mu_{12} - \mu_{22} \\
&\vdots \\
\mu_{1t-1} - \mu_{2t-1} &= \mu_{1t} - \mu_{2t}
\end{aligned}
\]

The null hypothesis matrix term is then

\(H_0: \mathbf{C}(\mu_1 - \mu_2) = \mathbf{0}_c\) , where \(c = t - 1\) and

\[
\mathbf{C} = 
\left(
\begin{array}
{ccccc}
1 & -1 & 0 & \ldots & 0 \\
0 & 1 & -1 & \ldots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \ldots & -1 
\end{array}
\right)_{(t-1) \times t}
\]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# One{-}sample Hotelling\textquotesingle{}s T\^{}2 test}
\CommentTok{\#  Create data frame}
\NormalTok{plants }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
    \AttributeTok{y1 =} \FunctionTok{c}\NormalTok{(}\FloatTok{2.11}\NormalTok{, }\FloatTok{2.36}\NormalTok{, }\FloatTok{2.13}\NormalTok{, }\FloatTok{2.78}\NormalTok{, }\FloatTok{2.17}\NormalTok{),}
    \AttributeTok{y2 =} \FunctionTok{c}\NormalTok{(}\FloatTok{10.1}\NormalTok{, }\FloatTok{35.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{, }\FloatTok{6.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{),}
    \AttributeTok{y3 =} \FunctionTok{c}\NormalTok{(}\FloatTok{3.4}\NormalTok{, }\FloatTok{4.1}\NormalTok{, }\FloatTok{1.9}\NormalTok{, }\FloatTok{3.8}\NormalTok{, }\FloatTok{1.7}\NormalTok{)}
\NormalTok{)}

\CommentTok{\# Center the data with }
\CommentTok{\# the hypothesized means and make a matrix}
\NormalTok{plants\_ctr }\OtherTok{\textless{}{-}}\NormalTok{ plants }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{transmute}\NormalTok{(}\AttributeTok{y1\_ctr =}\NormalTok{ y1 }\SpecialCharTok{{-}} \FloatTok{2.85}\NormalTok{,}
              \AttributeTok{y2\_ctr =}\NormalTok{ y2 }\SpecialCharTok{{-}} \FloatTok{15.0}\NormalTok{,}
              \AttributeTok{y3\_ctr =}\NormalTok{ y3 }\SpecialCharTok{{-}} \FloatTok{6.0}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{as.matrix}\NormalTok{()}

\CommentTok{\# Use anova.mlm to calculate Wilks\textquotesingle{} lambda}
\NormalTok{onesamp\_fit }\OtherTok{\textless{}{-}} \FunctionTok{anova}\NormalTok{(}\FunctionTok{lm}\NormalTok{(plants\_ctr }\SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{), }\AttributeTok{test =} \StringTok{"Wilks"}\NormalTok{)}
\NormalTok{onesamp\_fit}
\CommentTok{\#\textgreater{} Analysis of Variance Table}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}             Df    Wilks approx F num Df den Df  Pr(\textgreater{}F)  }
\CommentTok{\#\textgreater{} (Intercept)  1 0.054219   11.629      3      2 0.08022 .}
\CommentTok{\#\textgreater{} Residuals    4                                          }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

can't reject the null of hypothesized vector of means

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Paired{-}Sample Hotelling\textquotesingle{}s T\^{}2 test}
\FunctionTok{library}\NormalTok{(ICSNP)}

\CommentTok{\#  Create data frame}
\NormalTok{waste }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
    \AttributeTok{case =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{11}\NormalTok{,}
    \AttributeTok{com\_y1 =} \FunctionTok{c}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{18}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{11}\NormalTok{, }\DecValTok{34}\NormalTok{, }\DecValTok{28}\NormalTok{, }\DecValTok{71}\NormalTok{, }\DecValTok{43}\NormalTok{, }\DecValTok{33}\NormalTok{, }\DecValTok{20}\NormalTok{),}
    \AttributeTok{com\_y2 =} \FunctionTok{c}\NormalTok{(}\DecValTok{27}\NormalTok{, }\DecValTok{23}\NormalTok{, }\DecValTok{64}\NormalTok{, }\DecValTok{44}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{75}\NormalTok{, }\DecValTok{26}\NormalTok{, }\DecValTok{124}\NormalTok{, }\DecValTok{54}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{14}\NormalTok{),}
    \AttributeTok{state\_y1 =} \FunctionTok{c}\NormalTok{(}\DecValTok{25}\NormalTok{, }\DecValTok{28}\NormalTok{, }\DecValTok{36}\NormalTok{, }\DecValTok{35}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{44}\NormalTok{, }\DecValTok{42}\NormalTok{, }\DecValTok{54}\NormalTok{, }\DecValTok{34}\NormalTok{, }\DecValTok{29}\NormalTok{, }\DecValTok{39}\NormalTok{),}
    \AttributeTok{state\_y2 =} \FunctionTok{c}\NormalTok{(}\DecValTok{15}\NormalTok{, }\DecValTok{13}\NormalTok{, }\DecValTok{22}\NormalTok{, }\DecValTok{29}\NormalTok{, }\DecValTok{31}\NormalTok{, }\DecValTok{64}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{64}\NormalTok{, }\DecValTok{56}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{21}\NormalTok{)}
\NormalTok{)}

\CommentTok{\# Calculate the difference between commercial and state labs}
\NormalTok{waste\_diff }\OtherTok{\textless{}{-}}\NormalTok{ waste }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{transmute}\NormalTok{(}\AttributeTok{y1\_diff =}\NormalTok{ com\_y1 }\SpecialCharTok{{-}}\NormalTok{ state\_y1,}
              \AttributeTok{y2\_diff =}\NormalTok{ com\_y2 }\SpecialCharTok{{-}}\NormalTok{ state\_y2)}
\CommentTok{\# Run the test}
\NormalTok{paired\_fit }\OtherTok{\textless{}{-}} \FunctionTok{HotellingsT2}\NormalTok{(waste\_diff)}
\CommentTok{\# value T.2 in the output corresponds to }
\CommentTok{\# the approximate F{-}value in the output from anova.mlm}
\NormalTok{paired\_fit }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Hotelling\textquotesingle{}s one sample T2{-}test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  waste\_diff}
\CommentTok{\#\textgreater{} T.2 = 6.1377, df1 = 2, df2 = 9, p{-}value = 0.02083}
\CommentTok{\#\textgreater{} alternative hypothesis: true location is not equal to c(0,0)}
\end{Highlighting}
\end{Shaded}

reject the null that the two labs' measurements are equal

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Independent{-}Sample Hotelling\textquotesingle{}s T\^{}2 test with Bartlett\textquotesingle{}s test}

\CommentTok{\# Read in data}
\NormalTok{steel }\OtherTok{\textless{}{-}} \FunctionTok{read.table}\NormalTok{(}\StringTok{"images/steel.dat"}\NormalTok{)}
\FunctionTok{names}\NormalTok{(steel) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Temp"}\NormalTok{, }\StringTok{"Yield"}\NormalTok{, }\StringTok{"Strength"}\NormalTok{)}
\FunctionTok{str}\NormalTok{(steel)}
\CommentTok{\#\textgreater{} \textquotesingle{}data.frame\textquotesingle{}:    12 obs. of  3 variables:}
\CommentTok{\#\textgreater{}  $ Temp    : int  1 1 1 1 1 2 2 2 2 2 ...}
\CommentTok{\#\textgreater{}  $ Yield   : int  33 36 35 38 40 35 36 38 39 41 ...}
\CommentTok{\#\textgreater{}  $ Strength: int  60 61 64 63 65 57 59 59 61 63 ...}

\CommentTok{\# Plot the data}
\FunctionTok{ggplot}\NormalTok{(steel, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Yield, }\AttributeTok{y =}\NormalTok{ Strength)) }\SpecialCharTok{+}
    \FunctionTok{geom\_text}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =}\NormalTok{ Temp), }\AttributeTok{size =} \DecValTok{5}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_segment}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}
        \AttributeTok{x =} \DecValTok{33}\NormalTok{,}
        \AttributeTok{y =} \FloatTok{57.5}\NormalTok{,}
        \AttributeTok{xend =} \DecValTok{42}\NormalTok{,}
        \AttributeTok{yend =} \DecValTok{65}
\NormalTok{    ), }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{22-multivariate_files/figure-latex/unnamed-chunk-4-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]


\CommentTok{\# Bartlett\textquotesingle{}s test for equality of covariance matrices}
\CommentTok{\# same thing as Box\textquotesingle{}s M test in the multivariate setting}
\NormalTok{bart\_test }\OtherTok{\textless{}{-}} \FunctionTok{boxM}\NormalTok{(steel[, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{], steel}\SpecialCharTok{$}\NormalTok{Temp)}
\NormalTok{bart\_test }\CommentTok{\# fail to reject the null of equal covariances }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Box\textquotesingle{}s M{-}test for Homogeneity of Covariance Matrices}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  steel[, {-}1]}
\CommentTok{\#\textgreater{} Chi{-}Sq (approx.) = 0.38077, df = 3, p{-}value = 0.9442}

\CommentTok{\# anova.mlm}
\NormalTok{twosamp\_fit }\OtherTok{\textless{}{-}}
    \FunctionTok{anova}\NormalTok{(}\FunctionTok{lm}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(Yield, Strength) }\SpecialCharTok{\textasciitilde{}} \FunctionTok{factor}\NormalTok{(Temp), }
             \AttributeTok{data =}\NormalTok{ steel), }
          \AttributeTok{test =} \StringTok{"Wilks"}\NormalTok{)}
\NormalTok{twosamp\_fit}
\CommentTok{\#\textgreater{} Analysis of Variance Table}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}              Df    Wilks approx F num Df den Df    Pr(\textgreater{}F)    }
\CommentTok{\#\textgreater{} (Intercept)   1 0.001177   3818.1      2      9 6.589e{-}14 ***}
\CommentTok{\#\textgreater{} factor(Temp)  1 0.294883     10.8      2      9  0.004106 ** }
\CommentTok{\#\textgreater{} Residuals    10                                              }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}

\CommentTok{\# ICSNP package}
\NormalTok{twosamp\_fit2 }\OtherTok{\textless{}{-}}
    \FunctionTok{HotellingsT2}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(steel}\SpecialCharTok{$}\NormalTok{Yield, steel}\SpecialCharTok{$}\NormalTok{Strength) }\SpecialCharTok{\textasciitilde{}} 
                     \FunctionTok{factor}\NormalTok{(steel}\SpecialCharTok{$}\NormalTok{Temp))}
\NormalTok{twosamp\_fit2}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Hotelling\textquotesingle{}s two sample T2{-}test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  cbind(steel$Yield, steel$Strength) by factor(steel$Temp)}
\CommentTok{\#\textgreater{} T.2 = 10.76, df1 = 2, df2 = 9, p{-}value = 0.004106}
\CommentTok{\#\textgreater{} alternative hypothesis: true location difference is not equal to c(0,0)}
\end{Highlighting}
\end{Shaded}

reject null. Hence, there is a difference in the means of the bivariate normal distributions

\hypertarget{manova}{%
\section{MANOVA}\label{manova}}

Multivariate Analysis of Variance

One-way MANOVA

Compare treatment means for h different populations

Population 1: \(\mathbf{y}_{11}, \mathbf{y}_{12}, \dots, \mathbf{y}_{1n_1} \sim idd N_p (\mathbf{\mu}_1, \mathbf{\Sigma})\)

\(\vdots\)

Population h: \(\mathbf{y}_{h1}, \mathbf{y}_{h2}, \dots, \mathbf{y}_{hn_h} \sim idd N_p (\mathbf{\mu}_h, \mathbf{\Sigma})\)

\textbf{Assumptions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Independent random samples from \(h\) different populations
\item
  Common covariance matrices
\item
  Each population is multivariate \textbf{normal}
\end{enumerate}

Calculate the summary statistics \(\mathbf{\bar{y}}_i, \mathbf{S}\) and the pooled estimate of the covariance matrix \(\mathbf{S}\)

Similar to the univariate one-way ANVOA, we can use the effects model formulation \(\mathbf{\mu}_i = \mathbf{\mu} + \mathbf{\tau}_i\), where

\begin{itemize}
\item
  \(\mathbf{\mu}_i\) is the population mean for population i
\item
  \(\mathbf{\mu}\) is the overall mean effect
\item
  \(\mathbf{\tau}_i\) is the treatment effect of the i-th treatment.
\end{itemize}

For the one-way model: \(\mathbf{y}_{ij} = \mu + \tau_i + \epsilon_{ij}\) for \(i = 1,..,h; j = 1,..., n_i\) and \(\epsilon_{ij} \sim N_p(\mathbf{0, \Sigma})\)

However, the above model is over-parameterized (i.e., infinite number of ways to define \(\mathbf{\mu}\) and the \(\mathbf{\tau}_i\)'s such that they add up to \(\mu_i\). Thus we can constrain by having

\[
\sum_{i=1}^h n_i \tau_i = 0 
\]

or

\[
\mathbf{\tau}_h = 0
\]

The observational equivalent of the effects model is

\[
\begin{aligned}
\mathbf{y}_{ij} &= \mathbf{\bar{y}} + (\mathbf{\bar{y}}_i - \mathbf{\bar{y}}) + (\mathbf{y}_{ij} - \mathbf{\bar{y}}_i) \\
&= \text{overall sample mean} + \text{treatement effect} + \text{residual} \text{ (under univariate ANOVA)}
\end{aligned} 
\]

After manipulation

\[
\sum_{i = 1}^h \sum_{j = 1}^{n_i} (\mathbf{\bar{y}}_{ij} - \mathbf{\bar{y}})(\mathbf{\bar{y}}_{ij} - \mathbf{\bar{y}})' = \sum_{i = 1}^h n_i (\mathbf{\bar{y}}_i - \mathbf{\bar{y}})(\mathbf{\bar{y}}_i - \mathbf{\bar{y}})' + \sum_{i=1}^h \sum_{j = 1}^{n_i} (\mathbf{\bar{y}}_{ij} - \mathbf{\bar{y}})(\mathbf{\bar{y}}_{ij} - \mathbf{\bar{y}}_i)'
\]

LHS = Total corrected sums of squares and cross products (SSCP) matrix

RHS =

\begin{itemize}
\item
  1st term = Treatment (or between subjects) sum of squares and cross product matrix (denoted H;B)
\item
  2nd term = residual (or within subject) SSCP matrix denoted (E;W)
\end{itemize}

Note:

\[
\mathbf{E} = (n_1 - 1)\mathbf{S}_1  + ... + (n_h -1) \mathbf{S}_h = (n-h) \mathbf{S}
\]

MANOVA table

\begin{longtable}[]{@{}lll@{}}
\caption{MONOVA table}\tabularnewline
\toprule\noalign{}
Source & SSCP & df \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
Source & SSCP & df \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Treatment & \(\mathbf{H}\) & \(h -1\) \\
Residual (error) & \(\mathbf{E}\) & \(\sum_{i= 1}^h n_i - h\) \\
Total Corrected & \(\mathbf{H + E}\) & \(\sum_{i=1}^h n_i -1\) \\
\end{longtable}

\[
H_0: \tau_1 = \tau_2 = \dots = \tau_h = \mathbf{0}
\]

We consider the relative ``sizes'' of \(\mathbf{E}\) and \(\mathbf{H+E}\)

Wilk's Lambda

Define Wilk's Lambda

\[
\Lambda^* = \frac{|\mathbf{E}|}{|\mathbf{H+E}|}
\]

Properties:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Wilk's Lambda is equivalent to the F-statistic in the univariate case
\item
  The exact distribution of \(\Lambda^*\) can be determined for especial cases.
\item
  For large sample sizes, reject \(H_0\) if
\end{enumerate}

\[
-(\sum_{i=1}^h n_i - 1 - \frac{p+h}{2}) \log(\Lambda^*) > \chi^2_{(1-\alpha, p(h-1))}
\]

\hypertarget{testing-general-hypotheses}{%
\subsection{Testing General Hypotheses}\label{testing-general-hypotheses}}

\begin{itemize}
\item
  \(h\) different treatments
\item
  with the i-th treatment
\item
  applied to \(n_i\) subjects that
\item
  are observed for \(p\) repeated measures.
\end{itemize}

Consider this a \(p\) dimensional obs on a random sample from each of \(h\) different treatment populations.

\[
\mathbf{y}_{ij} = \mathbf{\mu} + \mathbf{\tau}_i + \mathbf{\epsilon}_{ij}
\]

for \(i = 1,..,h\) and \(j = 1,..,n_i\)

Equivalently,

\[
\mathbf{Y} = \mathbf{XB} + \mathbf{\epsilon}
\]

where \(n = \sum_{i = 1}^h n_i\) and with restriction \(\mathbf{\tau}_h = 0\)

\[
\mathbf{Y}_{(n \times p)} = 
\left[
\begin{array}
{c}
\mathbf{y}_{11}' \\
\vdots \\
\mathbf{y}_{1n_1}' \\
\vdots \\
\mathbf{y}_{hn_h}'
\end{array}
\right],
\mathbf{B}_{(h \times p)} = 
\left[
\begin{array}
{c}
\mathbf{\mu}' \\
\mathbf{\tau}_1' \\
\vdots \\
\mathbf{\tau}_{h-1}'
\end{array}
\right],
\mathbf{\epsilon}_{(n \times p)} = 
\left[
\begin{array}
{c}
\epsilon_{11}' \\
\vdots \\
\epsilon_{1n_1}' \\
\vdots \\
\epsilon_{hn_h}'
\end{array}
\right]
\]

\[
\mathbf{X}_{(n \times h)} = 
\left[
\begin{array}
{ccccc}
1 & 1 & 0 & \ldots & 0 \\
\vdots & \vdots & \vdots & & \vdots \\
1 & 1 & 0 & \ldots & 0 \\
\vdots & \vdots & \vdots & \ldots & \vdots \\
1 & 0 & 0 & \ldots & 0 \\
\vdots & \vdots & \vdots & & \vdots \\
1 & 0 & 0 & \ldots & 0 
\end{array}
\right]
\]

Estimation

\[
\mathbf{\hat{B}} = (\mathbf{X'X})^{-1} \mathbf{X'Y}
\]

Rows of \(\mathbf{Y}\) are independent (i.e., \(var(\mathbf{Y}) = \mathbf{I}_n \otimes \mathbf{\Sigma}\) , an \(np \times np\) matrix, where \(\otimes\) is the Kronecker product).

\[
\begin{aligned}
&H_0: \mathbf{LBM} = 0 \\
&H_a: \mathbf{LBM} \neq 0
\end{aligned}
\]

where

\begin{itemize}
\item
  \(\mathbf{L}\) is a \(g \times h\) matrix of full row rank (\(g \le h\)) = comparisons across groups
\item
  \(\mathbf{M}\) is a \(p \times u\) matrix of full column rank (\(u \le p\)) = comparisons across traits
\end{itemize}

The general treatment corrected sums of squares and cross product is

\[
\mathbf{H} = \mathbf{M'Y'X(X'X)^{-1}L'[L(X'X)^{-1}L']^{-1}L(X'X)^{-1}X'YM}
\]

or for the null hypothesis \(H_0: \mathbf{LBM} = \mathbf{D}\)

\[
\mathbf{H} = (\mathbf{\hat{LBM}} - \mathbf{D})'[\mathbf{X(X'X)^{-1}L}]^{-1}(\mathbf{\hat{LBM}} - \mathbf{D})
\]

The general matrix of residual sums of squares and cross product

\[
\mathbf{E} = \mathbf{M'Y'[I-X(X'X)^{-1}X']YM} = \mathbf{M'[Y'Y - \hat{B}'(X'X)^{-1}\hat{B}]M}
\]

We can compute the following statistic eigenvalues of \(\mathbf{HE}^{-1}\)

\begin{itemize}
\item
  Wilk's Criterion: \(\Lambda^* = \frac{|\mathbf{E}|}{|\mathbf{H} + \mathbf{E}|}\) . The df depend on the rank of \(\mathbf{L}, \mathbf{M}, \mathbf{X}\)
\item
  Lawley-Hotelling Trace: \(U = tr(\mathbf{HE}^{-1})\)
\item
  Pillai Trace: \(V = tr(\mathbf{H}(\mathbf{H}+ \mathbf{E}^{-1})\)
\item
  Roy's Maximum Root: largest eigenvalue of \(\mathbf{HE}^{-1}\)
\end{itemize}

If \(H_0\) is true and n is large, \(-(n-1- \frac{p+h}{2})\ln \Lambda^* \sim \chi^2_{p(h-1)}\). Some special values of p and h can give exact F-dist under \(H_0\)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# One{-}way MANOVA}

\FunctionTok{library}\NormalTok{(car)}
\FunctionTok{library}\NormalTok{(emmeans)}
\FunctionTok{library}\NormalTok{(profileR)}
\FunctionTok{library}\NormalTok{(tidyverse)}

\DocumentationTok{\#\# Read in the data}
\NormalTok{gpagmat }\OtherTok{\textless{}{-}} \FunctionTok{read.table}\NormalTok{(}\StringTok{"images/gpagmat.dat"}\NormalTok{)}

\DocumentationTok{\#\# Change the variable names}
\FunctionTok{names}\NormalTok{(gpagmat) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"y1"}\NormalTok{, }\StringTok{"y2"}\NormalTok{, }\StringTok{"admit"}\NormalTok{)}

\DocumentationTok{\#\# Check the structure}
\FunctionTok{str}\NormalTok{(gpagmat)}
\CommentTok{\#\textgreater{} \textquotesingle{}data.frame\textquotesingle{}:    85 obs. of  3 variables:}
\CommentTok{\#\textgreater{}  $ y1   : num  2.96 3.14 3.22 3.29 3.69 3.46 3.03 3.19 3.63 3.59 ...}
\CommentTok{\#\textgreater{}  $ y2   : int  596 473 482 527 505 693 626 663 447 588 ...}
\CommentTok{\#\textgreater{}  $ admit: int  1 1 1 1 1 1 1 1 1 1 ...}


\DocumentationTok{\#\# Plot the data}
\NormalTok{gg }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(gpagmat, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ y1, }\AttributeTok{y =}\NormalTok{ y2)) }\SpecialCharTok{+}
    \FunctionTok{geom\_text}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =}\NormalTok{ admit, }\AttributeTok{col =} \FunctionTok{as.character}\NormalTok{(admit))) }\SpecialCharTok{+}
    \FunctionTok{scale\_color\_discrete}\NormalTok{(}\AttributeTok{name =} \StringTok{"Admission"}\NormalTok{,}
                         \AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Admit"}\NormalTok{, }\StringTok{"Do not admit"}\NormalTok{, }\StringTok{"Borderline"}\NormalTok{)) }\SpecialCharTok{+}
    \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{name =} \StringTok{"GPA"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{name =} \StringTok{"GMAT"}\NormalTok{)}

\DocumentationTok{\#\# Fit one{-}way MANOVA}
\NormalTok{oneway\_fit }\OtherTok{\textless{}{-}} \FunctionTok{manova}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(y1, y2) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ admit, }\AttributeTok{data =}\NormalTok{ gpagmat)}
\FunctionTok{summary}\NormalTok{(oneway\_fit, }\AttributeTok{test =} \StringTok{"Wilks"}\NormalTok{)}
\CommentTok{\#\textgreater{}           Df  Wilks approx F num Df den Df    Pr(\textgreater{}F)    }
\CommentTok{\#\textgreater{} admit      1 0.6126   25.927      2     82 1.881e{-}09 ***}
\CommentTok{\#\textgreater{} Residuals 83                                            }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

reject the null of equal multivariate mean vectors between the three admmission groups

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Repeated Measures MANOVA}


\DocumentationTok{\#\# Create data frame}
\NormalTok{stress }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
    \AttributeTok{subject =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{8}\NormalTok{,}
    \AttributeTok{begin =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{),}
    \AttributeTok{middle =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{),}
    \AttributeTok{final =} \FunctionTok{c}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  If independent = time with 3 levels -\textgreater{} univariate ANOVA (require sphericity assumption (i.e., the variances for all differences are equal))
\item
  If each level of independent time as a separate variable -\textgreater{} MANOVA (does not require sphericity assumption)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# MANOVA}
\NormalTok{stress\_mod }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(begin, middle, final) }\SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }\AttributeTok{data =}\NormalTok{ stress)}
\NormalTok{idata }\OtherTok{\textless{}{-}}
    \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{time =} \FunctionTok{factor}\NormalTok{(}
        \FunctionTok{c}\NormalTok{(}\StringTok{"begin"}\NormalTok{, }\StringTok{"middle"}\NormalTok{, }\StringTok{"final"}\NormalTok{),}
        \AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{"begin"}\NormalTok{, }\StringTok{"middle"}\NormalTok{, }\StringTok{"final"}\NormalTok{)}
\NormalTok{    ))}
\NormalTok{repeat\_fit }\OtherTok{\textless{}{-}}
    \FunctionTok{Anova}\NormalTok{(}
\NormalTok{        stress\_mod,}
        \AttributeTok{idata =}\NormalTok{ idata,}
        \AttributeTok{idesign =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ time,}
        \AttributeTok{icontrasts =} \StringTok{"contr.poly"}
\NormalTok{    )}
\FunctionTok{summary}\NormalTok{(repeat\_fit) }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Type III Repeated Measures MANOVA Tests:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}  }
\CommentTok{\#\textgreater{} Term: (Intercept) }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Response transformation matrix:}
\CommentTok{\#\textgreater{}        (Intercept)}
\CommentTok{\#\textgreater{} begin            1}
\CommentTok{\#\textgreater{} middle           1}
\CommentTok{\#\textgreater{} final            1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Sum of squares and products for the hypothesis:}
\CommentTok{\#\textgreater{}             (Intercept)}
\CommentTok{\#\textgreater{} (Intercept)        1352}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Multivariate Tests: (Intercept)}
\CommentTok{\#\textgreater{}                  Df test stat approx F num Df den Df     Pr(\textgreater{}F)    }
\CommentTok{\#\textgreater{} Pillai            1  0.896552 60.66667      1      7 0.00010808 ***}
\CommentTok{\#\textgreater{} Wilks             1  0.103448 60.66667      1      7 0.00010808 ***}
\CommentTok{\#\textgreater{} Hotelling{-}Lawley  1  8.666667 60.66667      1      7 0.00010808 ***}
\CommentTok{\#\textgreater{} Roy               1  8.666667 60.66667      1      7 0.00010808 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}  }
\CommentTok{\#\textgreater{} Term: time }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Response transformation matrix:}
\CommentTok{\#\textgreater{}               time.L     time.Q}
\CommentTok{\#\textgreater{} begin  {-}7.071068e{-}01  0.4082483}
\CommentTok{\#\textgreater{} middle {-}7.850462e{-}17 {-}0.8164966}
\CommentTok{\#\textgreater{} final   7.071068e{-}01  0.4082483}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Sum of squares and products for the hypothesis:}
\CommentTok{\#\textgreater{}           time.L   time.Q}
\CommentTok{\#\textgreater{} time.L 18.062500 6.747781}
\CommentTok{\#\textgreater{} time.Q  6.747781 2.520833}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Multivariate Tests: time}
\CommentTok{\#\textgreater{}                  Df test stat approx F num Df den Df   Pr(\textgreater{}F)  }
\CommentTok{\#\textgreater{} Pillai            1 0.7080717 7.276498      2      6 0.024879 *}
\CommentTok{\#\textgreater{} Wilks             1 0.2919283 7.276498      2      6 0.024879 *}
\CommentTok{\#\textgreater{} Hotelling{-}Lawley  1 2.4254992 7.276498      2      6 0.024879 *}
\CommentTok{\#\textgreater{} Roy               1 2.4254992 7.276498      2      6 0.024879 *}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Univariate Type III Repeated{-}Measures ANOVA Assuming Sphericity}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}             Sum Sq num Df Error SS den Df F value    Pr(\textgreater{}F)    }
\CommentTok{\#\textgreater{} (Intercept) 450.67      1    52.00      7 60.6667 0.0001081 ***}
\CommentTok{\#\textgreater{} time         20.58      2    24.75     14  5.8215 0.0144578 *  }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Mauchly Tests for Sphericity}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}      Test statistic p{-}value}
\CommentTok{\#\textgreater{} time         0.7085 0.35565}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Greenhouse{-}Geisser and Huynh{-}Feldt Corrections}
\CommentTok{\#\textgreater{}  for Departure from Sphericity}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}       GG eps Pr(\textgreater{}F[GG])  }
\CommentTok{\#\textgreater{} time 0.77429    0.02439 *}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}         HF eps Pr(\textgreater{}F[HF])}
\CommentTok{\#\textgreater{} time 0.9528433 0.01611634}
\end{Highlighting}
\end{Shaded}

can't reject the null hypothesis of sphericity, hence univariate ANOVA is also appropriate.We also see linear significant time effect, but no quadratic time effect

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# Polynomial contrasts}
\CommentTok{\# What is the reference for the marginal means?}
\FunctionTok{ref\_grid}\NormalTok{(stress\_mod, }\AttributeTok{mult.name =} \StringTok{"time"}\NormalTok{)}
\CommentTok{\#\textgreater{} \textquotesingle{}emmGrid\textquotesingle{} object with variables:}
\CommentTok{\#\textgreater{}     1 = 1}
\CommentTok{\#\textgreater{}     time = multivariate response levels: begin, middle, final}

\CommentTok{\# marginal means for the levels of time}
\NormalTok{contr\_means }\OtherTok{\textless{}{-}} \FunctionTok{emmeans}\NormalTok{(stress\_mod, }\SpecialCharTok{\textasciitilde{}}\NormalTok{ time, }\AttributeTok{mult.name =} \StringTok{"time"}\NormalTok{)}
\FunctionTok{contrast}\NormalTok{(contr\_means, }\AttributeTok{method =} \StringTok{"poly"}\NormalTok{)}
\CommentTok{\#\textgreater{}  contrast  estimate    SE df t.ratio p.value}
\CommentTok{\#\textgreater{}  linear        2.12 0.766  7   2.773  0.0276}
\CommentTok{\#\textgreater{}  quadratic     1.38 0.944  7   1.457  0.1885}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# MANOVA}


\DocumentationTok{\#\# Read in Data}
\NormalTok{heart }\OtherTok{\textless{}{-}} \FunctionTok{read.table}\NormalTok{(}\StringTok{"images/heart.dat"}\NormalTok{)}
\FunctionTok{names}\NormalTok{(heart) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"drug"}\NormalTok{, }\StringTok{"y1"}\NormalTok{, }\StringTok{"y2"}\NormalTok{, }\StringTok{"y3"}\NormalTok{, }\StringTok{"y4"}\NormalTok{)}
\DocumentationTok{\#\# Create a subject ID nested within drug}
\NormalTok{heart }\OtherTok{\textless{}{-}}\NormalTok{ heart }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{group\_by}\NormalTok{(drug) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{subject =} \FunctionTok{row\_number}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{ungroup}\NormalTok{()}
\FunctionTok{str}\NormalTok{(heart)}
\CommentTok{\#\textgreater{} tibble [24 x 6] (S3: tbl\_df/tbl/data.frame)}
\CommentTok{\#\textgreater{}  $ drug   : chr [1:24] "ax23" "ax23" "ax23" "ax23" ...}
\CommentTok{\#\textgreater{}  $ y1     : int [1:24] 72 78 71 72 66 74 62 69 85 82 ...}
\CommentTok{\#\textgreater{}  $ y2     : int [1:24] 86 83 82 83 79 83 73 75 86 86 ...}
\CommentTok{\#\textgreater{}  $ y3     : int [1:24] 81 88 81 83 77 84 78 76 83 80 ...}
\CommentTok{\#\textgreater{}  $ y4     : int [1:24] 77 82 75 69 66 77 70 70 80 84 ...}
\CommentTok{\#\textgreater{}  $ subject: int [1:24] 1 2 3 4 5 6 7 8 1 2 ...}

\DocumentationTok{\#\# Create means summary for profile plot,}
\CommentTok{\# pivot longer for plotting with ggplot}
\NormalTok{heart\_means }\OtherTok{\textless{}{-}}\NormalTok{ heart }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{group\_by}\NormalTok{(drug) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{summarize\_at}\NormalTok{(}\FunctionTok{vars}\NormalTok{(}\FunctionTok{starts\_with}\NormalTok{(}\StringTok{"y"}\NormalTok{)), mean) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{pivot\_longer}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{drug, }\AttributeTok{names\_to =} \StringTok{"time"}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{"mean"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{time =} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{as.factor}\NormalTok{(time)))}
\NormalTok{gg\_profile }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(heart\_means, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ time, }\AttributeTok{y =}\NormalTok{ mean)) }\SpecialCharTok{+}
    \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{col =}\NormalTok{ drug)) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{col =}\NormalTok{ drug)) }\SpecialCharTok{+}
    \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Profile Plot"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{name =} \StringTok{"Response"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{scale\_x\_discrete}\NormalTok{(}\AttributeTok{name =} \StringTok{"Time"}\NormalTok{)}
\NormalTok{gg\_profile}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{22-multivariate_files/figure-latex/unnamed-chunk-9-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# Fit model}
\NormalTok{heart\_mod }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(y1, y2, y3, y4) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ drug, }\AttributeTok{data =}\NormalTok{ heart)}
\NormalTok{man\_fit }\OtherTok{\textless{}{-}}\NormalTok{ car}\SpecialCharTok{::}\FunctionTok{Anova}\NormalTok{(heart\_mod)}
\FunctionTok{summary}\NormalTok{(man\_fit)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Type II MANOVA Tests:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Sum of squares and products for error:}
\CommentTok{\#\textgreater{}        y1      y2      y3     y4}
\CommentTok{\#\textgreater{} y1 641.00 601.750 535.250 426.00}
\CommentTok{\#\textgreater{} y2 601.75 823.875 615.500 534.25}
\CommentTok{\#\textgreater{} y3 535.25 615.500 655.875 555.25}
\CommentTok{\#\textgreater{} y4 426.00 534.250 555.250 674.50}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}  }
\CommentTok{\#\textgreater{} Term: drug }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Sum of squares and products for the hypothesis:}
\CommentTok{\#\textgreater{}        y1       y2       y3    y4}
\CommentTok{\#\textgreater{} y1 567.00 335.2500  42.7500 387.0}
\CommentTok{\#\textgreater{} y2 335.25 569.0833 404.5417 367.5}
\CommentTok{\#\textgreater{} y3  42.75 404.5417 391.0833 171.0}
\CommentTok{\#\textgreater{} y4 387.00 367.5000 171.0000 316.0}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Multivariate Tests: drug}
\CommentTok{\#\textgreater{}                  Df test stat  approx F num Df den Df     Pr(\textgreater{}F)    }
\CommentTok{\#\textgreater{} Pillai            2  1.283456  8.508082      8     38 1.5010e{-}06 ***}
\CommentTok{\#\textgreater{} Wilks             2  0.079007 11.509581      8     36 6.3081e{-}08 ***}
\CommentTok{\#\textgreater{} Hotelling{-}Lawley  2  7.069384 15.022441      8     34 3.9048e{-}09 ***}
\CommentTok{\#\textgreater{} Roy               2  6.346509 30.145916      4     19 5.4493e{-}08 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

reject the null hypothesis of no difference in means between treatments

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# Contrasts}
\NormalTok{heart}\SpecialCharTok{$}\NormalTok{drug }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(heart}\SpecialCharTok{$}\NormalTok{drug)}
\NormalTok{L }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{,}
              \DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{), }\AttributeTok{nrow =} \DecValTok{3}\NormalTok{, }\AttributeTok{byrow =}\NormalTok{ T)}
\FunctionTok{colnames}\NormalTok{(L) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"bww9:ctrl"}\NormalTok{, }\StringTok{"ax23:rest"}\NormalTok{)}
\FunctionTok{rownames}\NormalTok{(L) }\OtherTok{\textless{}{-}} \FunctionTok{unique}\NormalTok{(heart}\SpecialCharTok{$}\NormalTok{drug)}
\FunctionTok{contrasts}\NormalTok{(heart}\SpecialCharTok{$}\NormalTok{drug) }\OtherTok{\textless{}{-}}\NormalTok{ L}
\FunctionTok{contrasts}\NormalTok{(heart}\SpecialCharTok{$}\NormalTok{drug)}
\CommentTok{\#\textgreater{}      bww9:ctrl ax23:rest}
\CommentTok{\#\textgreater{} ax23         0         2}
\CommentTok{\#\textgreater{} bww9         1        {-}1}
\CommentTok{\#\textgreater{} ctrl        {-}1        {-}1}

\CommentTok{\# do not set contrast L if you do further analysis (e.g., Anova, lm)}
\CommentTok{\# do M matrix instead}

\NormalTok{M }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{,}
              \DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{,}
              \DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{), }\AttributeTok{nrow =} \DecValTok{4}\NormalTok{)}
\DocumentationTok{\#\# update model to test contrasts}
\NormalTok{heart\_mod2 }\OtherTok{\textless{}{-}} \FunctionTok{update}\NormalTok{(heart\_mod)}
\FunctionTok{coef}\NormalTok{(heart\_mod2)}
\CommentTok{\#\textgreater{}                  y1         y2        y3    y4}
\CommentTok{\#\textgreater{} (Intercept)   75.00 78.9583333 77.041667 74.75}
\CommentTok{\#\textgreater{} drugbww9:ctrl  4.50  5.8125000  3.562500  4.25}
\CommentTok{\#\textgreater{} drugax23:rest {-}2.25  0.7708333  1.979167 {-}0.75}

\CommentTok{\# Hypothesis test for bww9 vs control after transformation M}
\CommentTok{\# same as linearHypothesis(heart\_mod, hypothesis.matrix = c(0,1,{-}1), P = M)}
\NormalTok{bww9vctrl }\OtherTok{\textless{}{-}}
\NormalTok{    car}\SpecialCharTok{::}\FunctionTok{linearHypothesis}\NormalTok{(heart\_mod2,}
                     \AttributeTok{hypothesis.matrix =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{),}
                     \AttributeTok{P =}\NormalTok{ M)}
\NormalTok{bww9vctrl}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Response transformation matrix:}
\CommentTok{\#\textgreater{}    [,1] [,2] [,3]}
\CommentTok{\#\textgreater{} y1    1    0    0}
\CommentTok{\#\textgreater{} y2   {-}1    1    0}
\CommentTok{\#\textgreater{} y3    0   {-}1    1}
\CommentTok{\#\textgreater{} y4    0    0   {-}1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Sum of squares and products for the hypothesis:}
\CommentTok{\#\textgreater{}          [,1]   [,2]     [,3]}
\CommentTok{\#\textgreater{} [1,]  27.5625 {-}47.25  14.4375}
\CommentTok{\#\textgreater{} [2,] {-}47.2500  81.00 {-}24.7500}
\CommentTok{\#\textgreater{} [3,]  14.4375 {-}24.75   7.5625}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Sum of squares and products for error:}
\CommentTok{\#\textgreater{}          [,1]     [,2]    [,3]}
\CommentTok{\#\textgreater{} [1,]  261.375 {-}141.875  28.000}
\CommentTok{\#\textgreater{} [2,] {-}141.875  248.750 {-}19.375}
\CommentTok{\#\textgreater{} [3,]   28.000  {-}19.375 219.875}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Multivariate Tests: }
\CommentTok{\#\textgreater{}                  Df test stat approx F num Df den Df Pr(\textgreater{}F)}
\CommentTok{\#\textgreater{} Pillai            1 0.2564306 2.184141      3     19 0.1233}
\CommentTok{\#\textgreater{} Wilks             1 0.7435694 2.184141      3     19 0.1233}
\CommentTok{\#\textgreater{} Hotelling{-}Lawley  1 0.3448644 2.184141      3     19 0.1233}
\CommentTok{\#\textgreater{} Roy               1 0.3448644 2.184141      3     19 0.1233}

\NormalTok{bww9vctrl }\OtherTok{\textless{}{-}}
\NormalTok{    car}\SpecialCharTok{::}\FunctionTok{linearHypothesis}\NormalTok{(heart\_mod,}
                     \AttributeTok{hypothesis.matrix =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{),}
                     \AttributeTok{P =}\NormalTok{ M)}
\NormalTok{bww9vctrl}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Response transformation matrix:}
\CommentTok{\#\textgreater{}    [,1] [,2] [,3]}
\CommentTok{\#\textgreater{} y1    1    0    0}
\CommentTok{\#\textgreater{} y2   {-}1    1    0}
\CommentTok{\#\textgreater{} y3    0   {-}1    1}
\CommentTok{\#\textgreater{} y4    0    0   {-}1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Sum of squares and products for the hypothesis:}
\CommentTok{\#\textgreater{}          [,1]   [,2]     [,3]}
\CommentTok{\#\textgreater{} [1,]  27.5625 {-}47.25  14.4375}
\CommentTok{\#\textgreater{} [2,] {-}47.2500  81.00 {-}24.7500}
\CommentTok{\#\textgreater{} [3,]  14.4375 {-}24.75   7.5625}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Sum of squares and products for error:}
\CommentTok{\#\textgreater{}          [,1]     [,2]    [,3]}
\CommentTok{\#\textgreater{} [1,]  261.375 {-}141.875  28.000}
\CommentTok{\#\textgreater{} [2,] {-}141.875  248.750 {-}19.375}
\CommentTok{\#\textgreater{} [3,]   28.000  {-}19.375 219.875}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Multivariate Tests: }
\CommentTok{\#\textgreater{}                  Df test stat approx F num Df den Df Pr(\textgreater{}F)}
\CommentTok{\#\textgreater{} Pillai            1 0.2564306 2.184141      3     19 0.1233}
\CommentTok{\#\textgreater{} Wilks             1 0.7435694 2.184141      3     19 0.1233}
\CommentTok{\#\textgreater{} Hotelling{-}Lawley  1 0.3448644 2.184141      3     19 0.1233}
\CommentTok{\#\textgreater{} Roy               1 0.3448644 2.184141      3     19 0.1233}
\end{Highlighting}
\end{Shaded}

there is no significant difference in means between the control and \texttt{bww9} drug

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Hypothesis test for ax23 vs rest after transformation M}
\NormalTok{axx23vrest }\OtherTok{\textless{}{-}}
\NormalTok{    car}\SpecialCharTok{::}\FunctionTok{linearHypothesis}\NormalTok{(heart\_mod2,}
                     \AttributeTok{hypothesis.matrix =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{),}
                     \AttributeTok{P =}\NormalTok{ M)}
\NormalTok{axx23vrest}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Response transformation matrix:}
\CommentTok{\#\textgreater{}    [,1] [,2] [,3]}
\CommentTok{\#\textgreater{} y1    1    0    0}
\CommentTok{\#\textgreater{} y2   {-}1    1    0}
\CommentTok{\#\textgreater{} y3    0   {-}1    1}
\CommentTok{\#\textgreater{} y4    0    0   {-}1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Sum of squares and products for the hypothesis:}
\CommentTok{\#\textgreater{}           [,1]       [,2]      [,3]}
\CommentTok{\#\textgreater{} [1,]  438.0208  175.20833 {-}395.7292}
\CommentTok{\#\textgreater{} [2,]  175.2083   70.08333 {-}158.2917}
\CommentTok{\#\textgreater{} [3,] {-}395.7292 {-}158.29167  357.5208}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Sum of squares and products for error:}
\CommentTok{\#\textgreater{}          [,1]     [,2]    [,3]}
\CommentTok{\#\textgreater{} [1,]  261.375 {-}141.875  28.000}
\CommentTok{\#\textgreater{} [2,] {-}141.875  248.750 {-}19.375}
\CommentTok{\#\textgreater{} [3,]   28.000  {-}19.375 219.875}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Multivariate Tests: }
\CommentTok{\#\textgreater{}                  Df test stat approx F num Df den Df     Pr(\textgreater{}F)    }
\CommentTok{\#\textgreater{} Pillai            1  0.855364 37.45483      3     19 3.5484e{-}08 ***}
\CommentTok{\#\textgreater{} Wilks             1  0.144636 37.45483      3     19 3.5484e{-}08 ***}
\CommentTok{\#\textgreater{} Hotelling{-}Lawley  1  5.913921 37.45483      3     19 3.5484e{-}08 ***}
\CommentTok{\#\textgreater{} Roy               1  5.913921 37.45483      3     19 3.5484e{-}08 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}

\NormalTok{axx23vrest }\OtherTok{\textless{}{-}}
\NormalTok{    car}\SpecialCharTok{::}\FunctionTok{linearHypothesis}\NormalTok{(heart\_mod,}
                     \AttributeTok{hypothesis.matrix =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{),}
                     \AttributeTok{P =}\NormalTok{ M)}
\NormalTok{axx23vrest}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Response transformation matrix:}
\CommentTok{\#\textgreater{}    [,1] [,2] [,3]}
\CommentTok{\#\textgreater{} y1    1    0    0}
\CommentTok{\#\textgreater{} y2   {-}1    1    0}
\CommentTok{\#\textgreater{} y3    0   {-}1    1}
\CommentTok{\#\textgreater{} y4    0    0   {-}1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Sum of squares and products for the hypothesis:}
\CommentTok{\#\textgreater{}           [,1]       [,2]      [,3]}
\CommentTok{\#\textgreater{} [1,]  402.5208  127.41667 {-}390.9375}
\CommentTok{\#\textgreater{} [2,]  127.4167   40.33333 {-}123.7500}
\CommentTok{\#\textgreater{} [3,] {-}390.9375 {-}123.75000  379.6875}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Sum of squares and products for error:}
\CommentTok{\#\textgreater{}          [,1]     [,2]    [,3]}
\CommentTok{\#\textgreater{} [1,]  261.375 {-}141.875  28.000}
\CommentTok{\#\textgreater{} [2,] {-}141.875  248.750 {-}19.375}
\CommentTok{\#\textgreater{} [3,]   28.000  {-}19.375 219.875}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Multivariate Tests: }
\CommentTok{\#\textgreater{}                  Df test stat approx F num Df den Df     Pr(\textgreater{}F)    }
\CommentTok{\#\textgreater{} Pillai            1  0.842450 33.86563      3     19 7.9422e{-}08 ***}
\CommentTok{\#\textgreater{} Wilks             1  0.157550 33.86563      3     19 7.9422e{-}08 ***}
\CommentTok{\#\textgreater{} Hotelling{-}Lawley  1  5.347205 33.86563      3     19 7.9422e{-}08 ***}
\CommentTok{\#\textgreater{} Roy               1  5.347205 33.86563      3     19 7.9422e{-}08 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

there is a significant difference in means between ax23 drug treatment and the rest of the treatments

\hypertarget{profile-analysis}{%
\subsection{Profile Analysis}\label{profile-analysis}}

Examine similarities between the treatment effects (between subjects), which is useful for longitudinal analysis. Null is that all treatments have the same average effect.

\[
H_0: \mu_1 = \mu_2 = \dots = \mu_h
\]

Equivalently,

\[
H_0: \tau_1 = \tau_2 = \dots = \tau_h
\]

The exact nature of the similarities and differences between the treatments can be examined under this analysis.

Sequential steps in profile analysis:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Are the profiles \textbf{parallel}? (i.e., is there no interaction between treatment and time)
\item
  Are the profiles \textbf{coincidental}? (i.e., are the profiles identical?)
\item
  Are the profiles \textbf{horizontal}? (i.e., are there no differences between any time points?)
\end{enumerate}

If we reject the null hypothesis that the profiles are parallel, we can test

\begin{itemize}
\item
  Are there differences among groups within some subset of the total time points?
\item
  Are there differences among time points in a particular group (or groups)?
\item
  Are there differences within some subset of the total time points in a particular group (or groups)?
\end{itemize}

Example

\begin{itemize}
\item
  4 times (p = 4)
\item
  3 treatments (h=3)
\end{itemize}

\hypertarget{parallel-profile}{%
\subsubsection{Parallel Profile}\label{parallel-profile}}

Are the profiles for each population identical expect for a mean shift?

\[
\begin{aligned}
H_0: \mu_{11} - \mu_{21} - \mu_{12} - \mu_{22} = &\dots = \mu_{1t} - \mu_{2t} \\
\mu_{11} - \mu_{31} - \mu_{12} - \mu_{32} = &\dots = \mu_{1t} - \mu_{3t} \\
&\dots
\end{aligned}
\]

for \(h-1\) equations

Equivalently,

\[
H_0: \mathbf{LBM = 0}
\]

\[
\mathbf{LBM} =
\left[
\begin{array}
{ccc}
1 & -1 & 0 \\
1 & 0 & -1
\end{array}
\right]
\left[
\begin{array}
{ccc}
\mu_{11} & \dots & \mu_{14} \\
\mu_{21} & \dots & \mu_{24} \\
\mu_{31} & \dots & \mu_{34} 
\end{array}
\right]
\left[
\begin{array}
{ccc}
1 & 1 & 1 \\
-1 & 0 & 0 \\
0 & -1 & 0 \\
0 & 0 & -1
\end{array}
\right]
= 
\mathbf{0}
\]

where this is the cell means parameterization of \(\mathbf{B}\)

The multiplication of the first 2 matrices \(\mathbf{LB}\) is

\[
\left[
\begin{array}
{cccc}
\mu_{11} - \mu_{21} & \mu_{12} - \mu_{22} & \mu_{13} - \mu_{23} & \mu_{14} - \mu_{24}\\
\mu_{11} - \mu_{31} & \mu_{12} - \mu_{32} & \mu_{13} - \mu_{33} & \mu_{14} - \mu_{34} 
\end{array}
\right]
\]

which is the differences in treatment means at the same time

Multiplying by \(\mathbf{M}\), we get the comparison across time

\[
\left[
\begin{array}
{ccc}
(\mu_{11} - \mu_{21}) - (\mu_{12} - \mu_{22}) & (\mu_{11} - \mu_{21}) -(\mu_{13} - \mu_{23}) & (\mu_{11} - \mu_{21}) - (\mu_{14} - \mu_{24}) \\
(\mu_{11} - \mu_{31}) - (\mu_{12} - \mu_{32}) & (\mu_{11} - \mu_{31}) - (\mu_{13} - \mu_{33}) & (\mu_{11} - \mu_{31}) -(\mu_{14} - \mu_{34}) 
\end{array}
\right]
\]

Alternatively, we can also use the effects parameterization

\[
\mathbf{LBM} =
\left[
\begin{array}
{cccc}
0 & 1 & -1 & 0 \\
0 & 1 & 0 & -1 
\end{array}
\right]
\left[
\begin{array}
{c}
\mu' \\
\tau'_1 \\
\tau_2' \\
\tau_3'
\end{array}
\right]
\left[
\begin{array}
{ccc}
1 & 1 & 1 \\
-1 & 0 & 0 \\
0 & -1 & 0 \\
0 & 0 & -1
\end{array}
\right]
= \mathbf{0}
\]

In both parameterizations, \(rank(\mathbf{L}) = h-1\) and \(rank(\mathbf{M}) = p-1\)

We could also choose \(\mathbf{L}\) and \(\mathbf{M}\) in other forms

\[
\mathbf{L} = \left[
\begin{array}
{cccc}
0 & 1 & 0 & -1 \\
0 & 0 & 1 & -1 
\end{array}
\right]
\]

and

\[
\mathbf{M} = \left[
\begin{array}
{ccc}
1 & 0 & 0 \\
-1 & 1 & 0 \\
0 & -1 & 1 \\
0 & 0 & -1
\end{array}
\right]
\]

and still obtain the same result.

\hypertarget{coincidental-profiles}{%
\subsubsection{Coincidental Profiles}\label{coincidental-profiles}}

After we have evidence that the profiles are parallel (i.e., fail to reject the parallel profile test), we can ask whether they are identical?

Given profiles are \textbf{parallel}, then if the sums of the components of \(\mu_i\) are identical for all the treatments, then the profiles are \textbf{identical}.

\[
H_0: \mathbf{1'}_p \mu_1 = \mathbf{1'}_p \mu_2 = \dots = \mathbf{1'}_p \mu_h 
\]

Equivalently,

\[
H_0: \mathbf{LBM} = \mathbf{0}
\]

where for the cell means parameterization

\[
\mathbf{L} = 
\left[
\begin{array}
{ccc}
1 & 0 & -1 \\
0 & 1 & -1
\end{array}
\right]
\]

and

\[
\mathbf{M} = 
\left[
\begin{array}
{cccc}
1 & 1 & 1 & 1
\end{array}
\right]'
\]

multiplication yields

\[
\left[
\begin{array}
{c}
(\mu_{11} + \mu_{12} + \mu_{13} + \mu_{14}) - (\mu_{31} + \mu_{32} + \mu_{33} + \mu_{34}) \\
(\mu_{21} + \mu_{22} + \mu_{23} + \mu_{24}) - (\mu_{31} + \mu_{32} + \mu_{33} + \mu_{34})
\end{array}
\right]
=
\left[
\begin{array}
{c}
0 \\
0 
\end{array}
\right]
\]

Different choices of \(\mathbf{L}\) and \(\mathbf{M}\) can yield the same result

\hypertarget{horizontal-profiles}{%
\subsubsection{Horizontal Profiles}\label{horizontal-profiles}}

Given that we can't reject the null hypothesis that all \(h\) profiles are the same, we can ask whether all of the elements of the common profile equal? (i.e., horizontal)

\[
H_0: \mathbf{LBM} = \mathbf{0}
\]

\[
\mathbf{L} = 
\left[
\begin{array}
{ccc}
1 & 0 & 0 
\end{array}
\right]
\]

and

\[
\mathbf{M} = \left[
\begin{array}
{ccc}
1 & 0 & 0 \\
-1 & 1 & 0 \\
0 & -1 & 1 \\
0 & 0 & -1
\end{array}
\right]
\]

hence,

\[
\left[
\begin{array}
{ccc}
(\mu_{11} - \mu_{12}) & (\mu_{12} - \mu_{13}) & (\mu_{13} + \mu_{14}) 
\end{array}
\right]
=
\left[
\begin{array}
{ccc}
0 & 0 & 0
\end{array}
\right]
\]

Note:

\begin{itemize}
\tightlist
\item
  If we fail to reject all 3 hypotheses, then we fail to reject the null hypotheses of both no difference between treatments and no differences between traits.
\end{itemize}

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Test & Equivalent test for \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Parallel profile & Interaction \\
Coincidental profile & main effect of between-subjects factor \\
Horizontal profile & main effect of repeated measures factor \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{profile\_fit }\OtherTok{\textless{}{-}}
    \FunctionTok{pbg}\NormalTok{(}
        \AttributeTok{data =} \FunctionTok{as.matrix}\NormalTok{(heart[, }\DecValTok{2}\SpecialCharTok{:}\DecValTok{5}\NormalTok{]),}
        \AttributeTok{group =} \FunctionTok{as.matrix}\NormalTok{(heart[, }\DecValTok{1}\NormalTok{]),}
        \AttributeTok{original.names =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{profile.plot =} \ConstantTok{FALSE}
\NormalTok{    )}
\FunctionTok{summary}\NormalTok{(profile\_fit)}
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} pbg(data = as.matrix(heart[, 2:5]), group = as.matrix(heart[, }
\CommentTok{\#\textgreater{}     1]), original.names = TRUE, profile.plot = FALSE)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Hypothesis Tests:}
\CommentTok{\#\textgreater{} $\textasciigrave{}Ho: Profiles are parallel\textasciigrave{}}
\CommentTok{\#\textgreater{}   Multivariate.Test Statistic  Approx.F num.df den.df      p.value}
\CommentTok{\#\textgreater{} 1             Wilks 0.1102861 12.737599      6     38 7.891497e{-}08}
\CommentTok{\#\textgreater{} 2            Pillai 1.0891707  7.972007      6     40 1.092397e{-}05}
\CommentTok{\#\textgreater{} 3  Hotelling{-}Lawley 6.2587852 18.776356      6     36 9.258571e{-}10}
\CommentTok{\#\textgreater{} 4               Roy 5.9550887 39.700592      3     20 1.302458e{-}08}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $\textasciigrave{}Ho: Profiles have equal levels\textasciigrave{}}
\CommentTok{\#\textgreater{}             Df Sum Sq Mean Sq F value  Pr(\textgreater{}F)   }
\CommentTok{\#\textgreater{} group        2  328.7  164.35   5.918 0.00915 **}
\CommentTok{\#\textgreater{} Residuals   21  583.2   27.77                   }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $\textasciigrave{}Ho: Profiles are flat\textasciigrave{}}
\CommentTok{\#\textgreater{}          F df1 df2      p{-}value}
\CommentTok{\#\textgreater{} 1 14.30928   3  19 4.096803e{-}05}
\CommentTok{\# reject null hypothesis of parallel profiles}
\CommentTok{\# reject the null hypothesis of coincidental profiles}
\CommentTok{\# reject the null hypothesis that the profiles are flat}
\end{Highlighting}
\end{Shaded}

\hypertarget{summary-6}{%
\subsection{Summary}\label{summary-6}}

\includegraphics[width=6.25in,height=4.16667in]{images/MANOVA_summary.PNG}

\hypertarget{principal-components}{%
\section{Principal Components}\label{principal-components}}

\begin{itemize}
\tightlist
\item
  Unsupervised learning
\item
  find important features
\item
  reduce the dimensions of the data set
\item
  ``decorrelate'' multivariate vectors that have dependence.
\item
  uses eigenvector/eigvenvalue decomposition of covariance (correlation) matrices.
\end{itemize}

According to the ``spectral decomposition theorem'', if \(\mathbf{\Sigma}_{p \times p}\) i s a positive semi-definite, symmetric, real matrix, then there exists an orthogonal matrix \(\mathbf{A}\) such that \(\mathbf{A'\Sigma A} = \Lambda\) where \(\Lambda\) is a diagonal matrix containing the eigenvalues \(\mathbf{\Sigma}\)

\[
\mathbf{\Lambda} = 
\left(
\begin{array}
{cccc}
\lambda_1 & 0 & \ldots & 0 \\
0 & \lambda_2 & \ldots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \ldots & \lambda_p
\end{array}
\right)
\]

\[
\mathbf{A} =
\left(
\begin{array}
{cccc}
\mathbf{a}_1 & \mathbf{a}_2 & \ldots & \mathbf{a}_p
\end{array}
\right)
\]

the i-th column of \(\mathbf{A}\) , \(\mathbf{a}_i\), is the i-th \(p \times 1\) eigenvector of \(\mathbf{\Sigma}\) that corresponds to the eigenvalue, \(\lambda_i\) , where \(\lambda_1 \ge \lambda_2 \ge \ldots \ge \lambda_p\) . Alternatively, express in matrix decomposition:

\[
\mathbf{\Sigma} = \mathbf{A \Lambda A}'
\]

\[
\mathbf{\Sigma} = \mathbf{A}
\left(
\begin{array}
{cccc}
\lambda_1 & 0 & \ldots & 0 \\
0 & \lambda_2 & \ldots & 0 \\
\vdots & \vdots& \ddots & \vdots \\
0 & 0 & \ldots & \lambda_p
\end{array}
\right)
\mathbf{A}'
= \sum_{i=1}^p \lambda_i \mathbf{a}_i \mathbf{a}_i'
\]

where the outer product \(\mathbf{a}_i \mathbf{a}_i'\) is a \(p \times p\) matrix of rank 1.

For example,

\(\mathbf{x} \sim N_2(\mathbf{\mu}, \mathbf{\Sigma})\)

\[
\mathbf{\mu} = 
\left(
\begin{array}
{c}
5 \\ 
12 
\end{array} 
\right);
\mathbf{\Sigma} = 
\left(
\begin{array}
{cc}
4 & 1 \\
1 & 2 
\end{array}
\right)
\]

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(MASS)}
\NormalTok{mu }\OtherTok{=} \FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{12}\NormalTok{))}
\NormalTok{Sigma }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{), }\AttributeTok{nrow =} \DecValTok{2}\NormalTok{, }\AttributeTok{byrow =}\NormalTok{ T)}
\NormalTok{sim }\OtherTok{\textless{}{-}} \FunctionTok{mvrnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{1000}\NormalTok{, }\AttributeTok{mu =}\NormalTok{ mu, }\AttributeTok{Sigma =}\NormalTok{ Sigma)}
\FunctionTok{plot}\NormalTok{(sim[, }\DecValTok{1}\NormalTok{], sim[, }\DecValTok{2}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{22-multivariate_files/figure-latex/unnamed-chunk-14-1} \end{center}

Here,

\[
\mathbf{A} = 
\left(
\begin{array}
{cc}
0.9239 & -0.3827 \\
0.3827 & 0.9239 \\
\end{array}
\right)
\]

Columns of \(\mathbf{A}\) are the eigenvectors for the decomposition

Under matrix multiplication (\(\mathbf{A'\Sigma A}\) or \(\mathbf{A'A}\) ), the off-diagonal elements equal to 0

Multiplying data by this matrix (i.e., projecting the data onto the orthogonal axes); the distribution of the resulting data (i.e., ``scores'') is

\[
N_2 (\mathbf{A'\mu,A'\Sigma A}) = N_2 (\mathbf{A'\mu, \Lambda})
\]

Equivalently,

\[
\mathbf{y} = \mathbf{A'x} \sim N
\left[
\left(
\begin{array}
{c}
9.2119 \\
9.1733 
\end{array}
\right),
\left(
\begin{array}
{cc}
4.4144 & 0 \\
0 & 1.5859 
\end{array}
\right)
\right]
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A\_matrix }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FloatTok{0.9239}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{0.3827}\NormalTok{, }\FloatTok{0.3827}\NormalTok{, }\FloatTok{0.9239}\NormalTok{),}
                  \AttributeTok{nrow =} \DecValTok{2}\NormalTok{,}
                  \AttributeTok{byrow =}\NormalTok{ T)}
\FunctionTok{t}\NormalTok{(A\_matrix) }\SpecialCharTok{\%*\%}\NormalTok{ A\_matrix}
\CommentTok{\#\textgreater{}          [,1]     [,2]}
\CommentTok{\#\textgreater{} [1,] 1.000051 0.000000}
\CommentTok{\#\textgreater{} [2,] 0.000000 1.000051}

\NormalTok{sim1 }\OtherTok{\textless{}{-}}
    \FunctionTok{mvrnorm}\NormalTok{(}
        \AttributeTok{n =} \DecValTok{1000}\NormalTok{,}
        \AttributeTok{mu =} \FunctionTok{t}\NormalTok{(A\_matrix) }\SpecialCharTok{\%*\%}\NormalTok{ mu,}
        \AttributeTok{Sigma =} \FunctionTok{t}\NormalTok{(A\_matrix) }\SpecialCharTok{\%*\%}\NormalTok{ Sigma }\SpecialCharTok{\%*\%}\NormalTok{ A\_matrix}
\NormalTok{    )}
\FunctionTok{plot}\NormalTok{(sim1[, }\DecValTok{1}\NormalTok{], sim1[, }\DecValTok{2}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{22-multivariate_files/figure-latex/unnamed-chunk-15-1} \end{center}

No more dependence in the data structure, plot

Notes:

\begin{itemize}
\item
  The i-th eigenvalue is the variance of a linear combination of the elements of \(\mathbf{x}\) ; \(var(y_i) = var(\mathbf{a'_i x}) = \lambda_i\)
\item
  The values on the transformed set of axes (i.e., the \(y_i\)'s) are called the scores. These are the orthogonal projections of the data onto the ``new principal component axes
\item
  Variances of \(y_1\) are greater than those for any other possible projection
\end{itemize}

Covariance matrix decomposition and projection onto orthogonal axes = PCA

\hypertarget{population-principal-components}{%
\subsection{Population Principal Components}\label{population-principal-components}}

\(p \times 1\) vectors \(\mathbf{x}_1, \dots , \mathbf{x}_n\) which are iid with \(var(\mathbf{x}_i) = \mathbf{\Sigma}\)

\begin{itemize}
\item
  The first PC is the linear combination \(y_1 = \mathbf{a}_1' \mathbf{x} = a_{11}x_1 + \dots + a_{1p}x_p\) with \(\mathbf{a}_1' \mathbf{a}_1 = 1\) such that \(var(y_1)\) is the maximum of all linear combinations of \(\mathbf{x}\) which have unit length
\item
  The second PC is the linear combination \(y_1 = \mathbf{a}_2' \mathbf{x} = a_{21}x_1 + \dots + a_{2p}x_p\) with \(\mathbf{a}_2' \mathbf{a}_2 = 1\) such that \(var(y_1)\) is the maximum of all linear combinations of \(\mathbf{x}\) which have unit length and uncorrelated with \(y_1\) (i.e., \(cov(\mathbf{a}_1' \mathbf{x}, \mathbf{a}'_2 \mathbf{x}) =0\)
\item
  continues for all \(y_i\) to \(y_p\)
\end{itemize}

\(\mathbf{a}_i\)'s are those that make up the matrix \(\mathbf{A}\) in the symmetric decomposition \(\mathbf{A'\Sigma A} = \mathbf{\Lambda}\) , where \(var(y_1) = \lambda_1, \dots , var(y_p) = \lambda_p\) And the total variance of \(\mathbf{x}\) is

\[
\begin{aligned}
var(x_1) + \dots + var(x_p) &= tr(\Sigma) = \lambda_1 + \dots + \lambda_p \\
&= var(y_1) + \dots + var(y_p) 
\end{aligned}
\]

Data Reduction

To reduce the dimension of data from p (original) to k dimensions without much ``loss of information'', we can use properties of the population principal components

\begin{itemize}
\item
  Suppose \(\mathbf{\Sigma} \approx \sum_{i=1}^k \lambda_i \mathbf{a}_i \mathbf{a}_i'\) . Even thought the true variance-covariance matrix has rank \(p\) , it can be be well approximate by a matrix of rank k (k \textless p)
\item
  New ``traits'' are linear combinations of the measured traits. We can attempt to make meaningful interpretation fo the combinations (with orthogonality constraints).
\item
  The proportion of the total variance accounted for by the j-th principal component is
\end{itemize}

\[
\frac{var(y_j)}{\sum_{i=1}^p var(y_i)} = \frac{\lambda_j}{\sum_{i=1}^p \lambda_i}
\]

\begin{itemize}
\item
  The proportion of the total variation accounted for by the first k principal components is \(\frac{\sum_{i=1}^k \lambda_i}{\sum_{i=1}^p \lambda_i}\)
\item
  Above example , we have \(4.4144/(4+2) = .735\) of the total variability can be explained by the first principal component
\end{itemize}

\hypertarget{sample-principal-components}{%
\subsection{Sample Principal Components}\label{sample-principal-components}}

Since \(\mathbf{\Sigma}\) is unknown, we use

\[
\mathbf{S} = \frac{1}{n-1}\sum_{i=1}^n (\mathbf{x}_i - \bar{\mathbf{x}})(\mathbf{x}_i - \bar{\mathbf{x}})'
\]

Let \(\hat{\lambda}_1 \ge \hat{\lambda}_2 \ge \dots \ge \hat{\lambda}_p \ge 0\) be the eigenvalues of \(\mathbf{S}\) and \(\hat{\mathbf{a}}_1, \hat{\mathbf{a}}_2, \dots, \hat{\mathbf{a}}_p\) denote the eigenvectors of \(\mathbf{S}\)

Then, the i-th sample principal component score (or principal component or score) is

\[
\hat{y}_{ij} = \sum_{k=1}^p \hat{a}_{ik}x_{kj} = \hat{\mathbf{a}}_i'\mathbf{x}_j
\]

\textbf{Properties of Sample Principal Components}

\begin{itemize}
\item
  The estimated variance of \(y_i = \hat{\mathbf{a}}_i'\mathbf{x}_j\) is \(\hat{\lambda}_i\)
\item
  The sample covariance between \(\hat{y}_i\) and \(\hat{y}_{i'}\) is 0 when \(i \neq i'\)
\item
  The proportion of the total sample variance accounted for by the i-th sample principal component is \(\frac{\hat{\lambda}_i}{\sum_{k=1}^p \hat{\lambda}_k}\)
\item
  The estimated correlation between the \(i\)-th principal component score and the \(l\)-th attribute of \(\mathbf{x}\) is
\end{itemize}

\[
r_{x_l , \hat{y}_i} = \frac{\hat{a}_{il}\sqrt{\lambda_i}}{\sqrt{s_{ll}}}
\]

\begin{itemize}
\item
  The correlation coefficient is typically used to interpret the components (i.e., if this correlation is high then it suggests that the l-th original trait is important in the i-th principle component). According to \citet{johnson2002applied}, pp.433-434, \(r_{x_l, \hat{y}_i}\) only measures the univariate contribution of an individual X to a component Y without taking into account the presence of the other X's. Hence, some prefer \(\hat{a}_{il}\) coefficient to interpret the principal component.
\item
  \(r_{x_l, \hat{y}_i} ; \hat{a}_{il}\) are referred to as ``loadings''
\end{itemize}

To use k principal components, we must calculate the scores for each data vector in the sample

\[
\mathbf{y}_j = 
\left(
\begin{array}
{c}
y_{1j} \\
y_{2j} \\
\vdots \\
y_{kj} 
\end{array}
\right) = 
\left(
\begin{array}
{c}
\hat{\mathbf{a}}_1' \mathbf{x}_j \\
\hat{\mathbf{a}}_2' \mathbf{x}_j \\
\vdots \\
\hat{\mathbf{a}}_k' \mathbf{x}_j
\end{array}
\right) = 
\left(
\begin{array}
{c}
\hat{\mathbf{a}}_1' \\
\hat{\mathbf{a}}_2' \\
\vdots \\
\hat{\mathbf{a}}_k'
\end{array}
\right) \mathbf{x}_j
\]

Issues:

\begin{itemize}
\item
  Large sample theory exists for eigenvalues and eigenvectors of sample covariance matrices if inference is necessary. But we do not do inference with PCA, we only use it as exploratory or descriptive analysis.
\item
  PC is not invariant to changes in scale (Exception: if all trait are rescaled by multiplying by the same constant, such as feet to inches).

  \begin{itemize}
  \item
    PCA based on the correlation matrix \(\mathbf{R}\) is different than that based on the covariance matrix \(\mathbf{\Sigma}\)
  \item
    PCA for the correlation matrix is just rescaling each trait to have unit variance
  \item
    Transform \(\mathbf{x}\) to \(\mathbf{z}\) where \(z_{ij} = (x_{ij} - \bar{x}_i)/\sqrt{s_{ii}}\) where the denominator affects the PCA
  \item
    After transformation, \(cov(\mathbf{z}) = \mathbf{R}\)
  \item
    PCA on \(\mathbf{R}\) is calculated in the same way as that on \(\mathbf{S}\) (where \(\hat{\lambda}{}_1 + \dots + \hat{\lambda}{}_p = p\) )
  \item
    The use of \(\mathbf{R}, \mathbf{S}\) depends on the purpose of PCA.

    \begin{itemize}
    \tightlist
    \item
      If the scale of the observations if different, covariance matrix is more preferable. but if they are dramatically different, analysis can still be dominated by the large variance traits.
    \end{itemize}
  \item
    How many PCs to use can be guided by

    \begin{itemize}
    \item
      Scree Graphs: plot the eigenvalues against their indices. Look for the ``elbow'' where the steep decline in the graph suddenly flattens out; or big gaps.
    \item
      minimum Percent of total variation (e.g., choose enough components to have 50\% or 90\%). can be used for interpretations.
    \item
      Kaiser's rule: use only those PC with eigenvalues larger than 1 (applied to PCA on the correlation matrix) - ad hoc
    \item
      Compare to the eigenvalue scree plot of data to the scree plot when the data are randomized.
    \end{itemize}
  \end{itemize}
\end{itemize}

\hypertarget{application-10}{%
\subsection{Application}\label{application-10}}

PCA on the covariance matrix is usually not preferred due to the fact that PCA is not invariant to changes in scale. Hence, PCA on the correlation matrix is more preferred

This also addresses the problem of multicollinearity

The eigvenvectors may differ by a multiplication of -1 for different implementation, but same interpretation.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\DocumentationTok{\#\# Read in and check data}
\NormalTok{stock }\OtherTok{\textless{}{-}} \FunctionTok{read.table}\NormalTok{(}\StringTok{"images/stock.dat"}\NormalTok{)}
\FunctionTok{names}\NormalTok{(stock) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"allied"}\NormalTok{, }\StringTok{"dupont"}\NormalTok{, }\StringTok{"carbide"}\NormalTok{, }\StringTok{"exxon"}\NormalTok{, }\StringTok{"texaco"}\NormalTok{)}
\FunctionTok{str}\NormalTok{(stock)}
\CommentTok{\#\textgreater{} \textquotesingle{}data.frame\textquotesingle{}:    100 obs. of  5 variables:}
\CommentTok{\#\textgreater{}  $ allied : num  0 0.027 0.1228 0.057 0.0637 ...}
\CommentTok{\#\textgreater{}  $ dupont : num  0 {-}0.04485 0.06077 0.02995 {-}0.00379 ...}
\CommentTok{\#\textgreater{}  $ carbide: num  0 {-}0.00303 0.08815 0.06681 {-}0.03979 ...}
\CommentTok{\#\textgreater{}  $ exxon  : num  0.0395 {-}0.0145 0.0862 0.0135 {-}0.0186 ...}
\CommentTok{\#\textgreater{}  $ texaco : num  0 0.0435 0.0781 0.0195 {-}0.0242 ...}

\DocumentationTok{\#\# Covariance matrix of data}
\FunctionTok{cov}\NormalTok{(stock)}
\CommentTok{\#\textgreater{}               allied       dupont      carbide        exxon       texaco}
\CommentTok{\#\textgreater{} allied  0.0016299269 0.0008166676 0.0008100713 0.0004422405 0.0005139715}
\CommentTok{\#\textgreater{} dupont  0.0008166676 0.0012293759 0.0008276330 0.0003868550 0.0003109431}
\CommentTok{\#\textgreater{} carbide 0.0008100713 0.0008276330 0.0015560763 0.0004872816 0.0004624767}
\CommentTok{\#\textgreater{} exxon   0.0004422405 0.0003868550 0.0004872816 0.0008023323 0.0004084734}
\CommentTok{\#\textgreater{} texaco  0.0005139715 0.0003109431 0.0004624767 0.0004084734 0.0007587370}

\DocumentationTok{\#\# Correlation matrix of data}
\FunctionTok{cor}\NormalTok{(stock)}
\CommentTok{\#\textgreater{}            allied    dupont   carbide     exxon    texaco}
\CommentTok{\#\textgreater{} allied  1.0000000 0.5769244 0.5086555 0.3867206 0.4621781}
\CommentTok{\#\textgreater{} dupont  0.5769244 1.0000000 0.5983841 0.3895191 0.3219534}
\CommentTok{\#\textgreater{} carbide 0.5086555 0.5983841 1.0000000 0.4361014 0.4256266}
\CommentTok{\#\textgreater{} exxon   0.3867206 0.3895191 0.4361014 1.0000000 0.5235293}
\CommentTok{\#\textgreater{} texaco  0.4621781 0.3219534 0.4256266 0.5235293 1.0000000}

\CommentTok{\# cov(scale(stock)) \# give the same result}

\DocumentationTok{\#\# PCA with covariance}
\NormalTok{cov\_pca }\OtherTok{\textless{}{-}} \FunctionTok{prcomp}\NormalTok{(stock) }
\CommentTok{\# uses singular value decomposition for calculation and an N {-}1 divisor}
\CommentTok{\# alternatively, princomp can do PCA via spectral decomposition, }
\CommentTok{\# but it has worse numerical accuracy}

\CommentTok{\# eigen values}
\NormalTok{cov\_results }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{eigen\_values =}\NormalTok{ cov\_pca}\SpecialCharTok{$}\NormalTok{sdev }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{)}
\NormalTok{cov\_results }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{proportion =}\NormalTok{ eigen\_values }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(eigen\_values),}
           \AttributeTok{cumulative =} \FunctionTok{cumsum}\NormalTok{(proportion)) }
\CommentTok{\#\textgreater{}   eigen\_values proportion cumulative}
\CommentTok{\#\textgreater{} 1 0.0035953867 0.60159252  0.6015925}
\CommentTok{\#\textgreater{} 2 0.0007921798 0.13255027  0.7341428}
\CommentTok{\#\textgreater{} 3 0.0007364426 0.12322412  0.8573669}
\CommentTok{\#\textgreater{} 4 0.0005086686 0.08511218  0.9424791}
\CommentTok{\#\textgreater{} 5 0.0003437707 0.05752091  1.0000000}
\CommentTok{\# first 2 PCs account for 73\% variance in the data}

\CommentTok{\# eigen vectors}
\NormalTok{cov\_pca}\SpecialCharTok{$}\NormalTok{rotation }\CommentTok{\# prcomp calls rotation}
\CommentTok{\#\textgreater{}               PC1         PC2        PC3         PC4         PC5}
\CommentTok{\#\textgreater{} allied  0.5605914  0.73884565 {-}0.1260222  0.28373183 {-}0.20846832}
\CommentTok{\#\textgreater{} dupont  0.4698673 {-}0.09286987 {-}0.4675066 {-}0.68793190  0.28069055}
\CommentTok{\#\textgreater{} carbide 0.5473322 {-}0.65401929 {-}0.1140581  0.50045312 {-}0.09603973}
\CommentTok{\#\textgreater{} exxon   0.2908932 {-}0.11267353  0.6099196 {-}0.43808002 {-}0.58203935}
\CommentTok{\#\textgreater{} texaco  0.2842017  0.07103332  0.6168831  0.06227778  0.72784638}
\CommentTok{\# princomp calls loadings.}

\CommentTok{\# first PC = overall average}
\CommentTok{\# second PC compares Allied to Carbide}

\DocumentationTok{\#\# PCA with correlation}
\CommentTok{\#same as scale(stock) \%\textgreater{}\% prcomp}
\NormalTok{cor\_pca }\OtherTok{\textless{}{-}} \FunctionTok{prcomp}\NormalTok{(stock, }\AttributeTok{scale =}\NormalTok{ T)}



\CommentTok{\# eigen values}
\NormalTok{cor\_results }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{eigen\_values =}\NormalTok{ cor\_pca}\SpecialCharTok{$}\NormalTok{sdev }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{)}
\NormalTok{cor\_results }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{proportion =}\NormalTok{ eigen\_values }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(eigen\_values),}
           \AttributeTok{cumulative =} \FunctionTok{cumsum}\NormalTok{(proportion))}
\CommentTok{\#\textgreater{}   eigen\_values proportion cumulative}
\CommentTok{\#\textgreater{} 1    2.8564869 0.57129738  0.5712974}
\CommentTok{\#\textgreater{} 2    0.8091185 0.16182370  0.7331211}
\CommentTok{\#\textgreater{} 3    0.5400440 0.10800880  0.8411299}
\CommentTok{\#\textgreater{} 4    0.4513468 0.09026936  0.9313992}
\CommentTok{\#\textgreater{} 5    0.3430038 0.06860076  1.0000000}

\CommentTok{\# first egiven values corresponds to less variance }
\CommentTok{\# than PCA based on the covariance matrix}

\CommentTok{\# eigen vectors}
\NormalTok{cor\_pca}\SpecialCharTok{$}\NormalTok{rotation}
\CommentTok{\#\textgreater{}               PC1        PC2        PC3        PC4        PC5}
\CommentTok{\#\textgreater{} allied  0.4635405 {-}0.2408499  0.6133570 {-}0.3813727  0.4532876}
\CommentTok{\#\textgreater{} dupont  0.4570764 {-}0.5090997 {-}0.1778996 {-}0.2113068 {-}0.6749814}
\CommentTok{\#\textgreater{} carbide 0.4699804 {-}0.2605774 {-}0.3370355  0.6640985  0.3957247}
\CommentTok{\#\textgreater{} exxon   0.4216770  0.5252647 {-}0.5390181 {-}0.4728036  0.1794482}
\CommentTok{\#\textgreater{} texaco  0.4213291  0.5822416  0.4336029  0.3812273 {-}0.3874672}
\CommentTok{\# interpretation of PC2 is different from above: }
\CommentTok{\# it is a comparison of Allied, Dupont and Carbid to Exxon and Texaco }
\end{Highlighting}
\end{Shaded}

Covid Example

To reduce collinearity problem in this dataset, we can use principal components as regressors.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{load}\NormalTok{(}\StringTok{\textquotesingle{}images/MOcovid.RData\textquotesingle{}}\NormalTok{)}
\NormalTok{covidpca }\OtherTok{\textless{}{-}} \FunctionTok{prcomp}\NormalTok{(ndat[,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{],}\AttributeTok{scale =}\NormalTok{ T,}\AttributeTok{center =}\NormalTok{ T)}

\NormalTok{covidpca}\SpecialCharTok{$}\NormalTok{rotation[,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{]}
\CommentTok{\#\textgreater{}                                                          PC1         PC2}
\CommentTok{\#\textgreater{} X..Population.in.Rural.Areas                      0.32865838  0.05090955}
\CommentTok{\#\textgreater{} Area..sq..miles.                                  0.12014444 {-}0.28579183}
\CommentTok{\#\textgreater{} Population.density..sq..miles.                   {-}0.29670124  0.28312922}
\CommentTok{\#\textgreater{} Literacy.rate                                    {-}0.12517700 {-}0.08999542}
\CommentTok{\#\textgreater{} Families                                         {-}0.25856941  0.16485752}
\CommentTok{\#\textgreater{} Area.of.farm.land..sq..miles.                     0.02101106 {-}0.31070363}
\CommentTok{\#\textgreater{} Number.of.farms                                  {-}0.03814582 {-}0.44809679}
\CommentTok{\#\textgreater{} Average.value.of.all.property.per.farm..dollars. {-}0.05410709  0.14404306}
\CommentTok{\#\textgreater{} Estimation.of.rurality..                         {-}0.19040210  0.12089501}
\CommentTok{\#\textgreater{} Male..                                            0.02182394 {-}0.09568768}
\CommentTok{\#\textgreater{} Number.of.Physcians.per.100.000                  {-}0.31451606  0.13598026}
\CommentTok{\#\textgreater{} average.age                                       0.29414708  0.35593459}
\CommentTok{\#\textgreater{} X0.4.age.proportion                              {-}0.11431336 {-}0.23574057}
\CommentTok{\#\textgreater{} X20.44.age.proportion                            {-}0.32802128 {-}0.22718550}
\CommentTok{\#\textgreater{} X65.and.over.age.proportion                       0.30585033  0.32201626}
\CommentTok{\#\textgreater{} prop..White..nonHisp                              0.35627561 {-}0.14142646}
\CommentTok{\#\textgreater{} prop..Hispanic                                   {-}0.16655381 {-}0.15105342}
\CommentTok{\#\textgreater{} prop..Black                                      {-}0.33333359  0.24405802}


\CommentTok{\# Variability of each principal component: pr.var}
\NormalTok{pr.var }\OtherTok{\textless{}{-}}\NormalTok{ covidpca}\SpecialCharTok{$}\NormalTok{sdev }\SpecialCharTok{\^{}} \DecValTok{2}
\CommentTok{\# Variance explained by each principal component: pve}
\NormalTok{pve }\OtherTok{\textless{}{-}}\NormalTok{ pr.var }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(pr.var)}
\FunctionTok{plot}\NormalTok{(}
\NormalTok{    pve,}
    \AttributeTok{xlab =} \StringTok{"Principal Component"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"Proportion of Variance Explained"}\NormalTok{,}
    \AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.5}\NormalTok{),}
    \AttributeTok{type =} \StringTok{"b"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{22-multivariate_files/figure-latex/unnamed-chunk-17-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\FunctionTok{plot}\NormalTok{(}
    \FunctionTok{cumsum}\NormalTok{(pve),}
    \AttributeTok{xlab =} \StringTok{"Principal Component"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"Cumulative Proportion of Variance Explained"}\NormalTok{,}
    \AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{),}
    \AttributeTok{type =} \StringTok{"b"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{22-multivariate_files/figure-latex/unnamed-chunk-17-2} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# the first six principe account for around 80\% of the variance. }


\CommentTok{\#using base lm function for PC regression}
\NormalTok{pcadat }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(covidpca}\SpecialCharTok{$}\NormalTok{x[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{])}
\NormalTok{pcadat}\SpecialCharTok{$}\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ ndat}\SpecialCharTok{$}\NormalTok{Y}
\NormalTok{pcr.man }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(y) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., pcadat)}
\FunctionTok{mean}\NormalTok{(pcr.man}\SpecialCharTok{$}\NormalTok{residuals }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 0.03453371}

\CommentTok{\#comparison to lm w/o prin comps}
\NormalTok{lm.fit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(Y) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ ndat)}
\FunctionTok{mean}\NormalTok{(lm.fit}\SpecialCharTok{$}\NormalTok{residuals }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 0.02335128}
\end{Highlighting}
\end{Shaded}

MSE for the PC-based model is larger than regular regression, because models with a large degree of collinearity can still perform well.

\texttt{pcr} function in \texttt{pls} can be used for fitting PC regression (it will select the optimal number of components in the model).

\hypertarget{factor-analysis}{%
\section{Factor Analysis}\label{factor-analysis}}

Purpose

\begin{itemize}
\item
  Using a few linear combinations of underlying unobservable (latent) traits, we try to describe the covariance relationship among a large number of measured traits
\item
  Similar to \protect\hyperlink{principal-components}{PCA}, but factor analysis is \textbf{model based}
\end{itemize}

More details can be found on \href{https://online.stat.psu.edu/stat505/book/export/html/691}{PSU stat} or \href{http://users.stat.umn.edu/~helwig/notes/factanal-Notes.pdf}{UMN stat}

Let \(\mathbf{y}\) be the set of \(p\) measured variables

\(E(\mathbf{y}) = \mathbf{\mu}\)

\(var(\mathbf{y}) = \mathbf{\Sigma}\)

We have

\[
\begin{aligned}
\mathbf{y} - \mathbf{\mu} &= \mathbf{Lf} + \epsilon \\
&= 
\left(
\begin{array}
{c}
l_{11}f_1 + l_{12}f_2 + \dots + l_{tm}f_m \\
\vdots \\
l_{p1}f_1 + l_{p2}f_2 + \dots + l_{pm} f_m
\end{array}
\right)
+ 
\left(
\begin{array}
{c}
\epsilon_1 \\
\vdots \\
\epsilon_p
\end{array}
\right)
\end{aligned}
\]

where

\begin{itemize}
\item
  \(\mathbf{y} - \mathbf{\mu}\) = the p centered measurements
\item
  \(\mathbf{L}\) = \(p \times m\) matrix of factor loadings
\item
  \(\mathbf{f}\) = unobserved common factors for the population
\item
  \(\mathbf{\epsilon}\) = random errors (i.e., variation that is not accounted for by the common factors).
\end{itemize}

We want \(m\) (the number of factors) to be much smaller than \(p\) (the number of measured attributes)

\textbf{Restrictions on the model}

\begin{itemize}
\item
  \(E(\epsilon) = \mathbf{0}\)
\item
  \(var(\epsilon) = \Psi_{p \times p} = diag( \psi_1, \dots, \psi_p)\)
\item
  \(\mathbf{\epsilon}, \mathbf{f}\) are independent
\item
  Additional assumption could be \(E(\mathbf{f}) = \mathbf{0}, var(\mathbf{f}) = \mathbf{I}_{m \times m}\) (known as the orthogonal factor model) , which imposes the following covariance structure on \(\mathbf{y}\)
\end{itemize}

\[
\begin{aligned}
var(\mathbf{y}) = \mathbf{\Sigma} &=  var(\mathbf{Lf} + \mathbf{\epsilon}) \\
&= var(\mathbf{Lf}) + var(\epsilon) \\
&= \mathbf{L} var(\mathbf{f}) \mathbf{L}' + \mathbf{\Psi} \\
&= \mathbf{LIL}' + \mathbf{\Psi} \\
&= \mathbf{LL}' + \mathbf{\Psi}
\end{aligned}
\]

Since \(\mathbf{\Psi}\) is diagonal, the off-diagonal elements of \(\mathbf{LL}'\) are \(\sigma_{ij}\), the co variances in \(\mathbf{\Sigma}\), which means \(cov(y_i, y_j) = \sum_{k=1}^m l_{ik}l_{jk}\) and the covariance of \(\mathbf{y}\) is completely determined by the m factors ( \(m <<p\))

\(var(y_i) = \sum_{k=1}^m l_{ik}^2 + \psi_i\) where \(\psi_i\) is the \textbf{specific variance} and the summation term is the i-th \textbf{communality} (i.e., portion of the variance of the i-th variable contributed by the \(m\) common factors (\(h_i^2 = \sum_{k=1}^m l_{ik}^2\))

The factor model is only uniquely determined up to an orthogonal transformation of the factors.

Let \(\mathbf{T}_{m \times m}\) be an orthogonal matrix \(\mathbf{TT}' = \mathbf{T'T} = \mathbf{I}\) then

\[
\begin{aligned}
\mathbf{y} - \mathbf{\mu} &= \mathbf{Lf} + \epsilon \\
&= \mathbf{LTT'f} + \epsilon \\
&= \mathbf{L}^*(\mathbf{T'f}) + \epsilon & \text{where } \mathbf{L}^* = \mathbf{LT}
\end{aligned}
\]

and

\[
\begin{aligned}
\mathbf{\Sigma} &= \mathbf{LL}' + \mathbf{\Psi} \\
&= \mathbf{LTT'L} + \mathbf{\Psi} \\
&= (\mathbf{L}^*)(\mathbf{L}^*)' + \mathbf{\Psi}
\end{aligned}
\]

Hence, any orthogonal transformation of the factors is an equally good description of the correlations among the observed traits.

Let \(\mathbf{y} = \mathbf{Cx}\) , where \(\mathbf{C}\) is any diagonal matrix, then \(\mathbf{L}_y = \mathbf{CL}_x\) and \(\mathbf{\Psi}_y = \mathbf{C\Psi}_x\mathbf{C}\)

Hence, we can see that factor analysis is also invariant to changes in scale

\hypertarget{methods-of-estimation}{%
\subsection{Methods of Estimation}\label{methods-of-estimation}}

To estimate \(\mathbf{L}\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \protect\hyperlink{principal-component-method}{Principal Component Method}
\item
  \protect\hyperlink{principal-factor-method}{Principal Factor Method}
\item
  \ref{maximum-likelihood-method-factor-analysis}
\end{enumerate}

\hypertarget{principal-component-method}{%
\subsubsection{Principal Component Method}\label{principal-component-method}}

Spectral decomposition

\[
\begin{aligned}
\mathbf{\Sigma} &= \lambda_1 \mathbf{a}_1 \mathbf{a}_1' + \dots + \lambda_p \mathbf{a}_p \mathbf{a}_p' \\
&= \mathbf{A\Lambda A}' \\
&= \sum_{k=1}^m \lambda+k \mathbf{a}_k \mathbf{a}_k' + \sum_{k= m+1}^p \lambda_k \mathbf{a}_k \mathbf{a}_k' \\
&= \sum_{k=1}^m l_k l_k' + \sum_{k=m+1}^p \lambda_k \mathbf{a}_k \mathbf{a}_k'
\end{aligned}
\]

where \(l_k = \mathbf{a}_k \sqrt{\lambda_k}\) and the second term is not diagonal in general.

Assume

\[
\psi_i = \sigma_{ii} - \sum_{k=1}^m l_{ik}^2 = \sigma_{ii} -  \sum_{k=1}^m \lambda_i a_{ik}^2
\]

then

\[
\mathbf{\Sigma} \approx \mathbf{LL}' + \mathbf{\Psi}
\]

To estimate \(\mathbf{L}\) and \(\Psi\) , we use the expected eigenvalues and eigenvectors from \(\mathbf{S}\) or \(\mathbf{R}\)

\begin{itemize}
\item
  The estimated factor loadings don't change as the number of actors increases
\item
  The diagonal elements of \(\hat{\mathbf{L}}\hat{\mathbf{L}}' + \hat{\mathbf{\Psi}}\) are equal to the diagonal elements of \(\mathbf{S}\) and \(\mathbf{R}\), but the covariances may not be exactly reproduced
\item
  We select \(m\) so that the off-diagonal elements close to the values in \(\mathbf{S}\) (or to make the off-diagonal elements of \(\mathbf{S} - \hat{\mathbf{L}} \hat{\mathbf{L}}' + \hat{\mathbf{\Psi}}\) small)
\end{itemize}

\hypertarget{principal-factor-method}{%
\subsubsection{Principal Factor Method}\label{principal-factor-method}}

Consider modeling the correlation matrix, \(\mathbf{R} = \mathbf{L} \mathbf{L}' + \mathbf{\Psi}\) . Then

\[
\mathbf{L} \mathbf{L}' = \mathbf{R} - \mathbf{\Psi} =
\left(
\begin{array}
{cccc}
h_1^2 & r_{12} & \dots & r_{1p} \\
r_{21} & h_2^2 & \dots & r_{2p} \\
\vdots & \vdots & \ddots & \vdots \\
r_{p1} & r_{p2} & \dots & h_p^2
\end{array}
\right)
\]

where \(h_i^2 = 1- \psi_i\) (the communality)

Suppose that initial estimates are available for the communalities, \((h_1^*)^2,(h_2^*)^2, \dots , (h_p^*)^2\), then we can regress each trait on all the others, and then use the \(r^2\) as \(h^2\)

The estimate of \(\mathbf{R} - \mathbf{\Psi}\) at step k is

\[
(\mathbf{R} - \mathbf{\Psi})_k = 
\left(
\begin{array}
{cccc}
(h_1^*)^2 & r_{12} & \dots & r_{1p} \\
r_{21} & (h_2^*)^2 & \dots & r_{2p} \\
\vdots & \vdots & \ddots & \vdots \\
r_{p1} & r_{p2} & \dots & (h_p^*)^2
\end{array}
\right) = 
\mathbf{L}_k^*(\mathbf{L}_k^*)' 
\]

where

\[
\mathbf{L}_k^* = (\sqrt{\hat{\lambda}_1^*\hat{\mathbf{a}}_1^* , \dots \hat{\lambda}_m^*\hat{\mathbf{a}}_m^*})
\]

and

\[
\hat{\psi}_{i,k}^* = 1 - \sum_{j=1}^m \hat{\lambda}_i^* (\hat{a}_{ij}^*)^2
\]

we used the spectral decomposition on the estimated matrix \((\mathbf{R}- \mathbf{\Psi})\) to calculate the \(\hat{\lambda}_i^* s\) and the \(\mathbf{\hat{a}}_i^* s\)

After updating the values of \((\hat{h}_i^*)^2 = 1 - \hat{\psi}_{i,k}^*\) we will use them to form a new \(\mathbf{L}_{k+1}^*\) via another spectral decomposition. Repeat the process

Notes:

\begin{itemize}
\item
  The matrix \((\mathbf{R} - \mathbf{\Psi})_k\) is not necessarily positive definite
\item
  The principal component method is similar to principal factor if one considers the initial communalities are \(h^2 = 1\)
\item
  if \(m\) is too large, some communalities may become larger than 1, causing the iterations to terminate. To combat, we can

  \begin{itemize}
  \item
    fix any communality that is greater than 1 at 1 and then continues.
  \item
    continue iterations regardless of the size of the communalities. However, results can be outside fo the parameter space.
  \end{itemize}
\end{itemize}

\hypertarget{maximum-likelihood-method-factor-analysis}{%
\subsubsection{Maximum Likelihood Method}\label{maximum-likelihood-method-factor-analysis}}

Since we need the likelihood function, we make the additional (critical) assumption that

\begin{itemize}
\item
  \(\mathbf{y}_j \sim N(\mathbf{\mu},\mathbf{\Sigma})\) for \(j = 1,..,n\)
\item
  \(\mathbf{f} \sim N(\mathbf{0}, \mathbf{I})\)
\item
  \(\epsilon_j \sim N(\mathbf{0}, \mathbf{\Psi})\)
\end{itemize}

and restriction

\begin{itemize}
\tightlist
\item
  \(\mathbf{L}' \mathbf{\Psi}^{-1}\mathbf{L} = \mathbf{\Delta}\) where \(\mathbf{\Delta}\) is a diagonal matrix. (since the factor loading matrix is not unique, we need this restriction).
\end{itemize}

Notes:

\begin{itemize}
\item
  Finding MLE can be computationally expensive
\item
  we typically use other methods for exploratory data analysis
\item
  Likelihood ratio tests could be used for testing hypotheses in this framework (i.e., Confirmatory Factor Analysis)
\end{itemize}

\hypertarget{factor-rotation}{%
\subsection{Factor Rotation}\label{factor-rotation}}

\(\mathbf{T}_{m \times m}\) is an orthogonal matrix that has the property that

\[
\hat{\mathbf{L}} \hat{\mathbf{L}}' + \hat{\mathbf{\Psi}} = \hat{\mathbf{L}}^*(\hat{\mathbf{L}}^*)' + \hat{\mathbf{\Psi}}
\]

where \(\mathbf{L}^* = \mathbf{LT}\)

This means that estimated specific variances and communalities are not altered by the orthogonal transformation.

Since there are an infinite number of choices for \(\mathbf{T}\), some selection criterion is necessary

For example, we can find the orthogonal transformation that maximizes the objective function

\[
\sum_{j = 1}^m [\frac{1}{p}\sum_{i=1}^p (\frac{l_{ij}^{*2}}{h_i})^2 - \{\frac{\gamma}{p} \sum_{i=1}^p (\frac{l_{ij}^{*2}}{h_i})^2 \}^2]
\]

where \(\frac{l_{ij}^{*2}}{h_i}\) are ``scaled loadings'', which gives variables with small communalities more influence.

Different choices of \(\gamma\) in the objective function correspond to different orthogonal rotation found in the literature;

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Varimax \(\gamma = 1\) (rotate the factors so that each of the \(p\) variables should have a high loading on only one factor, but this is not always possible).
\item
  Quartimax \(\gamma = 0\)
\item
  Equimax \(\gamma = m/2\)
\item
  Parsimax \(\gamma = \frac{p(m-1)}{p+m-2}\)
\item
  Promax: non-orthogonal or olique transformations
\item
  Harris-Kaiser (HK): non-orthogonal or oblique transformations
\end{enumerate}

\hypertarget{estimation-of-factor-scores}{%
\subsection{Estimation of Factor Scores}\label{estimation-of-factor-scores}}

Recall

\[
(\mathbf{y}_j - \mathbf{\mu}) = \mathbf{L}_{p \times m}\mathbf{f}_j + \epsilon_j
\]

If the factor model is correct then

\[
var(\epsilon_j) = \mathbf{\Psi} = diag (\psi_1, \dots , \psi_p)
\]

Thus we could consider using weighted least squares to estimate \(\mathbf{f}_j\) , the vector of factor scores for the j-th sampled unit by

\[
\begin{aligned}
\hat{\mathbf{f}} &= (\mathbf{L}'\mathbf{\Psi}^{-1} \mathbf{L})^{-1} \mathbf{L}' \mathbf{\Psi}^{-1}(\mathbf{y}_j - \mathbf{\mu}) \\
& \approx (\mathbf{L}'\mathbf{\Psi}^{-1} \mathbf{L})^{-1} \mathbf{L}' \mathbf{\Psi}^{-1}(\mathbf{y}_j - \mathbf{\bar{y}})
\end{aligned}
\]

\hypertarget{the-regression-method}{%
\subsubsection{The Regression Method}\label{the-regression-method}}

Alternatively, we can use the regression method to estimate the factor scores

Consider the joint distribution of \((\mathbf{y}_j - \mathbf{\mu})\) and \(\mathbf{f}_j\) assuming multivariate normality, as in the maximum likelihood approach. then,

\[
\left(
\begin{array}
{c}
\mathbf{y}_j - \mathbf{\mu} \\
\mathbf{f}_j
\end{array}
\right) \sim
N_{p + m}
\left(
\left[
\begin{array}
{cc}
\mathbf{LL}' + \mathbf{\Psi} & \mathbf{L} \\
\mathbf{L}' & \mathbf{I}_{m\times m}
\end{array}
\right]
\right)
\]

when the \(m\) factor model is correct

Hence,

\[
E(\mathbf{f}_j | \mathbf{y}_j - \mathbf{\mu}) = \mathbf{L}' (\mathbf{LL}' + \mathbf{\Psi})^{-1}(\mathbf{y}_j - \mathbf{\mu})
\]

notice that \(\mathbf{L}' (\mathbf{LL}' + \mathbf{\Psi})^{-1}\) is an \(m \times p\) matrix of regression coefficients

Then, we use the estimated conditional mean vector to estimate the factor scores

\[
\mathbf{\hat{f}}_j = \mathbf{\hat{L}}'(\mathbf{\hat{L}}\mathbf{\hat{L}}' + \mathbf{\hat{\Psi}})^{-1}(\mathbf{y}_j - \mathbf{\bar{y}})
\]

Alternatively, we could reduce the effect of possible incorrect determination fo the number of factors \(m\) by using \(\mathbf{S}\) as a substitute for \(\mathbf{\hat{L}}\mathbf{\hat{L}}' + \mathbf{\hat{\Psi}}\) then

\[
\mathbf{\hat{f}}_j = \mathbf{\hat{L}}'\mathbf{S}^{-1}(\mathbf{y}_j - \mathbf{\bar{y}})
\]

where \(j = 1,\dots,n\)

\hypertarget{model-diagnostic}{%
\subsection{Model Diagnostic}\label{model-diagnostic}}

\begin{itemize}
\item
  Plots
\item
  Check for outliers (recall that \(\mathbf{f}_j \sim iid N(\mathbf{0}, \mathbf{I}_{m \times m})\))
\item
  Check for multivariate normality assumption
\item
  Use univariate tests for normality to check the factor scores
\item
  \textbf{Confirmatory Factor Analysis}: formal testing of hypotheses about loadings, use MLE and full/reduced model testing paradigm and measures of model fit
\end{itemize}

\hypertarget{application-11}{%
\subsection{Application}\label{application-11}}

In the \texttt{psych} package,

\begin{itemize}
\item
  h2 = the communalities
\item
  u2 = the uniqueness
\item
  com = the complexity
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(psych)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\DocumentationTok{\#\# Load the data from the psych package}
\FunctionTok{data}\NormalTok{(Harman}\FloatTok{.5}\NormalTok{)}
\NormalTok{Harman}\FloatTok{.5}
\CommentTok{\#\textgreater{}         population schooling employment professional housevalue}
\CommentTok{\#\textgreater{} Tract1        5700      12.8       2500          270      25000}
\CommentTok{\#\textgreater{} Tract2        1000      10.9        600           10      10000}
\CommentTok{\#\textgreater{} Tract3        3400       8.8       1000           10       9000}
\CommentTok{\#\textgreater{} Tract4        3800      13.6       1700          140      25000}
\CommentTok{\#\textgreater{} Tract5        4000      12.8       1600          140      25000}
\CommentTok{\#\textgreater{} Tract6        8200       8.3       2600           60      12000}
\CommentTok{\#\textgreater{} Tract7        1200      11.4        400           10      16000}
\CommentTok{\#\textgreater{} Tract8        9100      11.5       3300           60      14000}
\CommentTok{\#\textgreater{} Tract9        9900      12.5       3400          180      18000}
\CommentTok{\#\textgreater{} Tract10       9600      13.7       3600          390      25000}
\CommentTok{\#\textgreater{} Tract11       9600       9.6       3300           80      12000}
\CommentTok{\#\textgreater{} Tract12       9400      11.4       4000          100      13000}

\CommentTok{\# Correlation matrix}
\NormalTok{cor\_mat }\OtherTok{\textless{}{-}} \FunctionTok{cor}\NormalTok{(Harman}\FloatTok{.5}\NormalTok{)}
\NormalTok{cor\_mat}
\CommentTok{\#\textgreater{}              population  schooling employment professional housevalue}
\CommentTok{\#\textgreater{} population   1.00000000 0.00975059  0.9724483    0.4388708 0.02241157}
\CommentTok{\#\textgreater{} schooling    0.00975059 1.00000000  0.1542838    0.6914082 0.86307009}
\CommentTok{\#\textgreater{} employment   0.97244826 0.15428378  1.0000000    0.5147184 0.12192599}
\CommentTok{\#\textgreater{} professional 0.43887083 0.69140824  0.5147184    1.0000000 0.77765425}
\CommentTok{\#\textgreater{} housevalue   0.02241157 0.86307009  0.1219260    0.7776543 1.00000000}

\DocumentationTok{\#\# Principal Component Method with Correlation}
\NormalTok{cor\_pca }\OtherTok{\textless{}{-}} \FunctionTok{prcomp}\NormalTok{(Harman}\FloatTok{.5}\NormalTok{, }\AttributeTok{scale =}\NormalTok{ T)}
\CommentTok{\# eigen values}
\NormalTok{cor\_results }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{eigen\_values =}\NormalTok{ cor\_pca}\SpecialCharTok{$}\NormalTok{sdev }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{)}

\NormalTok{cor\_results }\OtherTok{\textless{}{-}}\NormalTok{ cor\_results }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}
        \AttributeTok{proportion =}\NormalTok{ eigen\_values }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(eigen\_values),}
        \AttributeTok{cumulative =} \FunctionTok{cumsum}\NormalTok{(proportion),}
        \AttributeTok{number =} \FunctionTok{row\_number}\NormalTok{()}
\NormalTok{    )}
\NormalTok{cor\_results}
\CommentTok{\#\textgreater{}   eigen\_values  proportion cumulative number}
\CommentTok{\#\textgreater{} 1   2.87331359 0.574662719  0.5746627      1}
\CommentTok{\#\textgreater{} 2   1.79666009 0.359332019  0.9339947      2}
\CommentTok{\#\textgreater{} 3   0.21483689 0.042967377  0.9769621      3}
\CommentTok{\#\textgreater{} 4   0.09993405 0.019986811  0.9969489      4}
\CommentTok{\#\textgreater{} 5   0.01525537 0.003051075  1.0000000      5}

\CommentTok{\# Scree plot of Eigenvalues}
\NormalTok{scree\_gg }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(cor\_results, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ number, }\AttributeTok{y =}\NormalTok{ eigen\_values)) }\SpecialCharTok{+}
    \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_text}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =}\NormalTok{ number)) }\SpecialCharTok{+}
    \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{name =} \StringTok{"Number"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{name =} \StringTok{"Eigenvalue"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{theme\_bw}\NormalTok{()}
\NormalTok{scree\_gg}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{22-multivariate_files/figure-latex/unnamed-chunk-18-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\FunctionTok{screeplot}\NormalTok{(cor\_pca, }\AttributeTok{type =} \StringTok{\textquotesingle{}lines\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{22-multivariate_files/figure-latex/unnamed-chunk-18-2} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\DocumentationTok{\#\# Keep 2 factors based on scree plot and eigenvalues}
\NormalTok{factor\_pca }\OtherTok{\textless{}{-}} \FunctionTok{principal}\NormalTok{(Harman}\FloatTok{.5}\NormalTok{, }\AttributeTok{nfactors =} \DecValTok{2}\NormalTok{, }\AttributeTok{rotate =} \StringTok{"none"}\NormalTok{)}
\NormalTok{factor\_pca}
\CommentTok{\#\textgreater{} Principal Components Analysis}
\CommentTok{\#\textgreater{} Call: principal(r = Harman.5, nfactors = 2, rotate = "none")}
\CommentTok{\#\textgreater{} Standardized loadings (pattern matrix) based upon correlation matrix}
\CommentTok{\#\textgreater{}               PC1   PC2   h2    u2 com}
\CommentTok{\#\textgreater{} population   0.58  0.81 0.99 0.012 1.8}
\CommentTok{\#\textgreater{} schooling    0.77 {-}0.54 0.89 0.115 1.8}
\CommentTok{\#\textgreater{} employment   0.67  0.73 0.98 0.021 2.0}
\CommentTok{\#\textgreater{} professional 0.93 {-}0.10 0.88 0.120 1.0}
\CommentTok{\#\textgreater{} housevalue   0.79 {-}0.56 0.94 0.062 1.8}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}                        PC1  PC2}
\CommentTok{\#\textgreater{} SS loadings           2.87 1.80}
\CommentTok{\#\textgreater{} Proportion Var        0.57 0.36}
\CommentTok{\#\textgreater{} Cumulative Var        0.57 0.93}
\CommentTok{\#\textgreater{} Proportion Explained  0.62 0.38}
\CommentTok{\#\textgreater{} Cumulative Proportion 0.62 1.00}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Mean item complexity =  1.7}
\CommentTok{\#\textgreater{} Test of the hypothesis that 2 components are sufficient.}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} The root mean square of the residuals (RMSR) is  0.03 }
\CommentTok{\#\textgreater{}  with the empirical chi square  0.29  with prob \textless{}  0.59 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Fit based upon off diagonal values = 1}

\CommentTok{\# factor 1 = overall socioeconomic health}
\CommentTok{\# factor 2 = contrast of the population and employment against school and house value}


\DocumentationTok{\#\# Ssquared multiple correlation (SMC) prior, no rotation}
\NormalTok{factor\_pca\_smc }\OtherTok{\textless{}{-}} \FunctionTok{fa}\NormalTok{(}
\NormalTok{    Harman}\FloatTok{.5}\NormalTok{,}
    \AttributeTok{nfactors =} \DecValTok{2}\NormalTok{,}
    \AttributeTok{fm =} \StringTok{"pa"}\NormalTok{,}
    \AttributeTok{rotate =} \StringTok{"none"}\NormalTok{,}
    \AttributeTok{SMC =} \ConstantTok{TRUE}
\NormalTok{)}
\NormalTok{factor\_pca\_smc}
\CommentTok{\#\textgreater{} Factor Analysis using method =  pa}
\CommentTok{\#\textgreater{} Call: fa(r = Harman.5, nfactors = 2, rotate = "none", SMC = TRUE, fm = "pa")}
\CommentTok{\#\textgreater{} Standardized loadings (pattern matrix) based upon correlation matrix}
\CommentTok{\#\textgreater{}               PA1   PA2   h2      u2 com}
\CommentTok{\#\textgreater{} population   0.62  0.78 1.00 {-}0.0027 1.9}
\CommentTok{\#\textgreater{} schooling    0.70 {-}0.53 0.77  0.2277 1.9}
\CommentTok{\#\textgreater{} employment   0.70  0.68 0.96  0.0413 2.0}
\CommentTok{\#\textgreater{} professional 0.88 {-}0.15 0.80  0.2017 1.1}
\CommentTok{\#\textgreater{} housevalue   0.78 {-}0.60 0.96  0.0361 1.9}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}                        PA1  PA2}
\CommentTok{\#\textgreater{} SS loadings           2.76 1.74}
\CommentTok{\#\textgreater{} Proportion Var        0.55 0.35}
\CommentTok{\#\textgreater{} Cumulative Var        0.55 0.90}
\CommentTok{\#\textgreater{} Proportion Explained  0.61 0.39}
\CommentTok{\#\textgreater{} Cumulative Proportion 0.61 1.00}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Mean item complexity =  1.7}
\CommentTok{\#\textgreater{} Test of the hypothesis that 2 factors are sufficient.}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} df null model =  10  with the objective function =  6.38 with Chi Square =  54.25}
\CommentTok{\#\textgreater{} df of  the model are 1  and the objective function was  0.34 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} The root mean square of the residuals (RMSR) is  0.01 }
\CommentTok{\#\textgreater{} The df corrected root mean square of the residuals is  0.03 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} The harmonic n.obs is  12 with the empirical chi square  0.02  with prob \textless{}  0.88 }
\CommentTok{\#\textgreater{} The total n.obs was  12  with Likelihood Chi Square =  2.44  with prob \textless{}  0.12 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Tucker Lewis Index of factoring reliability =  0.596}
\CommentTok{\#\textgreater{} RMSEA index =  0.336  and the 90 \% confidence intervals are  0 0.967}
\CommentTok{\#\textgreater{} BIC =  {-}0.04}
\CommentTok{\#\textgreater{} Fit based upon off diagonal values = 1}

\DocumentationTok{\#\# SMC prior, Promax rotation}
\NormalTok{factor\_pca\_smc\_pro }\OtherTok{\textless{}{-}} \FunctionTok{fa}\NormalTok{(}
\NormalTok{    Harman}\FloatTok{.5}\NormalTok{,}
    \AttributeTok{nfactors =} \DecValTok{2}\NormalTok{,}
    \AttributeTok{fm =} \StringTok{"pa"}\NormalTok{,}
    \AttributeTok{rotate =} \StringTok{"Promax"}\NormalTok{,}
    \AttributeTok{SMC =} \ConstantTok{TRUE}
\NormalTok{)}
\NormalTok{factor\_pca\_smc\_pro}
\CommentTok{\#\textgreater{} Factor Analysis using method =  pa}
\CommentTok{\#\textgreater{} Call: fa(r = Harman.5, nfactors = 2, rotate = "Promax", SMC = TRUE, }
\CommentTok{\#\textgreater{}     fm = "pa")}
\CommentTok{\#\textgreater{} Standardized loadings (pattern matrix) based upon correlation matrix}
\CommentTok{\#\textgreater{}                PA1   PA2   h2      u2 com}
\CommentTok{\#\textgreater{} population   {-}0.11  1.02 1.00 {-}0.0027 1.0}
\CommentTok{\#\textgreater{} schooling     0.90 {-}0.11 0.77  0.2277 1.0}
\CommentTok{\#\textgreater{} employment    0.02  0.97 0.96  0.0413 1.0}
\CommentTok{\#\textgreater{} professional  0.75  0.33 0.80  0.2017 1.4}
\CommentTok{\#\textgreater{} housevalue    1.01 {-}0.14 0.96  0.0361 1.0}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}                        PA1  PA2}
\CommentTok{\#\textgreater{} SS loadings           2.38 2.11}
\CommentTok{\#\textgreater{} Proportion Var        0.48 0.42}
\CommentTok{\#\textgreater{} Cumulative Var        0.48 0.90}
\CommentTok{\#\textgreater{} Proportion Explained  0.53 0.47}
\CommentTok{\#\textgreater{} Cumulative Proportion 0.53 1.00}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  With factor correlations of }
\CommentTok{\#\textgreater{}      PA1  PA2}
\CommentTok{\#\textgreater{} PA1 1.00 0.25}
\CommentTok{\#\textgreater{} PA2 0.25 1.00}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Mean item complexity =  1.1}
\CommentTok{\#\textgreater{} Test of the hypothesis that 2 factors are sufficient.}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} df null model =  10  with the objective function =  6.38 with Chi Square =  54.25}
\CommentTok{\#\textgreater{} df of  the model are 1  and the objective function was  0.34 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} The root mean square of the residuals (RMSR) is  0.01 }
\CommentTok{\#\textgreater{} The df corrected root mean square of the residuals is  0.03 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} The harmonic n.obs is  12 with the empirical chi square  0.02  with prob \textless{}  0.88 }
\CommentTok{\#\textgreater{} The total n.obs was  12  with Likelihood Chi Square =  2.44  with prob \textless{}  0.12 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Tucker Lewis Index of factoring reliability =  0.596}
\CommentTok{\#\textgreater{} RMSEA index =  0.336  and the 90 \% confidence intervals are  0 0.967}
\CommentTok{\#\textgreater{} BIC =  {-}0.04}
\CommentTok{\#\textgreater{} Fit based upon off diagonal values = 1}

\DocumentationTok{\#\# SMC prior, varimax rotation}
\NormalTok{factor\_pca\_smc\_var }\OtherTok{\textless{}{-}} \FunctionTok{fa}\NormalTok{(}
\NormalTok{    Harman}\FloatTok{.5}\NormalTok{,}
    \AttributeTok{nfactors =} \DecValTok{2}\NormalTok{,}
    \AttributeTok{fm =} \StringTok{"pa"}\NormalTok{,}
    \AttributeTok{rotate =} \StringTok{"varimax"}\NormalTok{,}
    \AttributeTok{SMC =} \ConstantTok{TRUE}
\NormalTok{)}
\DocumentationTok{\#\# Make a data frame of the loadings for ggplot2}
\NormalTok{factors\_df }\OtherTok{\textless{}{-}}
    \FunctionTok{bind\_rows}\NormalTok{(}
        \FunctionTok{data.frame}\NormalTok{(}
            \AttributeTok{y =} \FunctionTok{rownames}\NormalTok{(factor\_pca\_smc}\SpecialCharTok{$}\NormalTok{loadings),}
            \FunctionTok{unclass}\NormalTok{(factor\_pca\_smc}\SpecialCharTok{$}\NormalTok{loadings)}
\NormalTok{        ),}
        \FunctionTok{data.frame}\NormalTok{(}
            \AttributeTok{y =} \FunctionTok{rownames}\NormalTok{(factor\_pca\_smc\_pro}\SpecialCharTok{$}\NormalTok{loadings),}
            \FunctionTok{unclass}\NormalTok{(factor\_pca\_smc\_pro}\SpecialCharTok{$}\NormalTok{loadings)}
\NormalTok{        ),}
        \FunctionTok{data.frame}\NormalTok{(}
            \AttributeTok{y =} \FunctionTok{rownames}\NormalTok{(factor\_pca\_smc\_var}\SpecialCharTok{$}\NormalTok{loadings),}
            \FunctionTok{unclass}\NormalTok{(factor\_pca\_smc\_var}\SpecialCharTok{$}\NormalTok{loadings)}
\NormalTok{        ),}
        \AttributeTok{.id =} \StringTok{"Rotation"}
\NormalTok{    )}
\NormalTok{flag\_gg }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(factors\_df) }\SpecialCharTok{+}
    \FunctionTok{geom\_vline}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{xintercept =} \DecValTok{0}\NormalTok{)) }\SpecialCharTok{+}
    \FunctionTok{geom\_hline}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{)) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}
        \AttributeTok{x =}\NormalTok{ PA2,}
        \AttributeTok{y =}\NormalTok{ PA1,}
        \AttributeTok{col =}\NormalTok{ y,}
        \AttributeTok{shape =}\NormalTok{ y}
\NormalTok{    ), }\AttributeTok{size =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{name =} \StringTok{"Factor 2"}\NormalTok{, }\AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{1.1}\NormalTok{, }\FloatTok{1.1}\NormalTok{)) }\SpecialCharTok{+}
    \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{name =} \StringTok{"Factor1"}\NormalTok{, }\AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{1.1}\NormalTok{, }\FloatTok{1.1}\NormalTok{)) }\SpecialCharTok{+}
    \FunctionTok{facet\_wrap}\NormalTok{(}\StringTok{"Rotation"}\NormalTok{, }\AttributeTok{labeller =} \FunctionTok{labeller}\NormalTok{(}\AttributeTok{Rotation =} \FunctionTok{c}\NormalTok{(}
        \StringTok{"1"} \OtherTok{=} \StringTok{"Original"}\NormalTok{, }\StringTok{"2"} \OtherTok{=} \StringTok{"Promax"}\NormalTok{, }\StringTok{"3"} \OtherTok{=} \StringTok{"Varimax"}
\NormalTok{    ))) }\SpecialCharTok{+}
    \FunctionTok{coord\_fixed}\NormalTok{(}\AttributeTok{ratio =} \DecValTok{1}\NormalTok{) }\CommentTok{\# make aspect ratio of each facet 1}

\NormalTok{flag\_gg}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{22-multivariate_files/figure-latex/unnamed-chunk-18-3} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# promax and varimax did a good job to assign trait to a particular factor}

\NormalTok{factor\_mle\_1 }\OtherTok{\textless{}{-}} \FunctionTok{fa}\NormalTok{(}
\NormalTok{    Harman}\FloatTok{.5}\NormalTok{,}
    \AttributeTok{nfactors =} \DecValTok{1}\NormalTok{,}
    \AttributeTok{fm =} \StringTok{"mle"}\NormalTok{,}
    \AttributeTok{rotate =} \StringTok{"none"}\NormalTok{,}
    \AttributeTok{SMC =} \ConstantTok{TRUE}
\NormalTok{)}
\NormalTok{factor\_mle\_1}
\CommentTok{\#\textgreater{} Factor Analysis using method =  ml}
\CommentTok{\#\textgreater{} Call: fa(r = Harman.5, nfactors = 1, rotate = "none", SMC = TRUE, fm = "mle")}
\CommentTok{\#\textgreater{} Standardized loadings (pattern matrix) based upon correlation matrix}
\CommentTok{\#\textgreater{}               ML1    h2     u2 com}
\CommentTok{\#\textgreater{} population   0.97 0.950 0.0503   1}
\CommentTok{\#\textgreater{} schooling    0.14 0.021 0.9791   1}
\CommentTok{\#\textgreater{} employment   1.00 0.995 0.0049   1}
\CommentTok{\#\textgreater{} professional 0.51 0.261 0.7388   1}
\CommentTok{\#\textgreater{} housevalue   0.12 0.014 0.9864   1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}                 ML1}
\CommentTok{\#\textgreater{} SS loadings    2.24}
\CommentTok{\#\textgreater{} Proportion Var 0.45}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Mean item complexity =  1}
\CommentTok{\#\textgreater{} Test of the hypothesis that 1 factor is sufficient.}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} df null model =  10  with the objective function =  6.38 with Chi Square =  54.25}
\CommentTok{\#\textgreater{} df of  the model are 5  and the objective function was  3.14 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} The root mean square of the residuals (RMSR) is  0.41 }
\CommentTok{\#\textgreater{} The df corrected root mean square of the residuals is  0.57 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} The harmonic n.obs is  12 with the empirical chi square  39.41  with prob \textless{}  2e{-}07 }
\CommentTok{\#\textgreater{} The total n.obs was  12  with Likelihood Chi Square =  24.56  with prob \textless{}  0.00017 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Tucker Lewis Index of factoring reliability =  0.022}
\CommentTok{\#\textgreater{} RMSEA index =  0.564  and the 90 \% confidence intervals are  0.374 0.841}
\CommentTok{\#\textgreater{} BIC =  12.14}
\CommentTok{\#\textgreater{} Fit based upon off diagonal values = 0.5}
\CommentTok{\#\textgreater{} Measures of factor score adequacy             }
\CommentTok{\#\textgreater{}                                                    ML1}
\CommentTok{\#\textgreater{} Correlation of (regression) scores with factors   1.00}
\CommentTok{\#\textgreater{} Multiple R square of scores with factors          1.00}
\CommentTok{\#\textgreater{} Minimum correlation of possible factor scores     0.99}

\NormalTok{factor\_mle\_2 }\OtherTok{\textless{}{-}} \FunctionTok{fa}\NormalTok{(}
\NormalTok{    Harman}\FloatTok{.5}\NormalTok{,}
    \AttributeTok{nfactors =} \DecValTok{2}\NormalTok{,}
    \AttributeTok{fm =} \StringTok{"mle"}\NormalTok{,}
    \AttributeTok{rotate =} \StringTok{"none"}\NormalTok{,}
    \AttributeTok{SMC =} \ConstantTok{TRUE}
\NormalTok{)}
\NormalTok{factor\_mle\_2}
\CommentTok{\#\textgreater{} Factor Analysis using method =  ml}
\CommentTok{\#\textgreater{} Call: fa(r = Harman.5, nfactors = 2, rotate = "none", SMC = TRUE, fm = "mle")}
\CommentTok{\#\textgreater{} Standardized loadings (pattern matrix) based upon correlation matrix}
\CommentTok{\#\textgreater{}                ML2  ML1   h2    u2 com}
\CommentTok{\#\textgreater{} population   {-}0.03 1.00 1.00 0.005 1.0}
\CommentTok{\#\textgreater{} schooling     0.90 0.04 0.81 0.193 1.0}
\CommentTok{\#\textgreater{} employment    0.09 0.98 0.96 0.036 1.0}
\CommentTok{\#\textgreater{} professional  0.78 0.46 0.81 0.185 1.6}
\CommentTok{\#\textgreater{} housevalue    0.96 0.05 0.93 0.074 1.0}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}                        ML2  ML1}
\CommentTok{\#\textgreater{} SS loadings           2.34 2.16}
\CommentTok{\#\textgreater{} Proportion Var        0.47 0.43}
\CommentTok{\#\textgreater{} Cumulative Var        0.47 0.90}
\CommentTok{\#\textgreater{} Proportion Explained  0.52 0.48}
\CommentTok{\#\textgreater{} Cumulative Proportion 0.52 1.00}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Mean item complexity =  1.1}
\CommentTok{\#\textgreater{} Test of the hypothesis that 2 factors are sufficient.}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} df null model =  10  with the objective function =  6.38 with Chi Square =  54.25}
\CommentTok{\#\textgreater{} df of  the model are 1  and the objective function was  0.31 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} The root mean square of the residuals (RMSR) is  0.01 }
\CommentTok{\#\textgreater{} The df corrected root mean square of the residuals is  0.05 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} The harmonic n.obs is  12 with the empirical chi square  0.05  with prob \textless{}  0.82 }
\CommentTok{\#\textgreater{} The total n.obs was  12  with Likelihood Chi Square =  2.22  with prob \textless{}  0.14 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Tucker Lewis Index of factoring reliability =  0.658}
\CommentTok{\#\textgreater{} RMSEA index =  0.307  and the 90 \% confidence intervals are  0 0.945}
\CommentTok{\#\textgreater{} BIC =  {-}0.26}
\CommentTok{\#\textgreater{} Fit based upon off diagonal values = 1}
\CommentTok{\#\textgreater{} Measures of factor score adequacy             }
\CommentTok{\#\textgreater{}                                                    ML2  ML1}
\CommentTok{\#\textgreater{} Correlation of (regression) scores with factors   0.98 1.00}
\CommentTok{\#\textgreater{} Multiple R square of scores with factors          0.95 1.00}
\CommentTok{\#\textgreater{} Minimum correlation of possible factor scores     0.91 0.99}

\NormalTok{factor\_mle\_3 }\OtherTok{\textless{}{-}} \FunctionTok{fa}\NormalTok{(}
\NormalTok{    Harman}\FloatTok{.5}\NormalTok{,}
    \AttributeTok{nfactors =} \DecValTok{3}\NormalTok{,}
    \AttributeTok{fm =} \StringTok{"mle"}\NormalTok{,}
    \AttributeTok{rotate =} \StringTok{"none"}\NormalTok{,}
    \AttributeTok{SMC =} \ConstantTok{TRUE}
\NormalTok{)}
\NormalTok{factor\_mle\_3}
\CommentTok{\#\textgreater{} Factor Analysis using method =  ml}
\CommentTok{\#\textgreater{} Call: fa(r = Harman.5, nfactors = 3, rotate = "none", SMC = TRUE, fm = "mle")}
\CommentTok{\#\textgreater{} Standardized loadings (pattern matrix) based upon correlation matrix}
\CommentTok{\#\textgreater{}                ML2  ML1   ML3   h2     u2 com}
\CommentTok{\#\textgreater{} population   {-}0.12 0.98 {-}0.11 0.98 0.0162 1.1}
\CommentTok{\#\textgreater{} schooling     0.89 0.15  0.29 0.90 0.0991 1.3}
\CommentTok{\#\textgreater{} employment    0.00 1.00  0.04 0.99 0.0052 1.0}
\CommentTok{\#\textgreater{} professional  0.72 0.52 {-}0.10 0.80 0.1971 1.9}
\CommentTok{\#\textgreater{} housevalue    0.97 0.13 {-}0.09 0.97 0.0285 1.1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}                        ML2  ML1  ML3}
\CommentTok{\#\textgreater{} SS loadings           2.28 2.26 0.11}
\CommentTok{\#\textgreater{} Proportion Var        0.46 0.45 0.02}
\CommentTok{\#\textgreater{} Cumulative Var        0.46 0.91 0.93}
\CommentTok{\#\textgreater{} Proportion Explained  0.49 0.49 0.02}
\CommentTok{\#\textgreater{} Cumulative Proportion 0.49 0.98 1.00}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Mean item complexity =  1.2}
\CommentTok{\#\textgreater{} Test of the hypothesis that 3 factors are sufficient.}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} df null model =  10  with the objective function =  6.38 with Chi Square =  54.25}
\CommentTok{\#\textgreater{} df of  the model are {-}2  and the objective function was  0 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} The root mean square of the residuals (RMSR) is  0 }
\CommentTok{\#\textgreater{} The df corrected root mean square of the residuals is  NA }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} The harmonic n.obs is  12 with the empirical chi square  0  with prob \textless{}  NA }
\CommentTok{\#\textgreater{} The total n.obs was  12  with Likelihood Chi Square =  0  with prob \textless{}  NA }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Tucker Lewis Index of factoring reliability =  1.318}
\CommentTok{\#\textgreater{} Fit based upon off diagonal values = 1}
\CommentTok{\#\textgreater{} Measures of factor score adequacy             }
\CommentTok{\#\textgreater{}                                                    ML2  ML1  ML3}
\CommentTok{\#\textgreater{} Correlation of (regression) scores with factors   0.99 1.00 0.82}
\CommentTok{\#\textgreater{} Multiple R square of scores with factors          0.98 1.00 0.68}
\CommentTok{\#\textgreater{} Minimum correlation of possible factor scores     0.96 0.99 0.36}
\end{Highlighting}
\end{Shaded}

The output info for the null hypothesis of no common factors is in the statement ``The degrees of freedom for the null model ..''

The output info for the null hypothesis that number of factors is sufficient is in the statement ``The total number of observations was \ldots{}''

One factor is not enough, two is sufficient, and not enough data for 3 factors (df of -2 and NA for p-value). Hence, we should use 2-factor model.

\hypertarget{discriminant-analysis}{%
\section{Discriminant Analysis}\label{discriminant-analysis}}

Suppose we have two or more different populations from which observations could come from. Discriminant analysis seeks to determine which of the possible population an observation comes from while making as few mistakes as possible

\begin{itemize}
\item
  This is an alternative to logistic approaches with the following advantages:

  \begin{itemize}
  \item
    when there is clear separation between classes, the parameter estimates for the logic regression model can be \textbf{surprisingly} unstable, while discriminant approaches do not suffer
  \item
    If X is normal in each of the classes and the sample size is small, then discriminant approaches can be more accurate
  \end{itemize}
\end{itemize}

Notation

Similar to MANOVA, let \(\mathbf{y}_{j1},\mathbf{y}_{j2},\dots, \mathbf{y}_{in_j} \sim iid f_j (\mathbf{y})\) for \(j = 1,\dots, h\)

Let \(f_j(\mathbf{y})\) be the density function for population j . Note that each vector \(\mathbf{y}\) contain measurements on all \(p\) traits

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Assume that each observation is from one of \(h\) possible populations.
\item
  We want to form a discriminant rule that will allocate an observation \(\mathbf{y}\) to population j when \(\mathbf{y}\) is in fact from this population
\end{enumerate}

\hypertarget{known-populations}{%
\subsection{Known Populations}\label{known-populations}}

The maximum likelihood discriminant rule for assigning an observation \(\mathbf{y}\) to one of the \(h\) populations allocates \(\mathbf{y}\) to the population that gives the largest likelihood to \(\mathbf{y}\)

Consider the likelihood for a single observation \(\mathbf{y}\), which has the form \(f_j (\mathbf{y})\) where j is the true population.

Since \(j\) is unknown, to make the likelihood as large as possible, we should choose the value j which causes \(f_j (\mathbf{y})\) to be as large as possible

Consider a simple univariate example. Suppose we have data from one of two binomial populations.

\begin{itemize}
\item
  The first population has \(n= 10\) trials with success probability \(p = .5\)
\item
  The second population has \(n= 10\) trials with success probability \(p = .7\)
\item
  to which population would we assign an observation of \(y = 7\)
\item
  Note:

  \begin{itemize}
  \item
    \(f(y = 7|n = 10, p = .5) = .117\)
  \item
    \(f(y = 7|n = 10, p = .7) = .267\) where \(f(.)\) is the binomial likelihood.
  \item
    Hence, we choose the second population
  \end{itemize}
\end{itemize}

Another example

We have 2 populations, where

\begin{itemize}
\item
  First population: \(N(\mu_1, \sigma^2_1)\)
\item
  Second population: \(N(\mu_2, \sigma^2_2)\)
\end{itemize}

The likelihood for a single observation is

\[
f_j (y) = (2\pi \sigma^2_j)^{-1/2} \exp\{ -\frac{1}{2}(\frac{y - \mu_j}{\sigma_j})^2\}
\]

Consider a likelihood ratio rule

\[
\begin{aligned}
\Lambda &= \frac{\text{likelihood of y from pop 1}}{\text{likelihood of y from pop 2}} \\
&= \frac{f_1(y)}{f_2(y)} \\
&= \frac{\sigma_2}{\sigma_1} \exp\{-\frac{1}{2}[(\frac{y - \mu_1}{\sigma_1})^2- (\frac{y - \mu_2}{\sigma_2})^2] \}
\end{aligned}
\]

Hence, we classify into

\begin{itemize}
\item
  pop 1 if \(\Lambda >1\)
\item
  pop 2 if \(\Lambda <1\)
\item
  for ties, flip a coin
\end{itemize}

Another way to think:

we classify into population 1 if the ``standardized distance'' of y from \(\mu_1\) is less than the ``standardized distance'' of y from \(\mu_2\) which is referred to as a \textbf{quadratic discriminant rule}.

(Significant simplification occurs in th special case where \(\sigma_1 = \sigma_2 = \sigma^2\))

Thus, we classify into population 1 if

\[
(y - \mu_2)^2 > (y - \mu_1)^2
\]

or

\[
|y- \mu_2| > |y - \mu_1|
\]

and

\[
-2 \log (\Lambda) = -2y  \frac{(\mu_1 - \mu_2)}{\sigma^2} + \frac{(\mu_1^2 - \mu_2^2)}{\sigma^2} = \beta y + \alpha
\]

Thus, we classify into population 1 if this is less than 0.

Discriminant classification rule is linear in y in this case.

\hypertarget{multivariate-expansion}{%
\subsubsection{Multivariate Expansion}\label{multivariate-expansion}}

Suppose that there are 2 populations

\begin{itemize}
\item
  \(N_p(\mathbf{\mu}_1, \mathbf{\Sigma}_1)\)
\item
  \(N_p(\mathbf{\mu}_2, \mathbf{\Sigma}_2)\)
\end{itemize}

\[
\begin{aligned}
-2 \log(\frac{f_1 (\mathbf{x})}{f_2 (\mathbf{x})}) &= \log|\mathbf{\Sigma}_1| + (\mathbf{x} - \mathbf{\mu}_1)' \mathbf{\Sigma}^{-1}_1 (\mathbf{x} - \mathbf{\mu}_1) \\
&- [\log|\mathbf{\Sigma}_2|+ (\mathbf{x} - \mathbf{\mu}_2)' \mathbf{\Sigma}^{-1}_2 (\mathbf{x} - \mathbf{\mu}_2) ]
\end{aligned}
\]

Again, we classify into population 1 if this is less than 0, otherwise, population 2. And like the univariate case with non-equal variances, this is a quadratic discriminant rule.

And if the covariance matrices are equal: \(\mathbf{\Sigma}_1 = \mathbf{\Sigma}_2 = \mathbf{\Sigma}_1\) classify into population 1 if

\[
(\mathbf{\mu}_1 - \mathbf{\mu}_2)' \mathbf{\Sigma}^{-1}\mathbf{x} - \frac{1}{2} (\mathbf{\mu}_1 - \mathbf{\mu}_2)' \mathbf{\Sigma}^{-1} (\mathbf{\mu}_1 - \mathbf{\mu}_2) \ge 0
\]

This linear discriminant rule is also referred to as \textbf{Fisher's linear discriminant function}

By \textbf{assuming the covariance matrices are equal, we assume that the shape and orientation fo the two populations must be the same (which can be a strong restriction)}

In other words, for each variable, it can have different mean but the same variance.

\begin{itemize}
\tightlist
\item
  Note: LDA Bayes decision boundary is linear. Hence, quadratic decision boundary might lead to better classification. Moreover, the assumption of same variance/covariance matrix across all classes for Gaussian densities imposes the linear rule, if we allow the predictors in each class to follow MVN distribution with class-specific mean vectors and variance/covariance matrices, then it is \textbf{Quadratic Discriminant Analysis.} But then, you will have more parameters to estimate (which gives more flexibility than LDA) at the cost of more variance (bias -variance tradeoff).
\end{itemize}

When \(\mathbf{\mu}_1, \mathbf{\mu}_2, \mathbf{\Sigma}\) are known, the probability of misclassification can be determined:

\[
\begin{aligned}
P(2|1) &= P(\text{calssify into pop 2| x is from pop 1}) \\
&= P((\mathbf{\mu}_1 - \mathbf{\mu}_2)' \mathbf{\Sigma}^{-1} \mathbf{x} \le \frac{1}{2} (\mathbf{\mu}_1 - \mathbf{\mu}_2)' \mathbf{\Sigma}^{-1} (\mathbf{\mu}_1 - \mathbf{\mu}_2)|\mathbf{x} \sim N(\mu_1, \mathbf{\Sigma}) \\
&= \Phi(-\frac{1}{2} \delta)
\end{aligned}
\]

where

\begin{itemize}
\item
  \(\delta^2 = (\mathbf{\mu}_1 - \mathbf{\mu}_2)' \mathbf{\Sigma}^{-1} (\mathbf{\mu}_1 - \mathbf{\mu}_2)\)
\item
  \(\Phi\) is the standard normal CDF
\end{itemize}

Suppose there are \(h\) possible populations, which are distributed as \(N_p (\mathbf{\mu}_p, \mathbf{\Sigma})\). Then, the maximum likelihood (linear) discriminant rule allocates \(\mathbf{y}\) to population j where j minimizes the squared Mahalanobis distance

\[
(\mathbf{y} - \mathbf{\mu}_j)' \mathbf{\Sigma}^{-1} (\mathbf{y} - \mathbf{\mu}_j)
\]

\hypertarget{bayes-discriminant-rules}{%
\subsubsection{Bayes Discriminant Rules}\label{bayes-discriminant-rules}}

If we know that population j has prior probabilities \(\pi_j\) (assume \(\pi_j >0\)) we can form the Bayes discriminant rule.

This rule allocates an observation \(\mathbf{y}\) to the population for which \(\pi_j f_j (\mathbf{y})\) is maximized.

Note:

\begin{itemize}
\tightlist
\item
  \textbf{Maximum likelihood discriminant rule} is a special case of the \textbf{Bayes discriminant rule}, where it sets all the \(\pi_j = 1/h\)
\end{itemize}

Optimal Properties of Bayes Discriminant Rules

\begin{itemize}
\item
  let \(p_{ii}\) be the probability of correctly assigning an observation from population i
\item
  then one rule (with probabilities \(p_{ii}\) ) is as good as another rule (with probabilities \(p_{ii}'\) ) if \(p_{ii} \ge p_{ii}'\) for all \(i = 1,\dots, h\)
\item
  The first rule is better than the alternative if \(p_{ii} > p_{ii}'\) for at least one i.
\item
  A rule for which there is no better alternative is called admissible
\item
  Bayes Discriminant Rules are admissible
\item
  If we utilized prior probabilities, then we can form the posterior probability of a correct allocation, \(\sum_{i=1}^h \pi_i p_{ii}\)
\item
  Bayes Discriminant Rules have the largest possible posterior probability of correct allocation with respect to the prior
\item
  These properties show that \textbf{Bayes Discriminant rule is our best approach}.
\end{itemize}

Unequal Cost

\begin{itemize}
\item
  We want to consider the cost misallocation

  \begin{itemize}
  \tightlist
  \item
    Define \(c_{ij}\) to be the cost associated with allocation a member of population j to population i.
  \end{itemize}
\item
  Assume that

  \begin{itemize}
  \item
    \(c_{ij} >0\) for all \(i \neq j\)
  \item
    \(c_{ij} = 0\) if \(i = j\)
  \end{itemize}
\item
  We could determine the expected amount of loss for an observation allocated to population i as \(\sum_j c_{ij} p_{ij}\) where the \(p_{ij}s\) are the probabilities of allocating an observation from population j into population i
\item
  We want to minimize the amount of loss expected for our rule. Using a Bayes Discrimination, allocate \(\mathbf{y}\) to the population j which minimizes \(\sum_{k \neq j} c_{ij} \pi_k f_k(\mathbf{y})\)
\item
  We could assign equal probabilities to each group and get a maximum likelihood type rule. here, we would allocate \(\mathbf{y}\) to population j which minimizes \(\sum_{k \neq j}c_{jk} f_k(\mathbf{y})\)
\end{itemize}

\textbf{Example}:

Two binomial populations, each of size 10, with probabilities \(p_1 = .5\) and \(p_2 = .7\)

And the probability of being in the first population is .9

However, suppose the cost of inappropriately allocating into the first population is 1 and the cost of incorrectly allocating into the second population is 5.

In this case, we pick population 1 over population 2

In general, we consider two regions, \(R_1\) and \(R_2\) associated with population 1 and 2:

\[
R_1: \frac{f_1 (\mathbf{x})}{f_2 (\mathbf{x})} \ge \frac{c_{12} \pi_2}{c_{21} \pi_1}
\]

\[
R_2: \frac{f_1 (\mathbf{x})}{f_2 (\mathbf{x})} < \frac{c_{12} \pi_2}{c_{21} \pi_1}
\]

where \(c_{12}\) is the cost of assigning a member of population 2 to population 1.

\hypertarget{discrimination-under-estimation}{%
\subsubsection{Discrimination Under Estimation}\label{discrimination-under-estimation}}

Suppose we know the form of the distributions for populations of interests, but we still have to estimate the parameters.

Example:

we know the distributions are multivariate normal, but we have to estimate the means and variances

The maximum likelihood discriminant rule allocates an observation \(\mathbf{y}\) to population j when j maximizes the function

\[
f_j (\mathbf{y} |\hat{\theta})
\]

where \(\hat{\theta}\) are the maximum likelihood estimates of the unknown parameters

For instance, we have 2 multivariate normal populations with distinct means, but common variance covariance matrix

MLEs for \(\mathbf{\mu}_1\) and \(\mathbf{\mu}_2\) are \(\mathbf{\bar{y}}_1\) and \(\mathbf{\bar{y}}_2\)and common \(\mathbf{\Sigma}\) is \(\mathbf{S}\).

Thus, an estimated discriminant rule could be formed by substituting these sample values for the population values

\hypertarget{native-bayes}{%
\subsubsection{Native Bayes}\label{native-bayes}}

\begin{itemize}
\item
  The challenge with classification using Bayes' is that we don't know the (true) densities, \(f_k, k = 1, \dots, K\), while LDA and QDA make \textbf{strong multivariate normality assumptions} to deal with this.
\item
  Naive Bayes makes only one assumption: \textbf{within the k-th class, the p predictors are independent (i.e,, for} \(k = 1,\dots, K\)
\end{itemize}

\[
f_k(x) = f_{k1}(x_1) \times f_{k2}(x_2) \times \dots \times f_{kp}(x_p)
\]

where \(f_{kj}\) is the density function of the j-th predictor among observation in the k-th class.

This assumption allows the use of joint distribution without the need to account for dependence between observations. However, this (native) assumption can be unrealistic, but still works well in cases where the number of sample (n) is not large relative to the number of features (p).

With this assumption, we have

\[
P(Y=k|X=x) = \frac{\pi_k \times f_{k1}(x_1) \times \dots \times f_{kp}(x_p)}{\sum_{l=1}^K \pi_l \times f_{l1}(x_1)\times \dots f_{lp}(x_p)}
\]

we only need to estimate the one-dimensional density function \(f_{kj}\) with either of these approaches:

\begin{itemize}
\item
  When \(X_j\) is quantitative, assume it has a univariate normal distribution (with independence): \(X_j | Y = k \sim N(\mu_{jk}, \sigma^2_{jk})\) which is more restrictive than QDA because it assumes predictors are independent (e.g., a diagonal covariance matrix)
\item
  When \(X_j\) is quantitative, use a kernel density estimator \protect\hyperlink{kernel-methods}{Kernel Methods} ; which is a smoothed histogram
\item
  When \(X_j\) is qualitative, we count the promotion of training observations for the j-th predictor corresponding to each class.
\end{itemize}

\hypertarget{comparison-of-classification-methods}{%
\subsubsection{Comparison of Classification Methods}\label{comparison-of-classification-methods}}

Assuming we have K classes and K is the baseline from (James , Witten, Hastie, and Tibshirani book)

Comparing the log odds relative to the K class

\hypertarget{logistic-regression-2}{%
\paragraph{Logistic Regression}\label{logistic-regression-2}}

\[
\log(\frac{P(Y=k|X = x)}{P(Y = K| X = x)}) = \beta_{k0} + \sum_{j=1}^p \beta_{kj}x_j
\]

\hypertarget{lda}{%
\paragraph{LDA}\label{lda}}

\[
\log(\frac{P(Y = k | X = x)}{P(Y = K | X = x)} = a_k + \sum_{j=1}^p b_{kj} x_j
\]

where \(a_k\) and \(b_{kj}\) are functions of \(\pi_k, \pi_K, \mu_k , \mu_K, \mathbf{\Sigma}\)

Similar to logistic regression, LDA assumes the log odds is linear in \(x\)

Even though they look like having the same form, the parameters in logistic regression are estimated by MLE, where as LDA linear parameters are specified by the prior and normal distributions

We expect LDA to outperform logistic regression when the normality assumption (approximately) holds, and logistic regression to perform better when it does not

\hypertarget{qda}{%
\paragraph{QDA}\label{qda}}

\[
\log(\frac{P(Y=k|X=x}{P(Y=K | X = x}) = a_k + \sum_{j=1}^{p}b_{kj}x_{j} + \sum_{j=1}^p \sum_{l=1}^p c_{kjl}x_j x_l 
\]

where \(a_k, b_{kj}, c_{kjl}\) are functions \(\pi_k , \pi_K, \mu_k, \mu_K ,\mathbf{\Sigma}_k, \mathbf{\Sigma}_K\)

\hypertarget{naive-bayes}{%
\paragraph{Naive Bayes}\label{naive-bayes}}

\[
\log (\frac{P(Y = k | X = x)}{P(Y = K | X = x}) = a_k + \sum_{j=1}^p g_{kj} (x_j)
\]

where \(a_k = \log (\pi_k / \pi_K)\) and \(g_{kj}(x_j) = \log(\frac{f_{kj}(x_j)}{f_{Kj}(x_j)})\) which is the form of generalized additive model

\hypertarget{summary-7}{%
\paragraph{Summary}\label{summary-7}}

\begin{itemize}
\item
  LDA is a special case of QDA
\item
  LDA is robust when it comes to high dimensions
\item
  Any classifier with a linear decision boundary is a special case of naive Bayes with \(g_{kj}(x_j) = b_{kj} x_j\), which means LDA is a special case of naive Bayes. LDA assumes that the features are normally distributed with a common within-class covariance matrix, and naive Bayes assumes independence of the features.
\item
  Naive bayes is also a special case of LDA with \(\mathbf{\Sigma}\) restricted to a diagonal matrix with diagonals, \(\sigma^2\) (another notation \(diag (\mathbf{\Sigma})\) ) assuming \(f_{kj}(x_j) = N(\mu_{kj}, \sigma^2_j)\)
\item
  QDA and naive Bayes are not special case of each other. In principal,e naive Bayes can produce a more flexible fit by the choice of \(g_{kj}(x_j)\) , but it's restricted to only purely additive fit, but QDA includes multiplicative terms of the form \(c_{kjl}x_j x_l\)
\item
  None of these methods uniformly dominates the others: the choice of method depends on the true distribution of the predictors in each of the K classes, n and p (i.e., related to the bias-variance tradeoff).
\end{itemize}

Compare to the non-parametric method (KNN)

\begin{itemize}
\item
  KNN would outperform both LDA and logistic regression when the decision boundary is highly nonlinear, but can't say which predictors are most important, and requires many observations
\item
  KNN is also limited in high-dimensions due to the curse of dimensionality
\item
  Since QDA is a special type of nonlinear decision boundary (quadratic), it can be considered as a compromise between the linear methods and KNN classification. QDA can have fewer training observations than KNN but not as flexible.
\end{itemize}

From simulation:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.6111}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.3889}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
True decision boundary
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Best performance
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Linear & LDA + Logistic regression \\
Moderately nonlinear & QDA + Naive Bayes \\
Highly nonlinear (many training, p is not large) & KNN \\
\end{longtable}

\begin{itemize}
\tightlist
\item
  like linear regression, we can also introduce flexibility by including transformed features \(\sqrt{X}, X^2, X^3\)
\end{itemize}

\hypertarget{probabilities-of-misclassification}{%
\subsection{Probabilities of Misclassification}\label{probabilities-of-misclassification}}

When the distribution are exactly known, we can determine the misclassification probabilities exactly. however, when we need to estimate the population parameters, we have to estimate the probability of misclassification

\begin{itemize}
\item
  Naive method

  \begin{itemize}
  \item
    Plugging the parameters estimates into the form for the misclassification probabilities results to derive at the estimates of the misclassification probability.
  \item
    But this will tend to be optimistic when the number of samples in one or more populations is small.
  \end{itemize}
\item
  Resubstitution method

  \begin{itemize}
  \item
    Use the proportion of the samples from population i that would be allocated to another population as an estimate of the misclassification probability
  \item
    But also optimistic when the number of samples is small
  \end{itemize}
\item
  Jack-knife estimates:

  \begin{itemize}
  \item
    The above two methods use observation to estimate both parameters and also misclassification probabilities based upon the discriminant rule
  \item
    Alternatively, we determine the discriminant rule based upon all of the data except the k-th observation from the j-th population
  \item
    then, determine if the k-th observation would be misclassified under this rule
  \item
    perform this process for all \(n_j\) observation in population j . An estimate fo the misclassification probability would be the fraction of \(n_j\) observations which were misclassified
  \item
    repeat the process for other \(i \neq j\) populations
  \item
    This method is more reliable than the others, but also computationally intensive
  \end{itemize}
\item
  Cross-Validation
\end{itemize}

\textbf{Summary}

Consider the group-specific densities \(f_j (\mathbf{x})\) for multivariate vector \(\mathbf{x}\).

Assume equal misclassifications costs, the Bayes classification probability of \(\mathbf{x}\) belonging to the j-th population is

\[
p(j |\mathbf{x}) = \frac{\pi_j f_j (\mathbf{x})}{\sum_{k=1}^h \pi_k f_k (\mathbf{x})}
\]

\(j = 1,\dots, h\)

where there are \(h\) possible groups.

We then classify into the group for which this probability of membership is largest

Alternatively, we can write this in terms of a \textbf{generalized squared distance} formation

\[
D_j^2 (\mathbf{x}) = d_j^2 (\mathbf{x})+ g_1(j) + g_2 (j)
\]

where

\begin{itemize}
\item
  \(d_j^2(\mathbf{x}) = (\mathbf{x} - \mathbf{\mu}_j)' \mathbf{V}_j^{-1} (\mathbf{x} - \mathbf{\mu}_j)\) is the squared Mahalanobis distance from \(\mathbf{x}\) to the centroid of group j, and

  \begin{itemize}
  \item
    \(\mathbf{V}_j = \mathbf{S}_j\) if the within group covariance matrices are not equal
  \item
    \(\mathbf{V}_j = \mathbf{S}_p\) if a pooled covariance estimate is appropriate
  \end{itemize}
\end{itemize}

and

\[
g_1(j) =
\begin{cases}
\ln |\mathbf{S}_j| & \text{within group covariances are not equal} \\
0 & \text{pooled covariance}
\end{cases}
\]

\[
g_2(j) = 
\begin{cases}
-2 \ln \pi_j & \text{prior probabilities are not equal} \\
0 & \text{prior probabilities are equal}
\end{cases}
\]

then, the posterior probability of belonging to group j is

\[
p(j| \mathbf{x})  = \frac{\exp(-.5 D_j^2(\mathbf{x}))}{\sum_{k=1}^h \exp(-.5 D^2_k (\mathbf{x}))}
\]

where \(j = 1,\dots , h\)

and \(\mathbf{x}\) is classified into group j if \(p(j | \mathbf{x})\) is largest for \(j = 1,\dots,h\) (or, \(D_j^2(\mathbf{x})\) is smallest).

\hypertarget{assessing-classification-performance}{%
\subsubsection{Assessing Classification Performance}\label{assessing-classification-performance}}

For binary classification, confusion matrix

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1765}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2353}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2353}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1029}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Predicted class
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
& & - or Null & + or Null & Total \\
True Class & - or Null & True Neg (TN) & False Pos (FP) & N \\
& + or Null & False Neg (FN) & True Pos (TP) & P \\
& Total & N* & P* & \\
\end{longtable}

and table 4.6 from \citep{james2013}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2535}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2535}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4930}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Synonyms
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
False Pos rate & FP/N & Type I error, 1 0 Specificity \\
True Pos. rate & TP/P & 1 - Type II error, power, sensitivity, recall \\
Pos Pred. value & TP/P* & Precision, 1 - false discovery promotion \\
Neg. Pred. value & TN/N* & \\
\end{longtable}

ROC curve (receiver Operating Characteristics) is a graphical comparison between \textbf{sensitivity} (true positive) and \textbf{specificity} ( = 1 - false positive)

y-axis = true positive rate

x-axis = false positive rate

as we change the threshold rate for classifying an observation as from 0 to 1

AUC (area under the ROC) ideally would equal to 1, a bad classifier would have AUC = 0.5 (pure chance)

\hypertarget{unknown-populations-nonparametric-discrimination}{%
\subsection{Unknown Populations/ Nonparametric Discrimination}\label{unknown-populations-nonparametric-discrimination}}

When your multivariate data are not Gaussian, or known distributional form at all, we can use the following methods

\hypertarget{kernel-methods}{%
\subsubsection{Kernel Methods}\label{kernel-methods}}

We approximate \(f_j (\mathbf{x})\) by a kernel density estimate

\[
\hat{f}_j(\mathbf{x}) = \frac{1}{n_j} \sum_{i = 1}^{n_j} K_j (\mathbf{x} - \mathbf{x}_i)
\]

where

\begin{itemize}
\item
  \(K_j (.)\) is a kernel function satisfying \(\int K_j(\mathbf{z})d\mathbf{z} =1\)
\item
  \(\mathbf{x}_i\) , \(i = 1,\dots , n_j\) is a random sample from the j-th population.
\end{itemize}

Thus, after finding \(\hat{f}_j (\mathbf{x})\) for each of the \(h\) populations, the posterior probability of group membership is

\[
p(j |\mathbf{x}) = \frac{\pi_j \hat{f}_j (\mathbf{x})}{\sum_{k-1}^h \pi_k \hat{f}_k (\mathbf{x})}
\]

where \(j = 1,\dots, h\)

There are different choices for the kernel function:

\begin{itemize}
\item
  Uniform
\item
  Normal
\item
  Epanechnikov
\item
  Biweight
\item
  Triweight
\end{itemize}

We these kernels, we have to pick the ``radius'' (or variance, width, window width, bandwidth) of the kernel, which is a smoothing parameter (the larger the radius, the more smooth the kernel estimate of the density).

To select the smoothness parameter, we can use the following method

If we believe the populations were close to multivariate normal, then

\[
R = (\frac{4/(2p+1)}{n_j})^{1/(p+1}
\]

But since we do not know for sure, we might choose several different values and select one that vies the best out of sample or cross-validation discrimination.

Moreover, you also have to decide whether to use different kernel smoothness for different populations, which is similar to the individual and pooled covariances in the classical methodology.

\hypertarget{nearest-neighbor-methods}{%
\subsubsection{Nearest Neighbor Methods}\label{nearest-neighbor-methods}}

The nearest neighbor (also known as k-nearest neighbor) method performs the classification of a new observation vector based on the group membership of its nearest neighbors. In practice, we find

\[
d_{ij}^2 (\mathbf{x}, \mathbf{x}_i) = (\mathbf{x}, \mathbf{x}_i) V_j^{-1}(\mathbf{x}, \mathbf{x}_i)
\]

which is the distance between the vector \(\mathbf{x}\) and the \(i\)-th observation in group \(j\)

We consider different choices for \(\mathbf{V}_j\)

For example,

\[
\begin{aligned}
\mathbf{V}_j &= \mathbf{S}_p \\
\mathbf{V}_j &= \mathbf{S}_j \\
\mathbf{V}_j &= \mathbf{I} \\
\mathbf{V}_j &= diag (\mathbf{S}_p)
\end{aligned}
\]

We find the \(k\) observations that are closest to \(\mathbf{x}\) (where users pick \(k\)). Then we classify into the most common population, weighted by the prior.

\hypertarget{modern-discriminant-methods}{%
\subsubsection{Modern Discriminant Methods}\label{modern-discriminant-methods}}

\textbf{Note}:

Logistic regression (with or without random effects) is a flexible model-based procedure for classification between two populations.

The extension of logistic regression to the multi-group setting is polychotomous logistic regression (or, mulinomial regression).

The machine learning and pattern recognition are growing with strong focus on nonlinear discriminant analysis methods such as:

\begin{itemize}
\item
  radial basis function networks
\item
  support vector machines
\item
  multiplayer perceptrons (neural networks)
\end{itemize}

The general framework

\[
g_j (\mathbf{x}) = \sum_{l = 1}^m w_{jl}\phi_l (\mathbf{x}; \mathbf{\theta}_l) + w_{j0}
\]

where

\begin{itemize}
\item
  \(j = 1,\dots, h\)
\item
  \(m\) nonlinear basis functions \(\phi_l\), each of which has \(n_m\) parameters given by \(\theta_l = \{ \theta_{lk}: k = 1, \dots , n_m \}\)
\end{itemize}

We assign \(\mathbf{x}\) to the \(j\)-th population if \(g_j(\mathbf{x})\) is the maximum for all \(j = 1,\dots, h\)

Development usually focuses on the choice and estimation of the basis functions, \(\phi_l\) and the estimation of the weights \(w_{jl}\)

More details can be found \citep{webb2011statistical}

\hypertarget{application-12}{%
\subsection{Application}\label{application-12}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(class)}
\FunctionTok{library}\NormalTok{(klaR)}
\FunctionTok{library}\NormalTok{(MASS)}
\FunctionTok{library}\NormalTok{(tidyverse)}

\DocumentationTok{\#\# Read in the data}
\NormalTok{crops }\OtherTok{\textless{}{-}} \FunctionTok{read.table}\NormalTok{(}\StringTok{"images/crops.txt"}\NormalTok{)}
\FunctionTok{names}\NormalTok{(crops) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"crop"}\NormalTok{, }\StringTok{"y1"}\NormalTok{, }\StringTok{"y2"}\NormalTok{, }\StringTok{"y3"}\NormalTok{, }\StringTok{"y4"}\NormalTok{)}
\FunctionTok{str}\NormalTok{(crops)}
\CommentTok{\#\textgreater{} \textquotesingle{}data.frame\textquotesingle{}:    36 obs. of  5 variables:}
\CommentTok{\#\textgreater{}  $ crop: chr  "Corn" "Corn" "Corn" "Corn" ...}
\CommentTok{\#\textgreater{}  $ y1  : int  16 15 16 18 15 15 12 20 24 21 ...}
\CommentTok{\#\textgreater{}  $ y2  : int  27 23 27 20 15 32 15 23 24 25 ...}
\CommentTok{\#\textgreater{}  $ y3  : int  31 30 27 25 31 32 16 23 25 23 ...}
\CommentTok{\#\textgreater{}  $ y4  : int  33 30 26 23 32 15 73 25 32 24 ...}


\DocumentationTok{\#\# Read in test data}
\NormalTok{crops\_test }\OtherTok{\textless{}{-}} \FunctionTok{read.table}\NormalTok{(}\StringTok{"images/crops\_test.txt"}\NormalTok{)}
\FunctionTok{names}\NormalTok{(crops\_test) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"crop"}\NormalTok{, }\StringTok{"y1"}\NormalTok{, }\StringTok{"y2"}\NormalTok{, }\StringTok{"y3"}\NormalTok{, }\StringTok{"y4"}\NormalTok{)}
\FunctionTok{str}\NormalTok{(crops\_test)}
\CommentTok{\#\textgreater{} \textquotesingle{}data.frame\textquotesingle{}:    5 obs. of  5 variables:}
\CommentTok{\#\textgreater{}  $ crop: chr  "Corn" "Soybeans" "Cotton" "Sugarbeets" ...}
\CommentTok{\#\textgreater{}  $ y1  : int  16 21 29 54 32}
\CommentTok{\#\textgreater{}  $ y2  : int  27 25 24 23 32}
\CommentTok{\#\textgreater{}  $ y3  : int  31 23 26 21 62}
\CommentTok{\#\textgreater{}  $ y4  : int  33 24 28 54 16}
\end{Highlighting}
\end{Shaded}

\hypertarget{lda-1}{%
\subsubsection{LDA}\label{lda-1}}

Default prior is proportional to sample size and \texttt{lda} and \texttt{qda} do not fit a constant or intercept term

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# Linear discriminant analysis}
\NormalTok{lda\_mod }\OtherTok{\textless{}{-}} \FunctionTok{lda}\NormalTok{(crop }\SpecialCharTok{\textasciitilde{}}\NormalTok{ y1 }\SpecialCharTok{+}\NormalTok{ y2 }\SpecialCharTok{+}\NormalTok{ y3 }\SpecialCharTok{+}\NormalTok{ y4,}
               \AttributeTok{data =}\NormalTok{ crops)}
\NormalTok{lda\_mod}
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lda(crop \textasciitilde{} y1 + y2 + y3 + y4, data = crops)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Prior probabilities of groups:}
\CommentTok{\#\textgreater{}     Clover       Corn     Cotton   Soybeans Sugarbeets }
\CommentTok{\#\textgreater{}  0.3055556  0.1944444  0.1666667  0.1666667  0.1666667 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Group means:}
\CommentTok{\#\textgreater{}                  y1       y2       y3       y4}
\CommentTok{\#\textgreater{} Clover     46.36364 32.63636 34.18182 36.63636}
\CommentTok{\#\textgreater{} Corn       15.28571 22.71429 27.42857 33.14286}
\CommentTok{\#\textgreater{} Cotton     34.50000 32.66667 35.00000 39.16667}
\CommentTok{\#\textgreater{} Soybeans   21.00000 27.00000 23.50000 29.66667}
\CommentTok{\#\textgreater{} Sugarbeets 31.00000 32.16667 20.00000 40.50000}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients of linear discriminants:}
\CommentTok{\#\textgreater{}              LD1          LD2         LD3          LD4}
\CommentTok{\#\textgreater{} y1 {-}6.147360e{-}02  0.009215431 {-}0.02987075 {-}0.014680566}
\CommentTok{\#\textgreater{} y2 {-}2.548964e{-}02  0.042838972  0.04631489  0.054842132}
\CommentTok{\#\textgreater{} y3  1.642126e{-}02 {-}0.079471595  0.01971222  0.008938745}
\CommentTok{\#\textgreater{} y4  5.143616e{-}05 {-}0.013917423  0.05381787 {-}0.025717667}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Proportion of trace:}
\CommentTok{\#\textgreater{}    LD1    LD2    LD3    LD4 }
\CommentTok{\#\textgreater{} 0.7364 0.1985 0.0576 0.0075}

\DocumentationTok{\#\# Look at accuracy on the training data}
\NormalTok{lda\_fitted }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(lda\_mod,}\AttributeTok{newdata =}\NormalTok{ crops)}
\CommentTok{\# Contingency table}
\NormalTok{lda\_table }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ crops}\SpecialCharTok{$}\NormalTok{crop, }\AttributeTok{fitted =}\NormalTok{ lda\_fitted}\SpecialCharTok{$}\NormalTok{class)}
\NormalTok{lda\_table}
\CommentTok{\#\textgreater{}             fitted}
\CommentTok{\#\textgreater{} truth        Clover Corn Cotton Soybeans Sugarbeets}
\CommentTok{\#\textgreater{}   Clover          6    0      3        0          2}
\CommentTok{\#\textgreater{}   Corn            0    6      0        1          0}
\CommentTok{\#\textgreater{}   Cotton          3    0      1        2          0}
\CommentTok{\#\textgreater{}   Soybeans        0    1      1        3          1}
\CommentTok{\#\textgreater{}   Sugarbeets      1    1      0        2          2}
\CommentTok{\# accuracy of 0.5 is just random (not good)}

\DocumentationTok{\#\# Posterior probabilities of membership}
\NormalTok{crops\_post }\OtherTok{\textless{}{-}} \FunctionTok{cbind.data.frame}\NormalTok{(crops,}
                               \AttributeTok{crop\_pred =}\NormalTok{ lda\_fitted}\SpecialCharTok{$}\NormalTok{class,}
\NormalTok{                               lda\_fitted}\SpecialCharTok{$}\NormalTok{posterior)}
\NormalTok{crops\_post }\OtherTok{\textless{}{-}}\NormalTok{ crops\_post }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{missed =}\NormalTok{ crop }\SpecialCharTok{!=}\NormalTok{ crop\_pred)}
\FunctionTok{head}\NormalTok{(crops\_post)}
\CommentTok{\#\textgreater{}   crop y1 y2 y3 y4 crop\_pred     Clover      Corn    Cotton  Soybeans}
\CommentTok{\#\textgreater{} 1 Corn 16 27 31 33      Corn 0.08935164 0.4054296 0.1763189 0.2391845}
\CommentTok{\#\textgreater{} 2 Corn 15 23 30 30      Corn 0.07690181 0.4558027 0.1420920 0.2530101}
\CommentTok{\#\textgreater{} 3 Corn 16 27 27 26      Corn 0.09817815 0.3422454 0.1365315 0.3073105}
\CommentTok{\#\textgreater{} 4 Corn 18 20 25 23      Corn 0.10521511 0.3633673 0.1078076 0.3281477}
\CommentTok{\#\textgreater{} 5 Corn 15 15 31 32      Corn 0.05879921 0.5753907 0.1173332 0.2086696}
\CommentTok{\#\textgreater{} 6 Corn 15 32 32 15  Soybeans 0.09723648 0.3278382 0.1318370 0.3419924}
\CommentTok{\#\textgreater{}   Sugarbeets missed}
\CommentTok{\#\textgreater{} 1 0.08971545  FALSE}
\CommentTok{\#\textgreater{} 2 0.07219340  FALSE}
\CommentTok{\#\textgreater{} 3 0.11573442  FALSE}
\CommentTok{\#\textgreater{} 4 0.09546233  FALSE}
\CommentTok{\#\textgreater{} 5 0.03980738  FALSE}
\CommentTok{\#\textgreater{} 6 0.10109590   TRUE}
\CommentTok{\# posterior shows that posterior of corn membership is much higher than the prior}

\DocumentationTok{\#\# LOOCV}
\CommentTok{\# leave{-}one{-}out cross validation for linear discriminant analysis}
\CommentTok{\# cannot run the predict function using the object with CV = TRUE }
\CommentTok{\# because it returns the within sample predictions}
\NormalTok{lda\_cv }\OtherTok{\textless{}{-}} \FunctionTok{lda}\NormalTok{(crop }\SpecialCharTok{\textasciitilde{}}\NormalTok{ y1 }\SpecialCharTok{+}\NormalTok{ y2 }\SpecialCharTok{+}\NormalTok{ y3 }\SpecialCharTok{+}\NormalTok{ y4,}
              \AttributeTok{data =}\NormalTok{ crops, }\AttributeTok{CV =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\# Contingency table}
\NormalTok{lda\_table\_cv }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ crops}\SpecialCharTok{$}\NormalTok{crop, }\AttributeTok{fitted =}\NormalTok{ lda\_cv}\SpecialCharTok{$}\NormalTok{class)}
\NormalTok{lda\_table\_cv}
\CommentTok{\#\textgreater{}             fitted}
\CommentTok{\#\textgreater{} truth        Clover Corn Cotton Soybeans Sugarbeets}
\CommentTok{\#\textgreater{}   Clover          4    3      1        0          3}
\CommentTok{\#\textgreater{}   Corn            0    4      1        2          0}
\CommentTok{\#\textgreater{}   Cotton          3    0      0        2          1}
\CommentTok{\#\textgreater{}   Soybeans        0    1      1        3          1}
\CommentTok{\#\textgreater{}   Sugarbeets      2    1      0        2          1}

\DocumentationTok{\#\# Predict the test data}
\NormalTok{lda\_pred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(lda\_mod, }\AttributeTok{newdata =}\NormalTok{ crops\_test)}

\DocumentationTok{\#\# Make a contingency table with truth and most likely class}
\FunctionTok{table}\NormalTok{(}\AttributeTok{truth=}\NormalTok{crops\_test}\SpecialCharTok{$}\NormalTok{crop, }\AttributeTok{predict=}\NormalTok{lda\_pred}\SpecialCharTok{$}\NormalTok{class)}
\CommentTok{\#\textgreater{}             predict}
\CommentTok{\#\textgreater{} truth        Clover Corn Cotton Soybeans Sugarbeets}
\CommentTok{\#\textgreater{}   Clover          0    0      1        0          0}
\CommentTok{\#\textgreater{}   Corn            0    1      0        0          0}
\CommentTok{\#\textgreater{}   Cotton          0    0      0        1          0}
\CommentTok{\#\textgreater{}   Soybeans        0    0      0        1          0}
\CommentTok{\#\textgreater{}   Sugarbeets      1    0      0        0          0}
\end{Highlighting}
\end{Shaded}

LDA didn't do well on both within sample and out-of-sample data.

\hypertarget{qda-1}{%
\subsubsection{QDA}\label{qda-1}}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# Quadratic discriminant analysis}
\NormalTok{qda\_mod }\OtherTok{\textless{}{-}} \FunctionTok{qda}\NormalTok{(crop }\SpecialCharTok{\textasciitilde{}}\NormalTok{ y1 }\SpecialCharTok{+}\NormalTok{ y2 }\SpecialCharTok{+}\NormalTok{ y3 }\SpecialCharTok{+}\NormalTok{ y4,}
               \AttributeTok{data =}\NormalTok{ crops)}

\DocumentationTok{\#\# Look at accuracy on the training data}
\NormalTok{qda\_fitted }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(qda\_mod, }\AttributeTok{newdata =}\NormalTok{ crops)}
\CommentTok{\# Contingency table}
\NormalTok{qda\_table }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ crops}\SpecialCharTok{$}\NormalTok{crop, }\AttributeTok{fitted =}\NormalTok{ qda\_fitted}\SpecialCharTok{$}\NormalTok{class)}
\NormalTok{qda\_table}
\CommentTok{\#\textgreater{}             fitted}
\CommentTok{\#\textgreater{} truth        Clover Corn Cotton Soybeans Sugarbeets}
\CommentTok{\#\textgreater{}   Clover          9    0      0        0          2}
\CommentTok{\#\textgreater{}   Corn            0    7      0        0          0}
\CommentTok{\#\textgreater{}   Cotton          0    0      6        0          0}
\CommentTok{\#\textgreater{}   Soybeans        0    0      0        6          0}
\CommentTok{\#\textgreater{}   Sugarbeets      0    0      1        1          4}

\DocumentationTok{\#\# LOOCV}
\NormalTok{qda\_cv }\OtherTok{\textless{}{-}} \FunctionTok{qda}\NormalTok{(crop }\SpecialCharTok{\textasciitilde{}}\NormalTok{ y1 }\SpecialCharTok{+}\NormalTok{ y2 }\SpecialCharTok{+}\NormalTok{ y3 }\SpecialCharTok{+}\NormalTok{ y4,}
              \AttributeTok{data =}\NormalTok{ crops, }\AttributeTok{CV =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\# Contingency table}
\NormalTok{qda\_table\_cv }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ crops}\SpecialCharTok{$}\NormalTok{crop, }\AttributeTok{fitted =}\NormalTok{ qda\_cv}\SpecialCharTok{$}\NormalTok{class)}
\NormalTok{qda\_table\_cv}
\CommentTok{\#\textgreater{}             fitted}
\CommentTok{\#\textgreater{} truth        Clover Corn Cotton Soybeans Sugarbeets}
\CommentTok{\#\textgreater{}   Clover          9    0      0        0          2}
\CommentTok{\#\textgreater{}   Corn            3    2      0        0          2}
\CommentTok{\#\textgreater{}   Cotton          3    0      2        0          1}
\CommentTok{\#\textgreater{}   Soybeans        3    0      0        2          1}
\CommentTok{\#\textgreater{}   Sugarbeets      3    0      1        1          1}

\DocumentationTok{\#\# Predict the test data}
\NormalTok{qda\_pred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(qda\_mod, }\AttributeTok{newdata =}\NormalTok{ crops\_test)}
\DocumentationTok{\#\# Make a contingency table with truth and most likely class}
\FunctionTok{table}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ crops\_test}\SpecialCharTok{$}\NormalTok{crop, }\AttributeTok{predict =}\NormalTok{ qda\_pred}\SpecialCharTok{$}\NormalTok{class)}
\CommentTok{\#\textgreater{}             predict}
\CommentTok{\#\textgreater{} truth        Clover Corn Cotton Soybeans Sugarbeets}
\CommentTok{\#\textgreater{}   Clover          1    0      0        0          0}
\CommentTok{\#\textgreater{}   Corn            0    1      0        0          0}
\CommentTok{\#\textgreater{}   Cotton          0    0      1        0          0}
\CommentTok{\#\textgreater{}   Soybeans        0    0      0        1          0}
\CommentTok{\#\textgreater{}   Sugarbeets      0    0      0        0          1}
\end{Highlighting}
\end{Shaded}

\hypertarget{knn-1}{%
\subsubsection{KNN}\label{knn-1}}

\texttt{knn} uses design matrices of the features.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# Design matrices}
\NormalTok{X\_train }\OtherTok{\textless{}{-}}\NormalTok{ crops }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{crop)}
\NormalTok{X\_test }\OtherTok{\textless{}{-}}\NormalTok{ crops\_test }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{crop)}
\NormalTok{Y\_train }\OtherTok{\textless{}{-}}\NormalTok{ crops}\SpecialCharTok{$}\NormalTok{crop}
\NormalTok{Y\_test }\OtherTok{\textless{}{-}}\NormalTok{ crops\_test}\SpecialCharTok{$}\NormalTok{crop}

\DocumentationTok{\#\# Nearest neighbors with 2 neighbors}
\NormalTok{knn\_2 }\OtherTok{\textless{}{-}} \FunctionTok{knn}\NormalTok{(X\_train, X\_train, Y\_train, }\AttributeTok{k =} \DecValTok{2}\NormalTok{)}
\FunctionTok{table}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ Y\_train, }\AttributeTok{fitted =}\NormalTok{ knn\_2)}
\CommentTok{\#\textgreater{}             fitted}
\CommentTok{\#\textgreater{} truth        Clover Corn Cotton Soybeans Sugarbeets}
\CommentTok{\#\textgreater{}   Clover          8    0      2        1          0}
\CommentTok{\#\textgreater{}   Corn            0    6      0        1          0}
\CommentTok{\#\textgreater{}   Cotton          0    0      5        0          1}
\CommentTok{\#\textgreater{}   Soybeans        0    0      0        3          3}
\CommentTok{\#\textgreater{}   Sugarbeets      0    0      1        1          4}

\DocumentationTok{\#\# Accuracy}
\FunctionTok{mean}\NormalTok{(Y\_train}\SpecialCharTok{==}\NormalTok{knn\_2)}
\CommentTok{\#\textgreater{} [1] 0.7222222}

\DocumentationTok{\#\# Performance on test data}
\NormalTok{knn\_2\_test }\OtherTok{\textless{}{-}} \FunctionTok{knn}\NormalTok{(X\_train, X\_test, Y\_train, }\AttributeTok{k =} \DecValTok{2}\NormalTok{)}
\FunctionTok{table}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ Y\_test, }\AttributeTok{predict =}\NormalTok{ knn\_2\_test)}
\CommentTok{\#\textgreater{}             predict}
\CommentTok{\#\textgreater{} truth        Clover Corn Cotton Soybeans Sugarbeets}
\CommentTok{\#\textgreater{}   Clover          1    0      0        0          0}
\CommentTok{\#\textgreater{}   Corn            0    1      0        0          0}
\CommentTok{\#\textgreater{}   Cotton          0    0      0        0          1}
\CommentTok{\#\textgreater{}   Soybeans        0    0      0        1          0}
\CommentTok{\#\textgreater{}   Sugarbeets      0    0      0        0          1}

\DocumentationTok{\#\# Accuracy}
\FunctionTok{mean}\NormalTok{(Y\_test}\SpecialCharTok{==}\NormalTok{knn\_2\_test)}
\CommentTok{\#\textgreater{} [1] 0.8}

\DocumentationTok{\#\# Nearest neighbors with 3 neighbors}
\NormalTok{knn\_3 }\OtherTok{\textless{}{-}} \FunctionTok{knn}\NormalTok{(X\_train, X\_train, Y\_train, }\AttributeTok{k =} \DecValTok{3}\NormalTok{)}
\FunctionTok{table}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ Y\_train, }\AttributeTok{fitted =}\NormalTok{ knn\_3)}
\CommentTok{\#\textgreater{}             fitted}
\CommentTok{\#\textgreater{} truth        Clover Corn Cotton Soybeans Sugarbeets}
\CommentTok{\#\textgreater{}   Clover          8    0      2        1          0}
\CommentTok{\#\textgreater{}   Corn            0    6      0        1          0}
\CommentTok{\#\textgreater{}   Cotton          1    1      4        0          0}
\CommentTok{\#\textgreater{}   Soybeans        1    0      0        4          1}
\CommentTok{\#\textgreater{}   Sugarbeets      0    0      0        2          4}

\DocumentationTok{\#\# Accuracy}
\FunctionTok{mean}\NormalTok{(Y\_train}\SpecialCharTok{==}\NormalTok{knn\_3)}
\CommentTok{\#\textgreater{} [1] 0.7222222}

\DocumentationTok{\#\# Performance on test data}
\NormalTok{knn\_3\_test }\OtherTok{\textless{}{-}} \FunctionTok{knn}\NormalTok{(X\_train, X\_test, Y\_train, }\AttributeTok{k =} \DecValTok{3}\NormalTok{)}
\FunctionTok{table}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ Y\_test, }\AttributeTok{predict =}\NormalTok{ knn\_3\_test)}
\CommentTok{\#\textgreater{}             predict}
\CommentTok{\#\textgreater{} truth        Clover Corn Cotton Soybeans Sugarbeets}
\CommentTok{\#\textgreater{}   Clover          1    0      0        0          0}
\CommentTok{\#\textgreater{}   Corn            0    1      0        0          0}
\CommentTok{\#\textgreater{}   Cotton          0    0      1        0          0}
\CommentTok{\#\textgreater{}   Soybeans        0    0      0        1          0}
\CommentTok{\#\textgreater{}   Sugarbeets      0    0      0        0          1}

\DocumentationTok{\#\# Accuracy}
\FunctionTok{mean}\NormalTok{(Y\_test}\SpecialCharTok{==}\NormalTok{knn\_3\_test)}
\CommentTok{\#\textgreater{} [1] 1}
\end{Highlighting}
\end{Shaded}

\hypertarget{stepwise}{%
\subsubsection{Stepwise}\label{stepwise}}

Stepwise discriminant analysis using the \texttt{stepclass} in function in the \texttt{klaR} package.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{step }\OtherTok{\textless{}{-}} \FunctionTok{stepclass}\NormalTok{(}
\NormalTok{    crop }\SpecialCharTok{\textasciitilde{}}\NormalTok{ y1 }\SpecialCharTok{+}\NormalTok{ y2 }\SpecialCharTok{+}\NormalTok{ y3 }\SpecialCharTok{+}\NormalTok{ y4,}
    \AttributeTok{data =}\NormalTok{ crops,}
    \AttributeTok{method =} \StringTok{"qda"}\NormalTok{,}
    \AttributeTok{improvement =} \FloatTok{0.15}
\NormalTok{)}
\CommentTok{\#\textgreater{} correctness rate: 0.45;  in: "y1";  variables (1): y1 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  hr.elapsed min.elapsed sec.elapsed }
\CommentTok{\#\textgreater{}        0.00        0.00        0.16}

\NormalTok{step}\SpecialCharTok{$}\NormalTok{process}
\CommentTok{\#\textgreater{}    step var varname result.pm}
\CommentTok{\#\textgreater{} 0 start   0      {-}{-}      0.00}
\CommentTok{\#\textgreater{} 1    in   1      y1      0.45}

\NormalTok{step}\SpecialCharTok{$}\NormalTok{performance.measure}
\CommentTok{\#\textgreater{} [1] "correctness rate"}
\end{Highlighting}
\end{Shaded}

Iris Data

\begin{Shaded}
\begin{Highlighting}[]

\FunctionTok{library}\NormalTok{(dplyr)}
\FunctionTok{data}\NormalTok{(}\StringTok{\textquotesingle{}iris\textquotesingle{}}\NormalTok{)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{samp }\OtherTok{\textless{}{-}}
    \FunctionTok{sample.int}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(iris), }\AttributeTok{size =} \FunctionTok{floor}\NormalTok{(}\FloatTok{0.70} \SpecialCharTok{*} \FunctionTok{nrow}\NormalTok{(iris)), }\AttributeTok{replace =}\NormalTok{ F)}

\NormalTok{train.iris }\OtherTok{\textless{}{-}}\NormalTok{ iris[samp,] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate\_if}\NormalTok{(is.numeric,scale)}
\NormalTok{test.iris }\OtherTok{\textless{}{-}}\NormalTok{ iris[}\SpecialCharTok{{-}}\NormalTok{samp,] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate\_if}\NormalTok{(is.numeric,scale)}

\FunctionTok{library}\NormalTok{(ggplot2)}
\NormalTok{iris.model }\OtherTok{\textless{}{-}} \FunctionTok{lda}\NormalTok{(Species }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ train.iris)}
\CommentTok{\#pred}
\NormalTok{pred.lda }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(iris.model, test.iris)}
\FunctionTok{table}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ test.iris}\SpecialCharTok{$}\NormalTok{Species, }\AttributeTok{prediction =}\NormalTok{ pred.lda}\SpecialCharTok{$}\NormalTok{class)}
\CommentTok{\#\textgreater{}             prediction}
\CommentTok{\#\textgreater{} truth        setosa versicolor virginica}
\CommentTok{\#\textgreater{}   setosa         15          0         0}
\CommentTok{\#\textgreater{}   versicolor      0         17         0}
\CommentTok{\#\textgreater{}   virginica       0          0        13}

\FunctionTok{plot}\NormalTok{(iris.model)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{22-multivariate_files/figure-latex/unnamed-chunk-24-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{iris.model.qda }\OtherTok{\textless{}{-}} \FunctionTok{qda}\NormalTok{(Species}\SpecialCharTok{\textasciitilde{}}\NormalTok{.,}\AttributeTok{data=}\NormalTok{train.iris)}
\CommentTok{\#pred}
\NormalTok{pred.qda }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(iris.model.qda,test.iris)}
\FunctionTok{table}\NormalTok{(}\AttributeTok{truth=}\NormalTok{test.iris}\SpecialCharTok{$}\NormalTok{Species,}\AttributeTok{prediction=}\NormalTok{pred.qda}\SpecialCharTok{$}\NormalTok{class)}
\CommentTok{\#\textgreater{}             prediction}
\CommentTok{\#\textgreater{} truth        setosa versicolor virginica}
\CommentTok{\#\textgreater{}   setosa         15          0         0}
\CommentTok{\#\textgreater{}   versicolor      0         16         1}
\CommentTok{\#\textgreater{}   virginica       0          0        13}
\end{Highlighting}
\end{Shaded}

\hypertarget{pca-with-discriminant-analysis}{%
\subsubsection{PCA with Discriminant Analysis}\label{pca-with-discriminant-analysis}}

we can use both PCA for dimension reduction in discriminant analysis

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{zeros }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{read.table}\NormalTok{(}\StringTok{"images/mnist0\_train\_b.txt"}\NormalTok{))}
\NormalTok{nines }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{read.table}\NormalTok{(}\StringTok{"images/mnist9\_train\_b.txt"}\NormalTok{))}
\NormalTok{train }\OtherTok{\textless{}{-}} \FunctionTok{rbind}\NormalTok{(zeros[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{1000}\NormalTok{, ], nines[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{1000}\NormalTok{, ])}
\NormalTok{train }\OtherTok{\textless{}{-}}\NormalTok{ train }\SpecialCharTok{/} \DecValTok{255} \CommentTok{\#divide by 255 per notes (so ranges from 0 to 1)}
\NormalTok{train }\OtherTok{\textless{}{-}} \FunctionTok{t}\NormalTok{(train) }\CommentTok{\#each column is an observation}
\FunctionTok{image}\NormalTok{(}\FunctionTok{matrix}\NormalTok{(train[, }\DecValTok{1}\NormalTok{], }\AttributeTok{nrow =} \DecValTok{28}\NormalTok{), }\AttributeTok{main =} \StringTok{\textquotesingle{}Example image, unrotated\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{22-multivariate_files/figure-latex/unnamed-chunk-25-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]


\NormalTok{test }\OtherTok{\textless{}{-}} \FunctionTok{rbind}\NormalTok{(zeros[}\DecValTok{2501}\SpecialCharTok{:}\DecValTok{3000}\NormalTok{, ], nines[}\DecValTok{2501}\SpecialCharTok{:}\DecValTok{3000}\NormalTok{, ])}
\NormalTok{test }\OtherTok{\textless{}{-}}\NormalTok{ test }\SpecialCharTok{/} \DecValTok{255}
\NormalTok{test }\OtherTok{\textless{}{-}} \FunctionTok{t}\NormalTok{(test)}
\NormalTok{y.train }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1000}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\DecValTok{9}\NormalTok{, }\DecValTok{1000}\NormalTok{))}
\NormalTok{y.test }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{500}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\DecValTok{9}\NormalTok{, }\DecValTok{500}\NormalTok{))}


\FunctionTok{library}\NormalTok{(MASS)}
\NormalTok{pc }\OtherTok{\textless{}{-}} \FunctionTok{prcomp}\NormalTok{(}\FunctionTok{t}\NormalTok{(train))}
\NormalTok{train.large }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(y.train, pc}\SpecialCharTok{$}\NormalTok{x[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{]))}
\NormalTok{large }\OtherTok{\textless{}{-}} \FunctionTok{lda}\NormalTok{(y.train }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ train.large)}
\CommentTok{\#the test data set needs to be constucted w/ the same 10 princomps}
\NormalTok{test.large }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(y.test, }\FunctionTok{predict}\NormalTok{(pc, }\FunctionTok{t}\NormalTok{(test))[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{]))}
\NormalTok{pred.lda }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(large, test.large)}
\FunctionTok{table}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ test.large}\SpecialCharTok{$}\NormalTok{y.test, }\AttributeTok{prediction =}\NormalTok{ pred.lda}\SpecialCharTok{$}\NormalTok{class)}
\CommentTok{\#\textgreater{}      prediction}
\CommentTok{\#\textgreater{} truth   0   9}
\CommentTok{\#\textgreater{}     0 491   9}
\CommentTok{\#\textgreater{}     9   5 495}

\NormalTok{large.qda }\OtherTok{\textless{}{-}} \FunctionTok{qda}\NormalTok{(y.train}\SpecialCharTok{\textasciitilde{}}\NormalTok{.,}\AttributeTok{data=}\NormalTok{train.large)}
\CommentTok{\#prediction}
\NormalTok{pred.qda }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(large.qda,test.large)}
\FunctionTok{table}\NormalTok{(}\AttributeTok{truth=}\NormalTok{test.large}\SpecialCharTok{$}\NormalTok{y.test,}\AttributeTok{prediction=}\NormalTok{pred.qda}\SpecialCharTok{$}\NormalTok{class)}
\CommentTok{\#\textgreater{}      prediction}
\CommentTok{\#\textgreater{} truth   0   9}
\CommentTok{\#\textgreater{}     0 493   7}
\CommentTok{\#\textgreater{}     9   3 497}
\end{Highlighting}
\end{Shaded}

\hypertarget{part-b.-quasi-experimental-design}{%
\part*{B. QUASI-EXPERIMENTAL DESIGN}\label{part-b.-quasi-experimental-design}}
\addcontentsline{toc}{part}{B. QUASI-EXPERIMENTAL DESIGN}

\hypertarget{quasi-experimental}{%
\chapter{Quasi-experimental}\label{quasi-experimental}}

In most cases, it means that you have pre- and post-intervention data.

A great resource for causal inference is \href{https://mixtape.scunning.com/introduction.html}{Causal Inference Mixtape}, especially if you like to read about the history of causal inference as a field as well (codes for Stata, R, and Python).

Libraries in R:

\begin{itemize}
\item
  \href{https://cran.r-project.org/web/views/Econometrics.html}{Econometrics}
\item
  \href{https://cran.r-project.org/web/views/CausalInference.html}{Causal Inference}
\end{itemize}

\textbf{Identification strategy} for any quasi-experiment (No ways to prove or formal statistical test, but you can provide plausible argument and evidence)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Where the exogenous variation comes from (by argument and institutional knowledge)
\item
  Exclusion restriction: Evidence that the variation in the exogenous shock and the outcome is due to no other factors

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    The stable unit treatment value assumption (SUTVA) states that the treatment of unit \(i\) affect only the outcome of unit \(i\) (i.e., no spillover to the control groups)
  \end{enumerate}
\end{enumerate}

All quasi-experimental methods involve a tradeoff between power and support for the exogeneity assumption (i.e., discard variation in the data that is not exogenous).

Consequently, we don't usually look at \(R^2\) \citep{ebbes2011sense}. And it can even be misleading to use \(R^2\) as the basis for model comparison.

Clustering should be based on the design, not the expectations of correlation \citep{abadie2023should}. With a \textbf{small sample}, you should use the \textbf{wild bootstrap procedure} \citep{cameron2008bootstrap} to correct for the downward bias (see \citep{cai2022implementation}for additional assumptions).

Typical robustness check: recommended by \citep{goldfarb2022conducting}

\begin{itemize}
\item
  Different controls: show models with and without controls. Typically, we want to see the change in the estimate of interest. See \citep{altonji2005selection} for a formal assessment based on Rosenbaum bounds (i.e., changes in the estimate and threat of Omitted variables on the estimate). For specific applications in marketing, see \citep{manchanda2015social} \citep{shin2012fire}
\item
  Different functional forms
\item
  Different window of time (in longitudinal setting)
\item
  Different dependent variables (those that are related) or different measures of the dependent variables
\item
  Different control group size (matched vs.~un-matched samples)
\item
  Placebo tests: see each placebo test for each setting below.
\end{itemize}

Showing the mechanism:

\begin{itemize}
\item
  Mediation analysis
\item
  Moderation analysis

  \begin{itemize}
  \item
    Estimate the model separately (for different groups)
  \item
    Assess whether the three-way interaction between the source of variation (e.g., under DID, cross-sectional and time series) and group membership is significant.
  \end{itemize}
\end{itemize}

External Validity:

\begin{itemize}
\item
  Assess how representative your sample is
\item
  Explain the limitation of the design.
\item
  Use quasi-experimental results in conjunction with structural models: see \citep{anderson2015growth} {[}\citet{einav2010beyond}{]}\citep{chung2014bonuses}
\end{itemize}

Limitation

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What is your identifying assumptions or identification strategy
\item
  What are threats to the validity of your assumptions?
\item
  What you do to address it? And maybe how future research can do to address it.
\end{enumerate}

\hypertarget{regression-discontinuity}{%
\chapter{Regression Discontinuity}\label{regression-discontinuity}}

\begin{itemize}
\item
  A regression discontinuity occurs when there is a discrete change (jump) in treatment likelihood in the distribution of a continuous (or roughly continuous) variable (i.e., \textbf{running/forcing/assignment variable}).

  \begin{itemize}
  \tightlist
  \item
    Running variable can also be time, but the argument for time to be continuous is hard to argue because usually we do not see increment of time (e.g., quarterly or annual data). Unless we have minute or hour data, then we might be able to argue for it.
  \end{itemize}
\item
  Review paper \citep{imbens2008regression, lee2010regression}
\item
  Other readings:

  \begin{itemize}
  \item
    \url{https://ies.ed.gov/ncee/wwc/Docs/ReferenceResources/wwc_rd.pdf}
  \item
    \url{https://ies.ed.gov/ncee/wwc/Docs/ReferenceResources/wwc_rdd_standards_122315.pdf}
  \end{itemize}
\item
  \citep{thistlethwaite1960}: first paper to use RD in the context of merit awards on future academic outcomes.
\item
  RD is a localized experiment at the cutoff point

  \begin{itemize}
  \tightlist
  \item
    Hence, we always have to qualify (perfunctory) our statement in research articles that ``our research might not generalize to beyond the bandwidth.''
  \end{itemize}
\item
  In reality, RD and experimental (from random assignment) estimates are very similar (\citep{chaplin2018internal}; \href{https://www.mathematica.org/publications/replicating-experimental-impact-estimates-using-a-regression-discontinuity-approach}{Mathematica}). But still, it's hard to prove empirically for every context (there might be future study that finds a huge difference between local estimate - causal - and overall estimate - random assignment.
\item
  Threats: only valid near threshold: inference at threshold is valid on average. Interestingly, random experiment showed the validity already.
\item
  Tradeoff between efficiency and bias
\item
  Regression discontinuity is under the framework of \protect\hyperlink{instrumental-variable}{Instrumental Variable} argued by \citep{angrist1999using} and a special case of the \protect\hyperlink{matching-methods}{Matching Methods} (matching at one point) argued by \citep{heckman1999economics}.
\item
  The hard part is to find a setting that can apply, but once you find one, it's easy to apply
\item
  We can also have multiple cutoff lines. However, for each cutoff line, there can only be one breakup point
\item
  RD can have multiple coinciding effects (i.e., joint distribution or bundled treatment), then RD effect in this case would be the joint effect.
\item
  As the running variable becomes more discrete your framework should be \protect\hyperlink{interrupted-time-series}{Interrupted Time Series}, but for more granular levels you can use RD. When you have infinite data (or substantially large) the two frameworks are identical. RD is always better than \protect\hyperlink{interrupted-time-series}{Interrupted Time Series}
\item
  Multiple alternative model specifications that produce consistent results are more reliable (parametric - linear regression with polynomials terms, and non-parametric - local linear regression). This is according to \citep{lee2010regression}, one straightforward method to ease the linearity assumption is by incorporating polynomial functions of the forcing variable. The choice of polynomial terms can be determined based on the data.

  \begin{itemize}
  \tightlist
  \item
    . According to \citep{gelman2019high}, accounting for global high-order polynomials presents three issues: (1) imprecise estimates due to noise, (2) sensitivity to the polynomial's degree, and (3) inadequate coverage of confidence intervals. To address this, researchers should instead employ estimators that rely on local linear or quadratic polynomials or other smooth functions.
  \end{itemize}
\item
  RD should be viewed more as a description of a data generating process, rather than a method or approach (similar to a randomized experiment)
\item
  RD is close to

  \begin{itemize}
  \item
    other quasi-experimental methods in the sense that it's based on the discontinuity at a threshold
  \item
    randomized experiments in the sense that it's local randomization.
  \end{itemize}
\end{itemize}

There are several types of Regression Discontinuity:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Sharp RD: Change in treatment probability at the cutoff point is 1

  \begin{itemize}
  \tightlist
  \item
    Kink design: Instead of a discontinuity in the level of running variable, we have a discontinuity in the slope of the function (while the function/level can remain continuous) \citep{nielsen2010estimating}. See \citep{bockerman2018kink} for application, and \citep{card2015inference} for theory.
  \end{itemize}
\item
  Kink RD
\item
  Fuzzy RD: Change in treatment probability less than 1
\item
  Fuzzy Kink RD
\item
  RDiT: running variable is time.
\end{enumerate}

Others:

\begin{itemize}
\item
  Multiple cutoff
\item
  Multiple Scores
\item
  Geographic RD
\item
  Dynamic Treatments
\item
  Continuous Treatments
\end{itemize}

Consider

\[
D_i = 1_{X_i > c}
\]

\[
D_i = 
\begin{cases}
D_i = 1 \text{ if } X_i > C \\
D_i = 0 \text{ if } X_i < C
\end{cases}
\]

where

\begin{itemize}
\item
  \(D_i\) = treatment effect
\item
  \(X_i\) = score variable (continuous)
\item
  \(c\) = cutoff point
\end{itemize}

\textbf{Identification (Identifying assumption}s) of RD:

Average Treatment Effect at the cutoff (\protect\hyperlink{continuity-based}{Continuity-based})

\[
\begin{aligned}
\alpha_{SRDD} &= E[Y_{1i} - Y_{0i} | X_i = c] \\
&= E[Y_{1i}|X_i = c] - E[Y_{0i}|X_i = c]\\
&= \lim_{x \to c^+} E[Y_{1i}|X_i = c] - \lim_{x \to c^=} E[Y_{0i}|X_i = c]
\end{aligned}
\]

Average Treatment Effect in a neighborhood (\protect\hyperlink{local-randomization-based}{Local Randomization-based}):

\[
\begin{aligned}
\alpha_{LR} &= E[Y_{1i} - Y_{0i}|X_i \in W] \\
&= \frac{1}{N_1} \sum_{X_i \in W, T_i = 1}Y_i - \frac{1}{N_0}\sum_{X_i \in W, T_i =0} Y_i
\end{aligned}
\]

RDD estimates the local average treatment effect (LATE), at the cutoff point which is not at the individual or population levels.

Since researchers typically care more about the internal validity, than external validity, localness affects only external validity.

\textbf{Assumptions}:

\begin{itemize}
\item
  Independent assignment
\item
  Continuity of conditional regression functions

  \begin{itemize}
  \tightlist
  \item
    \(E[Y(0)|X=x]\) and \(E[Y(1)|X=x]\) are continuous in x.
  \end{itemize}
\item
  RD is valid if cutpoint is \textbf{exogenous (i.e., no endogenous selection)} and running variable is \textbf{not manipulable}
\item
  Only treatment(s) (e.g., could be joint distribution of multiple treatments) cause discontinuity or jump in the outcome variable
\item
  All other factors are \textbf{smooth} through the cutoff (i.e., threshold) value. (we can also test this assumption by seeing no discontinuity in other factors). If they ``jump'', they will bias your causal estimate
\end{itemize}

\textbf{Threats to RD}

\begin{itemize}
\item
  Variables (other than treatment) change discontinuously at the cutoff

  \begin{itemize}
  \tightlist
  \item
    We can test for jumps in these variables (including pre-treatment outcome)
  \end{itemize}
\item
  Multiple discontinuities for the assignment variable
\item
  Manipulation of the assignment variable

  \begin{itemize}
  \tightlist
  \item
    At the cutoff point, check for continuity in the density of the assignment variable.
  \end{itemize}
\end{itemize}

\hypertarget{estimation-and-inference}{%
\section{Estimation and Inference}\label{estimation-and-inference}}

\hypertarget{local-randomization-based}{%
\subsection{Local Randomization-based}\label{local-randomization-based}}

\textbf{Additional Assumption}: Local Randomization approach assumes that inside the chosen window \(W = [c-w, c+w]\) are assigned to treatment as good as random:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Joint probability distribution of scores for units inside the chosen window \(W\) is known
\item
  Potential outcomes are not affected by value of the score
\end{enumerate}

This approach is stronger than the \protect\hyperlink{continuity-based}{Continuity-based} because we assume the regressions are continuously at \(c\) and unaffected by the running variable within window \(W\)

Because we can choose the window \(W\) (within which random assignment is plausible), the sample size can typically be small.

To choose the window \(W\), we can base on either

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  where the pre-treatment covariate-balance is observed
\item
  independent tests between outcome and score
\item
  domain knowledge
\end{enumerate}

To make inference, we can either use

\begin{itemize}
\item
  (Fisher) randomization inference
\item
  (Neyman) design-based
\end{itemize}

\hypertarget{continuity-based}{%
\subsection{Continuity-based}\label{continuity-based}}

\begin{itemize}
\item
  also known as the local polynomial method

  \begin{itemize}
  \tightlist
  \item
    as the name suggests, global polynomial regression is not recommended (because of lack of robustness, and over-fitting and Runge's phenomenon)
  \end{itemize}
\end{itemize}

Step to estimate local polynomial regression

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Choose polynomial order and weighting scheme
\item
  Choose bandwidth that has optimal MSE or coverage error
\item
  Estimate the parameter of interest
\item
  Examine robust bias-correct inference
\end{enumerate}

\hypertarget{specification-checks}{%
\section{Specification Checks}\label{specification-checks}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \protect\hyperlink{balance-checks}{Balance Checks}
\item
  \protect\hyperlink{sortingbunchingmanipulation}{Sorting/Bunching/Manipulation}
\item
  \protect\hyperlink{placebo-tests}{Placebo Tests}
\item
  \protect\hyperlink{sensitivity-to-bandwidth-choice}{Sensitivity to Bandwidth Choice}
\end{enumerate}

\hypertarget{balance-checks}{%
\subsection{Balance Checks}\label{balance-checks}}

\begin{itemize}
\item
  Also known as checking for Discontinuities in Average Covariates
\item
  Null Hypothesis: The average effect of covariates on pseudo outcomes (i.e., those qualitatively cannot be affected by the treatment) is 0.
\item
  If this hypothesis is rejected, you better have a good reason to why because it can cast serious doubt on your RD design.
\end{itemize}

\hypertarget{sortingbunchingmanipulation}{%
\subsection{Sorting/Bunching/Manipulation}\label{sortingbunchingmanipulation}}

\begin{itemize}
\item
  Also known as checking for A Discontinuity in the Distribution of the Forcing Variable
\item
  Also known as clustering or density test
\item
  Formal test is McCrary sorting test \citep{mccrary2008manipulation} or \citep{cattaneo2019practical}
\item
  Since human subjects can manipulate the running variable to be just above or below the cutoff (assuming that the running variable is manipulable), especially when the cutoff point is known in advance for all subjects, this can result in a discontinuity in the distribution of the running variable at the cutoff (i.e., we will see ``bunching'' behavior right before or after the cutoff)\textgreater{}

  \begin{itemize}
  \item
    People would like to sort into treatment if it's desirable. The density of the running variable would be 0 just below the threshold
  \item
    People would like to be out of treatment if it's undesirable
  \end{itemize}
\item
  \citep{mccrary2008manipulation} proposes a density test (i.e., a formal test for manipulation of the assignment variable).

  \begin{itemize}
  \item
    \(H_0\): The continuity of the density of the running variable (i.e., the covariate that underlies the assignment at the discontinuity point)
  \item
    \(H_a\): A jump in the density function at that point
  \item
    Even though it's not a requirement that the density of the running must be continuous at the cutoff, but a discontinuity can suggest manipulations.
  \end{itemize}
\item
  \citep{zhang2003estimation, lee2009training, aronow2019note} offers a guide to know when you should warrant the manipulation
\item
  Usually it's better to know your research design inside out so that you can suspect any manipulation attempts.

  \begin{itemize}
  \tightlist
  \item
    We would suspect the direction of the manipulation. And typically, it's one-way manipulation. In cases where we might have both ways, theoretically they would cancel each other out.
  \end{itemize}
\item
  We could also observe partial manipulation in reality (e.g., when subjects can only imperfectly manipulate). But typically, as we treat it like fuzzy RD, we would not have identification problems. But complete manipulation would lead to serious identification issues.
\item
  Remember: even in cases where we fail to reject the null hypothesis for the density test, we could not rule out completely that identification problem exists (just like any other hypotheses)
\item
  Bunching happens when people self-select to a specific value in the range of a variable (e.g., key policy thresholds).
\item
  Review paper \citep{kleven2016bunching}
\item
  \textbf{This test can only detect manipulation that changes the distribution of the running variable}. If you can choose the cutoff point or you have 2-sided manipulation, this test will fail to detect it.
\item
  Histogram in bunching is similar to a density curve (we want narrower bins, wider bins bias elasticity estimates)
\item
  We can also use bunching method to study individuals' or firm's responsiveness to changes in policy.
\item
  Under RD, we assume that we don't have any manipulation in the running variable. However, bunching behavior is a manipulation by firms or individuals. Thus, violating this assumption.

  \begin{itemize}
  \item
    Bunching can fix this problem by estimating what densities of individuals would have been without manipulation (i.e., manipulation-free counterfactual).
  \item
    \textbf{The fraction of persons who manipulated} is then calculated by comparing the observed distribution to manipulation-free counterfactual distributions.
  \item
    Under RD, we do not need this step because the observed and manipulation-free counterfactual distributions are assumed to be the same. RD assume there is no manipulation (i.e., assume the manipulation-free counterfactual distribution)
  \end{itemize}
\end{itemize}

When running variable and outcome variable are simultaneously determined, we can use a modified RDD estimator to have consistent estimate. \citep{bajari2011regression}

\begin{itemize}
\item
  \textbf{Assumptions}:

  \begin{itemize}
  \item
    Manipulation is \textbf{one-sided}: People move one way (i.e., either below the threshold to above the threshold or vice versa, but not to or away the threshold), which is similar to the monotonicity assumption under instrumental variable \ref{instrumental-variable}
  \item
    Manipulation is \textbf{bounded} (also known as regularity assumption): so that we can use people far away from this threshold to derive at our counterfactual distribution {[}\citet{blomquist2021bunching}{]}\citep{bertanha2021better}
  \end{itemize}
\end{itemize}

Steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Identify the window in which the running variable contains bunching behavior. We can do this step empirically based on \citet{bosch2020data}. Additionally robustness test is needed (i.e., varying the manipulation window).
\item
  Estimate the manipulation-free counterfactual
\item
  Calculating the standard errors for inference can follow \citep{chetty2016effects} where we bootstrap re-sampling residuals in the estimation of the counts of individuals within bins (large data can render this step unnecessary).
\end{enumerate}

If we pass the bunching test, we can move on to the \protect\hyperlink{placebo-test}{Placebo Test}

\citet{mccrary2008manipulation} test

A jump in the density at the threshold (i.e., discontinuity) hold can serve as evidence for sorting around the cutoff point

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rdd)}

\CommentTok{\# you only need the runing variable and the cutoff point}

\CommentTok{\# Example by the package\textquotesingle{}s authors}
\CommentTok{\#No discontinuity}
\NormalTok{x}\OtherTok{\textless{}{-}}\FunctionTok{runif}\NormalTok{(}\DecValTok{1000}\NormalTok{,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\FunctionTok{DCdensity}\NormalTok{(x,}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{24-regression-discontinuity_files/figure-latex/unnamed-chunk-1-1} \end{center}

\begin{verbatim}
#> [1] 0.8254815

#Discontinuity
x<-runif(1000,-1,1)
x<-x+2*(runif(1000,-1,1)>0&x<0)
DCdensity(x,0)
\end{verbatim}

\begin{center}\includegraphics[width=0.9\linewidth]{24-regression-discontinuity_files/figure-latex/unnamed-chunk-1-2} \end{center}

\begin{verbatim}
#> [1] 0.02607091
\end{verbatim}

\citet{cattaneo2019practical} test

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rddensity)}

\CommentTok{\# Example by the package\textquotesingle{}s authors}
\CommentTok{\# Continuous Density}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{2000}\NormalTok{, }\AttributeTok{mean =} \SpecialCharTok{{-}}\FloatTok{0.5}\NormalTok{)}
\NormalTok{rdd }\OtherTok{\textless{}{-}} \FunctionTok{rddensity}\NormalTok{(}\AttributeTok{X =}\NormalTok{ x, }\AttributeTok{vce =} \StringTok{"jackknife"}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(rdd)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Manipulation testing using local polynomial density estimation.}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of obs =       2000}
\CommentTok{\#\textgreater{} Model =               unrestricted}
\CommentTok{\#\textgreater{} Kernel =              triangular}
\CommentTok{\#\textgreater{} BW method =           estimated}
\CommentTok{\#\textgreater{} VCE method =          jackknife}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} c = 0                 Left of c           Right of c          }
\CommentTok{\#\textgreater{} Number of obs         1376                624                 }
\CommentTok{\#\textgreater{} Eff. Number of obs    354                 345                 }
\CommentTok{\#\textgreater{} Order est. (p)        2                   2                   }
\CommentTok{\#\textgreater{} Order bias (q)        3                   3                   }
\CommentTok{\#\textgreater{} BW est. (h)           0.514               0.609               }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Method                T                   P \textgreater{} |T|             }
\CommentTok{\#\textgreater{} Robust                {-}0.6798             0.4966              }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} P{-}values of binomial tests (H0: p=0.5).}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Window Length / 2          \textless{}c     \textgreater{}=c    P\textgreater{}|T|}
\CommentTok{\#\textgreater{} 0.036                      28      20    0.3123}
\CommentTok{\#\textgreater{} 0.072                      46      39    0.5154}
\CommentTok{\#\textgreater{} 0.107                      68      59    0.4779}
\CommentTok{\#\textgreater{} 0.143                      94      79    0.2871}
\CommentTok{\#\textgreater{} 0.179                     122     103    0.2301}
\CommentTok{\#\textgreater{} 0.215                     145     130    0.3986}
\CommentTok{\#\textgreater{} 0.250                     163     156    0.7370}
\CommentTok{\#\textgreater{} 0.286                     190     176    0.4969}
\CommentTok{\#\textgreater{} 0.322                     214     200    0.5229}
\CommentTok{\#\textgreater{} 0.358                     249     218    0.1650}

\CommentTok{\# you have to specify your own plot (read package manual)}
\end{Highlighting}
\end{Shaded}

\hypertarget{placebo-tests}{%
\subsection{Placebo Tests}\label{placebo-tests}}

\begin{itemize}
\item
  Also known as Discontinuities in Average Outcomes at Other Values
\item
  We should not see any jumps at other values (either \(X_i <c\) or \(X_i \ge c\))

  \begin{itemize}
  \tightlist
  \item
    Use the same bandwidth you use for the cutoff, and move it along the running variable: testing for a jump in the conditional mean of the outcome at the median of the running variable.
  \end{itemize}
\item
  Also known as falsification checks
\item
  Before and after the cutoff point, we can run the placebo test to see whether X's are different).
\item
  The placebo test is where you expect your coefficients to be not different from 0.
\item
  This test can be used for

  \begin{itemize}
  \item
    Testing no discontinuity in predetermined variables:
  \item
    Testing other discontinuities
  \item
    Placebo outcomes: we should see any changes in other outcomes that shouldn't have changed.
  \item
    Inclusion and exclusion of covariates: RDD parameter estimates should not be sensitive to the inclusion or exclusion of other covariates.
  \end{itemize}
\item
  This is analogous to \protect\hyperlink{experimental-design}{Experimental Design} where we cannot only test whether the observables are similar in both treatment and control groups (if we reject this, then we don't have random assignment), but we cannot test unobservables.
\end{itemize}

Balance on observable characteristics on both sides

\[
Z_i = \alpha_0 + \alpha_1 f(x_i) + [I(x_i \ge c)] \alpha_2 + [f(x_i) \times I(x_i \ge c)]\alpha_3 + u_i
\]

where

\begin{itemize}
\item
  \(x_i\) is the running variable
\item
  \(Z_i\) is other characteristics of people (e.g., age, etc)
\end{itemize}

Theoretically, \(Z_i\) should no be affected by treatment. Hence, \(E(\alpha_2) = 0\)

Moreover, when you have multiple \(Z_i\), you typically have to simulate joint distribution (to avoid having significant coefficient based on chance).

The only way that you don't need to generate joint distribution is when all \(Z_i\)'s are independent (unlikely in reality).

Under RD, you shouldn't have to do any \protect\hyperlink{matching-methods}{Matching Methods}. Because just like when you have random assignment, there is no need to make balanced dataset before and after the cutoff. If you have to do balancing, then your RD assumptions are probably wrong in the first place.

\hypertarget{sensitivity-to-bandwidth-choice}{%
\subsection{Sensitivity to Bandwidth Choice}\label{sensitivity-to-bandwidth-choice}}

\begin{itemize}
\item
  Methods for bandwidth selection

  \begin{itemize}
  \item
    Ad-hoc or substantively driven
  \item
    Data driven: cross validation
  \item
    Conservative approach: \citep{calonico2020optimal}
  \end{itemize}
\item
  The objective is to minimize the mean squared error between the estimated and actual treatment effects.
\item
  Then, we need to see how sensitive our results will be dependent on the choice of bandwidth.
\item
  In some cases, the best bandwidth for testing covariates may not be the best bandwidth for treating them, but it may be close.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# find optimal bandwidth by Imbens{-}Kalyanaraman}
\NormalTok{rdd}\SpecialCharTok{::}\FunctionTok{IKbandwidth}\NormalTok{(running\_var,}
\NormalTok{                 outcome\_var,}
                 \AttributeTok{cutpoint =} \StringTok{""}\NormalTok{,}
                 \AttributeTok{kernel =} \StringTok{"triangular"}\NormalTok{) }\CommentTok{\# can also pick other kernels}
\end{Highlighting}
\end{Shaded}

\hypertarget{fuzzy-rd-design}{%
\section{Fuzzy RD Design}\label{fuzzy-rd-design}}

When you have cutoff that does not perfectly determine treatment, but creates a discontinuity in the likelihood of receiving the treatment, you need another instrument

For those that are close to the cutoff, we create an instrument for \(D_i\)

\[
Z_i=
\begin{cases}
1 & \text{if } X_i \ge c \\
0 & \text{if } X_c < c
\end{cases}
\]

Then, we can estimate the effect of the treatment for compliers only (i.e., those treatment \(D_i\) depends on \(Z_i\))

The LATE parameter

\[
\lim_{c - \epsilon \le X \le c + \epsilon, \epsilon \to 0}( \frac{E(Y |Z = 1) - E(Y |Z=0)}{E(D|Z = 1) - E(D|Z = 0)})
\]

equivalently, the canonical parameter:

\[
\frac{lim_{x \downarrow c}E(Y|X = x) - \lim_{x \uparrow c} E(Y|X = x)}{\lim_{x \downarrow c } E(D |X = x) - \lim_{x \uparrow c}E(D |X=x)}
\]

Two equivalent ways to estimate

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  First

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    Sharp RDD for \(Y\)
  \item
    Sharp RDD for \(D\)
  \item
    Take the estimate from step 1 divide by that of step 2
  \end{enumerate}
\item
  Second: Subset those observations that are close to \(c\) and run instrumental variable \(Z\)
\end{enumerate}

\hypertarget{regression-kink-design}{%
\section{Regression Kink Design}\label{regression-kink-design}}

\begin{itemize}
\item
  If the slope of the treatment intensity changes at the cutoff (instead of the level of treatment assignment), we can have regression kink design
\item
  Example: unemployment benefits
\end{itemize}

Sharp Kink RD parameter

\[
\alpha_{KRD} = \frac{\lim_{x \downarrow c} \frac{d}{dx}E[Y_i |X_i = x]- \lim_{x \uparrow c} \frac{d}{dx}E[Y_i |X_i = x]}{\lim_{x \downarrow c} \frac{d}{dx}b(x) - \lim_{x \uparrow c} \frac{d}{dx}b(x)}
\]

where \(b(x)\) is a known function inducing ``kink''

Fuzzy Kink RD parameter

\[
\alpha_{KRD} = \frac{\lim_{x \downarrow c} \frac{d}{dx}E[Y_i |X_i = x]- \lim_{x \uparrow c} \frac{d}{dx}E[Y_i |X_i = x]}{\lim_{x \downarrow c} \frac{d}{dx}E[D_i |X_i = x]- \lim_{x \uparrow c} \frac{d}{dx}E[D_i |X_i = x]}
\]

\hypertarget{multi-cutoff}{%
\section{Multi-cutoff}\label{multi-cutoff}}

\[
\tau (x,c)= E[Y_{1i} - Y_{0i}|X_i = x, C_i = c]
\]

\hypertarget{multi-score}{%
\section{Multi-score}\label{multi-score}}

Multi-score (in multiple dimensions) (e.g., math and English cutoff for certain honor class):

\[
\tau (x_1, x_2) = E[Y_{1i} - Y_{0i}|X_{1i} = x_1, X_{2i} = x]
\]

\hypertarget{steps-for-sharp-rd}{%
\section{Steps for Sharp RD}\label{steps-for-sharp-rd}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Graph the data by computing the average value of the outcome variable over a set of bins (large enough to see a smooth graph, and small enough to make the jump around the cutoff clear).
\item
  Run regression on both sides of the cutoff to get the treatment effect
\item
  Robustness checks:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    Assess possible jumps in other variables around the cutoff
  \item
    Hypothesis testing for bunching
  \item
    Placebo tests
  \item
    Varying bandwidth
  \end{enumerate}
\end{enumerate}

\hypertarget{steps-for-fuzzy-rd}{%
\section{Steps for Fuzzy RD}\label{steps-for-fuzzy-rd}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Graph the data by computing the average value of the outcome variable over a set of bins (large enough to see a smooth graph, and small enough to make the jump around the cutoff clear).
\item
  Graph the probability of treatment
\item
  Estimate the treatment effect using 2SLS
\item
  Robustness checks:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    Assess possible jumps in other variables around the cutoff
  \item
    Hypothesis testing for bunching
  \item
    Placebo tests
  \item
    Varying bandwidth
  \end{enumerate}
\end{enumerate}

\hypertarget{steps-for-rdit-regression-discontinuity-in-time}{%
\section{Steps for RDiT (Regression Discontinuity in Time)}\label{steps-for-rdit-regression-discontinuity-in-time}}

Notes:

\begin{itemize}
\tightlist
\item
  Additional assumption: Time-varying confounders change smoothly across the cutoff date
\item
  Typically used in policy implementation in the same date for all subjects, but can also be used for cases where implementation dates are different between subjects. In the second case, researchers typically use different RDiT specification for each time series.
\item
  Sometimes the date of implementation is not randomly assigned by chosen strategically. Hence, RDiT should be thought of as the ``discontinuity at a threshold'' interpretation of RD (not as ``local randomization''). \citep[p.~8]{hausman2018}
\item
  Normal RD uses variation in the \(N\) dimension, while RDiT uses variation in the \(T\) dimension
\item
  Choose polynomials based on BIC typically. And can have either global polynomial or pre-period and post-period polynomial for each time series (but usually the global one will perform better)
\item
  Could use \textbf{augmented local linear} outlined by \citep[p.~12]{hausman2018}, where estimate the model with all the control first then take the residuals to include in the model with the RDiT treatment (remember to use bootstrapping method to account for the first-stage variance in the second stage).
\end{itemize}

Pros:

\begin{itemize}
\item
  can overcome cases where there is no cross-sectional variation in treatment implementation (DID is not feasible)

  \begin{itemize}
  \tightlist
  \item
    There are papers that use both RDiT and DID to (1) see the differential treatment effects across individuals/ space \citep{auffhammer2011clearing} or (2) compare the 2 estimates where the control group's validity is questionable \citep{gallego2013effect}.
  \end{itemize}
\item
  Better than pre/post comparison because it can include flexible controls
\item
  Better than event studies because it can use long-time horizons (may not be too relevant now since the development long-time horizon event studies), and it can use higher-order polynomials time control variables.
\end{itemize}

Cons:

\begin{itemize}
\item
  Taking observation for from the threshold (in time) can bias your estimates because of unobservables and time-series properties of the data generating process.
\item
  \citep{mccrary2008manipulation} test is not possible (see \protect\hyperlink{sortingbunchingmanipulation}{Sorting/Bunching/Manipulation}) because when the density of the running (time) is uniform, you can't use the test.
\item
  Time-varying unobservables may impact the dependent variable discontinuously
\item
  Error terms are likely to include persistence (serially correlated errors)
\item
  Researchers cannot model time-varying treatment under RDiT

  \begin{itemize}
  \tightlist
  \item
    In a small enough window, the local linear specification is fine, but the global polynomials can either be too big or too small \citep{hausman2018}
  \end{itemize}
\end{itemize}

Biases

\begin{itemize}
\item
  Time-Varying treatment Effects

  \begin{itemize}
  \item
    increase sample size either by

    \begin{itemize}
    \item
      more granular data (greater frequency): will not increase power because of the problem of serial correlation
    \item
      increasing time window: increases bias from other confounders
    \end{itemize}
  \item
    2 additional assumption:

    \begin{itemize}
    \item
      Model is correctly specified (with all confoudners or global polynomial approximation)
    \item
      Treatment effect is correctly specified (whether it's smooth and constant, or varies)
    \item
      These 2 assumptions do not interact ( we don't want them to interact - i.e., we don't want the polynomial correlated with the unobserved variation in the treatment effect)
    \end{itemize}
  \item
    There usually a difference between short-run and long-run treatment effects, but it's also possibly that the bias can stem from the over-fitting problem of the polynomial specification. \citep[p.~544]{hausman2018}
  \end{itemize}
\item
  Autoregression (serial dependence)

  \begin{itemize}
  \item
    Need to use \textbf{clustered standard errors} to account for serial dependence in the residuals
  \item
    In the case of serial dependence in \(\epsilon_{it}\), we don't have a solution, including a lagged dependent variable would misspecify the model (probably find another research project)
  \item
    In the case of serial dependence in \(y_{it}\), with long window, it becomes fuzzy to what you try to recover. You can include the \textbf{lagged dependent variable} (bias can still come from the time-varying treatment or over-fitting of the global polynomial)
  \end{itemize}
\item
  Sorting and Anticipation Effects

  \begin{itemize}
  \item
    Cannot run the \citep{mccrary2008manipulation} because the density of the time running variable is uniform
  \item
    Can still run tests to check discontinuities in other covariates (you want no discontinuities) and discontinuities in the outcome variable at other placebo thresholds ( you don't want discontinuities)
  \item
    Hence, it's hard to argue for the causal effect here because it could be the total effect of the causal treatment and the unobserved sorting/anticipation/adaptation/avoidance effects. You can only argue that there is no such behavior
  \end{itemize}
\end{itemize}

Recommendations for robustness check following \citep[p.~549]{hausman2018}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Plot the raw data and residuals (after removing confounders or trend). With varying polynomial and local linear controls, inconsistent results can be a sign of time-varying treatment effects.
\item
  Using global polynomial, you could overfit, then show polynomial with different order and alternative local linear bandwidths. If the results are consistent, you're okay
\item
  \protect\hyperlink{placebo-tests}{Placebo Tests}: estimate another RD (1) on another location or subject (that did not receive the treatment) or (2) use another date.
\item
  Plot RD discontinuity on continuous controls
\item
  Donut RD to see if avoiding the selection close to the cutoff would yield better results \citep{barreca2011saving}
\item
  Test for auto-regression (using only pre-treatment data). If there is evidence for autoregression, include the lagged dependent variable
\item
  Augmented local linear (no need to use global polynomial and avoid over-fitting)

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    Use full sample to exclude the effect of important predictors
  \item
    Estimate the conditioned second stage on a smaller sample bandwidth
  \end{enumerate}
\end{enumerate}

Examples from \citep[p.~534]{hausman2018} in

econ

\begin{itemize}
\item
  \citep{davis2008effect}: Air quality
\item
  \citep{auffhammer2011clearing}: Air quality
\item
  \citep{chen2018effect}: Air quality
\item
  \citep{de2013deterrent}: car accidents
\item
  \citep{gallego2013effect}: air quality
\item
  \citep{bento2014effects}: Traffic
\item
  \citep{anderson2014subways}: Traffic
\item
  \citep{burger2014did}: Car accidents
\item
  \citep{brodeur2021covid}: Covid19 lock-downs on well-being
\end{itemize}

marketing

\begin{itemize}
\item
  \citep[\citet{busse2013estimating}]{busse20061}: Vehicle prices
\item
  \citep{chen2009learning}: Customer Satisfaction
\item
  \citep{busse2010best}: Vehicle prices
\item
  \citep{davis2010international}: vehicle prices
\end{itemize}

\hypertarget{evaluation-of-an-rd}{%
\section{Evaluation of an RD}\label{evaluation-of-an-rd}}

\begin{itemize}
\item
  Evidence for (either formal tests or graphs)

  \begin{itemize}
  \item
    Treatment and outcomes change discontinuously at the cutoff, while other variables and pre-treatment outcomes do not.
  \item
    No manipulation of the assignment variable.
  \end{itemize}
\item
  Results are robust to various functional forms of the forcing variable
\item
  Is there any other (unobserved) confound that could cause the discontinuous change at the cutoff (i.e., multiple forcing variables / bundling of institutions)?
\item
  External Validity: How likely the result at the cutoff will generalize?
\end{itemize}

\textbf{General Model}

\[
Y_i = \beta_0 + f(x_i) \beta_1 + [I(x_i \ge c)]\beta_2 + \epsilon_i
\]

where \(f(x_i)\) is any functional form of \(x_i\)

\textbf{Simple case}

When \(f(x_i) = x_i\) (linear function)

\[
Y_i = \beta_0 + x_i \beta_1 + [I(x_i \ge c)]\beta_2 + \epsilon_i
\]

\includegraphics[width=6.25in,height=3.125in]{images/rd1.PNG}

RD gives you \(\beta_2\) (causal effect) of \(X\) on \(Y\) at the cutoff point

In practice, everyone does

\[
Y_i = \alpha_0 + f(x) \alpha _1 + [I(x_i \ge c)]\alpha_2 + [f(x_i)\times [I(x_i \ge c)]\alpha_3 + u_i
\]

\includegraphics[width=6.25in,height=3.125in]{images/rd2.PNG}

where we estimate different slope on different sides of the line

and if you estimate \(\alpha_3\) to be no different from 0 then we return to the simple case

\textbf{Notes}:

\begin{itemize}
\item
  Sparse data can make \(\alpha_3\) large differential effect
\item
  People are very skeptical when you have complex \(f(x_i)\), usual simple function forms (e.g., linear, squared term, etc.) should be good. However, if you still insist, then \textbf{non-parametric estimation} can be your best bet.
\end{itemize}

Bandwidth of \(c\) (window)

\begin{itemize}
\item
  Closer to \(c\) can give you lower bias, but also efficiency
\item
  Wider \(c\) can increase bias, but higher efficiency.
\item
  Optimal bandwidth is very controversial, but usually we have to do it in the appendix for research article anyway.
\item
  We can either

  \begin{itemize}
  \item
    drop observations outside of bandwidth or
  \item
    weight depends on how far and close to \(c\)
  \end{itemize}
\end{itemize}

\hypertarget{applications}{%
\section{Applications}\label{applications}}

Examples in marketing:

\begin{itemize}
\item
  \citep{narayanan2015position}
\item
  \citep{hartmann2011identifying}: nonparametric estimation and guide to identifying causal marketing mix effects
\end{itemize}

\href{https://rdpackages.github.io/}{Packages} in R (see \citep{thoemmes2017analysis} for detailed comparisons): all can handle both sharp and fuzzy RD

\begin{itemize}
\item
  \texttt{rdd}
\item
  \texttt{rdrobust} estimation, inference and plot
\item
  \texttt{rddensity} discontinuity in density tests (\protect\hyperlink{sortingbunchingmanipulation}{Sorting/Bunching/Manipulation}) using local polynomials and binomial test
\item
  \texttt{rdlocrand} covariate balance, binomial tests, window selection
\item
  \texttt{rdmulti} multiple cutoffs and multiple scores
\item
  \texttt{rdpower} power, sample selection
\item
  \texttt{rddtools}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1860}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2016}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2326}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3643}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Package
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
rdd
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
rdrobust
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
rddtools
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Coefficient estimator & Local linear regression & local polynomial regression & local polynomial regression \\
bandwidth selectors & \citep{imbens2012optimal} & \citep{calonico2020optimal}

\citep{imbens2012optimal}

\citep{calonico2014robust} & \citep{imbens2012optimal} \\
\begin{minipage}[t]{\linewidth}\raggedright
Kernel functions

\begin{itemize}
\item
  Triangular
\item
  Rectangular
\end{itemize}
\end{minipage} & Epanechnikov

Gaussian & Epanechnikov & Gaussian \\
Bias Correction & & Local polynomial regression & \\
Covariate options & Include & Include & Include

Residuals \\
Assumptions testing & McCrary sorting & & McCrary sorting

Equality of covariates distribution and mean \\
\end{longtable}

based on table 1 \citep{thoemmes2017analysis} (p.~347)

\hypertarget{example-1-1}{%
\subsection{Example 1}\label{example-1-1}}

Example by \href{https://towardsdatascience.com/the-crown-jewel-of-causal-inference-regression-discontinuity-design-rdd-bad37a68e786}{Leihua Ye}

\[
Y_i = \beta_0 + \beta_1 X_i + \beta_2 W_i + u_i
\]

\[
X_i = 
\begin{cases}
1, W_i \ge c \\
0, W_i < c
\end{cases}
\]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#cutoff point = 3.5}
\NormalTok{GPA }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{4}\NormalTok{)}
\NormalTok{future\_success }\OtherTok{\textless{}{-}} \DecValTok{10} \SpecialCharTok{+} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ GPA }\SpecialCharTok{+} \DecValTok{10} \SpecialCharTok{*}\NormalTok{ (GPA }\SpecialCharTok{\textgreater{}=} \FloatTok{3.5}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{1000}\NormalTok{)}
\CommentTok{\#install and load the package â€˜rddtoolsâ€™}
\CommentTok{\#install.packages(â€œrddtoolsâ€)}
\FunctionTok{library}\NormalTok{(rddtools)}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{rdd\_data}\NormalTok{(future\_success, GPA, }\AttributeTok{cutpoint =} \FloatTok{3.5}\NormalTok{)}
\CommentTok{\# plot the dataset}
\FunctionTok{plot}\NormalTok{(}
\NormalTok{    data,}
    \AttributeTok{col =}  \StringTok{"red"}\NormalTok{,}
    \AttributeTok{cex =} \FloatTok{0.1}\NormalTok{,}
    \AttributeTok{xlab =}  \StringTok{"GPA"}\NormalTok{,}
    \AttributeTok{ylab =}  \StringTok{"future\_success"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{24-regression-discontinuity_files/figure-latex/unnamed-chunk-4-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# estimate the sharp RDD model}
\NormalTok{rdd\_mod }\OtherTok{\textless{}{-}} \FunctionTok{rdd\_reg\_lm}\NormalTok{(}\AttributeTok{rdd\_object =}\NormalTok{ data, }\AttributeTok{slope =}  \StringTok{"same"}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(rdd\_mod)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = y \textasciitilde{} ., data = dat\_step1, weights = weights)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}      Min       1Q   Median       3Q      Max }
\CommentTok{\#\textgreater{} {-}2.93235 {-}0.66786 {-}0.00799  0.69991  3.01768 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept) 17.08582    0.07178  238.03   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} D            9.95513    0.11848   84.03   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} x            2.01615    0.03546   56.85   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 1.046 on 997 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.9617, Adjusted R{-}squared:  0.9616 }
\CommentTok{\#\textgreater{} F{-}statistic: 1.253e+04 on 2 and 997 DF,  p{-}value: \textless{} 2.2e{-}16}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# plot the RDD model along with binned observations}
\FunctionTok{plot}\NormalTok{(}
\NormalTok{    rdd\_mod,}
    \AttributeTok{cex =} \FloatTok{0.1}\NormalTok{,}
    \AttributeTok{col =}  \StringTok{"red"}\NormalTok{,}
    \AttributeTok{xlab =}  \StringTok{"GPA"}\NormalTok{,}
    \AttributeTok{ylab =}  \StringTok{"future\_success"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{24-regression-discontinuity_files/figure-latex/unnamed-chunk-6-1} \end{center}

\hypertarget{example-2}{%
\subsection{Example 2}\label{example-2}}

\citet{bowblis2021occupational}

Occupational licensing can either increase or decrease market efficiency:

\begin{itemize}
\item
  More information means more efficiency
\item
  Increased entry barriers (i.e., friction) increase efficiency
\end{itemize}

Components of RD

\begin{itemize}
\tightlist
\item
  Running variable
\item
  Cutoff: 120 beds or above
\item
  Treatment: you have to have the treatment before the cutoff point.
\end{itemize}

Under OLS

\[
Y_i = \alpha_0 + X_i \alpha_1 + LW_i \alpha_2 + \epsilon_i
\]

where

\begin{itemize}
\item
  \(LW_i\) Licensed/certified workers (in fraction format for each center).
\item
  \(Y_i\) = Quality of service
\end{itemize}

Bias in \(\alpha_2\)

\begin{itemize}
\item
  Mitigation-based: terrible quality can lead to more hiring, which negatively bias \(\alpha_2\)
\item
  Preference-based: places that have higher quality staff want to keep high quality staffs.
\end{itemize}

Under RD

\[
\begin{aligned}
Y_{ist} &= \beta_0 + [I(Bed \ge121)_{ist}]\beta_1 + f(Size_{ist}) \beta_2\\
&+ [f(Size_{ist}) \times I(Bed \ge 121)_{ist}] \beta_3 \\
&+ X_{it} \delta + \gamma_s + \theta_t + \epsilon_{ist}
\end{aligned}
\]

where

\begin{itemize}
\item
  \(s\) = state
\item
  \(t\) = year
\item
  \(i\) = hospital
\end{itemize}

This RD is fuzzy

\begin{itemize}
\item
  If right near the threshold (bandwidth), we have states with different sorting (i.e., non-random), then we need the fixed-effect for state \(s\). But then your RD assumption wrong anyway, then you won't do it in the first place
\item
  Technically, we could also run the fixed-effect regression, but because it's lower in the causal inference hierarchy. Hence, we don't do it.
\item
  Moreover, in the RD framework, we don't include \(t\) before treatment (but in the FE we have to include before and after)
\item
  If we include \(\pi_i\) for each hospital, then we don't have variation in the causal estimates (because hardly any hospital changes their bed size in the panel)
\item
  When you have \(\beta_1\) as the intent to treat (because the treatment effect does not coincide with the intent to treat)
\item
  You cannot take those fuzzy cases out, because it will introduce the selection bias.
\item
  Note that we cannot drop cases based on behavioral choice (because we will exclude non-compliers), but we can drop when we have particular behaviors ((e.g., people like round numbers).
\end{itemize}

Thus, we have to use Instrument variable \ref{instrumental-variable}

\textbf{Stage 1:}

\[
\begin{aligned}
QSW_{ist} &= \alpha_0 + [I(Bed \ge121)_{ist}]\alpha_1 + f(Size_{ist}) \alpha_2\\
&+ [f(Size_{ist}) \times I(Bed \ge 121)_{ist}] \alpha_3 \\
&+ X_{it} \delta + \gamma_s + \theta_t + \epsilon_{ist}
\end{aligned}
\]

(Note: you should have different fixed effects and error term - \(\delta, \gamma_s, \theta_t, \epsilon_{ist}\) from the first equation, but I ran out of Greek letters)

\textbf{Stage 2:}

\[
\begin{aligned}
Y_{ist} &= \gamma_0 + \gamma_1 \hat{QWS}_{ist} + f(Size_{ist}) \delta_2 \\
&+ [f(Size_{ist}) \times I(Bed \ge 121)] \delta_3 \\
&+ X_{it} \lambda + \eta_s + \tau_t + u_{ist}
\end{aligned}
\]

\begin{itemize}
\item
  The bigger the jump (discontinuity), the more similar the 2 coefficients (\(\gamma_1 \approx \beta_1\)) where \(\gamma_1\) is the average treatment effect (of exposing to the policy)
\item
  \(\beta_1\) will always be closer to 0 than \(\gamma_1\)
\item
  Figure 1 shows bunching at every 5 units cutoff, but 120 is still out there.
\item
  If we have manipulable bunching, there should be decrease at 130
\item
  Since we have limited number of mass points (at the round numbers), we should clustered standard errors by the mass point
\end{itemize}

\hypertarget{example-3}{%
\subsection{Example 3}\label{example-3}}

Replication of \citep{carpenter2009effect} by \href{https://rpubs.com/phle/r_tutorial_regression_discontinuity_design}{Philipp Leppert}, dataset from \href{https://www.openicpsr.org/openicpsr/project/113550/version/V1/view?flag=follow\&pageSize=100\&sortOrder=(?title)\&sortAsc=true}{here}

\hypertarget{example-4}{%
\subsection{Example 4}\label{example-4}}

For a detailed application, see \citep{thoemmes2017analysis} where they use \texttt{rdd}, \texttt{rdrobust}, \texttt{rddtools}

\hypertarget{synthetic-difference-in-differences}{%
\chapter{Synthetic Difference-in-Differences}\label{synthetic-difference-in-differences}}

by \citep{arkhangelsky2021synthetic}

also known as weighted double-differencing estimators

\begin{itemize}
\item
  Setting: Researchers use panel data to study effects of policy changes.

  \begin{itemize}
  \item
    Panel data: repeated observations across time for various units.
  \item
    Some units exposed to policy at different times than others.
  \end{itemize}
\item
  Policy changes often aren't random across units or time.

  \begin{itemize}
  \tightlist
  \item
    Challenge: Observed covariates might not lead to credible conclusions of no confounding \citep{imbens2015causal}
  \end{itemize}
\item
  To estimate the effects, either

  \begin{itemize}
  \item
    \protect\hyperlink{difference-in-differences}{Difference-in-differences} (DID) method widely used in applied economics.
  \item
    \protect\hyperlink{synthetic-control}{Synthetic Control} (SC) methods offer alternative approach for comparative case studies.
  \end{itemize}
\item
  Difference between DID and SC:

  \begin{itemize}
  \item
    DID: used with many policy-exposed units; relies on ``parallel trends'' assumption.
  \item
    SC: used with few policy-exposed units; compensates lack of parallel trends by reweighting units based on pre-exposure trends.
  \end{itemize}
\item
  \textbf{New proposition}: Synthetic Difference in Differences (SDID).

  \begin{itemize}
  \item
    Combines features of DID and SC.
  \item
    Reweights and matches pre-exposure trends (similar to SC).
  \item
    Invariant to additive unit-level shifts, valid for large-panel inference (like DID).
  \end{itemize}
\item
  Attractive features:

  \begin{itemize}
  \item
    SDID provides consistent and asymptotically normal estimates.
  \item
    SDID performs on par with or better than DID in traditional DID settings.

    \begin{itemize}
    \tightlist
    \item
      where DID can only handle completely random treatment assignment, SDID can handle cases where treatment assignment is correlated with some time or unit latent factors.
    \end{itemize}
  \item
    Similarly, SDID is as good as or better than SC in traditional SC settings.
  \item
    Uniformly random treatment assignment results in unbiased outcomes for all methods, but SDID is more precise.
  \item
    SDID reduces bias effectively for non-uniformly random assignments.
  \item
    SDID's double robustness is akin to the augmented inverse probability weighting estimator \citep[\citet{scharfstein1999adjusting}]{ben2021augmented}.
  \item
    Very much similar to augmented SC estimator by \citep[p.~4112]{ben2021augmented, arkhangelsky2021synthetic}
  \end{itemize}
\end{itemize}

To apply to staggered adoption settings using the SDID estimator (see examples in \citet{arkhangelsky2021synthetic}, p.~4115), we can:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Apply the SDID estimator repeatedly, once for every adoption date.
\item
  Using \citet{ben2022synthetic} 's method, form matrices for each adoption date. Apply SDID and average based on treated unit/time-period fractions.
\item
  Create multiple samples by splitting the data up by time periods. Each sample should have a consistent adoption date.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{understanding}{%
\section{Understanding}\label{understanding}}

Consider a traditional time-series cross-sectional data

Let \(Y_{it}\) denote the outcome for unit \(i\) in period \(t\)

A balanced panel of \(N\) units and \(T\) time periods

\begin{itemize}
\item
  \(W_{it} \in \{0, 1\}\) is the binary treatment
\item
  \(N_c\) never-treated units (control)
\item
  \(N_t\) treated units after time \(T_{pre}\)
\end{itemize}

\textbf{Steps}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Find unit weights \(\hat{w}^{sdid}\) such that \(\sum_{i = 1}^{N_c} \hat{w}_i^{sdid} Y_{it} \approx N_t^{-1} \sum_{i = N_c + 1}^N Y_{it} \forall t = 1, \dots, T_{pre}\) (i.e., pre-treatment trends in outcome of the treated similar to those of control units) (similar to SC).
\item
  Find time weights \(\hat{\lambda}_t\) such that we have a balanced window (i.e., posttreatment outcomes for control units differ consistently from their weighted average pretreatment outcomes).
\item
  Estimate the average causal effect of treatment
\end{enumerate}

\[
(\hat{\tau}^{sdid}, \hat{\mu}, \hat{\alpha}, \hat{\beta}) = \arg \min_{\tau, \mu, \alpha, \beta} \{ \sum_{i = 1}^N \sum_{t = 1}^T (Y_{it} - \mu - \alpha_i - \beta_ t - W_{it} \tau)^2 \hat{w}_i^{sdid} \hat{\lambda}_t^{sdid} \}
\]

Better than DiD estimator because \(\tau^{did}\) does not consider time or unit weights

\[
(\hat{\tau}^{did}, \hat{\mu}, \hat{\alpha}, \hat{\beta}) = \arg \min_{\tau, \mu, \alpha, \beta} \{ \sum_{i = 1}^N \sum_{t = 1}^T (Y_{it} - \mu - \alpha_i - \beta_ t - W_{it} \tau)^2 \}
\]

Better than SC estimator because \(\tau^{sc}\) lacks unit fixed effete and time weights

\[
(\hat{\tau}^{sc}, \hat{\mu}, \hat{\beta}) = \arg \min_{\tau, \mu, \beta} \{ \sum_{i = 1}^N \sum_{t = 1}^T (Y_{it} - \mu - \beta_ t - W_{it} \tau)^2 \hat{w}_i^{sdid}  \}
\]

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1119}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2413}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2867}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3531}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{DID}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{SC}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{SDID}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Primary Assumption} & Absence of intervention leads to parallel evolution across states. & Reweights unexposed states to match pre-intervention outcomes of treated state. & Reweights control units to ensure a parallel time trend with the treated pre-intervention trend. \\
\textbf{Reliability Concern} & Can be unreliable when pre-intervention trends aren't parallel. & Accounts for non-parallel pre-intervention trends by reweighting. & Uses reweighting to adjust for non-parallel pre-intervention trends. \\
\textbf{Treatment of Time Periods} & All pre-treatment periods are given equal weight. & Doesn't specifically emphasize equal weight for pre-treatment periods. & Focuses only on a subset of pre-intervention time periods, selected based on historical outcomes. \\
\textbf{Goal with Reweighting} & N/A (doesn't use reweighting). & To match treated state as closely as possible before the intervention. & Make trends of control units parallel (not necessarily identical) to the treated pre-intervention. \\
\end{longtable}

Alternatively, think of our parameter of interest as:

\[
\hat{\tau} = \hat{\delta}_t - \sum_{i = 1}^{N_c} \hat{w}_i \hat{\delta}_i
\]

where \(\hat{\delta}_t = \frac{1}{N_t} \sum_{i = N_c + 1}^N \hat{\delta}_i\)

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.0492}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2121}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.4167}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3144}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Sample Weight
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Adjusted outcomes (\(\hat{\delta}_i\))
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Interpretation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
SC & \(\hat{w}^{sc} = \min_{w \in R}l_{unit}(w)\) & \(\frac{1}{T_{post}} \sum_{t = T_{pre} + 1}^T Y_{it}\) & Unweighted treatment period averages \\
DID & \(\hat{w}_i^{did} = N_c^{-1}\) & \(\frac{1}{T_{post}} \sum_{t = T_{pre}+ 1}^T Y_{it} - \frac{1}{T_{pre}} \sum_{t = 1}^{T_{pre}}Y_{it}\) & Unweighted differences between average treatment period and pretreatment outcome \\
SDID & \((\hat{w}_0, \hat{w}^{sdid}) = \min l_{unit}(w_0, w)\) & \(\frac{1}{T_{post}} \sum_{t = T_{pre} + 1}^T Y_{it} - \sum_{t = 1}^{T_{pre}} \hat{\lambda}_t^{sdid} Y_{it}\) & Weighted differences between average treatment period and pretreatment outcome \\
\end{longtable}

\begin{itemize}
\item
  The SDID estimator uses weights:

  \begin{itemize}
  \item
    Makes two-way fixed effect regression ``local.''
  \item
    Emphasizes units similar in their past to treated units.
  \item
    Prioritizes periods resembling treated periods.
  \end{itemize}
\item
  Benefits of this localization:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \item
    \textbf{Robustness}: Using similar units and periods boosts estimator's robustness.
  \item
    \textbf{Improved Precision}: Weights can eliminate predictable outcome components.

    \begin{itemize}
    \item
      The SEs of SDID are smaller than those of SC and DID
    \item
      Caveat: If there's minor systematic heterogeneity in outcomes, unequal weighting might reduce precision compared to standard DID.
    \end{itemize}
  \end{enumerate}
\item
  Weight Design:

  \begin{itemize}
  \item
    \textbf{Unit Weights}: Makes average outcome for treated units roughly parallel to the weighted average for control units.
  \item
    \textbf{Time Weights}: Ensures posttreatment outcomes for control units differ consistently from their weighted average pretreatment outcomes.
  \end{itemize}
\item
  Weights enhance DID's plausibility:

  \begin{itemize}
  \item
    Raw data often lacks parallel time trends for treated/control units.
  \item
    Similar techniques (e.g., adjusting for covariates or selecting specific time periods) were used before \citep{callaway2021difference}.
  \item
    SDID automates this process, applying a similar logic to weight both units and time periods.
  \end{itemize}
\item
  Time Weights in SDID:

  \begin{itemize}
  \tightlist
  \item
    Removes bias and boosts precision (i.e., minimizes the influence of time periods vastly different from posttreatment periods).
  \end{itemize}
\item
  Argument for Unit Fixed Effects:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \item
    \textbf{Flexibility}: Increases model flexibility and thereby bolsters robustness.
  \item
    \textbf{Enhanced Precision}: Unit fixed effects explain a significant portion of outcome variation.
  \end{enumerate}
\item
  SC Weighting \& Unit Fixed Effects:

  \begin{itemize}
  \item
    Under certain conditions, SC weighting can inherently account for unit fixed effects.

    \begin{itemize}
    \tightlist
    \item
      For example, when the weighted average outcome for control units in pretreatment is the same as that of the treated units. (unlikely in reality)
    \end{itemize}
  \item
    The use of unit fixed effect in synthetic control regression (i.e., synthetic control with intercept) was proposed before in \citet{doudchenko2016balancing} and \citet{ferman2021synthetic} (called DIFP)
  \end{itemize}
\end{itemize}

More details on application

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Choose unit weights
\end{enumerate}

\begin{itemize}
\item
  Regularization Parameter:

  \begin{itemize}
  \tightlist
  \item
    Equal to the size of a typical one-period outcome change for control units in the pre-period, then multiplied by a scaling factor \citep[p.~4092]{arkhangelsky2021synthetic}.
  \end{itemize}
\item
  Relation to SC Weights:

  \begin{itemize}
  \item
    SDID weights are similar to those used in \citep{abadie2010synthetic} except two distinctions:

    \begin{enumerate}
    \def\labelenumi{\arabic{enumi}.}
    \item
      Inclusion of an Intercept Term:

      \begin{itemize}
      \item
        The weights in SynthDiD do not necessarily make the control pre-trends perfectly match the treated trends, just make them parallel.
      \item
        This flexibility comes from the use of unit fixed effects, which can absorb any consistent differences between units.
      \end{itemize}
    \item
      Regularization Penalty:

      \begin{itemize}
      \item
        Adopted from \citet{doudchenko2016balancing} .
      \item
        Enhances the dispersion and ensures the uniqueness of the weights.
      \end{itemize}
    \end{enumerate}
  \item
    DID weights are identical to those used in \citep{abadie2010synthetic} without intercept and regularization penalty and 1 treated unit.
  \end{itemize}
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Choose time weights
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Also include an intercept term, but no regularization (because correlated observations within time periods for the same unit is plausible, but not across units within the same period).
\end{itemize}

\textbf{Note}: To account for time-varying variables in the weights, one can use the residuals of the regression of the observed outcome on these time-varying variables, instead of the observed outcomes themselves (\(Y_{it}^{res} = Y_{it} - X_{it} \hat{\beta}\), where \(\hat{\beta}\) come from \(Y = \beta X_{it}\)).

The SDID method can account for systematic effects, often referred to as unit effects or unit heterogeneity, which influence treatment assignment (i.e., when treatment assignment is correlated with these systematic effects). Consequently, it provides unbiased estimates, especially valuable when there's a suspicion that the treatment might be influenced by persistent, unit-specific attributes.

Even in cases where we have completely random assignment, SDID, DiD, and SC are unbiased, but SynthDiD has the smallest SE.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{application-13}{%
\section{Application}\label{application-13}}

\textbf{SDID Algorithm}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute regularization parameter \(\zeta\)
\end{enumerate}

\[
\zeta = (N_{t}T_{post})^{1/4} \hat{\sigma}
\]

where

\[
\hat{\sigma}^2 = \frac{1}{N_c(T_{pre}- 1)} \sum_{i = 1}^{N_c} \sum_{t = 1}^{T_{re}-1}(\Delta_{it} - \hat{\Delta})^2
\]

\begin{itemize}
\item
  \(\Delta_{it} = Y_{i(t + 1)} - Y_{it}\)
\item
  \(\hat{\Delta} = \frac{1}{N_c(T_{pre} - 1)}\sum_{i = 1}^{N_c}\sum_{t = 1}^{T_{pre}-1} \Delta_{it}\)
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Compute unit weights \(\hat{w}^{sdid}\)
\end{enumerate}

\[
(\hat{w}_0, \hat{w}^{sidid}) = \arg \min_{w_0 \in R, w \in \Omega}l_{unit}(w_0, w)
\]

where

\begin{itemize}
\item
  \(l_{unit} (w_0, w) = \sum_{t = 1}^{T_{pre}}(w_0 + \sum_{i = 1}^{N_c}w_i Y_{it} - \frac{1}{N_t}\sum_{i = N_c + 1}^NY_{it})^2 + \zeta^2 T_{pre}||w||_2^2\)
\item
  \(\Omega = \{w \in R_+^N: \sum_{i = 1}^{N_c} w_i = 1, w_i = N_t^{-1} \forall i = N_c + 1, \dots, N \}\)
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Compute time weights \(\hat{\lambda}^{sdid}\)
\end{enumerate}

\[
(\hat{\lambda}_0 , \hat{\lambda}^{sdid}) = \arg \min_{\lambda_0 \in R, \lambda \in \Lambda} l_{time}(\lambda_0, \lambda)
\]

where

\begin{itemize}
\item
  \(l_{time} (\lambda_0, \lambda) = \sum_{i = 1}^{N_c}(\lambda_0 + \sum_{t = 1}^{T_{pre}} \lambda_t Y_{it} - \frac{1}{T_{post}} \sum_{t = T_{pre} + 1}^T Y_{it})^2\)
\item
  \(\Lambda = \{ \lambda \in R_+^T: \sum_{t = 1}^{T_{pre}} \lambda_t = 1, \lambda_t = T_{post}^{-1} \forall t = T_{pre} + 1, \dots, T\}\)
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Compute the SDID estimator
\end{enumerate}

\[
(\hat{\tau}^{sdid}, \hat{\mu}, \hat{\alpha}, \hat{\beta}) = \arg \min_{\tau, \mu, \alpha, \beta}\{ \sum_{i = 1}^N \sum_{t = 1}^T (Y_{it} - \mu - \alpha_i - \beta_t - W_{it} \tau)^2 \hat{w}_i^{sdid}\hat{\lambda}_t^{sdid}
\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{SE Estimation}

\begin{itemize}
\item
  Under certain assumptions (errors, samples, and interaction properties between time and unit fixed effects) detailed in \citep[p.~4107]{arkhangelsky2019synthetic}, SDID is asymptotically normal and zero-centered
\item
  Using its asymptotic variance, conventional confidence intervals can be applied to SDID.
\end{itemize}

\[
\tau \in \hat{\tau}^{sdid} \pm z_{\alpha/2}\sqrt{\hat{V}_\tau}
\]

\begin{itemize}
\item
  There are 3 approaches for variance estimation in confidence intervals:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \item
    \textbf{Clustered Bootstrap \citep{efron1992bootstrap}:}

    \begin{itemize}
    \item
      Independently resample units.
    \item
      Advantages: Simple to use; robust performance in large panels due to natural approach to inference with panel data where observations of the same unit might be correlated.
    \item
      Disadvantage: Computationally expensive.
    \end{itemize}
  \item
    \textbf{Jackknife \citep{miller1974jackknife}:}

    \begin{itemize}
    \item
      Applied to weighted SDID regression with fixed weights.
    \item
      Generally conservative and precise when treated and control units are sufficiently similar.
    \item
      Not recommended for some methods, like the SC estimator, due to potential biases.
    \item
      Appropriate for jackknifing DID without random weights.
    \end{itemize}
  \item
    \textbf{Placebo Variance Estimation:}

    \begin{itemize}
    \item
      Can used in cases with only one treated unit or large panels.
    \item
      Placebo evaluations swap out the treated unit for untreated ones to estimate noise.
    \item
      Relies on homoskedasticity across units.
    \item
      Depends on homoskedasticity across units. It hinges on the empirical distribution of residuals from placebo estimators on control units.
    \item
      The validity of the placebo method hinges on consistent noise distribution across units. One treated unit makes nonparametric variance estimation difficult, necessitating homoskedasticity for feasible inference. Detailed analysis available in \citet{conley2011inference}.
    \end{itemize}
  \end{enumerate}
\end{itemize}

All algorithms are from \citet{arkhangelsky2021synthetic}, p.~4109:

\begin{quote}
\textbf{Bootstrap Variance Estimation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  For each \(b\) from \(1 \to B\):

  \begin{itemize}
  \item
    Sample \(N\) rows from \((\mathbf{Y}, \mathbf{W})\) to get (\(\mathbf{Y}^{(b)}, \mathbf{W}^{(b)}\)) with replacement.
  \item
    If the sample lacks treated or control units, resample.
  \item
    Calculate \(\tau^{(b)}\) using (\(\mathbf{Y}^{(b)}, \mathbf{W}^{(b)}\)).
  \end{itemize}
\item
  Calculate variance: \(\hat{V}_\tau = \frac{1}{B} \sum_{b = 1}^B (\hat{\tau}^{b} - \frac{1}{B} \sum_{b = 1}^B \hat{\tau}^b)^2\)
\end{enumerate}
\end{quote}

\begin{quote}
\textbf{Jackknife Variance Estimation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  For each \(i\) from \(1 \to N\):

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Calculate \(\hat{\tau}^{(-i)}\): \(\arg\min_{\tau, \{\alpha_j, \beta_t\}} \sum_{j \neq, i, t}(\mathbf{Y}_{jt} - \alpha_j - \beta_t - \tau \mathbf{W}_{it})^2 \hat{w}_j \hat{\lambda}_t\)
  \end{enumerate}
\item
  Calculate: \(\hat{V}_{\tau} = (N - 1) N^{-1} \sum_{i = 1}^N (\hat{\tau}^{(-i)} - \hat{\tau})^2\)
\end{enumerate}
\end{quote}

\begin{quote}
\textbf{Placebo Variance Estimation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  For each \(b\) from \(1 \to B\)

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Sample \(N_t\) out of \(N_c\) without replacement to get the ``placebo'' treatment
  \item
    Construct a placebo treatment matrix \(\mathbf{W}_c^b\) for the controls
  \item
    Calculate \(\hat{\tau}\) based on~\((\mathbf{Y}_c, \mathbf{W}_c^b)\)
  \end{enumerate}
\item
  Calculate \(\hat{V}_\tau = \frac{1}{B}\sum_{b = 1}^B (\hat{\tau}^b - \frac{1}{B} \sum_{b = 1}^B \hat{\tau}^b)^2\)
\end{enumerate}
\end{quote}

\hypertarget{block-treatment}{%
\subsection{Block Treatment}\label{block-treatment}}

Code provided by the \texttt{synthdid} package

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(synthdid)}
\FunctionTok{library}\NormalTok{(tidyverse)}

\CommentTok{\# Estimate the effect of California Proposition 99 on cigarette consumption}
\FunctionTok{data}\NormalTok{(}\StringTok{\textquotesingle{}california\_prop99\textquotesingle{}}\NormalTok{)}

\NormalTok{setup }\OtherTok{=}\NormalTok{ synthdid}\SpecialCharTok{::}\FunctionTok{panel.matrices}\NormalTok{(synthdid}\SpecialCharTok{::}\NormalTok{california\_prop99)}

\NormalTok{tau.hat }\OtherTok{=}\NormalTok{ synthdid}\SpecialCharTok{::}\FunctionTok{synthdid\_estimate}\NormalTok{(setup}\SpecialCharTok{$}\NormalTok{Y, setup}\SpecialCharTok{$}\NormalTok{N0, setup}\SpecialCharTok{$}\NormalTok{T0)}

\CommentTok{\# se = sqrt(vcov(tau.hat, method = \textquotesingle{}placebo\textquotesingle{}))}

\FunctionTok{plot}\NormalTok{(tau.hat) }\SpecialCharTok{+}\NormalTok{ causalverse}\SpecialCharTok{::}\FunctionTok{ama\_theme}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{25-synthdid_files/figure-latex/unnamed-chunk-1-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{setup }\OtherTok{=}\NormalTok{ synthdid}\SpecialCharTok{::}\FunctionTok{panel.matrices}\NormalTok{(synthdid}\SpecialCharTok{::}\NormalTok{california\_prop99)}

\CommentTok{\# Run for specific estimators}
\NormalTok{results\_selected }\OtherTok{=}\NormalTok{ causalverse}\SpecialCharTok{::}\FunctionTok{panel\_estimate}\NormalTok{(setup,}
                                               \AttributeTok{selected\_estimators =} \FunctionTok{c}\NormalTok{(}\StringTok{"synthdid"}\NormalTok{, }\StringTok{"did"}\NormalTok{, }\StringTok{"sc"}\NormalTok{))}

\NormalTok{results\_selected}
\CommentTok{\#\textgreater{} $synthdid}
\CommentTok{\#\textgreater{} $synthdid$estimate}
\CommentTok{\#\textgreater{} synthdid: {-}15.604 +{-} NA. Effective N0/N0 = 16.4/38\textasciitilde{}0.4. Effective T0/T0 = 2.8/19\textasciitilde{}0.1. N1,T1 = 1,12. }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $synthdid$std.error}
\CommentTok{\#\textgreater{} [1] 10.05324}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $did}
\CommentTok{\#\textgreater{} $did$estimate}
\CommentTok{\#\textgreater{} synthdid: {-}27.349 +{-} NA. Effective N0/N0 = 38.0/38\textasciitilde{}1.0. Effective T0/T0 = 19.0/19\textasciitilde{}1.0. N1,T1 = 1,12. }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $did$std.error}
\CommentTok{\#\textgreater{} [1] 15.81479}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $sc}
\CommentTok{\#\textgreater{} $sc$estimate}
\CommentTok{\#\textgreater{} synthdid: {-}19.620 +{-} NA. Effective N0/N0 = 3.8/38\textasciitilde{}0.1. Effective T0/T0 = Inf/19\textasciitilde{}Inf. N1,T1 = 1,12. }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $sc$std.error}
\CommentTok{\#\textgreater{} [1] 11.16422}

\CommentTok{\# to access more details in the estimate object}
\FunctionTok{summary}\NormalTok{(results\_selected}\SpecialCharTok{$}\NormalTok{did}\SpecialCharTok{$}\NormalTok{estimate)}
\CommentTok{\#\textgreater{} $estimate}
\CommentTok{\#\textgreater{} [1] {-}27.34911}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $se}
\CommentTok{\#\textgreater{}      [,1]}
\CommentTok{\#\textgreater{} [1,]   NA}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $controls}
\CommentTok{\#\textgreater{}                estimate 1}
\CommentTok{\#\textgreater{} Wyoming             0.026}
\CommentTok{\#\textgreater{} Wisconsin           0.026}
\CommentTok{\#\textgreater{} West Virginia       0.026}
\CommentTok{\#\textgreater{} Virginia            0.026}
\CommentTok{\#\textgreater{} Vermont             0.026}
\CommentTok{\#\textgreater{} Utah                0.026}
\CommentTok{\#\textgreater{} Texas               0.026}
\CommentTok{\#\textgreater{} Tennessee           0.026}
\CommentTok{\#\textgreater{} South Dakota        0.026}
\CommentTok{\#\textgreater{} South Carolina      0.026}
\CommentTok{\#\textgreater{} Rhode Island        0.026}
\CommentTok{\#\textgreater{} Pennsylvania        0.026}
\CommentTok{\#\textgreater{} Oklahoma            0.026}
\CommentTok{\#\textgreater{} Ohio                0.026}
\CommentTok{\#\textgreater{} North Dakota        0.026}
\CommentTok{\#\textgreater{} North Carolina      0.026}
\CommentTok{\#\textgreater{} New Mexico          0.026}
\CommentTok{\#\textgreater{} New Hampshire       0.026}
\CommentTok{\#\textgreater{} Nevada              0.026}
\CommentTok{\#\textgreater{} Nebraska            0.026}
\CommentTok{\#\textgreater{} Montana             0.026}
\CommentTok{\#\textgreater{} Missouri            0.026}
\CommentTok{\#\textgreater{} Mississippi         0.026}
\CommentTok{\#\textgreater{} Minnesota           0.026}
\CommentTok{\#\textgreater{} Maine               0.026}
\CommentTok{\#\textgreater{} Louisiana           0.026}
\CommentTok{\#\textgreater{} Kentucky            0.026}
\CommentTok{\#\textgreater{} Kansas              0.026}
\CommentTok{\#\textgreater{} Iowa                0.026}
\CommentTok{\#\textgreater{} Indiana             0.026}
\CommentTok{\#\textgreater{} Illinois            0.026}
\CommentTok{\#\textgreater{} Idaho               0.026}
\CommentTok{\#\textgreater{} Georgia             0.026}
\CommentTok{\#\textgreater{} Delaware            0.026}
\CommentTok{\#\textgreater{} Connecticut         0.026}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $periods}
\CommentTok{\#\textgreater{}      estimate 1}
\CommentTok{\#\textgreater{} 1988      0.053}
\CommentTok{\#\textgreater{} 1987      0.053}
\CommentTok{\#\textgreater{} 1986      0.053}
\CommentTok{\#\textgreater{} 1985      0.053}
\CommentTok{\#\textgreater{} 1984      0.053}
\CommentTok{\#\textgreater{} 1983      0.053}
\CommentTok{\#\textgreater{} 1982      0.053}
\CommentTok{\#\textgreater{} 1981      0.053}
\CommentTok{\#\textgreater{} 1980      0.053}
\CommentTok{\#\textgreater{} 1979      0.053}
\CommentTok{\#\textgreater{} 1978      0.053}
\CommentTok{\#\textgreater{} 1977      0.053}
\CommentTok{\#\textgreater{} 1976      0.053}
\CommentTok{\#\textgreater{} 1975      0.053}
\CommentTok{\#\textgreater{} 1974      0.053}
\CommentTok{\#\textgreater{} 1973      0.053}
\CommentTok{\#\textgreater{} 1972      0.053}
\CommentTok{\#\textgreater{} 1971      0.053}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $dimensions}
\CommentTok{\#\textgreater{}           N1           N0 N0.effective           T1           T0 T0.effective }
\CommentTok{\#\textgreater{}            1           38           38           12           19           19}

\NormalTok{causalverse}\SpecialCharTok{::}\FunctionTok{process\_panel\_estimate}\NormalTok{(results\_selected)}
\CommentTok{\#\textgreater{}     Method Estimate    SE}
\CommentTok{\#\textgreater{} 1 SYNTHDID   {-}15.60 10.05}
\CommentTok{\#\textgreater{} 2      DID   {-}27.35 15.81}
\CommentTok{\#\textgreater{} 3       SC   {-}19.62 11.16}
\end{Highlighting}
\end{Shaded}

\hypertarget{staggered-adoption}{%
\subsection{Staggered Adoption}\label{staggered-adoption}}

Using the second approach to apply to staggered adoption settings using the SDID estimator \citep{ben2022synthetic}.

To explore heterogeneity of treatment effect, we can do subgroup analysis \citep[p.~1092]{berman2022value}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1783}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1757}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3902}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2506}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Method}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Advantages}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Disadvantages}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Procedure}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Split Data into Subsets & Compares treated units to control units within the same subgroup. & Each subset uses a different synthetic control, making it challenging to compare effects across subgroups. & \begin{minipage}[t]{\linewidth}\raggedright
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Split the data into separate subsets for each subgroup.
\item
  Compute synthetic DID effects for each subset.
\end{enumerate}
\end{minipage} \\
Control Group Comprising All Non-adopters & Control weights match pretrends well for each treated subgroup. & Each control unit receives a different weight for each treatment subgroup, making it difficult to compare results due to varying synthetic controls. & \begin{minipage}[t]{\linewidth}\raggedright
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Use a control group consisting of all non-adopters in each balanced panel cohort analysis.
\item
  Switch treatment units to the subgroup being analyzed.
\item
  Perform \texttt{synthdid} analysis.
\end{enumerate}
\end{minipage} \\
Use All Data to Estimate Synthetic Control Weights \textbf{(recommend)} & All units have the same synthetic control. & Pretrend match may not be as accurate since it aims to match the average outcome of all treated units, not just a specific subgroup. & \begin{minipage}[t]{\linewidth}\raggedright
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Use all the data to estimate the synthetic DID control weights.
\item
  Compute treatment effects using only the treated subgroup units as the treatment units.
\end{enumerate}
\end{minipage} \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{df }\OtherTok{\textless{}{-}}\NormalTok{ fixest}\SpecialCharTok{::}\NormalTok{base\_stagg }\SpecialCharTok{|\textgreater{}}
\NormalTok{   dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{treatvar =} \FunctionTok{if\_else}\NormalTok{(time\_to\_treatment }\SpecialCharTok{\textgreater{}=} \DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
\NormalTok{   dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{treatvar =} \FunctionTok{as.integer}\NormalTok{(}\FunctionTok{if\_else}\NormalTok{(year\_treated }\SpecialCharTok{\textgreater{}}\NormalTok{ (}\DecValTok{5} \SpecialCharTok{+} \DecValTok{2}\NormalTok{), }\DecValTok{0}\NormalTok{, treatvar)))}


\NormalTok{est }\OtherTok{\textless{}{-}}\NormalTok{ causalverse}\SpecialCharTok{::}\FunctionTok{synthdid\_est\_ate}\NormalTok{(}
  \AttributeTok{data               =}\NormalTok{ df,}
  \AttributeTok{adoption\_cohorts   =} \DecValTok{5}\SpecialCharTok{:}\DecValTok{7}\NormalTok{,}
  \AttributeTok{lags               =} \DecValTok{2}\NormalTok{,}
  \AttributeTok{leads              =} \DecValTok{2}\NormalTok{,}
  \AttributeTok{time\_var           =} \StringTok{"year"}\NormalTok{,}
  \AttributeTok{unit\_id\_var        =} \StringTok{"id"}\NormalTok{,}
  \AttributeTok{treated\_period\_var =} \StringTok{"year\_treated"}\NormalTok{,}
  \AttributeTok{treat\_stat\_var     =} \StringTok{"treatvar"}\NormalTok{,}
  \AttributeTok{outcome\_var        =} \StringTok{"y"}
\NormalTok{)}
\CommentTok{\#\textgreater{} adoption\_cohort: 5 }
\CommentTok{\#\textgreater{} Treated units: 5 Control units: 65 }
\CommentTok{\#\textgreater{} adoption\_cohort: 6 }
\CommentTok{\#\textgreater{} Treated units: 5 Control units: 60 }
\CommentTok{\#\textgreater{} adoption\_cohort: 7 }
\CommentTok{\#\textgreater{} Treated units: 5 Control units: 55}

\FunctionTok{data.frame}\NormalTok{(}
    \AttributeTok{Period =} \FunctionTok{names}\NormalTok{(est}\SpecialCharTok{$}\NormalTok{TE\_mean\_w),}
    \AttributeTok{ATE    =}\NormalTok{ est}\SpecialCharTok{$}\NormalTok{TE\_mean\_w,}
    \AttributeTok{SE     =}\NormalTok{ est}\SpecialCharTok{$}\NormalTok{SE\_mean\_w}
\NormalTok{) }\SpecialCharTok{|\textgreater{}}
\NormalTok{    causalverse}\SpecialCharTok{::}\FunctionTok{nice\_tab}\NormalTok{()}
\CommentTok{\#\textgreater{}    Period   ATE   SE}
\CommentTok{\#\textgreater{} 1      {-}2 {-}0.05 0.22}
\CommentTok{\#\textgreater{} 2      {-}1  0.05 0.22}
\CommentTok{\#\textgreater{} 3       0 {-}5.07 0.80}
\CommentTok{\#\textgreater{} 4       1 {-}4.68 0.51}
\CommentTok{\#\textgreater{} 5       2 {-}3.70 0.79}
\CommentTok{\#\textgreater{} 6 cumul.0 {-}5.07 0.80}
\CommentTok{\#\textgreater{} 7 cumul.1 {-}4.87 0.55}
\CommentTok{\#\textgreater{} 8 cumul.2 {-}4.48 0.53}


\NormalTok{causalverse}\SpecialCharTok{::}\FunctionTok{synthdid\_plot\_ate}\NormalTok{(est)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{25-synthdid_files/figure-latex/unnamed-chunk-2-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{est\_sub }\OtherTok{\textless{}{-}}\NormalTok{ causalverse}\SpecialCharTok{::}\FunctionTok{synthdid\_est\_ate}\NormalTok{(}
  \AttributeTok{data               =}\NormalTok{ df,}
  \AttributeTok{adoption\_cohorts   =} \DecValTok{5}\SpecialCharTok{:}\DecValTok{7}\NormalTok{,}
  \AttributeTok{lags               =} \DecValTok{2}\NormalTok{,}
  \AttributeTok{leads              =} \DecValTok{2}\NormalTok{,}
  \AttributeTok{time\_var           =} \StringTok{"year"}\NormalTok{,}
  \AttributeTok{unit\_id\_var        =} \StringTok{"id"}\NormalTok{,}
  \AttributeTok{treated\_period\_var =} \StringTok{"year\_treated"}\NormalTok{,}
  \AttributeTok{treat\_stat\_var     =} \StringTok{"treatvar"}\NormalTok{,}
  \AttributeTok{outcome\_var        =} \StringTok{"y"}\NormalTok{,}
  \CommentTok{\# a vector of subgroup id (from unit id)}
  \AttributeTok{subgroup           =}  \FunctionTok{c}\NormalTok{(}
    \CommentTok{\# some are treated}
    \StringTok{"11"}\NormalTok{, }\StringTok{"30"}\NormalTok{, }\StringTok{"49"}\NormalTok{ ,}
    \CommentTok{\# some are control within this period}
    \StringTok{"20"}\NormalTok{, }\StringTok{"25"}\NormalTok{, }\StringTok{"21"}\NormalTok{)}
\NormalTok{)}
\CommentTok{\#\textgreater{} adoption\_cohort: 5 }
\CommentTok{\#\textgreater{} Treated units: 3 Control units: 65 }
\CommentTok{\#\textgreater{} adoption\_cohort: 6 }
\CommentTok{\#\textgreater{} Treated units: 0 Control units: 60 }
\CommentTok{\#\textgreater{} adoption\_cohort: 7 }
\CommentTok{\#\textgreater{} Treated units: 0 Control units: 55}

\FunctionTok{data.frame}\NormalTok{(}
    \AttributeTok{Period =} \FunctionTok{names}\NormalTok{(est\_sub}\SpecialCharTok{$}\NormalTok{TE\_mean\_w),}
    \AttributeTok{ATE =}\NormalTok{ est\_sub}\SpecialCharTok{$}\NormalTok{TE\_mean\_w,}
    \AttributeTok{SE =}\NormalTok{ est\_sub}\SpecialCharTok{$}\NormalTok{SE\_mean\_w}
\NormalTok{) }\SpecialCharTok{|\textgreater{}}
\NormalTok{    causalverse}\SpecialCharTok{::}\FunctionTok{nice\_tab}\NormalTok{()}
\CommentTok{\#\textgreater{}    Period   ATE   SE}
\CommentTok{\#\textgreater{} 1      {-}2  0.32 0.44}
\CommentTok{\#\textgreater{} 2      {-}1 {-}0.32 0.44}
\CommentTok{\#\textgreater{} 3       0 {-}4.29 1.68}
\CommentTok{\#\textgreater{} 4       1 {-}4.00 1.52}
\CommentTok{\#\textgreater{} 5       2 {-}3.44 2.90}
\CommentTok{\#\textgreater{} 6 cumul.0 {-}4.29 1.68}
\CommentTok{\#\textgreater{} 7 cumul.1 {-}4.14 1.52}
\CommentTok{\#\textgreater{} 8 cumul.2 {-}3.91 1.82}

\NormalTok{causalverse}\SpecialCharTok{::}\FunctionTok{synthdid\_plot\_ate}\NormalTok{(est)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{25-synthdid_files/figure-latex/synthdid subgroup analysis-1} \end{center}

Plot different estimators

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(causalverse)}
\NormalTok{methods }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"synthdid"}\NormalTok{, }\StringTok{"did"}\NormalTok{, }\StringTok{"sc"}\NormalTok{, }\StringTok{"sc\_ridge"}\NormalTok{, }\StringTok{"difp"}\NormalTok{, }\StringTok{"difp\_ridge"}\NormalTok{)}

\NormalTok{estimates }\OtherTok{\textless{}{-}} \FunctionTok{lapply}\NormalTok{(methods, }\ControlFlowTok{function}\NormalTok{(method) \{}
  \FunctionTok{synthdid\_est\_ate}\NormalTok{(}
    \AttributeTok{data               =}\NormalTok{ df,}
    \AttributeTok{adoption\_cohorts   =} \DecValTok{5}\SpecialCharTok{:}\DecValTok{7}\NormalTok{,}
    \AttributeTok{lags               =} \DecValTok{2}\NormalTok{,}
    \AttributeTok{leads              =} \DecValTok{2}\NormalTok{,}
    \AttributeTok{time\_var           =} \StringTok{"year"}\NormalTok{,}
    \AttributeTok{unit\_id\_var        =} \StringTok{"id"}\NormalTok{,}
    \AttributeTok{treated\_period\_var =} \StringTok{"year\_treated"}\NormalTok{,}
    \AttributeTok{treat\_stat\_var     =} \StringTok{"treatvar"}\NormalTok{,}
    \AttributeTok{outcome\_var        =} \StringTok{"y"}\NormalTok{,}
    \AttributeTok{method =}\NormalTok{ method}
\NormalTok{  )}
\NormalTok{\})}

\NormalTok{plots }\OtherTok{\textless{}{-}} \FunctionTok{lapply}\NormalTok{(}\FunctionTok{seq\_along}\NormalTok{(estimates), }\ControlFlowTok{function}\NormalTok{(i) \{}
\NormalTok{  causalverse}\SpecialCharTok{::}\FunctionTok{synthdid\_plot\_ate}\NormalTok{(estimates[[i]],}
                                 \AttributeTok{title =}\NormalTok{ methods[i],}
                                 \AttributeTok{theme =}\NormalTok{ causalverse}\SpecialCharTok{::}\FunctionTok{ama\_theme}\NormalTok{(}\AttributeTok{base\_size =} \DecValTok{6}\NormalTok{))}
\NormalTok{\})}

\NormalTok{gridExtra}\SpecialCharTok{::}\FunctionTok{grid.arrange}\NormalTok{(}\AttributeTok{grobs =}\NormalTok{ plots, }\AttributeTok{ncol =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{difference-in-differences}{%
\chapter{Difference-in-differences}\label{difference-in-differences}}

\href{https://github.com/lnsongxf/DiD-1}{List of packages}

Examples in marketing

\begin{itemize}
\tightlist
\item
  \citep{liaukonyte2015television}: TV ad on online shopping
\item
  \citep{akca2020value}: aggregators for airlines business effect
\item
  \citep{pattabhiramaiah2019paywalls}: paywall affects readership
\item
  \citep{wang2018border}: political ad source and message tone on vote shares and turnout using discontinuities in the level of political ads at the borders
\item
  \citep{datta2018changing}: streaming service on total music consumption using timing of users adoption of a music streaming service
\item
  \citep{janakiraman2018effect}: data breach announcement affect customer spending using timing of data breach and variation whether customer info was breached in that event
\item
  \citep{lim2020competitive}: nutritional labels on nutritional quality for other brands in a category using variation in timing of adoption of nutritional labels across categories
\item
  \citep{guo2020let}: payment disclosure laws effect on physician prescription behavior using Timing of the Massachusetts open payment law as the exogenous shock
\item
  \citep{israeli2018online}: digital monitoring and enforcement on violations using enforcement of min ad price policies
\item
  \citep{ramani2019effects}: firms respond to foreign direct investment liberalization using India's reform in 1991.
\item
  \citep{he2022market}: using Amazon policy change to examine the causal impact of fake reviews on sales, average ratings.
\item
  \citep{peukert2022regulatory}: using European General data protection Regulation, examine the impact of policy change on website usage.
\end{itemize}

Show the mechanism via

\begin{itemize}
\item
  Mediation analysis: see \citep{habel2021variable}
\item
  Moderation analysis: see \citep{goldfarb2011online}
\end{itemize}

\hypertarget{simple-dif-n-dif}{%
\section{Simple Dif-n-dif}\label{simple-dif-n-dif}}

\begin{itemize}
\item
  A tool developed intuitively to study ``natural experiment'', but its uses are much broader.
\item
  \protect\hyperlink{fixed-effects-estimator}{Fixed Effects Estimator} is the foundation for DID
\item
  Why is dif-in-dif attractive? Identification strategy: Inter-temporal variation between groups

  \begin{itemize}
  \item
    \textbf{Cross-sectional estimator} helps avoid omitted (unobserved) \textbf{common trends}
  \item
    \textbf{Time-series estimator} helps overcome omitted (unobserved) \textbf{cross-sectional differences}
  \end{itemize}
\end{itemize}

Consider

\begin{itemize}
\item
  \(D_i = 1\) treatment group
\item
  \(D_i = 0\) control group
\item
  \(T= 1\) After the treatment
\item
  \(T =0\) Before the treatment
\end{itemize}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
& After (T = 1) & Before (T = 0) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Treated \(D_i =1\) & \(E[Y_{1i}(1)|D_i = 1]\) & \(E[Y_{0i}(0)|D)i=1]\) \\
Control \(D_i = 0\) & \(E[Y_{0i}(1) |D_i =0]\) & \(E[Y_{0i}(0)|D_i=0]\) \\
\end{longtable}

missing \(E[Y_{0i}(1)|D=1]\)

\textbf{The Average Treatment Effect on Treated (ATT)}

\[
\begin{aligned}
E[Y_1(1) - Y_0(1)|D=1] &= \{E[Y(1)|D=1] - E[Y(1)|D=0] \} \\
&- \{E[Y(0)|D=1] - E[Y(0)|D=0] \}
\end{aligned}
\]

More elaboration:

\begin{itemize}
\tightlist
\item
  For the treatment group, we isolate the difference between being treated and not being treated. If the untreated group would have been affected in a different way, the DiD design and estimate would tell us nothing.
\item
  Alternatively, because we can't observe treatment variation in the control group, we can't say anything about the treatment effect on this group.
\end{itemize}

\textbf{Extension}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{More than 2 groups} (multiple treatments and multiple controls), and more than 2 period (pre and post)
\end{enumerate}

\[
Y_{igt} = \alpha_g + \gamma_t + \beta I_{gt} + \delta X_{igt} + \epsilon_{igt}
\]

where

\begin{itemize}
\item
  \(\alpha_g\) is the group-specific fixed effect
\item
  \(\gamma_t\) = time specific fixed effect
\item
  \(\beta\) = dif-in-dif effect
\item
  \(I_{gt}\) = interaction terms (n treatment indicators x n post-treatment dummies) (capture effect heterogeneity over time)
\end{itemize}

This specification is the ``two-way fixed effects DiD'' - \textbf{TWFE} (i.e., 2 sets of fixed effects: group + time).

\begin{itemize}
\tightlist
\item
  However, if you have \protect\hyperlink{staggered-dif-n-dif}{Staggered Dif-n-dif} (i.e., treatment is applied at different times to different groups). TWFE is really bad.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Long-term Effects}
\end{enumerate}

To examine the dynamic treatment effects (that are not under rollout/staggered design), we can create a centered time variable,

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.3378}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.6622}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Centered Time Variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Period
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\ldots{} & \\
\(t = -1\) & 2 periods before treatment period \\
\(t = 0\) & Last period right before treatment period

Remember to use this period as reference group \\
\(t = 1\) & Treatment period \\
\ldots{} & \\
\end{longtable}

By interacting this factor variable, we can examine the dynamic effect of treatment (i.e., whether it's fading or intensifying)

\[
\begin{aligned}
Y &= \alpha_0 + \alpha_1 Group + \alpha_2 Time  \\
&+ \beta_{-T_1} Treatment+  \beta_{-(T_1 -1)} Treatment + \dots +  \beta_{-1} Treatment \\
&+ \beta_1 + \dots + \beta_{T_2} Treatment
\end{aligned}
\]

where

\begin{itemize}
\item
  \(\beta_0\) is used as the reference group (i.e., drop from the model)
\item
  \(T_1\) is the pre-treatment period
\item
  \(T_2\) is the post-treatment period
\end{itemize}

With more variables (i.e., interaction terms), coefficients estimates can be less precise (i.e., higher SE).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  DiD on the relationship, not levels. Technically, we can apply DiD research design not only on variables, but also on coefficients estimates of some other regression models with before and after a policy is implemented.
\end{enumerate}

Goal:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Pre-treatment coefficients should be non-significant \(\beta_{-T_1}, \dots, \beta_{-1} = 0\) (similar to the \protect\hyperlink{placebo-test}{Placebo Test})
\item
  Post-treatment coefficients are expected to be significant \(\beta_1, \dots, \beta_{T_2} \neq0\)

  \begin{itemize}
  \tightlist
  \item
    You can now examine the trend in post-treatment coefficients (i.e., increasing or decreasing)
  \end{itemize}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(fixest)}

\NormalTok{od }\OtherTok{\textless{}{-}}\NormalTok{ causaldata}\SpecialCharTok{::}\NormalTok{organ\_donations }\SpecialCharTok{\%\textgreater{}\%}
    
    \CommentTok{\# Treatment variable}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{California =}\NormalTok{ State }\SpecialCharTok{==} \StringTok{\textquotesingle{}California\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \CommentTok{\# centered time variable}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{center\_time =} \FunctionTok{as.factor}\NormalTok{(Quarter\_Num }\SpecialCharTok{{-}} \DecValTok{3}\NormalTok{))  }
\CommentTok{\# where 3 is the reference period precedes the treatment period}

\FunctionTok{class}\NormalTok{(od}\SpecialCharTok{$}\NormalTok{California)}
\CommentTok{\#\textgreater{} [1] "logical"}
\FunctionTok{class}\NormalTok{(od}\SpecialCharTok{$}\NormalTok{State)}
\CommentTok{\#\textgreater{} [1] "character"}

\NormalTok{cali }\OtherTok{\textless{}{-}} \FunctionTok{feols}\NormalTok{(Rate }\SpecialCharTok{\textasciitilde{}} \FunctionTok{i}\NormalTok{(center\_time, California, }\AttributeTok{ref =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{|}
\NormalTok{                  State }\SpecialCharTok{+}\NormalTok{ center\_time,}
              \AttributeTok{data =}\NormalTok{ od)}

\FunctionTok{etable}\NormalTok{(cali)}
\CommentTok{\#\textgreater{}                                              cali}
\CommentTok{\#\textgreater{} Dependent Var.:                              Rate}
\CommentTok{\#\textgreater{}                                                  }
\CommentTok{\#\textgreater{} California x center\_time = {-}2    {-}0.0029 (0.0051)}
\CommentTok{\#\textgreater{} California x center\_time = {-}1   0.0063** (0.0023)}
\CommentTok{\#\textgreater{} California x center\_time = 1  {-}0.0216*** (0.0050)}
\CommentTok{\#\textgreater{} California x center\_time = 2  {-}0.0203*** (0.0045)}
\CommentTok{\#\textgreater{} California x center\_time = 3    {-}0.0222* (0.0100)}
\CommentTok{\#\textgreater{} Fixed{-}Effects:                {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} State                                         Yes}
\CommentTok{\#\textgreater{} center\_time                                   Yes}
\CommentTok{\#\textgreater{} \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}
\CommentTok{\#\textgreater{} S.E.: Clustered                         by: State}
\CommentTok{\#\textgreater{} Observations                                  162}
\CommentTok{\#\textgreater{} R2                                        0.97934}
\CommentTok{\#\textgreater{} Within R2                                 0.00979}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes: 0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}

\FunctionTok{iplot}\NormalTok{(cali, }\AttributeTok{pt.join =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-1-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{coefplot}\NormalTok{(cali)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-1-2} \end{center}

Notes:

\begin{itemize}
\item
  \protect\hyperlink{matching-methods}{Matching Methods}

  \begin{itemize}
  \item
    Match treatment and control based on pre-treatment observables
  \item
    Modify SEs appropriately \citep{heckman1997matching}. It's might be easier to just use the \protect\hyperlink{doubly-robust-did}{Doubly Robust DiD} \citep{sant2020doubly} where you just need either matching or regression to work in order to identify your treatment effect
  \item
    Whereas the group fixed effects control for the group time-invariant effects, it does not control for selection bias (i.e., certain groups are more likely to be treated than others). Hence, with these backdoor open (i.e., selection bias) between (1) propensity to be treated and (2) dynamics evolution of the outcome post-treatment, matching can potential close these backdoor.
  \item
    Be careful when matching time-varying covariates because you might encounter ``regression to the mean'' problem, where pre-treatment periods can have an unusually bad or good time (that is out of the ordinary), then the post-treatment period outcome can just be an artifact of the regression to the mean \citep{daw2018matching}. This problem is not of concern to time-invariant variables.
  \item
    Matching and DiD can use pre-treatment outcomes to correct for selection bias. From real world data and simulation, \citep{chabe2015analysis} found that matching generally underestimates the average causal effect and gets closer to the true effect with more number of pre-treatment outcomes. When selection bias is symmetric around the treatment date, DID is still consistent when implemented symmetrically (i.e., the same number of period before and after treatment). In cases where selection bias is asymmetric, the MC simulations show that Symmetric DiD still performs better than Matching.
  \end{itemize}
\item
  It's always good to show results with and without controls because

  \begin{itemize}
  \item
    If the controls are fixed within group or within time, then those should be absorbed under those fixed effects
  \item
    If the controls are dynamic across group and across, then your parallel trends assumption is not plausible.
  \end{itemize}
\item
  SEs are typically clustered within groups, but this approach can make our SEs too small, that leads to overconfidence in our estimates. Hence, \citet{bertrand2004much} suggest either

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \item
    aggregating data to just one pre-treatment and one post-treatment period per group
  \item
    using cluster bootstrapped SEs.
  \end{enumerate}
\end{itemize}

\hypertarget{examples}{%
\subsection{Examples}\label{examples}}

\hypertarget{example-by-doleac2020unintended}{%
\subsubsection{\texorpdfstring{Example by \citet{doleac2020unintended}}{Example by @doleac2020unintended}}\label{example-by-doleac2020unintended}}

\begin{itemize}
\item
  The purpose of banning a checking box for ex-criminal was banned because we thought that it gives more access to felons
\item
  Even if we ban the box, employers wouldn't just change their behaviors. But then the unintended consequence is that employers statistically discriminate based on race
\end{itemize}

3 types of ban the box

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Public employer only
\item
  Private employer with government contract
\item
  All employers
\end{enumerate}

Main identification strategy

\begin{itemize}
\tightlist
\item
  If any county in the Metropolitan Statistical Area (MSA) adopts ban the box, it means the whole MSA is treated. Or if the state adopts ``ban the ban,'' every county is treated
\end{itemize}

Under \protect\hyperlink{simple-dif-n-dif}{Simple Dif-n-dif}

\[ Y_{it} = \beta_0 + \beta_1 Post_t + \beta_2 treat_i + \beta_2 (Post_t \times Treat_i) + \epsilon_{it} \]

But if there is no common post time, then we should use \protect\hyperlink{staggered-dif-n-dif}{Staggered Dif-n-dif}

\[ \begin{aligned} E_{imrt} &= \alpha + \beta_1 BTB_{imt} W_{imt} + \beta_2 BTB_{mt} + \beta_3 BTB_{mt} H_{imt}\\  &+ \delta_m + D_{imt} \beta_5 + \lambda_{rt} + \delta_m\times f(t) \beta_7 + e_{imrt} \end{aligned} \]

where

\begin{itemize}
\item
  \(i\) = person; \(m\) = MSA; \(r\) = region (US regions e.g., Midwest) ; \(r\) = region; \(t\) = year
\item
  \(W\) = White; \(B\) = Black; \(H\) = Hispanic
\item
  \(\beta_1 BTB_{imt} W_{imt} + \beta_2 BTB_{mt} + \beta_3 BTB_{mt} H_{imt}\) are the 3 dif-n-dif variables (\(BTB\) = ``ban the box'')
\item
  \(\delta_m\) = dummy for MSI
\item
  \(D_{imt}\) = control for people
\item
  \(\lambda_{rt}\) = region by time fixed effect
\item
  \(\delta_m \times f(t)\) = linear time trend within MSA (but we should not need this if we have good pre-trend)
\end{itemize}

If we put \(\lambda_r - \lambda_t\) (separately) we will more broad fixed effect, while \(\lambda_{rt}\) will give us deeper and narrower fixed effect.

Before running this model, we have to drop all other races. And \(\beta_1, \beta_2, \beta_3\) are not collinear because there are all interaction terms with \(BTB_{mt}\)

If we just want to estimate the model for black men, we will modify it to be

\[ E_{imrt} = \alpha + BTB_{mt} \beta_1 + \delta_m + D_{imt} \beta_5 + \lambda_{rt} + (\delta_m \times f(t)) \beta_7 + e_{imrt} \]

\[ \begin{aligned} E_{imrt} &= \alpha + BTB_{m (t - 3t)} \theta_1 + BTB_{m(t-2)} \theta_2 + BTB_{mt} \theta_4 \\ &+ BTB_{m(t+1)}\theta_5 + BTB_{m(t+2)}\theta_6 + BTB_{m(t+3t)}\theta_7 \\ &+ [\delta_m + D_{imt}\beta_5 + \lambda_r + (\delta_m \times (f(t))\beta_7 + e_{imrt}] \end{aligned} \]

We have to leave \(BTB_{m(t-1)}\theta_3\) out for the category would not be perfect collinearity

So the year before BTB (\(\theta_1, \theta_2, \theta_3\)) should be similar to each other (i.e., same pre-trend). Remember, we only run for places with BTB.

If \(\theta_2\) is statistically different from \(\theta_3\) (baseline), then there could be a problem, but it could also make sense if we have pre-trend announcement.

Example by \href{https://rpubs.com/phle/r_tutorial_difference_in_differences}{Philipp Leppert} replicating \href{https://davidcard.berkeley.edu/data_sets.html}{Card and Krueger (1994)}

Example by \href{https://bookdown.org/aschmi11/causal_inf/difference-in-differences.html}{Anthony Schmidt}

\hypertarget{example-from-princeton}{%
\subsubsection{\texorpdfstring{Example from \href{https://www.princeton.edu/~otorres/DID101R.pdf}{Princeton}}{Example from Princeton}}\label{example-from-princeton}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(foreign)}
\NormalTok{mydata }\OtherTok{=} \FunctionTok{read.dta}\NormalTok{(}\StringTok{"http://dss.princeton.edu/training/Panel101.dta"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \CommentTok{\# create a dummy variable to indicate the time when the treatment started}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{time =} \FunctionTok{ifelse}\NormalTok{(year }\SpecialCharTok{\textgreater{}=} \DecValTok{1994}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
    \CommentTok{\# create a dummy variable to identify the treatment group}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{treated =} \FunctionTok{ifelse}\NormalTok{(country }\SpecialCharTok{==} \StringTok{"E"} \SpecialCharTok{|}
\NormalTok{                                country }\SpecialCharTok{==} \StringTok{"F"} \SpecialCharTok{|}\NormalTok{ country }\SpecialCharTok{==} \StringTok{"G"}\NormalTok{ ,}
                            \DecValTok{1}\NormalTok{,}
                            \DecValTok{0}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
    \CommentTok{\# create an interaction between time and treated}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{did =}\NormalTok{ time }\SpecialCharTok{*}\NormalTok{ treated)}
\end{Highlighting}
\end{Shaded}

estimate the DID estimator

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{didreg }\OtherTok{=} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ treated }\SpecialCharTok{+}\NormalTok{ time }\SpecialCharTok{+}\NormalTok{ did, }\AttributeTok{data =}\NormalTok{ mydata)}
\FunctionTok{summary}\NormalTok{(didreg)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = y \textasciitilde{} treated + time + did, data = mydata)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}        Min         1Q     Median         3Q        Max }
\CommentTok{\#\textgreater{} {-}9.768e+09 {-}1.623e+09  1.167e+08  1.393e+09  6.807e+09 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}               Estimate Std. Error t value Pr(\textgreater{}|t|)  }
\CommentTok{\#\textgreater{} (Intercept)  3.581e+08  7.382e+08   0.485   0.6292  }
\CommentTok{\#\textgreater{} treated      1.776e+09  1.128e+09   1.575   0.1200  }
\CommentTok{\#\textgreater{} time         2.289e+09  9.530e+08   2.402   0.0191 *}
\CommentTok{\#\textgreater{} did         {-}2.520e+09  1.456e+09  {-}1.731   0.0882 .}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 2.953e+09 on 66 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.08273,    Adjusted R{-}squared:  0.04104 }
\CommentTok{\#\textgreater{} F{-}statistic: 1.984 on 3 and 66 DF,  p{-}value: 0.1249}
\end{Highlighting}
\end{Shaded}

The \texttt{did} coefficient is the differences-in-differences estimator. Treat has a negative effect

\hypertarget{example-by-card1993minimum}{%
\subsubsection{\texorpdfstring{Example by \citet{card1993minimum}}{Example by @card1993minimum}}\label{example-by-card1993minimum}}

found that increase in minimum wage increases employment

Experimental Setting:

\begin{itemize}
\item
  New Jersey (treatment) increased minimum wage
\item
  Penn (control) did not increase minimum wage
\end{itemize}

\begin{longtable}[]{@{}lllll@{}}
\toprule\noalign{}
& & After & Before & \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Treatment & NJ & A & B & A - B \\
Control & PA & C & D & C - D \\
& & A - C & B - D & (A - B) - (C - D) \\
\end{longtable}

where

\begin{itemize}
\item
  A - B = treatment effect + effect of time (additive)
\item
  C - D = effect of time
\item
  (A - B) - (C - D) = dif-n-dif
\end{itemize}

\textbf{The identifying assumptions}:

\begin{itemize}
\item
  Can't have \textbf{switchers}
\item
  PA is the control group

  \begin{itemize}
  \item
    is a good counter factual
  \item
    is what NJ would look like if they hadn't had the treatment
  \end{itemize}
\end{itemize}

\[
Y_{jt} = \beta_0 + NJ_j \beta_1 + POST_t \beta_2 + (NJ_j \times POST_t)\beta_3+ X_{jt}\beta_4 + \epsilon_{jt}
\]

where

\begin{itemize}
\item
  \(j\) = restaurant
\item
  \(NJ\) = dummy where 1 = NJ, and 0 = PA
\item
  \(POST\) = dummy where 1 = post, and 0 = pre
\end{itemize}

Notes:

\begin{itemize}
\item
  We don't need \(\beta_4\) in our model to have unbiased \(\beta_3\), but including it would give our coefficients efficiency
\item
  If we use \(\Delta Y_{jt}\) as the dependent variable, we don't need \(POST_t \beta_2\) anymore
\item
  Alternative model specification is that the authors use NJ high wage restaurant as control group (still choose those that are close to the border)
\item
  The reason why they can't control for everything (PA + NJ high wage) is because it's hard to interpret the causal treatment
\item
  Dif-n-dif utilizes similarity in pretrend of the dependent variables. However, this is neither a necessary nor sufficient for the identifying assumption.

  \begin{itemize}
  \item
    It's not sufficient because they can have multiple treatments (technically, you could include more control, but your treatment can't interact)
  \item
    It's not necessary because trends can be parallel after treatment
  \end{itemize}
\item
  However, we can't never be certain; we just try to find evidence consistent with our theory so that dif-n-dif can work.
\item
  Notice that we don't need before treatment the \textbf{levels of the dependent variable} to be the same (e.g., same wage average in both NJ and PA), dif-n-dif only needs \textbf{pre-trend (i.e., slope)} to be the same for the two groups.
\end{itemize}

\hypertarget{example-by-butcher2014effects}{%
\subsubsection{\texorpdfstring{Example by \citet{butcher2014effects}}{Example by @butcher2014effects}}\label{example-by-butcher2014effects}}

Theory:

\begin{itemize}
\item
  Highest achieving students are usually in hard science. Why?

  \begin{itemize}
  \item
    Hard to give students students the benefit of doubt for hard science
  \item
    How unpleasant and how easy to get a job. Degrees with lower market value typically want to make you feel more pleasant
  \end{itemize}
\end{itemize}

Under OLS

\[
E_{ij} = \beta_0 + X_i \beta_1 + G_j \beta_2 + \epsilon_{ij}
\]

where

\begin{itemize}
\item
  \(X_i\) = student attributes
\item
  \(\beta_2\) = causal estimate (from grade change)
\item
  \(E_{ij}\) = Did you choose to enroll in major \(j\)
\item
  \(G_j\) = grade given in major \(j\)
\end{itemize}

Examine \(\hat{\beta}_2\)

\begin{itemize}
\item
  Negative bias: Endogenous response because department with lower enrollment rate will give better grade
\item
  Positive bias: hard science is already having best students (i.e., ability), so if they don't their grades can be even lower
\end{itemize}

Under dif-n-dif

\[
Y_{idt} = \beta_0 + POST_t \beta_1 + Treat_d \beta_2 + (POST_t \times Treat_d)\beta_3 + X_{idt} + \epsilon_{idt}
\]

where

\begin{itemize}
\tightlist
\item
  \(Y_{idt}\) = grade average
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1867}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.4667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.0933}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.0800}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1733}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Intercept
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Treat
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Post
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Treat*Post
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Treat Pre & 1 & 1 & 0 & 0 \\
Treat Post & 1 & 1 & 1 & 1 \\
Control Pre & 1 & 0 & 0 & 0 \\
Control Post & 1 & 0 & 1 & 0 \\
& Average for pre-control \(\beta_0\) & & & \\
\end{longtable}

A more general specification of the dif-n-dif is that

\[
Y_{idt} = \alpha_0 + (POST_t \times Treat_d) \alpha_1 + \theta_d + \delta_t + X_{idt} + u_{idt}
\]

where

\begin{itemize}
\item
  \((\theta_d + \delta_t)\) richer , more df than \(Treat_d \beta_2 + Post_t \beta_1\) (because fixed effects subsume Post and treat)
\item
  \(\alpha_1\) should be equivalent to \(\beta_3\) (if your model assumptions are correct)
\end{itemize}

Under causal inference, \(R^2\) is not so important.

\hypertarget{doubly-robust-did}{%
\subsection{Doubly Robust DiD}\label{doubly-robust-did}}

Also known as the locally efficient doubly robust DiD \citep{sant2020doubly}

\href{https://psantanna.com/DRDID/index.html}{Code example by the authors}

The package (not method) is rather limited application:

\begin{itemize}
\item
  Use OLS (cannot handle \texttt{glm})
\item
  Canonical DiD only (cannot handle DDD).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(DRDID)}
\FunctionTok{data}\NormalTok{(}\StringTok{"nsw\_long"}\NormalTok{)}
\NormalTok{eval\_lalonde\_cps }\OtherTok{\textless{}{-}}
    \FunctionTok{subset}\NormalTok{(nsw\_long, nsw\_long}\SpecialCharTok{$}\NormalTok{treated }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{|}\NormalTok{ nsw\_long}\SpecialCharTok{$}\NormalTok{sample }\SpecialCharTok{==} \DecValTok{2}\NormalTok{)}
\FunctionTok{head}\NormalTok{(eval\_lalonde\_cps)}
\CommentTok{\#\textgreater{}   id year treated age educ black married nodegree dwincl      re74 hisp}
\CommentTok{\#\textgreater{} 1  1 1975      NA  42   16     0       1        0     NA     0.000    0}
\CommentTok{\#\textgreater{} 2  1 1978      NA  42   16     0       1        0     NA     0.000    0}
\CommentTok{\#\textgreater{} 3  2 1975      NA  20   13     0       0        0     NA  2366.794    0}
\CommentTok{\#\textgreater{} 4  2 1978      NA  20   13     0       0        0     NA  2366.794    0}
\CommentTok{\#\textgreater{} 5  3 1975      NA  37   12     0       1        0     NA 25862.322    0}
\CommentTok{\#\textgreater{} 6  3 1978      NA  37   12     0       1        0     NA 25862.322    0}
\CommentTok{\#\textgreater{}   early\_ra sample experimental         re}
\CommentTok{\#\textgreater{} 1       NA      2            0     0.0000}
\CommentTok{\#\textgreater{} 2       NA      2            0   100.4854}
\CommentTok{\#\textgreater{} 3       NA      2            0  3317.4678}
\CommentTok{\#\textgreater{} 4       NA      2            0  4793.7451}
\CommentTok{\#\textgreater{} 5       NA      2            0 22781.8555}
\CommentTok{\#\textgreater{} 6       NA      2            0 25564.6699}


\CommentTok{\# locally efficient doubly robust DiD Estimators for the ATT}
\NormalTok{out }\OtherTok{\textless{}{-}}
    \FunctionTok{drdid}\NormalTok{(}
        \AttributeTok{yname =} \StringTok{"re"}\NormalTok{,}
        \AttributeTok{tname =} \StringTok{"year"}\NormalTok{,}
        \AttributeTok{idname =} \StringTok{"id"}\NormalTok{,}
        \AttributeTok{dname =} \StringTok{"experimental"}\NormalTok{,}
        \AttributeTok{xformla =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ educ }\SpecialCharTok{+}\NormalTok{ black }\SpecialCharTok{+}\NormalTok{ married }\SpecialCharTok{+}\NormalTok{ nodegree }\SpecialCharTok{+}\NormalTok{ hisp }\SpecialCharTok{+}\NormalTok{ re74,}
        \AttributeTok{data =}\NormalTok{ eval\_lalonde\_cps,}
        \AttributeTok{panel =} \ConstantTok{TRUE}
\NormalTok{    )}
\FunctionTok{summary}\NormalTok{(out)}
\CommentTok{\#\textgreater{}  Call:}
\CommentTok{\#\textgreater{} drdid(yname = "re", tname = "year", idname = "id", dname = "experimental", }
\CommentTok{\#\textgreater{}     xformla = \textasciitilde{}age + educ + black + married + nodegree + hisp + }
\CommentTok{\#\textgreater{}         re74, data = eval\_lalonde\_cps, panel = TRUE)}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}  Further improved locally efficient DR DID estimator for the ATT:}
\CommentTok{\#\textgreater{}  }
\CommentTok{\#\textgreater{}    ATT     Std. Error  t value    Pr(\textgreater{}|t|)  [95\% Conf. Interval] }
\CommentTok{\#\textgreater{} {-}901.2703   393.6247   {-}2.2897     0.022    {-}1672.7747  {-}129.766 }
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}  Estimator based on panel data.}
\CommentTok{\#\textgreater{}  Outcome regression est. method: weighted least squares.}
\CommentTok{\#\textgreater{}  Propensity score est. method: inverse prob. tilting.}
\CommentTok{\#\textgreater{}  Analytical standard error.}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}  See Sant\textquotesingle{}Anna and Zhao (2020) for details.}



\CommentTok{\# Improved locally efficient doubly robust DiD estimator }
\CommentTok{\# for the ATT, with panel data}
\CommentTok{\# drdid\_imp\_panel()}

\CommentTok{\# Locally efficient doubly robust DiD estimator for the ATT, }
\CommentTok{\# with panel data}
\CommentTok{\# drdid\_panel()}

\CommentTok{\# Locally efficient doubly robust DiD estimator for the ATT, }
\CommentTok{\# with repeated cross{-}section data}
\CommentTok{\# drdid\_rc()}

\CommentTok{\# Improved locally efficient doubly robust DiD estimator for the ATT, }
\CommentTok{\# with repeated cross{-}section data}
\CommentTok{\# drdid\_imp\_rc()}
\end{Highlighting}
\end{Shaded}

\hypertarget{one-difference}{%
\section{One Difference}\label{one-difference}}

The regression formula is as follows \citep{liaukonyte2023frontiers}:

\[
y_{ut} = \beta \text{Post}_t + \gamma_u + \gamma_w(t) + \gamma_l + \gamma_g(u)p(t) + \epsilon_{ut}
\]

where

\begin{itemize}
\tightlist
\item
  \(y_{ut}\): Outcome of interest for unit u in time t.
\item
  \(\text{Post}_t\): Dummy variable representing a specific post-event period.
\item
  \(\beta\): Coefficient measuring the average change in the outcome after the event relative to the pre-period.
\item
  \(\gamma_u\): Fixed effects for each unit.
\item
  \(\gamma_w(t)\): Time-specific fixed effects to account for periodic variations.
\item
  \(\gamma_l\): Dummy variable for a specific significant period (e.g., a major event change).
\item
  \(\gamma_g(u)p(t)\): Group x period fixed effects for flexible trends that may vary across different categories (e.g., geographical regions) and periods.
\item
  \(\epsilon_{ut}\): Error term.
\end{itemize}

This model can be used to analyze the impact of an event on the outcome of interest while controlling for various fixed effects and time-specific variations, but using units themselves pre-treatment as controls.

\hypertarget{two-way-fixed-effects}{%
\section{Two-way Fixed-effects}\label{two-way-fixed-effects}}

A generalization of the dif-n-dif model is the two-way fixed-effects models where you have multiple groups and time effects. But this is not a designed-based, non-parametric causal estimator \citep{imai2021use}

When applying TWFE to multiple groups and multiple periods, the supposedly causal coefficient is the weighted average of all two-group/two-period DiD estimators in the data where some of the weights can be negative. More specifically, the weights are proportional to group sizes and treatment indicator's variation in each pair, where units in the middle of the panel have the highest weight.

The canonical/standard TWFE only works when

\begin{itemize}
\item
  Effects are homogeneous across units and across time periods (i.e., no dynamic changes in the effects of treatment). See \citep{goodman2021difference, de2020two, sun2021estimating, borusyak2021revisiting} for details. Similarly, it relies on the assumption of \textbf{linear additive effects} \citep{imai2021use}

  \begin{itemize}
  \item
    Have to argue why treatment heterogeneity is not a problem (e.g., plot treatment timing and decompose treatment coefficient using \protect\hyperlink{goodman-bacon-decomposition}{Goodman-Bacon Decomposition}) know the percentage of observation are never treated (because as the never-treated group increases, the bias of TWFE decreases, with 80\% sample to be never-treated, bias is negligible). The problem is worsen when you have long-run effects.
  \item
    Need to manually drop two relative time periods if everyone is eventually treated (to avoid multicollinearity). Programs might do this randomly and if it chooses to drop a post-treatment period, it will create biases. The choice usually -1, and -2 periods.
  \item
    Treatment heterogeneity can come in because (1) it might take some time for a treatment to have measurable changes in outcomes or (2) for each period after treatment, the effect can be different (phase in or increasing effects).
  \end{itemize}
\item
  2 time periods.
\end{itemize}

Within this setting, TWFE works because, using the baseline (e.g., control units where their treatment status is unchanged across time periods), the comparison can be

\begin{itemize}
\item
  Good for

  \begin{itemize}
  \item
    Newly treated units vs.~control
  \item
    Newly treated units vs not-yet treated
  \end{itemize}
\item
  Bad for

  \begin{itemize}
  \tightlist
  \item
    Newly treated vs.~already treated (because already treated cannot serve as the potential outcome for the newly treated).
  \end{itemize}
\end{itemize}

Note: Notation for this section is consistent with \citep{arkhangelsky2021double}

\[
Y_{it} = \alpha_i + \lambda_t + \tau W_{it} + \beta X_{it} + \epsilon_{it}
\]

where

\begin{itemize}
\item
  \(Y_{it}\) is the outcome
\item
  \(\alpha_i\) is the unit FE
\item
  \(\lambda_t\) is the time FE
\item
  \(\tau\) is the causal effect of treatment
\item
  \(W_{it}\) is the treatment indicator
\item
  \(X_{it}\) are covariates
\end{itemize}

When \(T = 2\), the TWFE is the traditional DiD model

Under the following assumption, \(\hat{\tau}_{OLS}\) is unbiased:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  homogeneous treatment effect
\item
  parallel trends assumptions
\item
  linear additive effects \citep{imai2021use}
\end{enumerate}

\textbf{Remedies for TWFE's shortcomings}

\begin{itemize}
\item
  \citep{goodman2021difference}: diagnostic robustness tests of the TWFE DiD and identify influential observations to the DiD estimate (\protect\hyperlink{goodman-bacon-decomposition}{Goodman-Bacon Decomposition})
\item
  \citep{callaway2021difference}: 2-step estimation with a bootstrap procedure that can account for autocorrelation and clustering,

  \begin{itemize}
  \item
    the parameters of interest are the group-time average treatment effects, where each group is defined by when it was first treated (\protect\hyperlink{multiple-periods-and-variation-in-treatment-timing}{Multiple periods and variation in treatment timing})
  \item
    Comparing post-treatment outcomes fo groups treated in a period against a similar group that is never treated (using matching).
  \item
    Treatment status cannot switch (once treated, stay treated for the rest of the panel)
  \item
    Package: \texttt{did}
  \end{itemize}
\item
  \citep{sun2021estimating}: a specialization of \citep{callaway2021difference} in the event-study context.

  \begin{itemize}
  \item
    They include lags and leads in their design
  \item
    have cohort-specific estimates (similar to group-time estimates in \citep{callaway2021difference}
  \item
    They propose the ``interaction-weighted'' estimator.
  \item
    Package: \texttt{fixest}
  \end{itemize}
\item
  \citep{imai2021use}

  \begin{itemize}
  \item
    Different from \citep{callaway2021difference} because they allow units to switch in and out of treatment.
  \item
    Based on matching methods, to have weighted TWFE
  \item
    Package: \texttt{wfe} and \texttt{PanelMatch}
  \end{itemize}
\item
  \citep{gardner2022two}: two-stage DiD

  \begin{itemize}
  \tightlist
  \item
    \texttt{did2s}
  \end{itemize}
\item
  \citep{arkhangelsky2021double}: see below
\end{itemize}

To be robust against

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  time- and unit-varying effects
\end{enumerate}

We can use the reshaped inverse probability weighting (RIPW)- TWFE estimator

With the following assumptions:

\begin{itemize}
\item
  SUTVA
\item
  Binary treatment: \(\mathbf{W}_i = (W_{i1}, \dots, W_{it})\) where \(\mathbf{W}_i \sim \mathbf{\pi}_i\) generalized propensity score (i.e., each person treatment likelihood follow \(\pi\) regardless of the period)
\end{itemize}

Then, the unit-time specific effect is \(\tau_{it} = Y_{it}(1) - Y_{it}(0)\)

Then the Doubly Average Treatment Effect (DATE) is

\[
\tau(\xi) = \sum_{T=1}^T \xi_t \left(\frac{1}{n} \sum_{i = 1}^n \tau_{it} \right)
\]

where

\begin{itemize}
\item
  \(\frac{1}{n} \sum_{i = 1}^n \tau_{it}\) is the unweighted effect of treatment across units (i.e., time-specific ATE).
\item
  \(\xi = (\xi_1, \dots, \xi_t)\) are user-specific weights for each time period.
\item
  This estimand is called DATE because it's weighted (averaged) across both time and units.
\end{itemize}

A special case of DATE is when both time and unit-weights are equal

\[
\tau_{eq} = \frac{1}{nT} \sum_{t=1}^T \sum_{i = 1}^n \tau_{it} 
\]

Borrowing the idea of inverse propensity-weighted least squares estimator in the cross-sectional case that we reweight the objective function via the treatment assignment mechanism:

\[
\hat{\tau} \triangleq \arg \min_{\tau} \sum_{i = 1}^n (Y_i -\mu - W_i \tau)^2 \frac{1}{\pi_i (W_i)}
\]

where

\begin{itemize}
\item
  the first term is the least squares objective
\item
  the second term is the propensity score
\end{itemize}

In the panel data case, the IPW estimator will be

\[
\hat{\tau}_{IPW} \triangleq \arg \min_{\tau} \sum_{i = 1}^n \sum_{t =1}^T (Y_{i t}-\alpha_i - \lambda_t - W_{it} \tau)^2 \frac{1}{\pi_i (W_i)}
\]

Then, to have DATE that users can specify the structure of time weight, we use reshaped IPW estimator \citep{arkhangelsky2021double}

\[
\hat{\tau}_{RIPW} (\Pi) \triangleq \arg \min_{\tau} \sum_{i = 1}^n \sum_{t =1}^T (Y_{i t}-\alpha_i - \lambda_t - W_{it} \tau)^2 \frac{\Pi(W_i)}{\pi_i (W_i)}
\]

where it's a function of a data-independent distribution \(\Pi\) that depends on the support of the treatment path \(\mathbb{S} = \cup_i Supp(W_i)\)

This generalization can transform to

\begin{itemize}
\item
  IPW-TWFE estimator when \(\Pi \sim Unif(\mathbb{S})\)
\item
  randomized experiment when \(\Pi = \pi_i\)
\end{itemize}

To choose \(\Pi\), we don't need to data, we just need possible assignments in your setting.

\begin{itemize}
\item
  For most practical problems (DiD, staggered, transient), we have closed form solutions
\item
  For generic solver, we can use nonlinear programming (e..g, BFGS algorithm)
\end{itemize}

As argued in \citep{imai2021use} that TWFE is not a non-parametric approach, it can be subjected to incorrect model assumption (i.e., model dependence).

\begin{itemize}
\item
  Hence, they advocate for matching methods for time-series cross-sectional data \citep{imai2021use}
\item
  Use \texttt{wfe} and \texttt{PanelMatch} to apply their paper.
\end{itemize}

This package is based on \citep{somaini2016algorithm}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# dataset}
\FunctionTok{library}\NormalTok{(bacondecomp)}
\NormalTok{df }\OtherTok{\textless{}{-}}\NormalTok{ bacondecomp}\SpecialCharTok{::}\NormalTok{castle}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# devtools::install\_github("paulosomaini/xtreg2way")}

\FunctionTok{library}\NormalTok{(xtreg2way)}
\CommentTok{\# output \textless{}{-} xtreg2way(y,}
\CommentTok{\#                     data.frame(x1, x2),}
\CommentTok{\#                     iid,}
\CommentTok{\#                     tid,}
\CommentTok{\#                     w,}
\CommentTok{\#                     noise = "1",}
\CommentTok{\#                     se = "1")}

\CommentTok{\# equilvalently}
\NormalTok{output }\OtherTok{\textless{}{-}} \FunctionTok{xtreg2way}\NormalTok{(l\_homicide }\SpecialCharTok{\textasciitilde{}}\NormalTok{ post,}
\NormalTok{                    df,}
                    \AttributeTok{iid =}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{state, }\CommentTok{\# group id}
                    \AttributeTok{tid =}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{year, }\CommentTok{\# time id}
                    \CommentTok{\# w, \# vector of weight}
                    \AttributeTok{se =} \StringTok{"1"}\NormalTok{)}
\NormalTok{output}\SpecialCharTok{$}\NormalTok{betaHat}
\CommentTok{\#\textgreater{}                  [,1]}
\CommentTok{\#\textgreater{} l\_homicide 0.08181162}
\NormalTok{output}\SpecialCharTok{$}\NormalTok{aVarHat}
\CommentTok{\#\textgreater{}             [,1]}
\CommentTok{\#\textgreater{} [1,] 0.003396724}

\CommentTok{\# to save time, you can use your structure in the }
\CommentTok{\# last output for a new set of variables}
\CommentTok{\# output2 \textless{}{-} xtreg2way(y, x1, struc=output$struc)}
\end{Highlighting}
\end{Shaded}

Standard errors estimation options

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.1913}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.8087}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Set
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Estimation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{se\ =\ "0"} & Assume homoskedasticity and no within group correlation or serial correlation \\
\texttt{se\ =\ "1"} (default) & robust to heteroskadasticity and serial correlation \citep{arellano1987computing} \\
\texttt{se\ =\ "2"} & robust to heteroskedasticity, but assumes no correlation within group or serial correlation \\
\texttt{se\ =\ "11"} & Aerllano SE with df correction performed by Stata xtreg \citep{somaini2021twfem} \\
\end{longtable}

Alternatively, you can also do it manually or with the \texttt{plm} package, but you have to be careful with how the SEs are estimated

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(multiwayvcov) }\CommentTok{\# get vcov matrix }
\FunctionTok{library}\NormalTok{(lmtest) }\CommentTok{\# robust SEs estimation}

\CommentTok{\# manual}
\NormalTok{output3 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(l\_homicide }\SpecialCharTok{\textasciitilde{}}\NormalTok{ post }\SpecialCharTok{+} \FunctionTok{factor}\NormalTok{(state) }\SpecialCharTok{+} \FunctionTok{factor}\NormalTok{(year),}
              \AttributeTok{data =}\NormalTok{ df)}

\CommentTok{\# get variance{-}covariance matrix}
\NormalTok{vcov\_tw }\OtherTok{\textless{}{-}}\NormalTok{ multiwayvcov}\SpecialCharTok{::}\FunctionTok{cluster.vcov}\NormalTok{(output3,}
                        \FunctionTok{cbind}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{state, df}\SpecialCharTok{$}\NormalTok{year),}
                        \AttributeTok{use\_white =}\NormalTok{ F,}
                        \AttributeTok{df\_correction =}\NormalTok{ F)}

\CommentTok{\# get coefficients}
\FunctionTok{coeftest}\NormalTok{(output3, vcov\_tw)[}\DecValTok{2}\NormalTok{,] }
\CommentTok{\#\textgreater{}   Estimate Std. Error    t value   Pr(\textgreater{}|t|) }
\CommentTok{\#\textgreater{} 0.08181162 0.05671410 1.44252696 0.14979397}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# using the plm package}
\FunctionTok{library}\NormalTok{(plm)}

\NormalTok{output4 }\OtherTok{\textless{}{-}} \FunctionTok{plm}\NormalTok{(l\_homicide }\SpecialCharTok{\textasciitilde{}}\NormalTok{ post, }
               \AttributeTok{data =}\NormalTok{ df, }
               \AttributeTok{index =} \FunctionTok{c}\NormalTok{(}\StringTok{"state"}\NormalTok{, }\StringTok{"year"}\NormalTok{), }
               \AttributeTok{model =} \StringTok{"within"}\NormalTok{, }
               \AttributeTok{effect =} \StringTok{"twoways"}\NormalTok{)}

\CommentTok{\# get coefficients}
\FunctionTok{coeftest}\NormalTok{(output4, }\AttributeTok{vcov =}\NormalTok{ vcovHC, }\AttributeTok{type =} \StringTok{"HC1"}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} t test of coefficients:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}      Estimate Std. Error t value Pr(\textgreater{}|t|)}
\CommentTok{\#\textgreater{} post 0.081812   0.057748  1.4167   0.1572}
\end{Highlighting}
\end{Shaded}

As you can see, differences stem from SE estimation, not the coefficient estimate.

\hypertarget{multiple-periods-and-variation-in-treatment-timing}{%
\subsection{Multiple periods and variation in treatment timing}\label{multiple-periods-and-variation-in-treatment-timing}}

This is an extension of the DiD framework to settings where you have

\begin{itemize}
\item
  more than 2 time periods
\item
  different treatment timing
\end{itemize}

When treatment effects are heterogeneous across time or units, the standard \protect\hyperlink{two-way-fixed-effects}{Two-way Fixed-effects} is inappropriate.

Notation is consistent with \texttt{did} \href{https://cran.r-project.org/web/packages/did/vignettes/multi-period-did.html}{package} \citep{callaway2021difference}

\begin{itemize}
\item
  \(Y_{it}(0)\) is the potential outcome for unit \(i\)
\item
  \(Y_{it}(g)\) is the potential outcome for unit \(i\) in time period \(t\) if it's treated in period \(g\)
\item
  \(Y_{it}\) is the observed outcome for unit \(i\) in time period \(t\)
\end{itemize}

\[
Y_{it} = 
\begin{cases}
Y_{it} = Y_{it}(0) & \forall i \in \text{never-treated group} \\
Y_{it} = 1\{G_i > t\} Y_{it}(0) +  1\{G_i \le t \}Y_{it}(G_i) & \forall i \in \text{other groups}
\end{cases}
\]

\begin{itemize}
\item
  \(G_i\) is the time period when \(i\) is treated
\item
  \(C_i\) is a dummy when \(i\) belongs to the \textbf{never-treated} group
\item
  \(D_{it}\) is a dummy for whether \(i\) is treated in period \(t\)
\end{itemize}

\textbf{Assumptions}:

\begin{itemize}
\item
  Staggered treatment adoption: once treated, a unit cannot be untreated (revert)
\item
  Parallel trends assumptions (conditional on covariates):

  \begin{itemize}
  \item
    Based on never-treated units: \(E[Y_t(0)- Y_{t-1}(0)|G= g] = E[Y_t(0) - Y_{t-1}(0)|C=1]\)

    \begin{itemize}
    \tightlist
    \item
      Without treatment, the average potential outcomes for group \(g\) equals the average potential outcomes for the never-treated group (i.e., control group), which means that we have (1) enough data on the never-treated group (2) the control group is similar to the eventually treated group.
    \end{itemize}
  \item
    Based on not-yet treated units: \(E[Y_t(0) - Y_{t-1}(0)|G = g] = E[Y_t(0) - Y_{t-1}(0)|D_s = 0, G \neq g]\)

    \begin{itemize}
    \item
      Not-yet treated units by time \(s\) ( \(s \ge t\)) can be used as comparison groups to calculate the average treatment effects for the group first treated in time \(g\)
    \item
      Additional assumption: pre-treatment trends across groups \citep{marcus2021role}
    \end{itemize}
  \end{itemize}
\item
  Random sampling
\item
  Irreversibility of treatment (once treated, cannot be untreated)
\item
  Overlap (the treatment propensity \(e \in [0,1]\))
\end{itemize}

Group-Time ATE

\begin{itemize}
\tightlist
\item
  This is the equivalent of the average treatment effect in the standard case (2 groups, 2 periods) under multiple time periods.
\end{itemize}

\[
ATT(g,t) = E[Y_t(g) - Y_t(0) |G = g]
\]

which is the average treatment effect for group \(g\) in period \(t\)

\begin{itemize}
\item
  Identification: When the parallel trends assumption based on

  \begin{itemize}
  \item
    Never-treated units: \(ATT(g,t) = E[Y_t - Y_{g-1} |G = g] - E[Y_t - Y_{g-1}|C=1] \forall t \ge g\)
  \item
    Not-yet-treated units: \(ATT(g,t) = E[Y_t - Y_{g-1}|G= g] - E[Y_t - Y_{g-1}|D_t = 0, G \neq g] \forall t \ge g\)
  \end{itemize}
\item
  Identification: when the parallel trends assumption only holds conditional on covariates and based on

  \begin{itemize}
  \item
    Never-treated units: \(ATT(g,t) = E[Y_t - Y_{g-1} |X, G = g] - E[Y_t - Y_{g-1}|X, C=1] \forall t \ge g\)
  \item
    Not-yet-treated units: \(ATT(g,t) = E[Y_t - Y_{g-1}|X, G= g] - E[Y_t - Y_{g-1}|X, D_t = 0, G \neq g] \forall t \ge g\)
  \item
    This is plausible when you have suspected selection bias that can be corrected by using covariates (i.e., very much similar to matching methods to have plausible parallel trends).
  \end{itemize}
\end{itemize}

Possible parameters of interest are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Average treatment effect per group
\end{enumerate}

\[
\theta_S(g) = \frac{1}{\tau - g + 1} \sum_{t = 2}^\tau \mathbb{1} \{ \le t \} ATT(g,t)
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Average treatment effect across groups (that were treated) (similar to average treatment effect on the treated in the canonical case)
\end{enumerate}

\[
\theta_S^O := \sum_{g=2}^\tau \theta_S(g) P(G=g)
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Average treatment effect dynamics (i.e., average treatment effect for groups that have been exposed to the treatment for \(e\) time periods):
\end{enumerate}

\[
\theta_D(e) := \sum_{g=2}^\tau \mathbb{1} \{g + e \le \tau \}ATT(g,g + e) P(G = g|G + e \le \tau)
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Average treatment effect in period \(t\) for all groups that have treated by period \(t\))
\end{enumerate}

\[
\theta_C(t) = \sum_{g=2}^\tau \mathbb{1}\{g \le t\} ATT(g,t) P(G = g|g \le t)
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Average treatment effect by calendar time
\end{enumerate}

\[
\theta_C = \frac{1}{\tau-1}\sum_{t=2}^\tau \theta_C(t)
\]

\hypertarget{staggered-dif-n-dif}{%
\subsection{Staggered Dif-n-dif}\label{staggered-dif-n-dif}}

\begin{itemize}
\tightlist
\item
  When subjects are treated at different point in time (variation in treatment timing across units), we have to use staggered DiD (also known as DiD event study or dynamic DiD).
\item
  For design where a treatment is applied and units are exposed to this treatment at all time afterward, see \citep{athey2022design}
\end{itemize}

Basic design \citep{stevenson2006bargaining}

\[
\begin{aligned}
Y_{it} &= \sum_k \beta_k Treatment_{it}^k + \sum_i \eta_i  State_i \\
&+ \sum_t \lambda_t Year_t + Controls_{it} + \epsilon_{it}
\end{aligned}
\]

where

\begin{itemize}
\item
  \(Treatment_{it}^k\) is a series of dummy variables equal to 1 if state \(i\) is treated \(k\) years ago in period \(t\)
\item
  SE is usually clustered at the group level (occasionally time level).
\item
  To avoid collinearity, the period right before treatment is usually chosen to drop.
\end{itemize}

In this setting, we try to show that the treatment and control groups are not statistically different (i.e., the coefficient estimates before treatment are not different from 0) to show pre-treatment parallel trends.

However, this two-way fixed effects design has been criticized by \citep{sun2021estimating, callaway2021difference, goodman2021difference}. When researchers include leads and lags of the treatment to see the long-term effects of the treatment, these leads and lags can be biased by effects from other periods, and pre-trends can falsely arise due to treatment effects heterogeneity.

Applying the new proposed method, finance and accounting researchers find that in many cases, the causal estimates turn out to be null \citep{baker2022much}.

Robustness Check

\begin{itemize}
\tightlist
\item
  The \textbf{triple-difference strategy} involves examining the interaction between the \textbf{treatment variable} and \textbf{the probability of being affected by the program}, and the group-level participation rate. The identification assumption is that there are no differential trends between high and low participation groups in early versus late implementing countries.
\end{itemize}

\textbf{Assumptions}

\begin{itemize}
\item
  \textbf{Rollout Exogeneity} (i.e., exogeneity of treatment adoption): if the treatment is randomly implemented over time (i.e., unrelated to variables that could also affect our dependent variables)

  \begin{itemize}
  \tightlist
  \item
    Evidence: Regress adoption on pre-treatment variables. And if you find evidence of correlation, include linear trends interacted with pre-treatment variables \citep{hoynes2009consumption}
  \end{itemize}
\item
  \textbf{No confounding events}
\item
  \textbf{Exclusion restrictions}

  \begin{itemize}
  \item
    \textbf{\emph{No-anticipation assumption}}: future treatment time do not affect current outcomes
  \item
    \textbf{\emph{Invariance-to-history assumption}}: the time a unit under treatment does not affect the outcome (i.e., the time exposed does not matter, just whether exposed or not). This presents causal effect of early or late adoption on the outcome.
  \end{itemize}
\item
  And all the assumptions in listed in the \protect\hyperlink{multiple-periods-and-variation-in-treatment-timing}{Multiple periods and variation in treatment timing}
\item
  Auxiliary assumptions:

  \begin{itemize}
  \item
    Constant treatment effects across units
  \item
    Constant treatment effect over time
  \item
    Random sampling
  \item
    Effect Additivity
  \end{itemize}
\end{itemize}

Remedies for staggered DiD:

\begin{itemize}
\item
  Each treated cohort is compared to appropriate controls (not-yet-treated, never-treated)

  \begin{itemize}
  \item
    \citep{goodman2021difference}
  \item
    \citep{callaway2021difference} consistent for average ATT. more complicated but also more flexible than \citep{sun2021estimating}

    \begin{itemize}
    \tightlist
    \item
      \citep{sun2021estimating} (a special case of \citep{callaway2021difference})
    \end{itemize}
  \item
    \citep{de2020two}
  \item
    \citep{borusyak2017revisiting}
  \end{itemize}
\item
  Stacked Regression (biased but simple):

  \begin{itemize}
  \item
    \citep{gormley2011growing}
  \item
    \citep{cengiz2019effect}
  \item
    \citep{deshpande2019screened}
  \end{itemize}
\end{itemize}

\hypertarget{stacked-did}{%
\subsubsection{Stacked DID}\label{stacked-did}}

Notations following \href{https://scholarworks.iu.edu/dspace/bitstream/handle/2022/26875/2021-10-22_wim_wing_did_slides.pdf?sequence=1\&isAllowed=y}{these slides}

\[
Y_{it} = \beta_{FE} D_{it} + A_i + B_t + \epsilon_{it}
\]

where

\begin{itemize}
\item
  \(A_i\) is the group fixed effects
\item
  \(B_t\) is the period fixed effects
\end{itemize}

Steps

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Choose Event Window
\item
  Enumerate Sub-experiments
\item
  Define Inclusion Criteria
\item
  Stack Data
\item
  Specify Estimating Equation
\end{enumerate}

\textbf{Event Window}

Let

\begin{itemize}
\item
  \(\kappa_a\) be the length of the pre-event window
\item
  \(\kappa_b\) be the length of the post-event window
\end{itemize}

By setting a common event window for the analysis, we essentially exclude all those events that do not meet this criteria.

\textbf{Sub-experiments}

Let \(T_1\) be the earliest period in the dataset

\(T_T\) be the last period in the dataset

Then, the collection of all policy adoption periods that are under our event window is

\[
\Omega_A = \{ A_i |T_1 + \kappa_a \le A_i \le T_T - \kappa_b\}
\]

where these events exist

\begin{itemize}
\item
  at least \(\kappa_a\) periods after the earliest period
\item
  at least \(\kappa_b\) periods before the last period
\end{itemize}

Let \(d = 1, \dots, D\) be the index column of the sub-experiments in \(\Omega_A\)

and \(\omega_d\) be the event date of the d-th sub-experiment (e.g., \(\omega_1\) = adoption date of the 1st experiment)

\textbf{Inclusion Criteria}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Valid treated Units

  \begin{itemize}
  \item
    Within sub-experiment \(d\), all treated units have the same adoption date
  \item
    This makes sure a unit can only serve as a treated unit in only 1 sub-experiment
  \end{itemize}
\item
  Clean controls

  \begin{itemize}
  \item
    Only units satisfying \(A_i >\omega_d + \kappa_b\) are included as controls in sub-experiment d
  \item
    This ensures controls are only

    \begin{itemize}
    \item
      never treated units
    \item
      units that are treated in far future
    \end{itemize}
  \item
    But a unit can be control unit in multiple sub-experiments (need to correct SE)
  \end{itemize}
\item
  Valid Time Periods

  \begin{itemize}
  \item
    All observations within sub-experiment d are from time periods within the sub-experiment's event window
  \item
    This ensures in sub-experiment d, only observations satisfying \(\omega_d - \kappa_a \le t \le \omega_d + \kappa_b\) are included
  \end{itemize}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(did)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(fixest)}

\FunctionTok{data}\NormalTok{(base\_stagg)}



\CommentTok{\# first make the stacked datasets}
\CommentTok{\# get the treatment cohorts}
\NormalTok{cohorts }\OtherTok{\textless{}{-}}\NormalTok{ base\_stagg }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{select}\NormalTok{(year\_treated) }\SpecialCharTok{\%\textgreater{}\%}
    \CommentTok{\# exclude never{-}treated group}
    \FunctionTok{filter}\NormalTok{(year\_treated }\SpecialCharTok{!=} \DecValTok{10000}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{unique}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{pull}\NormalTok{()}

\CommentTok{\# make formula to create the sub{-}datasets}
\NormalTok{getdata }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(j, window) \{}
    \CommentTok{\#keep what we need}
\NormalTok{    base\_stagg }\SpecialCharTok{\%\textgreater{}\%}
        \CommentTok{\# keep treated units and all units not treated within {-}5 to 5}
        \CommentTok{\# keep treated units and all units not treated within {-}window to window}
        \FunctionTok{filter}\NormalTok{(year\_treated }\SpecialCharTok{==}\NormalTok{ j }\SpecialCharTok{|}\NormalTok{ year\_treated }\SpecialCharTok{\textgreater{}}\NormalTok{ j }\SpecialCharTok{+}\NormalTok{ window) }\SpecialCharTok{\%\textgreater{}\%}
        \CommentTok{\# keep just year {-}window to window}
        \FunctionTok{filter}\NormalTok{(year }\SpecialCharTok{\textgreater{}=}\NormalTok{ j }\SpecialCharTok{{-}}\NormalTok{ window }\SpecialCharTok{\&}\NormalTok{ year }\SpecialCharTok{\textless{}=}\NormalTok{ j }\SpecialCharTok{+}\NormalTok{ window) }\SpecialCharTok{\%\textgreater{}\%}
        \CommentTok{\# create an indicator for the dataset}
        \FunctionTok{mutate}\NormalTok{(}\AttributeTok{df =}\NormalTok{ j)}
\NormalTok{\}}

\CommentTok{\# get data stacked}
\NormalTok{stacked\_data }\OtherTok{\textless{}{-}} \FunctionTok{map\_df}\NormalTok{(cohorts, }\SpecialCharTok{\textasciitilde{}} \FunctionTok{getdata}\NormalTok{(., }\AttributeTok{window =} \DecValTok{5}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{rel\_year =} \FunctionTok{if\_else}\NormalTok{(df }\SpecialCharTok{==}\NormalTok{ year\_treated, time\_to\_treatment, }\ConstantTok{NA\_real\_}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    fastDummies}\SpecialCharTok{::}\FunctionTok{dummy\_cols}\NormalTok{(}\StringTok{"rel\_year"}\NormalTok{, }\AttributeTok{ignore\_na =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{starts\_with}\NormalTok{(}\StringTok{"rel\_year\_"}\NormalTok{), }\SpecialCharTok{\textasciitilde{}} \FunctionTok{replace\_na}\NormalTok{(., }\DecValTok{0}\NormalTok{)))}

\CommentTok{\# get stacked value}
\NormalTok{stacked }\OtherTok{\textless{}{-}}
    \FunctionTok{feols}\NormalTok{(}
\NormalTok{        y }\SpecialCharTok{\textasciitilde{}} \StringTok{\textasciigrave{}}\AttributeTok{rel\_year\_{-}5}\StringTok{\textasciigrave{}} \SpecialCharTok{+} \StringTok{\textasciigrave{}}\AttributeTok{rel\_year\_{-}4}\StringTok{\textasciigrave{}} \SpecialCharTok{+} \StringTok{\textasciigrave{}}\AttributeTok{rel\_year\_{-}3}\StringTok{\textasciigrave{}} \SpecialCharTok{+}
            \StringTok{\textasciigrave{}}\AttributeTok{rel\_year\_{-}2}\StringTok{\textasciigrave{}} \SpecialCharTok{+}\NormalTok{ rel\_year\_0 }\SpecialCharTok{+}\NormalTok{ rel\_year\_1 }\SpecialCharTok{+}\NormalTok{ rel\_year\_2 }\SpecialCharTok{+}\NormalTok{ rel\_year\_3 }\SpecialCharTok{+}
\NormalTok{            rel\_year\_4 }\SpecialCharTok{+}\NormalTok{ rel\_year\_5 }\SpecialCharTok{|}
\NormalTok{            id }\SpecialCharTok{\^{}}\NormalTok{ df }\SpecialCharTok{+}\NormalTok{ year }\SpecialCharTok{\^{}}\NormalTok{ df,}
        \AttributeTok{data =}\NormalTok{ stacked\_data}
\NormalTok{    )}\SpecialCharTok{$}\NormalTok{coefficients}

\NormalTok{stacked\_se }\OtherTok{=} \FunctionTok{feols}\NormalTok{(}
\NormalTok{    y }\SpecialCharTok{\textasciitilde{}} \StringTok{\textasciigrave{}}\AttributeTok{rel\_year\_{-}5}\StringTok{\textasciigrave{}} \SpecialCharTok{+} \StringTok{\textasciigrave{}}\AttributeTok{rel\_year\_{-}4}\StringTok{\textasciigrave{}} \SpecialCharTok{+} \StringTok{\textasciigrave{}}\AttributeTok{rel\_year\_{-}3}\StringTok{\textasciigrave{}} \SpecialCharTok{+}
        \StringTok{\textasciigrave{}}\AttributeTok{rel\_year\_{-}2}\StringTok{\textasciigrave{}} \SpecialCharTok{+}\NormalTok{ rel\_year\_0 }\SpecialCharTok{+}\NormalTok{ rel\_year\_1 }\SpecialCharTok{+}\NormalTok{ rel\_year\_2 }\SpecialCharTok{+}\NormalTok{ rel\_year\_3 }\SpecialCharTok{+}
\NormalTok{        rel\_year\_4 }\SpecialCharTok{+}\NormalTok{ rel\_year\_5 }\SpecialCharTok{|}
\NormalTok{        id }\SpecialCharTok{\^{}}\NormalTok{ df }\SpecialCharTok{+}\NormalTok{ year }\SpecialCharTok{\^{}}\NormalTok{ df,}
    \AttributeTok{data =}\NormalTok{ stacked\_data}
\NormalTok{)}\SpecialCharTok{$}\NormalTok{se}

\CommentTok{\# add in 0 for omitted {-}1}
\NormalTok{stacked }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(stacked[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{], }\DecValTok{0}\NormalTok{, stacked[}\DecValTok{5}\SpecialCharTok{:}\DecValTok{10}\NormalTok{])}
\NormalTok{stacked\_se }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(stacked\_se[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{], }\DecValTok{0}\NormalTok{, stacked\_se[}\DecValTok{5}\SpecialCharTok{:}\DecValTok{10}\NormalTok{])}


\NormalTok{cs\_out }\OtherTok{\textless{}{-}} \FunctionTok{att\_gt}\NormalTok{(}
    \AttributeTok{yname =} \StringTok{"y"}\NormalTok{,}
    \AttributeTok{data =}\NormalTok{ base\_stagg,}
    \AttributeTok{gname =} \StringTok{"year\_treated"}\NormalTok{,}
    \AttributeTok{idname =} \StringTok{"id"}\NormalTok{,}
    \CommentTok{\# xformla = "\textasciitilde{}x1",}
    \AttributeTok{tname =} \StringTok{"year"}
\NormalTok{)}
\NormalTok{cs }\OtherTok{\textless{}{-}}
    \FunctionTok{aggte}\NormalTok{(}
\NormalTok{        cs\_out,}
        \AttributeTok{type =} \StringTok{"dynamic"}\NormalTok{,}
        \AttributeTok{min\_e =} \SpecialCharTok{{-}}\DecValTok{5}\NormalTok{,}
        \AttributeTok{max\_e =} \DecValTok{5}\NormalTok{,}
        \AttributeTok{bstrap =} \ConstantTok{FALSE}\NormalTok{,}
        \AttributeTok{cband =} \ConstantTok{FALSE}
\NormalTok{    )}



\NormalTok{res\_sa20 }\OtherTok{=} \FunctionTok{feols}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}} \FunctionTok{sunab}\NormalTok{(year\_treated, year) }\SpecialCharTok{|}
\NormalTok{                     id }\SpecialCharTok{+}\NormalTok{ year, base\_stagg)}
\NormalTok{sa }\OtherTok{=} \FunctionTok{tidy}\NormalTok{(res\_sa20)[}\DecValTok{5}\SpecialCharTok{:}\DecValTok{14}\NormalTok{, ] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{pull}\NormalTok{(estimate)}
\NormalTok{sa }\OtherTok{=} \FunctionTok{c}\NormalTok{(sa[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{], }\DecValTok{0}\NormalTok{, sa[}\DecValTok{5}\SpecialCharTok{:}\DecValTok{10}\NormalTok{])}

\NormalTok{sa\_se }\OtherTok{=} \FunctionTok{tidy}\NormalTok{(res\_sa20)[}\DecValTok{6}\SpecialCharTok{:}\DecValTok{15}\NormalTok{, ] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{pull}\NormalTok{(std.error)}
\NormalTok{sa\_se }\OtherTok{=} \FunctionTok{c}\NormalTok{(sa\_se[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{], }\DecValTok{0}\NormalTok{, sa\_se[}\DecValTok{5}\SpecialCharTok{:}\DecValTok{10}\NormalTok{])}

\NormalTok{compare\_df\_est }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}
    \AttributeTok{period =} \SpecialCharTok{{-}}\DecValTok{5}\SpecialCharTok{:}\DecValTok{5}\NormalTok{,}
    \AttributeTok{cs =}\NormalTok{ cs}\SpecialCharTok{$}\NormalTok{att.egt,}
    \AttributeTok{sa =}\NormalTok{ sa,}
    \AttributeTok{stacked =}\NormalTok{ stacked}
\NormalTok{)}

\NormalTok{compare\_df\_se }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}
    \AttributeTok{period =} \SpecialCharTok{{-}}\DecValTok{5}\SpecialCharTok{:}\DecValTok{5}\NormalTok{,}
    \AttributeTok{cs =}\NormalTok{ cs}\SpecialCharTok{$}\NormalTok{se.egt,}
    \AttributeTok{sa =}\NormalTok{ sa\_se,}
    \AttributeTok{stacked =}\NormalTok{ stacked\_se}
\NormalTok{)}

\NormalTok{compare\_df\_longer }\OtherTok{\textless{}{-}}\NormalTok{ compare\_df\_est }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{pivot\_longer}\NormalTok{(}\SpecialCharTok{!}\NormalTok{period, }\AttributeTok{names\_to =} \StringTok{"estimator"}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{"est"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    
    \FunctionTok{full\_join}\NormalTok{(compare\_df\_se }\SpecialCharTok{\%\textgreater{}\%} 
                  \FunctionTok{pivot\_longer}\NormalTok{(}\SpecialCharTok{!}\NormalTok{period, }\AttributeTok{names\_to =} \StringTok{"estimator"}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{"se"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
    
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{upper =}\NormalTok{ est }\SpecialCharTok{+}  \FloatTok{1.96} \SpecialCharTok{*}\NormalTok{ se,}
           \AttributeTok{lower =}\NormalTok{ est }\SpecialCharTok{{-}} \FloatTok{1.96} \SpecialCharTok{*}\NormalTok{ se)}


\FunctionTok{ggplot}\NormalTok{(compare\_df\_longer) }\SpecialCharTok{+}
    \FunctionTok{geom\_ribbon}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}
        \AttributeTok{x =}\NormalTok{ period,}
        \AttributeTok{ymin =}\NormalTok{ lower,}
        \AttributeTok{ymax =}\NormalTok{ upper,}
        \AttributeTok{group =}\NormalTok{ estimator}
\NormalTok{    )) }\SpecialCharTok{+}
    \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}
        \AttributeTok{x =}\NormalTok{ period,}
        \AttributeTok{y =}\NormalTok{ est,}
        \AttributeTok{group =}\NormalTok{ estimator,}
        \AttributeTok{col =}\NormalTok{ estimator}
\NormalTok{    ),}
    \AttributeTok{linewidth =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-9-1} \end{center}

\textbf{Stack Data}

Estimating Equation

\[
Y_{itd} = \beta_0 + \beta_1 + T_{id} + \beta_2 + P_{td} + \beta_3 (T_{id} \times P_{td}) + \epsilon_{itd}
\]

where

\begin{itemize}
\item
  \(T_{id}\) = 1 if unit \(i\) is treated in sub-experiment d, 0 if control
\item
  \(P_{td}\) = 1 if it's the period after the treatment in sub-experiment d
\end{itemize}

Equivalently,

\[
Y_{itd} = \beta_3 (T_{id} \times P_{td}) + \theta_{id} + \gamma_{td} + \epsilon_{itd}
\]

\(\beta_3\) averages all the time-varying effects into a single number (can't see the time-varying effects)

\textbf{Stacked Event Study}

Let \(YSE_{td} = t - \omega_d\) be the ``time since event'' variable in sub-experiment \(d\)

Then, \(YSE_{td} = -\kappa_a, \dots, 0, \dots, \kappa_b\) in every sub-experiment

In each sub-experiment, we can fit

\[
Y_{it}^d = \sum_{j = -\kappa_a}^{\kappa_b} \beta_j^d \times 1(TSE_{td} = j) + \sum_{m = -\kappa_a}^{\kappa_b} \delta_j^d (T_{id} \times 1 (TSE_{td} = j)) + \theta_i^d + \epsilon_{it}^d
\]

\begin{itemize}
\tightlist
\item
  Different set of event study coefficients in each sub-experiment
\end{itemize}

\[
Y_{itd} = \sum_{j = -\kappa_a}^{\kappa_b} \beta_j \times 1(TSE_{td} = j) + \sum_{m = -\kappa_a}^{\kappa_b} \delta_j (T_{id} \times 1 (TSE_{td} = j)) + \theta_{id} + \epsilon_{itd}
\]

\textbf{Clustering}

\begin{itemize}
\item
  Clustered at the unit x sub-experiment level \citep{cengiz2019effect}
\item
  Clustered at the unit level \citep{deshpande2019screened}
\end{itemize}

\hypertarget{goodman-bacon-decomposition}{%
\subsubsection{Goodman-Bacon Decomposition}\label{goodman-bacon-decomposition}}

Paper: \citep{goodman2021difference}

For an excellent explanation slides by the author, \href{https://www.stata.com/meeting/chicago19/slides/chicago19_Goodman-Bacon.pdf}{see}

Takeaways:

\begin{itemize}
\item
  A pairwise DID (\(\tau\)) gets more weight if the change is close to the middle of the study window
\item
  A pairwise DID (\(\tau\)) gets more weight if it includes more observations.
\end{itemize}

Code from \texttt{bacondecomp} vignette

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(bacondecomp)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{data}\NormalTok{(}\StringTok{"castle"}\NormalTok{)}
\NormalTok{castle }\OtherTok{\textless{}{-}}\NormalTok{ castle }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{select}\NormalTok{(l\_homicide, post, state, year)}
\FunctionTok{head}\NormalTok{(castle)}
\CommentTok{\#\textgreater{}   l\_homicide post   state year}
\CommentTok{\#\textgreater{} 1   2.027356    0 Alabama 2000}
\CommentTok{\#\textgreater{} 2   2.164867    0 Alabama 2001}
\CommentTok{\#\textgreater{} 3   1.936334    0 Alabama 2002}
\CommentTok{\#\textgreater{} 4   1.919567    0 Alabama 2003}
\CommentTok{\#\textgreater{} 5   1.749841    0 Alabama 2004}
\CommentTok{\#\textgreater{} 6   2.130440    0 Alabama 2005}


\NormalTok{df\_bacon }\OtherTok{\textless{}{-}} \FunctionTok{bacon}\NormalTok{(}
\NormalTok{    l\_homicide }\SpecialCharTok{\textasciitilde{}}\NormalTok{ post,}
    \AttributeTok{data =}\NormalTok{ castle,}
    \AttributeTok{id\_var =} \StringTok{"state"}\NormalTok{,}
    \AttributeTok{time\_var =} \StringTok{"year"}
\NormalTok{)}
\CommentTok{\#\textgreater{}                       type  weight  avg\_est}
\CommentTok{\#\textgreater{} 1 Earlier vs Later Treated 0.05976 {-}0.00554}
\CommentTok{\#\textgreater{} 2 Later vs Earlier Treated 0.03190  0.07032}
\CommentTok{\#\textgreater{} 3     Treated vs Untreated 0.90834  0.08796}

\CommentTok{\# weighted average of the decomposition}
\FunctionTok{sum}\NormalTok{(df\_bacon}\SpecialCharTok{$}\NormalTok{estimate }\SpecialCharTok{*}\NormalTok{ df\_bacon}\SpecialCharTok{$}\NormalTok{weight)}
\CommentTok{\#\textgreater{} [1] 0.08181162}
\end{Highlighting}
\end{Shaded}

Two-way Fixed effect estimate

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(broom)}
\NormalTok{fit\_tw }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(l\_homicide }\SpecialCharTok{\textasciitilde{}}\NormalTok{ post }\SpecialCharTok{+} \FunctionTok{factor}\NormalTok{(state) }\SpecialCharTok{+} \FunctionTok{factor}\NormalTok{(year), }
             \AttributeTok{data =}\NormalTok{ bacondecomp}\SpecialCharTok{::}\NormalTok{castle)}
\FunctionTok{head}\NormalTok{(}\FunctionTok{tidy}\NormalTok{(fit\_tw))}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 5}
\CommentTok{\#\textgreater{}   term                    estimate std.error statistic   p.value}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}                      \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 (Intercept)               1.95      0.0624    31.2   2.84e{-}118}
\CommentTok{\#\textgreater{} 2 post                      0.0818    0.0317     2.58  1.02e{-}  2}
\CommentTok{\#\textgreater{} 3 factor(state)Alaska      {-}0.373     0.0797    {-}4.68  3.77e{-}  6}
\CommentTok{\#\textgreater{} 4 factor(state)Arizona      0.0158    0.0797     0.198 8.43e{-}  1}
\CommentTok{\#\textgreater{} 5 factor(state)Arkansas    {-}0.118     0.0810    {-}1.46  1.44e{-}  1}
\CommentTok{\#\textgreater{} 6 factor(state)California  {-}0.108     0.0810    {-}1.34  1.82e{-}  1}
\end{Highlighting}
\end{Shaded}

Hence, naive TWFE fixed effect equals the weighted average of the Bacon decomposition (= 0.08).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}

\FunctionTok{ggplot}\NormalTok{(df\_bacon) }\SpecialCharTok{+}
    \FunctionTok{aes}\NormalTok{(}
        \AttributeTok{x =}\NormalTok{ weight,}
        \AttributeTok{y =}\NormalTok{ estimate,}
        \CommentTok{\# shape = factor(type),}
        \AttributeTok{color =}\NormalTok{ type}
\NormalTok{    ) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Weight"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Estimate"}\NormalTok{, }\AttributeTok{shape =} \StringTok{"Type"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
\NormalTok{    causalverse}\SpecialCharTok{::}\FunctionTok{ama\_theme}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-12-1} \end{center}

With time-varying controls that can identify variation within-treatment timing group, the''early vs.~late'' and ``late vs.~early'' estimates collapse to just one estimate (i.e., both treated).

\hypertarget{did-with-in-and-out-treatment-condition}{%
\subsubsection{DID with in and out treatment condition}\label{did-with-in-and-out-treatment-condition}}

\citet{imai2021use}

This case generalizes the staggered adoption setting, allowing units to vary in treatment over time. For \(N\) units across \(T\) time periods (with potentially unbalanced panels), let \(X_{it}\) represent treatment and \(Y_{it}\) the outcome for unit \(i\) at time \(t\). We use the two-way linear fixed effects model:

\[
Y_{it} = \alpha_i + \gamma_t + \beta X_{it} + \epsilon_{it}
\]

for \(i = 1, \dots, N\) and \(t = 1, \dots, T\). Here, \(\alpha_i\) and \(\gamma_t\) are unit and time fixed effects. They capture time-invariant unit-specific and unit-invariant time-specific unobserved confounders, respectively. We can express these as \(\alpha_i = h(\mathbf{U}_i)\) and \(\gamma_t = f(\mathbf{V}_t)\), with \(\mathbf{U}_i\) and \(\mathbf{V}_t\) being the confounders. The model doesn't assume a specific form for \(h(.)\) and \(f(.)\), but that they're additive and separable given binary treatment.

The least squares estimate of \(\beta\) leverages the covariance in outcome and treatment \citep[p.~406]{imai2021use}. Specifically, it uses the within-unit and within-time variations. Many researchers prefer the two fixed effects (2FE) estimator because it adjusts for both types of unobserved confounders without specific functional-form assumptions, but this is wrong \citep{imai2019should}. We do need functional-form assumption (i.e., linearity assumption) for the 2FE to work \citep[p.~406]{imai2021use}

\begin{itemize}
\item
  \textbf{Two-Way Matching Estimator}:

  \begin{itemize}
  \item
    It can lead to mismatches; units with the same treatment status get matched when estimating counterfactual outcomes.
  \item
    Observations need to be matched with opposite treatment status for correct causal effects estimation.
  \item
    Mismatches can cause attenuation bias.
  \item
    The 2FE estimator adjusts for this bias using the factor \(K\), which represents the net proportion of proper matches between observations with opposite treatment status.
  \end{itemize}
\item
  \textbf{Weighting in 2FE}:

  \begin{itemize}
  \item
    Observation \((i,t)\) is weighted based on how often it acts as a control unit.
  \item
    The weighted 2FE estimator still has mismatches, but fewer than the standard 2FE estimator.
  \item
    Adjustments are made based on observations that neither belong to the same unit nor the same time period as the matched observation.
  \item
    This means there are challenges in adjusting for unit-specific and time-specific unobserved confounders under the two-way fixed effect framework.
  \end{itemize}
\item
  \textbf{Equivalence \& Assumptions}:

  \begin{itemize}
  \item
    Equivalence between the 2FE estimator and the DID estimator is dependent on the linearity assumption.
  \item
    The multi-period DiD estimator is described as an average of two-time-period, two-group DiD estimators applied during changes from control to treatment.
  \end{itemize}
\item
  \textbf{Comparison with DiD}:

  \begin{itemize}
  \item
    In simple settings (two time periods, treatment given to one group in the second period), the standard nonparametric DiD estimator equals the 2FE estimator.
  \item
    This doesn't hold in multi-period DiD designs where units change treatment status multiple times at different intervals.
  \item
    Contrary to popular belief, the unweighted 2FE estimator isn't generally equivalent to the multi-period DiD estimator.
  \item
    While the multi-period DiD can be equivalent to the weighted 2FE, some control observations may have negative regression weights.
  \end{itemize}
\item
  \textbf{Conclusion}:

  \begin{itemize}
  \tightlist
  \item
    Justifying the 2FE estimator as the DID estimator isn't warranted without imposing the linearity assumption.
  \end{itemize}
\end{itemize}

\textbf{Application \citep{imai2021matching}}

\begin{itemize}
\item
  \textbf{Matching Methods}:

  \begin{itemize}
  \item
    Enhance the validity of causal inference.
  \item
    Reduce model dependence and provide intuitive diagnostics \citep{ho2007matching}
  \item
    Rarely utilized in analyzing time series cross-sectional data.
  \item
    The proposed matching estimators are more robust than the standard two-way fixed effects estimator, which can be biased if mis-specified
  \item
    Better than synthetic controls (e.g., \citep{xu2017generalized}) because it needs less data to achieve good performance and and adapt the the context of unit switching treatment status multiple times.
  \end{itemize}
\item
  Notes:

  \begin{itemize}
  \tightlist
  \item
    Potential carryover effects (treatment may have a long-term effect), leading to post-treatment bias.
  \end{itemize}
\item
  \textbf{Proposed Approach}:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \item
    Treated observations are matched with control observations from other units in the same time period with the same treatment history up to a specified number of lags.
  \item
    Standard matching and weighting techniques are employed to further refine the matched set.
  \item
    Apply a DiD estimator to adjust for time trend.
  \item
    The goal is to have treated and matched control observations with similar covariate values.
  \end{enumerate}
\item
  \textbf{Assessment}:

  \begin{itemize}
  \tightlist
  \item
    The quality of matches is evaluated through covariate balancing.
  \end{itemize}
\item
  \textbf{Estimation}:

  \begin{itemize}
  \tightlist
  \item
    Both short-term and long-term average treatment effects on the treated (ATT) are estimated.
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(PanelMatch)}
\end{Highlighting}
\end{Shaded}

\textbf{Treatment Variation plot}

\begin{itemize}
\item
  Visualize the variation of the treatment across space and time
\item
  Aids in discerning whether the treatment fluctuates adequately over time and units or if the variation is primarily clustered in a subset of data.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{DisplayTreatment}\NormalTok{(}
    \AttributeTok{unit.id =} \StringTok{"wbcode2"}\NormalTok{,}
    \AttributeTok{time.id =} \StringTok{"year"}\NormalTok{,}
    \AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"year"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"Country Code"}\NormalTok{,}
    \AttributeTok{treatment =} \StringTok{"dem"}\NormalTok{,}
    
    \AttributeTok{hide.x.tick.label =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{hide.y.tick.label =} \ConstantTok{TRUE}\NormalTok{, }
    \CommentTok{\# dense.plot = TRUE,}
    \AttributeTok{data =}\NormalTok{ dem}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-14-1} \end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Select \(F\) (i.e., the number of leads - time periods after treatment). Driven by what authors are interested in estimating:
\end{enumerate}

\begin{itemize}
\item
  \(F = 0\) is the contemporaneous effect (short-term effect)
\item
  \(F = n\) is the the treatment effect on the outcome two time periods after the treatment. (cumulative or long-term effect)
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Select \(L\) (number of lags to adjust).
\end{enumerate}

\begin{itemize}
\item
  Driven by the identification assumption.
\item
  Balances bias-variance tradeoff.
\item
  Higher \(L\) values increase credibility but reduce efficiency by limiting potential matches.
\end{itemize}

\textbf{Model assumption}:

\begin{itemize}
\item
  No spillover effect assumed.
\item
  Carryover effect allowed up to \(L\) periods.
\item
  Potential outcome for a unit depends neither on others' treatment status nor on its past treatment after \(L\) periods.
\end{itemize}

After defining causal quantity with parameters \(L\) and \(F\).

\begin{itemize}
\tightlist
\item
  Focus on the average treatment effect of treatment status change.
\item
  \(\delta(F,L)\) is the average causal effect of treatment change (ATT), \(F\) periods post-treatment, considering treatment history up to \(L\) periods.
\item
  Causal quantity considers potential future treatment reversals, meaning treatment could revert to control before outcome measurement.
\end{itemize}

Also possible to estimate the average treatment effect of treatment reversal on the reversed (ART).

Choose \(L,F\) based on specific needs.

\begin{itemize}
\item
  A large \(L\) value:

  \begin{itemize}
  \item
    Increases the credibility of the limited carryover effect assumption.
  \item
    Allows more past treatments (up to \(tâˆ’L\)) to influence the outcome \(Y_{i,t+F}\).
  \item
    Might reduce the number of matches and lead to less precise estimates.
  \end{itemize}
\item
  Selecting an appropriate number of lags

  \begin{itemize}
  \item
    Researchers should base this choice on substantive knowledge.
  \item
    Sensitivity of empirical results to this choice should be examined.
  \end{itemize}
\item
  The choice of \(F\) should be:

  \begin{itemize}
  \item
    Substantively motivated.
  \item
    Decides whether the interest lies in short-term or long-term causal effects.
  \item
    A large \(F\) value can complicate causal effect interpretation, especially if many units switch treatment status during the \(F\) lead time period.
  \end{itemize}
\end{itemize}

\textbf{Identification Assumption}

\begin{itemize}
\item
  Parallel trend assumption conditioned on treatment, outcome (excluding immediate lag), and covariate histories.
\item
  Doesn't require strong unconfoundedness assumption.
\item
  Cannot account for unobserved time-varying confounders.
\item
  Essential to examine outcome time trends.

  \begin{itemize}
  \tightlist
  \item
    Check if they're parallel between treated and matched control units using pre-treatment data
  \end{itemize}
\item
  \textbf{Constructing the Matched Sets}:

  \begin{itemize}
  \item
    For each treated observation, create matched control units with identical treatment history from \(tâˆ’L\) to \(tâˆ’1\).
  \item
    Matching based on treatment history helps control for carryover effects.
  \item
    Past treatments often act as major confounders, but this method can correct for it.
  \item
    Exact matching on time period adjusts for time-specific unobserved confounders.
  \item
    Unlike staggered adoption methods, units can change treatment status multiple times.
  \item
    Matched set allows treatment switching in and out of treatment
  \end{itemize}
\item
  \textbf{Refining the Matched Sets}:

  \begin{itemize}
  \item
    Initially, matched sets adjust only for treatment history.
  \item
    Parallel trend assumption requires adjustments for other confounders like past outcomes and covariates.
  \item
    Matching methods:

    \begin{itemize}
    \item
      Match each treated observation with up to \(J\) control units.
    \item
      Distance measures like Mahalanobis distance or propensity score can be used.
    \item
      Match based on estimated propensity score, considering pretreatment covariates.
    \item
      Refined matched set selects most similar control units based on observed confounders.
    \end{itemize}
  \item
    Weighting methods:

    \begin{itemize}
    \item
      Assign weight to each control unit in a matched set.
    \item
      Weights prioritize more similar units.
    \item
      Inverse propensity score weighting method can be applied.
    \item
      Weighting is a more generalized method than matching.
    \end{itemize}
  \end{itemize}
\end{itemize}

\textbf{The Difference-in-Differences Estimator}:

\begin{itemize}
\item
  Using refined matched sets, the ATT (Average Treatment Effect on the Treated) of policy change is estimated.
\item
  For each treated observation, estimate the counterfactual outcome using the weighted average of control units in the refined set.
\item
  The DiD estimate of the ATT is computed for each treated observation, then averaged across all such observations.
\item
  For noncontemporaneous treatment effects where \(F > 0\):

  \begin{itemize}
  \item
    The ATT doesn't specify future treatment sequence.
  \item
    Matched control units might have units receiving treatment between time \(t\) and \(t + F\).
  \item
    Some treated units could return to control conditions between these times.
  \end{itemize}
\end{itemize}

\textbf{Checking Covariate Balance}:

\begin{itemize}
\item
  The proposed methodology offers the advantage of checking covariate balance between treated and matched control observations.
\item
  This check helps to see if treated and matched control observations are comparable with respect to observed confounders.
\item
  Once matched sets are refined, covariate balance examination becomes straightforward.
\item
  Examine the mean difference of each covariate between a treated observation and its matched controls for each pretreatment time period.
\item
  Standardize this difference using the standard deviation of each covariate across all treated observations in the dataset.
\item
  Aggregate this covariate balance measure across all treated observations for each covariate and pretreatment time period.
\item
  Examine balance for lagged outcome variables over multiple pretreatment periods and time-varying covariates.

  \begin{itemize}
  \tightlist
  \item
    This helps evaluate the validity of the parallel trend assumption underlying the proposed DiD estimator.
  \end{itemize}
\end{itemize}

\textbf{Relations with Linear Fixed Effects Regression Estimators}:

\begin{itemize}
\item
  The standard DiD estimator is equivalent to the linear two-way fixed effects regression estimator when:

  \begin{itemize}
  \item
    Only two time periods exist.
  \item
    Treatment is given to some units exclusively in the second period.
  \end{itemize}
\item
  This equivalence doesn't extend to multiperiod DiD designs, where:

  \begin{itemize}
  \item
    More than two time periods are considered.
  \item
    Units might receive treatment multiple times.
  \end{itemize}
\item
  Despite this, many researchers relate the use of the two-way fixed effects estimator to the DiD design.
\end{itemize}

\textbf{Standard Error Calculation}:

\begin{itemize}
\item
  Approach:

  \begin{itemize}
  \item
    Condition on the weights implied by the matching process.
  \item
    These weights denote how often an observation is utilized in matching \citep{imbens2015causal}
  \end{itemize}
\item
  Context:

  \begin{itemize}
  \item
    Analogous to the conditional variance seen in regression models.
  \item
    Resulting standard errors don't factor in uncertainties around the matching procedure.
  \item
    They can be viewed as a measure of uncertainty conditional upon the matching process \citep{ho2007matching}.
  \end{itemize}
\end{itemize}

\textbf{Key Findings}:

\begin{itemize}
\item
  Even in conditions favoring OLS, the proposed matching estimator displayed higher robustness to omitted relevant lags than the linear regression model with fixed effects.
\item
  The robustness offered by matching came at a cost - reduced statistical power.
\item
  This emphasizes the classic statistical tradeoff between bias (where matching has an advantage) and variance (where regression models might be more efficient).
\end{itemize}

\textbf{Data Requirements}

\begin{itemize}
\item
  The treatment variable is binary:

  \begin{itemize}
  \item
    0 signifies ``assignment'' to control.
  \item
    1 signifies assignment to treatment.
  \end{itemize}
\item
  Variables identifying units in the data must be: Numeric or integer.
\item
  Variables identifying time periods should be: Consecutive numeric/integer data.
\item
  Data format requirement: Must be provided as a standard \texttt{data.frame} object.
\end{itemize}

Basic functions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Utilize treatment histories to create matching sets of treated and control units.
\item
  Refine these matched sets by determining weights for each control unit in the set.

  \begin{itemize}
  \tightlist
  \item
    Units with higher weights have a larger influence during estimations.
  \end{itemize}
\end{enumerate}

\textbf{Matching on Treatment History}:

\begin{itemize}
\item
  Goal is to match units transitioning from untreated to treated status with control units that have similar past treatment histories.
\item
  Setting the Quantity of Interest (\texttt{qoi\ =})

  \begin{itemize}
  \item
    \texttt{att} average treatment effect on treated units
  \item
    \texttt{atc} average treatment effect of treatment on the control units
  \item
    \texttt{art} average effect of treatment reversal for units that experience treatment reversal
  \item
    \texttt{ate} average treatment effect
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# All examples follow the package\textquotesingle{}s vignette}
\CommentTok{\# Create the matched sets}
\NormalTok{PM.results.none }\OtherTok{\textless{}{-}}
    \FunctionTok{PanelMatch}\NormalTok{(}
        \AttributeTok{lag =} \DecValTok{4}\NormalTok{,}
        \AttributeTok{time.id =} \StringTok{"year"}\NormalTok{,}
        \AttributeTok{unit.id =} \StringTok{"wbcode2"}\NormalTok{,}
        \AttributeTok{treatment =} \StringTok{"dem"}\NormalTok{,}
        \AttributeTok{refinement.method =} \StringTok{"none"}\NormalTok{,}
        \AttributeTok{data =}\NormalTok{ dem,}
        \AttributeTok{match.missing =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{size.match =} \DecValTok{5}\NormalTok{,}
        \AttributeTok{qoi =} \StringTok{"att"}\NormalTok{,}
        \AttributeTok{outcome.var =} \StringTok{"y"}\NormalTok{,}
        \AttributeTok{lead =} \DecValTok{0}\SpecialCharTok{:}\DecValTok{4}\NormalTok{,}
        \AttributeTok{forbid.treatment.reversal =} \ConstantTok{FALSE}\NormalTok{,}
        \AttributeTok{use.diagonal.variance.matrix =} \ConstantTok{TRUE}
\NormalTok{    )}

\CommentTok{\# visualize the treated unit and matched controls}
\FunctionTok{DisplayTreatment}\NormalTok{(}
    \AttributeTok{unit.id =} \StringTok{"wbcode2"}\NormalTok{,}
    \AttributeTok{time.id =} \StringTok{"year"}\NormalTok{,}
    \AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"year"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"Country Code"}\NormalTok{,}
    \AttributeTok{treatment =} \StringTok{"dem"}\NormalTok{,}
    \AttributeTok{data =}\NormalTok{ dem,}
    \AttributeTok{matched.set =}\NormalTok{ PM.results.none}\SpecialCharTok{$}\NormalTok{att[}\DecValTok{1}\NormalTok{],}
    \CommentTok{\# highlight the particular set}
    \AttributeTok{show.set.only =} \ConstantTok{TRUE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-15-1} \end{center}

Control units and the treated unit have identical treatment histories over the lag window (1988-1991)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{DisplayTreatment}\NormalTok{(}
    \AttributeTok{unit.id =} \StringTok{"wbcode2"}\NormalTok{,}
    \AttributeTok{time.id =} \StringTok{"year"}\NormalTok{,}
    \AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"year"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"Country Code"}\NormalTok{,}
    \AttributeTok{treatment =} \StringTok{"dem"}\NormalTok{,}
    \AttributeTok{data =}\NormalTok{ dem,}
    \AttributeTok{matched.set =}\NormalTok{ PM.results.none}\SpecialCharTok{$}\NormalTok{att[}\DecValTok{2}\NormalTok{],}
    \CommentTok{\# highlight the particular set}
    \AttributeTok{show.set.only =} \ConstantTok{TRUE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-16-1} \end{center}

This set is more limited than the first one, but we can still see that we have exact past histories.

\begin{itemize}
\item
  \textbf{Refining Matched Sets}

  \begin{itemize}
  \item
    Refinement involves assigning weights to control units.
  \item
    Users must:

    \begin{enumerate}
    \def\labelenumi{\arabic{enumi}.}
    \item
      Specify a method for calculating unit similarity/distance.
    \item
      Choose variables for similarity/distance calculations.
    \end{enumerate}
  \end{itemize}
\item
  \textbf{Select a Refinement Method}

  \begin{itemize}
  \item
    Users determine the refinement method via the \textbf{\texttt{refinement.method}} argument.
  \item
    Options include:

    \begin{itemize}
    \item
      \texttt{mahalanobis}
    \item
      \texttt{ps.match}
    \item
      \texttt{CBPS.match}
    \item
      \texttt{ps.weight}
    \item
      \texttt{CBPS.weight}
    \item
      \texttt{ps.msm.weight}
    \item
      \texttt{CBPS.msm.weight}
    \item
      \texttt{none}
    \end{itemize}
  \item
    Methods with ``match'' in the name and Mahalanobis will assign equal weights to similar control units.
  \item
    ``Weighting'' methods give higher weights to control units more similar to treated units.
  \end{itemize}
\item
  \textbf{Variable Selection}

  \begin{itemize}
  \item
    Users need to define which covariates will be used through the \textbf{\texttt{covs.formula}} argument, a one-sided formula object.
  \item
    Variables on the right side of the formula are used for calculations.
  \item
    ``Lagged'' versions of variables can be included using the format: \textbf{\texttt{I(lag(name.of.var,\ 0:n))}}.
  \end{itemize}
\item
  \textbf{Understanding \texttt{PanelMatch} and \texttt{matched.set} objects}

  \begin{itemize}
  \item
    The \textbf{\texttt{PanelMatch} function} returns a \textbf{\texttt{PanelMatch} object}.
  \item
    The most crucial element within the \texttt{PanelMatch} object is the \textbf{matched.set object}.
  \item
    Within the \texttt{PanelMatch} object, the matched.set object will have names like att, art, or atc.
  \item
    If \textbf{\texttt{qoi\ =\ ate}}, there will be two matched.set objects: att and atc.
  \end{itemize}
\item
  \textbf{Matched.set Object Details}

  \begin{itemize}
  \item
    matched.set is a named list with added attributes.
  \item
    Attributes include:

    \begin{itemize}
    \item
      Lag
    \item
      Names of treatment
    \item
      Unit and time variables
    \end{itemize}
  \item
    Each list entry represents a matched set of treated and control units.
  \item
    Naming follows a structure: \textbf{\texttt{{[}id\ variable{]}.{[}time\ variable{]}}}.
  \item
    Each list element is a vector of control unit ids that match the treated unit mentioned in the element name.
  \item
    Since it's a matching method, weights are only given to the \textbf{\texttt{size.match}} most similar control units based on distance calculations.
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# PanelMatch without any refinement}
\NormalTok{PM.results.none }\OtherTok{\textless{}{-}}
    \FunctionTok{PanelMatch}\NormalTok{(}
        \AttributeTok{lag =} \DecValTok{4}\NormalTok{,}
        \AttributeTok{time.id =} \StringTok{"year"}\NormalTok{,}
        \AttributeTok{unit.id =} \StringTok{"wbcode2"}\NormalTok{,}
        \AttributeTok{treatment =} \StringTok{"dem"}\NormalTok{,}
        \AttributeTok{refinement.method =} \StringTok{"none"}\NormalTok{,}
        \AttributeTok{data =}\NormalTok{ dem,}
        \AttributeTok{match.missing =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{size.match =} \DecValTok{5}\NormalTok{,}
        \AttributeTok{qoi =} \StringTok{"att"}\NormalTok{,}
        \AttributeTok{outcome.var =} \StringTok{"y"}\NormalTok{,}
        \AttributeTok{lead =} \DecValTok{0}\SpecialCharTok{:}\DecValTok{4}\NormalTok{,}
        \AttributeTok{forbid.treatment.reversal =} \ConstantTok{FALSE}\NormalTok{,}
        \AttributeTok{use.diagonal.variance.matrix =} \ConstantTok{TRUE}
\NormalTok{    )}

\CommentTok{\# Extract the matched.set object}
\NormalTok{msets.none }\OtherTok{\textless{}{-}}\NormalTok{ PM.results.none}\SpecialCharTok{$}\NormalTok{att}

\CommentTok{\# PanelMatch with refinement}
\NormalTok{PM.results.maha }\OtherTok{\textless{}{-}}
    \FunctionTok{PanelMatch}\NormalTok{(}
        \AttributeTok{lag =} \DecValTok{4}\NormalTok{,}
        \AttributeTok{time.id =} \StringTok{"year"}\NormalTok{,}
        \AttributeTok{unit.id =} \StringTok{"wbcode2"}\NormalTok{,}
        \AttributeTok{treatment =} \StringTok{"dem"}\NormalTok{,}
        \AttributeTok{refinement.method =} \StringTok{"mahalanobis"}\NormalTok{, }\CommentTok{\# use Mahalanobis distance}
        \AttributeTok{data =}\NormalTok{ dem,}
        \AttributeTok{match.missing =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{covs.formula =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ tradewb,}
        \AttributeTok{size.match =} \DecValTok{5}\NormalTok{,}
        \AttributeTok{qoi =} \StringTok{"att"}\NormalTok{ ,}
        \AttributeTok{outcome.var =} \StringTok{"y"}\NormalTok{,}
        \AttributeTok{lead =} \DecValTok{0}\SpecialCharTok{:}\DecValTok{4}\NormalTok{,}
        \AttributeTok{forbid.treatment.reversal =} \ConstantTok{FALSE}\NormalTok{,}
        \AttributeTok{use.diagonal.variance.matrix =} \ConstantTok{TRUE}
\NormalTok{    )}
\NormalTok{msets.maha }\OtherTok{\textless{}{-}}\NormalTok{ PM.results.maha}\SpecialCharTok{$}\NormalTok{att}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# these 2 should be identical because weights are not shown}
\NormalTok{msets.none }\SpecialCharTok{|\textgreater{}} \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{}   wbcode2 year matched.set.size}
\CommentTok{\#\textgreater{} 1       4 1992               74}
\CommentTok{\#\textgreater{} 2       4 1997                2}
\CommentTok{\#\textgreater{} 3       6 1973               63}
\CommentTok{\#\textgreater{} 4       6 1983               73}
\CommentTok{\#\textgreater{} 5       7 1991               81}
\CommentTok{\#\textgreater{} 6       7 1998                1}
\NormalTok{msets.maha }\SpecialCharTok{|\textgreater{}} \FunctionTok{head}\NormalTok{()}
\CommentTok{\#\textgreater{}   wbcode2 year matched.set.size}
\CommentTok{\#\textgreater{} 1       4 1992               74}
\CommentTok{\#\textgreater{} 2       4 1997                2}
\CommentTok{\#\textgreater{} 3       6 1973               63}
\CommentTok{\#\textgreater{} 4       6 1983               73}
\CommentTok{\#\textgreater{} 5       7 1991               81}
\CommentTok{\#\textgreater{} 6       7 1998                1}
\CommentTok{\# summary(msets.none)}
\CommentTok{\# summary(msets.maha)}
\end{Highlighting}
\end{Shaded}

\textbf{Visualizing Matched Sets with the plot method}

\begin{itemize}
\item
  Users can visualize the distribution of the matched set sizes.
\item
  A red line, by default, indicates the count of matched sets where treated units had no matching control units (i.e., empty matched sets).
\item
  Plot adjustments can be made using \textbf{\texttt{graphics::plot}}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(msets.none)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-19-1} \end{center}

\textbf{Comparing Methods of Refinement}

\begin{itemize}
\item
  Users are encouraged to:

  \begin{itemize}
  \item
    Use substantive knowledge for experimentation and evaluation.
  \item
    Consider the following when configuring \texttt{PanelMatch}:

    \begin{enumerate}
    \def\labelenumi{\arabic{enumi}.}
    \item
      The number of matched sets.
    \item
      The number of controls matched to each treated unit.
    \item
      Achieving covariate balance.
    \end{enumerate}
  \item
    \textbf{Note}: Large numbers of small matched sets can lead to larger standard errors during the estimation stage.
  \item
    Covariates that aren't well balanced can lead to undesirable comparisons between treated and control units.
  \item
    Aspects to consider include:

    \begin{itemize}
    \item
      Refinement method.
    \item
      Variables for weight calculation.
    \item
      Size of the lag window.
    \item
      Procedures for addressing missing data (refer to \textbf{\texttt{match.missing}} and \textbf{\texttt{listwise.delete}} arguments).
    \item
      Maximum size of matched sets (for matching methods).
    \end{itemize}
  \end{itemize}
\item
  \textbf{Supportive Features:}

  \begin{itemize}
  \item
    \textbf{\texttt{print}}, \textbf{\texttt{plot}}, and \textbf{\texttt{summary}} methods assist in understanding matched sets and their sizes.
  \item
    \textbf{\texttt{get\_covariate\_balance}} helps evaluate covariate balance:

    \begin{itemize}
    \tightlist
    \item
      Lower values in the covariate balance calculations are preferred.
    \end{itemize}
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PM.results.none }\OtherTok{\textless{}{-}}
    \FunctionTok{PanelMatch}\NormalTok{(}
        \AttributeTok{lag =} \DecValTok{4}\NormalTok{,}
        \AttributeTok{time.id =} \StringTok{"year"}\NormalTok{,}
        \AttributeTok{unit.id =} \StringTok{"wbcode2"}\NormalTok{,}
        \AttributeTok{treatment =} \StringTok{"dem"}\NormalTok{,}
        \AttributeTok{refinement.method =} \StringTok{"none"}\NormalTok{,}
        \AttributeTok{data =}\NormalTok{ dem,}
        \AttributeTok{match.missing =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{size.match =} \DecValTok{5}\NormalTok{,}
        \AttributeTok{qoi =} \StringTok{"att"}\NormalTok{,}
        \AttributeTok{outcome.var =} \StringTok{"y"}\NormalTok{,}
        \AttributeTok{lead =} \DecValTok{0}\SpecialCharTok{:}\DecValTok{4}\NormalTok{,}
        \AttributeTok{forbid.treatment.reversal =} \ConstantTok{FALSE}\NormalTok{,}
        \AttributeTok{use.diagonal.variance.matrix =} \ConstantTok{TRUE}
\NormalTok{    )}
\NormalTok{PM.results.maha }\OtherTok{\textless{}{-}}
    \FunctionTok{PanelMatch}\NormalTok{(}
        \AttributeTok{lag =} \DecValTok{4}\NormalTok{,}
        \AttributeTok{time.id =} \StringTok{"year"}\NormalTok{,}
        \AttributeTok{unit.id =} \StringTok{"wbcode2"}\NormalTok{,}
        \AttributeTok{treatment =} \StringTok{"dem"}\NormalTok{,}
        \AttributeTok{refinement.method =} \StringTok{"mahalanobis"}\NormalTok{,}
        \AttributeTok{data =}\NormalTok{ dem,}
        \AttributeTok{match.missing =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{covs.formula =} \SpecialCharTok{\textasciitilde{}} \FunctionTok{I}\NormalTok{(}\FunctionTok{lag}\NormalTok{(tradewb, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{)) }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(}\FunctionTok{lag}\NormalTok{(y, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{)),}
        \AttributeTok{size.match =} \DecValTok{5}\NormalTok{,}
        \AttributeTok{qoi =} \StringTok{"att"}\NormalTok{,}
        \AttributeTok{outcome.var =} \StringTok{"y"}\NormalTok{,}
        \AttributeTok{lead =} \DecValTok{0}\SpecialCharTok{:}\DecValTok{4}\NormalTok{,}
        \AttributeTok{forbid.treatment.reversal =} \ConstantTok{FALSE}\NormalTok{,}
        \AttributeTok{use.diagonal.variance.matrix =} \ConstantTok{TRUE}
\NormalTok{    )}

\CommentTok{\# listwise deletion used for missing data}
\NormalTok{PM.results.listwise }\OtherTok{\textless{}{-}}
    \FunctionTok{PanelMatch}\NormalTok{(}
        \AttributeTok{lag =} \DecValTok{4}\NormalTok{,}
        \AttributeTok{time.id =} \StringTok{"year"}\NormalTok{,}
        \AttributeTok{unit.id =} \StringTok{"wbcode2"}\NormalTok{,}
        \AttributeTok{treatment =} \StringTok{"dem"}\NormalTok{,}
        \AttributeTok{refinement.method =} \StringTok{"mahalanobis"}\NormalTok{,}
        \AttributeTok{data =}\NormalTok{ dem,}
        \AttributeTok{match.missing =} \ConstantTok{FALSE}\NormalTok{,}
        \AttributeTok{listwise.delete =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{covs.formula =} \SpecialCharTok{\textasciitilde{}} \FunctionTok{I}\NormalTok{(}\FunctionTok{lag}\NormalTok{(tradewb, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{)) }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(}\FunctionTok{lag}\NormalTok{(y, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{)),}
        \AttributeTok{size.match =} \DecValTok{5}\NormalTok{,}
        \AttributeTok{qoi =} \StringTok{"att"}\NormalTok{,}
        \AttributeTok{outcome.var =} \StringTok{"y"}\NormalTok{,}
        \AttributeTok{lead =} \DecValTok{0}\SpecialCharTok{:}\DecValTok{4}\NormalTok{,}
        \AttributeTok{forbid.treatment.reversal =} \ConstantTok{FALSE}\NormalTok{,}
        \AttributeTok{use.diagonal.variance.matrix =} \ConstantTok{TRUE}
\NormalTok{    )}

\CommentTok{\# propensity score based weighting method}
\NormalTok{PM.results.ps.weight }\OtherTok{\textless{}{-}}
    \FunctionTok{PanelMatch}\NormalTok{(}
        \AttributeTok{lag =} \DecValTok{4}\NormalTok{,}
        \AttributeTok{time.id =} \StringTok{"year"}\NormalTok{,}
        \AttributeTok{unit.id =} \StringTok{"wbcode2"}\NormalTok{,}
        \AttributeTok{treatment =} \StringTok{"dem"}\NormalTok{,}
        \AttributeTok{refinement.method =} \StringTok{"ps.weight"}\NormalTok{,}
        \AttributeTok{data =}\NormalTok{ dem,}
        \AttributeTok{match.missing =} \ConstantTok{FALSE}\NormalTok{,}
        \AttributeTok{listwise.delete =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{covs.formula =} \SpecialCharTok{\textasciitilde{}} \FunctionTok{I}\NormalTok{(}\FunctionTok{lag}\NormalTok{(tradewb, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{)) }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(}\FunctionTok{lag}\NormalTok{(y, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{)),}
        \AttributeTok{size.match =} \DecValTok{5}\NormalTok{,}
        \AttributeTok{qoi =} \StringTok{"att"}\NormalTok{,}
        \AttributeTok{outcome.var =} \StringTok{"y"}\NormalTok{,}
        \AttributeTok{lead =} \DecValTok{0}\SpecialCharTok{:}\DecValTok{4}\NormalTok{,}
        \AttributeTok{forbid.treatment.reversal =} \ConstantTok{FALSE}
\NormalTok{    )}

\FunctionTok{get\_covariate\_balance}\NormalTok{(}
\NormalTok{    PM.results.none}\SpecialCharTok{$}\NormalTok{att,}
    \AttributeTok{data =}\NormalTok{ dem,}
    \AttributeTok{covariates =} \FunctionTok{c}\NormalTok{(}\StringTok{"tradewb"}\NormalTok{, }\StringTok{"y"}\NormalTok{),}
    \AttributeTok{plot =} \ConstantTok{FALSE}
\NormalTok{)}
\CommentTok{\#\textgreater{}         tradewb            y}
\CommentTok{\#\textgreater{} t\_4 {-}0.07245466  0.291871990}
\CommentTok{\#\textgreater{} t\_3 {-}0.20930129  0.208654876}
\CommentTok{\#\textgreater{} t\_2 {-}0.24425207  0.107736647}
\CommentTok{\#\textgreater{} t\_1 {-}0.10806125 {-}0.004950238}

\FunctionTok{get\_covariate\_balance}\NormalTok{(}
\NormalTok{    PM.results.maha}\SpecialCharTok{$}\NormalTok{att,}
    \AttributeTok{data =}\NormalTok{ dem,}
    \AttributeTok{covariates =} \FunctionTok{c}\NormalTok{(}\StringTok{"tradewb"}\NormalTok{, }\StringTok{"y"}\NormalTok{),}
    \AttributeTok{plot =} \ConstantTok{FALSE}
\NormalTok{)}
\CommentTok{\#\textgreater{}         tradewb          y}
\CommentTok{\#\textgreater{} t\_4  0.04558637 0.09701606}
\CommentTok{\#\textgreater{} t\_3 {-}0.03312750 0.10844046}
\CommentTok{\#\textgreater{} t\_2 {-}0.01396793 0.08890753}
\CommentTok{\#\textgreater{} t\_1  0.10474894 0.06618865}


\FunctionTok{get\_covariate\_balance}\NormalTok{(}
\NormalTok{    PM.results.listwise}\SpecialCharTok{$}\NormalTok{att,}
    \AttributeTok{data =}\NormalTok{ dem,}
    \AttributeTok{covariates =} \FunctionTok{c}\NormalTok{(}\StringTok{"tradewb"}\NormalTok{, }\StringTok{"y"}\NormalTok{),}
    \AttributeTok{plot =} \ConstantTok{FALSE}
\NormalTok{)}
\CommentTok{\#\textgreater{}         tradewb          y}
\CommentTok{\#\textgreater{} t\_4  0.05634922 0.05223623}
\CommentTok{\#\textgreater{} t\_3 {-}0.01104797 0.05217896}
\CommentTok{\#\textgreater{} t\_2  0.01411473 0.03094133}
\CommentTok{\#\textgreater{} t\_1  0.06850180 0.02092209}

\FunctionTok{get\_covariate\_balance}\NormalTok{(}
\NormalTok{    PM.results.ps.weight}\SpecialCharTok{$}\NormalTok{att,}
    \AttributeTok{data =}\NormalTok{ dem,}
    \AttributeTok{covariates =} \FunctionTok{c}\NormalTok{(}\StringTok{"tradewb"}\NormalTok{, }\StringTok{"y"}\NormalTok{),}
    \AttributeTok{plot =} \ConstantTok{FALSE}
\NormalTok{)}
\CommentTok{\#\textgreater{}         tradewb          y}
\CommentTok{\#\textgreater{} t\_4 0.014362590 0.04035905}
\CommentTok{\#\textgreater{} t\_3 0.005529734 0.04188731}
\CommentTok{\#\textgreater{} t\_2 0.009410044 0.04195008}
\CommentTok{\#\textgreater{} t\_1 0.027907540 0.03975173}
\end{Highlighting}
\end{Shaded}

\textbf{get\_covariate\_balance Function Options:}

\begin{itemize}
\item
  Allows for the generation of plots displaying covariate balance using \textbf{\texttt{plot\ =\ TRUE}}.
\item
  Plots can be customized using arguments typically used with the base R \textbf{\texttt{plot}} method.
\item
  Option to set \textbf{\texttt{use.equal.weights\ =\ TRUE}} for:

  \begin{itemize}
  \item
    Obtaining the balance of unrefined sets.
  \item
    Facilitating understanding of the refinement's impact.
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Use equal weights}
\FunctionTok{get\_covariate\_balance}\NormalTok{(}
\NormalTok{    PM.results.ps.weight}\SpecialCharTok{$}\NormalTok{att,}
    \AttributeTok{data =}\NormalTok{ dem,}
    \AttributeTok{use.equal.weights =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{covariates =} \FunctionTok{c}\NormalTok{(}\StringTok{"tradewb"}\NormalTok{, }\StringTok{"y"}\NormalTok{),}
    \AttributeTok{plot =} \ConstantTok{TRUE}\NormalTok{,}
    \CommentTok{\# visualize by setting plot to TRUE}
    \AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-21-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Compare covariate balance to refined sets}
\CommentTok{\# See large improvement in balance}
\FunctionTok{get\_covariate\_balance}\NormalTok{(}
\NormalTok{    PM.results.ps.weight}\SpecialCharTok{$}\NormalTok{att,}
    \AttributeTok{data =}\NormalTok{ dem,}
    \AttributeTok{covariates =} \FunctionTok{c}\NormalTok{(}\StringTok{"tradewb"}\NormalTok{, }\StringTok{"y"}\NormalTok{),}
    \AttributeTok{plot =} \ConstantTok{TRUE}\NormalTok{,}
    \CommentTok{\# visualize by setting plot to TRUE}
    \AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-21-2} \end{center}

\begin{Shaded}
\begin{Highlighting}[]


\FunctionTok{balance\_scatter}\NormalTok{(}
    \AttributeTok{matched\_set\_list =} \FunctionTok{list}\NormalTok{(PM.results.maha}\SpecialCharTok{$}\NormalTok{att,}
\NormalTok{                            PM.results.ps.weight}\SpecialCharTok{$}\NormalTok{att),}
    \AttributeTok{data =}\NormalTok{ dem,}
    \AttributeTok{covariates =} \FunctionTok{c}\NormalTok{(}\StringTok{"y"}\NormalTok{, }\StringTok{"tradewb"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-21-3} \end{center}

\textbf{\texttt{PanelEstimate}}

\begin{itemize}
\item
  \textbf{Standard Error Calculation Methods}

  \begin{itemize}
  \item
    There are different methods available:

    \begin{itemize}
    \item
      \textbf{Bootstrap} (default method with 1000 iterations).
    \item
      \textbf{Conditional}: Assumes independence across units, but not time.
    \item
      \textbf{Unconditional}: Doesn't make assumptions of independence across units or time.
    \end{itemize}
  \item
    For \textbf{\texttt{qoi}} values set to \texttt{att}, \texttt{art}, or \texttt{atc} \citep{imai2021matching}:

    \begin{itemize}
    \tightlist
    \item
      You can use analytical methods for calculating standard errors, which include both ``conditional'' and ``unconditional'' methods.
    \end{itemize}
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PE.results }\OtherTok{\textless{}{-}} \FunctionTok{PanelEstimate}\NormalTok{(}
    \AttributeTok{sets              =}\NormalTok{ PM.results.ps.weight,}
    \AttributeTok{data              =}\NormalTok{ dem,}
    \AttributeTok{se.method         =} \StringTok{"bootstrap"}\NormalTok{,}
    \AttributeTok{number.iterations =} \DecValTok{1000}\NormalTok{,}
    \AttributeTok{confidence.level  =}\NormalTok{ .}\DecValTok{95}
\NormalTok{)}

\CommentTok{\# point estimates}
\NormalTok{PE.results[[}\StringTok{"estimates"}\NormalTok{]]}
\CommentTok{\#\textgreater{}       t+0       t+1       t+2       t+3       t+4 }
\CommentTok{\#\textgreater{} 0.2609565 0.9630847 1.2851017 1.7370930 1.4871846}

\CommentTok{\# standard errors}
\NormalTok{PE.results[[}\StringTok{"standard.error"}\NormalTok{]]}
\CommentTok{\#\textgreater{}       t+0       t+1       t+2       t+3       t+4 }
\CommentTok{\#\textgreater{} 0.6512004 1.0719451 1.4529810 1.8020579 2.2214071}


\CommentTok{\# use conditional method}
\NormalTok{PE.results }\OtherTok{\textless{}{-}} \FunctionTok{PanelEstimate}\NormalTok{(}
    \AttributeTok{sets             =}\NormalTok{ PM.results.ps.weight,}
    \AttributeTok{data             =}\NormalTok{ dem,}
    \AttributeTok{se.method        =} \StringTok{"conditional"}\NormalTok{,}
    \AttributeTok{confidence.level =}\NormalTok{ .}\DecValTok{95}
\NormalTok{)}

\CommentTok{\# point estimates}
\NormalTok{PE.results[[}\StringTok{"estimates"}\NormalTok{]]}
\CommentTok{\#\textgreater{}       t+0       t+1       t+2       t+3       t+4 }
\CommentTok{\#\textgreater{} 0.2609565 0.9630847 1.2851017 1.7370930 1.4871846}

\CommentTok{\# standard errors}
\NormalTok{PE.results[[}\StringTok{"standard.error"}\NormalTok{]]}
\CommentTok{\#\textgreater{}       t+0       t+1       t+2       t+3       t+4 }
\CommentTok{\#\textgreater{} 0.4844805 0.8170604 1.1171942 1.4116879 1.7172143}

\FunctionTok{summary}\NormalTok{(PE.results)}
\CommentTok{\#\textgreater{} Weighted Difference{-}in{-}Differences with Propensity Score}
\CommentTok{\#\textgreater{} Matches created with 4 lags}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Standard errors computed with conditional  method}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Estimate of Average Treatment Effect on the Treated (ATT) by Period:}
\CommentTok{\#\textgreater{} $summary}
\CommentTok{\#\textgreater{}      estimate std.error       2.5\%    97.5\%}
\CommentTok{\#\textgreater{} t+0 0.2609565 0.4844805 {-}0.6886078 1.210521}
\CommentTok{\#\textgreater{} t+1 0.9630847 0.8170604 {-}0.6383243 2.564494}
\CommentTok{\#\textgreater{} t+2 1.2851017 1.1171942 {-}0.9045586 3.474762}
\CommentTok{\#\textgreater{} t+3 1.7370930 1.4116879 {-}1.0297644 4.503950}
\CommentTok{\#\textgreater{} t+4 1.4871846 1.7172143 {-}1.8784937 4.852863}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $lag}
\CommentTok{\#\textgreater{} [1] 4}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $qoi}
\CommentTok{\#\textgreater{} [1] "att"}

\FunctionTok{plot}\NormalTok{(PE.results)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-22-1} \end{center}

\textbf{Moderating Variables}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# moderating variable}
\NormalTok{dem}\SpecialCharTok{$}\NormalTok{moderator }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{dem}\SpecialCharTok{$}\NormalTok{moderator }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(dem}\SpecialCharTok{$}\NormalTok{wbcode2 }\SpecialCharTok{\textgreater{}} \DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{)}

\NormalTok{PM.results }\OtherTok{\textless{}{-}}
    \FunctionTok{PanelMatch}\NormalTok{(}
        \AttributeTok{lag                          =} \DecValTok{4}\NormalTok{,}
        \AttributeTok{time.id                      =} \StringTok{"year"}\NormalTok{,}
        \AttributeTok{unit.id                      =} \StringTok{"wbcode2"}\NormalTok{,}
        \AttributeTok{treatment                    =} \StringTok{"dem"}\NormalTok{,}
        \AttributeTok{refinement.method            =} \StringTok{"mahalanobis"}\NormalTok{,}
        \AttributeTok{data                         =}\NormalTok{ dem,}
        \AttributeTok{match.missing                =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{covs.formula                 =} \SpecialCharTok{\textasciitilde{}} \FunctionTok{I}\NormalTok{(}\FunctionTok{lag}\NormalTok{(tradewb, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{)) }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(}\FunctionTok{lag}\NormalTok{(y, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{)),}
        \AttributeTok{size.match                   =} \DecValTok{5}\NormalTok{,}
        \AttributeTok{qoi                          =} \StringTok{"att"}\NormalTok{,}
        \AttributeTok{outcome.var                  =} \StringTok{"y"}\NormalTok{,}
        \AttributeTok{lead                         =} \DecValTok{0}\SpecialCharTok{:}\DecValTok{4}\NormalTok{,}
        \AttributeTok{forbid.treatment.reversal    =} \ConstantTok{FALSE}\NormalTok{,}
        \AttributeTok{use.diagonal.variance.matrix =} \ConstantTok{TRUE}
\NormalTok{    )}
\NormalTok{PE.results }\OtherTok{\textless{}{-}}
    \FunctionTok{PanelEstimate}\NormalTok{(}\AttributeTok{sets      =}\NormalTok{ PM.results,}
                  \AttributeTok{data      =}\NormalTok{ dem,}
                  \AttributeTok{moderator =} \StringTok{"moderator"}\NormalTok{)}

\CommentTok{\# Each element in the list corresponds to a level in the moderator}
\FunctionTok{plot}\NormalTok{(PE.results[[}\DecValTok{1}\NormalTok{]])}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-23-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\FunctionTok{plot}\NormalTok{(PE.results[[}\DecValTok{2}\NormalTok{]])}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-23-2} \end{center}

To write up for journal submission, you can follow the following report:

In this study, closely aligned with the research by \citep{acemoglu2019democracy}, two key effects of democracy on economic growth are estimated: the impact of democratization and that of authoritarian reversal. The treatment variable, \(X_{it}\), is defined to be one if country \(i\) is democratic in year \(t\), and zero otherwise.

The Average Treatment Effect for the Treated (ATT) under democratization is formulated as follows:

\[
\begin{aligned}
\delta(F, L) &= \mathbb{E} \left\{ Y_{i, t + F} (X_{it} = 1, X_{i, t - 1} = 0, \{X_{i,t-l}\}_{l=2}^L) \right. \\
&\left. - Y_{i, t + F} (X_{it} = 0, X_{i, t - 1} = 0, \{X_{i,t-l}\}_{l=2}^L) | X_{it} = 1, X_{i, t - 1} = 0 \right\}
\end{aligned}
\]

In this framework, the treated observations are countries that transition from an authoritarian regime \(X_{it-1} = 0\) to a democratic one \(X_{it} = 1\). The variable \(F\) represents the number of leads, denoting the time periods following the treatment, and \(L\) signifies the number of lags, indicating the time periods preceding the treatment.

The ATT under authoritarian reversal is given by:

\[
\begin{aligned}
&\mathbb{E} \left[ Y_{i, t + F} (X_{it} = 0, X_{i, t - 1} = 1, \{ X_{i, t - l}\}_{l=2}^L ) \right. \\
&\left. - Y_{i, t + F} (X_{it} = 1, X_{it-1} = 1, \{X_{i, t - l} \}_{l=2}^L ) | X_{it} = 0, X_{i, t - 1} = 1 \right]
\end{aligned}
\]

The ATT is calculated conditioning on 4 years of lags (\(L = 4\)) and up to 4 years following the policy change \(F = 1, 2, 3, 4\). Matched sets for each treated observation are constructed based on its treatment history, with the number of matched control units generally decreasing when considering a 4-year treatment history as compared to a 1-year history.

To enhance the quality of matched sets, methods such as Mahalanobis distance matching, propensity score matching, and propensity score weighting are utilized. These approaches enable us to evaluate the effectiveness of each refinement method. In the process of matching, we employ both up-to-five and up-to-ten matching to investigate how sensitive our empirical results are to the maximum number of allowed matches. For more information on the refinement process, please see the Web Appendix

\begin{quote}
The Mahalanobis distance is expressed through a specific formula. We aim to pair each treated unit with a maximum of \(J\) control units, permitting replacement, denoted as \(| \mathcal{M}_{it} \le J|\). The average Mahalanobis distance between a treated and each control unit over time is computed as:

\[ S_{it} (i') = \frac{1}{L} \sum_{l = 1}^L \sqrt{(\mathbf{V}_{i, t - l} - \mathbf{V}_{i', t -l})^T \mathbf{\Sigma}_{i, t - l}^{-1} (\mathbf{V}_{i, t - l} - \mathbf{V}_{i', t -l})} \]

For a matched control unit \(i' \in \mathcal{M}_{it}\), \(\mathbf{V}_{it'}\) represents the time-varying covariates to adjust for, and \(\mathbf{\Sigma}_{it'}\) is the sample covariance matrix for \(\mathbf{V}_{it'}\). Essentially, we calculate a standardized distance using time-varying covariates and average this across different time intervals.

In the context of propensity score matching, we employ a logistic regression model with balanced covariates to derive the propensity score. Defined as the conditional likelihood of treatment given pre-treatment covariates \citep{rosenbaum1983central}, the propensity score is estimated by first creating a data subset comprised of all treated and their matched control units from the same year. This logistic regression model is then fitted as follows:

\[ \begin{aligned} & e_{it} (\{\mathbf{U}_{i, t - l} \}^L_{l = 1}) \\ &= Pr(X_{it} = 1| \mathbf{U}_{i, t -1}, \ldots, \mathbf{U}_{i, t - L}) \\ &= \frac{1}{1 = \exp(- \sum_{l = 1}^L \beta_l^T \mathbf{U}_{i, t - l})} \end{aligned} \]

where \(\mathbf{U}_{it'} = (X_{it'}, \mathbf{V}_{it'}^T)^T\). Given this model, the estimated propensity score for all treated and matched control units is then computed. This enables the adjustment for lagged covariates via matching on the calculated propensity score, resulting in the following distance measure:

\[ S_{it} (i') = | \text{logit} \{ \hat{e}_{it} (\{ \mathbf{U}_{i, t - l}\}^L_{l = 1})\} - \text{logit} \{ \hat{e}_{i't}( \{ \mathbf{U}_{i', t - l} \}^L_{l = 1})\} | \]

Here, \(\hat{e}_{i't} (\{ \mathbf{U}_{i, t - l}\}^L_{l = 1})\) represents the estimated propensity score for each matched control unit \(i' \in \mathcal{M}_{it}\).

Once the distance measure \(S_{it} (i')\) has been determined for all control units in the original matched set, we fine-tune this set by selecting up to \(J\) closest control units, which meet a researcher-defined caliper constraint \(C\). All other control units receive zero weight. This results in a refined matched set for each treated unit \((i, t)\):

\[ \mathcal{M}_{it}^* = \{i' : i' \in \mathcal{M}_{it}, S_{it} (i') < C, S_{it} \le S_{it}^{(J)}\} \]

\(S_{it}^{(J)}\) is the \(J\)th smallest distance among the control units in the original set \(\mathcal{M}_{it}\).

For further refinement using weighting, a weight is assigned to each control unit \(i'\) in a matched set corresponding to a treated unit \((i, t)\), with greater weight accorded to more similar units. We utilize inverse propensity score weighting, based on the propensity score model mentioned earlier:

\[ w_{it}^{i'} \propto \frac{\hat{e}_{i't} (\{ \mathbf{U}_{i, t-l} \}^L_{l = 1} )}{1 - \hat{e}_{i't} (\{ \mathbf{U}_{i, t-l} \}^L_{l = 1} )} \]

In this model, \(\sum_{i' \in \mathcal{M}_{it}} w_{it}^{i'} = 1\) and \(w_{it}^{i'} = 0\) for \(i' \notin \mathcal{M}_{it}\). The model is fitted to the complete sample of treated and matched control units.
\end{quote}

\begin{quote}
Checking Covariate Balance A distinct advantage of the proposed methodology over regression methods is the ability it offers researchers to inspect the covariate balance between treated and matched control observations. This facilitates the evaluation of whether treated and matched control observations are comparable regarding observed confounders. To investigate the mean difference of each covariate (e.g., \(V_{it'j}\), representing the \(j\)-th variable in \(\mathbf{V}_{it'}\)) between the treated observation and its matched control observation at each pre-treatment time period (i.e., \(t' < t\)), we further standardize this difference. For any given pretreatment time period, we adjust by the standard deviation of each covariate across all treated observations in the dataset. Thus, the mean difference is quantified in terms of standard deviation units. Formally, for each treated observation \((i,t)\) where \(D_{it} = 1\), we define the covariate balance for variable \(j\) at the pretreatment time period \(t - l\) as: \begin{equation}
B_{it}(j, l) = \frac{V_{i, t- l,j}- \sum_{i' \in \mathcal{M}_{it}}w_{it}^{i'}V_{i', t-l,j}}{\sqrt{\frac{1}{N_1 - 1} \sum_{i'=1}^N \sum_{t' = L+1}^{T-F}D_{i't'}(V_{i', t'-l, j} - \bar{V}_{t' - l, j})^2}}
\label{eq:covbalance}
\end{equation} where \(N_1 = \sum_{i'= 1}^N \sum_{t' = L+1}^{T-F} D_{i't'}\) denotes the total number of treated observations and \(\bar{V}_{t-l,j} = \sum_{i=1}^N D_{i,t-l,j}/N\). We then aggregate this covariate balance measure across all treated observations for each covariate and pre-treatment time period: \begin{equation}
\bar{B}(j, l) = \frac{1}{N_1} \sum_{i=1}^N \sum_{t = L+ 1}^{T-F}D_{it} B_{it}(j,l)
\label{eq:aggbalance}
\end{equation} Lastly, we evaluate the balance of lagged outcome variables over several pre-treatment periods and that of time-varying covariates. This examination aids in assessing the validity of the parallel trend assumption integral to the DiD estimator justification.
\end{quote}

In Figure \ref{fig:balancescatter}, we demonstrate the enhancement of covariate balance thank to the refinement of matched sets. Each scatter plot contrasts the absolute standardized mean difference, as detailed in Equation \eqref{eq:aggbalance}, before (horizontal axis) and after (vertical axis) this refinement. Points below the 45-degree line indicate an improved standardized mean balance for certain time-varying covariates post-refinement. The majority of variables benefit from this refinement process. Notably, the propensity score weighting (bottom panel) shows the most significant improvement, whereas Mahalanobis matching (top panel) yields a more modest improvement.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(PanelMatch)}
\FunctionTok{library}\NormalTok{(causalverse)}

\NormalTok{runPanelMatch }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(method, lag, }\AttributeTok{size.match=}\ConstantTok{NULL}\NormalTok{, }\AttributeTok{qoi=}\StringTok{"att"}\NormalTok{) \{}
    
    \CommentTok{\# Default parameters for PanelMatch}
\NormalTok{    common.args }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
        \AttributeTok{lag =}\NormalTok{ lag,}
        \AttributeTok{time.id =} \StringTok{"year"}\NormalTok{,}
        \AttributeTok{unit.id =} \StringTok{"wbcode2"}\NormalTok{,}
        \AttributeTok{treatment =} \StringTok{"dem"}\NormalTok{,}
        \AttributeTok{data =}\NormalTok{ dem,}
        \AttributeTok{covs.formula =} \SpecialCharTok{\textasciitilde{}} \FunctionTok{I}\NormalTok{(}\FunctionTok{lag}\NormalTok{(tradewb, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{)) }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(}\FunctionTok{lag}\NormalTok{(y, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{)),}
        \AttributeTok{qoi =}\NormalTok{ qoi,}
        \AttributeTok{outcome.var =} \StringTok{"y"}\NormalTok{,}
        \AttributeTok{lead =} \DecValTok{0}\SpecialCharTok{:}\DecValTok{4}\NormalTok{,}
        \AttributeTok{forbid.treatment.reversal =} \ConstantTok{FALSE}\NormalTok{,}
        \AttributeTok{size.match =}\NormalTok{ size.match  }\CommentTok{\# setting size.match here for all methods}
\NormalTok{    )}
    
    \ControlFlowTok{if}\NormalTok{(method }\SpecialCharTok{==} \StringTok{"mahalanobis"}\NormalTok{) \{}
\NormalTok{        common.args}\SpecialCharTok{$}\NormalTok{refinement.method }\OtherTok{\textless{}{-}} \StringTok{"mahalanobis"}
\NormalTok{        common.args}\SpecialCharTok{$}\NormalTok{match.missing }\OtherTok{\textless{}{-}} \ConstantTok{TRUE}
\NormalTok{        common.args}\SpecialCharTok{$}\NormalTok{use.diagonal.variance.matrix }\OtherTok{\textless{}{-}} \ConstantTok{TRUE}
\NormalTok{    \} }\ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{(method }\SpecialCharTok{==} \StringTok{"ps.match"}\NormalTok{) \{}
\NormalTok{        common.args}\SpecialCharTok{$}\NormalTok{refinement.method }\OtherTok{\textless{}{-}} \StringTok{"ps.match"}
\NormalTok{        common.args}\SpecialCharTok{$}\NormalTok{match.missing }\OtherTok{\textless{}{-}} \ConstantTok{FALSE}
\NormalTok{        common.args}\SpecialCharTok{$}\NormalTok{listwise.delete }\OtherTok{\textless{}{-}} \ConstantTok{TRUE}
\NormalTok{    \} }\ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{(method }\SpecialCharTok{==} \StringTok{"ps.weight"}\NormalTok{) \{}
\NormalTok{        common.args}\SpecialCharTok{$}\NormalTok{refinement.method }\OtherTok{\textless{}{-}} \StringTok{"ps.weight"}
\NormalTok{        common.args}\SpecialCharTok{$}\NormalTok{match.missing }\OtherTok{\textless{}{-}} \ConstantTok{FALSE}
\NormalTok{        common.args}\SpecialCharTok{$}\NormalTok{listwise.delete }\OtherTok{\textless{}{-}} \ConstantTok{TRUE}
\NormalTok{    \}}
    
    \FunctionTok{return}\NormalTok{(}\FunctionTok{do.call}\NormalTok{(PanelMatch, common.args))}
\NormalTok{\}}

\NormalTok{methods }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"mahalanobis"}\NormalTok{, }\StringTok{"ps.match"}\NormalTok{, }\StringTok{"ps.weight"}\NormalTok{)}
\NormalTok{lags }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{)}
\NormalTok{sizes }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

You can either do it sequentailly

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res\_pm }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}

\ControlFlowTok{for}\NormalTok{(method }\ControlFlowTok{in}\NormalTok{ methods) \{}
    \ControlFlowTok{for}\NormalTok{(lag }\ControlFlowTok{in}\NormalTok{ lags) \{}
        \ControlFlowTok{for}\NormalTok{(size }\ControlFlowTok{in}\NormalTok{ sizes) \{}
\NormalTok{            name }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(method, }\StringTok{"."}\NormalTok{, lag, }\StringTok{"lag."}\NormalTok{, size, }\StringTok{"m"}\NormalTok{)}
\NormalTok{            res\_pm[[name]] }\OtherTok{\textless{}{-}} \FunctionTok{runPanelMatch}\NormalTok{(method, lag, size)}
\NormalTok{        \}}
\NormalTok{    \}}
\NormalTok{\}}

\CommentTok{\# Now, you can access res\_pm using res\_pm[["mahalanobis.1lag.5m"]] etc.}

\CommentTok{\# for treatment reversal}
\NormalTok{res\_pm\_rev }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}

\ControlFlowTok{for}\NormalTok{(method }\ControlFlowTok{in}\NormalTok{ methods) \{}
    \ControlFlowTok{for}\NormalTok{(lag }\ControlFlowTok{in}\NormalTok{ lags) \{}
        \ControlFlowTok{for}\NormalTok{(size }\ControlFlowTok{in}\NormalTok{ sizes) \{}
\NormalTok{            name }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(method, }\StringTok{"."}\NormalTok{, lag, }\StringTok{"lag."}\NormalTok{, size, }\StringTok{"m"}\NormalTok{)}
\NormalTok{            res\_pm\_rev[[name]] }\OtherTok{\textless{}{-}} \FunctionTok{runPanelMatch}\NormalTok{(method, lag, size, }\AttributeTok{qoi =} \StringTok{"art"}\NormalTok{)}
\NormalTok{        \}}
\NormalTok{    \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

or in parallel

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(foreach)}
\FunctionTok{library}\NormalTok{(doParallel)}
\FunctionTok{registerDoParallel}\NormalTok{(}\AttributeTok{cores =} \DecValTok{4}\NormalTok{)}
\CommentTok{\# Initialize an empty list to store results}
\NormalTok{res\_pm }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}

\CommentTok{\# Replace nested for{-}loops with foreach}
\NormalTok{results }\OtherTok{\textless{}{-}}
  \FunctionTok{foreach}\NormalTok{(}
    \AttributeTok{method =}\NormalTok{ methods,}
    \AttributeTok{.combine =} \StringTok{\textquotesingle{}c\textquotesingle{}}\NormalTok{,}
    \AttributeTok{.multicombine =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{.packages =} \FunctionTok{c}\NormalTok{(}\StringTok{"PanelMatch"}\NormalTok{, }\StringTok{"causalverse"}\NormalTok{)}
\NormalTok{  ) }\SpecialCharTok{\%dopar\%}\NormalTok{ \{}
\NormalTok{    tmp }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}
    \ControlFlowTok{for}\NormalTok{ (lag }\ControlFlowTok{in}\NormalTok{ lags) \{}
      \ControlFlowTok{for}\NormalTok{ (size }\ControlFlowTok{in}\NormalTok{ sizes) \{}
\NormalTok{        name }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(method, }\StringTok{"."}\NormalTok{, lag, }\StringTok{"lag."}\NormalTok{, size, }\StringTok{"m"}\NormalTok{)}
\NormalTok{        tmp[[name]] }\OtherTok{\textless{}{-}} \FunctionTok{runPanelMatch}\NormalTok{(method, lag, size)}
\NormalTok{      \}}
\NormalTok{    \}}
\NormalTok{    tmp}
\NormalTok{  \}}

\CommentTok{\# Collate results}
\ControlFlowTok{for}\NormalTok{ (name }\ControlFlowTok{in} \FunctionTok{names}\NormalTok{(results)) \{}
\NormalTok{  res\_pm[[name]] }\OtherTok{\textless{}{-}}\NormalTok{ results[[name]]}
\NormalTok{\}}

\CommentTok{\# Treatment reversal}
\CommentTok{\# Initialize an empty list to store results}
\NormalTok{res\_pm\_rev }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}

\CommentTok{\# Replace nested for{-}loops with foreach}
\NormalTok{results\_rev }\OtherTok{\textless{}{-}}
  \FunctionTok{foreach}\NormalTok{(}
    \AttributeTok{method =}\NormalTok{ methods,}
    \AttributeTok{.combine =} \StringTok{\textquotesingle{}c\textquotesingle{}}\NormalTok{,}
    \AttributeTok{.multicombine =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{.packages =} \FunctionTok{c}\NormalTok{(}\StringTok{"PanelMatch"}\NormalTok{, }\StringTok{"causalverse"}\NormalTok{)}
\NormalTok{  ) }\SpecialCharTok{\%dopar\%}\NormalTok{ \{}
\NormalTok{    tmp }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}
    \ControlFlowTok{for}\NormalTok{ (lag }\ControlFlowTok{in}\NormalTok{ lags) \{}
      \ControlFlowTok{for}\NormalTok{ (size }\ControlFlowTok{in}\NormalTok{ sizes) \{}
\NormalTok{        name }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(method, }\StringTok{"."}\NormalTok{, lag, }\StringTok{"lag."}\NormalTok{, size, }\StringTok{"m"}\NormalTok{)}
\NormalTok{        tmp[[name]] }\OtherTok{\textless{}{-}}
          \FunctionTok{runPanelMatch}\NormalTok{(method, lag, size, }\AttributeTok{qoi =} \StringTok{"art"}\NormalTok{)}
\NormalTok{      \}}
\NormalTok{    \}}
\NormalTok{    tmp}
\NormalTok{  \}}

\CommentTok{\# Collate results}
\ControlFlowTok{for}\NormalTok{ (name }\ControlFlowTok{in} \FunctionTok{names}\NormalTok{(results\_rev)) \{}
\NormalTok{  res\_pm\_rev[[name]] }\OtherTok{\textless{}{-}}\NormalTok{ results\_rev[[name]]}
\NormalTok{\}}


\FunctionTok{stopImplicitCluster}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(gridExtra)}

\CommentTok{\# Updated plotting function}
\NormalTok{create\_balance\_plot }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(method, lag, sizes, res\_pm, dem) \{}
\NormalTok{    matched\_set\_lists }\OtherTok{\textless{}{-}} \FunctionTok{lapply}\NormalTok{(sizes, }\ControlFlowTok{function}\NormalTok{(size) \{}
\NormalTok{        res\_pm[[}\FunctionTok{paste0}\NormalTok{(method, }\StringTok{"."}\NormalTok{, lag, }\StringTok{"lag."}\NormalTok{, size, }\StringTok{"m"}\NormalTok{)]]}\SpecialCharTok{$}\NormalTok{att}
\NormalTok{    \})}
    
    \FunctionTok{return}\NormalTok{(}
        \FunctionTok{balance\_scatter\_custom}\NormalTok{(}
            \AttributeTok{matched\_set\_list =}\NormalTok{ matched\_set\_lists,}
            \AttributeTok{legend.title =} \StringTok{"Possible Matches"}\NormalTok{,}
            \AttributeTok{set.names =} \FunctionTok{as.character}\NormalTok{(sizes),}
            \AttributeTok{legend.position =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\FloatTok{0.8}\NormalTok{),}
            
            \CommentTok{\# for compiled plot, you don\textquotesingle{}t need x,y, or main labs}
            \AttributeTok{x.axis.label =} \StringTok{""}\NormalTok{,}
            \AttributeTok{y.axis.label =} \StringTok{""}\NormalTok{,}
            \AttributeTok{main =} \StringTok{""}\NormalTok{,}
            \AttributeTok{data =}\NormalTok{ dem,}
            \AttributeTok{dot.size =} \DecValTok{5}\NormalTok{,}
            \CommentTok{\# show.legend = F,}
            \AttributeTok{them\_use =}\NormalTok{ causalverse}\SpecialCharTok{::}\FunctionTok{ama\_theme}\NormalTok{(}\AttributeTok{base\_size =} \DecValTok{32}\NormalTok{),}
            \AttributeTok{covariates =} \FunctionTok{c}\NormalTok{(}\StringTok{"y"}\NormalTok{, }\StringTok{"tradewb"}\NormalTok{)}
\NormalTok{        )}
\NormalTok{    )}
\NormalTok{\}}

\NormalTok{plots }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}

\ControlFlowTok{for}\NormalTok{ (method }\ControlFlowTok{in}\NormalTok{ methods) \{}
    \ControlFlowTok{for}\NormalTok{ (lag }\ControlFlowTok{in}\NormalTok{ lags) \{}
\NormalTok{        plots[[}\FunctionTok{paste0}\NormalTok{(method, }\StringTok{"."}\NormalTok{, lag, }\StringTok{"lag"}\NormalTok{)]] }\OtherTok{\textless{}{-}}
            \FunctionTok{create\_balance\_plot}\NormalTok{(method, lag, sizes, res\_pm, dem)}
\NormalTok{    \}}
\NormalTok{\}}

\CommentTok{\# \# Arranging plots in a 3x2 grid}
\CommentTok{\# grid.arrange(plots[["mahalanobis.1lag"]],}
\CommentTok{\#              plots[["mahalanobis.4lag"]],}
\CommentTok{\#              plots[["ps.match.1lag"]],}
\CommentTok{\#              plots[["ps.match.4lag"]],}
\CommentTok{\#              plots[["ps.weight.1lag"]],}
\CommentTok{\#              plots[["ps.weight.4lag"]],}
\CommentTok{\#              ncol=2, nrow=3)}


\CommentTok{\# Standardized Mean Difference of Covariates}
\FunctionTok{library}\NormalTok{(gridExtra)}
\FunctionTok{library}\NormalTok{(grid)}

\CommentTok{\# Create column and row labels using textGrob}
\NormalTok{col\_labels }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"1{-}year Lag"}\NormalTok{, }\StringTok{"4{-}year Lag"}\NormalTok{)}
\NormalTok{row\_labels }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Maha Matching"}\NormalTok{, }\StringTok{"PS Matching"}\NormalTok{, }\StringTok{"PS Weigthing"}\NormalTok{)}

\NormalTok{major.axes.fontsize }\OtherTok{=} \DecValTok{40}
\NormalTok{minor.axes.fontsize }\OtherTok{=} \DecValTok{30}

\FunctionTok{png}\NormalTok{(}
    \FunctionTok{file.path}\NormalTok{(}\FunctionTok{getwd}\NormalTok{(), }\StringTok{"images"}\NormalTok{, }\StringTok{"did\_balance\_scatter.png"}\NormalTok{),}
    \AttributeTok{width =} \DecValTok{1200}\NormalTok{,}
    \AttributeTok{height =} \DecValTok{1000}
\NormalTok{)}

\CommentTok{\# Create a list{-}of{-}lists, where each inner list represents a row}
\NormalTok{grid\_list }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
    \FunctionTok{list}\NormalTok{(}
        \FunctionTok{nullGrob}\NormalTok{(),}
        \FunctionTok{textGrob}\NormalTok{(col\_labels[}\DecValTok{1}\NormalTok{], }\AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ minor.axes.fontsize)),}
        \FunctionTok{textGrob}\NormalTok{(col\_labels[}\DecValTok{2}\NormalTok{], }\AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ minor.axes.fontsize))}
\NormalTok{    ),}
    
    \FunctionTok{list}\NormalTok{(}\FunctionTok{textGrob}\NormalTok{(}
\NormalTok{        row\_labels[}\DecValTok{1}\NormalTok{],}
        \AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ minor.axes.fontsize),}
        \AttributeTok{rot =} \DecValTok{90}
\NormalTok{    ), plots[[}\StringTok{"mahalanobis.1lag"}\NormalTok{]], plots[[}\StringTok{"mahalanobis.4lag"}\NormalTok{]]),}
    
    \FunctionTok{list}\NormalTok{(}\FunctionTok{textGrob}\NormalTok{(}
\NormalTok{        row\_labels[}\DecValTok{2}\NormalTok{],}
        \AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ minor.axes.fontsize),}
        \AttributeTok{rot =} \DecValTok{90}
\NormalTok{    ), plots[[}\StringTok{"ps.match.1lag"}\NormalTok{]], plots[[}\StringTok{"ps.match.4lag"}\NormalTok{]]),}
    
    \FunctionTok{list}\NormalTok{(}\FunctionTok{textGrob}\NormalTok{(}
\NormalTok{        row\_labels[}\DecValTok{3}\NormalTok{],}
        \AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ minor.axes.fontsize),}
        \AttributeTok{rot =} \DecValTok{90}
\NormalTok{    ), plots[[}\StringTok{"ps.weight.1lag"}\NormalTok{]], plots[[}\StringTok{"ps.weight.4lag"}\NormalTok{]])}
\NormalTok{)}

\CommentTok{\# "Flatten" the list{-}of{-}lists into a single list of grobs}
\NormalTok{grobs }\OtherTok{\textless{}{-}} \FunctionTok{do.call}\NormalTok{(c, grid\_list)}

\FunctionTok{grid.arrange}\NormalTok{(}
    \AttributeTok{grobs =}\NormalTok{ grobs,}
    \AttributeTok{ncol =} \DecValTok{3}\NormalTok{,}
    \AttributeTok{nrow =} \DecValTok{4}\NormalTok{,}
    \AttributeTok{widths =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.15}\NormalTok{, }\FloatTok{0.42}\NormalTok{, }\FloatTok{0.42}\NormalTok{),}
    \AttributeTok{heights =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.15}\NormalTok{, }\FloatTok{0.28}\NormalTok{, }\FloatTok{0.28}\NormalTok{, }\FloatTok{0.28}\NormalTok{)}
\NormalTok{)}

\FunctionTok{grid.text}\NormalTok{(}
    \StringTok{"Before Refinement"}\NormalTok{,}
    \AttributeTok{x =} \FloatTok{0.5}\NormalTok{,}
    \AttributeTok{y =} \FloatTok{0.03}\NormalTok{,}
    \AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ major.axes.fontsize)}
\NormalTok{)}
\FunctionTok{grid.text}\NormalTok{(}
    \StringTok{"After Refinement"}\NormalTok{,}
    \AttributeTok{x =} \FloatTok{0.03}\NormalTok{,}
    \AttributeTok{y =} \FloatTok{0.5}\NormalTok{,}
    \AttributeTok{rot =} \DecValTok{90}\NormalTok{,}
    \AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ major.axes.fontsize)}
\NormalTok{)}
\FunctionTok{dev.off}\NormalTok{()}
\CommentTok{\#\textgreater{} pdf }
\CommentTok{\#\textgreater{}   2}
\end{Highlighting}
\end{Shaded}

Note: Scatter plots display the standardized mean difference of each covariate \(j\) and lag year \(l\) as defined in Equation \eqref{eq:aggbalance} before (x-axis) and after (y-axis) matched set refinement. Each plot includes varying numbers of possible matches for each matching method. Rows represent different matching/weighting methods, while columns indicate adjustments for various lag lengths.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Step 1: Define configurations}
\NormalTok{configurations }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
    \FunctionTok{list}\NormalTok{(}\AttributeTok{refinement.method =} \StringTok{"none"}\NormalTok{, }\AttributeTok{qoi =} \StringTok{"att"}\NormalTok{),}
    \FunctionTok{list}\NormalTok{(}\AttributeTok{refinement.method =} \StringTok{"none"}\NormalTok{, }\AttributeTok{qoi =} \StringTok{"art"}\NormalTok{),}
    \FunctionTok{list}\NormalTok{(}\AttributeTok{refinement.method =} \StringTok{"mahalanobis"}\NormalTok{, }\AttributeTok{qoi =} \StringTok{"att"}\NormalTok{),}
    \FunctionTok{list}\NormalTok{(}\AttributeTok{refinement.method =} \StringTok{"mahalanobis"}\NormalTok{, }\AttributeTok{qoi =} \StringTok{"art"}\NormalTok{),}
    \FunctionTok{list}\NormalTok{(}\AttributeTok{refinement.method =} \StringTok{"ps.match"}\NormalTok{, }\AttributeTok{qoi =} \StringTok{"att"}\NormalTok{),}
    \FunctionTok{list}\NormalTok{(}\AttributeTok{refinement.method =} \StringTok{"ps.match"}\NormalTok{, }\AttributeTok{qoi =} \StringTok{"art"}\NormalTok{),}
    \FunctionTok{list}\NormalTok{(}\AttributeTok{refinement.method =} \StringTok{"ps.weight"}\NormalTok{, }\AttributeTok{qoi =} \StringTok{"att"}\NormalTok{),}
    \FunctionTok{list}\NormalTok{(}\AttributeTok{refinement.method =} \StringTok{"ps.weight"}\NormalTok{, }\AttributeTok{qoi =} \StringTok{"art"}\NormalTok{)}
\NormalTok{)}

\CommentTok{\# Step 2: Use lapply or loop to generate results}
\NormalTok{results }\OtherTok{\textless{}{-}} \FunctionTok{lapply}\NormalTok{(configurations, }\ControlFlowTok{function}\NormalTok{(config) \{}
    \FunctionTok{PanelMatch}\NormalTok{(}
        \AttributeTok{lag                       =} \DecValTok{4}\NormalTok{,}
        \AttributeTok{time.id                   =} \StringTok{"year"}\NormalTok{,}
        \AttributeTok{unit.id                   =} \StringTok{"wbcode2"}\NormalTok{,}
        \AttributeTok{treatment                 =} \StringTok{"dem"}\NormalTok{,}
        \AttributeTok{data                      =}\NormalTok{ dem,}
        \AttributeTok{match.missing             =} \ConstantTok{FALSE}\NormalTok{,}
        \AttributeTok{listwise.delete           =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{size.match                =} \DecValTok{5}\NormalTok{,}
        \AttributeTok{outcome.var               =} \StringTok{"y"}\NormalTok{,}
        \AttributeTok{lead                      =} \DecValTok{0}\SpecialCharTok{:}\DecValTok{4}\NormalTok{,}
        \AttributeTok{forbid.treatment.reversal =} \ConstantTok{FALSE}\NormalTok{,}
        \AttributeTok{refinement.method         =}\NormalTok{ config}\SpecialCharTok{$}\NormalTok{refinement.method,}
        \AttributeTok{covs.formula              =} \SpecialCharTok{\textasciitilde{}} \FunctionTok{I}\NormalTok{(}\FunctionTok{lag}\NormalTok{(tradewb, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{)) }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(}\FunctionTok{lag}\NormalTok{(y, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{)),}
        \AttributeTok{qoi                       =}\NormalTok{ config}\SpecialCharTok{$}\NormalTok{qoi}
\NormalTok{    )}
\NormalTok{\})}

\CommentTok{\# Step 3: Get covariate balance and plot}
\NormalTok{plots }\OtherTok{\textless{}{-}} \FunctionTok{mapply}\NormalTok{(}\ControlFlowTok{function}\NormalTok{(result, config) \{}
\NormalTok{    df }\OtherTok{\textless{}{-}} \FunctionTok{get\_covariate\_balance}\NormalTok{(}
        \ControlFlowTok{if}\NormalTok{ (config}\SpecialCharTok{$}\NormalTok{qoi }\SpecialCharTok{==} \StringTok{"att"}\NormalTok{)}
\NormalTok{            result}\SpecialCharTok{$}\NormalTok{att}
        \ControlFlowTok{else}
\NormalTok{            result}\SpecialCharTok{$}\NormalTok{art,}
        \AttributeTok{data =}\NormalTok{ dem,}
        \AttributeTok{covariates =} \FunctionTok{c}\NormalTok{(}\StringTok{"tradewb"}\NormalTok{, }\StringTok{"y"}\NormalTok{),}
        \AttributeTok{plot =}\NormalTok{ F}
\NormalTok{    )}
\NormalTok{    causalverse}\SpecialCharTok{::}\FunctionTok{plot\_covariate\_balance\_pretrend}\NormalTok{(df, }\AttributeTok{main =} \StringTok{""}\NormalTok{, }\AttributeTok{show\_legend =}\NormalTok{ F)}
\NormalTok{\}, results, configurations, }\AttributeTok{SIMPLIFY =} \ConstantTok{FALSE}\NormalTok{)}

\CommentTok{\# Set names for plots}
\FunctionTok{names}\NormalTok{(plots) }\OtherTok{\textless{}{-}} \FunctionTok{sapply}\NormalTok{(configurations, }\ControlFlowTok{function}\NormalTok{(config) \{}
    \FunctionTok{paste}\NormalTok{(config}\SpecialCharTok{$}\NormalTok{qoi, config}\SpecialCharTok{$}\NormalTok{refinement.method, }\AttributeTok{sep =} \StringTok{"."}\NormalTok{)}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

To export

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(gridExtra)}
\FunctionTok{library}\NormalTok{(grid)}

\CommentTok{\# Column and row labels}
\NormalTok{col\_labels }\OtherTok{\textless{}{-}}
    \FunctionTok{c}\NormalTok{(}\StringTok{"None"}\NormalTok{,}
      \StringTok{"Mahalanobis"}\NormalTok{,}
      \StringTok{"Propensity Score Matching"}\NormalTok{,}
      \StringTok{"Propensity Score Weighting"}\NormalTok{)}
\NormalTok{row\_labels }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"ATT"}\NormalTok{, }\StringTok{"ART"}\NormalTok{)}

\CommentTok{\# Specify your desired fontsize for labels}
\NormalTok{minor.axes.fontsize }\OtherTok{\textless{}{-}} \DecValTok{16}
\NormalTok{major.axes.fontsize }\OtherTok{\textless{}{-}} \DecValTok{20}

\FunctionTok{png}\NormalTok{(}\FunctionTok{file.path}\NormalTok{(}\FunctionTok{getwd}\NormalTok{(), }\StringTok{"images"}\NormalTok{, }\StringTok{"p\_covariate\_balance.png"}\NormalTok{), }\AttributeTok{width=}\DecValTok{1200}\NormalTok{, }\AttributeTok{height=}\DecValTok{1000}\NormalTok{)}

\CommentTok{\# Create a list{-}of{-}lists, where each inner list represents a row}
\NormalTok{grid\_list }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
    \FunctionTok{list}\NormalTok{(}
        \FunctionTok{nullGrob}\NormalTok{(),}
        \FunctionTok{textGrob}\NormalTok{(col\_labels[}\DecValTok{1}\NormalTok{], }\AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ minor.axes.fontsize)),}
        \FunctionTok{textGrob}\NormalTok{(col\_labels[}\DecValTok{2}\NormalTok{], }\AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ minor.axes.fontsize)),}
        \FunctionTok{textGrob}\NormalTok{(col\_labels[}\DecValTok{3}\NormalTok{], }\AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ minor.axes.fontsize)),}
        \FunctionTok{textGrob}\NormalTok{(col\_labels[}\DecValTok{4}\NormalTok{], }\AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ minor.axes.fontsize))}
\NormalTok{    ),}
    
    \FunctionTok{list}\NormalTok{(}
        \FunctionTok{textGrob}\NormalTok{(}
\NormalTok{            row\_labels[}\DecValTok{1}\NormalTok{],}
            \AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ minor.axes.fontsize),}
            \AttributeTok{rot =} \DecValTok{90}
\NormalTok{        ),}
\NormalTok{        plots}\SpecialCharTok{$}\NormalTok{att.none,}
\NormalTok{        plots}\SpecialCharTok{$}\NormalTok{att.mahalanobis,}
\NormalTok{        plots}\SpecialCharTok{$}\NormalTok{att.ps.match,}
\NormalTok{        plots}\SpecialCharTok{$}\NormalTok{att.ps.weight}
\NormalTok{    ),}
    
    \FunctionTok{list}\NormalTok{(}
        \FunctionTok{textGrob}\NormalTok{(}
\NormalTok{            row\_labels[}\DecValTok{2}\NormalTok{],}
            \AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ minor.axes.fontsize),}
            \AttributeTok{rot =} \DecValTok{90}
\NormalTok{        ),}
\NormalTok{        plots}\SpecialCharTok{$}\NormalTok{art.none,}
\NormalTok{        plots}\SpecialCharTok{$}\NormalTok{art.mahalanobis,}
\NormalTok{        plots}\SpecialCharTok{$}\NormalTok{art.ps.match,}
\NormalTok{        plots}\SpecialCharTok{$}\NormalTok{art.ps.weight}
\NormalTok{    )}
\NormalTok{)}

\CommentTok{\# "Flatten" the list{-}of{-}lists into a single list of grobs}
\NormalTok{grobs }\OtherTok{\textless{}{-}} \FunctionTok{do.call}\NormalTok{(c, grid\_list)}

\CommentTok{\# Arrange your plots with text labels}
\FunctionTok{grid.arrange}\NormalTok{(}
    \AttributeTok{grobs   =}\NormalTok{ grobs,}
    \AttributeTok{ncol    =} \DecValTok{5}\NormalTok{,}
    \AttributeTok{nrow    =} \DecValTok{3}\NormalTok{,}
    \AttributeTok{widths  =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.225}\NormalTok{, }\FloatTok{0.225}\NormalTok{, }\FloatTok{0.225}\NormalTok{, }\FloatTok{0.225}\NormalTok{),}
    \AttributeTok{heights =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.45}\NormalTok{, }\FloatTok{0.45}\NormalTok{)}
\NormalTok{)}

\CommentTok{\# Add main x and y axis titles}
\FunctionTok{grid.text}\NormalTok{(}
    \StringTok{"Refinement Methods"}\NormalTok{,}
    \AttributeTok{x  =} \FloatTok{0.5}\NormalTok{,}
    \AttributeTok{y  =} \FloatTok{0.01}\NormalTok{,}
    \AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ major.axes.fontsize)}
\NormalTok{)}
\FunctionTok{grid.text}\NormalTok{(}
    \StringTok{"Quantities of Interest"}\NormalTok{,}
    \AttributeTok{x   =} \FloatTok{0.02}\NormalTok{,}
    \AttributeTok{y   =} \FloatTok{0.5}\NormalTok{,}
    \AttributeTok{rot =} \DecValTok{90}\NormalTok{,}
    \AttributeTok{gp  =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ major.axes.fontsize)}
\NormalTok{)}

\FunctionTok{dev.off}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(knitr)}
\FunctionTok{include\_graphics}\NormalTok{(}\FunctionTok{file.path}\NormalTok{(}\FunctionTok{getwd}\NormalTok{(), }\StringTok{"images"}\NormalTok{, }\StringTok{"p\_covariate\_balance.png"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{images/p_covariate_balance} \end{center}

Note: Each graph displays the standardized mean difference, as outlined in Equation \eqref{eq:aggbalance}, plotted on the vertical axis across a pre-treatment duration of four years represented on the horizontal axis. The leftmost column illustrates the balance prior to refinement, while the subsequent three columns depict the covariate balance post the application of distinct refinement techniques. Each individual line signifies the balance of a specific variable during the pre-treatment phase.The red line is tradewb and blue line is the lagged outcome variable.

In Figure \ref{fig:balancepretreat}, we observe a marked improvement in covariate balance due to the implemented matching procedures during the pre-treatment period. Our analysis prioritizes methods that adjust for time-varying covariates over a span of four years preceding the treatment initiation. The two rows delineate the standardized mean balance for both treatment modalities, with individual lines representing the balance for each covariate.

Across all scenarios, the refinement attributed to matched sets significantly enhances balance. Notably, using propensity score weighting considerably mitigates imbalances in confounders. While some degree of imbalance remains evident in the Mahalanobis distance and propensity score matching techniques, the standardized mean difference for the lagged outcome remains stable throughout the pre-treatment phase. This consistency lends credence to the validity of the proposed DiD estimator.

\textbf{Estimation Results}

We now detail the estimated ATTs derived from the matching techniques. Figure below offers visual representations of the impacts of treatment initiation (upper panel) and treatment reversal (lower panel) on the outcome variable for a duration of 5 years post-transition, specifically, (F = 0, 1, \ldots, 4). Across the five methods (columns), it becomes evident that the point estimates of effects associated with treatment initiation consistently approximate zero over the 5-year window. In contrast, the estimated outcomes of treatment reversal are notably negative and maintain statistical significance through all refinement techniques during the initial year of transition and the 1 to 4 years that follow, provided treatment reversal is permissible. These effects are notably pronounced, pointing to an estimated reduction of roughly X\% in the outcome variable.

Collectively, these findings indicate that the transition into the treated state from its absence doesn't invariably lead to a heightened outcome. Instead, the transition from the treated state back to its absence exerts a considerable negative effect on the outcome variable in both the short and intermediate terms. Hence, the positive effect of the treatment (if we were to use traditional DiD) is actually driven by the negative effect of treatment reversal.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# sequential}
\CommentTok{\# Step 1: Apply PanelEstimate function}

\CommentTok{\# Initialize an empty list to store results}
\NormalTok{res\_est }\OtherTok{\textless{}{-}} \FunctionTok{vector}\NormalTok{(}\StringTok{"list"}\NormalTok{, }\FunctionTok{length}\NormalTok{(res\_pm))}

\CommentTok{\# Iterate over each element in res\_pm}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(res\_pm)) \{}
\NormalTok{  res\_est[[i]] }\OtherTok{\textless{}{-}} \FunctionTok{PanelEstimate}\NormalTok{(}
\NormalTok{    res\_pm[[i]],}
    \AttributeTok{data =}\NormalTok{ dem,}
    \AttributeTok{se.method =} \StringTok{"bootstrap"}\NormalTok{,}
    \AttributeTok{number.iterations =} \DecValTok{1000}\NormalTok{,}
    \AttributeTok{confidence.level =}\NormalTok{ .}\DecValTok{95}
\NormalTok{  )}
  \CommentTok{\# Transfer the name of the current element to the res\_est list}
  \FunctionTok{names}\NormalTok{(res\_est)[i] }\OtherTok{\textless{}{-}} \FunctionTok{names}\NormalTok{(res\_pm)[i]}
\NormalTok{\}}

\CommentTok{\# Step 2: Apply plot\_PanelEstimate function}

\CommentTok{\# Initialize an empty list to store plot results}
\NormalTok{res\_est\_plot }\OtherTok{\textless{}{-}} \FunctionTok{vector}\NormalTok{(}\StringTok{"list"}\NormalTok{, }\FunctionTok{length}\NormalTok{(res\_est))}

\CommentTok{\# Iterate over each element in res\_est}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(res\_est)) \{}
\NormalTok{    res\_est\_plot[[i]] }\OtherTok{\textless{}{-}}
        \FunctionTok{plot\_PanelEstimate}\NormalTok{(res\_est[[i]],}
                           \AttributeTok{main =} \StringTok{""}\NormalTok{,}
                           \AttributeTok{theme\_use =}\NormalTok{ causalverse}\SpecialCharTok{::}\FunctionTok{ama\_theme}\NormalTok{(}\AttributeTok{base\_size =} \DecValTok{14}\NormalTok{))}
    \CommentTok{\# Transfer the name of the current element to the res\_est\_plot list}
    \FunctionTok{names}\NormalTok{(res\_est\_plot)[i] }\OtherTok{\textless{}{-}} \FunctionTok{names}\NormalTok{(res\_est)[i]}
\NormalTok{\}}

\CommentTok{\# check results}
\CommentTok{\# res\_est\_plot$mahalanobis.1lag.5m}


\CommentTok{\# Step 1: Apply PanelEstimate function for res\_pm\_rev}

\CommentTok{\# Initialize an empty list to store results}
\NormalTok{res\_est\_rev }\OtherTok{\textless{}{-}} \FunctionTok{vector}\NormalTok{(}\StringTok{"list"}\NormalTok{, }\FunctionTok{length}\NormalTok{(res\_pm\_rev))}

\CommentTok{\# Iterate over each element in res\_pm\_rev}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(res\_pm\_rev)) \{}
\NormalTok{  res\_est\_rev[[i]] }\OtherTok{\textless{}{-}} \FunctionTok{PanelEstimate}\NormalTok{(}
\NormalTok{    res\_pm\_rev[[i]],}
    \AttributeTok{data =}\NormalTok{ dem,}
    \AttributeTok{se.method =} \StringTok{"bootstrap"}\NormalTok{,}
    \AttributeTok{number.iterations =} \DecValTok{1000}\NormalTok{,}
    \AttributeTok{confidence.level =}\NormalTok{ .}\DecValTok{95}
\NormalTok{  )}
  \CommentTok{\# Transfer the name of the current element to the res\_est\_rev list}
  \FunctionTok{names}\NormalTok{(res\_est\_rev)[i] }\OtherTok{\textless{}{-}} \FunctionTok{names}\NormalTok{(res\_pm\_rev)[i]}
\NormalTok{\}}

\CommentTok{\# Step 2: Apply plot\_PanelEstimate function for res\_est\_rev}

\CommentTok{\# Initialize an empty list to store plot results}
\NormalTok{res\_est\_plot\_rev }\OtherTok{\textless{}{-}} \FunctionTok{vector}\NormalTok{(}\StringTok{"list"}\NormalTok{, }\FunctionTok{length}\NormalTok{(res\_est\_rev))}

\CommentTok{\# Iterate over each element in res\_est\_rev}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(res\_est\_rev)) \{}
\NormalTok{    res\_est\_plot\_rev[[i]] }\OtherTok{\textless{}{-}}
        \FunctionTok{plot\_PanelEstimate}\NormalTok{(res\_est\_rev[[i]],}
                           \AttributeTok{main =} \StringTok{""}\NormalTok{,}
                           \AttributeTok{theme\_use =}\NormalTok{ causalverse}\SpecialCharTok{::}\FunctionTok{ama\_theme}\NormalTok{(}\AttributeTok{base\_size =} \DecValTok{14}\NormalTok{))}
  \CommentTok{\# Transfer the name of the current element to the res\_est\_plot\_rev list}
  \FunctionTok{names}\NormalTok{(res\_est\_plot\_rev)[i] }\OtherTok{\textless{}{-}} \FunctionTok{names}\NormalTok{(res\_est\_rev)[i]}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# parallel}
\FunctionTok{library}\NormalTok{(doParallel)}
\FunctionTok{library}\NormalTok{(foreach)}

\CommentTok{\# Detect the number of cores to use for parallel processing}
\NormalTok{num\_cores }\OtherTok{\textless{}{-}} \DecValTok{4}

\CommentTok{\# Register the parallel backend}
\NormalTok{cl }\OtherTok{\textless{}{-}} \FunctionTok{makeCluster}\NormalTok{(num\_cores)}
\FunctionTok{registerDoParallel}\NormalTok{(cl)}

\CommentTok{\# Step 1: Apply PanelEstimate function in parallel}
\NormalTok{res\_est }\OtherTok{\textless{}{-}}
    \FunctionTok{foreach}\NormalTok{(}\AttributeTok{i =} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(res\_pm), }\AttributeTok{.packages =} \StringTok{"PanelMatch"}\NormalTok{) }\SpecialCharTok{\%dopar\%}\NormalTok{ \{}
        \FunctionTok{PanelEstimate}\NormalTok{(}
\NormalTok{            res\_pm[[i]],}
            \AttributeTok{data =}\NormalTok{ dem,}
            \AttributeTok{se.method =} \StringTok{"bootstrap"}\NormalTok{,}
            \AttributeTok{number.iterations =} \DecValTok{1000}\NormalTok{,}
            \AttributeTok{confidence.level =}\NormalTok{ .}\DecValTok{95}
\NormalTok{        )}
\NormalTok{    \}}

\CommentTok{\# Transfer names from res\_pm to res\_est}
\FunctionTok{names}\NormalTok{(res\_est) }\OtherTok{\textless{}{-}} \FunctionTok{names}\NormalTok{(res\_pm)}

\CommentTok{\# Step 2: Apply plot\_PanelEstimate function in parallel}
\NormalTok{res\_est\_plot }\OtherTok{\textless{}{-}}
    \FunctionTok{foreach}\NormalTok{(}
        \AttributeTok{i =} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(res\_est),}
        \AttributeTok{.packages =} \FunctionTok{c}\NormalTok{(}\StringTok{"PanelMatch"}\NormalTok{, }\StringTok{"causalverse"}\NormalTok{, }\StringTok{"ggplot2"}\NormalTok{)}
\NormalTok{    ) }\SpecialCharTok{\%dopar\%}\NormalTok{ \{}
        \FunctionTok{plot\_PanelEstimate}\NormalTok{(res\_est[[i]],}
                           \AttributeTok{main =} \StringTok{""}\NormalTok{,}
                           \AttributeTok{theme\_use =}\NormalTok{ causalverse}\SpecialCharTok{::}\FunctionTok{ama\_theme}\NormalTok{(}\AttributeTok{base\_size =} \DecValTok{10}\NormalTok{))}
\NormalTok{    \}}

\CommentTok{\# Transfer names from res\_est to res\_est\_plot}
\FunctionTok{names}\NormalTok{(res\_est\_plot) }\OtherTok{\textless{}{-}} \FunctionTok{names}\NormalTok{(res\_est)}



\CommentTok{\# Step 1: Apply PanelEstimate function for res\_pm\_rev in parallel}
\NormalTok{res\_est\_rev }\OtherTok{\textless{}{-}}
    \FunctionTok{foreach}\NormalTok{(}\AttributeTok{i =} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(res\_pm\_rev), }\AttributeTok{.packages =} \StringTok{"PanelMatch"}\NormalTok{) }\SpecialCharTok{\%dopar\%}\NormalTok{ \{}
        \FunctionTok{PanelEstimate}\NormalTok{(}
\NormalTok{            res\_pm\_rev[[i]],}
            \AttributeTok{data =}\NormalTok{ dem,}
            \AttributeTok{se.method =} \StringTok{"bootstrap"}\NormalTok{,}
            \AttributeTok{number.iterations =} \DecValTok{1000}\NormalTok{,}
            \AttributeTok{confidence.level =}\NormalTok{ .}\DecValTok{95}
\NormalTok{        )}
\NormalTok{    \}}

\CommentTok{\# Transfer names from res\_pm\_rev to res\_est\_rev}
\FunctionTok{names}\NormalTok{(res\_est\_rev) }\OtherTok{\textless{}{-}} \FunctionTok{names}\NormalTok{(res\_pm\_rev)}

\CommentTok{\# Step 2: Apply plot\_PanelEstimate function for res\_est\_rev in parallel}
\NormalTok{res\_est\_plot\_rev }\OtherTok{\textless{}{-}}
    \FunctionTok{foreach}\NormalTok{(}
        \AttributeTok{i =} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(res\_est\_rev),}
        \AttributeTok{.packages =} \FunctionTok{c}\NormalTok{(}\StringTok{"PanelMatch"}\NormalTok{, }\StringTok{"causalverse"}\NormalTok{, }\StringTok{"ggplot2"}\NormalTok{)}
\NormalTok{    ) }\SpecialCharTok{\%dopar\%}\NormalTok{ \{}
        \FunctionTok{plot\_PanelEstimate}\NormalTok{(res\_est\_rev[[i]],}
                           \AttributeTok{main =} \StringTok{""}\NormalTok{,}
                           \AttributeTok{theme\_use =}\NormalTok{ causalverse}\SpecialCharTok{::}\FunctionTok{ama\_theme}\NormalTok{(}\AttributeTok{base\_size =} \DecValTok{10}\NormalTok{))}
\NormalTok{    \}}

\CommentTok{\# Transfer names from res\_est\_rev to res\_est\_plot\_rev}
\FunctionTok{names}\NormalTok{(res\_est\_plot\_rev) }\OtherTok{\textless{}{-}} \FunctionTok{names}\NormalTok{(res\_est\_rev)}

\CommentTok{\# Stop the cluster}
\FunctionTok{stopCluster}\NormalTok{(cl)}
\end{Highlighting}
\end{Shaded}

To export

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(gridExtra)}
\FunctionTok{library}\NormalTok{(grid)}

\CommentTok{\# Column and row labels}
\NormalTok{col\_labels }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Mahalanobis 5m"}\NormalTok{, }
                \StringTok{"Mahalanobis 10m"}\NormalTok{, }
                \StringTok{"PS Matching 5m"}\NormalTok{, }
                \StringTok{"PS Matching 10m"}\NormalTok{, }
                \StringTok{"PS Weighting 5m"}\NormalTok{)}

\NormalTok{row\_labels }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"ATT"}\NormalTok{, }\StringTok{"ART"}\NormalTok{)}

\CommentTok{\# Specify your desired fontsize for labels}
\NormalTok{minor.axes.fontsize }\OtherTok{\textless{}{-}} \DecValTok{16}
\NormalTok{major.axes.fontsize }\OtherTok{\textless{}{-}} \DecValTok{20}

\FunctionTok{png}\NormalTok{(}\FunctionTok{file.path}\NormalTok{(}\FunctionTok{getwd}\NormalTok{(), }\StringTok{"images"}\NormalTok{, }\StringTok{"p\_did\_est\_in\_n\_out.png"}\NormalTok{), }\AttributeTok{width=}\DecValTok{1200}\NormalTok{, }\AttributeTok{height=}\DecValTok{1000}\NormalTok{)}

\CommentTok{\# Create a list{-}of{-}lists, where each inner list represents a row}
\NormalTok{grid\_list }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
  \FunctionTok{list}\NormalTok{(}
    \FunctionTok{nullGrob}\NormalTok{(),}
    \FunctionTok{textGrob}\NormalTok{(col\_labels[}\DecValTok{1}\NormalTok{], }\AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ minor.axes.fontsize)),}
    \FunctionTok{textGrob}\NormalTok{(col\_labels[}\DecValTok{2}\NormalTok{], }\AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ minor.axes.fontsize)),}
    \FunctionTok{textGrob}\NormalTok{(col\_labels[}\DecValTok{3}\NormalTok{], }\AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ minor.axes.fontsize)),}
    \FunctionTok{textGrob}\NormalTok{(col\_labels[}\DecValTok{4}\NormalTok{], }\AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ minor.axes.fontsize)),}
    \FunctionTok{textGrob}\NormalTok{(col\_labels[}\DecValTok{5}\NormalTok{], }\AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ minor.axes.fontsize))}
\NormalTok{  ),}
  
  \FunctionTok{list}\NormalTok{(}
    \FunctionTok{textGrob}\NormalTok{(row\_labels[}\DecValTok{1}\NormalTok{], }\AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ minor.axes.fontsize), }\AttributeTok{rot =} \DecValTok{90}\NormalTok{),}
\NormalTok{    res\_est\_plot}\SpecialCharTok{$}\NormalTok{mahalanobis}\FloatTok{.1}\NormalTok{lag}\FloatTok{.5}\NormalTok{m,}
\NormalTok{    res\_est\_plot}\SpecialCharTok{$}\NormalTok{mahalanobis}\FloatTok{.1}\NormalTok{lag}\FloatTok{.10}\NormalTok{m,}
\NormalTok{    res\_est\_plot}\SpecialCharTok{$}\NormalTok{ps.match}\FloatTok{.1}\NormalTok{lag}\FloatTok{.5}\NormalTok{m,}
\NormalTok{    res\_est\_plot}\SpecialCharTok{$}\NormalTok{ps.match}\FloatTok{.1}\NormalTok{lag}\FloatTok{.10}\NormalTok{m,}
\NormalTok{    res\_est\_plot}\SpecialCharTok{$}\NormalTok{ps.weight}\FloatTok{.1}\NormalTok{lag}\FloatTok{.5}\NormalTok{m}
\NormalTok{  ),}
  
  \FunctionTok{list}\NormalTok{(}
    \FunctionTok{textGrob}\NormalTok{(row\_labels[}\DecValTok{2}\NormalTok{], }\AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ minor.axes.fontsize), }\AttributeTok{rot =} \DecValTok{90}\NormalTok{),}
\NormalTok{    res\_est\_plot\_rev}\SpecialCharTok{$}\NormalTok{mahalanobis}\FloatTok{.1}\NormalTok{lag}\FloatTok{.5}\NormalTok{m,}
\NormalTok{    res\_est\_plot\_rev}\SpecialCharTok{$}\NormalTok{mahalanobis}\FloatTok{.1}\NormalTok{lag}\FloatTok{.10}\NormalTok{m,}
\NormalTok{    res\_est\_plot\_rev}\SpecialCharTok{$}\NormalTok{ps.match}\FloatTok{.1}\NormalTok{lag}\FloatTok{.5}\NormalTok{m,}
\NormalTok{    res\_est\_plot\_rev}\SpecialCharTok{$}\NormalTok{ps.match}\FloatTok{.1}\NormalTok{lag}\FloatTok{.10}\NormalTok{m,}
\NormalTok{    res\_est\_plot\_rev}\SpecialCharTok{$}\NormalTok{ps.weight}\FloatTok{.1}\NormalTok{lag}\FloatTok{.5}\NormalTok{m}
\NormalTok{  )}
\NormalTok{)}

\CommentTok{\# "Flatten" the list{-}of{-}lists into a single list of grobs}
\NormalTok{grobs }\OtherTok{\textless{}{-}} \FunctionTok{do.call}\NormalTok{(c, grid\_list)}

\CommentTok{\# Arrange your plots with text labels}
\FunctionTok{grid.arrange}\NormalTok{(}
  \AttributeTok{grobs   =}\NormalTok{ grobs,}
  \AttributeTok{ncol    =} \DecValTok{6}\NormalTok{,}
  \AttributeTok{nrow    =} \DecValTok{3}\NormalTok{,}
  \AttributeTok{widths  =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.18}\NormalTok{, }\FloatTok{0.18}\NormalTok{, }\FloatTok{0.18}\NormalTok{, }\FloatTok{0.18}\NormalTok{, }\FloatTok{0.18}\NormalTok{),}
  \AttributeTok{heights =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.45}\NormalTok{, }\FloatTok{0.45}\NormalTok{)}
\NormalTok{)}

\CommentTok{\# Add main x and y axis titles}
\FunctionTok{grid.text}\NormalTok{(}
  \StringTok{"Methods"}\NormalTok{,}
  \AttributeTok{x  =} \FloatTok{0.5}\NormalTok{,}
  \AttributeTok{y  =} \FloatTok{0.02}\NormalTok{,}
  \AttributeTok{gp =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ major.axes.fontsize)}
\NormalTok{)}
\FunctionTok{grid.text}\NormalTok{(}
  \StringTok{""}\NormalTok{,}
  \AttributeTok{x   =} \FloatTok{0.02}\NormalTok{,}
  \AttributeTok{y   =} \FloatTok{0.5}\NormalTok{,}
  \AttributeTok{rot =} \DecValTok{90}\NormalTok{,}
  \AttributeTok{gp  =} \FunctionTok{gpar}\NormalTok{(}\AttributeTok{fontsize =}\NormalTok{ major.axes.fontsize)}
\NormalTok{)}

\FunctionTok{dev.off}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(knitr)}
\FunctionTok{include\_graphics}\NormalTok{(}\FunctionTok{file.path}\NormalTok{(}\FunctionTok{getwd}\NormalTok{(), }\StringTok{"images"}\NormalTok{, }\StringTok{"p\_did\_est\_in\_n\_out.png"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{chaisemartin-dhaultfoeuille}{%
\subsubsection{Chaisemartin-d'Haultfoeuille}\label{chaisemartin-dhaultfoeuille}}

use \texttt{twowayfeweights} from \href{https://github.com/shuo-zhang-ucsb/twowayfeweights}{GitHub} \citep{de2020two}

\hypertarget{didimputation}{%
\subsubsection{didimputation}\label{didimputation}}

use \texttt{didimputation} from \href{https://github.com/kylebutts/didimputation}{GitHub}

\hypertarget{staggered}{%
\subsubsection{staggered}\label{staggered}}

\texttt{staggered} \href{https://github.com/jonathandroth/staggered}{package}

\hypertarget{wooldridges-solution}{%
\subsubsection{Wooldridge's Solution}\label{wooldridges-solution}}

use \href{https://grantmcdermott.com/etwfe/}{etwfe}(Extended two-way Fixed Effects) \citep{wooldridge2022simple}

\hypertarget{two-stage-did}{%
\subsection{Two-stage DiD}\label{two-stage-did}}

\href{https://cran.r-project.org/web/packages/did2s/vignettes/Two-Stage-Difference-in-Differences.html}{Example} from CRAN

\hypertarget{multiple-treatment-groups}{%
\subsection{Multiple Treatment groups}\label{multiple-treatment-groups}}

When you have 2 treatments in a setting, you should always try to model both of them under one regression to see whether they are significantly different.

\begin{itemize}
\tightlist
\item
  Never use one treated groups as control for the other, and run separate regression.
\item
  Could check this \href{https://stats.stackexchange.com/questions/474533/difference-in-difference-with-two-treatment-groups-and-one-control-group-classi}{answer}
\end{itemize}

\[
\begin{aligned}
Y_{it} &= \alpha + \gamma_1 Treat1_{i} + \gamma_2 Treat2_{i} + \lambda Post_t  \\
&+ \delta_1(Treat1_i \times Post_t) + \delta_2(Treat2_i \times Post_t) + \epsilon_{it}
\end{aligned}
\]

\citep{fricke2017identification}

\hypertarget{multiple-treatments}{%
\subsection{Multiple Treatments}\label{multiple-treatments}}

\citep{de2022two} \href{https://www.youtube.com/watch?v=UHeJoc27qEM\&ab_channel=TaylorWright}{video} \href{https://drive.google.com/file/d/156Fu73avBvvV_H64wePm7eW04V0jEG3K/view}{code}

\hypertarget{assumption-violation}{%
\section{Assumption Violation}\label{assumption-violation}}

\hypertarget{endogenous-timing}{%
\subsection{Endogenous Timing}\label{endogenous-timing}}

If the timing of units can be influenced by strategic decisions in a DID analysis, an instrumental variable approach with a control function can be used to control for endogeneity in timing.

\hypertarget{questionable-counterfactuals}{%
\subsection{Questionable Counterfactuals}\label{questionable-counterfactuals}}

In situations where the control units may not serve as a reliable counterfactual for the treated units, matching methods such as propensity score matching or generalized random forest can be utilized. Additional methods can be found in \protect\hyperlink{matching-methods}{Matching Methods}.

\hypertarget{mediation-under-did}{%
\section{Mediation Under DiD}\label{mediation-under-did}}

Check this \href{https://stats.stackexchange.com/questions/261218/difference-in-difference-model-with-mediators-estimating-the-effect-of-differen}{post}

\hypertarget{assumptions-2}{%
\section{Assumptions}\label{assumptions-2}}

\begin{itemize}
\item
  \textbf{Parallel Trends}: Difference between the treatment and control groups remain constant if there were no treatment.

  \begin{itemize}
  \item
    should be used in cases where

    \begin{itemize}
    \item
      you observe before and after an event
    \item
      you have treatment and control groups
    \end{itemize}
  \item
    not in cases where

    \begin{itemize}
    \item
      treatment is not random
    \item
      confounders.
    \end{itemize}
  \item
    To support we use

    \begin{itemize}
    \item
      \protect\hyperlink{placebo-test}{Placebo test}
    \item
      \protect\hyperlink{prior-parallel-trends-test}{Prior Parallel Trends Test}
    \end{itemize}
  \end{itemize}
\item
  \textbf{Linear additive effects} (of group/unit specific and time-specific):

  \begin{itemize}
  \item
    If they are not additively interact, we have to use the weighted 2FE estimator \citep{imai2021use}
  \item
    Typically seen in the \protect\hyperlink{staggered-dif-n-dif}{Staggered Dif-n-dif}
  \end{itemize}
\item
  No anticipation: There is no causal effect of the treatment before its implementation.
\end{itemize}

\textbf{Possible issues}

\begin{itemize}
\item
  Estimate dependent on functional form:

  \begin{itemize}
  \tightlist
  \item
    When the size of the response depends (nonlinearly) on the size of the intervention, we might want to look at the the difference in the group with high intensity vs.~low.
  \end{itemize}
\item
  Selection on (time--varying) unobservables

  \begin{itemize}
  \tightlist
  \item
    Can use the overall sensitivity of coefficient estimates to hidden bias using \protect\hyperlink{rosenbaum-bounds}{Rosenbaum Bounds}
  \end{itemize}
\item
  Long-term effects

  \begin{itemize}
  \tightlist
  \item
    Parallel trends are more likely to be observed over shorter period (window of observation)
  \end{itemize}
\item
  Heterogeneous effects

  \begin{itemize}
  \tightlist
  \item
    Different intensity (e.g., doses) for different groups.
  \end{itemize}
\item
  Ashenfelter dip \citep{ashenfelter1985} (job training program participant are more likely to experience an earning drop prior enrolling in these programs)

  \begin{itemize}
  \tightlist
  \item
    Participants are systemically different from nonparticipants before the treatment, leading to the question of permanent or transitory changes.
  \item
    A fix to this transient endogeneity is to calculate long-run differences (exclude a number of periods symmetrically around the adoption/ implementation date). If we see a sustained impact, then we have strong evidence for the causal impact of a policy. \citep{proserpio2017} \citep{heckman1999c} \citep{jepsen2014} \citep{li2011}
  \end{itemize}
\item
  Response to event might not be immediate (can't be observed right away in the dependent variable)

  \begin{itemize}
  \tightlist
  \item
    Using lagged dependent variable \(Y_{it-1}\) might be more appropriate \citep{blundell1998initial}
  \end{itemize}
\item
  Other factors that affect the difference in trends between the two groups (i.e., treatment and control) will bias your estimation.
\item
  Correlated observations within a group or time
\item
  Incidental parameters problems \citep{lancaster2000incidental}: it's always better to use individual and time fixed effect.
\item
  When examining the effects of variation in treatment timing, we have to be careful because negative weights (per group) can be negative if there is a heterogeneity in the treatment effects over time. Example: {[}\citet{athey2022design}{]}\citep{borusyak2021revisiting}\citep{goodman2021difference}. In this case you should use new estimands proposed by {[}\citet{callaway2021difference}{]}\citep{de2020two}, in the \texttt{did} package. If you expect lags and leads, see \citep{sun2021estimating}
\item
  \citep{gibbons2018broken} caution when we suspect the treatment effect and treatment variance vary across groups
\end{itemize}

\hypertarget{prior-parallel-trends-test}{%
\subsection{Prior Parallel Trends Test}\label{prior-parallel-trends-test}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Plot the average outcomes over time for both treatment and control group before and after the treatment in time.
\item
  Statistical test for difference in trends (\textbf{using data from before the treatment period})
\end{enumerate}

\[
Y = \alpha_g + \beta_1 T + \beta_2 T\times G + \epsilon
\]

where

\begin{itemize}
\item
  \(Y\) = the outcome variable
\item
  \(\alpha_g\) = group fixed effects
\item
  \(T\) = time (e.g., specific year, or month)
\item
  \(\beta_2\) = different time trends for each group
\end{itemize}

Hence, if \(\beta_2 =0\) provides evidence that there are no differences in the trend for the two groups prior the time treatment.

You can also use different functional forms (e..g, polynomial or nonlinear).

If \(\beta_2 \neq 0\) statistically, possible reasons can be:

\begin{itemize}
\item
  Statistical significance can be driven by large sample
\item
  Or the trends are so consistent, and just one period deviation can throw off the trends. Hence, statistical statistical significance.
\end{itemize}

Technically, we can still salvage the research by including time fixed effects, instead of just the before-and-after time fixed effect (actually, most researchers do this mechanically anyway nowadays). However, a side effect can be that the time fixed effects can also absorb some part your treatment effect as well, especially in cases where the treatment effects vary with time (i.e., stronger or weaker over time) \citep{wolfers2003business}.

Debate:

\begin{itemize}
\item
  \citep{kahn2020promise} argue that DiD will be more plausible when the treatment and control groups are similar not only in \textbf{trends}, but also in \textbf{levels}. Because when we observe dissimilar in levels prior to the treatment, why is it okay to think that this will not affect future trends?

  \begin{itemize}
  \item
    Show a plot of the dependent variable's time series for treated and control groups and also a similar plot with matched sample. \citep{ryan2019now} show evidence of matched DiD did well in the setting of non-parallel trends (at least in health care setting).
  \item
    In the case that we don't have similar levels ex ante between treatment and control groups, functional form assumptions matter and we need justification for our choice.
  \end{itemize}
\item
  Pre-trend statistical tests: \citep{roth2022pretest} provides evidence that these test are usually under powered.

  \begin{itemize}
  \tightlist
  \item
    See \href{https://github.com/jonathandroth/PretrendsPower}{PretrendsPower} and \href{https://github.com/jonathandroth/pretrends}{pretrends} packages for correcting this.
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(fixest)}
\NormalTok{od }\OtherTok{\textless{}{-}}\NormalTok{ causaldata}\SpecialCharTok{::}\NormalTok{organ\_donations }\SpecialCharTok{\%\textgreater{}\%}
    \CommentTok{\# Use only pre{-}treatment data}
    \FunctionTok{filter}\NormalTok{(Quarter\_Num }\SpecialCharTok{\textless{}=} \DecValTok{3}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
    \CommentTok{\# Treatment variable}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{California =}\NormalTok{ State }\SpecialCharTok{==} \StringTok{\textquotesingle{}California\textquotesingle{}}\NormalTok{)}

\CommentTok{\# use my package}
\NormalTok{causalverse}\SpecialCharTok{::}\FunctionTok{plot\_par\_trends}\NormalTok{(}
    \AttributeTok{data =}\NormalTok{ od,}
    \AttributeTok{metrics\_and\_names =} \FunctionTok{list}\NormalTok{(}\StringTok{"Rate"} \OtherTok{=} \StringTok{"Rate"}\NormalTok{),}
    \AttributeTok{treatment\_status\_var =} \StringTok{"California"}\NormalTok{,}
    \AttributeTok{time\_var =} \FunctionTok{list}\NormalTok{(}\AttributeTok{Quarter\_Num =} \StringTok{"Time"}\NormalTok{),}
    \AttributeTok{display\_CI =}\NormalTok{ F}
\NormalTok{)}
\CommentTok{\#\textgreater{} [[1]]}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-35-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# do it manually}
\CommentTok{\# always good but plot the dependent out}
\NormalTok{od }\SpecialCharTok{|\textgreater{}}
    \CommentTok{\# group by treatment status and time}
    \FunctionTok{group\_by}\NormalTok{(California, Quarter) }\SpecialCharTok{|\textgreater{}}
    \FunctionTok{summarize\_all}\NormalTok{(mean) }\SpecialCharTok{|\textgreater{}}
    \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
    \CommentTok{\# view()}
    
    \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Quarter\_Num, }\AttributeTok{y =}\NormalTok{ Rate, }\AttributeTok{color =}\NormalTok{ California)) }\SpecialCharTok{+}
    \FunctionTok{geom\_line}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-35-2} \end{center}

\begin{Shaded}
\begin{Highlighting}[]


\CommentTok{\# but it\textquotesingle{}s also important to use statistical test}
\NormalTok{prior\_trend }\OtherTok{\textless{}{-}} \FunctionTok{feols}\NormalTok{(Rate }\SpecialCharTok{\textasciitilde{}} \FunctionTok{i}\NormalTok{(Quarter\_Num, California) }\SpecialCharTok{|}\NormalTok{ State }\SpecialCharTok{+}\NormalTok{ Quarter,}
               \AttributeTok{data =}\NormalTok{ od)}

\FunctionTok{coefplot}\NormalTok{(prior\_trend)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-35-3} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{iplot}\NormalTok{(prior\_trend)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{26-dif-in-dif_files/figure-latex/unnamed-chunk-35-4} \end{center}

This is alarming since one of the periods is significantly different from 0, which means that our parallel trends assumption is not plausible.

In cases where you might have violations of parallel trends assumption, check \citep{rambachan2023more}

\begin{itemize}
\item
  Impose restrictions on how different the post-treatment violations of parallel trends can be from the pre-trends.
\item
  Partial identification of causal parameter
\item
  A type of sensitivity analysis
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# https://github.com/asheshrambachan/HonestDiD}
\CommentTok{\# install.packages("HonestDiD")}
\end{Highlighting}
\end{Shaded}

Alternatively, \citet{ban2022generalized} propose a method that with an information set (i.e., pre-treatment covariates), and an assumption on the selection bias in the post-treatment period (i.e., lies within the convex hull of all selection biases), they can still identify a set of ATT, and with stricter assumption on selection bias from the policymakers perspective, they can also have a point estimate.

Alternatively, we can use the \texttt{pretrends} package to examine our assumptions \citep{roth2022pretest}

\hypertarget{placebo-test}{%
\subsection{Placebo Test}\label{placebo-test}}

Procedure:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Sample data only in the period before the treatment in time.
\item
  Consider different fake cutoff in time, either

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    Try the whole sequence in time
  \item
    Generate random treatment period, and use \textbf{randomization inference} to account for sampling distribution of the fake effect.
  \end{enumerate}
\item
  Estimate the DiD model but with the post-time = 1 with the fake cutoff
\item
  A significant DiD coefficient means that you violate the parallel trends! You have a big problem.
\end{enumerate}

Alternatively,

\begin{itemize}
\tightlist
\item
  When data have multiple control groups, drop the treated group, and assign another control group as a ``fake'' treated group. But even if it fails (i.e., you find a significant DiD effect) among the control groups, it can still be fine. However, this method is used under \protect\hyperlink{synthetic-control}{Synthetic Control}
\end{itemize}

\href{https://theeffectbook.net/ch-DifferenceinDifference.html}{Code by theeffectbook.net}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(fixest)}
\NormalTok{od }\OtherTok{\textless{}{-}}\NormalTok{ causaldata}\SpecialCharTok{::}\NormalTok{organ\_donations }\SpecialCharTok{\%\textgreater{}\%}
    \CommentTok{\# Use only pre{-}treatment data}
    \FunctionTok{filter}\NormalTok{(Quarter\_Num }\SpecialCharTok{\textless{}=} \DecValTok{3}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 

\CommentTok{\# Create fake treatment variables}
    \FunctionTok{mutate}\NormalTok{(}
        \AttributeTok{FakeTreat1 =}\NormalTok{ State }\SpecialCharTok{==} \StringTok{\textquotesingle{}California\textquotesingle{}} \SpecialCharTok{\&}
\NormalTok{            Quarter }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Q12011\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Q22011\textquotesingle{}}\NormalTok{),}
        \AttributeTok{FakeTreat2 =}\NormalTok{ State }\SpecialCharTok{==} \StringTok{\textquotesingle{}California\textquotesingle{}} \SpecialCharTok{\&}
\NormalTok{            Quarter }\SpecialCharTok{==} \StringTok{\textquotesingle{}Q22011\textquotesingle{}}
\NormalTok{    )}


\NormalTok{clfe1 }\OtherTok{\textless{}{-}} \FunctionTok{feols}\NormalTok{(Rate }\SpecialCharTok{\textasciitilde{}}\NormalTok{ FakeTreat1 }\SpecialCharTok{|}\NormalTok{ State }\SpecialCharTok{+}\NormalTok{ Quarter,}
               \AttributeTok{data =}\NormalTok{ od)}
\NormalTok{clfe2 }\OtherTok{\textless{}{-}} \FunctionTok{feols}\NormalTok{(Rate }\SpecialCharTok{\textasciitilde{}}\NormalTok{ FakeTreat2 }\SpecialCharTok{|}\NormalTok{ State }\SpecialCharTok{+}\NormalTok{ Quarter,}
               \AttributeTok{data =}\NormalTok{ od)}

\FunctionTok{etable}\NormalTok{(clfe1,clfe2)}
\CommentTok{\#\textgreater{}                           clfe1            clfe2}
\CommentTok{\#\textgreater{} Dependent Var.:            Rate             Rate}
\CommentTok{\#\textgreater{}                                                 }
\CommentTok{\#\textgreater{} FakeTreat1TRUE  0.0061 (0.0051)                 }
\CommentTok{\#\textgreater{} FakeTreat2TRUE                  {-}0.0017 (0.0028)}
\CommentTok{\#\textgreater{} Fixed{-}Effects:  {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} State                       Yes              Yes}
\CommentTok{\#\textgreater{} Quarter                     Yes              Yes}
\CommentTok{\#\textgreater{} \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}
\CommentTok{\#\textgreater{} S.E.: Clustered       by: State        by: State}
\CommentTok{\#\textgreater{} Observations                 81               81}
\CommentTok{\#\textgreater{} R2                      0.99377          0.99376}
\CommentTok{\#\textgreater{} Within R2               0.00192          0.00015}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes: 0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

We would like the ``supposed'' DiD to be insignificant.

\textbf{Robustness Check}

\begin{itemize}
\item
  Placebo DiD (if the DiD estimate \(\neq 0\), parallel trend is violated, and original DiD is biased):

  \begin{itemize}
  \item
    Group: Use fake treatment groups: A population that was \textbf{not} affect by the treatment
  \item
    Time: Redo the DiD analysis for period before the treatment (expected treatment effect is 0) (e..g, for previous year or period).
  \end{itemize}
\item
  Possible alternative control group: Expected results should be similar
\item
  Try different windows (further away from the treatment point, other factors can creep in and nullify your effect).
\item
  Treatment Reversal (what if we don't see the treatment event)
\item
  Higher-order polynomial time trend (to relax linearity assumption)
\item
  Test whether other dependent variables that should not be affected by the event are indeed unaffected.

  \begin{itemize}
  \tightlist
  \item
    Use the same control and treatment period (DiD \(\neq0\), there is a problem)
  \end{itemize}
\end{itemize}

\hypertarget{rosenbaum-bounds}{%
\subsection{Rosenbaum Bounds}\label{rosenbaum-bounds}}

\protect\hyperlink{rosenbaum-bounds}{Rosenbaum Bounds} assess the overall sensitivity of coefficient estimates to hidden bias \citep{rosenbaum2002overt} without having knowledge (e.g., direction) of the bias. This method is also known as \textbf{worst case analyses} \citep{diprete2004assessing}.

Consider the treatment assignment is based in a way that the odds of treatment of a unit and its control is different by a multiplier \(\Gamma\) (where \(\Gamma = 1\) mean that the odds of assignment is identical, which mean random treatment assignment).

\begin{itemize}
\tightlist
\item
  This bias is the product of an unobservable that influences both treatment selection and outcome by a factor \(\Gamma\) (omitted variable bias)
\end{itemize}

Using this technique, we may estimate the upper limit of the p-value for the treatment effect while assuming selection on unobservables of magnitude \(\Gamma\).

Usually, we would create a table of different levels of \(\Gamma\) to assess how the magnitude of biases can affect our evidence of the treatment effect (estimate).

If we have treatment assignment is clustered (e.g., within school, within state) we need to adjust the bounds for clustered treatment assignment \citep{hansen2014clustered} (similar to clustered standard errors)

Then, we can report the minimum value of \(\Gamma\) at which the treatment treat is nullified (i.e., become insignificant). And the literature's rules of thumb is that if \(\Gamma > 2\), then we have strong evidence for our treatment effect is robust to large biases \citep{proserpio2017online}

Packages

\begin{itemize}
\item
  \texttt{rbounds} \citep{keele2010overview}
\item
  \texttt{sensitivitymv} \citep{rosenbaum2015two}
\item
  \texttt{sensitivitymw} \citep{rosenbaum2015two}
\end{itemize}

\hypertarget{synthetic-control}{%
\chapter{Synthetic Control}\label{synthetic-control}}

Examples in marketing:

\begin{itemize}
\tightlist
\item
  \citep{tirunillai2017}: offline TV ad on Online Chatter
\item
  \citep{wang2019mobile}: mobile hailing technology adoption on drivers' hourly earnings
\item
  \citep{guo2020let}: payment disclosure laws effect on physician prescription behavior using Timing of the Massachusetts open payment law as the exogenous shock
\item
  \citep{adalja2023gmo}: mandatory GMO labels had no impact on consumer demand (Using Vermont as a mandatory state)
\end{itemize}

Notes

\begin{itemize}
\item
  Synthetic control method (SCM) is a generalization of the \protect\hyperlink{difference-in-differences}{Difference-in-differences} model
\item
  SCM is \textbf{superior} than \protect\hyperlink{matching-methods}{Matching Methods} because it not only matches on covariates (i.e., pre-treatment variables), but also outcomes.
\item
  For a review of the method, see \citep{abadie2021using}
\item
  SCMs can also be used under the Bayesian framework where we do not have to impose any restrictive priori \citep{kim2020bayesian}
\item
  Different from \protect\hyperlink{matching-methods}{Matching Methods} because SCMs match on the pre-treatment outcomes in each period while \protect\hyperlink{matching-methods}{Matching Methods} match on the number of covariates.
\item
  A data driven procedure to construct more comparable control groups (i.e., black box).
\item
  To do causal inference with control and treatment group using \protect\hyperlink{matching-methods}{Matching Methods}, you typically have to have similar covariates in the control and the treated groups. However, if you don't methods like \protect\hyperlink{propensity-scores}{Propensity Scores} and DID can perform rather poorly (i.e., large bias).
\end{itemize}

Advantages over \protect\hyperlink{difference-in-differences}{Difference-in-differences}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Maximization of the observable similarity between control and treatment (maybe also unobservables)
\item
  Can also be used in cases where no untreated case with similar on matching dimensions with treated cases
\item
  Objective selection of controls.
\end{enumerate}

Advantages over linear regression

\begin{itemize}
\item
  Regression weights for the estimator will be outside of {[}0,1{]} (because regression allows extrapolation), and it will not be sparse (i.e., can be less than 0).
\item
  No extrapolation under SCMs
\item
  Explicitly state the fit (i.e., the weight)
\item
  Can be estimated without the post-treatment outcomes for the control group (can't p-hack)
\end{itemize}

Advantages:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  From the selection criteria, researchers can understand the relative importance of each candidate
\item
  Post-intervention outcomes are not used in synthetic. Hence, you can't retro-fit.
\item
  Observable similarity between control and treatment cases is maximized
\end{enumerate}

Disadvantages:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  It's hard to argue for the weights you use to create the ``synthetic control''
\end{enumerate}

SCM is recommended when

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Social events to evaluate large-scale program or policy
\item
  Only one treated case with several control candidates.
\end{enumerate}

\textbf{Assumptions}

\begin{itemize}
\item
  Donor subject is a good match for the synthetic control (i.e., gap between the dependent of the donor subject and that of the synthetic control should be 0 before treatment)
\item
  Only the treated subject undergoes the treatment and not any of the subjects in the donor pool.
\item
  No other changes to the subjects during the whole window.
\item
  The counterfactual outcome of the treatment group can be imputed in a \textbf{linear combination} of control groups.
\end{itemize}

\textbf{Identification}: The exclusion restriction is met conditional on the pre-treatment outcomes.

\texttt{Synth} provides an algorithm that finds weighted combination of the comparison units where the weights are chosen such that it best resembles the values of predictors of the outcome variable for the affected units before the intervention

Setting (notation followed professor \href{https://conference.nber.org/confer/2021/SI2021/ML/AbadieSlides.pdf}{Alberto Abadie})

\begin{itemize}
\item
  \(J + 1\) units in periods \(1, \dots, T\)
\item
  The first unit is the treated one during \(T_0 + 1, \dots, T\)
\item
  \(J\) units are called a donor pool
\item
  \(Y_{it}^I\) is the outcome for unit \(i\) if it's exposed to the treatment during \(T_0 + 1 , \dots T\)
\item
  \(Y_{it}^N\) is the outcome for unit \(i\) if it's not exposed to the treatment
\end{itemize}

We try to estimate the effect of treatment on the treated unit

\[
\tau_{1t} = Y_{1t}^I - Y_{1t}^N
\]

where we observe the first treated unit already \(Y_{1t}^I = Y_{1t}\)

To construct the synthetic control unit, we have to find appropriate weight for each donor in the donor pool by finding \(\mathbf{W} = (w_2, \dots, w_{J=1})'\) where

\begin{itemize}
\item
  \(w_j \ge 0\) for \(j = 2, \dots, J+1\)
\item
  \(w_2 + \dots + w_{J+1} = 1\)
\end{itemize}

The ``appropriate'' vector \(\mathbf{W}\) here is constrained to

\[
\min||\mathbf{X}_1 - \mathbf{X}_0 \mathbf{W}||
\]

where

\begin{itemize}
\item
  \(\mathbf{X}_1\) is the \(k \times 1\) vector of pre-treatment characteristics for the treated unit
\item
  \(\mathbf{X}_0\) is the \(k \times J\) matrix of pre-treatment characteristics for the untreated units
\end{itemize}

For simplicity, researchers usually use

\[
\begin{aligned}
&\min||\mathbf{X}_1 - \mathbf{X}_0 \mathbf{W}|| \\
&= (\sum_{h=1}^k v_h(X_{h1}- w_2 X-{h2} - \dots - w_{J+1} X_{hJ +1})^{1/2}
\end{aligned}
\]

where

\begin{itemize}
\tightlist
\item
  \(v_1, \dots, v_k\) is a vector positive constants that represent the predictive power of the \(k\) predictors on \(Y_{1t}^N\) (i.e., the potential outcome of the treated without treatment) and it can be chosen either explicitly by the researcher or by data-driven methods
\end{itemize}

For penalized synthetic control \citep{abadie2021penalized}, the minimization problem becomes

\[
\min_{\mathbf{W}} ||\mathbf{X}_1 - \sum_{j=2}^{J + 1}W_j \mathbf{X}_j ||^2 + \lambda \sum_{j=2}^{J+1} W_j ||\mathbf{X}_1 - \mathbf{X}_j||^2
\]

where

\begin{itemize}
\item
  \(W_j \ge 0\) and \(\sum_{j=2}^{J+1} W_j = 1\)
\item
  \(\lambda >0\) balances over-fitting of the treated and minimize the sum of pairwise distances

  \begin{itemize}
  \item
    \(\lambda \to 0\): pure synthetic control (i.e solution for the unpenalized estimator)
  \item
    \(\lambda \to \infty\): nearest neighbor matching
  \end{itemize}
\end{itemize}

Advantages:

\begin{itemize}
\item
  For \(\lambda >0\), you have unique and sparse solution
\item
  Reduces the interpolation bias when averaging dissimilar units
\item
  Penalized SC never uses dissimilar units
\end{itemize}

Then the synthetic control estimator is

\[
\hat{\tau}_{1t} = Y_{1t} - \sum_{j=2}^{J+1} w_j^* Y_{jt}
\]

where \(Y_{jt}\) is the outcome for unit \(j\) at time \(t\)

Consideration

Under the factor model \citep{abadie2010synthetic}

\[
Y_{it}^N = \mathbf{\theta}_t \mathbf{Z}_i + \mathbf{\lambda}_t \mathbf{\mu}_i + \epsilon_{it}
\]

where

\begin{itemize}
\item
  \(Z_i\) = observables
\item
  \(\mu_i\) = unobservables
\item
  \(\epsilon_{it}\) = unit-level transitory shock (i.e., random noise)
\end{itemize}

with assumptions of \(\mathbf{W}^*\) such that

\[
\begin{aligned}
\sum_{j=2}^{J+1} w_j^* \mathbf{Z}_j  &= \mathbf{Z}_1 \\
&\dots \\
\sum_{j=2}^{J+1} w_j^* Y_{j1} &= Y_{11} \\
\sum_{j=2}^{J+1} w_j^* Y_{jT_0} &= Y_{1T_0}
\end{aligned}
\]

Basically, we assume that the synthetic control is a good counterfactual when the treated unit is not exposed to the treatment.

Then,

\begin{itemize}
\item
  the bias bound depends on close fit, which is controlled by the ratio between \(\epsilon_{it}\) (transitory shock) and \(T_0\) (the number of pre-treatment periods). In other words, you should have good fit for \(Y_{1t}\) for pre-treatment period (i.e., \(T_0\) should be large while small variance in \(\epsilon_{it}\))
\item
  When you have poor fit, you have to use bias correction version of the synthetic control. See \citep[\citet{abadie2021using}, \citet{ben2020varying}]{arkhangelsky2019synthetic}
\item
  Overfitting can be the result of small \(T_0\) (the number of pre-treatment periods), large \(J\) (the number of units in the donor pool), and large \(\epsilon_{it}\) (noise)

  \begin{itemize}
  \tightlist
  \item
    Mitigation: put only similar units (to the treated one) in the donor pool
  \end{itemize}
\end{itemize}

To make inference, we have to create a permutation distribution (by iteratively reassigning the treatment to the units in the donor pool and estimate the placebo effects in each iteration). We say there is an effect of the treatment when the magnitude of value of the treatment effect on the treated unit is extreme relative to the permutation distribution.

It's recommended to use one-sided inference. And the permutation distribution is superior to the p-values alone (because sampling-based inference is hard under SCMs either because of undefined sampling mechanism or the sample is the population).

For benchmark (permutation) distribution (e.g., uniform), see \citep{firpo2018synthetic}

\hypertarget{applications-1}{%
\section{Applications}\label{applications-1}}

\hypertarget{example-1-2}{%
\subsection{Example 1}\label{example-1-2}}

by \href{https://rpubs.com/danilofreire/synth}{Danilo Freire}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# install.packages("Synth")}
\CommentTok{\# install.packages("gsynth")}
\FunctionTok{library}\NormalTok{(}\StringTok{"Synth"}\NormalTok{)}
\FunctionTok{library}\NormalTok{(}\StringTok{"gsynth"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

simulate data for 10 states and 30 years. State A receives the treatment \texttt{T\ =\ 20} after year 15.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{year         }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{30}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{state        }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(LETTERS[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{], }\AttributeTok{each =} \DecValTok{30}\NormalTok{)}
\NormalTok{X1           }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{300}\NormalTok{, }\AttributeTok{mean =} \DecValTok{2}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{), }\DecValTok{2}\NormalTok{)}
\NormalTok{X2           }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(}\FunctionTok{rbinom}\NormalTok{(}\DecValTok{300}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{300}\NormalTok{), }\DecValTok{2}\NormalTok{)}
\NormalTok{Y            }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(}\DecValTok{1} \SpecialCharTok{+} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ X1 }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{300}\NormalTok{), }\DecValTok{2}\NormalTok{)}
\NormalTok{df           }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(Y, X1, X2, state, year))}
\NormalTok{df}\SpecialCharTok{$}\NormalTok{Y         }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{as.character}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{Y))}
\NormalTok{df}\SpecialCharTok{$}\NormalTok{X1        }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{as.character}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{X1))}
\NormalTok{df}\SpecialCharTok{$}\NormalTok{X2        }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{as.character}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{X2))}
\NormalTok{df}\SpecialCharTok{$}\NormalTok{year      }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{as.character}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{year))}
\NormalTok{df}\SpecialCharTok{$}\NormalTok{state.num }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{, }\AttributeTok{each =} \DecValTok{30}\NormalTok{)}
\NormalTok{df}\SpecialCharTok{$}\NormalTok{state     }\OtherTok{\textless{}{-}} \FunctionTok{as.character}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{state)}
\NormalTok{df}\SpecialCharTok{$}\StringTok{\textasciigrave{}}\AttributeTok{T}\StringTok{\textasciigrave{}}       \OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{state }\SpecialCharTok{==} \StringTok{"A"} \SpecialCharTok{\&}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{year }\SpecialCharTok{\textgreater{}=} \DecValTok{15}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{df}\SpecialCharTok{$}\NormalTok{Y         }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{state }\SpecialCharTok{==} \StringTok{"A"} \SpecialCharTok{\&}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{year }\SpecialCharTok{\textgreater{}=} \DecValTok{15}\NormalTok{, }
\NormalTok{                       df}\SpecialCharTok{$}\NormalTok{Y }\SpecialCharTok{+} \DecValTok{20}\NormalTok{, df}\SpecialCharTok{$}\NormalTok{Y)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(df)}
\CommentTok{\#\textgreater{} \textquotesingle{}data.frame\textquotesingle{}:    300 obs. of  7 variables:}
\CommentTok{\#\textgreater{}  $ Y        : num  2.29 4.51 2.07 8.87 4.37 1.32 8 7.49 6.98 3.72 ...}
\CommentTok{\#\textgreater{}  $ X1       : num  1.37 2.18 1.16 3.6 2.33 1.18 2.49 2.74 2.58 1.69 ...}
\CommentTok{\#\textgreater{}  $ X2       : num  1.96 0.4 {-}0.75 {-}0.56 {-}0.45 1.06 0.51 {-}2.1 0 0.54 ...}
\CommentTok{\#\textgreater{}  $ state    : chr  "A" "A" "A" "A" ...}
\CommentTok{\#\textgreater{}  $ year     : num  1 2 3 4 5 6 7 8 9 10 ...}
\CommentTok{\#\textgreater{}  $ state.num: int  1 1 1 1 1 1 1 1 1 1 ...}
\CommentTok{\#\textgreater{}  $ T        : num  0 0 0 0 0 0 0 0 0 0 ...}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataprep.out }\OtherTok{\textless{}{-}}
    \FunctionTok{dataprep}\NormalTok{(}
\NormalTok{        df,}
        \AttributeTok{predictors            =} \FunctionTok{c}\NormalTok{(}\StringTok{"X1"}\NormalTok{, }\StringTok{"X2"}\NormalTok{),}
        \AttributeTok{dependent             =} \StringTok{"Y"}\NormalTok{,}
        \AttributeTok{unit.variable         =} \StringTok{"state.num"}\NormalTok{,}
        \AttributeTok{time.variable         =} \StringTok{"year"}\NormalTok{,}
        \AttributeTok{unit.names.variable   =} \StringTok{"state"}\NormalTok{,}
        \AttributeTok{treatment.identifier  =} \DecValTok{1}\NormalTok{,}
        \AttributeTok{controls.identifier   =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\SpecialCharTok{:}\DecValTok{10}\NormalTok{),}
        \AttributeTok{time.predictors.prior =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{14}\NormalTok{),}
        \AttributeTok{time.optimize.ssr     =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{14}\NormalTok{),}
        \AttributeTok{time.plot             =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{30}\NormalTok{)}
\NormalTok{    )}


\NormalTok{synth.out }\OtherTok{\textless{}{-}} \FunctionTok{synth}\NormalTok{(dataprep.out)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} X1, X0, Z1, Z0 all come directly from dataprep object.}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} **************** }
\CommentTok{\#\textgreater{}  searching for synthetic control unit  }
\CommentTok{\#\textgreater{}  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} **************** }
\CommentTok{\#\textgreater{} **************** }
\CommentTok{\#\textgreater{} **************** }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} MSPE (LOSS V): 9.831789 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} solution.v:}
\CommentTok{\#\textgreater{}  0.3888387 0.6111613 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} solution.w:}
\CommentTok{\#\textgreater{}  0.1115941 0.1832781 0.1027237 0.312091 0.06096758 0.03509706 0.05893735 0.05746256 0.07784853}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(synth.tables   }\OtherTok{\textless{}{-}} \FunctionTok{synth.tab}\NormalTok{(}
        \AttributeTok{dataprep.res =}\NormalTok{ dataprep.out,}
        \AttributeTok{synth.res    =}\NormalTok{ synth.out)}
\NormalTok{      )}
\CommentTok{\#\textgreater{} $tab.pred}
\CommentTok{\#\textgreater{}    Treated Synthetic Sample Mean}
\CommentTok{\#\textgreater{} X1   2.028     2.028       2.017}
\CommentTok{\#\textgreater{} X2   0.513     0.513       0.394}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $tab.v}
\CommentTok{\#\textgreater{}    v.weights}
\CommentTok{\#\textgreater{} X1 0.389    }
\CommentTok{\#\textgreater{} X2 0.611    }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $tab.w}
\CommentTok{\#\textgreater{}    w.weights unit.names unit.numbers}
\CommentTok{\#\textgreater{} 2      0.112          B            2}
\CommentTok{\#\textgreater{} 3      0.183          C            3}
\CommentTok{\#\textgreater{} 4      0.103          D            4}
\CommentTok{\#\textgreater{} 5      0.312          E            5}
\CommentTok{\#\textgreater{} 6      0.061          F            6}
\CommentTok{\#\textgreater{} 7      0.035          G            7}
\CommentTok{\#\textgreater{} 8      0.059          H            8}
\CommentTok{\#\textgreater{} 9      0.057          I            9}
\CommentTok{\#\textgreater{} 10     0.078          J           10}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $tab.loss}
\CommentTok{\#\textgreater{}            Loss W   Loss V}
\CommentTok{\#\textgreater{} [1,] 9.761708e{-}12 9.831789}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{path.plot}\NormalTok{(}\AttributeTok{synth.res    =}\NormalTok{ synth.out,}
          \AttributeTok{dataprep.res =}\NormalTok{ dataprep.out,}
          \AttributeTok{Ylab         =} \FunctionTok{c}\NormalTok{(}\StringTok{"Y"}\NormalTok{),}
          \AttributeTok{Xlab         =} \FunctionTok{c}\NormalTok{(}\StringTok{"Year"}\NormalTok{),}
          \AttributeTok{Legend       =} \FunctionTok{c}\NormalTok{(}\StringTok{"State A"}\NormalTok{,}\StringTok{"Synthetic State A"}\NormalTok{),}
          \AttributeTok{Legend.position =} \FunctionTok{c}\NormalTok{(}\StringTok{"topleft"}\NormalTok{)}
\NormalTok{)}

\FunctionTok{abline}\NormalTok{(}\AttributeTok{v   =} \DecValTok{15}\NormalTok{,}
       \AttributeTok{lty =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{27-synthetic-control_files/figure-latex/unnamed-chunk-6-1} \end{center}

Gaps plot:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gaps.plot}\NormalTok{(}\AttributeTok{synth.res    =}\NormalTok{ synth.out,}
          \AttributeTok{dataprep.res =}\NormalTok{ dataprep.out,}
          \AttributeTok{Ylab         =} \FunctionTok{c}\NormalTok{(}\StringTok{"Gap"}\NormalTok{),}
          \AttributeTok{Xlab         =} \FunctionTok{c}\NormalTok{(}\StringTok{"Year"}\NormalTok{),}
          \AttributeTok{Ylim         =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{30}\NormalTok{, }\DecValTok{30}\NormalTok{),}
          \AttributeTok{Main         =} \StringTok{""}
\NormalTok{)}

\FunctionTok{abline}\NormalTok{(}\AttributeTok{v   =} \DecValTok{15}\NormalTok{,}
       \AttributeTok{lty =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{27-synthetic-control_files/figure-latex/unnamed-chunk-7-1} \end{center}

Alternatively, \texttt{gsynth} provides options to estimate iterative fixed effects, and handle multiple treated units at tat time.

Here, we use two=way fixed effects and bootstrapped standard errors

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gsynth.out }\OtherTok{\textless{}{-}} \FunctionTok{gsynth}\NormalTok{(}
\NormalTok{  Y }\SpecialCharTok{\textasciitilde{}} \StringTok{\textasciigrave{}}\AttributeTok{T}\StringTok{\textasciigrave{}} \SpecialCharTok{+}\NormalTok{ X1 }\SpecialCharTok{+}\NormalTok{ X2,}
  \AttributeTok{data =}\NormalTok{ df,}
  \AttributeTok{index =} \FunctionTok{c}\NormalTok{(}\StringTok{"state"}\NormalTok{, }\StringTok{"year"}\NormalTok{),}
  \AttributeTok{force =} \StringTok{"two{-}way"}\NormalTok{,}
  \AttributeTok{CV =} \ConstantTok{TRUE}\NormalTok{,}
  \AttributeTok{r =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{),}
  \AttributeTok{se =} \ConstantTok{TRUE}\NormalTok{,}
  \AttributeTok{inference =} \StringTok{"parametric"}\NormalTok{,}
  \AttributeTok{nboots =} \DecValTok{1000}\NormalTok{,}
  \AttributeTok{parallel =}\NormalTok{ F }\CommentTok{\# TRUE}
\NormalTok{)}
\CommentTok{\#\textgreater{} Cross{-}validating ... }
\CommentTok{\#\textgreater{}  r = 0; sigma2 = 1.13533; IC = 0.95632; PC = 0.96713; MSPE = 1.65502}
\CommentTok{\#\textgreater{}  r = 1; sigma2 = 0.96885; IC = 1.54420; PC = 4.30644; MSPE = 1.33375}
\CommentTok{\#\textgreater{}  r = 2; sigma2 = 0.81855; IC = 2.08062; PC = 6.58556; MSPE = 1.27341*}
\CommentTok{\#\textgreater{}  r = 3; sigma2 = 0.71670; IC = 2.61125; PC = 8.35187; MSPE = 1.79319}
\CommentTok{\#\textgreater{}  r = 4; sigma2 = 0.62823; IC = 3.10156; PC = 9.59221; MSPE = 2.02301}
\CommentTok{\#\textgreater{}  r = 5; sigma2 = 0.55497; IC = 3.55814; PC = 10.48406; MSPE = 2.79596}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  r* = 2}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\NormalTok{Simulating errors .............}
\NormalTok{Bootstrapping ...}
\CommentTok{\#\textgreater{} ..........}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(gsynth.out)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{27-synthetic-control_files/figure-latex/unnamed-chunk-9-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(gsynth.out, }\AttributeTok{type =} \StringTok{"counterfactual"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{27-synthetic-control_files/figure-latex/unnamed-chunk-10-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(gsynth.out, }\AttributeTok{type =} \StringTok{"counterfactual"}\NormalTok{, }\AttributeTok{raw =} \StringTok{"all"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{27-synthetic-control_files/figure-latex/unnamed-chunk-11-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# shows estimations for the control cases}
\end{Highlighting}
\end{Shaded}

\hypertarget{example-2-1}{%
\subsection{Example 2}\label{example-2-1}}

by \href{https://towardsdatascience.com/causal-inference-using-synthetic-control-the-ultimate-guide-a622ad5cf827}{Leihua Ye}

\begin{Shaded}
\begin{Highlighting}[]

\FunctionTok{library}\NormalTok{(Synth)}
\FunctionTok{data}\NormalTok{(}\StringTok{"basque"}\NormalTok{)}
\FunctionTok{dim}\NormalTok{(basque) }\CommentTok{\#774*17}
\CommentTok{\#\textgreater{} [1] 774  17}
\FunctionTok{head}\NormalTok{(basque)}
\CommentTok{\#\textgreater{}   regionno     regionname year   gdpcap sec.agriculture sec.energy sec.industry}
\CommentTok{\#\textgreater{} 1        1 Spain (Espana) 1955 2.354542              NA         NA           NA}
\CommentTok{\#\textgreater{} 2        1 Spain (Espana) 1956 2.480149              NA         NA           NA}
\CommentTok{\#\textgreater{} 3        1 Spain (Espana) 1957 2.603613              NA         NA           NA}
\CommentTok{\#\textgreater{} 4        1 Spain (Espana) 1958 2.637104              NA         NA           NA}
\CommentTok{\#\textgreater{} 5        1 Spain (Espana) 1959 2.669880              NA         NA           NA}
\CommentTok{\#\textgreater{} 6        1 Spain (Espana) 1960 2.869966              NA         NA           NA}
\CommentTok{\#\textgreater{}   sec.construction sec.services.venta sec.services.nonventa school.illit}
\CommentTok{\#\textgreater{} 1               NA                 NA                    NA           NA}
\CommentTok{\#\textgreater{} 2               NA                 NA                    NA           NA}
\CommentTok{\#\textgreater{} 3               NA                 NA                    NA           NA}
\CommentTok{\#\textgreater{} 4               NA                 NA                    NA           NA}
\CommentTok{\#\textgreater{} 5               NA                 NA                    NA           NA}
\CommentTok{\#\textgreater{} 6               NA                 NA                    NA           NA}
\CommentTok{\#\textgreater{}   school.prim school.med school.high school.post.high popdens invest}
\CommentTok{\#\textgreater{} 1          NA         NA          NA               NA      NA     NA}
\CommentTok{\#\textgreater{} 2          NA         NA          NA               NA      NA     NA}
\CommentTok{\#\textgreater{} 3          NA         NA          NA               NA      NA     NA}
\CommentTok{\#\textgreater{} 4          NA         NA          NA               NA      NA     NA}
\CommentTok{\#\textgreater{} 5          NA         NA          NA               NA      NA     NA}
\CommentTok{\#\textgreater{} 6          NA         NA          NA               NA      NA     NA}
\end{Highlighting}
\end{Shaded}

transform data to be used in \texttt{synth()}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataprep.out }\OtherTok{\textless{}{-}} \FunctionTok{dataprep}\NormalTok{(}
    \AttributeTok{foo =}\NormalTok{ basque,}
    \AttributeTok{predictors =} \FunctionTok{c}\NormalTok{(}
        \StringTok{"school.illit"}\NormalTok{,}
        \StringTok{"school.prim"}\NormalTok{,}
        \StringTok{"school.med"}\NormalTok{,}
        \StringTok{"school.high"}\NormalTok{,}
        \StringTok{"school.post.high"}\NormalTok{,}
        \StringTok{"invest"}
\NormalTok{    ),}
    \AttributeTok{predictors.op =}  \StringTok{"mean"}\NormalTok{,}
    \CommentTok{\# the operator}
    \AttributeTok{time.predictors.prior =} \DecValTok{1964}\SpecialCharTok{:}\DecValTok{1969}\NormalTok{,}
    \CommentTok{\#the entire time frame from the \#beginning to the end}
    \AttributeTok{special.predictors =} \FunctionTok{list}\NormalTok{(}
        \FunctionTok{list}\NormalTok{(}\StringTok{"gdpcap"}\NormalTok{, }\DecValTok{1960}\SpecialCharTok{:}\DecValTok{1969}\NormalTok{,  }\StringTok{"mean"}\NormalTok{),}
        \FunctionTok{list}\NormalTok{(}\StringTok{"sec.agriculture"}\NormalTok{, }\FunctionTok{seq}\NormalTok{(}\DecValTok{1961}\NormalTok{, }\DecValTok{1969}\NormalTok{, }\DecValTok{2}\NormalTok{), }\StringTok{"mean"}\NormalTok{),}
        \FunctionTok{list}\NormalTok{(}\StringTok{"sec.energy"}\NormalTok{, }\FunctionTok{seq}\NormalTok{(}\DecValTok{1961}\NormalTok{, }\DecValTok{1969}\NormalTok{, }\DecValTok{2}\NormalTok{), }\StringTok{"mean"}\NormalTok{),}
        \FunctionTok{list}\NormalTok{(}\StringTok{"sec.industry"}\NormalTok{, }\FunctionTok{seq}\NormalTok{(}\DecValTok{1961}\NormalTok{, }\DecValTok{1969}\NormalTok{, }\DecValTok{2}\NormalTok{), }\StringTok{"mean"}\NormalTok{),}
        \FunctionTok{list}\NormalTok{(}\StringTok{"sec.construction"}\NormalTok{, }\FunctionTok{seq}\NormalTok{(}\DecValTok{1961}\NormalTok{, }\DecValTok{1969}\NormalTok{, }\DecValTok{2}\NormalTok{), }\StringTok{"mean"}\NormalTok{),}
        \FunctionTok{list}\NormalTok{(}\StringTok{"sec.services.venta"}\NormalTok{, }\FunctionTok{seq}\NormalTok{(}\DecValTok{1961}\NormalTok{, }\DecValTok{1969}\NormalTok{, }\DecValTok{2}\NormalTok{), }\StringTok{"mean"}\NormalTok{),}
        \FunctionTok{list}\NormalTok{(}\StringTok{"sec.services.nonventa"}\NormalTok{, }\FunctionTok{seq}\NormalTok{(}\DecValTok{1961}\NormalTok{, }\DecValTok{1969}\NormalTok{, }\DecValTok{2}\NormalTok{), }\StringTok{"mean"}\NormalTok{),}
        \FunctionTok{list}\NormalTok{(}\StringTok{"popdens"}\NormalTok{, }\DecValTok{1969}\NormalTok{,  }\StringTok{"mean"}\NormalTok{)}
\NormalTok{    ),}
    \AttributeTok{dependent =}  \StringTok{"gdpcap"}\NormalTok{,}
    \CommentTok{\# dv}
    \AttributeTok{unit.variable =}  \StringTok{"regionno"}\NormalTok{,}
    \CommentTok{\#identifying unit numbers}
    \AttributeTok{unit.names.variable =}  \StringTok{"regionname"}\NormalTok{,}
    \CommentTok{\#identifying unit names}
    \AttributeTok{time.variable =}  \StringTok{"year"}\NormalTok{,}
    \CommentTok{\#time{-}periods}
    \AttributeTok{treatment.identifier =} \DecValTok{17}\NormalTok{,}
    \CommentTok{\#the treated case}
    \AttributeTok{controls.identifier =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\SpecialCharTok{:}\DecValTok{16}\NormalTok{, }\DecValTok{18}\NormalTok{),}
    \CommentTok{\#the control cases; all others \#except number 17}
    \AttributeTok{time.optimize.ssr =} \DecValTok{1960}\SpecialCharTok{:}\DecValTok{1969}\NormalTok{,}
    \CommentTok{\#the time{-}period over which to optimize}
    \AttributeTok{time.plot =} \DecValTok{1955}\SpecialCharTok{:}\DecValTok{1997}
\NormalTok{) }\CommentTok{\#the entire time period before/after the treatment}
\end{Highlighting}
\end{Shaded}

where

\begin{itemize}
\item
  \(X_1\) = the control case before the treatment
\item
  \(X_0\) = the control cases after the treatment
\item
  \(Z_1\): the treatment case before the treatment
\item
  \(Z_0\): the treatment case after the treatment
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{synth.out }\OtherTok{=} \FunctionTok{synth}\NormalTok{(}\AttributeTok{data.prep.obj =}\NormalTok{ dataprep.out, }\AttributeTok{method =} \StringTok{"BFGS"}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} X1, X0, Z1, Z0 all come directly from dataprep object.}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} **************** }
\CommentTok{\#\textgreater{}  searching for synthetic control unit  }
\CommentTok{\#\textgreater{}  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} **************** }
\CommentTok{\#\textgreater{} **************** }
\CommentTok{\#\textgreater{} **************** }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} MSPE (LOSS V): 0.008864606 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} solution.v:}
\CommentTok{\#\textgreater{}  0.02773094 1.194e{-}07 1.60609e{-}05 0.0007163836 1.486e{-}07 0.002423908 0.0587055 0.2651997 0.02851006 0.291276 0.007994382 0.004053188 0.009398579 0.303975 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} solution.w:}
\CommentTok{\#\textgreater{}  2.53e{-}08 4.63e{-}08 6.44e{-}08 2.81e{-}08 3.37e{-}08 4.844e{-}07 4.2e{-}08 4.69e{-}08 0.8508145 9.75e{-}08 3.2e{-}08 5.54e{-}08 0.1491843 4.86e{-}08 9.89e{-}08 1.162e{-}07}
\end{Highlighting}
\end{Shaded}

Calculate the difference between the real basque region and the synthetic control

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gaps }\OtherTok{=}\NormalTok{ dataprep.out}\SpecialCharTok{$}\NormalTok{Y1plot }\SpecialCharTok{{-}}\NormalTok{ (dataprep.out}\SpecialCharTok{$}\NormalTok{Y0plot }
                                     \SpecialCharTok{\%*\%}\NormalTok{ synth.out}\SpecialCharTok{$}\NormalTok{solution.w)}
\NormalTok{gaps[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{]}
\CommentTok{\#\textgreater{}       1955       1956       1957 }
\CommentTok{\#\textgreater{} 0.15023473 0.09168035 0.03716475}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{synth.tables }\OtherTok{=} \FunctionTok{synth.tab}\NormalTok{(}\AttributeTok{dataprep.res =}\NormalTok{ dataprep.out,}
                         \AttributeTok{synth.res =}\NormalTok{ synth.out)}
\FunctionTok{names}\NormalTok{(synth.tables)}
\CommentTok{\#\textgreater{} [1] "tab.pred" "tab.v"    "tab.w"    "tab.loss"}
\NormalTok{synth.tables}\SpecialCharTok{$}\NormalTok{tab.pred[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{13}\NormalTok{,]}
\CommentTok{\#\textgreater{}                                          Treated Synthetic Sample Mean}
\CommentTok{\#\textgreater{} school.illit                              39.888   256.337     170.786}
\CommentTok{\#\textgreater{} school.prim                             1031.742  2730.104    1127.186}
\CommentTok{\#\textgreater{} school.med                                90.359   223.340      76.260}
\CommentTok{\#\textgreater{} school.high                               25.728    63.437      24.235}
\CommentTok{\#\textgreater{} school.post.high                          13.480    36.153      13.478}
\CommentTok{\#\textgreater{} invest                                    24.647    21.583      21.424}
\CommentTok{\#\textgreater{} special.gdpcap.1960.1969                   5.285     5.271       3.581}
\CommentTok{\#\textgreater{} special.sec.agriculture.1961.1969          6.844     6.179      21.353}
\CommentTok{\#\textgreater{} special.sec.energy.1961.1969               4.106     2.760       5.310}
\CommentTok{\#\textgreater{} special.sec.industry.1961.1969            45.082    37.636      22.425}
\CommentTok{\#\textgreater{} special.sec.construction.1961.1969         6.150     6.952       7.276}
\CommentTok{\#\textgreater{} special.sec.services.venta.1961.1969      33.754    41.104      36.528}
\CommentTok{\#\textgreater{} special.sec.services.nonventa.1961.1969    4.072     5.371       7.111}
\end{Highlighting}
\end{Shaded}

Relative importance of each unit

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{synth.tables}\SpecialCharTok{$}\NormalTok{tab.w[}\DecValTok{8}\SpecialCharTok{:}\DecValTok{14}\NormalTok{, ]}
\CommentTok{\#\textgreater{}    w.weights            unit.names unit.numbers}
\CommentTok{\#\textgreater{} 9      0.000    Castilla{-}La Mancha            9}
\CommentTok{\#\textgreater{} 10     0.851              Cataluna           10}
\CommentTok{\#\textgreater{} 11     0.000  Comunidad Valenciana           11}
\CommentTok{\#\textgreater{} 12     0.000           Extremadura           12}
\CommentTok{\#\textgreater{} 13     0.000               Galicia           13}
\CommentTok{\#\textgreater{} 14     0.149 Madrid (Comunidad De)           14}
\CommentTok{\#\textgreater{} 15     0.000    Murcia (Region de)           15}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# plot the changes before and after the treatment }
\FunctionTok{path.plot}\NormalTok{(}
    \AttributeTok{synth.res =}\NormalTok{ synth.out,}
    \AttributeTok{dataprep.res =}\NormalTok{ dataprep.out,}
    \AttributeTok{Ylab =} \StringTok{"real per{-}capita gdp (1986 USD, thousand)"}\NormalTok{,}
    \AttributeTok{Xlab =} \StringTok{"year"}\NormalTok{,}
    \AttributeTok{Ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{12}\NormalTok{),}
    \AttributeTok{Legend =} \FunctionTok{c}\NormalTok{(}\StringTok{"Basque country"}\NormalTok{,}
               \StringTok{"synthetic Basque country"}\NormalTok{),}
    \AttributeTok{Legend.position =} \StringTok{"bottomright"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{27-synthetic-control_files/figure-latex/unnamed-chunk-18-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gaps.plot}\NormalTok{(}
    \AttributeTok{synth.res =}\NormalTok{ synth.out,}
    \AttributeTok{dataprep.res =}\NormalTok{ dataprep.out,}
    \AttributeTok{Ylab =}  \StringTok{"gap in real per {-} capita GDP (1986 USD, thousand)"}\NormalTok{,}
    \AttributeTok{Xlab =}  \StringTok{"year"}\NormalTok{,}
    \AttributeTok{Ylim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{1.5}\NormalTok{, }\FloatTok{1.5}\NormalTok{),}
    \AttributeTok{Main =} \ConstantTok{NA}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{27-synthetic-control_files/figure-latex/unnamed-chunk-19-1} \end{center}

Doubly Robust Difference-in-Differences

Example from \texttt{DRDID} package

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(DRDID)}
\FunctionTok{data}\NormalTok{(nsw\_long)}
\CommentTok{\# Form the Lalonde sample with CPS comparison group}
\NormalTok{eval\_lalonde\_cps }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(nsw\_long, nsw\_long}\SpecialCharTok{$}\NormalTok{treated }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{|} 
\NormalTok{                               nsw\_long}\SpecialCharTok{$}\NormalTok{sample }\SpecialCharTok{==} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Estimate Average Treatment Effect on Treated using Improved Locally Efficient Doubly Robust DID estimator

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{out }\OtherTok{\textless{}{-}}
    \FunctionTok{drdid}\NormalTok{(}
        \AttributeTok{yname =} \StringTok{"re"}\NormalTok{,}
        \AttributeTok{tname =} \StringTok{"year"}\NormalTok{,}
        \AttributeTok{idname =} \StringTok{"id"}\NormalTok{,}
        \AttributeTok{dname =} \StringTok{"experimental"}\NormalTok{,}
        \AttributeTok{xformla =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ educ }\SpecialCharTok{+}\NormalTok{ black }\SpecialCharTok{+}\NormalTok{ married }\SpecialCharTok{+}\NormalTok{ nodegree }\SpecialCharTok{+}\NormalTok{ hisp }\SpecialCharTok{+}\NormalTok{ re74,}
        \AttributeTok{data =}\NormalTok{ eval\_lalonde\_cps,}
        \AttributeTok{panel =} \ConstantTok{TRUE}
\NormalTok{    )}
\FunctionTok{summary}\NormalTok{(out)}
\CommentTok{\#\textgreater{}  Call:}
\CommentTok{\#\textgreater{} drdid(yname = "re", tname = "year", idname = "id", dname = "experimental", }
\CommentTok{\#\textgreater{}     xformla = \textasciitilde{}age + educ + black + married + nodegree + hisp + }
\CommentTok{\#\textgreater{}         re74, data = eval\_lalonde\_cps, panel = TRUE)}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}  Further improved locally efficient DR DID estimator for the ATT:}
\CommentTok{\#\textgreater{}  }
\CommentTok{\#\textgreater{}    ATT     Std. Error  t value    Pr(\textgreater{}|t|)  [95\% Conf. Interval] }
\CommentTok{\#\textgreater{} {-}901.2703   393.6247   {-}2.2897     0.022    {-}1672.7747  {-}129.766 }
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}  Estimator based on panel data.}
\CommentTok{\#\textgreater{}  Outcome regression est. method: weighted least squares.}
\CommentTok{\#\textgreater{}  Propensity score est. method: inverse prob. tilting.}
\CommentTok{\#\textgreater{}  Analytical standard error.}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}  See Sant\textquotesingle{}Anna and Zhao (2020) for details.}
\end{Highlighting}
\end{Shaded}

\hypertarget{example-3-1}{%
\subsection{Example 3}\label{example-3-1}}

by \texttt{Synth} package's authors

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(Synth)}
\FunctionTok{data}\NormalTok{(}\StringTok{"basque"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\texttt{synth()} requires

\begin{itemize}
\item
  \(X_1\) vector of treatment predictors
\item
  \(X_0\) matrix of same variables for control group
\item
  \(Z_1\) vector of outcome variable for treatment group
\item
  \(Z_0\) matrix of outcome variable for control group
\end{itemize}

use \texttt{dataprep()} to prepare data in the format that can be used throughout the \texttt{Synth} package

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataprep.out }\OtherTok{\textless{}{-}} \FunctionTok{dataprep}\NormalTok{(}
    \AttributeTok{foo =}\NormalTok{ basque,}
    \AttributeTok{predictors =} \FunctionTok{c}\NormalTok{(}
        \StringTok{"school.illit"}\NormalTok{,}
        \StringTok{"school.prim"}\NormalTok{,}
        \StringTok{"school.med"}\NormalTok{,}
        \StringTok{"school.high"}\NormalTok{,}
        \StringTok{"school.post.high"}\NormalTok{,}
        \StringTok{"invest"}
\NormalTok{    ),}
    \AttributeTok{predictors.op =} \StringTok{"mean"}\NormalTok{,}
    \AttributeTok{time.predictors.prior =} \DecValTok{1964}\SpecialCharTok{:}\DecValTok{1969}\NormalTok{,}
    \AttributeTok{special.predictors =} \FunctionTok{list}\NormalTok{(}
        \FunctionTok{list}\NormalTok{(}\StringTok{"gdpcap"}\NormalTok{, }\DecValTok{1960}\SpecialCharTok{:}\DecValTok{1969}\NormalTok{ , }\StringTok{"mean"}\NormalTok{),}
        \FunctionTok{list}\NormalTok{(}\StringTok{"sec.agriculture"}\NormalTok{, }\FunctionTok{seq}\NormalTok{(}\DecValTok{1961}\NormalTok{, }\DecValTok{1969}\NormalTok{, }\DecValTok{2}\NormalTok{), }\StringTok{"mean"}\NormalTok{),}
        \FunctionTok{list}\NormalTok{(}\StringTok{"sec.energy"}\NormalTok{, }\FunctionTok{seq}\NormalTok{(}\DecValTok{1961}\NormalTok{, }\DecValTok{1969}\NormalTok{, }\DecValTok{2}\NormalTok{), }\StringTok{"mean"}\NormalTok{),}
        \FunctionTok{list}\NormalTok{(}\StringTok{"sec.industry"}\NormalTok{, }\FunctionTok{seq}\NormalTok{(}\DecValTok{1961}\NormalTok{, }\DecValTok{1969}\NormalTok{, }\DecValTok{2}\NormalTok{), }\StringTok{"mean"}\NormalTok{),}
        \FunctionTok{list}\NormalTok{(}\StringTok{"sec.construction"}\NormalTok{, }\FunctionTok{seq}\NormalTok{(}\DecValTok{1961}\NormalTok{, }\DecValTok{1969}\NormalTok{, }\DecValTok{2}\NormalTok{), }\StringTok{"mean"}\NormalTok{),}
        \FunctionTok{list}\NormalTok{(}\StringTok{"sec.services.venta"}\NormalTok{, }\FunctionTok{seq}\NormalTok{(}\DecValTok{1961}\NormalTok{, }\DecValTok{1969}\NormalTok{, }\DecValTok{2}\NormalTok{), }\StringTok{"mean"}\NormalTok{),}
        \FunctionTok{list}\NormalTok{(}\StringTok{"sec.services.nonventa"}\NormalTok{, }\FunctionTok{seq}\NormalTok{(}\DecValTok{1961}\NormalTok{, }\DecValTok{1969}\NormalTok{, }\DecValTok{2}\NormalTok{), }\StringTok{"mean"}\NormalTok{),}
        \FunctionTok{list}\NormalTok{(}\StringTok{"popdens"}\NormalTok{, }\DecValTok{1969}\NormalTok{, }\StringTok{"mean"}\NormalTok{)}
\NormalTok{    ),}
    \AttributeTok{dependent =} \StringTok{"gdpcap"}\NormalTok{,}
    \AttributeTok{unit.variable =} \StringTok{"regionno"}\NormalTok{,}
    \AttributeTok{unit.names.variable =} \StringTok{"regionname"}\NormalTok{,}
    \AttributeTok{time.variable =} \StringTok{"year"}\NormalTok{,}
    \AttributeTok{treatment.identifier =} \DecValTok{17}\NormalTok{,}
    \AttributeTok{controls.identifier =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\SpecialCharTok{:}\DecValTok{16}\NormalTok{, }\DecValTok{18}\NormalTok{),}
    \AttributeTok{time.optimize.ssr =} \DecValTok{1960}\SpecialCharTok{:}\DecValTok{1969}\NormalTok{,}
    \AttributeTok{time.plot =} \DecValTok{1955}\SpecialCharTok{:}\DecValTok{1997}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

find optimal weights that identifies the synthetic control for the treatment group

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{synth.out }\OtherTok{\textless{}{-}} \FunctionTok{synth}\NormalTok{(}\AttributeTok{data.prep.obj =}\NormalTok{ dataprep.out, }\AttributeTok{method =} \StringTok{"BFGS"}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} X1, X0, Z1, Z0 all come directly from dataprep object.}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} **************** }
\CommentTok{\#\textgreater{}  searching for synthetic control unit  }
\CommentTok{\#\textgreater{}  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} **************** }
\CommentTok{\#\textgreater{} **************** }
\CommentTok{\#\textgreater{} **************** }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} MSPE (LOSS V): 0.008864606 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} solution.v:}
\CommentTok{\#\textgreater{}  0.02773094 1.194e{-}07 1.60609e{-}05 0.0007163836 1.486e{-}07 0.002423908 0.0587055 0.2651997 0.02851006 0.291276 0.007994382 0.004053188 0.009398579 0.303975 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} solution.w:}
\CommentTok{\#\textgreater{}  2.53e{-}08 4.63e{-}08 6.44e{-}08 2.81e{-}08 3.37e{-}08 4.844e{-}07 4.2e{-}08 4.69e{-}08 0.8508145 9.75e{-}08 3.2e{-}08 5.54e{-}08 0.1491843 4.86e{-}08 9.89e{-}08 1.162e{-}07}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gaps }\OtherTok{\textless{}{-}}\NormalTok{ dataprep.out}\SpecialCharTok{$}\NormalTok{Y1plot }\SpecialCharTok{{-}}\NormalTok{ (dataprep.out}\SpecialCharTok{$}\NormalTok{Y0plot }\SpecialCharTok{\%*\%}\NormalTok{ synth.out}\SpecialCharTok{$}\NormalTok{solution.w)}
\NormalTok{gaps[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{]}
\CommentTok{\#\textgreater{}       1955       1956       1957 }
\CommentTok{\#\textgreater{} 0.15023473 0.09168035 0.03716475}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{synth.tables }\OtherTok{\textless{}{-}}
    \FunctionTok{synth.tab}\NormalTok{(}\AttributeTok{dataprep.res =}\NormalTok{ dataprep.out, }\AttributeTok{synth.res =}\NormalTok{ synth.out)}
\FunctionTok{names}\NormalTok{(synth.tables) }\CommentTok{\# you can pick tables to see }
\CommentTok{\#\textgreater{} [1] "tab.pred" "tab.v"    "tab.w"    "tab.loss"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{path.plot}\NormalTok{(}
    \AttributeTok{synth.res =}\NormalTok{ synth.out,}
    \AttributeTok{dataprep.res =}\NormalTok{ dataprep.out,}
    \AttributeTok{Ylab =} \StringTok{"real per{-}capita GDP (1986 USD, thousand)"}\NormalTok{,}
    \AttributeTok{Xlab =} \StringTok{"year"}\NormalTok{,}
    \AttributeTok{Ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{12}\NormalTok{),}
    \AttributeTok{Legend =} \FunctionTok{c}\NormalTok{(}\StringTok{"Basque country"}\NormalTok{,}
               \StringTok{"synthetic Basque country"}\NormalTok{),}
    \AttributeTok{Legend.position =} \StringTok{"bottomright"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{27-synthetic-control_files/figure-latex/unnamed-chunk-27-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gaps.plot}\NormalTok{(}
    \AttributeTok{synth.res =}\NormalTok{ synth.out,}
    \AttributeTok{dataprep.res =}\NormalTok{ dataprep.out,}
    \AttributeTok{Ylab =} \StringTok{"gap in real per{-}capita GDP (1986 USD, thousand)"}\NormalTok{,}
    \AttributeTok{Xlab =} \StringTok{"year"}\NormalTok{,}
    \AttributeTok{Ylim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{1.5}\NormalTok{, }\FloatTok{1.5}\NormalTok{),}
    \AttributeTok{Main =} \ConstantTok{NA}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{27-synthetic-control_files/figure-latex/unnamed-chunk-28-1} \end{center}

You could also run placebo tests

\hypertarget{example-4-1}{%
\subsection{Example 4}\label{example-4-1}}

by \href{https://cran.r-project.org/web/packages/microsynth/vignettes/introduction.html}{Michael Robbins and Steven Davenport} who are authors of \texttt{MicroSynth} with the following improvements:

\begin{itemize}
\item
  Standardization \texttt{use.survey\ =\ TRUE} and permutation ( \texttt{perm\ =\ 250} and \texttt{jack\ =\ TRUE} ) for placebo tests
\item
  Omnibus statistic (set to \texttt{omnibus.var} ) for multiple outcome variables
\item
  incorporate multiple follow-up periods \texttt{end.post}
\end{itemize}

Notes:

\begin{itemize}
\item
  Both predictors and outcome will be used to match units before intervention

  \begin{itemize}
  \item
    Outcome variable has to be \textbf{time-variant}
  \item
    Predictors are \textbf{time-invariant}
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# right now the package is not availabe for R version 4.2}
\FunctionTok{library}\NormalTok{(microsynth)}
\FunctionTok{data}\NormalTok{(}\StringTok{"seattledmi"}\NormalTok{)}


\NormalTok{cov.var }\OtherTok{\textless{}{-}}
    \FunctionTok{c}\NormalTok{(}
        \StringTok{"TotalPop"}\NormalTok{,}
        \StringTok{"BLACK"}\NormalTok{,}
        \StringTok{"HISPANIC"}\NormalTok{,}
        \StringTok{"Males\_1521"}\NormalTok{,}
        \StringTok{"HOUSEHOLDS"}\NormalTok{,}
        \StringTok{"FAMILYHOUS"}\NormalTok{,}
        \StringTok{"FEMALE\_HOU"}\NormalTok{,}
        \StringTok{"RENTER\_HOU"}\NormalTok{,}
        \StringTok{"VACANT\_HOU"}
\NormalTok{    )}
\NormalTok{match.out }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"i\_felony"}\NormalTok{, }\StringTok{"i\_misdemea"}\NormalTok{, }\StringTok{"i\_drugs"}\NormalTok{, }\StringTok{"any\_crime"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sea1 }\OtherTok{\textless{}{-}} \FunctionTok{microsynth}\NormalTok{(}
\NormalTok{    seattledmi,}
    \AttributeTok{idvar       =} \StringTok{"ID"}\NormalTok{,}
    \AttributeTok{timevar     =} \StringTok{"time"}\NormalTok{,}
    \AttributeTok{intvar      =} \StringTok{"Intervention"}\NormalTok{,}
    \AttributeTok{start.pre   =} \DecValTok{1}\NormalTok{,}
    \AttributeTok{end.pre     =} \DecValTok{12}\NormalTok{,}
    \AttributeTok{end.post    =} \DecValTok{16}\NormalTok{,}
    \AttributeTok{match.out   =}\NormalTok{ match.out, }\CommentTok{\# outcome variable will be matched on exactly}
    \AttributeTok{match.covar =}\NormalTok{ cov.var, }\CommentTok{\# specify covariates will be matched on exactly}
    \AttributeTok{result.var  =}\NormalTok{ match.out, }\CommentTok{\# used to report results}
    \AttributeTok{omnibus.var =}\NormalTok{ match.out, }\CommentTok{\# feature in the omnibus p{-}value}
    \AttributeTok{test        =} \StringTok{"lower"}\NormalTok{,}
    \AttributeTok{n.cores     =} \FunctionTok{min}\NormalTok{(parallel}\SpecialCharTok{::}\FunctionTok{detectCores}\NormalTok{(), }\DecValTok{2}\NormalTok{)}
\NormalTok{)}
\NormalTok{sea1}
\FunctionTok{summary}\NormalTok{(sea1)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot\_microsynth}\NormalTok{(sea1)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sea2 }\OtherTok{\textless{}{-}} \FunctionTok{microsynth}\NormalTok{(}
\NormalTok{    seattledmi,}
    \AttributeTok{idvar =} \StringTok{"ID"}\NormalTok{,}
    \AttributeTok{timevar =} \StringTok{"time"}\NormalTok{,}
    \AttributeTok{intvar =} \StringTok{"Intervention"}\NormalTok{,}
    \AttributeTok{start.pre =} \DecValTok{1}\NormalTok{,}
    \AttributeTok{end.pre =} \DecValTok{12}\NormalTok{,}
    \AttributeTok{end.post =} \FunctionTok{c}\NormalTok{(}\DecValTok{14}\NormalTok{, }\DecValTok{16}\NormalTok{),}
    \AttributeTok{match.out =}\NormalTok{ match.out,}
    \AttributeTok{match.covar =}\NormalTok{ cov.var,}
    \AttributeTok{result.var =}\NormalTok{ match.out,}
    \AttributeTok{omnibus.var =}\NormalTok{ match.out,}
    \AttributeTok{test =} \StringTok{"lower"}\NormalTok{,}
    \AttributeTok{perm =} \DecValTok{250}\NormalTok{,}
    \AttributeTok{jack =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{n.cores =} \FunctionTok{min}\NormalTok{(parallel}\SpecialCharTok{::}\FunctionTok{detectCores}\NormalTok{(), }\DecValTok{2}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{synthetic-difference-in-differences-1}{%
\section{Synthetic Difference-in-differences}\label{synthetic-difference-in-differences-1}}

reference: \citep{arkhangelsky2021synthetic}

\hypertarget{augmented-synthetic-control-method}{%
\section{Augmented Synthetic Control Method}\label{augmented-synthetic-control-method}}

package: \texttt{augsynth} * \citep{ben2021augmented}

\hypertarget{synthetic-controls-with-staggered-adoption}{%
\section{Synthetic Controls with Staggered Adoption}\label{synthetic-controls-with-staggered-adoption}}

references: * \url{https://ebenmichael.github.io/assets/research/jamboree.pdf} * \citep{ben2022synthetic} package: \texttt{augsynth}

\hypertarget{generalized-synthetic-control}{%
\section{Generalized Synthetic Control}\label{generalized-synthetic-control}}

reference: * \citep{xu2017generalized}

\hypertarget{event-studies}{%
\chapter{Event Studies}\label{event-studies}}

The earliest paper that used event study was \citep{dolley1933characteristics}

\citep{campbell1998econometrics} introduced this method, which based on the efficient markets theory by \citep{fama1970efficient}

Review:

\begin{itemize}
\item
  \citep{mcwilliams1997event}: in management
\item
  \citep{sorescu2017}: in marketing
\end{itemize}

Previous marketing studies:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Firm-initiated activities
\end{enumerate}

\begin{itemize}
\item
  \citep{horsky1987does}: name change
\item
  \citep{chaney1991impact} new product announcements
\item
  \citep{agrawal1995economic}: celebrity endorsement
\item
  \citep{lane1995stock}: brand extensions
\item
  \citep{houston2000buyer}: joint venture
\item
  \citep{geyskens2002market}: Internet channel (for newspapers)
\item
  \citep{cornwell2005relationship}: sponsorship announcements
\item
  \citep{elberse2007power}: casting announcements
\item
  \citep{sorescu2007some}: M\&A
\item
  \citep{sood2009innovations}: innovation payoff
\item
  \citep{wiles2009worth}: product placements in movies
\item
  \citep{joshi2009movie}: movie releases
\item
  \citep{wiles2010stock}: Regulatory Reports of Deceptive Advertising
\item
  \citep{boyd2010chief}: new CMO appointments
\item
  \citep{karniouchina2011marketing}: product placement
\item
  \citep{wiles2012effect}: Brand Acquisition and Disposal
\item
  \citep{kalaignanam2013corporate}: corporate brand name change
\item
  \citep{raassens2012market}: new product development outsourcing
\item
  \citep{mazodier2013sponsorship}: sports announcements
\item
  \citep{borah2014make}: make, buy or ally for innovations
\item
  \citep{homburg2014firm}: channel expansions
\item
  \citep{fang2015timing}: Co-development agreements
\item
  \citep{wu2015sleeping}: horizontal collaboration in new product development
\item
  \citep{fama1969adjustment}: stock split
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Non-firm-initiated activities
\end{enumerate}

\begin{itemize}
\item
  \citep{sorescu2003}: FDA approvals
\item
  \citep{pandey2005relationship}: diversity elite list
\item
  \citep{balasubramanian2005impact}: high-quality achievements
\item
  \citep{tellis2007}: quality reviews by Walter Mossberg
\item
  \citep{fornell2006customer}: customer satisfaction
\item
  \citep{gielens2008dancing}: Walmart's entry into the UK market
\item
  \citep{boyd2008market}: indirect ties
\item
  \citep{rao2008fruits}: FDA approvals
\item
  \citep{ittner2009commentary}: customer satisfaction
\item
  \citep{tipton2009regulatory}: Deceptive advertising
\item
  \citep{chen2009does}: product recalls
\item
  \citep{jacobson2009financial}: satisfaction score release
\item
  \citep{karniouchina2009impact}: Mad money with Jim Cramer
\item
  \citep{wiles2010stock}: deceptive advertising
\item
  \citep{chen2012third}: third-party movie reviews
\item
  \citep{xiong2013asymmetric}: positive and negative news
\item
  \citep{gao2015should}: product recall
\item
  \citep{malhotra2011evaluating}: data breach
\item
  \citep{bhagat1998shareholder}: litigation
\end{itemize}

Potential avenues:

\begin{itemize}
\item
  Ad campaigns
\item
  Market entry
\item
  product failure/recalls
\item
  Patents
\end{itemize}

Pros:

\begin{itemize}
\item
  Better than accounting based measures (e.g., profits) because managers can manipulate profits \citep{benston1985validity}
\item
  Easy to do
\end{itemize}

Fun fact:

\begin{itemize}
\tightlist
\item
  \citep{dubow2006measuring} came up with a way to gauge how `clean' a market is. They based their measure on how much prices seemed to move in a way that suggested insider knowledge, before the release of important regulatory announcements that could affect the stock prices. Such price shifts might suggest that insider trading was occurring. Essentially, they were watching for any unusual price changes before the day of the announcement.
\end{itemize}

Events can be

\begin{itemize}
\item
  Internal (e.g., stock repurchase)
\item
  External (e.g., macroeconomic variables)
\end{itemize}

\textbf{Assumptions}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Efficient market theory
\item
  Shareholders are the most important group among stakeholders
\item
  The event sharply affects share price
\item
  Expected return is calculated appropriately
\end{enumerate}

\textbf{Steps}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Event Identification: (e.g., dividends, M\&A, stock buyback, laws or regulation, privatization vs.~nationalization, celebrity endorsements, name changes, or brand extensions etc. To see the list of events in US and international, see WRDS \textbf{S\&P Capital IQ Key Developments}). Events must affect either cash flows or on the discount rate of firms \citep[p.~191]{sorescu2017}

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    Estimation window: Normal return expected return (\(T_0 \to T_1\)) (sometimes include days before to capture leakages).

    \begin{itemize}
    \item
      Recommendation by \citep{johnston2007review} is to use 250 days before the event (and 45-day between the estimation window and the event window).

      \begin{itemize}
      \item
        \citep{wiles2012effect} used an 90-trading-day estimation window ending 6 days before the event (this is consistent with the finance literature).
      \item
        \citep{gielens2008dancing} 260 to 10 days before or 300 to 46 days before
      \item
        \citep{tirunillai2012does} estimation window of 255 days and ends 46 days before the event.
      \end{itemize}
    \item
      Similarly, \citep{mcwilliams1997event} and \citep{fornell2006customer} 255 days ending 46 days before the event date
    \item
      \citep[p.~194]{sorescu2017} suggest 100 days before the event date
    \item
      Leakage: try to cover as broad news sources as possible (LexisNexis, Factiva, and RavenPack).
    \end{itemize}
  \item
    Event window: contain the event date (\(T_1 \to T_2\)) (have to argue for the event window and can't do it empirically)

    \begin{itemize}
    \item
      One day: \citep{balasubramanian2005impact, boyd2010chief, fornell2006customer}
    \item
      Two days: \citep{raassens2012market, sood2009innovations}
    \item
      Up to 10 days: \citep{cornwell2005relationship, kalaignanam2013corporate, sorescu2007some}
    \end{itemize}
  \item
    Post Event window: \(T_2 \to T_3\)
  \end{enumerate}
\item
  Normal vs.~Abnormal returns
\end{enumerate}

\[
\epsilon_{it}^* = \frac{P_{it} - E(P_{it})}{P_{it-1}} = R_{it} - E(R_{it}|X_t)
\]

where

\begin{itemize}
\item
  \(\epsilon_{it}^*\) = abnormal return
\item
  \(R_{it}\) = realized (actual) return
\item
  \(P\) = dividend-adjusted price of the stock
\item
  \(E(R_{it}|X_t)\) normal expected return
\end{itemize}

There are several model to calculate the expected return

A. \protect\hyperlink{statistical-models}{Statistical Models}: assumes jointly multivariate normal and iid over time (need distributional assumptions for valid finite-sample estimation) rather robust (hence, recommended)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \protect\hyperlink{constant-mean-return-model}{Constant Mean Return Model}
\item
  \protect\hyperlink{market-model}{Market Model}
\item
  Adjusted Market Return Model
\item
  Factor Model
\end{enumerate}

B. \protect\hyperlink{economic-model}{Economic Model} (strong assumption regarding investor behavior)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \protect\hyperlink{capital-asset-pricing-model-capm}{Capital Asset Pricing Model (CAPM)}
\item
  \protect\hyperlink{arbitrage-pricing-theory-apt}{Arbitrage Pricing Theory (APT)}
\end{enumerate}

\hypertarget{other-issues}{%
\section{Other Issues}\label{other-issues}}

\hypertarget{event-studies-in-marketing}{%
\subsection{Event Studies in marketing}\label{event-studies-in-marketing}}

\citep{skiera2017should} What should be the dependent variable in marketing-related event studies?

\begin{itemize}
\item
  Based on valuation theory, Shareholder value = the value of the operating business + non-operating asset - debt \citep{schulze2012linking}

  \begin{itemize}
  \tightlist
  \item
    Many marketing events only affect the operating business value, but not non-operating assets and debt
  \end{itemize}
\item
  Ignoring the differences in firm-specific leverage effects has dual effects:

  \begin{itemize}
  \item
    inflates the impact of observation pertaining to firms with large debt
  \item
    deflates those pertaining to firms with large non-operating asset.
  \end{itemize}
\item
  It's recommended that marketing papers should report both \(CAR^{OB}\) and \(CAR^{SHV}\) and argue for whichever one more appropriate.
\item
  Up until this paper, only two previous event studies control for financial structure: \citep{gielens2008dancing} \citep{chaney1991impact}
\end{itemize}

Definitions:

\begin{itemize}
\item
  Cumulative abnormal percentage return on shareholder value (\(CAR^{SHV}\))

  \begin{itemize}
  \tightlist
  \item
    Shareholder value refers to a firm's market capitalization = share price x \# of shares.
  \end{itemize}
\item
  Cumulative abnormal percentage return on the value of the operating business (\(CAR^{OB}\))

  \begin{itemize}
  \item
    \(CAR^{OB} = CAR^{SHV}/\text{leverage effect}_{before}\)
  \item
    Leverage effect = Operating business value / Shareholder value (LE describes how a 1\% change in operating business translates into a percentage change in shareholder value).
  \item
    Value of operating business = shareholder value - non-operating assets + debt
  \item
    Leverage effect \(\neq\) leverage ratio, where leverage ratio is debt / firm size

    \begin{itemize}
    \item
      debt = long-term + short-term debt; long-term debt
    \item
      firm size = book value of equity; market cap; total assets; debt + equity
    \end{itemize}
  \end{itemize}
\item
  Operating assets are those used by firm in their core business operations (e..g, property, plant, equipment, natural resources, intangible asset)
\item
  Non--operating assets (redundant assets), do not play a role in a firm's operations, but still generate some form of return (e.g., excess cash , marketable securities - commercial papers, market instruments)
\end{itemize}

Marketing events usually influence the value of a firm's operating assets (more specifically intangible assets). Then, changes in the value of the operating business can impact shareholder value.

\begin{itemize}
\item
  Three rare instances where marketing events can affect non-operating assets and debt

  \begin{itemize}
  \item
    \citep{hall2004determinants}: excess pre-orderings can influence short-term debt
  \item
    \citep{berger1997managerial} Firing CMO increase debt as the manager's tenure is negatively associated with the firm's debt
  \item
    \citep{bhaduri2002determinants} production of unique products.
  \end{itemize}
\end{itemize}

A marketing-related event can either influence

\begin{itemize}
\item
  value components of a firm's value (= firm's operating business, non-operating assets and its debt)
\item
  only the operating business.
\end{itemize}

Replication of the leverage effect

\[
\begin{aligned}
\text{leverage effect} &= \frac{\text{operating business}}{\text{shareholder value}} \\
&= \frac{\text{(shareholder value - non-operating assets + debt)}}{\text{shareholder value}} \\
&= \frac{prcc_f \times csho - ivst + dd1 + dltt + pstk}{prcc_f \times csho}
\end{aligned}
\]

Compustat Data Item

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.1944}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.4583}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Label
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Variable
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{prcc\_f} & Share price \\
\texttt{csho} & Common shares outstanding \\
\texttt{ivst} & short-term investments

(Non-operating assets) \\
\texttt{dd1} & long-term debt due in one year \\
\texttt{dltt} & long-term debt \\
\texttt{pstk} & preferred stock \\
\end{longtable}

Since WRDS no longer maintains the S\&P 500 list as of the time of this writing, I can't replicate the list used in \citep{skiera2017should} paper.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{df\_leverage\_effect }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"data/leverage\_effect.csv.gz"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
    
    \CommentTok{\# get active firms only}
    \FunctionTok{filter}\NormalTok{(costat }\SpecialCharTok{==} \StringTok{"A"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
    
    \CommentTok{\# drop missing values}
    \FunctionTok{drop\_na}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
    
    \CommentTok{\# create the leverage effect variable}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{le =}\NormalTok{ (prcc\_f }\SpecialCharTok{*}\NormalTok{ csho }\SpecialCharTok{{-}}\NormalTok{ ivst }\SpecialCharTok{+}\NormalTok{ dd1 }\SpecialCharTok{+}\NormalTok{ dltt }\SpecialCharTok{+}\NormalTok{ pstk)}\SpecialCharTok{/}\NormalTok{ (prcc\_f }\SpecialCharTok{*}\NormalTok{ csho)) }\SpecialCharTok{\%\textgreater{}\%} 
    
    \CommentTok{\# get shareholder value}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{shv =}\NormalTok{ prcc\_f }\SpecialCharTok{*}\NormalTok{ csho) }\SpecialCharTok{\%\textgreater{}\%} 
    
    \CommentTok{\# remove Infinity value for leverage effect (i.e., shareholder value = 0)}
    \FunctionTok{filter\_all}\NormalTok{(}\FunctionTok{all\_vars}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.infinite}\NormalTok{(.))) }\SpecialCharTok{\%\textgreater{}\%} 
    
    \CommentTok{\# positive values only }
    \FunctionTok{filter\_all}\NormalTok{(}\FunctionTok{all\_vars}\NormalTok{(. }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
    
    \CommentTok{\# get the within coefficient of variation}
    \FunctionTok{group\_by}\NormalTok{(gvkey) }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{within\_var\_mean\_le =} \FunctionTok{mean}\NormalTok{(le),}
           \AttributeTok{within\_var\_sd\_le =} \FunctionTok{sd}\NormalTok{(le)) }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{ungroup}\NormalTok{()}


\CommentTok{\# get the mean and standard deviation}
\FunctionTok{mean}\NormalTok{(df\_leverage\_effect}\SpecialCharTok{$}\NormalTok{le)}
\CommentTok{\#\textgreater{} [1] 150.1087}
\FunctionTok{max}\NormalTok{(df\_leverage\_effect}\SpecialCharTok{$}\NormalTok{le)}
\CommentTok{\#\textgreater{} [1] 183629.6}
\FunctionTok{hist}\NormalTok{(df\_leverage\_effect}\SpecialCharTok{$}\NormalTok{le)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{28-event-study_files/figure-latex/unnamed-chunk-1-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# coefficient of variation }
\FunctionTok{sd}\NormalTok{(df\_leverage\_effect}\SpecialCharTok{$}\NormalTok{le) }\SpecialCharTok{/} \FunctionTok{mean}\NormalTok{(df\_leverage\_effect}\SpecialCharTok{$}\NormalTok{le) }\SpecialCharTok{*} \DecValTok{100}
\CommentTok{\#\textgreater{} [1] 2749.084}

\CommentTok{\# Within{-}firm variation (similar to fig 3a)}
\NormalTok{df\_leverage\_effect }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{group\_by}\NormalTok{(gvkey) }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(within\_var\_mean\_le, within\_var\_sd\_le) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{cv =}\NormalTok{ within\_var\_sd\_le}\SpecialCharTok{/}\NormalTok{ within\_var\_mean\_le) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(cv) }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{pull}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{hist}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{28-event-study_files/figure-latex/unnamed-chunk-1-2} \end{center}

\hypertarget{economic-significance}{%
\subsection{Economic significance}\label{economic-significance}}

Total wealth gain (loss) from the event

\[
\Delta W_t = CAR_t \times MKTVAL_0
\]

where

\begin{itemize}
\item
  \(\Delta W_t\) = gain (loss)
\item
  \(CAR_t\) = cumulative residuals to date \(t\)
\item
  \(MKTVAL_0\) market value of the firm before the event window
\end{itemize}

\hypertarget{statistical-power}{%
\subsection{Statistical Power}\label{statistical-power}}

increases with

\begin{itemize}
\item
  more firms
\item
  less days in the event window (avoiding potential contamination from confounds)
\end{itemize}

\hypertarget{testing}{%
\section{Testing}\label{testing}}

\hypertarget{parametric-test}{%
\subsection{Parametric Test}\label{parametric-test}}

\citep{brown1985using} provide evidence that even in the presence of non-normality, the parametric tests still perform well. Since the proportion of positive and negative abnormal returns tends to be equal in the sample (of at least 5 securities). The excess returns will coverage to normality as the sample size increases. Hence, parametric test is advocated than non-parametric one.

Low power to detect significance \citep{kothari1997measuring}

\begin{itemize}
\tightlist
\item
  Power = f(sample, size, the actual size of abnormal returns, the variance of abnormal returns across firms)
\end{itemize}

\hypertarget{t-test}{%
\subsubsection{T-test}\label{t-test}}

Applying CLT

\[
\begin{aligned}
t_{CAR} &= \frac{\bar{CAR_{it}}}{\sigma (CAR_{it})/\sqrt{n}} \\
t_{BHAR} &= \frac{\bar{BHAR_{it}}}{\sigma (BHAR_{it})/\sqrt{n}}
\end{aligned}
\]

Assume

\begin{itemize}
\item
  Abnormal returns are normally distributed
\item
  Var(abnormal returns) are equal across firms
\item
  No cross-correlation in abnormal returns.
\end{itemize}

Hence, it will be misspecified if you suspected

\begin{itemize}
\item
  Heteroskedasticity
\item
  Cross-sectional dependence
\item
  Technically, abnormal returns could follow non-normal distribution (but because of the design of abnormal returns calculation, it typically forces the distribution to be normal)
\end{itemize}

To address these concerns, \protect\hyperlink{patell-standardized-residual-psr}{Patell Standardized Residual (PSR)} can sometimes help.

\hypertarget{patell-standardized-residual-psr}{%
\subsubsection{Patell Standardized Residual (PSR)}\label{patell-standardized-residual-psr}}

\citep{patell1976corporate}

\begin{itemize}
\tightlist
\item
  Since market model uses observations outside the event window, abnormal returns contain prediction errors on top of the true residuals , and should be standardized:
\end{itemize}

\[
AR_{it} = \frac{\hat{u}_{it}}{s_i \sqrt{C_{it}}}
\]

where

\begin{itemize}
\item
  \(\hat{u}_{it}\) = estimated residual
\item
  \(s_i\) = standard deviation estimate of residuals (from the estimation period)
\item
  \(C_{it}\) = a correction to account for the prediction's increased variation outside of the estimation period \citep{strong1992}
\end{itemize}

\[
C_{it} = 1 + \frac{1}{T} + \frac{(R_{mt} - \bar{R}_m)^2}{\sum_t (R_{mt} - \bar{R}_m)^2}
\]

where

\begin{itemize}
\item
  \(T\) = number of observations (from estimation period)
\item
  \(R_{mt}\) = average rate of return of all stocks trading the the stock market at time \(t\)
\item
  \(\bar{R}_m = \frac{1}{T} \sum_{t=1}^T R_{mt}\)
\end{itemize}

\hypertarget{non-parametric-test}{%
\subsection{Non-parametric Test}\label{non-parametric-test}}

\begin{itemize}
\item
  No assumptions about return distribution
\item
  Sign Test (assumes symmetry in returns)

  \begin{itemize}
  \tightlist
  \item
    \texttt{binom.test()}
  \end{itemize}
\item
  Wilcoxon Signed-Rank Test (allows for non-symmetry in returns)

  \begin{itemize}
  \tightlist
  \item
    Use \texttt{wilcox.test(sample)}
  \end{itemize}
\item
  Gen Sign Test
\item
  Corrado Rank Test
\end{itemize}

\hypertarget{sample}{%
\section{Sample}\label{sample}}

\begin{itemize}
\item
  Sample can be relative small

  \begin{itemize}
  \item
    \citep{wiles2012effect} 572 acquisition announcements, 308 disposal announcements
  \item
    Can range from 71 \citep{markovitch2008findings} to 3552 \citep{borah2014make}
  \end{itemize}
\end{itemize}

\hypertarget{confounders}{%
\subsection{Confounders}\label{confounders}}

\begin{itemize}
\tightlist
\item
  Avoid confounding events: earnings announcements, key executive changes, unexpected stock buybacks, changes in dividends within the two-trading day window surrounding the event, mergers and acquisitions, spin-offers, stock splits, management changes, joint ventures, unexpected dividend, IPO, debt defaults, dividend cancellations \citep{mcwilliams1997event}
\end{itemize}

According to \citep{fornell2006customer}, need to control:

\begin{itemize}
\item
  one-day event period = day when Wall Street Journal publish ACSI announcement.
\item
  5 days before and after event to rule out other news (PR Newswires, Dow Jones, Business Wires)

  \begin{itemize}
  \item
    M\&A, Spin-offs, stock splits
  \item
    CEO or CFO changes,
  \item
    Layoffs, restructurings, earnings announcements, lawsuits
  \item
    Capital IQ - Key Developments: covers almost all important events so you don't have to search on news.
  \end{itemize}
\end{itemize}

\citep{sorescu2017} examine confounding events in the short-term windows:

\begin{itemize}
\item
  From RavenPack, 3982 US publicly traded firms, with all the press releases (2000-2013)
\item
  3-day window around event dates
\item
  The difference between a sample with full observations and a sample without confounded events is negligible (non-significant).
\item
  Conclusion: \textbf{excluding confounded observations may be unnecessary for short-term event studies.}

  \begin{itemize}
  \item
    Biases can stem from researchers pick and choose events to exclude
  \item
    As time progresses, more and more events you need to exclude which can be infeasible.
  \end{itemize}
\end{itemize}

To further illustrate this point, let's do a quick simulation exercise

In this example, we will explore three types of events:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Focal events
\item
  Correlated events (i.e., events correlated with the focal events; the presence of correlated events can follow the presence of the focal event)
\item
  Uncorrelated events (i.e., events with dates that might randomly coincide with the focal events, but are not correlated with them).
\end{enumerate}

We have the ability to control the strength of correlation between focal and correlated events in this study, as well as the number of unrelated events we wish to examine.

Let's examine the implications of including and excluding correlated and uncorrelated events on the estimates of our focal events.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load required libraries}
\FunctionTok{library}\NormalTok{(dplyr)}
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(tidyr)}
\FunctionTok{library}\NormalTok{(tidyverse)}

\CommentTok{\# Parameters}
\NormalTok{n                  }\OtherTok{\textless{}{-}} \DecValTok{100000}         \CommentTok{\# Number of observations}
\NormalTok{n\_focal            }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(n }\SpecialCharTok{*} \FloatTok{0.2}\NormalTok{) }\CommentTok{\# Number of focal events}
\NormalTok{overlap\_correlated }\OtherTok{\textless{}{-}} \FloatTok{0.5}            \CommentTok{\# Overlapping percentage between focal and correlated events}

\CommentTok{\# Function to compute mean and confidence interval}
\NormalTok{mean\_ci }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{}
\NormalTok{    m }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(x)}
\NormalTok{    ci }\OtherTok{\textless{}{-}} \FunctionTok{qt}\NormalTok{(}\FloatTok{0.975}\NormalTok{, }\FunctionTok{length}\NormalTok{(x)}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{) }\SpecialCharTok{*} \FunctionTok{sd}\NormalTok{(x) }\SpecialCharTok{/} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{length}\NormalTok{(x)) }\CommentTok{\# 95\% confidence interval}
    \FunctionTok{list}\NormalTok{(}\AttributeTok{mean =}\NormalTok{ m, }\AttributeTok{lower =}\NormalTok{ m }\SpecialCharTok{{-}}\NormalTok{ ci, }\AttributeTok{upper =}\NormalTok{ m }\SpecialCharTok{+}\NormalTok{ ci)}
\NormalTok{\}}

\CommentTok{\# Simulate data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{date       =} \FunctionTok{seq.Date}\NormalTok{(}\AttributeTok{from =} \FunctionTok{as.Date}\NormalTok{(}\StringTok{"2010{-}01{-}01"}\NormalTok{), }\AttributeTok{by =} \StringTok{"day"}\NormalTok{, }\AttributeTok{length.out =}\NormalTok{ n), }\CommentTok{\# Date sequence}
    \AttributeTok{focal      =} \FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, n),}
    \AttributeTok{correlated =} \FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, n),}
    \AttributeTok{ab\_ret     =} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{)}


\CommentTok{\# Define focal events}
\NormalTok{focal\_idx }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n, n\_focal)}
\NormalTok{data}\SpecialCharTok{$}\NormalTok{focal[focal\_idx] }\OtherTok{\textless{}{-}} \DecValTok{1}

\NormalTok{true\_effect }\OtherTok{\textless{}{-}} \FloatTok{0.25}

\CommentTok{\# Adjust the ab\_ret for the focal events to have a mean of true\_effect}
\NormalTok{data}\SpecialCharTok{$}\NormalTok{ab\_ret[focal\_idx] }\OtherTok{\textless{}{-}}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{ab\_ret[focal\_idx] }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{ab\_ret[focal\_idx]) }\SpecialCharTok{+}\NormalTok{ true\_effect}



\CommentTok{\# Determine the number of correlated events that overlap with focal and those that don\textquotesingle{}t}
\NormalTok{n\_correlated\_overlap }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(}\FunctionTok{length}\NormalTok{(focal\_idx) }\SpecialCharTok{*}\NormalTok{ overlap\_correlated)}
\NormalTok{n\_correlated\_non\_overlap }\OtherTok{\textless{}{-}}\NormalTok{ n\_correlated\_overlap}

\CommentTok{\# Sample the overlapping correlated events from the focal indices}
\NormalTok{correlated\_idx }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(focal\_idx, }\AttributeTok{size =}\NormalTok{ n\_correlated\_overlap)}

\CommentTok{\# Get the remaining indices that are not part of focal}
\NormalTok{remaining\_idx }\OtherTok{\textless{}{-}} \FunctionTok{setdiff}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n, focal\_idx)}

\CommentTok{\# Check to ensure that we\textquotesingle{}re not attempting to sample more than the available remaining indices}
\ControlFlowTok{if}\NormalTok{ (}\FunctionTok{length}\NormalTok{(remaining\_idx) }\SpecialCharTok{\textless{}}\NormalTok{ n\_correlated\_non\_overlap) \{}
    \FunctionTok{stop}\NormalTok{(}\StringTok{"Not enough remaining indices for non{-}overlapping correlated events"}\NormalTok{)}
\NormalTok{\}}

\CommentTok{\# Sample the non{-}overlapping correlated events from the remaining indices}
\NormalTok{correlated\_non\_focal\_idx }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(remaining\_idx, }\AttributeTok{size =}\NormalTok{ n\_correlated\_non\_overlap)}

\CommentTok{\# Combine the two to get all correlated indices}
\NormalTok{all\_correlated\_idx }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(correlated\_idx, correlated\_non\_focal\_idx)}

\CommentTok{\# Set the correlated events in the data}
\NormalTok{data}\SpecialCharTok{$}\NormalTok{correlated[all\_correlated\_idx] }\OtherTok{\textless{}{-}} \DecValTok{1}


\CommentTok{\# Inflate the effect for correlated events to have a mean of }
\NormalTok{correlated\_non\_focal\_idx }\OtherTok{\textless{}{-}} \FunctionTok{setdiff}\NormalTok{(all\_correlated\_idx, focal\_idx) }\CommentTok{\# Fixing the selection of non{-}focal correlated events}
\NormalTok{data}\SpecialCharTok{$}\NormalTok{ab\_ret[correlated\_non\_focal\_idx] }\OtherTok{\textless{}{-}}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{ab\_ret[correlated\_non\_focal\_idx] }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{ab\_ret[correlated\_non\_focal\_idx]) }\SpecialCharTok{+} \DecValTok{1}


\CommentTok{\# Define the numbers of uncorrelated events for each scenario}
\NormalTok{num\_uncorrelated }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{40}\NormalTok{)}

\CommentTok{\# Define uncorrelated events}
\ControlFlowTok{for}\NormalTok{ (num }\ControlFlowTok{in}\NormalTok{ num\_uncorrelated) \{}
    \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{num) \{}
\NormalTok{        data[}\FunctionTok{paste0}\NormalTok{(}\StringTok{"uncorrelated\_"}\NormalTok{, i)] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{        uncorrelated\_idx }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n, }\FunctionTok{round}\NormalTok{(n }\SpecialCharTok{*} \FloatTok{0.1}\NormalTok{))}
\NormalTok{        data[uncorrelated\_idx, }\FunctionTok{paste0}\NormalTok{(}\StringTok{"uncorrelated\_"}\NormalTok{, i)] }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{    \}}
\NormalTok{\}}


\CommentTok{\# Define uncorrelated columns and scenarios}
\NormalTok{unc\_cols }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\StringTok{"uncorrelated\_"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\NormalTok{num\_uncorrelated)}
\NormalTok{results }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{Scenario =} \FunctionTok{c}\NormalTok{(}\StringTok{"Include Correlated"}\NormalTok{, }\StringTok{"Correlated Effects"}\NormalTok{, }\StringTok{"Exclude Correlated"}\NormalTok{, }\StringTok{"Exclude Correlated and All Uncorrelated"}\NormalTok{),}
    \AttributeTok{MeanEffect =} \FunctionTok{c}\NormalTok{(}
        \FunctionTok{mean\_ci}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{ab\_ret[data}\SpecialCharTok{$}\NormalTok{focal }\SpecialCharTok{==} \DecValTok{1}\NormalTok{])}\SpecialCharTok{$}\NormalTok{mean,}
        \FunctionTok{mean\_ci}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{ab\_ret[data}\SpecialCharTok{$}\NormalTok{focal }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{|}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{correlated }\SpecialCharTok{==} \DecValTok{1}\NormalTok{])}\SpecialCharTok{$}\NormalTok{mean,}
        \FunctionTok{mean\_ci}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{ab\_ret[data}\SpecialCharTok{$}\NormalTok{focal }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{correlated }\SpecialCharTok{==} \DecValTok{0}\NormalTok{])}\SpecialCharTok{$}\NormalTok{mean,}
        \FunctionTok{mean\_ci}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{ab\_ret[data}\SpecialCharTok{$}\NormalTok{focal }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{correlated }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\&} \FunctionTok{rowSums}\NormalTok{(data[, }\FunctionTok{paste0}\NormalTok{(}\StringTok{"uncorrelated\_"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\NormalTok{num\_uncorrelated)]) }\SpecialCharTok{==} \DecValTok{0}\NormalTok{])}\SpecialCharTok{$}\NormalTok{mean}
\NormalTok{    ),}
    \AttributeTok{LowerCI =} \FunctionTok{c}\NormalTok{(}
        \FunctionTok{mean\_ci}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{ab\_ret[data}\SpecialCharTok{$}\NormalTok{focal }\SpecialCharTok{==} \DecValTok{1}\NormalTok{])}\SpecialCharTok{$}\NormalTok{lower,}
        \FunctionTok{mean\_ci}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{ab\_ret[data}\SpecialCharTok{$}\NormalTok{focal }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{|}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{correlated }\SpecialCharTok{==} \DecValTok{1}\NormalTok{])}\SpecialCharTok{$}\NormalTok{lower,}
        \FunctionTok{mean\_ci}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{ab\_ret[data}\SpecialCharTok{$}\NormalTok{focal }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{correlated }\SpecialCharTok{==} \DecValTok{0}\NormalTok{])}\SpecialCharTok{$}\NormalTok{lower,}
        \FunctionTok{mean\_ci}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{ab\_ret[data}\SpecialCharTok{$}\NormalTok{focal }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{correlated }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\&} \FunctionTok{rowSums}\NormalTok{(data[, }\FunctionTok{paste0}\NormalTok{(}\StringTok{"uncorrelated\_"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\NormalTok{num\_uncorrelated)]) }\SpecialCharTok{==} \DecValTok{0}\NormalTok{])}\SpecialCharTok{$}\NormalTok{lower}
\NormalTok{    ),}
    \AttributeTok{UpperCI =} \FunctionTok{c}\NormalTok{(}
        \FunctionTok{mean\_ci}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{ab\_ret[data}\SpecialCharTok{$}\NormalTok{focal }\SpecialCharTok{==} \DecValTok{1}\NormalTok{])}\SpecialCharTok{$}\NormalTok{upper,}
        \FunctionTok{mean\_ci}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{ab\_ret[data}\SpecialCharTok{$}\NormalTok{focal }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{|}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{correlated }\SpecialCharTok{==} \DecValTok{1}\NormalTok{])}\SpecialCharTok{$}\NormalTok{upper,}
        \FunctionTok{mean\_ci}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{ab\_ret[data}\SpecialCharTok{$}\NormalTok{focal }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{correlated }\SpecialCharTok{==} \DecValTok{0}\NormalTok{])}\SpecialCharTok{$}\NormalTok{upper,}
        \FunctionTok{mean\_ci}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{ab\_ret[data}\SpecialCharTok{$}\NormalTok{focal }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{correlated }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\&} \FunctionTok{rowSums}\NormalTok{(data[, }\FunctionTok{paste0}\NormalTok{(}\StringTok{"uncorrelated\_"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\NormalTok{num\_uncorrelated)]) }\SpecialCharTok{==} \DecValTok{0}\NormalTok{])}\SpecialCharTok{$}\NormalTok{upper}
\NormalTok{    )}
\NormalTok{)}

\CommentTok{\# Add the scenarios for excluding 5, 10, 20, and 50 uncorrelated}
\ControlFlowTok{for}\NormalTok{ (num }\ControlFlowTok{in}\NormalTok{ num\_uncorrelated) \{}
\NormalTok{    unc\_cols }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\StringTok{"uncorrelated\_"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\NormalTok{num)}
\NormalTok{    results }\OtherTok{\textless{}{-}}\NormalTok{ results }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{add\_row}\NormalTok{(}
            \AttributeTok{Scenario =} \FunctionTok{paste}\NormalTok{(}\StringTok{"Exclude"}\NormalTok{, num, }\StringTok{"Uncorrelated"}\NormalTok{),}
            \AttributeTok{MeanEffect =} \FunctionTok{mean\_ci}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{ab\_ret[data}\SpecialCharTok{$}\NormalTok{focal }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{correlated }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\&} \FunctionTok{rowSums}\NormalTok{(data[, unc\_cols]) }\SpecialCharTok{==} \DecValTok{0}\NormalTok{])}\SpecialCharTok{$}\NormalTok{mean,}
            \AttributeTok{LowerCI =} \FunctionTok{mean\_ci}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{ab\_ret[data}\SpecialCharTok{$}\NormalTok{focal }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{correlated }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\&} \FunctionTok{rowSums}\NormalTok{(data[, unc\_cols]) }\SpecialCharTok{==} \DecValTok{0}\NormalTok{])}\SpecialCharTok{$}\NormalTok{lower,}
            \AttributeTok{UpperCI =} \FunctionTok{mean\_ci}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{ab\_ret[data}\SpecialCharTok{$}\NormalTok{focal }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{correlated }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\&} \FunctionTok{rowSums}\NormalTok{(data[, unc\_cols]) }\SpecialCharTok{==} \DecValTok{0}\NormalTok{])}\SpecialCharTok{$}\NormalTok{upper}
\NormalTok{        )}
\NormalTok{\}}


\FunctionTok{ggplot}\NormalTok{(results,}
       \FunctionTok{aes}\NormalTok{(}
           \AttributeTok{x =} \FunctionTok{factor}\NormalTok{(Scenario, }\AttributeTok{levels =}\NormalTok{ Scenario),}
           \AttributeTok{y =}\NormalTok{ MeanEffect,}
           \AttributeTok{ymin =}\NormalTok{ LowerCI,}
           \AttributeTok{ymax =}\NormalTok{ UpperCI}
\NormalTok{       )) }\SpecialCharTok{+}
    \FunctionTok{geom\_pointrange}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{coord\_flip}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{ylab}\NormalTok{(}\StringTok{"Mean Effect"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{xlab}\NormalTok{(}\StringTok{"Scenario"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Mean Effect of Focal Events under Different Scenarios"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =}\NormalTok{ true\_effect,}
               \AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{,}
               \AttributeTok{color =} \StringTok{"red"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{28-event-study_files/figure-latex/unnamed-chunk-2-1} \end{center}

As depicted in the plot, the inclusion of correlated events demonstrates minimal impact on the estimation of our focal events. Conversely, excluding these correlated events can diminish our statistical power. This is true in cases of pronounced correlation.

However, the consequences of excluding unrelated events are notably more significant. It becomes evident that by omitting around 40 unrelated events from our study, we lose the ability to accurately identify the true effects of the focal events. In reality and within research, we often rely on the Key Developments database, excluding over 150 events, a practice that can substantially impair our capacity to ascertain the authentic impact of the focal events.

This little experiment really drives home the point -- you better have a darn good reason to exclude an event from your study (make it super convincing)!

\hypertarget{biases}{%
\section{Biases}\label{biases}}

\begin{itemize}
\item
  Different closing time obscure estimation of the abnormal returns, check \citep{campbell1998econometrics}
\item
  Upward bias in aggregating CAR + transaction prices (bid and ask)
\item
  Cross-sectional dependence in the returns bias the standard deviation estimates downward, which inflates the test statistics when events share common dates \citep{mackinlay1997event}. Hence, \citep{jaffe1974special} \protect\hyperlink{calendar-time-portfolio-abnormal-returns-ctars}{Calendar-time Portfolio Abnormal Returns (CTARs)} should be used to correct for this bias.
\item
  \citep{wiles2012effect}: For events confined to relatively few industries, cross-sectional dependence in the returns can bias the SD estimate downward, inflating the associated test statistics'' (p.~47). To control for potential cross-sectional correlation in the abnormal returns, you can use time-series standard deviation test statistic \citep{brown1980measuring}
\item
  Sample selection bias (self-selection of firms into the event treatment) similar to omitted variable bias where the omitted variable is the private info that leads a firm to take the action.

  \begin{itemize}
  \item
    See \protect\hyperlink{endogenous-sample-selection}{Endogenous Sample Selection} for more methods to correct this bias.
  \item
    Use Heckman model \citep{acharya1993value}

    \begin{itemize}
    \item
      But hard to find an instrument that meets the exclusion requirements (and strong, because weak instruments can lead to multicollinearity in the second equation)
    \item
      Can estimate the private information unknown to investors (which is Mills ratio \(\lambda\) itself). Testing \(\lambda\) significance is to see whether private info can explain outcomes (e.g., magnitude of the CARs to the announcement).
    \item
      Examples: \citep{chen2009does} \citep{wiles2012effect} \citep{fang2015timing}
    \end{itemize}
  \item
    Counterfactual observations

    \begin{itemize}
    \item
      Propensity score matching:

      \begin{itemize}
      \item
        Finance: \citep[\citet{doan2021does}]{iskandar2013valuation} \citep{masulis2011venture}
      \item
        Marketing: \citep{warren2017how} \citep{borah2014make} \citep{cao2013wedded}
      \end{itemize}
    \item
      Switching regression: comparison between 2 specific outcomes (also account for selection on unobservables - using instruments) \citep{cao2013wedded}
    \end{itemize}
  \end{itemize}
\end{itemize}

\hypertarget{long-run-event-studies}{%
\section{Long-run event studies}\label{long-run-event-studies}}

\begin{itemize}
\item
  Usually make an assumption that the distribution of the abnormal returns to these events has a mean of 0 \citep[p.~192]{sorescu2017}. And \citep{sorescu2017} provide evidence that for all events they examine the results from samples with and without confounding events do not differ.
\item
  Long-horizon event studies face challenges due to systematic errors over time and sensitivity to model choice.
\item
  Two main approaches are used to measure long-term abnormal stock returns

  \begin{itemize}
  \item
    \protect\hyperlink{buy-and-hold-abnormal-returns-bhar}{Buy and Hold Abnormal Returns (BHAR)}
  \item
    \protect\hyperlink{long-term-cumulative-abnormal-returns-lcars}{Long-term Cumulative Abnormal Returns (LCARs)}
  \item
    \protect\hyperlink{calendar-time-portfolio-abnormal-returns-ctars}{Calendar-time Portfolio Abnormal Returns (CTARs)} (Jensen's Alpha): manages cross-sectional dependence better and is less sensitive to (asset pricing) model misspecification
  \end{itemize}
\item
  Two types:

  \begin{itemize}
  \item
    Unexpected changes in firm specific variables (typically not announced, may not be immediately visible to all investors, impact on firm value is not straightforward): customer satisfaction scores effect on firm value \citep{jacobson2009financial} or unexpected changes in marketing expenditures \citep{kim2011stock} to determine mispricing.
  \item
    Complex consequences (investors take time to learn and incorporate info): acquisition depends on integration \citep{sorescu2007some}
  \end{itemize}
\item
  12 - 60 months event window: \citep{loughran1995new} \citep{brav1997myth}
\item
  Example: \citep{dutta2018robust}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(crseEventStudy)}

\CommentTok{\# example by the package\textquotesingle{}s author}
\FunctionTok{data}\NormalTok{(demo\_returns)}
\NormalTok{SAR }\OtherTok{\textless{}{-}}
    \FunctionTok{sar}\NormalTok{(}\AttributeTok{event =}\NormalTok{ demo\_returns}\SpecialCharTok{$}\NormalTok{EON,}
        \AttributeTok{control =}\NormalTok{ demo\_returns}\SpecialCharTok{$}\NormalTok{RWE,}
        \AttributeTok{logret =} \ConstantTok{FALSE}\NormalTok{)}
\FunctionTok{mean}\NormalTok{(SAR)}
\CommentTok{\#\textgreater{} [1] 0.006870196}
\end{Highlighting}
\end{Shaded}

\hypertarget{buy-and-hold-abnormal-returns-bhar}{%
\subsection{Buy and Hold Abnormal Returns (BHAR)}\label{buy-and-hold-abnormal-returns-bhar}}

\begin{itemize}
\tightlist
\item
  Classic references: \citep{loughran1995new} \citep{barber1997firm} \citep{lyon1999improved}
\end{itemize}

Use a portfolio of stocks that are close matches of the current firm over the same period as benchmark, and see the difference between the firm return and that of the portfolio.

\begin{itemize}
\tightlist
\item
  More technical note is that it measures returns from buying stocks in event-experiencing firms and shorting stocks in similar non-event firms within the same time.
\item
  Because of high cross-sectional correlations, BHARs' t-stat can be inflated, but its rank order is not affected \citep{markovitch2008findings, sorescu2007some}
\end{itemize}

To construct the portfolio, use similar

\begin{itemize}
\tightlist
\item
  size
\item
  book-to-market
\item
  momentum
\end{itemize}

Matching Procedure \citep{barber1997firm}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Each year from July to June, all common stocks in the CRSP database are categorized into ten groups (deciles) based on their market capitalization from the previous June.
\item
  Within these deciles, firms are further sorted into five groups (quintiles) based on their book-to-market ratios as of December of the previous year or earlier, considering possible delays in financial statement reporting.
\item
  Benchmark portfolios are designed to exclude firms with specific events but include all firms that can be classified into the characteristic-based portfolios.
\end{enumerate}

Similarly, \citet{wiles2010stock} uses the following matching procedure:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  All firms in the same two-digit SIC code with market values of 50\% to 150\% of the focal firms are selected
\item
  From this list, the 10 firms with the most comparable book-to-market ratios are chosen to serve as the matched portfolio (the matched portfolio can have less than 10 firms).
\end{enumerate}

Calculations:

\[
AR_{it} = R_{it} - E(R_{it}|X_t)
\]

Cumulative Abnormal Return (CAR):

\[
CAR_{it} = \sum_{t=1}^T (R_{it} - E(R_{it}))
\]

Buy-and-Hold Abnormal Return (BHAR)

\[
BHAR_{t = 1}^T = \Pi_{t=1}^T(1 + R_{it}) - \Pi_{t = 1}^T (1 + E(R_{it}))
\]

where as CAR is the arithmetic sum, BHAR is the geometric sum.

\begin{itemize}
\tightlist
\item
  In short-term event studies, differences between CAR and BHAR are often minimal. However, in long-term studies, this difference could significantly skew results. \citep{barber1997firm} shows that while BHAR is usually slightly lower than annual CAR, but it dramatically surpasses CAR when annual BHAR exceeds 28\%.
\end{itemize}

To calculate the long-run return (\(\Pi_{t=1}^T (1 + E(R_{it}))\)) of the benchmark portfolio, we can:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{With annual rebalance}: In each period, each portfolio is re-balanced and then compound mean stock returns in a portfolio over a given period:
\end{enumerate}

\[
\Pi_{t = 1}^T (1 + E(R_{it})) = \Pi_{t}^T (1 + \sum_{i = s}^{n_t}w_{it} R_{it})
\]

where \(n_t\) is the number of firms in period \(t\), and \(w_{it}\) is (1) \(1/n_t\) or (2) value-weight of firm \(i\) in period \(t\).

To avoid favoring recent events, in cross-sectional event studies, researchers usually treat all events equally when studying their impact on the stock market over time. This approach helps identify any abnormal changes in stock prices, especially when dealing with a series of unplanned events.

Potential problems:

\begin{itemize}
\item
  Solution first: Form benchmark portfolios that will never change constituent firms \citep{mitchell2000managerial}, because of these problems:

  \begin{itemize}
  \item
    Newly public companies often perform worse than a balanced market index \citep{ritter1991long}, and this, over time, might distort long-term return expectations due to the inclusion of these new companies (a phenomenon called ``new listing bias'' identified by \citet{barber1997firm}).
  \item
    Regularly rebalancing an equal-weight portfolio can lead to overestimated long-term returns and potentially skew buy-and-hold abnormal returns (BHARs) negatively due to constant selling of winning stocks and buying of underperformers (i.e., ``rebalancing bias'' \citep{barber1997firm}).
  \item
    Value-weight portfolios, which favor larger market cap stocks, can be viewed as an active investment strategy that keeps buying winning stocks and selling underperformers. Over time, this approach tends to positively distort BHARs.
  \end{itemize}
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Without annual rebalance}: Compounding the returns of the securities comprising the portfolio, followed by calculating the average across all securities
\end{enumerate}

\[
\Pi_{t = s}^{T} (1 + E(R_{it})) = \sum_{i=s}^{n_t} (w_{is} \Pi_{t=1}^T (1 + R_{it}))
\]

where \(t\) is the investment period, \(R_{it}\) is the return on security \(i\), \(n_i\) is the number of securities, \(w_{it}\) is either \(1/n_s\) or value-weight factor of security \(i\) at initial period \(s\). This portfolio's profits come from a simple investment where all the included stocks are given equal importance, or weighted according to their market value, as they were in a specific past period (period s). This means that it doesn't consider any stocks that were listed after this period, nor does it adjust the portfolio each month. However, one problem with this method is that the value assigned to each stock, based on its market size, needs to be corrected. This is to make sure that recent stocks don't end up having too much influence.

Fortunately, on \href{https://wrds-www.wharton.upenn.edu/pages/get-data/event-study-wrds/long-run-event-study-upload-you-own-events/}{WRDS}, it will give you all types of BHAR (2x2) (equal-weighted vs.~value-weighted and with annual rebalance and without annual rebalance)

\begin{itemize}
\tightlist
\item
  ``MINWIN'' is the smallest number of months a company trades after an event to be included in the study.
\end{itemize}

\begin{itemize}
\item
  ``MAXWIN'' is the most months that the study considers in its calculations.

  \begin{itemize}
  \tightlist
  \item
    Companies aren't excluded if they have less than MAXWIN months, unless they also have fewer than MINWIN months.
  \end{itemize}
\item
  The term ``MONTH'' signifies chosen months (typically 12, 24, or 36) used to work out BHAR.

  \begin{itemize}
  \tightlist
  \item
    If monthly returns are missing during the set period, matching portfolio returns fill in the gaps.
  \end{itemize}
\end{itemize}

\hypertarget{long-term-cumulative-abnormal-returns-lcars}{%
\subsection{Long-term Cumulative Abnormal Returns (LCARs)}\label{long-term-cumulative-abnormal-returns-lcars}}

Formula for LCARs during the \((1,T)\) postevent horizon \citep{sorescu2007some}

\[
LCAR_{pT} = \sum_{t = 1}^{t = T} (R_{it} - R_{pt})
\]

where \(R_{it}\) is the rate of return of stock \(i\) in month \(t\)

\(R_{pt}\) is the rate of return on the counterfactual portfolio in month \(t\)

\hypertarget{calendar-time-portfolio-abnormal-returns-ctars}{%
\subsection{Calendar-time Portfolio Abnormal Returns (CTARs)}\label{calendar-time-portfolio-abnormal-returns-ctars}}

This section follows strictly the procedure in \citep{wiles2010stock}

A portfolio for every day in calendar time (including all securities which experience an event that time).

For each portfolio, the securities and their returns are equally weighted

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  For all portfolios, the average abnormal return are calculated as
\end{enumerate}

\[
AAR_{Pt} = \frac{\sum_{i=1}^S AR_i}{S}
\]

where

\begin{itemize}
\tightlist
\item
  \(S\) is the number of securities in portfolio \(P\)
\item
  \(AR_i\) is the abnormal return for the stock \(i\) in the portfolio
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  For every portfolio \(P\), a time series estimate of \(\sigma(AAR_{Pt})\) is calculated for the preceding \(k\) days, assuming that the \(AAR_{Pt}\) are independent over time.
\item
  Each portfolio's average abnormal return is standardized
\end{enumerate}

\[
SAAR_{Pt} = \frac{AAR_{Pt}}{SD(AAR_{Pt})}
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Average standardized residual across all portfolio's in calendar time
\end{enumerate}

\[
ASAAR = \frac{1}{n}\sum_{i=1}^{255} SAAR_{Pt} \times D_t
\]

where

\begin{itemize}
\item
  \(D_t = 1\) when there is at least one security in portfolio \(t\)
\item
  \(D_t = 0\) when there are no security in portfolio \(t\)
\item
  \(n\) is the number of days in which the portfolio have at least one security \(n = \sum_{i = 1}^{255}D_t\)
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  The cumulative average standardized average abnormal returns is
\end{enumerate}

\[
CASSAR_{S_1, S_2} = \sum_{i=S_1}^{S_2} ASAAR
\]

If the ASAAR are independent over time, then standard deviation for the above estimate is \(\sqrt{S_2 - S_1 + 1}\)

then, the test statistics is

\[
t = \frac{CASAAR_{S_1,S_2}}{\sqrt{S_2 - S_1 + 1}}
\]

Limitations

\begin{itemize}
\item
  Cannot examine individual stock difference, can only see the difference at the portfolio level.

  \begin{itemize}
  \tightlist
  \item
    One can construct multiple portfolios (based on the metrics of interest) so that firms in the same portfolio shares that same characteristics. Then, one can compare the intercepts in each portfolio.
  \end{itemize}
\item
  Low power \citep{loughran2000uniformly}, type II error is likely.
\end{itemize}

\hypertarget{aggregation}{%
\section{Aggregation}\label{aggregation}}

\hypertarget{over-time}{%
\subsection{Over Time}\label{over-time}}

We calculate the cumulative abnormal (CAR) for the event windows

\(H_0\): Standardized cumulative abnormal return for stock \(i\) is 0 (no effect of events on stock performance)

\(H_1\): SCAR is not 0 (there is an effect of events on stock performance)

\hypertarget{across-firms-over-time}{%
\subsection{Across Firms + Over Time}\label{across-firms-over-time}}

Additional assumptions: Abnormal returns of different socks are uncorrelated (rather strong), but it's very valid if event windows for different stocks do not overlap. If the windows for different overlap, follow \citep{bernard1987cross} and \citep[\citet{schipper1983effects}]{schipper1983evidence}

\(H_0\): The mean of the abnormal returns across all firms is 0 (no effect)

\(H_1\): The mean of the abnormal returns across all firms is different form 0 (there is an effect)

Parametric (empirically either one works fine) (assume abnormal returns is normally distributed) :

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Aggregate the CAR of all stocks (Use this if the true abnormal variance is greater for stocks with higher variance)
\item
  Aggregate the SCAR of all stocks (Use this if the true abnormal return is constant across all stocks)
\end{enumerate}

Non-parametric (no parametric assumptions):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Sign test:

  \begin{itemize}
  \item
    Assume both the abnormal returns and CAR to be independent across stocks
  \item
    Assume 50\% with positive abnormal returns and 50\% with negative abnormal return
  \item
    The null will be that there is a positive abnormal return correlated with the event (if you want the alternative to be there is a negative relationship)
  \item
    With skewed distribution (likely in daily stock data), the size test is not trustworthy. Hence, rank test might be better
  \end{itemize}
\item
  Rank test

  \begin{itemize}
  \tightlist
  \item
    Null: there is no abnormal return during the event window
  \end{itemize}
\end{enumerate}

\hypertarget{heterogeneity-in-the-event-effect}{%
\section{Heterogeneity in the event effect}\label{heterogeneity-in-the-event-effect}}

\[
y = X \theta + \eta
\]

where

\begin{itemize}
\item
  \(y\) = CAR
\item
  \(X\) = Characteristics that lead to heterogeneity in the event effect (i.e., abnormal returns) (e.g., firm or event specific)
\item
  \(\eta\) = error term
\end{itemize}

Note:

\begin{itemize}
\tightlist
\item
  In cases with selection bias (firm characteristics and investor anticipation of the event: larger firms might enjoy great positive effect of an event, and investors endogenously anticipate this effect and overvalue the stock), we have to use the White's \(t\)-statistics to have the lower bounds of the true significance of the estimates.
\item
  This technique should be employed even if the average CAR is not significantly different from 0, especially when the CAR variance is high \citep{boyd2010chief}
\end{itemize}

\hypertarget{common-variables-in-marketing}{%
\subsection{Common variables in marketing}\label{common-variables-in-marketing}}

\citep{sorescu2017} Table 4

\begin{itemize}
\item
  Firm size is negatively correlated with abnormal return in finance \citep{sorescu2017}, but mixed results in marketing.
\item
  \# of event occurrences
\item
  R\&D expenditure
\item
  Advertising expense
\item
  Marketing investment (SG\&A)
\item
  Industry concentration (HHI, \# of competitors)
\item
  Financial leverage
\item
  Market share
\item
  Market size (total sales volume within the firm's SIC code)
\item
  marketing capability
\item
  Book to market value
\item
  ROA
\item
  Free cash flow
\item
  Sales growth
\item
  Firm age
\end{itemize}

\hypertarget{expected-return-calculation}{%
\section{Expected Return Calculation}\label{expected-return-calculation}}

\hypertarget{statistical-models}{%
\subsection{Statistical Models}\label{statistical-models}}

\begin{itemize}
\item
  based on statistical assumptions about the behavior of returns (e..g, multivariate normality)
\item
  we only need to assume stable distributions \citep{owen1983class}
\end{itemize}

\hypertarget{constant-mean-return-model}{%
\subsubsection{Constant Mean Return Model}\label{constant-mean-return-model}}

The expected normal return is the mean of the real returns

\[
Ra_{it} = R_{it} - \bar{R}_i
\]

Assumption:

\begin{itemize}
\tightlist
\item
  returns revert to its mean (very questionable)
\end{itemize}

The basic mean returns model generally delivers similar findings to more complex models since the variance of abnormal returns is not decreased considerably \citep{brown1985using}

\hypertarget{market-model}{%
\subsubsection{Market Model}\label{market-model}}

\[
R_{it} = \alpha_i + \beta R_{mt} + \epsilon_{it}
\]

where

\begin{itemize}
\item
  \(R_{it}\) = stock return \(i\) in period \(t\)
\item
  \(R_{mt}\) = market return
\item
  \(\epsilon_{it}\) = zero mean (\(E(e_{it}) = 0\)) error term with its own variance \(\sigma^2\)
\end{itemize}

Notes:

\begin{itemize}
\item
  People typically use S\&P 500, CRSP value-weighed or equal-weighted index as the market portfolio.
\item
  When \(\beta =0\), the \protect\hyperlink{market-model}{Market Model} is the \protect\hyperlink{constant-mean-return-model}{Constant Mean Return Model}
\item
  better fit of the market-model, the less variance in abnormal return, and the more easy to detect the event's effect
\item
  recommend generalized method of moments to be robust against auto-correlation and heteroskedasticity
\end{itemize}

\hypertarget{fama-french-model}{%
\subsubsection{Fama-French Model}\label{fama-french-model}}

Please note that there is a difference between between just taking the return versus taking the excess return as the dependent variable.

The correct way is to use the excess return for firm and for market \citep[p.~1917]{fama2010luck}.

\begin{itemize}
\tightlist
\item
  \(\alpha_i\) ``is the average return left unexplained by the benchmark model'' (i.e., abnormal return)
\end{itemize}

\hypertarget{ff3}{%
\paragraph{FF3}\label{ff3}}

\citep{fama1993common}

\[
\begin{aligned}
E(R_{it}|X_t) - r_{ft} = \alpha_i &+ \beta_{1i} (E(R_{mt}|X_t )- r_{ft}) \\
&+ b_{2i} SML_t + b_{3i} HML_t
\end{aligned}
\]

where

\begin{itemize}
\item
  \(r_{ft}\) risk-free rate (e.g., 3-month Treasury bill)
\item
  \(R_{mt}\) is the market-rate (e.g., S\&P 500)
\item
  SML: returns on small (size) portfolio minus returns on big portfolio
\item
  HML: returns on high (B/M) portfolio minus returns on low portfolio.
\end{itemize}

\hypertarget{ff4}{%
\paragraph{FF4}\label{ff4}}

\citep[p.~195]{sorescu2017} suggest the use of \protect\hyperlink{market-model}{Market Model} in marketing for short-term window and \protect\hyperlink{fama-french-model}{Fama-French Model} for the long-term window (the statistical properties of this model have not been examined the the daily setting).

\citep{carhart1997persistence}

\[
\begin{aligned}
E(R_{it}|X_t) - r_{ft} = \alpha_i &+ \beta_{1i} (E(R_{mt}|X_t )- r_{ft}) \\
&+ b_{2i} SML_t + b_{3i} HML_t + b_{4i} UMD_t
\end{aligned}
\]

where

\begin{itemize}
\tightlist
\item
  \(UMD_t\) is the momentum factor (difference between high and low prior return stock portfolios) in day \(t\).
\end{itemize}

\hypertarget{economic-model}{%
\subsection{Economic Model}\label{economic-model}}

The only difference between CAPM and APT is that APT has multiple factors (including factors beyond the focal company)

Economic models put limits on a statistical model that come from assumed behavior that is derived from theory.

\hypertarget{capital-asset-pricing-model-capm}{%
\subsubsection{Capital Asset Pricing Model (CAPM)}\label{capital-asset-pricing-model-capm}}

\[
E(R_i) = R_f + \beta_i (E(R_m) - R_f)
\]

where

\begin{itemize}
\item
  \(E(R_i)\) = expected firm return
\item
  \(R_f\) = risk free rate
\item
  \(E(R_m - R_f)\) = market risk premium
\item
  \(\beta_i\) = firm sensitivity
\end{itemize}

\hypertarget{arbitrage-pricing-theory-apt}{%
\subsubsection{Arbitrage Pricing Theory (APT)}\label{arbitrage-pricing-theory-apt}}

\[
R = R_f + \Lambda f + \epsilon
\]

where

\begin{itemize}
\item
  \(\epsilon \sim N(0, \Psi)\)
\item
  \(\Lambda\) = factor loadings
\item
  \(f \sim N(\mu, \Omega)\) = general factor model

  \begin{itemize}
  \item
    \(\mu\) = expected risk premium vector
  \item
    \(\Omega\) = factor covariance matrix
  \end{itemize}
\end{itemize}

\hypertarget{application-14}{%
\section{Application}\label{application-14}}

Packages:

\begin{itemize}
\item
  \texttt{eventstudies}
\item
  \texttt{erer}
\item
  \texttt{EventStudy}
\item
  \texttt{AbnormalReturns}
\item
  \href{https://www.eventstudytools.com/}{Event Study Tools}
\item
  \href{https://irudnyts.github.io/estudy2/}{estudy2}
\item
  \texttt{PerformanceAnalytics}
\end{itemize}

In practice, people usually sort portfolio because they are not sure whether the FF model is specified correctly.

Steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Sort all returns in CRSP into 10 deciles based on size.
\item
  In each decile, sort returns into 10 decides based on BM
\item
  Get the average return of the 100 portfolios for each period (i.e., expected returns of stocks given decile - characteristics)
\item
  For each stock in the event study: Compare the return of the stock to the corresponding portfolio based on size and BM.
\end{enumerate}

Notes:

\begin{itemize}
\item
  Sorting produces outcomes that are often more conservative (e.g., FF abnormal returns can be greater than those that used sorting).
\item
  If the results change when we do B/M first then size or vice versa, then the results are not robust (this extends to more than just two characteristics - e.g., momentum).
\end{itemize}

Examples:

Forestry:

\begin{itemize}
\item
  \citep{mei2008} M\&A on financial performance (forest product)
\item
  \citep{sun2011effects} litigation on firm values
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(erer)}

\CommentTok{\# example by the package\textquotesingle{}s author}
\FunctionTok{data}\NormalTok{(daEsa)}
\NormalTok{hh }\OtherTok{\textless{}{-}} \FunctionTok{evReturn}\NormalTok{(}
    \AttributeTok{y =}\NormalTok{ daEsa,       }\CommentTok{\# dataset}
    \AttributeTok{firm =} \StringTok{"wpp"}\NormalTok{,    }\CommentTok{\# firm name}
    \AttributeTok{y.date =} \StringTok{"date"}\NormalTok{, }\CommentTok{\# date in y }
    \AttributeTok{index =} \StringTok{"sp500"}\NormalTok{, }\CommentTok{\# index}
    \AttributeTok{est.win =} \DecValTok{250}\NormalTok{,   }\CommentTok{\# estimation window wedith in days}
    \AttributeTok{digits =} \DecValTok{3}\NormalTok{, }
    \AttributeTok{event.date =} \DecValTok{19990505}\NormalTok{, }\CommentTok{\# firm event dates }
    \AttributeTok{event.win =} \DecValTok{5}          \CommentTok{\# one{-}side event window wdith in days (default = 3, where 3 before + 1 event date + 3 days after = 7 days)}
\NormalTok{)}
\NormalTok{hh; }\FunctionTok{plot}\NormalTok{(hh)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} === Regression coefficients by firm =========}
\CommentTok{\#\textgreater{}   N firm event.date alpha.c alpha.e alpha.t alpha.p alpha.s beta.c beta.e}
\CommentTok{\#\textgreater{} 1 1  wpp   19990505  {-}0.135   0.170  {-}0.795   0.428          0.665  0.123}
\CommentTok{\#\textgreater{}   beta.t beta.p beta.s}
\CommentTok{\#\textgreater{} 1  5.419  0.000    ***}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} === Abnormal returns by date ================}
\CommentTok{\#\textgreater{}    day Ait.wpp    HNt}
\CommentTok{\#\textgreater{} 1   {-}5   4.564  4.564}
\CommentTok{\#\textgreater{} 2   {-}4   0.534  5.098}
\CommentTok{\#\textgreater{} 3   {-}3  {-}1.707  3.391}
\CommentTok{\#\textgreater{} 4   {-}2   2.582  5.973}
\CommentTok{\#\textgreater{} 5   {-}1  {-}0.942  5.031}
\CommentTok{\#\textgreater{} 6    0  {-}3.247  1.784}
\CommentTok{\#\textgreater{} 7    1  {-}0.646  1.138}
\CommentTok{\#\textgreater{} 8    2  {-}2.071 {-}0.933}
\CommentTok{\#\textgreater{} 9    3   0.368 {-}0.565}
\CommentTok{\#\textgreater{} 10   4   4.141  3.576}
\CommentTok{\#\textgreater{} 11   5   0.861  4.437}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} === Average abnormal returns across firms ===}
\CommentTok{\#\textgreater{}      name estimate error t.value p.value sig}
\CommentTok{\#\textgreater{} 1 CiT.wpp    4.437 8.888   0.499   0.618    }
\CommentTok{\#\textgreater{} 2     GNT    4.437 8.888   0.499   0.618}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{28-event-study_files/figure-latex/unnamed-chunk-4-1} \end{center}

Example by \href{https://lamfo-unb.github.io/2017/08/17/Teste-de-Eventos-en/}{Ana Julia Akaishi Padula, Pedro Albuquerque (posted on LAMFO)}

Example in \texttt{AbnormalReturns} package

\hypertarget{eventus}{%
\subsection{Eventus}\label{eventus}}

2 types of output:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \begin{itemize}
  \item
    Using different estimation methods (e.g., market model to calendar-time approach)
  \item
    Does not include event-specific returns. Hence, no regression later to determine variables that can affect abnormal stock returns.
  \end{itemize}
\item
  \protect\hyperlink{cross-sectional-analysis-of-eventus}{Cross-sectional Analysis of Eventus}: Event-specific abnormal returns (using monthly or data data) for cross-sectional analysis (under \textbf{Cross-Sectional Analysis} section)

  \begin{itemize}
  \tightlist
  \item
    Since it has the stock-specific abnormal returns, we can do regression on CARs later. But it only gives market-adjusted model. However, according to \citep{sorescu2017}, they advocate for the use of market-adjusted model for the short-term only, and reserve the FF4 for the longer-term event studies using monthly daily.
  \end{itemize}
\end{enumerate}

\hypertarget{basic-event-study}{%
\subsubsection{Basic Event Study}\label{basic-event-study}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Input a text file contains a firm identifier (e.g., PERMNO, CUSIP) and the event date
\item
  Choose market indices: equally weighted and the value weighted index (i.e., weighted by their market capitalization). And check Fama-French and Carhart factors.
\item
  Estimation options

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    Estimation period: \texttt{ESTLEN\ =\ 100} is the convention so that the estimation is not impacted by outliers.
  \item
    Use ``autodate'' options: the first trading after the event date is used if the event falls on a weekend or holiday
  \end{enumerate}
\item
  Abnormal returns window: depends on the specific event
\item
  Choose test: either parametric (including \protect\hyperlink{patell-standardized-residual-psr}{Patell Standardized Residual (PSR)}) or non-parametric
\end{enumerate}

\hypertarget{cross-sectional-analysis-of-eventus}{%
\subsubsection{Cross-sectional Analysis of Eventus}\label{cross-sectional-analysis-of-eventus}}

Similar to the \href{Average\%20abnormal\%20returns\%20across\%20a\%20sample}{Basic Event Study}, but now you can have event-specific abnormal returns.

\hypertarget{evenstudies}{%
\subsection{Evenstudies}\label{evenstudies}}

This package does not use the Fama-French model, only the market models.

This example is by the author of the package

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(eventstudies)}
\CommentTok{\# firm and date data}
\FunctionTok{data}\NormalTok{(}\StringTok{"SplitDates"}\NormalTok{)}
\FunctionTok{head}\NormalTok{(SplitDates)}

\CommentTok{\# stock price data }
\FunctionTok{data}\NormalTok{(}\StringTok{"StockPriceReturns"}\NormalTok{)}
\FunctionTok{head}\NormalTok{(StockPriceReturns)}
\FunctionTok{class}\NormalTok{(StockPriceReturns)}

\NormalTok{es }\OtherTok{\textless{}{-}}
    \FunctionTok{eventstudy}\NormalTok{(}
        \AttributeTok{firm.returns =}\NormalTok{ StockPriceReturns,}
        \AttributeTok{event.list =}\NormalTok{ SplitDates,}
        \AttributeTok{event.window =} \DecValTok{5}\NormalTok{,}
        \AttributeTok{type =} \StringTok{"None"}\NormalTok{,}
        \AttributeTok{to.remap =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{remap =} \StringTok{"cumsum"}\NormalTok{,}
        \AttributeTok{inference =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{inference.strategy =} \StringTok{"bootstrap"}
\NormalTok{    )}

\FunctionTok{plot}\NormalTok{(es)}
\end{Highlighting}
\end{Shaded}

\hypertarget{eventstudy}{%
\subsection{EventStudy}\label{eventstudy}}

You have to pay for the API key. (It's \$10/month).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(EventStudy)}
\end{Highlighting}
\end{Shaded}

\href{https://cran.rstudio.com/web/packages/EventStudy/vignettes/get_started.html}{Example} by the authors of the package

Data Prep

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyquant)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(readr)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"Quandl"}\NormalTok{)}
\FunctionTok{library}\NormalTok{(}\StringTok{"quantmod"}\NormalTok{)}
\FunctionTok{Quandl.auth}\NormalTok{(}\StringTok{"LDqWhYXzVd2omw4zipN2"}\NormalTok{)}
\NormalTok{TWTR }\OtherTok{\textless{}{-}} \FunctionTok{Quandl}\NormalTok{(}\StringTok{"NSE/OIL"}\NormalTok{,}\AttributeTok{type =}\StringTok{"xts"}\NormalTok{)}
\FunctionTok{candleChart}\NormalTok{(TWTR)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{28-event-study_files/figure-latex/unnamed-chunk-8-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{addSMA}\NormalTok{(}\AttributeTok{col=}\StringTok{"red"}\NormalTok{) }\CommentTok{\#Adding a Simple Moving Average}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{28-event-study_files/figure-latex/unnamed-chunk-8-2} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{addEMA}\NormalTok{() }\CommentTok{\#Adding an Exponential Moving Average}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{28-event-study_files/figure-latex/unnamed-chunk-8-3} \end{center}

Reference market in Germany is DAX

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Index Data}
\CommentTok{\# indexName \textless{}{-} c("DAX")}

\NormalTok{indexData }\OtherTok{\textless{}{-}} \FunctionTok{tq\_get}\NormalTok{(}\StringTok{"\^{}GDAXI"}\NormalTok{, }\AttributeTok{from =} \StringTok{"2014{-}05{-}01"}\NormalTok{, }\AttributeTok{to =} \StringTok{"2015{-}12{-}31"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{date =} \FunctionTok{format}\NormalTok{(date, }\StringTok{"\%d.\%m.\%Y"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{symbol =} \StringTok{"DAX"}\NormalTok{)}

\FunctionTok{head}\NormalTok{(indexData)}
\end{Highlighting}
\end{Shaded}

Create files

\begin{itemize}
\tightlist
\item
  \texttt{01\_RequestFile.csv}
\item
  \texttt{02\_FirmData.csv}
\item
  \texttt{03\_MarketData.csv}
\end{itemize}

Calculating abnormal returns

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# get \& set parameters for abnormal return Event Study}
\CommentTok{\# we use a garch model and csv as return}
\CommentTok{\# Attention: fitting a GARCH(1, 1) model is compute intensive}
\NormalTok{esaParams }\OtherTok{\textless{}{-}}\NormalTok{ EventStudy}\SpecialCharTok{::}\NormalTok{ARCApplicationInput}\SpecialCharTok{$}\FunctionTok{new}\NormalTok{()}
\NormalTok{esaParams}\SpecialCharTok{$}\FunctionTok{setResultFileType}\NormalTok{(}\StringTok{"csv"}\NormalTok{)}
\NormalTok{esaParams}\SpecialCharTok{$}\FunctionTok{setBenchmarkModel}\NormalTok{(}\StringTok{"garch"}\NormalTok{)}


\NormalTok{dataFiles }\OtherTok{\textless{}{-}}
    \FunctionTok{c}\NormalTok{(}
        \StringTok{"request\_file"} \OtherTok{=} \FunctionTok{file.path}\NormalTok{(}\FunctionTok{getwd}\NormalTok{(), }\StringTok{"data"}\NormalTok{, }\StringTok{"EventStudy"}\NormalTok{, }\StringTok{"01\_requestFile.csv"}\NormalTok{),}
        \StringTok{"firm\_data"}    \OtherTok{=} \FunctionTok{file.path}\NormalTok{(}\FunctionTok{getwd}\NormalTok{(), }\StringTok{"data"}\NormalTok{, }\StringTok{"EventStudy"}\NormalTok{, }\StringTok{"02\_firmDataPrice.csv"}\NormalTok{),}
        \StringTok{"market\_data"}  \OtherTok{=} \FunctionTok{file.path}\NormalTok{(}\FunctionTok{getwd}\NormalTok{(), }\StringTok{"data"}\NormalTok{, }\StringTok{"EventStudy"}\NormalTok{, }\StringTok{"03\_marketDataPrice.csv"}\NormalTok{)}
\NormalTok{    )}

\CommentTok{\# check data files, you can do it also in our R6 class}
\NormalTok{EventStudy}\SpecialCharTok{::}\FunctionTok{checkFiles}\NormalTok{(dataFiles)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{arEventStudy }\OtherTok{\textless{}{-}}\NormalTok{ estSetup}\SpecialCharTok{$}\FunctionTok{performEventStudy}\NormalTok{(}\AttributeTok{estParams     =}\NormalTok{ esaParams, }
                                      \AttributeTok{dataFiles     =}\NormalTok{ dataFiles, }
                                      \AttributeTok{downloadFiles =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(EventStudy)}

\NormalTok{apiUrl }\OtherTok{\textless{}{-}} \StringTok{"https://api.eventstudytools.com"}
\FunctionTok{Sys.setenv}\NormalTok{(}\AttributeTok{EventStudyapiKey =} \StringTok{""}\NormalTok{)}

\CommentTok{\# The URL is already set by default}
\FunctionTok{options}\NormalTok{(}\AttributeTok{EventStudy.URL =}\NormalTok{ apiUrl)}
\FunctionTok{options}\NormalTok{(}\AttributeTok{EventStudy.KEY =} \FunctionTok{Sys.getenv}\NormalTok{(}\StringTok{"EventStudyapiKey"}\NormalTok{))}

\CommentTok{\# use EventStudy estAPIKey function}
\FunctionTok{estAPIKey}\NormalTok{(}\FunctionTok{Sys.getenv}\NormalTok{(}\StringTok{"EventStudyapiKey"}\NormalTok{))}

\CommentTok{\# initialize object}
\NormalTok{estSetup }\OtherTok{\textless{}{-}}\NormalTok{ EventStudyAPI}\SpecialCharTok{$}\FunctionTok{new}\NormalTok{()}
\NormalTok{estSetup}\SpecialCharTok{$}\FunctionTok{authentication}\NormalTok{(}\AttributeTok{apiKey =} \FunctionTok{Sys.getenv}\NormalTok{(}\StringTok{"EventStudyapiKey"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{matching-methods}{%
\chapter{Matching Methods}\label{matching-methods}}

Matching is a process that aims to close back doors - potential sources of bias - by constructing comparison groups that are similar according to a set of matching variables. This helps to ensure that any observed differences in outcomes between the treatment and comparison groups can be more confidently attributed to the treatment itself, rather than other factors that may differ between the groups.

Matching and DiD can use pre-treatment outcomes to correct for selection bias. From real world data and simulation, \citep{chabe2015analysis} found that matching generally underestimates the average causal effect and gets closer to the true effect with more number of pre-treatment outcomes. When selection bias is symmetric around the treatment date, DID is still consistent when implemented symmetrically (i.e., the same number of period before and after treatment). In cases where selection bias is asymmetric, the MC simulations show that Symmetric DID still performs better than Matching.

\textbf{Assumption}: Observables can identify the selection into the treatment and control groups

\textbf{Identification}: The exclusion restriction can be met conditional on the observables

\textbf{Motivation}

Effect of college quality on earnings

\begin{itemize}
\tightlist
\item
  They ultimately estimate the treatment effect on the treated of attending a top (high ACT) versus bottom (low ACT) quartile college
\end{itemize}

\textbf{Example}

\citet{aaronson2007teachers}

Do teachers qualifications (causally) affect student test scores?

Step 1:

\[
Y_{ijt} = \delta_0 + Y_{ij(t-1)} \delta_1 + X_{it} \delta_2 + Z_{jt} \delta_3 + \epsilon_{ijt}
\]

There can always be another variable

Any observable sorting is imperfect

Step 2:

\[
Y_{ijst} = \alpha_0 + Y_{ij(t-1)}\alpha_1 + X_{it} \alpha_2 + Z_{jt} \alpha_3 + \gamma_s + u_{isjt}
\]

\begin{itemize}
\item
  \(\delta_3 >0\)
\item
  \(\delta_3 > \alpha_3\)
\item
  \(\gamma_s\) = school fixed effect
\end{itemize}

Sorting is less within school. Hence, we can introduce the school fixed effect

Step 3:

Find schools that look like they are putting students in class randomly (or as good as random) + we run step 2

\[
\begin{aligned}
Y_{isjt} = Y_{isj(t-1)} \lambda &+ X_{it} \alpha_1 +Z_{jt} \alpha_{21} \\
&+ (Z_{jt} \times D_i)\alpha_{22}+ \gamma_5 + u_{isjt}
\end{aligned}
\]

\begin{itemize}
\item
  \(D_{it}\) is an element of \(X_{it}\)
\item
  \(Z_{it}\) = teacher experience
\end{itemize}

\[
D_{it}=
\begin{cases}
1 & \text{ if high poverty} \\
0 & \text{otherwise}
\end{cases}
\]

\(H_0:\) \(\alpha_{22} = 0\) test for effect heterogeneity whether the effect of teacher experience (\(Z_{jt}\)) is different

\begin{itemize}
\item
  For low poverty is \(\alpha_{21}\)
\item
  For high poverty effect is \(\alpha_{21} + \alpha_{22}\)
\end{itemize}

Matching is \textbf{selection on observables} and only works if you have good observables.

Sufficient identification assumption under Selection on observable/ back-door criterion (based on Bernard Koch's \href{https://www.youtube.com/watch?v=v9uf9rDYEMg\&ab_channel=SummerInstituteinComputationalSocialScience}{presentation})

\begin{itemize}
\item
  Strong conditional ignorability

  \begin{itemize}
  \item
    \(Y(0),Y(1) \perp T|X\)
  \item
    No hidden confounders
  \end{itemize}
\item
  Overlap

  \begin{itemize}
  \item
    \(\forall x \in X, t \in \{0, 1\}: p (T = t | X = x> 0\)
  \item
    All treatments have non-zero probability of being observed
  \end{itemize}
\item
  SUTVA/ Consistency

  \begin{itemize}
  \tightlist
  \item
    Treatment and outcomes of different subjects are independent
  \end{itemize}
\end{itemize}

Relative to \protect\hyperlink{ordinary-least-squares}{OLS}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Matching makes the \textbf{common support} explicit (and changes default from ``ignore'' to ``enforce'')
\item
  Relaxes linear function form. Thus, less parametric.
\end{enumerate}

It also helps if you have high ratio of controls to treatments.

For detail summary \citep{stuart2010matching}

Matching is defined as ``any method that aims to equate (or''balance'') the distribution of covariates in the treated and control groups.'' \citep[pp.~1]{stuart2010matching}

Equivalently, matching is a selection on observables identifications strategy.

\textbf{If you think your OLS estimate is biased, a matching estimate (almost surely) is too.}

Unconditionally, consider

\[
\begin{aligned}
E(Y_i^T | T) - E(Y_i^C |C) &+ E(Y_i^C | T) - E(Y_i^C | T) \\
= E(Y_i^T - Y_i^C | T) &+ [E(Y_i^C | T) - E(Y_i^C |C)] \\
= E(Y_i^T - Y_i^C | T) &+ \text{selection bias}
\end{aligned}
\]

where \(E(Y_i^T - Y_i^C | T)\) is the causal inference that we want to know.

Randomization eliminates the selection bias.

If we don't have randomization, then \(E(Y_i^C | T) \neq E(Y_i^C |C)\)

Matching tries to do selection on observables \(E(Y_i^C | X, T) = E(Y_i^C|X, C)\)

\protect\hyperlink{propensity-scores}{Propensity Scores} basically do \(E(Y_i^C| P(X) , T) = E(Y_i^C | P(X), C)\)

\textbf{Matching standard errors will exceed OLS standard errors}

The treatment should have larger predictive power than the control because you use treatment to pick control (not control to pick treatment).

The average treatment effect (ATE) is

\[
\frac{1}{N_T} \sum_{i=1}^{N_T} (Y_i^T - \frac{1}{N_{C_T}} \sum_{i=1}^{N_{C_T}} Y_i^C)
\]

Since there is no closed-form solution for the standard error of the average treatment effect, we have to use bootstrapping to get standard error.

Professor Gary King advocates instead of using the word ``matching'', we should use ``\textbf{pruning}'' (i.e., deleting observations). It is a preprocessing step where it prunes nonmatches to make control variables less important in your analysis.

Without Matching

\begin{itemize}
\tightlist
\item
  \textbf{Imbalance data} leads to \textbf{model dependence} lead to a lot of \textbf{researcher discretion} leads to \textbf{bias}
\end{itemize}

With Matching

\begin{itemize}
\tightlist
\item
  We have balance data which essentially erase human discretion
\end{itemize}

\begin{longtable}[]{@{}lll@{}}
\caption{Table @ref(tab:Gary King - International Methods Colloquium talk 2015)}\tabularnewline
\toprule\noalign{}
Balance Covariates & Complete Randomization & Fully Exact \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
Balance Covariates & Complete Randomization & Fully Exact \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Observed & On average & Exact \\
Unobserved & On average & On average \\
\end{longtable}

Fully blocked is superior on

\begin{itemize}
\item
  imbalance
\item
  model dependence
\item
  power
\item
  efficiency
\item
  bias
\item
  research costs
\item
  robustness
\end{itemize}

Matching is used when

\begin{itemize}
\item
  Outcomes are not available to select subjects for follow-up
\item
  Outcomes are available to improve precision of the estimate (i.e., reduce bias)
\end{itemize}

Hence, we can only observe one outcome of a unit (either treated or control), we can think of this problem as missing data as well. Thus, this section is closely related to \protect\hyperlink{imputation-missing-data}{Imputation (Missing Data)}

In observational studies, we cannot randomize the treatment effect. Subjects select their own treatments, which could introduce selection bias (i.e., systematic differences between group differences that confound the effects of response variable differences).

Matching is used to

\begin{itemize}
\item
  reduce model dependence
\item
  diagnose balance in the dataset
\end{itemize}

Assumptions of matching:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  treatment assignment is independent of potential outcomes given the covariates

  \begin{itemize}
  \item
    \(T \perp (Y(0),Y(1))|X\)
  \item
    known as ignorability, or ignorable, no hidden bias, or unconfounded.
  \item
    You typically satisfy this assumption when unobserved covariates correlated with observed covariates.

    \begin{itemize}
    \tightlist
    \item
      But when unobserved covariates are unrelated to the observed covariates, you can use sensitivity analysis to check your result, or use ``design sensitivity'' \citep{heller2009split}
    \end{itemize}
  \end{itemize}
\item
  positive probability of receiving treatment for all X

  \begin{itemize}
  \tightlist
  \item
    \(0 < P(T=1|X)<1 \forall X\)
  \end{itemize}
\item
  Stable Unit Treatment value Assumption (SUTVA)

  \begin{itemize}
  \item
    Outcomes of A are not affected by treatment of B.

    \begin{itemize}
    \tightlist
    \item
      Very hard in cases where there is ``spillover'' effects (interactions between control and treatment). To combat, we need to reduce interactions.
    \end{itemize}
  \end{itemize}
\end{enumerate}

Generalization

\begin{itemize}
\item
  \(P_t\): treated population -\textgreater{} \(N_t\): random sample from treated
\item
  \(P_c\): control population -\textgreater{} \(N_c\): random sample from control
\item
  \(\mu_i\) = means ; \(\Sigma_i\) = variance covariance matrix of the \(p\) covariates in group i (\(i = t,c\))
\item
  \(X_j\) = \(p\) covariates of individual \(j\)
\item
  \(T_j\) = treatment assignment
\item
  \(Y_j\) = observed outcome
\item
  Assume: \(N_t < N_c\)
\item
  Treatment effect is \(\tau(x) = R_1(x) - R_0(x)\) where

  \begin{itemize}
  \item
    \(R_1(x) = E(Y(1)|X)\)
  \item
    \(R_0(x) = E(Y(0)|X)\)
  \end{itemize}
\item
  Assume: parallel trends hence \(\tau(x) = \tau \forall x\)

  \begin{itemize}
  \tightlist
  \item
    If the parallel trends are not assumed, an average effect can be estimated.
  \end{itemize}
\item
  Common estimands:

  \begin{itemize}
  \item
    Average effect of the treatment on the treated (ATT): effects on treatment group
  \item
    Average treatment effect (ATE): effect on both treatment and control
  \end{itemize}
\end{itemize}

Steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Define ``closeness'': decide distance measure to be used

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    Which variables to include:

    \begin{enumerate}
    \def\labelenumiii{\arabic{enumiii}.}
    \item
      Ignorability (no unobserved differences between treatment and control)

      \begin{enumerate}
      \def\labelenumiv{\arabic{enumiv}.}
      \item
        Since cost of including unrelated variables is small, you should include as many as possible (unless sample size/power doesn't allow you to because of increased variance)
      \item
        Do not include variables that were affected by the treatment.
      \item
        Note: if a matching variable (i.e., heavy drug users) is highly correlated to the outcome variable (i.e., heavy drinkers) , you will be better to exclude it in the matching set.
      \end{enumerate}
    \end{enumerate}
  \item
    Which distance measures: more below
  \end{enumerate}
\item
  Matching methods

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    Nearest neighbor matching

    \begin{enumerate}
    \def\labelenumiii{\arabic{enumiii}.}
    \item
      Simple (greedy) matching: performs poorly when there is competition for controls.
    \item
      Optimal matching: considers global distance measure
    \item
      Ratio matching: to combat increase bias and reduced variation when you have k:1 matching, one can use approximations by \citet{rubin1996matching}.
    \item
      With or without replacement: with replacement is typically better, but one needs to account for dependent in the matched sample when doing later analysis (can use frequency weights to combat).
    \end{enumerate}
  \item
    Subclassification, Full Matching and Weighting

    Nearest neighbor matching assign is 0 (control) or 1 (treated), while these methods use weights between 0 and 1.

    \begin{enumerate}
    \def\labelenumiii{\arabic{enumiii}.}
    \item
      Subclassification: distribution into multiple subclass (e.g., 5-10)
    \item
      Full matching: optimal ly minimize the average of the distances between each treated unit and each control unit within each matched set.
    \item
      Weighting adjustments: weighting technique uses propensity scores to estimate ATE. If the weights are extreme, the variance can be large not due to the underlying probabilities, but due to the estimation procure. To combat this, use (1) weight trimming, or (2) doubly -robust methods when propensity scores are used for weighing or matching.

      \begin{enumerate}
      \def\labelenumiv{\arabic{enumiv}.}
      \item
        Inverse probability of treatment weighting (IPTW) \(w_i = \frac{T_i}{\hat{e}_i} + \frac{1 - T_i}{1 - \hat{e}_i}\)
      \item
        Odds \(w_i = T_i + (1-T_i) \frac{\hat{e}_i}{1-\hat{e}_i}\)
      \item
        Kernel weighting (e.g., in economics) averages over multiple units in the control group.
      \end{enumerate}
    \end{enumerate}
  \item
    Assessing Common Support

    \begin{itemize}
    \tightlist
    \item
      common support means overlapping of the propensity score distributions in the treatment and control groups. Propensity score is used to discard control units from the common support. Alternatively, convex hull of the covariates in the multi-dimensional space.
    \end{itemize}
  \end{enumerate}
\item
  Assessing the quality of matched samples (Diagnose)

  \begin{itemize}
  \item
    Balance = similarity of the empirical distribution of the full set of covariates in the matched treated and control groups. Equivalently, treatment is unrelated to the covariates

    \begin{itemize}
    \tightlist
    \item
      \(\tilde{p}(X|T=1) = \tilde{p}(X|T=0)\) where \(\tilde{p}\) is the empirical distribution.
    \end{itemize}
  \item
    Numerical Diagnostics

    \begin{enumerate}
    \def\labelenumii{\arabic{enumii}.}
    \item
      standardized difference in means of each covariate (most common), also known as''standardized bias'', ``standardized difference in means''.
    \item
      standardized difference of means of the propensity score (should be \textless{} 0.25) \citep{rubin2001using}
    \item
      ratio of the variances of the propensity score in the treated and control groups (should be between 0.5 and 2). \citep{rubin2001using}
    \item
      For each covariate, the ratio fo the variance of the residuals orthogonal to the propensity score in the treated and control groups.

      Note: can't use hypothesis tests or p-values because of (1) in-sample property (not population), (2) conflation of changes in balance with changes in statistical power.
    \end{enumerate}
  \item
    Graphical Diagnostics

    \begin{itemize}
    \item
      QQ plots
    \item
      Empirical Distribution Plot
    \end{itemize}
  \end{itemize}
\item
  Estimate the treatment effect

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    After k:1

    \begin{enumerate}
    \def\labelenumiii{\arabic{enumiii}.}
    \tightlist
    \item
      Need to account for weights when use matching with replacement.
    \end{enumerate}
  \item
    After Subclassification and Full Matching

    \begin{enumerate}
    \def\labelenumiii{\arabic{enumiii}.}
    \item
      Weighting the subclass estimates by the number of treated units in each subclass for ATT
    \item
      Weighting by the overall number of individual in each subclass for ATE.
    \end{enumerate}
  \item
    Variance estimation: should incorporate uncertainties in both the matching procedure (step 3) and the estimation procedure (step 4)
  \end{enumerate}
\end{enumerate}

Notes:

\begin{itemize}
\item
  With missing data, use generalized boosted models, or multiple imputation \citep{qu2009propensity}
\item
  Violation of ignorable treatment assignment (i.e., unobservables affect treatment and outcome). control by

  \begin{itemize}
  \item
    measure pre-treatment measure of the outcome variable
  \item
    find the difference in outcomes between multiple control groups. If there is a significant difference, there is evidence for violation.
  \item
    find the range of correlations between unobservables and both treatment assignment and outcome to nullify the significant effect.
  \end{itemize}
\item
  Choosing between methods

  \begin{itemize}
  \item
    smallest standardized difference of mean across the largest number of covariates
  \item
    minimize the standardized difference of means of a few particularly prognostic covariates
  \item
    fest number of large standardized difference of means (\textgreater{} 0.25)
  \item
    \citep{diamond2013genetic} automates the process
  \end{itemize}
\item
  In practice

  \begin{itemize}
  \item
    If ATE, ask if there is enough overlap of the treated and control groups' propensity score to estimate ATE, if not use ATT instead
  \item
    If ATT, ask if there are controls across the full range of the treated group
  \end{itemize}
\item
  Choose matching method

  \begin{itemize}
  \item
    If ATE, use IPTW or full matching
  \item
    If ATT, and more controls than treated (at least 3 times), k:1 nearest neighbor without replacement
  \item
    If ATT, and few controls , use subclassification, full matching, and weighting by the odds
  \end{itemize}
\item
  Diagnostic

  \begin{itemize}
  \item
    If balance, use regression on matched samples
  \item
    If imbalance on few covariates, treat them with Mahalanobis
  \item
    If imbalance on many covariates, try k:1 matching with replacement
  \end{itemize}
\end{itemize}

Ways to define the distance \(D_{ij}\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Exact
\end{enumerate}

\[
D_{ij} = 
\begin{cases}
0, \text{ if } X_i = X_j, \\
\infty, \text{ if } X_i \neq X_j
\end{cases}
\]

An advanced is \protect\hyperlink{coarsened-exact-matching}{Coarsened Exact Matching}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Mahalanobis
\end{enumerate}

\[
D_{ij} = (X_i - X_j)'\Sigma^{-1} (X_i - X_j)
\]

where

\(\Sigma\) = variance covariance matrix of X in the

\begin{itemize}
\item
  control group if ATT is interested
\item
  polled treatment and control groups if ATE is interested
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Propensity score:
\end{enumerate}

\[
D_{ij} = |e_i - e_j|
\]

where \(e_k\) = the propensity score for individual k

An advanced is Prognosis score \citep{hansen2008prognostic}, but you have to know (i.e., specify) the relationship between the covariates and outcome.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Linear propensity score
\end{enumerate}

\[
D_{ij} = |logit(e_i) - logit(e_j)|
\]

The exact and Mahalanobis are not good in high dimensional or non normally distributed X's cases.

We can combine Mahalanobis matching with propensity score calipers \citep{rubin2000combining}

Other advanced methods for longitudinal settings

\begin{itemize}
\item
  marginal structural models \citep{robins2000marginal}
\item
  balanced risk set matching \citep{li2001balanced}
\end{itemize}

Most matching methods are based on (ex-post)

\begin{itemize}
\item
  propensity score
\item
  distance metric
\item
  covariates
\end{itemize}

Packages

\begin{itemize}
\item
  \texttt{cem} Coarsened exact matching
\item
  \texttt{Matching} Multivariate and propensity score matching with balance optimization
\item
  \texttt{MatchIt} Nonparametric preprocessing for parametric causal inference. Have nearest neighbor, Mahalanobis, caliper, exact, full, optimal, subclassification
\item
  \texttt{MatchingFrontier} optimize balance and sample size \citep{king2017balance}
\item
  \texttt{optmatch}optimal matching with variable ratio, optimal and full matching
\item
  \texttt{PSAgraphics} Propensity score graphics
\item
  \texttt{rbounds} sensitivity analysis with matched data, examine ignorable treatment assignment assumption
\item
  \texttt{twang} weighting and analysis of non-equivalent groups
\item
  \texttt{CBPS} covariate balancing propensity score. Can also be used in the longitudinal setting with marginal structural models.
\item
  \texttt{PanelMatch} based on \href{https://imai.fas.harvard.edu/research/files/tscs.pdf}{Imai, Kim, and Wang (2018)}
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.6201}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.3799}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Matching
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Regression
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Not as sensitive to the functional form of the covariates & can estimate the effect of a continuous treatment \\
Easier to asses whether it's working

Easier to explain

allows a nice visualization of an evaluation & estimate the effect of all the variables (not just the treatment) \\
If you treatment is fairly rare, you may have a lot of control observations that are obviously no comparable & can estimate interactions of treatment with covariates \\
Less parametric & More parametric \\
Enforces common support (i.e., space where treatment and control have the same characteristics) & \\
\end{longtable}

However, the problem of \textbf{omitted variables} (i.e., those that affect both the outcome and whether observation was treated) - unobserved confounders is still present in matching methods.

Difference between matching and regression following Pischke's \href{https://econ.lse.ac.uk/staff/spischke/ec533/regression\%20vs\%20matching.pdf}{lecture}

Suppose we want to estimate the effect of treatment on the treated

\[
\begin{aligned}
\delta_{TOT} &= E[ Y_{1i} - Y_{0i} | D_i = 1 ] \\
&= E\{E[Y_{1i} | X_i, D_i = 1] \\
& - E[Y_{0i}|X_i, D_i = 1]|D_i = 1\} && \text{law of itereated expectations}
\end{aligned}
\]

Under conditional independence

\[
E[Y_{0i} |X_i , D_i = 0 ] = E[Y_{0i} | X_i, D_i = 1]
\]

then

\[
\begin{aligned}
\delta_{TOT} &= E \{ E[ Y_{1i} | X_i, D_i = 1] - E[ Y_{0i}|X_i, D_i = 0 ]|D_i = 1\} \\
&= E\{E[y_i | X_i, D_i = 1] - E[y_i |X_i, D_i = 0 ] | D_i = 1\} \\
&= E[\delta_X |D_i = 1]
\end{aligned}
\]

where \(\delta_X\) is an X-specific difference in means at covariate value \(X_i\)

When \(X_i\) is discrete, the matching estimand is

\[
\delta_M = \sum_x \delta_x P(X_i = x |D_i = 1)
\]

where \(P(X_i = x |D_i = 1)\) is the probability mass function for \(X_i\) given \(D_i = 1\)

According to Bayes rule,

\[
P(X_i = x | D_i = 1) = \frac{P(D_i = 1 | X_i = x) \times P(X_i = x)}{P(D_i = 1)}
\]

hence,

\[
\begin{aligned}
\delta_M &= \frac{\sum_x \delta_x P (D_i = 1 | X_i = x) P (X_i = x)}{\sum_x P(D_i = 1 |X_i = x)P(X_i = x)} \\
&= \sum_x \delta_x \frac{ P (D_i = 1 | X_i = x) P (X_i = x)}{\sum_x P(D_i = 1 |X_i = x)P(X_i = x)}
\end{aligned}
\]

On the other hand, suppose we have regression

\[
y_i = \sum_x d_{ix} \beta_x + \delta_R D_i + \epsilon_i
\]

where

\begin{itemize}
\item
  \(d_{ix}\) = dummy that indicates \(X_i = x\)
\item
  \(\beta_x\) = regression-effect for \(X_i = x\)
\item
  \(\delta_R\) = regression estimand where
\end{itemize}

\[
\begin{aligned}
\delta_R &= \frac{\sum_x \delta_x [P(D_i = 1 | X_i = x) (1 - P(D_i = 1 | X_i = x))]P(X_i = x)}{\sum_x [P(D_i = 1| X_i = x)(1 - P(D_i = 1 | X_i = x))]P(X_i = x)} \\
&= \sum_x \delta_x \frac{[P(D_i = 1 | X_i = x) (1 - P(D_i = 1 | X_i = x))]P(X_i = x)}{\sum_x [P(D_i = 1| X_i = x)(1 - P(D_i = 1 | X_i = x))]P(X_i = x)}
\end{aligned}
\]

the difference between the regression and matching estimand is the weights they use to combine the covariate specific treatment effect \(\delta_x\)

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.0253}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1715}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.4464}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3528}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
uses weights which depend on
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
interpretation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
makes sense because
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Matching & \(P(D_i = 1|X_i = x)\)

the fraction of treated observations in a covariate cell (i.e., or the mean of \(D_i\)) & This is larger in cells with many treated observations. & we want the effect of treatment on the treated \\
Regression & \(P(D_i = 1 |X_i = x)(1 - P(D_i = 1| X_i ))\)

the variance of \(D_i\) in the covariate cell & This weight is largest in cells where there are half treated and half untreated observations. (this is the reason why we want to treat our sample so it is balanced, before running regular regression model, as mentioned above). & these cells will produce the lowest variance estimates of \(\delta_x\). If all the \(\delta_x\) are the same, the most efficient estimand uses the lowest variance cells most heavily. \\
\end{longtable}

The goal of matching is to produce covariate balance (i.e., distributions of covariates in treatment and control groups are approximately similar as they would be in a successful randomized experiment).

\hypertarget{matchit}{%
\section{MatchIt}\label{matchit}}

Procedure typically involves (proposed by \href{https://cran.r-project.org/web/packages/MatchIt/vignettes/MatchIt.html}{Noah Freifer} using \texttt{MatchIt})

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  planning
\item
  matching
\item
  checking (balance)
\item
  estimating the treatment effect
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(MatchIt)}
\FunctionTok{data}\NormalTok{(}\StringTok{"lalonde"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

examine \texttt{treat} on \texttt{re78}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Planning
\end{enumerate}

\begin{itemize}
\item
  select type of effect to be estimated (e.g., mediation effect, conditional effect, marginal effect)
\item
  select the target population
\item
  select variables to match/balance \citep{austin2011optimal} \citep{vanderweele2019principles}
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Check Initial Imbalance
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# No matching; constructing a pre{-}match matchit object}
\NormalTok{m.out0 }\OtherTok{\textless{}{-}} \FunctionTok{matchit}\NormalTok{(}
    \FunctionTok{formula}\NormalTok{(treat }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ educ }\SpecialCharTok{+}\NormalTok{ race }
            \SpecialCharTok{+}\NormalTok{ married }\SpecialCharTok{+}\NormalTok{ nodegree }\SpecialCharTok{+}\NormalTok{ re74 }\SpecialCharTok{+}\NormalTok{ re75, }\AttributeTok{env =}\NormalTok{ lalonde),}
    \AttributeTok{data =} \FunctionTok{data.frame}\NormalTok{(lalonde),}
    \AttributeTok{method =} \ConstantTok{NULL}\NormalTok{,}
    \CommentTok{\# assess balance before matching}
    \AttributeTok{distance =} \StringTok{"glm"} \CommentTok{\# logistic regression}
\NormalTok{)}

\CommentTok{\# Checking balance prior to matching}
\FunctionTok{summary}\NormalTok{(m.out0)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Matching
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 1:1 NN PS matching w/o replacement}
\NormalTok{m.out1 }\OtherTok{\textless{}{-}} \FunctionTok{matchit}\NormalTok{(treat }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ educ,}
                  \AttributeTok{data =}\NormalTok{ lalonde,}
                  \AttributeTok{method =} \StringTok{"nearest"}\NormalTok{,}
                  \AttributeTok{distance =} \StringTok{"glm"}\NormalTok{)}
\NormalTok{m.out1}
\CommentTok{\#\textgreater{} A matchit object}
\CommentTok{\#\textgreater{}  {-} method: 1:1 nearest neighbor matching without replacement}
\CommentTok{\#\textgreater{}  {-} distance: Propensity score}
\CommentTok{\#\textgreater{}              {-} estimated with logistic regression}
\CommentTok{\#\textgreater{}  {-} number of obs.: 614 (original), 370 (matched)}
\CommentTok{\#\textgreater{}  {-} target estimand: ATT}
\CommentTok{\#\textgreater{}  {-} covariates: age, educ}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Check balance
\end{enumerate}

Sometimes you have to make trade-off between balance and sample size.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Checking balance after NN matching}
\FunctionTok{summary}\NormalTok{(m.out1, }\AttributeTok{un =} \ConstantTok{FALSE}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} matchit(formula = treat \textasciitilde{} age + educ, data = lalonde, method = "nearest", }
\CommentTok{\#\textgreater{}     distance = "glm")}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Summary of Balance for Matched Data:}
\CommentTok{\#\textgreater{}          Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean}
\CommentTok{\#\textgreater{} distance        0.3080        0.3077          0.0094     0.9963    0.0033}
\CommentTok{\#\textgreater{} age            25.8162       25.8649         {-}0.0068     1.0300    0.0050}
\CommentTok{\#\textgreater{} educ           10.3459       10.2865          0.0296     0.5886    0.0253}
\CommentTok{\#\textgreater{}          eCDF Max Std. Pair Dist.}
\CommentTok{\#\textgreater{} distance   0.0432          0.0146}
\CommentTok{\#\textgreater{} age        0.0162          0.0597}
\CommentTok{\#\textgreater{} educ       0.1189          0.8146}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Sample Sizes:}
\CommentTok{\#\textgreater{}           Control Treated}
\CommentTok{\#\textgreater{} All           429     185}
\CommentTok{\#\textgreater{} Matched       185     185}
\CommentTok{\#\textgreater{} Unmatched     244       0}
\CommentTok{\#\textgreater{} Discarded       0       0}

\CommentTok{\# examine visually}
\FunctionTok{plot}\NormalTok{(m.out1, }\AttributeTok{type =} \StringTok{"jitter"}\NormalTok{, }\AttributeTok{interactive =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{29-matching-methods_files/figure-latex/unnamed-chunk-4-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\FunctionTok{plot}\NormalTok{(}
\NormalTok{    m.out1,}
    \AttributeTok{type =} \StringTok{"qq"}\NormalTok{,}
    \AttributeTok{interactive =} \ConstantTok{FALSE}\NormalTok{,}
    \AttributeTok{which.xs =} \FunctionTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{29-matching-methods_files/figure-latex/unnamed-chunk-4-2} \end{center}

Try Full Match (i.e., every treated matches with one control, and every control with one treated).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Full matching on a probit PS}
\NormalTok{m.out2 }\OtherTok{\textless{}{-}} \FunctionTok{matchit}\NormalTok{(treat }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ educ, }
                  \AttributeTok{data =}\NormalTok{ lalonde,}
                  \AttributeTok{method =} \StringTok{"full"}\NormalTok{, }
                  \AttributeTok{distance =} \StringTok{"glm"}\NormalTok{, }
                  \AttributeTok{link =} \StringTok{"probit"}\NormalTok{)}
\NormalTok{m.out2}
\CommentTok{\#\textgreater{} A matchit object}
\CommentTok{\#\textgreater{}  {-} method: Optimal full matching}
\CommentTok{\#\textgreater{}  {-} distance: Propensity score}
\CommentTok{\#\textgreater{}              {-} estimated with probit regression}
\CommentTok{\#\textgreater{}  {-} number of obs.: 614 (original), 614 (matched)}
\CommentTok{\#\textgreater{}  {-} target estimand: ATT}
\CommentTok{\#\textgreater{}  {-} covariates: age, educ}
\end{Highlighting}
\end{Shaded}

Checking balance again

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Checking balance after full matching}
\FunctionTok{summary}\NormalTok{(m.out2, }\AttributeTok{un =} \ConstantTok{FALSE}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} matchit(formula = treat \textasciitilde{} age + educ, data = lalonde, method = "full", }
\CommentTok{\#\textgreater{}     distance = "glm", link = "probit")}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Summary of Balance for Matched Data:}
\CommentTok{\#\textgreater{}          Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean}
\CommentTok{\#\textgreater{} distance        0.3082        0.3081          0.0023     0.9815    0.0028}
\CommentTok{\#\textgreater{} age            25.8162       25.8035          0.0018     0.9825    0.0062}
\CommentTok{\#\textgreater{} educ           10.3459       10.2315          0.0569     0.4390    0.0481}
\CommentTok{\#\textgreater{}          eCDF Max Std. Pair Dist.}
\CommentTok{\#\textgreater{} distance   0.0270          0.0382}
\CommentTok{\#\textgreater{} age        0.0249          0.1110}
\CommentTok{\#\textgreater{} educ       0.1300          0.9805}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Sample Sizes:}
\CommentTok{\#\textgreater{}               Control Treated}
\CommentTok{\#\textgreater{} All            429.       185}
\CommentTok{\#\textgreater{} Matched (ESS)  145.23     185}
\CommentTok{\#\textgreater{} Matched        429.       185}
\CommentTok{\#\textgreater{} Unmatched        0.         0}
\CommentTok{\#\textgreater{} Discarded        0.         0}

\FunctionTok{plot}\NormalTok{(}\FunctionTok{summary}\NormalTok{(m.out2))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{29-matching-methods_files/figure-latex/unnamed-chunk-6-1} \end{center}

Exact Matching

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Full matching on a probit PS}
\NormalTok{m.out3 }\OtherTok{\textless{}{-}}
    \FunctionTok{matchit}\NormalTok{(}
\NormalTok{        treat }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ educ,}
        \AttributeTok{data =}\NormalTok{ lalonde,}
        \AttributeTok{method =} \StringTok{"exact"}
\NormalTok{    )}
\NormalTok{m.out3}
\CommentTok{\#\textgreater{} A matchit object}
\CommentTok{\#\textgreater{}  {-} method: Exact matching}
\CommentTok{\#\textgreater{}  {-} number of obs.: 614 (original), 332 (matched)}
\CommentTok{\#\textgreater{}  {-} target estimand: ATT}
\CommentTok{\#\textgreater{}  {-} covariates: age, educ}
\end{Highlighting}
\end{Shaded}

Subclassfication

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m.out4 }\OtherTok{\textless{}{-}} \FunctionTok{matchit}\NormalTok{(}
\NormalTok{    treat }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ educ, }
    \AttributeTok{data =}\NormalTok{ lalonde,}
    \AttributeTok{method =} \StringTok{"subclass"}
\NormalTok{)}
\NormalTok{m.out4}
\CommentTok{\#\textgreater{} A matchit object}
\CommentTok{\#\textgreater{}  {-} method: Subclassification (6 subclasses)}
\CommentTok{\#\textgreater{}  {-} distance: Propensity score}
\CommentTok{\#\textgreater{}              {-} estimated with logistic regression}
\CommentTok{\#\textgreater{}  {-} number of obs.: 614 (original), 614 (matched)}
\CommentTok{\#\textgreater{}  {-} target estimand: ATT}
\CommentTok{\#\textgreater{}  {-} covariates: age, educ}

\CommentTok{\# Or you can use in conjunction with "nearest"}
\NormalTok{m.out4 }\OtherTok{\textless{}{-}} \FunctionTok{matchit}\NormalTok{(}
\NormalTok{    treat }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ educ,}
    \AttributeTok{data =}\NormalTok{ lalonde,}
    \AttributeTok{method =} \StringTok{"nearest"}\NormalTok{,}
    \AttributeTok{option =} \StringTok{"subclass"}
\NormalTok{)}
\NormalTok{m.out4}
\CommentTok{\#\textgreater{} A matchit object}
\CommentTok{\#\textgreater{}  {-} method: 1:1 nearest neighbor matching without replacement}
\CommentTok{\#\textgreater{}  {-} distance: Propensity score}
\CommentTok{\#\textgreater{}              {-} estimated with logistic regression}
\CommentTok{\#\textgreater{}  {-} number of obs.: 614 (original), 370 (matched)}
\CommentTok{\#\textgreater{}  {-} target estimand: ATT}
\CommentTok{\#\textgreater{}  {-} covariates: age, educ}
\end{Highlighting}
\end{Shaded}

Optimal Matching

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m.out5 }\OtherTok{\textless{}{-}} \FunctionTok{matchit}\NormalTok{(}
\NormalTok{    treat }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ educ, }
    \AttributeTok{data =}\NormalTok{ lalonde,}
    \AttributeTok{method =} \StringTok{"optimal"}\NormalTok{,}
    \AttributeTok{ratio =} \DecValTok{2}
\NormalTok{)}
\NormalTok{m.out5}
\CommentTok{\#\textgreater{} A matchit object}
\CommentTok{\#\textgreater{}  {-} method: 2:1 optimal pair matching}
\CommentTok{\#\textgreater{}  {-} distance: Propensity score}
\CommentTok{\#\textgreater{}              {-} estimated with logistic regression}
\CommentTok{\#\textgreater{}  {-} number of obs.: 614 (original), 555 (matched)}
\CommentTok{\#\textgreater{}  {-} target estimand: ATT}
\CommentTok{\#\textgreater{}  {-} covariates: age, educ}
\end{Highlighting}
\end{Shaded}

Genetic Matching

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m.out6 }\OtherTok{\textless{}{-}} \FunctionTok{matchit}\NormalTok{(}
\NormalTok{    treat }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ educ, }
    \AttributeTok{data =}\NormalTok{ lalonde,}
    \AttributeTok{method =} \StringTok{"genetic"}
\NormalTok{)}
\NormalTok{m.out6}
\CommentTok{\#\textgreater{} A matchit object}
\CommentTok{\#\textgreater{}  {-} method: 1:1 genetic matching without replacement}
\CommentTok{\#\textgreater{}  {-} distance: Propensity score}
\CommentTok{\#\textgreater{}              {-} estimated with logistic regression}
\CommentTok{\#\textgreater{}  {-} number of obs.: 614 (original), 370 (matched)}
\CommentTok{\#\textgreater{}  {-} target estimand: ATT}
\CommentTok{\#\textgreater{}  {-} covariates: age, educ}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Estimating the Treatment Effect
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# get matched data}
\NormalTok{m.data1 }\OtherTok{\textless{}{-}} \FunctionTok{match.data}\NormalTok{(m.out1)}

\FunctionTok{head}\NormalTok{(m.data1)}
\CommentTok{\#\textgreater{}      treat age educ   race married nodegree re74 re75       re78  distance}
\CommentTok{\#\textgreater{} NSW1     1  37   11  black       1        1    0    0  9930.0460 0.2536942}
\CommentTok{\#\textgreater{} NSW2     1  22    9 hispan       0        1    0    0  3595.8940 0.3245468}
\CommentTok{\#\textgreater{} NSW3     1  30   12  black       0        0    0    0 24909.4500 0.2881139}
\CommentTok{\#\textgreater{} NSW4     1  27   11  black       0        1    0    0  7506.1460 0.3016672}
\CommentTok{\#\textgreater{} NSW5     1  33    8  black       0        1    0    0   289.7899 0.2683025}
\CommentTok{\#\textgreater{} NSW6     1  22    9  black       0        1    0    0  4056.4940 0.3245468}
\CommentTok{\#\textgreater{}      weights subclass}
\CommentTok{\#\textgreater{} NSW1       1        1}
\CommentTok{\#\textgreater{} NSW2       1       98}
\CommentTok{\#\textgreater{} NSW3       1      109}
\CommentTok{\#\textgreater{} NSW4       1      120}
\CommentTok{\#\textgreater{} NSW5       1      131}
\CommentTok{\#\textgreater{} NSW6       1      142}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"lmtest"}\NormalTok{) }\CommentTok{\#coeftest}
\FunctionTok{library}\NormalTok{(}\StringTok{"sandwich"}\NormalTok{) }\CommentTok{\#vcovCL}

\CommentTok{\# imbalance matched dataset}
\NormalTok{fit1 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(re78 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ treat }\SpecialCharTok{+}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ educ ,}
           \AttributeTok{data =}\NormalTok{ m.data1, }
           \AttributeTok{weights =}\NormalTok{ weights)}

\FunctionTok{coeftest}\NormalTok{(fit1, }\AttributeTok{vcov. =}\NormalTok{ vcovCL, }\AttributeTok{cluster =} \SpecialCharTok{\textasciitilde{}}\NormalTok{subclass)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} t test of coefficients:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}              Estimate Std. Error t value Pr(\textgreater{}|t|)   }
\CommentTok{\#\textgreater{} (Intercept)  {-}174.902   2445.013 {-}0.0715 0.943012   }
\CommentTok{\#\textgreater{} treat       {-}1139.085    780.399 {-}1.4596 0.145253   }
\CommentTok{\#\textgreater{} age           153.133     55.317  2.7683 0.005922 **}
\CommentTok{\#\textgreater{} educ          358.577    163.860  2.1883 0.029278 * }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

\texttt{treat} coefficient = estimated ATT

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# balance matched dataset }
\NormalTok{m.data2 }\OtherTok{\textless{}{-}} \FunctionTok{match.data}\NormalTok{(m.out2)}

\NormalTok{fit2 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(re78 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ treat }\SpecialCharTok{+}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ educ , }
           \AttributeTok{data =}\NormalTok{ m.data2, }\AttributeTok{weights =}\NormalTok{ weights)}

\FunctionTok{coeftest}\NormalTok{(fit2, }\AttributeTok{vcov. =}\NormalTok{ vcovCL, }\AttributeTok{cluster =} \SpecialCharTok{\textasciitilde{}}\NormalTok{subclass)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} t test of coefficients:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)  }
\CommentTok{\#\textgreater{} (Intercept) 2151.952   3141.152  0.6851  0.49355  }
\CommentTok{\#\textgreater{} treat       {-}725.184    703.297 {-}1.0311  0.30289  }
\CommentTok{\#\textgreater{} age          120.260     53.933  2.2298  0.02612 *}
\CommentTok{\#\textgreater{} educ         175.693    241.694  0.7269  0.46755  }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

When reporting, remember to mention

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  the matching specification (method, and additional options)
\item
  the distance measure (e.g., propensity score)
\item
  other methods, and rationale for the final chosen method.
\item
  balance statistics of the matched dataset.
\item
  number of matched, unmatched, discarded
\item
  estimation method for treatment effect.
\end{enumerate}

\hypertarget{designmatch}{%
\section{designmatch}\label{designmatch}}

This package includes

\begin{itemize}
\item
  \texttt{distmatch} optimal distance matching
\item
  \texttt{bmatch} optimal bipartile matching
\item
  \texttt{cardmatch} optimal cardinality matching
\item
  \texttt{profmatch} optimal profile matching
\item
  \texttt{nmatch} optimal nonbipartile matching
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(designmatch)}
\end{Highlighting}
\end{Shaded}

\hypertarget{matchingfrontier}{%
\section{MatchingFrontier}\label{matchingfrontier}}

As mentioned in \texttt{MatchIt}, you have to make trade-off (also known as bias-variance trade-off) between balance and sample size. An automated procedure to optimize this trade-off is implemented in \texttt{MatchingFrontier} \citep{king2017balance}, which solves this joint optimization problem.

Following \texttt{MatchingFrontier} \href{https://projects.iq.harvard.edu/files/frontier/files/using_matchingfrontier.pdf}{guide}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# library(devtools)}
\CommentTok{\# install\_github(\textquotesingle{}ChristopherLucas/MatchingFrontier\textquotesingle{})}
\FunctionTok{library}\NormalTok{(MatchingFrontier)}
\FunctionTok{data}\NormalTok{(}\StringTok{"lalonde"}\NormalTok{)}
\CommentTok{\# choose var to match on}
\NormalTok{match.on }\OtherTok{\textless{}{-}}
    \FunctionTok{colnames}\NormalTok{(lalonde)[}\SpecialCharTok{!}\NormalTok{(}\FunctionTok{colnames}\NormalTok{(lalonde) }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}re78\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}treat\textquotesingle{}}\NormalTok{))]}
\NormalTok{match.on}

\CommentTok{\# Mahanlanobis frontier (default)}
\NormalTok{mahal.frontier }\OtherTok{\textless{}{-}}
    \FunctionTok{makeFrontier}\NormalTok{(}
        \AttributeTok{dataset =}\NormalTok{ lalonde,}
        \AttributeTok{treatment =} \StringTok{"treat"}\NormalTok{,}
        \AttributeTok{match.on =}\NormalTok{ match.on}
\NormalTok{    )}
\NormalTok{mahal.frontier}

\CommentTok{\# L1 frontier}
\NormalTok{L1.frontier }\OtherTok{\textless{}{-}}
    \FunctionTok{makeFrontier}\NormalTok{(}
        \AttributeTok{dataset =}\NormalTok{ lalonde,}
        \AttributeTok{treatment =} \StringTok{\textquotesingle{}treat\textquotesingle{}}\NormalTok{,}
        \AttributeTok{match.on =}\NormalTok{ match.on,}
        \AttributeTok{QOI =} \StringTok{\textquotesingle{}SATT\textquotesingle{}}\NormalTok{,}
        \AttributeTok{metric =} \StringTok{\textquotesingle{}L1\textquotesingle{}}\NormalTok{,}
        \AttributeTok{ratio =} \StringTok{\textquotesingle{}fixed\textquotesingle{}}
\NormalTok{    )}
\NormalTok{L1.frontier}

\CommentTok{\# estimate effects along the frontier}

\CommentTok{\# Set base form}
\NormalTok{my.form }\OtherTok{\textless{}{-}}
    \FunctionTok{as.formula}\NormalTok{(re78 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ treat }\SpecialCharTok{+}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ black }\SpecialCharTok{+}\NormalTok{ education }
               \SpecialCharTok{+}\NormalTok{ hispanic }\SpecialCharTok{+}\NormalTok{ married }\SpecialCharTok{+}\NormalTok{ nodegree }\SpecialCharTok{+}\NormalTok{ re74 }\SpecialCharTok{+}\NormalTok{ re75)}

\CommentTok{\# Estimate effects for the mahalanobis frontier}
\NormalTok{mahal.estimates }\OtherTok{\textless{}{-}}
    \FunctionTok{estimateEffects}\NormalTok{(}
\NormalTok{        mahal.frontier,}
        \StringTok{\textquotesingle{}re78 \textasciitilde{} treat\textquotesingle{}}\NormalTok{,}
        \AttributeTok{mod.dependence.formula =}\NormalTok{ my.form,}
        \AttributeTok{continuous.vars =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}education\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}re74\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}re75\textquotesingle{}}\NormalTok{),}
        \AttributeTok{prop.estimated =}\NormalTok{ .}\DecValTok{1}\NormalTok{,}
        \AttributeTok{means.as.cutpoints =} \ConstantTok{TRUE}
\NormalTok{    )}

\CommentTok{\# Estimate effects for the L1 frontier}
\NormalTok{L1.estimates }\OtherTok{\textless{}{-}}
    \FunctionTok{estimateEffects}\NormalTok{(}
\NormalTok{        L1.frontier,}
        \StringTok{\textquotesingle{}re78 \textasciitilde{} treat\textquotesingle{}}\NormalTok{,}
        \AttributeTok{mod.dependence.formula =}\NormalTok{ my.form,}
        \AttributeTok{continuous.vars =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}education\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}re74\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}re75\textquotesingle{}}\NormalTok{),}
        \AttributeTok{prop.estimated =}\NormalTok{ .}\DecValTok{1}\NormalTok{,}
        \AttributeTok{means.as.cutpoints =} \ConstantTok{TRUE}
\NormalTok{    )}

\CommentTok{\# Plot covariates means }
\CommentTok{\# plotPrunedMeans()}


\CommentTok{\# Plot estimates (deprecated)}
\CommentTok{\# plotEstimates(}
\CommentTok{\#     L1.estimates,}
\CommentTok{\#     ylim = c({-}10000, 3000),}
\CommentTok{\#     cex.lab = 1.4,}
\CommentTok{\#     cex.axis = 1.4,}
\CommentTok{\#     panel.first = grid(NULL, NULL, lwd = 2,)}
\CommentTok{\# )}

\CommentTok{\# Plot estimates}
\FunctionTok{plotMeans}\NormalTok{(L1.frontier)}


\CommentTok{\# parallel plot}
\FunctionTok{parallelPlot}\NormalTok{(}
\NormalTok{    L1.frontier,}
    \AttributeTok{N =} \DecValTok{400}\NormalTok{,}
    \AttributeTok{variables =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}re74\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}re75\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{),}
    \AttributeTok{treated.col =} \StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{,}
    \AttributeTok{control.col =} \StringTok{\textquotesingle{}gray\textquotesingle{}}
\NormalTok{)}

\CommentTok{\# export matched dataset}
\CommentTok{\# take 400 units}
\NormalTok{matched.data }\OtherTok{\textless{}{-}} \FunctionTok{generateDataset}\NormalTok{(L1.frontier, }\AttributeTok{N =} \DecValTok{400}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\hypertarget{propensity-scores}{%
\section{Propensity Scores}\label{propensity-scores}}

Even though I mention the propensity scores matching method here, it is no longer recommended to use such method in research and publication \citep{king2019propensity} because it increases

\begin{itemize}
\item
  imbalance
\item
  inefficiency
\item
  model dependence: small changes in the model specification lead to big changes in model results
\item
  bias
\end{itemize}

PSM tries to accomplish complete randomization while other methods try to achieve fully blocked. Hence, you probably better off use any other methods.

Propensity is ``the probability of receiving the treatment given the observed covariates.'' \citep{rosenbaum1985bias}

Equivalently, it can to understood as the probability of being treated.

\[
e_i (X_i) = P(T_i = 1 | X_i)
\]

Estimation using

\begin{itemize}
\item
  logistic regression
\item
  Non parametric methods:

  \begin{itemize}
  \item
    boosted CART
  \item
    generalized boosted models (gbm)
  \end{itemize}
\end{itemize}

Steps by Gary King's \href{https://www.youtube.com/watch?v=rBv39pK1iEs\&ab_channel=MethodsColloquium}{slides}

\begin{itemize}
\item
  reduce k elements of X to scalar
\item
  \(\pi_i \equiv P(T_i = 1|X) = \frac{1}{1+e^{X_i \beta}}\)
\item
  Distance (\(X_c, X_t\)) = \(|\pi_c - \pi_t|\)
\item
  match each treated unit to the nearest control unit
\item
  control units: not reused; pruned if unused
\item
  prune matches if distances \textgreater{} caliper
\end{itemize}

In the best case scenario, you randomly prune, which increases imbalance

Other methods dominate because they try to match exactly hence

\begin{itemize}
\item
  \(X_c = X_t \to \pi_c = \pi_t\) (exact match leads to equal propensity scores) but
\item
  \(\pi_c = \pi_t \nrightarrow X_c = X_t\) (equal propensity scores do not necessarily lead to exact match)
\end{itemize}

Notes:

\begin{itemize}
\item
  Do not include/control for irrelevant covariates because it leads your PSM to be more random, hence more imbalance
\item
  Do not include for \citep{bhattacharya2007instrumental} instrumental variable in the predictor set of a propensity score matching estimator. More generally, using variables that do not control for potential confounders, even if they are predictive of the treatment, can result in biased estimates
\end{itemize}

What you left with after pruning is more important than what you start with then throw out.

Diagnostics:

\begin{itemize}
\item
  balance of the covariates
\item
  no need to concern about collinearity
\item
  can't use c-stat or stepwise because those model fit stat do not apply
\end{itemize}

\hypertarget{mahalanobis-distance}{%
\section{Mahalanobis Distance}\label{mahalanobis-distance}}

Approximates fully blocked experiment

Distance \((X_c,X_t)\) = \(\sqrt{(X_c - X_t)'S^{-1}(X_c - X_t)}\)

where \(S^{-1}\) standardize the distance

In application we use Euclidean distance.

Prune unused control units, and prune matches if distance \textgreater{} caliper

\hypertarget{coarsened-exact-matching}{%
\section{Coarsened Exact Matching}\label{coarsened-exact-matching}}

Steps from Gray King's \href{https://www.youtube.com/watch?v=rBv39pK1iEs\&ab_channel=MethodsColloquium}{slides} International Methods Colloquium talk 2015

\begin{itemize}
\item
  Temporarily coarsen \(X\)
\item
  Apply exact matching to the coarsened \(X, C(X)\)

  \begin{itemize}
  \item
    sort observation into strata, each with unique values of \(C(X)\)
  \item
    prune stratum with 0 treated or 0 control units
  \end{itemize}
\item
  Pass on original (uncoarsened) units except those pruned
\end{itemize}

Properties:

\begin{itemize}
\item
  Monotonic imbalance bounding (MIB) matching method

  \begin{itemize}
  \tightlist
  \item
    maximum imbalance between the treated and control chosen ex ante
  \end{itemize}
\item
  meets congruence principle
\item
  robust to measurement error
\item
  can be implemented with multiple imputation
\item
  works well for multi-category treatments
\end{itemize}

Assumptions:

\begin{itemize}
\tightlist
\item
  Ignorability (i.e., no omitted variable bias)
\end{itemize}

More detail in \citep{iacus2012causal}

Example by \href{https://cran.r-project.org/web/packages/cem/vignettes/cem.pdf}{package's authors}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(cem)}
\FunctionTok{data}\NormalTok{(LeLonde)}

\NormalTok{Le }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\FunctionTok{na.omit}\NormalTok{(LeLonde)) }\CommentTok{\# remove missing data}
\CommentTok{\# treated and control groups}
\NormalTok{tr }\OtherTok{\textless{}{-}} \FunctionTok{which}\NormalTok{(Le}\SpecialCharTok{$}\NormalTok{treated}\SpecialCharTok{==}\DecValTok{1}\NormalTok{)}
\NormalTok{ct }\OtherTok{\textless{}{-}} \FunctionTok{which}\NormalTok{(Le}\SpecialCharTok{$}\NormalTok{treated}\SpecialCharTok{==}\DecValTok{0}\NormalTok{)}
\NormalTok{ntr }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(tr)}
\NormalTok{nct }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(ct)}

\CommentTok{\# unadjusted, biased difference in means}
\FunctionTok{mean}\NormalTok{(Le}\SpecialCharTok{$}\NormalTok{re78[tr]) }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(Le}\SpecialCharTok{$}\NormalTok{re78[ct])}
\CommentTok{\#\textgreater{} [1] 759.0479}

\CommentTok{\# pre{-}treatment covariates}
\NormalTok{vars }\OtherTok{\textless{}{-}}
    \FunctionTok{c}\NormalTok{(}
        \StringTok{"age"}\NormalTok{,}
        \StringTok{"education"}\NormalTok{,}
        \StringTok{"black"}\NormalTok{,}
        \StringTok{"married"}\NormalTok{,}
        \StringTok{"nodegree"}\NormalTok{,}
        \StringTok{"re74"}\NormalTok{,}
        \StringTok{"re75"}\NormalTok{,}
        \StringTok{"hispanic"}\NormalTok{,}
        \StringTok{"u74"}\NormalTok{,}
        \StringTok{"u75"}\NormalTok{,}
        \StringTok{"q1"}
\NormalTok{    )}

\CommentTok{\# overall imbalance statistics}
\FunctionTok{imbalance}\NormalTok{(}\AttributeTok{group=}\NormalTok{Le}\SpecialCharTok{$}\NormalTok{treated, }\AttributeTok{data=}\NormalTok{Le[vars]) }\CommentTok{\# L1 = 0.902}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Multivariate Imbalance Measure: L1=0.902}
\CommentTok{\#\textgreater{} Percentage of local common support: LCS=5.8\%}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Univariate Imbalance Measures:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}               statistic   type           L1 min 25\%      50\%       75\%}
\CommentTok{\#\textgreater{} age        {-}0.252373042 (diff) 5.102041e{-}03   0   0   0.0000   {-}1.0000}
\CommentTok{\#\textgreater{} education   0.153634710 (diff) 8.463851e{-}02   1   0   1.0000    1.0000}
\CommentTok{\#\textgreater{} black      {-}0.010322734 (diff) 1.032273e{-}02   0   0   0.0000    0.0000}
\CommentTok{\#\textgreater{} married    {-}0.009551495 (diff) 9.551495e{-}03   0   0   0.0000    0.0000}
\CommentTok{\#\textgreater{} nodegree   {-}0.081217371 (diff) 8.121737e{-}02   0  {-}1   0.0000    0.0000}
\CommentTok{\#\textgreater{} re74      {-}18.160446880 (diff) 5.551115e{-}17   0   0 284.0715  806.3452}
\CommentTok{\#\textgreater{} re75      101.501761679 (diff) 5.551115e{-}17   0   0 485.6310 1238.4114}
\CommentTok{\#\textgreater{} hispanic   {-}0.010144756 (diff) 1.014476e{-}02   0   0   0.0000    0.0000}
\CommentTok{\#\textgreater{} u74        {-}0.045582186 (diff) 4.558219e{-}02   0   0   0.0000    0.0000}
\CommentTok{\#\textgreater{} u75        {-}0.065555292 (diff) 6.555529e{-}02   0   0   0.0000    0.0000}
\CommentTok{\#\textgreater{} q1          7.494021189 (Chi2) 1.067078e{-}01  NA  NA       NA        NA}
\CommentTok{\#\textgreater{}                  max}
\CommentTok{\#\textgreater{} age          {-}6.0000}
\CommentTok{\#\textgreater{} education     1.0000}
\CommentTok{\#\textgreater{} black         0.0000}
\CommentTok{\#\textgreater{} married       0.0000}
\CommentTok{\#\textgreater{} nodegree      0.0000}
\CommentTok{\#\textgreater{} re74      {-}2139.0195}
\CommentTok{\#\textgreater{} re75        490.3945}
\CommentTok{\#\textgreater{} hispanic      0.0000}
\CommentTok{\#\textgreater{} u74           0.0000}
\CommentTok{\#\textgreater{} u75           0.0000}
\CommentTok{\#\textgreater{} q1                NA}

\CommentTok{\# drop other variables that are not pre {-} treatmentt matching variables}
\NormalTok{todrop }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"treated"}\NormalTok{, }\StringTok{"re78"}\NormalTok{)}
\FunctionTok{imbalance}\NormalTok{(}\AttributeTok{group=}\NormalTok{Le}\SpecialCharTok{$}\NormalTok{treated, }\AttributeTok{data=}\NormalTok{Le, }\AttributeTok{drop=}\NormalTok{todrop)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Multivariate Imbalance Measure: L1=0.902}
\CommentTok{\#\textgreater{} Percentage of local common support: LCS=5.8\%}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Univariate Imbalance Measures:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}               statistic   type           L1 min 25\%      50\%       75\%}
\CommentTok{\#\textgreater{} age        {-}0.252373042 (diff) 5.102041e{-}03   0   0   0.0000   {-}1.0000}
\CommentTok{\#\textgreater{} education   0.153634710 (diff) 8.463851e{-}02   1   0   1.0000    1.0000}
\CommentTok{\#\textgreater{} black      {-}0.010322734 (diff) 1.032273e{-}02   0   0   0.0000    0.0000}
\CommentTok{\#\textgreater{} married    {-}0.009551495 (diff) 9.551495e{-}03   0   0   0.0000    0.0000}
\CommentTok{\#\textgreater{} nodegree   {-}0.081217371 (diff) 8.121737e{-}02   0  {-}1   0.0000    0.0000}
\CommentTok{\#\textgreater{} re74      {-}18.160446880 (diff) 5.551115e{-}17   0   0 284.0715  806.3452}
\CommentTok{\#\textgreater{} re75      101.501761679 (diff) 5.551115e{-}17   0   0 485.6310 1238.4114}
\CommentTok{\#\textgreater{} hispanic   {-}0.010144756 (diff) 1.014476e{-}02   0   0   0.0000    0.0000}
\CommentTok{\#\textgreater{} u74        {-}0.045582186 (diff) 4.558219e{-}02   0   0   0.0000    0.0000}
\CommentTok{\#\textgreater{} u75        {-}0.065555292 (diff) 6.555529e{-}02   0   0   0.0000    0.0000}
\CommentTok{\#\textgreater{} q1          7.494021189 (Chi2) 1.067078e{-}01  NA  NA       NA        NA}
\CommentTok{\#\textgreater{}                  max}
\CommentTok{\#\textgreater{} age          {-}6.0000}
\CommentTok{\#\textgreater{} education     1.0000}
\CommentTok{\#\textgreater{} black         0.0000}
\CommentTok{\#\textgreater{} married       0.0000}
\CommentTok{\#\textgreater{} nodegree      0.0000}
\CommentTok{\#\textgreater{} re74      {-}2139.0195}
\CommentTok{\#\textgreater{} re75        490.3945}
\CommentTok{\#\textgreater{} hispanic      0.0000}
\CommentTok{\#\textgreater{} u74           0.0000}
\CommentTok{\#\textgreater{} u75           0.0000}
\CommentTok{\#\textgreater{} q1                NA}
\end{Highlighting}
\end{Shaded}

automated coarsening

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mat }\OtherTok{\textless{}{-}}
    \FunctionTok{cem}\NormalTok{(}
        \AttributeTok{treatment =} \StringTok{"treated"}\NormalTok{,}
        \AttributeTok{data =}\NormalTok{ Le,}
        \AttributeTok{drop =} \StringTok{"re78"}\NormalTok{,}
        \AttributeTok{keep.all =} \ConstantTok{TRUE}
\NormalTok{    )}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Using \textquotesingle{}treated\textquotesingle{}=\textquotesingle{}1\textquotesingle{} as baseline group}
\NormalTok{mat}
\CommentTok{\#\textgreater{}            G0  G1}
\CommentTok{\#\textgreater{} All       392 258}
\CommentTok{\#\textgreater{} Matched    95  84}
\CommentTok{\#\textgreater{} Unmatched 297 174}

\CommentTok{\# mat$w}
\end{Highlighting}
\end{Shaded}

coarsening by explicit user choice

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# categorial variables}
\FunctionTok{levels}\NormalTok{(Le}\SpecialCharTok{$}\NormalTok{q1) }\CommentTok{\# grouping option}
\CommentTok{\#\textgreater{} [1] "agree"             "disagree"          "neutral"          }
\CommentTok{\#\textgreater{} [4] "no opinion"        "strongly agree"    "strongly disagree"}
\NormalTok{q1.grp }\OtherTok{\textless{}{-}}
    \FunctionTok{list}\NormalTok{(}
        \FunctionTok{c}\NormalTok{(}\StringTok{"strongly agree"}\NormalTok{, }\StringTok{"agree"}\NormalTok{),}
        \FunctionTok{c}\NormalTok{(}\StringTok{"neutral"}\NormalTok{, }\StringTok{"no opinion"}\NormalTok{),}
        \FunctionTok{c}\NormalTok{(}\StringTok{"strongly disagree"}\NormalTok{, }\StringTok{"disagree"}\NormalTok{)}
\NormalTok{    ) }\CommentTok{\# if you want ordered categories}

\CommentTok{\# continuous variables}
\FunctionTok{table}\NormalTok{(Le}\SpecialCharTok{$}\NormalTok{education)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   3   4   5   6   7   8   9  10  11  12  13  14  15 }
\CommentTok{\#\textgreater{}   1   5   4   6  12  55 106 146 173 113  19   9   1}
\NormalTok{educut }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{6.5}\NormalTok{, }\FloatTok{8.5}\NormalTok{, }\FloatTok{12.5}\NormalTok{, }\DecValTok{17}\NormalTok{)  }\CommentTok{\# use cutpoints}

\NormalTok{mat1 }\OtherTok{\textless{}{-}}
    \FunctionTok{cem}\NormalTok{(}
        \AttributeTok{treatment =} \StringTok{"treated"}\NormalTok{,}
        \AttributeTok{data =}\NormalTok{ Le,}
        \AttributeTok{drop =} \StringTok{"re78"}\NormalTok{,}
        \AttributeTok{cutpoints =} \FunctionTok{list}\NormalTok{(}\AttributeTok{education =}\NormalTok{ educut),}
        \AttributeTok{grouping =} \FunctionTok{list}\NormalTok{(}\AttributeTok{q1 =}\NormalTok{ q1.grp)}
\NormalTok{    )}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Using \textquotesingle{}treated\textquotesingle{}=\textquotesingle{}1\textquotesingle{} as baseline group}
\NormalTok{mat1}
\CommentTok{\#\textgreater{}            G0  G1}
\CommentTok{\#\textgreater{} All       392 258}
\CommentTok{\#\textgreater{} Matched   158 115}
\CommentTok{\#\textgreater{} Unmatched 234 143}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  Can also use progressive coarsening method to control the number of matches.
\item
  \texttt{cem} can also handle some missingness.
\end{itemize}

\hypertarget{genetic-matching}{%
\section{Genetic Matching}\label{genetic-matching}}

\begin{itemize}
\item
  GM uses iterative checking process of propensity scores, which combines propensity scores and Mahalanobis distance.
\item
  GM is arguably ``superior'' method than nearest neighbor or full matching in imbalanced data
\item
  Use a genetic search algorithm to find weights for each covariate such that we have optimal balance.
\item
  Implementation

  \begin{itemize}
  \item
    could use \emph{with replacement}
  \item
    balance can be based on

    \begin{itemize}
    \item
      paired \(t\)-tests (dichotomous variables)
    \item
      Kolmogorov-Smirnov (multinomial and continuous)
    \end{itemize}
  \end{itemize}
\end{itemize}

Packages

\texttt{Matching}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(Matching)}
\FunctionTok{data}\NormalTok{(lalonde)}
\FunctionTok{attach}\NormalTok{(lalonde)}

\CommentTok{\#The covariates we want to match on}
\NormalTok{X }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(age, educ, black, hisp, married, nodegr, u74, u75, re75, re74)}

\CommentTok{\#The covariates we want to obtain balance on}
\NormalTok{BalanceMat }\OtherTok{\textless{}{-}}
    \FunctionTok{cbind}\NormalTok{(age,}
\NormalTok{          educ,}
\NormalTok{          black,}
\NormalTok{          hisp,}
\NormalTok{          married,}
\NormalTok{          nodegr,}
\NormalTok{          u74,}
\NormalTok{          u75,}
\NormalTok{          re75,}
\NormalTok{          re74,}
          \FunctionTok{I}\NormalTok{(re74 }\SpecialCharTok{*}\NormalTok{ re75))}

\CommentTok{\#}
\CommentTok{\#Let\textquotesingle{}s call GenMatch() to find the optimal weight to give each}
\CommentTok{\#covariate in \textquotesingle{}X\textquotesingle{} so as we have achieved balance on the covariates in}
\CommentTok{\#\textquotesingle{}BalanceMat\textquotesingle{}. This is only an example so we want GenMatch to be quick}
\CommentTok{\#so the population size has been set to be only 16 via the \textquotesingle{}pop.size\textquotesingle{}}
\CommentTok{\#option. This is *WAY* too small for actual problems.}
\CommentTok{\#For details see http://sekhon.berkeley.edu/papers/MatchingJSS.pdf.}
\CommentTok{\#}
\NormalTok{genout }\OtherTok{\textless{}{-}}
    \FunctionTok{GenMatch}\NormalTok{(}
        \AttributeTok{Tr =}\NormalTok{ treat,}
        \AttributeTok{X =}\NormalTok{ X,}
        \AttributeTok{BalanceMatrix =}\NormalTok{ BalanceMat,}
        \AttributeTok{estimand =} \StringTok{"ATE"}\NormalTok{,}
        \AttributeTok{M =} \DecValTok{1}\NormalTok{,}
        \AttributeTok{pop.size =} \DecValTok{16}\NormalTok{,}
        \AttributeTok{max.generations =} \DecValTok{10}\NormalTok{,}
        \AttributeTok{wait.generations =} \DecValTok{1}
\NormalTok{    )}

\CommentTok{\#The outcome variable}
\NormalTok{Y}\OtherTok{=}\NormalTok{re78}\SpecialCharTok{/}\DecValTok{1000}

\CommentTok{\#}
\CommentTok{\# Now that GenMatch() has found the optimal weights, let\textquotesingle{}s estimate}
\CommentTok{\# our causal effect of interest using those weights}
\CommentTok{\#}
\NormalTok{mout }\OtherTok{\textless{}{-}}
    \FunctionTok{Match}\NormalTok{(}
        \AttributeTok{Y =}\NormalTok{ Y,}
        \AttributeTok{Tr =}\NormalTok{ treat,}
        \AttributeTok{X =}\NormalTok{ X,}
        \AttributeTok{estimand =} \StringTok{"ATE"}\NormalTok{,}
        \AttributeTok{Weight.matrix =}\NormalTok{ genout}
\NormalTok{    )}
\FunctionTok{summary}\NormalTok{(mout)}

\CommentTok{\#                        }
\CommentTok{\#Let\textquotesingle{}s determine if balance has actually been obtained on the variables of interest}
\CommentTok{\#                        }
\NormalTok{mb }\OtherTok{\textless{}{-}}
    \FunctionTok{MatchBalance}\NormalTok{(}
\NormalTok{        treat }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ educ }\SpecialCharTok{+}\NormalTok{ black }\SpecialCharTok{+}\NormalTok{ hisp }\SpecialCharTok{+}\NormalTok{ married }\SpecialCharTok{+}\NormalTok{ nodegr }
        \SpecialCharTok{+}\NormalTok{ u74 }\SpecialCharTok{+}\NormalTok{ u75 }\SpecialCharTok{+}\NormalTok{ re75 }\SpecialCharTok{+}\NormalTok{ re74 }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(re74 }\SpecialCharTok{*}\NormalTok{ re75),}
        \AttributeTok{match.out =}\NormalTok{ mout,}
        \AttributeTok{nboots =} \DecValTok{500}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\hypertarget{matching-for-time-series-cross-section-data}{%
\section{Matching for time series-cross-section data}\label{matching-for-time-series-cross-section-data}}

Examples: \citep{scheve2012democracy} and \citep{acemoglu2019democracy}

Materials from Imai's \href{https://imai.fas.harvard.edu/talk/files/polmeth18.pdf}{slides}

Identification strategy:

\begin{itemize}
\item
  Within-unit over-time variation
\item
  within-time across-units variation
\end{itemize}

See \protect\hyperlink{did-with-in-and-out-treatment-condition}{DID with in and out treatment condition} for details of this method

\hypertarget{matching-for-multiple-treatments}{%
\section{Matching for multiple treatments}\label{matching-for-multiple-treatments}}

In cases where you have multiple treatment groups, and you want to do matching, it's important to have the same baseline (control) group. For more details, see

\begin{itemize}
\item
  \citep{mccaffrey2013tutorial}
\item
  \citep{lopez2017estimation}
\item
  \citep{zhao2021propensity}: also for continuous treatment
\end{itemize}

If you insist on using the \texttt{MatchIt} package, then see this \href{https://stats.stackexchange.com/questions/405019/matching-with-multiple-treatments}{answer}

\hypertarget{matching-for-multi-level-treatments}{%
\section{Matching for multi-level treatments}\label{matching-for-multi-level-treatments}}

See \citep{yang2016propensity}

Package in R \texttt{shuyang1987/multilevelMatching} on Github

\hypertarget{matching-for-repeated-treatments}{%
\section{Matching for repeated treatments}\label{matching-for-repeated-treatments}}

\url{https://cran.r-project.org/web/packages/twang/vignettes/iptw.pdf}

package in R \texttt{twang}

\hypertarget{interrupted-time-series}{%
\chapter{Interrupted Time Series}\label{interrupted-time-series}}

\begin{itemize}
\item
  Regression Discontinuity in Time
\item
  Control for

  \begin{itemize}
  \item
    Seasonable trends
  \item
    Concurrent events
  \end{itemize}
\item
  Pros \citep{penfold2013use}

  \begin{itemize}
  \tightlist
  \item
    control for long-term trends
  \end{itemize}
\item
  Cons

  \begin{itemize}
  \item
    Min of 8 data points before and 8 after an intervention
  \item
    Multiple events hard to distinguish
  \end{itemize}
\end{itemize}

Notes:

\begin{itemize}
\tightlist
\item
  For subgroup analysis (heterogeneity in effect size), see \citep{harper2017did}
\item
  To interpret with control variables, see \citep{bottomley2019analysing}
\end{itemize}

Interrupted time series should be used when

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  longitudinal data (outcome over time - observations before and after the intervention)
\item
  full population was affected at one specific point in time (or can be stacked based on intervention)
\end{enumerate}

In each ITS framework, there can be 4 possible scenarios of outcome after an intervention

\begin{itemize}
\item
  No effects
\item
  Immediate effect
\item
  Sustained (long-term) effect (smooth)
\item
  Both immediate and sustained effect
\end{itemize}

\[
Y = \beta_0 + \beta_1 T + \beta_2 D + \beta_3 P + \epsilon
\]

where

\begin{itemize}
\item
  \(Y\) is the outcome variable

  \begin{itemize}
  \tightlist
  \item
    \(\beta_0\) is the baseline level of the outcome
  \end{itemize}
\item
  \(T\) is the time variable (e.g., days, weeks, etc.) passed from the start of the observation period

  \begin{itemize}
  \tightlist
  \item
    \(\beta_1\) is the slope of the line before the intervention
  \end{itemize}
\item
  \(D\) is the treatment variable where \(1\) is after the intervention and \(0\) is before the intervention.

  \begin{itemize}
  \tightlist
  \item
    \(\beta_2\) is the \textbf{immediate effect} after the intervention
  \end{itemize}
\item
  \(P\) is the time variable indicating time passed since the intervention (before the intervention, the value is set to 0) (to examine the sustained effect).

  \begin{itemize}
  \tightlist
  \item
    \(\beta_3\) is the \textbf{sustained effect} = difference between the slope of the line prior to the intervention and the slope of the line subsequent to the intervention
  \end{itemize}
\end{itemize}

\textbf{Example}

Create a fictitious dataset where we know the true data generating process

\[
Outcome = 10 \times time + 20 \times treatment + 25 \times timesincetreatment + noise
\]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# number of days}
\NormalTok{n }\OtherTok{=} \DecValTok{365}


\CommentTok{\# intervention at day}
\NormalTok{interven }\OtherTok{=} \DecValTok{200}

\CommentTok{\# time index from 1 to 365}
\NormalTok{time }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n)}

\CommentTok{\# treatment variable: before internvation = day 1 to 200, }
\CommentTok{\# after intervention = day 201 to 365}
\NormalTok{treatment }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, interven), }\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, n }\SpecialCharTok{{-}}\NormalTok{ interven))}

\CommentTok{\# time since treatment}
\NormalTok{timesincetreat }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, interven), }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(n }\SpecialCharTok{{-}}\NormalTok{ interven)))}

\CommentTok{\# outcome}
\NormalTok{outcome }\OtherTok{=} \DecValTok{10} \SpecialCharTok{+} \DecValTok{15} \SpecialCharTok{*}\NormalTok{ time }\SpecialCharTok{+} \DecValTok{20} \SpecialCharTok{*}\NormalTok{ treatment }\SpecialCharTok{+} 
    \DecValTok{25} \SpecialCharTok{*}\NormalTok{ timesincetreat }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)}

\NormalTok{df }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(outcome, time, treatment, timesincetreat)}

\FunctionTok{head}\NormalTok{(df, }\DecValTok{10}\NormalTok{)}
\CommentTok{\#\textgreater{}      outcome time treatment timesincetreat}
\CommentTok{\#\textgreater{} 1   25.63491    1         0              0}
\CommentTok{\#\textgreater{} 2   40.96409    2         0              0}
\CommentTok{\#\textgreater{} 3   55.07205    3         0              0}
\CommentTok{\#\textgreater{} 4   70.59040    4         0              0}
\CommentTok{\#\textgreater{} 5   84.64772    5         0              0}
\CommentTok{\#\textgreater{} 6   99.90750    6         0              0}
\CommentTok{\#\textgreater{} 7  114.88262    7         0              0}
\CommentTok{\#\textgreater{} 8  129.18464    8         0              0}
\CommentTok{\#\textgreater{} 9  145.03044    9         0              0}
\CommentTok{\#\textgreater{} 10 161.96847   10         0              0}
\end{Highlighting}
\end{Shaded}

Visualize

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{time, df}\SpecialCharTok{$}\NormalTok{outcome)}

\CommentTok{\# intervention date}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{v =}\NormalTok{ interven, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{)}

\CommentTok{\# regression line}
\NormalTok{ts }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(outcome }\SpecialCharTok{\textasciitilde{}}\NormalTok{ time }\SpecialCharTok{+}\NormalTok{ treatment }\SpecialCharTok{+}\NormalTok{ timesincetreat, }\AttributeTok{data =}\NormalTok{ df)}
\FunctionTok{lines}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{time, ts}\SpecialCharTok{$}\NormalTok{fitted.values, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{30-interrupted-time-series_files/figure-latex/unnamed-chunk-2-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(ts)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = outcome \textasciitilde{} time + treatment + timesincetreat, data = df)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}      Min       1Q   Median       3Q      Max }
\CommentTok{\#\textgreater{} {-}2.39154 {-}0.56425 {-}0.05071  0.65465  2.72381 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}                 Estimate Std. Error  t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)     9.881227   0.137363    71.94   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} time           14.999826   0.001185 12656.45   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} treatment      20.409779   0.203703   100.19   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} timesincetreat 24.997499   0.001976 12648.18   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 0.9677 on 361 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:      1,  Adjusted R{-}squared:      1 }
\CommentTok{\#\textgreater{} F{-}statistic: 1.018e+09 on 3 and 361 DF,  p{-}value: \textless{} 2.2e{-}16}
\end{Highlighting}
\end{Shaded}

Interpretation

\begin{itemize}
\item
  Time coefficient shows before-intervention outcome trend. Positive and significant, indicating a rising trend. Every day adds 15 points.
\item
  The treatment coefficient shows the \textbf{immediate} increase in outcome. \textbf{Immediate effect} is positive and significant, increasing outcome by 20 points.
\item
  The time since treatment coefficient reflects a change in trend subsequent to the intervention. The \textbf{sustained effect} is positive and statistically significant, showing that the outcome increases by 25 points per day after the intervention.
\end{itemize}

See \citet{lee2014graphical} for suggestions

Plot of counterfactual

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# treatment prediction}
\NormalTok{pred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(ts, df)}

\CommentTok{\# counterfactual dataset}
\NormalTok{new\_df }\OtherTok{\textless{}{-}}
    \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(}
        \AttributeTok{time =}\NormalTok{ time,}
        \CommentTok{\# treatment = 0 means counterfactual}
        \AttributeTok{treatment =} \FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, n),}
        \CommentTok{\# time since treatment = 0 means counterfactual}
        \AttributeTok{timesincetreat =} \FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{)}
\NormalTok{    ))}

\CommentTok{\# counterfactual predictions}
\NormalTok{pred\_cf }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(ts, new\_df)}

\CommentTok{\# plot}
\FunctionTok{plot}\NormalTok{(}
\NormalTok{    outcome,}
    \AttributeTok{col =} \FunctionTok{gray}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\FloatTok{0.2}\NormalTok{),}
    \AttributeTok{pch =} \DecValTok{19}\NormalTok{,}
    \AttributeTok{xlim  =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{365}\NormalTok{),}
    \AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{10000}\NormalTok{),}
    \AttributeTok{xlab =} \StringTok{"xlab"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"ylab"}
\NormalTok{)}

\CommentTok{\# regression line before treatment}
\FunctionTok{lines}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{interven), pred[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{interven], }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{3}\NormalTok{)}

\CommentTok{\# regression line after treatment}
\FunctionTok{lines}\NormalTok{(}\FunctionTok{rep}\NormalTok{((interven }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{n), pred[(interven }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{n], }
      \AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{3}\NormalTok{)}

\CommentTok{\# regression line after treatment (counterfactual)}
\FunctionTok{lines}\NormalTok{(}
    \FunctionTok{rep}\NormalTok{(interven}\SpecialCharTok{:}\NormalTok{n),}
\NormalTok{    pred\_cf[(interven)}\SpecialCharTok{:}\NormalTok{n],}
    \AttributeTok{col =} \StringTok{"yellow"}\NormalTok{,}
    \AttributeTok{lwd =} \DecValTok{3}\NormalTok{,}
    \AttributeTok{lty =} \DecValTok{5}
\NormalTok{)}

\FunctionTok{abline}\NormalTok{(}\AttributeTok{v =}\NormalTok{ interven, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{30-interrupted-time-series_files/figure-latex/unnamed-chunk-4-1} \end{center}

Possible threats to the validity of interrupted time series analysis \citep{baicker2019testing}

\begin{itemize}
\item
  Delayed effects \citep{rodgers2005did} (may have to make assess some time after the intervention - do not assess the immediate dates).
\item
  Other confounding events \citep[\citet{linden2017comprehensive}]{linden2016using}
\item
  Intervention is introduced but later withdrawn \citep{linden2015conducting}
\item
  \protect\hyperlink{autocorrelation}{Autocorrelation} (for every time series data): might cause underestimation in the standard errors (i.e., overestimating the statistical significance of the treatment effect)
\item
  Regression to the mean: after a the short-term shock to the outcome, individuals can revert back to their initial states.
\item
  Selection bias: only certain individuals are affected by the treatment (could use a \protect\hyperlink{multiple-groups}{Multiple Groups}).
\end{itemize}

\hypertarget{autocorrelation}{%
\section{Autocorrelation}\label{autocorrelation}}

Assess autocorrelation from residual

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# simple regression on time }
\NormalTok{simple\_ts }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(outcome }\SpecialCharTok{\textasciitilde{}}\NormalTok{ time, }\AttributeTok{data =}\NormalTok{ df)}

\FunctionTok{plot}\NormalTok{(}\FunctionTok{resid}\NormalTok{(simple\_ts))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{30-interrupted-time-series_files/figure-latex/unnamed-chunk-5-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# alternatively}
\FunctionTok{acf}\NormalTok{(}\FunctionTok{resid}\NormalTok{(simple\_ts))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{30-interrupted-time-series_files/figure-latex/unnamed-chunk-5-2} \end{center}

This is not the best example since I created this dataset. But when residuals do have autocorrelation, you should not see any patterns (i.e., points should be randomly distributed on the plot)

To formally test for autocorrelation, we can use the Durbin-Watson test

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lmtest}\SpecialCharTok{::}\FunctionTok{dwtest}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{outcome }\SpecialCharTok{\textasciitilde{}}\NormalTok{ df}\SpecialCharTok{$}\NormalTok{time)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Durbin{-}Watson test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  df$outcome \textasciitilde{} df$time}
\CommentTok{\#\textgreater{} DW = 0.00037532, p{-}value \textless{} 2.2e{-}16}
\CommentTok{\#\textgreater{} alternative hypothesis: true autocorrelation is greater than 0}
\end{Highlighting}
\end{Shaded}

From the p-value, we know that there is autocorrelation in the time series

A solution to this problem is to use more advanced time series analysis (e.g., ARIMA - coming up in the book) to adjust for seasonality and other dependency.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{forecast}\SpecialCharTok{::}\FunctionTok{auto.arima}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{outcome, }\AttributeTok{xreg =} \FunctionTok{as.matrix}\NormalTok{(df[,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]))}
\CommentTok{\#\textgreater{} Series: df$outcome }
\CommentTok{\#\textgreater{} Regression with ARIMA(0,0,1) errors }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}           ma1  intercept     time  treatment  timesincetreat}
\CommentTok{\#\textgreater{}       {-}0.0794     9.8817  14.9998    20.4170         24.9975}
\CommentTok{\#\textgreater{} s.e.   0.0533     0.1255   0.0011     0.1864          0.0018}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} sigma\^{}2 = 0.9332:  log likelihood = {-}502.78}
\CommentTok{\#\textgreater{} AIC=1017.56   AICc=1017.79   BIC=1040.95}
\end{Highlighting}
\end{Shaded}

\hypertarget{multiple-groups}{%
\section{Multiple Groups}\label{multiple-groups}}

When you suspect that you might have confounding events or selection bias, you can add a control group that did not experience the treatment (very much similar to \protect\hyperlink{difference-in-differences}{Difference-in-differences})

The model then becomes

\[
\begin{aligned}
Y = \beta_0 &+ \beta_1 time+ \beta_2 treatment +\beta_3 \times timesincetreat \\
&+\beta_4 group + \beta_5 group \times time + \beta_6 group \times treatment \\
&+ \beta_7 group \times timesincetreat
\end{aligned}
\]

where

\begin{itemize}
\item
  Group = 1 when the observation is under treatment and 0 under control
\item
  \(\beta_4\) = baseline difference between the treatment and control group
\item
  \(\beta_5\) = slope difference between the treatment and control group before treatment
\item
  \(\beta_6\) = baseline difference between the treatment and control group associated with the treatment.
\item
  \(\beta_7\) = difference between the sustained effect of the treatment and control group after the treatment.
\end{itemize}

\hypertarget{part-c.-other-concerns}{%
\part*{C. OTHER CONCERNS}\label{part-c.-other-concerns}}
\addcontentsline{toc}{part}{C. OTHER CONCERNS}

\hypertarget{endogeneity}{%
\chapter{Endogeneity}\label{endogeneity}}

Refresher

A general model framework

\[
\mathbf{Y = X \beta + \epsilon}
\]

where

\begin{itemize}
\item
  \(\mathbf{Y} = n \times 1\)
\item
  \(\mathbf{X} = n \times k\)
\item
  \(\beta = k \times 1\)
\item
  \(\epsilon = n \times 1\)
\end{itemize}

Then, OLS estimates of coefficients are

\[
\begin{aligned}
\hat{\beta}_{OLS} &= (\mathbf{X}'\mathbf{X})^{-1}(\mathbf{X}'\mathbf{Y}) \\
&= (\mathbf{X}'\mathbf{X})^{-1}(\mathbf{X}'(\mathbf{X \beta + \epsilon})) \\
&= (\mathbf{X}'\mathbf{X})^{-1} (\mathbf{X}'\mathbf{X}) \beta + (\mathbf{X}'\mathbf{X})^{-1} (\mathbf{X}'\mathbf{\epsilon}) \\
\hat{\beta}_{OLS} & \to \beta + (\mathbf{X}'\mathbf{X})^{-1} (\mathbf{X}'\mathbf{\epsilon})
\end{aligned}
\]

To have unbiased estimates, we have to get rid of the second part \((\mathbf{X}'\mathbf{X})^{-1} (\mathbf{X}'\mathbf{\epsilon})\)

There are 2 conditions to achieve unbiased estimates:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(E(\epsilon |X) = 0\) (This is easy, putting an intercept can solve this issue)
\item
  \(Cov(\mathbf{X}, \epsilon) = 0\) (This is the hard part)
\end{enumerate}

We only care about omitted variable

Usually, the problem will stem Omitted Variables Bias, but we only care about omitted variable bias when

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Omitted variables correlate with the variables we care about (\(X\)). If OMV does not correlate with \(X\), we don't care, and random assignment makes this correlation goes to 0)
\item
  Omitted variables correlates with outcome/ dependent variable
\end{enumerate}

There are more types of endogeneity listed below.

Types of endogeneity

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \protect\hyperlink{endogenous-treatment}{Endogenous Treatment}
\end{enumerate}

\begin{itemize}
\item
  Omitted Variables Bias

  \begin{itemize}
  \tightlist
  \item
    Motivation
  \item
    Ability/talent
  \item
    Self-selection
  \end{itemize}
\item
  Feedback Effect (\protect\hyperlink{simultaneity}{Simultaneity}): also known as bidirectionality
\item
  Reverse Causality: Subtle difference from \protect\hyperlink{simultaneity}{Simultaneity}: Technically, two variables affect each other sequentially, but in a big enough time frame, (e.g., monthly, or yearly), our coefficient will be biased just like simultaneity.
\item
  \protect\hyperlink{measurement-error}{Measurement Error}
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \protect\hyperlink{endogenous-sample-selection}{Endogenous Sample Selection}
\end{enumerate}

To deal with this problem, we have a toolbox (that has been mentioned in previous chapter \ref{causal-inference})

Using control variables in regression is a ``selection on observables'' identification strategy.

In other words, if you believe you have an omitted variable, and you can measure it, including it in the regression model solves your problem. These uninterested variables are called control variables in your model.

However, this is rarely the case (because the problem is we don't have their measurements). Hence, we need more elaborate methods:

\begin{itemize}
\item
  \protect\hyperlink{endogenous-treatment}{Endogenous Treatment}
\item
  \protect\hyperlink{endogenous-sample-selection}{Endogenous Sample Selection}
\end{itemize}

Before we get to methods that deal with bias arises from omitted variables, we consider cases where we do have measurements of a variable, but there is measurement error (bias).

\hypertarget{endogenous-treatment}{%
\section{Endogenous Treatment}\label{endogenous-treatment}}

\hypertarget{measurement-error}{%
\subsection{Measurement Error}\label{measurement-error}}

\begin{itemize}
\item
  Data error can stem from

  \begin{itemize}
  \item
    Coding errors
  \item
    Reporting errors
  \end{itemize}
\end{itemize}

Two forms of measurement error:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Random (stochastic) (indeterminate error) (\protect\hyperlink{classical-measurement-errors}{Classical Measurement Errors}): noise or measurement errors do not show up in a consistent or predictable way.
\item
  Systematic (determinate error) (\protect\hyperlink{non-classical-measurement-errors}{Non-classical Measurement Errors}): When measurement error is consistent and predictable across observations.

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Instrument errors (e.g., faulty scale) -\textgreater{} calibration or adjustment
  \item
    Method errors (e.g., sampling errors) -\textgreater{} better method development + study design
  \item
    Human errors (e.g., judgement)
  \end{enumerate}
\end{enumerate}

Usually the systematic measurement error is a bigger issue because it introduces ``bias'' into our estimates, while random error introduces noise into our estimates

\begin{itemize}
\tightlist
\item
  Noise -\textgreater{} regression estimate to 0
\item
  Bias -\textgreater{} can pull estimate to upward or downward.
\end{itemize}

\hypertarget{classical-measurement-errors}{%
\subsubsection{Classical Measurement Errors}\label{classical-measurement-errors}}

\hypertarget{right-hand-side}{%
\paragraph{Right-hand side}\label{right-hand-side}}

\begin{itemize}
\tightlist
\item
  Right-hand side measurement error: When the measurement is in the covariates, then we have the endogeneity problem.
\end{itemize}

Say you know the true model is

\[
Y_i = \beta_0 + \beta_1 X_i + u_i
\]

But you don't observe \(X_i\), but you observe

\[
\tilde{X}_i = X_i + e_i
\]

which is known as classical measurement errors where we \textbf{assume} \(e_i\) is uncorrelated with \(X_i\) (i.e., \(E(X_i e_i) = 0\))

Then, when you estimate your observed variables, you have (substitute \(X_i\) with \(\tilde{X}_i - e_i\) ):

\[
\begin{aligned}
Y_i &= \beta_0 + \beta_1 (\tilde{X}_i - e_i)+ u_i \\
&= \beta_0 + \beta_1 \tilde{X}_i + u_i - \beta_1 e_i \\
&= \beta_0 + \beta_1 \tilde{X}_i + v_i
\end{aligned}
\]

In words, the measurement error in \(X_i\) is now a part of the error term in the regression equation \(v_i\). Hence, we have an endogeneity bias.

Endogeneity arises when

\[
\begin{aligned}
E(\tilde{X}_i v_i) &= E((X_i + e_i )(u_i - \beta_1 e_i)) \\
&= -\beta_1 Var(e_i) \neq 0
\end{aligned}
\]

Since \(\tilde{X}_i\) and \(e_i\) are positively correlated, then it leads to

\begin{itemize}
\item
  a negative bias in \(\hat{\beta}_1\) if the true \(\beta_1\) is positive
\item
  a positive bias if \(\beta_1\) is negative
\end{itemize}

In other words, measurement errors cause \textbf{attenuation bias}, which inter turn pushes the coefficient towards 0

As \(Var(e_i)\) increases or \(\frac{Var(e_i)}{Var(\tilde{X})} \to 1\) then \(e_i\) is a random (noise) and \(\beta_1 \to 0\) (random variable \(\tilde{X}\) should not have any relation to \(Y_i\))

Technical note:

The size of the bias in the OLS-estimator is

\[
\hat{\beta}_{OLS} = \frac{ cov(\tilde{X}, Y)}{var(\tilde{X})} = \frac{cov(X + e, \beta X + u)}{var(X + e)}
\]

then

\[
plim \hat{\beta}_{OLS} = \beta \frac{\sigma^2_X}{\sigma^2_X + \sigma^2_e} = \beta \lambda
\]

where \(\lambda\) is \textbf{reliability} or signal-to-total variance ratio or attenuation factor

Reliability affect the extent to which measurement error attenuates \(\hat{\beta}\). The attenuation bias is

\[
\hat{\beta}_{OLS} - \beta = -(1-\lambda)\beta
\]

Thus, \(\hat{\beta}_{OLS} < \beta\) (unless \(\lambda = 1\), in which case we don't even have measurement error).

Note:

\textbf{Data transformation worsen (magnify) the measurement error}

\[
y= \beta x + \gamma x^2 + \epsilon
\]

then, the attenuation factor for \(\hat{\gamma}\) is the square of the attenuation factor for \(\hat{\beta}\) (i.e., \(\lambda_{\hat{\gamma}} = \lambda_{\hat{\beta}}^2\))

\textbf{Adding covariates increases attenuation bias}

To fix classical measurement error problem, we can

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Find estimates of either \(\sigma^2_X, \sigma^2_\epsilon\) or \(\lambda\) from validation studies, or survey data.
\item
  \protect\hyperlink{endogenous-treatment}{Endogenous Treatment} Use instrument \(Z\) correlated with \(X\) but uncorrelated with \(\epsilon\)
\item
  Abandon your project
\end{enumerate}

\hypertarget{left-hand-side}{%
\paragraph{Left-hand side}\label{left-hand-side}}

When the measurement is in the outcome variable, econometricians or causal scientists do not care because they still have an unbiased estimate of the coefficients (the zero conditional mean assumption is not violated, hence we don't have endogeneity). However, statisticians might care because it might inflate our uncertainty in the coefficient estimates (i.e., higher standard errors).

\[
\tilde{Y} = Y + v
\]

then the model you estimate is

\[
\tilde{Y} = \beta X + u + v
\]

Since \(v\) is uncorrelated with \(X\), then \(\hat{\beta}\) is consistently estimated by OLS

If we have measurement error in \(Y_i\), it will pass through \(\beta_1\) and go to \(u_i\)

\hypertarget{non-classical-measurement-errors}{%
\subsubsection{Non-classical Measurement Errors}\label{non-classical-measurement-errors}}

Relaxing the assumption that \(X\) and \(\epsilon\) are uncorrelated

Recall the true model we have true estimate is

\[
\hat{\beta} = \frac{cov(X + \epsilon, \beta X + u)}{var(X + \epsilon)}
\]

then without the above assumption, we have

\[
\begin{aligned}
plim \hat{\beta} &= \frac{\beta (\sigma^2_X + \sigma_{X \epsilon})}{\sigma^2_X + \sigma^2_\epsilon + 2 \sigma_{X \epsilon}} \\
&= (1 - \frac{\sigma^2_{\epsilon} + \sigma_{X \epsilon}}{\sigma^2_X + \sigma^2_\epsilon + 2 \sigma_{X \epsilon}}) \beta \\
&= (1 - b_{\epsilon \tilde{X}}) \beta
\end{aligned}
\]

where \(b_{\epsilon \tilde{X}}\) is the covariance between \(\tilde{X}\) and \(\epsilon\) (also the regression coefficient of a regression of \(\epsilon\) on \(\tilde{X}\))

Hence, the \protect\hyperlink{classical-measurement-errors}{Classical Measurement Errors} is just a special case of \protect\hyperlink{non-classical-measurement-errors}{Non-classical Measurement Errors} where \(b_{\epsilon \tilde{X}} = 1 - \lambda\)

So when \(\sigma_{X \epsilon} = 0\) (\protect\hyperlink{classical-measurement-errors}{Classical Measurement Errors}), increasing this covariance \(b_{\epsilon \tilde{X}}\) increases the covariance increases the attenuation factor if more than half of the variance in \(\tilde{X}\) is measurement error, and decreases the attenuation factor otherwise. This is also known as \textbf{mean reverting measurement error} \citep[\citet{bound2001measurement}]{bound1989measurement}

A general framework for both right-hand side and left-hand side measurement error is \citep{bound2001measurement}:

consider the true model

\[
\mathbf{Y = X \beta + \epsilon}
\]

then

\[
\begin{aligned}
\hat{\beta} &= \mathbf{(\tilde{X}' \tilde{X})^{-1}\tilde{X} \tilde{Y}} \\
&= \mathbf{(\tilde{X}' \tilde{X})^{-1} \tilde{X}' (\tilde{X} \beta - U \beta + v + \epsilon )} \\
&= \mathbf{\beta + (\tilde{X}' \tilde{X})^{-1} \tilde{X}' (-U \beta + v + \epsilon)} \\
plim \hat{\beta} &= \beta + plim (\tilde{X}' \tilde{X})^{-1} \tilde{X}' ( -U\beta + v) \\
&= \beta + plim (\tilde{X}' \tilde{X})^{-1} \tilde{X}' W 
\left[
\begin{array}
{c}
- \beta \\
1
\end{array}
\right]
\end{aligned}
\]

Since we collect the measurement errors in a matrix \(W = [U|v]\), then

\[
( -U\beta + v) = W 
\left[
\begin{array}
{c}
- \beta \\
1
\end{array}
\right]
\]

Hence, in general, biases in the coefficients \(\beta\) are regression coefficients from regressing the measurement errors on the mis-measured \(\tilde{X}\)

Notes:

\begin{itemize}
\item
  \protect\hyperlink{instrumental-variable}{Instrumental Variable} can help fix this problem
\item
  There can also be measurement error in dummy variables and you can still use \protect\hyperlink{instrumental-variable}{Instrumental Variable} to fix it.
\end{itemize}

\hypertarget{solution-to-measurement-errors}{%
\subsubsection{Solution to Measurement Errors}\label{solution-to-measurement-errors}}

\hypertarget{correlation}{%
\paragraph{Correlation}\label{correlation}}

\[
\begin{aligned}
P(\rho | data) &= \frac{P(data|\rho)P(\rho)}{P(data)} \\
\text{Posterior Probability} &\propto \text{Likelihood} \times \text{Prior Probability}
\end{aligned}
\] where

\begin{itemize}
\tightlist
\item
  \(\rho\) is a correlation coefficient
\item
  \(P(data|\rho)\) is the likelihood function evaluated at \(\rho\)
\item
  \(P(\rho)\) prior probability
\item
  \(P(data)\) is the normalizing constant
\end{itemize}

With sample correlation coefficient \(r\):

\[
r = \frac{S_{xy}}{\sqrt{S_{xx}S_{yy}}}
\] Then the posterior density approximation of \(\rho\) is \citep[pp.3]{schisterman2003estimation}

\[
P(\rho| x, y)  \propto P(\rho) \frac{(1- \rho^2)^{(n-1)/2}}{(1- \rho \times r)^{n - (3/2)}}
\]

where

\begin{itemize}
\tightlist
\item
  \(\rho = \tanh \xi\) where \(\xi \sim N(z, 1/n)\)
\item
  \(r = \tanh z\)
\end{itemize}

Then the posterior density follow a normal distribution where

\textbf{Mean}

\[
\mu_{posterior} = \sigma^2_{posterior} \times (n_{prior} \times \tanh^{-1} r_{prior}+ n_{likelihood} \times \tanh^{-1} r_{likelihood})
\]

\textbf{variance}

\[
\sigma^2_{posterior} = \frac{1}{n_{prior} + n_{Likelihood}}
\]

To simplify the integration process, we choose prior that is

\[
P(\rho) \propto (1 - \rho^2)^c
\] where

\begin{itemize}
\tightlist
\item
  \(c\) is the weight the prior will have in estimation (i.e., \(c = 0\) if no prior info, hence \(P(\rho) \propto 1\))
\end{itemize}

Example:

Current study: \(r_{xy} = 0.5, n = 200\)

Previous study: \(r_{xy} = 0.2765, (n=50205)\)

Combining two, we have the posterior following a normal distribution with the \textbf{variance} of

\[
\sigma^2_{posterior} =  \frac{1}{n_{prior} + n_{Likelihood}} = \frac{1}{200 + 50205} = 0.0000198393
\]

\textbf{Mean}

\[
\begin{aligned}
\mu_{Posterior} &= \sigma^2_{Posterior}  \times (n_{prior} \times \tanh^{-1} r_{prior}+ n_{likelihood} \times \tanh^{-1} r_{likelihood}) \\
&= 0.0000198393 \times (50205 \times \tanh^{-1} 0.2765 + 200 \times \tanh^{-1}0.5 )\\
&= 0.2849415
\end{aligned}
\]

Hence, \(Posterior \sim N(0.691, 0.0009)\), which means the correlation coefficient is \(\tanh(0.691) = 0.598\) and 95\% CI is

\[
\mu_{posterior} \pm 1.96 \times \sqrt{\sigma^2_{Posterior}} = 0.2849415 \pm 1.96 \times (0.0000198393)^{1/2} = (0.2762115, 0.2936714)
\]

Hence, the interval for posterior \(\rho\) is \((0.2693952, 0.2855105)\)

If future authors suspect that they have

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Large sampling variation
\item
  Measurement error in either measures in the correlation, which attenuates the relationship between the two variables
\end{enumerate}

Applying this Bayesian correction can give them a better estimate of the correlation between the two.

To implement this calculation in R, see below

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n\_new              }\OtherTok{\textless{}{-}} \DecValTok{200}
\NormalTok{r\_new              }\OtherTok{\textless{}{-}} \FloatTok{0.5}
\NormalTok{alpha              }\OtherTok{\textless{}{-}} \FloatTok{0.05}

\NormalTok{update\_correlation }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(n\_new, r\_new, alpha) \{}
\NormalTok{    n\_meta             }\OtherTok{\textless{}{-}} \DecValTok{50205}
\NormalTok{    r\_meta             }\OtherTok{\textless{}{-}} \FloatTok{0.2765}
    
    \CommentTok{\# Variance}
\NormalTok{    var\_xi         }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{/}\NormalTok{ (n\_new }\SpecialCharTok{+}\NormalTok{ n\_meta)}
    \FunctionTok{format}\NormalTok{(var\_xi, }\AttributeTok{scientific =} \ConstantTok{FALSE}\NormalTok{)}
    
    \CommentTok{\# mean}
\NormalTok{    mu\_xi          }\OtherTok{\textless{}{-}}\NormalTok{ var\_xi }\SpecialCharTok{*}\NormalTok{ (n\_meta }\SpecialCharTok{*} \FunctionTok{atanh}\NormalTok{(r\_meta) }\SpecialCharTok{+}\NormalTok{ n\_new }\SpecialCharTok{*}\NormalTok{ (}\FunctionTok{atanh}\NormalTok{(r\_new)))}
    \FunctionTok{format}\NormalTok{(mu\_xi, }\AttributeTok{scientific  =} \ConstantTok{FALSE}\NormalTok{)}
    
    \CommentTok{\# confidence interval}
\NormalTok{    upper\_xi       }\OtherTok{\textless{}{-}}\NormalTok{ mu\_xi }\SpecialCharTok{+} \FunctionTok{qnorm}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alpha }\SpecialCharTok{/} \DecValTok{2}\NormalTok{) }\SpecialCharTok{*} \FunctionTok{sqrt}\NormalTok{(var\_xi)}
\NormalTok{    lower\_xi       }\OtherTok{\textless{}{-}}\NormalTok{ mu\_xi }\SpecialCharTok{{-}} \FunctionTok{qnorm}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alpha }\SpecialCharTok{/} \DecValTok{2}\NormalTok{) }\SpecialCharTok{*} \FunctionTok{sqrt}\NormalTok{(var\_xi)}
    
    \CommentTok{\# rho}
\NormalTok{    mean\_rho       }\OtherTok{\textless{}{-}} \FunctionTok{tanh}\NormalTok{(mu\_xi)}
\NormalTok{    upper\_rho      }\OtherTok{\textless{}{-}} \FunctionTok{tanh}\NormalTok{(upper\_xi)}
\NormalTok{    lower\_rho      }\OtherTok{\textless{}{-}} \FunctionTok{tanh}\NormalTok{(lower\_xi)}
    
    \CommentTok{\# return a list}
    \FunctionTok{return}\NormalTok{(}
        \FunctionTok{list}\NormalTok{(}
            \StringTok{"mu\_xi"} \OtherTok{=}\NormalTok{ mu\_xi,}
            \StringTok{"var\_xi"} \OtherTok{=}\NormalTok{ var\_xi,}
            \StringTok{"upper\_xi"} \OtherTok{=}\NormalTok{ upper\_xi,}
            \StringTok{"lower\_xi"} \OtherTok{=}\NormalTok{ lower\_xi,}
            \StringTok{"mean\_rho"} \OtherTok{=}\NormalTok{ mean\_rho,}
            \StringTok{"upper\_rho"} \OtherTok{=}\NormalTok{ upper\_rho,}
            \StringTok{"lower\_rho"} \OtherTok{=}\NormalTok{ lower\_rho}
\NormalTok{        )}
\NormalTok{    )}
\NormalTok{\}}




\CommentTok{\# Old confidence interval}
\NormalTok{r\_new }\SpecialCharTok{+} \FunctionTok{qnorm}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alpha }\SpecialCharTok{/} \DecValTok{2}\NormalTok{) }\SpecialCharTok{*} \FunctionTok{sqrt}\NormalTok{(}\DecValTok{1}\SpecialCharTok{/}\NormalTok{n\_new)}
\CommentTok{\#\textgreater{} [1] 0.6385904}
\NormalTok{r\_new }\SpecialCharTok{{-}} \FunctionTok{qnorm}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alpha }\SpecialCharTok{/} \DecValTok{2}\NormalTok{) }\SpecialCharTok{*} \FunctionTok{sqrt}\NormalTok{(}\DecValTok{1}\SpecialCharTok{/}\NormalTok{n\_new)}
\CommentTok{\#\textgreater{} [1] 0.3614096}

\NormalTok{testing }\OtherTok{=} \FunctionTok{update\_correlation}\NormalTok{(}\AttributeTok{n\_new =}\NormalTok{ n\_new, }\AttributeTok{r\_new =}\NormalTok{ r\_new, }\AttributeTok{alpha =}\NormalTok{ alpha)}

\CommentTok{\# Updated rho}
\NormalTok{testing}\SpecialCharTok{$}\NormalTok{mean\_rho}
\CommentTok{\#\textgreater{} [1] 0.2774723}

\CommentTok{\# Updated confidence interval}
\NormalTok{testing}\SpecialCharTok{$}\NormalTok{upper\_rho}
\CommentTok{\#\textgreater{} [1] 0.2855105}
\NormalTok{testing}\SpecialCharTok{$}\NormalTok{lower\_rho}
\CommentTok{\#\textgreater{} [1] 0.2693952}
\end{Highlighting}
\end{Shaded}

\hypertarget{simultaneity}{%
\subsection{Simultaneity}\label{simultaneity}}

\begin{itemize}
\item
  When independent variables (\(X\)'s) are jointly determined with the dependent variable \(Y\), typically through an equilibrium mechanism, violates the second condition for causality (i.e., temporal order).
\item
  Examples: quantity and price by demand and supply, investment and productivity, sales and advertisement
\end{itemize}

General Simultaneous (Structural) Equations

\[
\begin{aligned}
Y_i &= \beta_0 + \beta_1 X_i + u_i \\
X_i &= \alpha_0 + \alpha_1 Y_i + v_i
\end{aligned}
\]

Hence, the solutions are

\[
\begin{aligned}
Y_i &= \frac{\beta_0 + \beta_1 \alpha_0}{1 - \alpha_1 \beta_1} + \frac{\beta_1 v_i + u_i}{1 - \alpha_1 \beta_1} \\
X_i &= \frac{\alpha_0 + \alpha_1 \beta_0}{1 - \alpha_1 \beta_1} + \frac{v_i + \alpha_1 u_i}{1 - \alpha_1 \beta_1}
\end{aligned}
\]

If we run only one regression, we will have biased estimators (because of \textbf{simultaneity bias}):

\[
\begin{aligned}
Cov(X_i, u_i) &= Cov(\frac{v_i + \alpha_1 u_i}{1 - \alpha_1 \beta_1}, u_i) \\
&= \frac{\alpha_1}{1- \alpha_1 \beta_1} Var(u_i)
\end{aligned}
\]

In an even more general model

\[
\begin{cases}
Y_i = \beta_0 + \beta_1 X_i + \beta_2 T_i + u_i \\
X_i = \alpha_0 + \alpha_1 Y_i + \alpha_2 Z_i + v_i
\end{cases}
\]

where

\begin{itemize}
\item
  \(X_i, Y_i\) are \textbf{endogenous} variables determined within the system
\item
  \(T_i, Z_i\) are \textbf{exogenous} variables
\end{itemize}

Then, the reduced form of the model is

\[
\begin{cases}
\begin{aligned}
Y_i &= \frac{\beta_0 + \beta_1 \alpha_0}{1 - \alpha_1 \beta_1} + \frac{\beta_1 \alpha_2}{1 - \alpha_1 \beta_1} Z_i + \frac{\beta_2}{1 - \alpha_1 \beta_1} T_i + \tilde{u}_i \\
&= B_0 + B_1 Z_i + B_2 T_i + \tilde{u}_i
\end{aligned}
\\
\begin{aligned}
X_i &= \frac{\alpha_0 + \alpha_1 \beta_0}{1 - \alpha_1 \beta_1} + \frac{\alpha_2}{1 - \alpha_1 \beta_1} Z_i + \frac{\alpha_1\beta_2}{1 - \alpha_1 \beta_1} T_i + \tilde{v}_i \\
&= A_0 + A_1 Z_i + A_2 T_i + \tilde{v}_i
\end{aligned}
\end{cases}
\]

Then, now we can get consistent estimates of the reduced form parameters

And to get the original parameter estimates

\[
\begin{aligned}
\frac{B_1}{A_1} &= \beta_1 \\
B_2 (1 - \frac{B_1 A_2}{A_1B_2}) &= \beta_2 \\
\frac{A_2}{B_2} &= \alpha_1 \\
A_1 (1 - \frac{B_1 A_2}{A_1 B_2}) &= \alpha_2
\end{aligned}
\]

Rules for Identification

\textbf{Order Condition} (necessary but not sufficient)

\[
K - k \ge m - 1
\]

where

\begin{itemize}
\item
  \(M\) = number of endogenous variables in the model
\item
  K = number of exogenous variables int he model
\item
  \(m\) = number of endogenous variables in a given
\item
  \(k\) = is the number of exogenous variables in a given equation
\end{itemize}

This is actually the general framework for instrumental variables

\hypertarget{endogenous-treatment-solutions}{%
\subsection{Endogenous Treatment Solutions}\label{endogenous-treatment-solutions}}

Using the OLS estimates as a reference point

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(AER)}
\FunctionTok{library}\NormalTok{(REndo)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{421}\NormalTok{)}
\FunctionTok{data}\NormalTok{(}\StringTok{"CASchools"}\NormalTok{)}
\NormalTok{school }\OtherTok{\textless{}{-}}\NormalTok{ CASchools}
\NormalTok{school}\SpecialCharTok{$}\NormalTok{stratio }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(CASchools, students }\SpecialCharTok{/}\NormalTok{ teachers)}
\NormalTok{m1.ols }\OtherTok{\textless{}{-}}
    \FunctionTok{lm}\NormalTok{(read }\SpecialCharTok{\textasciitilde{}}\NormalTok{ stratio }\SpecialCharTok{+}\NormalTok{ english }\SpecialCharTok{+}\NormalTok{ lunch }
       \SpecialCharTok{+}\NormalTok{ grades }\SpecialCharTok{+}\NormalTok{ income }\SpecialCharTok{+}\NormalTok{ calworks }\SpecialCharTok{+}\NormalTok{ county,}
       \AttributeTok{data =}\NormalTok{ school)}
\FunctionTok{summary}\NormalTok{(m1.ols)}\SpecialCharTok{$}\NormalTok{coefficients[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{7}\NormalTok{,]}
\CommentTok{\#\textgreater{}                 Estimate Std. Error     t value      Pr(\textgreater{}|t|)}
\CommentTok{\#\textgreater{} (Intercept) 683.45305948 9.56214469  71.4748711 3.011667e{-}218}
\CommentTok{\#\textgreater{} stratio      {-}0.30035544 0.25797023  {-}1.1643027  2.450536e{-}01}
\CommentTok{\#\textgreater{} english      {-}0.20550107 0.03765408  {-}5.4576041  8.871666e{-}08}
\CommentTok{\#\textgreater{} lunch        {-}0.38684059 0.03700982 {-}10.4523759  1.427370e{-}22}
\CommentTok{\#\textgreater{} gradesKK{-}08  {-}1.91291321 1.35865394  {-}1.4079474  1.599886e{-}01}
\CommentTok{\#\textgreater{} income        0.71615378 0.09832843   7.2832829  1.986712e{-}12}
\CommentTok{\#\textgreater{} calworks     {-}0.05273312 0.06154758  {-}0.8567863  3.921191e{-}01}
\end{Highlighting}
\end{Shaded}

\hypertarget{instrumental-variable}{%
\subsubsection{Instrumental Variable}\label{instrumental-variable}}

\protect\hyperlink{a3a}{A3a} requires \(\epsilon_i\) to be uncorrelated with \(\mathbf{x}_i\)

Assume \protect\hyperlink{a1-linearity}{A1} , \protect\hyperlink{a2-full-rank}{A2}, \protect\hyperlink{a5-data-generation-random-sampling}{A5}

\[
plim(\hat{\beta}_{OLS}) = \beta + [E(\mathbf{x_i'x_i})]^{-1}E(\mathbf{x_i'}\epsilon_i)
\]

\protect\hyperlink{a3a}{A3a} is the weakest assumption needed for OLS to be \textbf{consistent}

\protect\hyperlink{a3-exogeneity-of-independent-variables}{A3} fails when \(x_{ik}\) is correlated with \(\epsilon_i\)

\begin{itemize}
\tightlist
\item
  Omitted Variables Bias: \(\epsilon_i\) includes any other factors that may influence the dependent variable (linearly)
\item
  \protect\hyperlink{simultaneity}{Simultaneity} Demand and prices are simultaneously determined.
\item
  \protect\hyperlink{endogenous-sample-selection}{Endogenous Sample Selection} we did not have iid sample
\item
  \protect\hyperlink{measurement-error}{Measurement Error}
\end{itemize}

\textbf{Note}

\begin{itemize}
\tightlist
\item
  Omitted Variable: an omitted variable is a variable, omitted from the model (but is in the \(\epsilon_i\)) and unobserved has predictive power towards the outcome.
\item
  Omitted Variable Bias: is the bias (and inconsistency when looking at large sample properties) of the OLS estimator when the omitted variable.
\item
  We cam have both positive and negative selection bias (it depends on what our story is)
\end{itemize}

The \textbf{structural equation} is used to emphasize that we are interested understanding a \textbf{causal relationship}

\[
y_{i1} = \beta_0 + \mathbf{z}_i1 \beta_1 + y_{i2}\beta_2 +  \epsilon_i
\]

where

\begin{itemize}
\tightlist
\item
  \(y_{it}\) is the outcome variable (inherently correlated with \(\epsilon_i\))
\item
  \(y_{i2}\) is the endogenous covariate (presumed to be correlated with \(\epsilon_i\))
\item
  \(\beta_1\) represents the causal effect of \(y_{i2}\) on \(y_{i1}\)
\item
  \(\mathbf{z}_{i1}\) is exogenous controls (uncorrelated with \(\epsilon_i\)) (\(E(z_{1i}'\epsilon_i) = 0\))
\end{itemize}

OLS is an inconsistent estimator of the causal effect \(\beta_2\)

If there was no endogeneity

\begin{itemize}
\tightlist
\item
  \(E(y_{i2}'\epsilon_i) = 0\)
\item
  the exogenous variation in \(y_{i2}\) is what identifies the causal effect
\end{itemize}

If there is endogeneity

\begin{itemize}
\tightlist
\item
  Any wiggle in \(y_{i2}\) will shift simultaneously with \(\epsilon_i\)
\end{itemize}

\[
plim(\hat{\beta}_{OLS}) = \beta + [E(\mathbf{x'_ix_i})]^{-1}E(\mathbf{x'_i}\epsilon_i)
\]

where

\begin{itemize}
\tightlist
\item
  \(\beta\) is the causal effect
\item
  \([E(\mathbf{x'_ix_i})]^{-1}E(\mathbf{x'_i}\epsilon_i)\) is the endogenous effect
\end{itemize}

Hence \(\hat{\beta}_{OLS}\) can be either more positive and negative than the true causal effect.

Motivation for \textbf{Two Stage Least Squares (2SLS)}

\[
y_{i1}=\beta_0 + \mathbf{z}_{i1}\beta_1 + y_{i2}\beta_2 + \epsilon_i
\]

We want to understand how movement in \(y_{i2}\) effects movement in \(y_{i1}\), but whenever we move \(y_{i2}\), \(\epsilon_i\) also moves.

\textbf{Solution}\\
We need a way to move \(y_{i2}\) independently of \(\epsilon_i\), then we can analyze the response in \(y_{i1}\) as a causal effect

\begin{itemize}
\item
  Find an \textbf{instrumental variable(s)} \(z_{i2}\)

  \begin{itemize}
  \tightlist
  \item
    Instrument \textbf{Relevance}: when** \(z_{i2}\) moves then \(y_{i2}\) also moves
  \item
    Instrument \textbf{Exogeneity}: when \(z_{i2}\) moves then \(\epsilon_i\) does not move.
  \end{itemize}
\item
  \(z_{i2}\) is the \textbf{exogenous variation that identifies} the causal effect \(\beta_2\)
\end{itemize}

Finding an Instrumental variable:

\begin{itemize}
\tightlist
\item
  Random Assignment: + Effect of class size on educational outcomes: instrument is initial random
\item
  Relation's Choice + Effect of Education on Fertility: instrument is parent's educational level
\item
  Eligibility + Trade-off between IRA and 401K retirement savings: instrument is 401k eligibility
\end{itemize}

\textbf{Example}

Return to College

\begin{itemize}
\item
  education is correlated with ability - endogenous
\item
  \textbf{Near 4year} as an instrument

  \begin{itemize}
  \tightlist
  \item
    Instrument Relevance: when \textbf{near} moves then education also moves
  \item
    Instrument Exogeneity: when \textbf{near} moves then \(\epsilon_i\) does not move.
  \end{itemize}
\item
  Other potential instruments; near a 2-year college. Parent's Education. Owning Library Card
\end{itemize}

\[
y_{i1}=\beta_0 + \mathbf{z}_{i1}\beta_1 + y_{i2}\beta_2 + \epsilon_i
\]

First Stage (Reduced Form) Equation:

\[
y_{i2} = \pi_0 + \mathbf{z_{i1}\pi_1} + \mathbf{z_{i2}\pi_2} + v_i
\]

where

\begin{itemize}
\tightlist
\item
  \(\pi_0 + \mathbf{z_{i1}\pi_1} + \mathbf{z_{i2}\pi_2}\) is exogenous variation \(v_i\) is endogenous variation
\end{itemize}

This is called a \textbf{reduced form equation}

\begin{itemize}
\item
  Not interested in the causal interpretation of \(\pi_1\) or \(\pi_2\)
\item
  A linear projection of \(z_{i1}\) and \(z_{i2}\) on \(y_{i2}\) (simple correlations)
\item
  The projections \(\pi_1\) and \(\pi_2\) guarantee that \(E(z_{i1}'v_i)=0\) and \(E(z_{i2}'v_i)=0\)
\end{itemize}

Instrumental variable \(z_{i2}\)

\begin{itemize}
\tightlist
\item
  \textbf{Instrument Relevance}: \(\pi_2 \neq 0\)
\item
  \textbf{Instrument Exogeneity}: \(E(\mathbf{z_{i2}\epsilon_i})=0\)
\end{itemize}

Moving only the exogenous part of \(y_i2\) is moving

\[
\tilde{y}_{i2} = \pi_0 + \mathbf{z_{i1}\pi_1 + z_{i2}\pi_2}
\]

\textbf{two Stage Least Squares (2SLS)}

\[
y_{i1} = \beta_0 +\mathbf{z_{i1}\beta_1}+ y_{i2}\beta_2 + \epsilon_i
\]

\[
y_{i2} = \pi_0 + \mathbf{z_{i2}\pi_2} + \mathbf{v_i}
\]

Equivalently,

\begin{equation}
\begin{split}
y_{i1} = \beta_0 + \mathbf{z_{i1}}\beta_1 + \tilde{y}_{i2}\beta_2 + u_i
\end{split}
\label{eq:2SLS}
\end{equation}

where

\begin{itemize}
\tightlist
\item
  \(\tilde{y}_{i2} =\pi_0 + \mathbf{z_{i2}\pi_2}\)
\item
  \(u_i = v_i \beta_2+ \epsilon_i\)
\end{itemize}

The \eqref{eq:2SLS} holds for \protect\hyperlink{a1-linearity}{A1}, \protect\hyperlink{a5-data-generation-random-sampling}{A5}

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{a2-full-rank}{A2} holds if the instrument is relevant \(\pi_2 \neq 0\) + \(y_{i1} = \beta_0 + \mathbf{z_{i1}\beta_1 + (\pi_0 + z_{i1}\pi_1 + z_{i2}\pi_2)}\beta_2 + u_i\)
\item
  \protect\hyperlink{a3a}{A3a} holds if the instrument is exogenous \(E(\mathbf{z}_{i2}\epsilon_i)=0\)
\end{itemize}

\[
\begin{aligned}
E(\tilde{y}_{i2}'u_i) &= E((\pi_0 + \mathbf{z_{i1}\pi_1+z_{i2}})(v_i\beta_2 + \epsilon_i)) \\
&= E((\pi_0 + \mathbf{z_{i1}\pi_1+z_{i2}})( \epsilon_i)) \\
&= E(\epsilon_i)\pi_0 + E(\epsilon_iz_{i1})\pi_1 + E(\epsilon_iz_{i2}) \\
&=0 
\end{aligned}
\]

Hence, \eqref{eq:2SLS} is consistent

The 2SLS Estimator\\
1. Estimate the first stage using \protect\hyperlink{ordinary-least-squares}{OLS}

\[
y_{i2} = \pi_0 + \mathbf{z_{i2}\pi_2} + \mathbf{v_i}
\]

and obtained estimated value \(\hat{y}_{i2}\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Estimate the altered equation using \protect\hyperlink{ordinary-least-squares}{OLS}
\end{enumerate}

\[
y_{i1} = \beta_0 +\mathbf{z_{i1}\beta_1}+ \hat{y}_{i2}\beta_2 + \epsilon_i
\]

\textbf{Properties of the 2SLS Estimator}

\begin{itemize}
\tightlist
\item
  Under \protect\hyperlink{a1-linearity}{A1}, \protect\hyperlink{a2-full-rank}{A2}, \protect\hyperlink{a3a}{A3a} (for \(z_{i1}\)), \protect\hyperlink{a5-data-generation-random-sampling}{A5} and if the instrument satisfies the following two conditions, + \textbf{Instrument Relevance}: \(\pi_2 \neq 0\) + \textbf{Instrument Exogeneity}: \(E(\mathbf{z}_{i2}'\epsilon_i) = 0\) then the 2SLS estimator is consistent
\item
  Can handle more than one endogenous variable and more than one instrumental variable
\end{itemize}

\[
\begin{aligned}
y_{i1} &= \beta_0 + z_{i1}\beta_1 + y_{i2}\beta_2 + y_{i3}\beta_3 + \epsilon_i \\
y_{i2} &= \pi_0 + z_{i1}\pi_1 + z_{i2}\pi_2 + z_{i3}\pi_3 + z_{i4}\pi_4 + v_{i2} \\
y_{i3} &= \gamma_0 + z_{i1}\gamma_1 + z_{i2}\gamma_2 + z_{i3}\gamma_3 + z_{i4}\gamma_4 + v_{i3}
\end{aligned}
\]

\begin{verbatim}
    + **IV estimator**: one endogenous variable with a single instrument 
    + **2SLS estimator**: one endogenous variable with multiple instruments 
    + **GMM estimator**: multiple endogenous variables with multiple instruments
    
\end{verbatim}

\begin{itemize}
\item
  Standard errors produced in the second step are not correct

  \begin{itemize}
  \tightlist
  \item
    Because we do not know \(\tilde{y}\) perfectly and need to estimate it in the firs step, we are introducing additional variation
  \item
    We did not have this problem with \protect\hyperlink{feasible-generalized-least-squares}{FGLS} because ``the first stage was orthogonal to the second stage.'' This is generally not true for most multi-step procedure.\\
  \item
    If \protect\hyperlink{a4-homoskedasticity}{A4} does not hold, need to report robust standard errors.
  \end{itemize}
\item
  2SLS is less efficient than OLS and will always have larger standard errors.\\

  \begin{itemize}
  \tightlist
  \item
    First, \(Var(u_i) = Var(v_i\beta_2 + \epsilon_i) > Var(\epsilon_i)\)\\
  \item
    Second, \(\hat{y}_{i2}\) is generally highly collinear with \(\mathbf{z}_{i1}\)
  \end{itemize}
\item
  The number of instruments need to be at least as many or more the number of endogenous variables.
\end{itemize}

\textbf{Note}

\begin{itemize}
\tightlist
\item
  2SLS can be combined with \protect\hyperlink{feasible-generalized-least-squares}{FGLS} to make the estimator more efficient: You have the same first-stage, and in the second-stage, instead of using OLS, you can use FLGS with the weight matrix \(\hat{w}\)
\item
  Generalized Method of Moments can be more efficient than 2SLS.
\item
  In the second-stage of 2SLS, you can also use \protect\hyperlink{maximum-likelihood-regression}{MLE}, but then you are making assumption on the distribution of the outcome variable, the endogenous variable, and their relationship (joint distribution).
\end{itemize}

\hypertarget{testing-assumptions}{%
\paragraph{Testing Assumptions}\label{testing-assumptions}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \protect\hyperlink{test-of-endogeneity}{Test of Endogeneity}: Is \(y_{i2}\) truly endogenous (i.e., can we just use OLS instead of 2SLS)?
\item
  \protect\hyperlink{testing-instruments-assumptions}{Testing Instrument's assumptions}

  \begin{itemize}
  \item
    \protect\hyperlink{exogeneity}{Exogeneity} (Cannot always test ``and when you can it might not be informative'')
  \item
    \protect\hyperlink{relevancy}{Relevancy} (need to avoid ``weak instruments'')
  \end{itemize}
\end{enumerate}

\hypertarget{test-of-endogeneity}{%
\subparagraph{Test of Endogeneity}\label{test-of-endogeneity}}

\begin{itemize}
\item
  2SLS is generally so inefficient that we may prefer OLS if there is not much endogeneity
\item
  Biased but inefficient vs efficient but biased
\item
  Want a sense of ``how endogenous'' \(y_{i2}\) is

  \begin{itemize}
  \tightlist
  \item
    if ``very'' endogenous - should use 2SLS
  \item
    if not ``very'' endogenous - perhaps prefer OLS
  \end{itemize}
\end{itemize}

\textbf{Invalid} Test of Endogeneity: \(y_{i2}\) is endogenous if it is correlated with \(\epsilon_i\),

\[
\epsilon_i = \gamma_0 + y_{i2}\gamma_1 + error_i
\]

where \(\gamma_1 \neq 0\) implies that there is endogeneity

\begin{itemize}
\tightlist
\item
  \(\epsilon_i\) is not observed, but using the residuals
\end{itemize}

\[
e_i = \gamma_0 + y_{i2}\gamma_1 + error_i
\]

is \textbf{NOT} a valid test of endogeneity + The OLS residual, e is mechanically uncorrelated with \(y_{i2}\) (by FOC for OLS) + In every situation, \(\gamma_1\) will be essentially 0 and you will never be able to reject the null of no endogeneity

\textbf{Valid} test of endogeneity

\begin{itemize}
\tightlist
\item
  If \(y_{i2}\) is not endogenous then \(\epsilon_i\) and v are uncorrelated
\end{itemize}

\[
\begin{aligned}
y_{i1} &= \beta_0 + \mathbf{z}_{i1}\beta_1 + y_{i2}\beta_2 + \epsilon_i \\
y_{i2} &= \pi_0 + \mathbf{z}_{i1}\pi_1 + z_{i2}\pi_2 + v_i
\end{aligned}
\]

\textbf{variable Addition test}: include the first stage residuals as an additional variable,

\[
y_{i1} = \beta_0 + \mathbf{z}_{i1}\beta_1 + y_{i2}\beta_2 + \hat{v}_i \theta + error_i
\]

Then the usual \(t\)-test of significance is a valid test to evaluate the following hypothesis. \textbf{note} this test requires your instrument to be valid instrument.

\[
\begin{aligned}
&H_0: \theta = 0 & \text{  (not endogenous)} \\
&H_1: \theta \neq 0 & \text{  (endogenous)}
\end{aligned}
\]

\hypertarget{testing-instruments-assumptions}{%
\subparagraph{Testing Instrument's assumptions}\label{testing-instruments-assumptions}}

The instrumental variable must satisfy

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \protect\hyperlink{exogeneity}{Exogeneity} (Cannot always test ``and when you can it might not be informative'')
\item
  \protect\hyperlink{relevancy}{Relevancy} (need to avoid ``weak instruments'')
\end{enumerate}

\hypertarget{exogeneity}{%
\subparagraph{Exogeneity}\label{exogeneity}}

Why exogeneity matter?

\[
E(\mathbf{z}_{i2}'\epsilon_i) = 0
\]

\begin{itemize}
\tightlist
\item
  If \protect\hyperlink{a3a}{A3a} fails - 2SLS is also inconsistent
\item
  If instrument is not exogenous, then we need to find a new one.
\item
  Similar to \protect\hyperlink{test-of-endogeneity}{Test of Endogeneity}, when there is a single instrument
\end{itemize}

\[
\begin{aligned}
e_i &= \gamma_0 + \mathbf{z}_{i2}\gamma_1 + error_i \\
H_0: \gamma_1 &= 0
\end{aligned}
\]

is \textbf{NOT} a valid test of endogeneity

\begin{itemize}
\tightlist
\item
  the OLS residual, e is mechanically uncorrelated with \(z_{i2}\): \(\hat{\gamma}_1\) will be essentially 0 and you will never be able to determine if the instrument is endogenous.
\end{itemize}

\textbf{Solution}

Testing Instrumental Exogeneity in an Over-identified Model

\begin{itemize}
\item
  When there is more than one exogenous instrument (per endogenous variable), we can test for instrument exogeneity.

  \begin{itemize}
  \item
    When we have multiple instruments, the model is said to be over-identified.
  \item
    Could estimate the same model several ways (i.e., can identify/ estimate \(\beta_1\) more than one way)
  \end{itemize}
\item
  Idea behind the test: if the controls and instruments are truly exogenous then OLS estimation of the following regression,
\end{itemize}

\[
\epsilon_i = \gamma_0 + \mathbf{z}_{i1}\gamma_1 + \mathbf{z}_{i2}\gamma_2 + error_i
\]

should have a very low \(R^2\)

\begin{itemize}
\tightlist
\item
  if the model is \textbf{just identified} (one instrument per endogenous variable) then the \(R^2 = 0\)
\end{itemize}

Steps:

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\item
  Estimate the structural equation by 2SLS (using all available instruments) and obtain the residuals e
\item
  Regress e on all controls and instruments and obtain the \(R^2\)
\item
  Under the null hypothesis (all IV's are uncorrelated), \(nR^2 \sim \chi^2(q)\), where q is the number of instrumental variables minus the number of endogenous variables

  \begin{itemize}
  \tightlist
  \item
    if the model is just identified (one instrument per endogenous variable) then q = 0, and the distribution under the null collapses.
  \end{itemize}
\end{enumerate}

low p-value means you reject the null of exogenous instruments. Hence you would like to have high p-value in this test.

\textbf{Pitfalls for the Overid test}

\begin{itemize}
\item
  the overid test is essentially compiling the following information.

  \begin{itemize}
  \tightlist
  \item
    Conditional on first instrument being exogenous is the other instrument exogenous?
  \item
    Conditional on the other instrument being exogenous, is the first instrument exogenous?
  \end{itemize}
\item
  If all instruments are endogenous than neither test will be valid
\item
  really only useful if one instrument is thought to be truly exogenous (randomly assigned). even f you do reject the null, the test does not tell you which instrument is exogenous and which is endogenous.
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.2769}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.7231}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Result
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Implication
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
reject the null & you can be pretty sure there is an endogenous instrument, but don't know which one. \\
fail to reject & could be either (1) they are both exogenous, (2) they are both endogenous. \\
\end{longtable}

\hypertarget{relevancy}{%
\subparagraph{Relevancy}\label{relevancy}}

Why Relevance matter?

\[
\pi_2 \neq 0 
\]

\begin{itemize}
\item
  used to show \protect\hyperlink{a2-full-rank}{A2} holds

  \begin{itemize}
  \item
    If \(\pi_2 = 0\) (instrument is not relevant) then \protect\hyperlink{a2-full-rank}{A2} fails - perfect multicollinearity
  \item
    If \(\pi_2\) is close to 0 (\textbf{weak instrument}) then there is near perfect multicollinearity - 2SLS is highly inefficient (Large standard errors).
  \end{itemize}
\item
  A weak instrument will exacerbate any inconsistency due to an instrument being (even slightly) endogenous.

  \begin{itemize}
  \tightlist
  \item
    In the simple case with no controls and a single endogenous variable and single instrumental variable,
  \end{itemize}
\end{itemize}

\[
plim(\hat{\beta}_{2_{2SLS}}) = \beta_2 + \frac{E(z_{i2}\epsilon_i)}{E(z_{i2}y_{i2})}
\]

\textbf{Testing Weak Instruments}

\begin{itemize}
\item
  can use \(t\)-test (or \(F\)-test for over-identified models) in the first stage to determine if there is a weak instrument problem.
\item
  \citep[\citet{stock2005asymptotic}]{stock2002testing}: a statistical rejection of the null hypothesis in the first stage at the 5\% (or even 1\%) level is not enough to insure the instrument is not weak

  \begin{itemize}
  \tightlist
  \item
    Rule of Thumb: need a \(F\)-stat of at least 10 (or a \(t\)-stat of at least 3.2) to reject the null hypothesis that the instrument is weak.
  \end{itemize}
\end{itemize}

\textbf{Summary of the 2SLS Estimator}

\[
\begin{aligned}
y_{i1} &=\beta_0 + \mathbf{z}_{i1}\beta_1 + y_{i2}\beta_2 + \epsilon_i \\
y_{i2} &= \pi_0 + \mathbf{z_{i1}\pi_1} + \mathbf{z_{i2}\pi_2} + v_i
\end{aligned}
\]

\begin{itemize}
\tightlist
\item
  when \protect\hyperlink{a3a}{A3a} does not hold
\end{itemize}

\[
E(y_{i2}'\epsilon_i) \neq 0
\]

\begin{itemize}
\item
  Then the OLS estimator is no longer unbiased or consistent.
\item
  If we have valid instruments \(\mathbf{z}_{i2}\)
\item
  \protect\hyperlink{relevancy}{Relevancy} (need to avoid ``weak instruments''): \(\pi_2 \neq 0\) Then the 2SLS estimator is consistent under \protect\hyperlink{a1-linearity}{A1}, \protect\hyperlink{a2-full-rank}{A2}, \protect\hyperlink{a5a}{A5a}, and the above two conditions.

  \begin{itemize}
  \item
    If \protect\hyperlink{a4-homoskedasticity}{A4} also holds, then the usual standard errors are valid.
  \item
    If \protect\hyperlink{a4-homoskedasticity}{A4} does not hold then use the robust standard errors.
  \end{itemize}
\end{itemize}

\[
\begin{aligned}
y_{i1} &= \beta_0 + \mathbf{z}_{i1}\beta_1 + y_{i2}\beta_2 + \epsilon_i \\
y_{i2} &= \pi_0 + \mathbf{z_{i1}\pi_1} + \mathbf{z_{i2}\pi_2} + v_i
\end{aligned}
\]

\begin{itemize}
\tightlist
\item
  When \protect\hyperlink{a3a}{A3a} does hold
\end{itemize}

\[
E(y_{i2}'\epsilon_i) = 0
\]

and we have valid instruments, then both the OLS and 2SLS estimators are consistent.

\begin{itemize}
\tightlist
\item
  The OLS estimator is always more efficient
\item
  can use the variable addition test to determine if 2SLS is need (A3a does hold) or if OLS is valid (A3a does not hold)
\end{itemize}

Sometimes we can test the assumption for instrument to be valid:

\begin{itemize}
\tightlist
\item
  \protect\hyperlink{exogeneity}{Exogeneity} : Only table when there are more instruments than endogenous variables.
\item
  \protect\hyperlink{relevancy}{Relevancy} (need to avoid ``weak instruments''): Always testable, need the F-stat to be greater than 10 to rule out a weak instrument
\end{itemize}

Application

Expenditure as observed instrument

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m2}\FloatTok{.2}\NormalTok{sls }\OtherTok{\textless{}{-}}
    \FunctionTok{ivreg}\NormalTok{(}
\NormalTok{        read }\SpecialCharTok{\textasciitilde{}}\NormalTok{ stratio }\SpecialCharTok{+}\NormalTok{ english }\SpecialCharTok{+}\NormalTok{ lunch }
        \SpecialCharTok{+}\NormalTok{ grades }\SpecialCharTok{+}\NormalTok{ income }\SpecialCharTok{+}\NormalTok{ calworks }\SpecialCharTok{+}\NormalTok{ county }\SpecialCharTok{|}
            
\NormalTok{            expenditure }\SpecialCharTok{+}\NormalTok{ english }\SpecialCharTok{+}\NormalTok{ lunch }
        \SpecialCharTok{+}\NormalTok{ grades }\SpecialCharTok{+}\NormalTok{ income }\SpecialCharTok{+}\NormalTok{ calworks }\SpecialCharTok{+}\NormalTok{ county ,}
        \AttributeTok{data =}\NormalTok{ school}
\NormalTok{    )}
\FunctionTok{summary}\NormalTok{(m2}\FloatTok{.2}\NormalTok{sls)}\SpecialCharTok{$}\NormalTok{coefficients[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{7}\NormalTok{,]}
\CommentTok{\#\textgreater{}                 Estimate  Std. Error     t value      Pr(\textgreater{}|t|)}
\CommentTok{\#\textgreater{} (Intercept) 700.47891593 13.58064436  51.5792106 8.950497e{-}171}
\CommentTok{\#\textgreater{} stratio      {-}1.13674002  0.53533638  {-}2.1234126  3.438427e{-}02}
\CommentTok{\#\textgreater{} english      {-}0.21396934  0.03847833  {-}5.5607753  5.162571e{-}08}
\CommentTok{\#\textgreater{} lunch        {-}0.39384225  0.03773637 {-}10.4366757  1.621794e{-}22}
\CommentTok{\#\textgreater{} gradesKK{-}08  {-}1.89227865  1.37791820  {-}1.3732881  1.704966e{-}01}
\CommentTok{\#\textgreater{} income        0.62487986  0.11199008   5.5797785  4.668490e{-}08}
\CommentTok{\#\textgreater{} calworks     {-}0.04950501  0.06244410  {-}0.7927892  4.284101e{-}01}
\end{Highlighting}
\end{Shaded}

\hypertarget{checklist}{%
\paragraph{Checklist}\label{checklist}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Regress the dependent variable on the instrument (reduced form). Since under OLS, we have unbiased estimate, the coefficient estimate should be significant (make sure the sign makes sense)
\item
  Report F-stat on the excluded instruments. F-stat \textless{} 10 means you have a weak instrument \citep{stock2002survey}.
\item
  Present \(R^2\) before and after including the instrument \citep{rossi2014even}
\item
  For models with multiple instrument, present firs-t and second-stage result for each instrument separately. Overid test should be conducted (e.g., Sargan-Hansen J)
\item
  Hausman test between OLS and 2SLS (don't confuse this test for evidence that endogeneity is irrelevant - under invalid IV, the test is useless)
\item
  Compare the 2SLS with the limited information ML. If they are different, you have evidence for weak instruments.
\end{enumerate}

\hypertarget{good-instruments}{%
\subsubsection{Good Instruments}\label{good-instruments}}

\protect\hyperlink{exogeneity}{Exogeneity} and \protect\hyperlink{relevancy}{Relevancy} are necessary but not sufficient for IV to produce consistent estimates.

Without theory or possible explanation, you can always create a new variable that is correlated with \(X\) and uncorrelated with \(\epsilon\)

For example, we want to estimate the effect of price on quantity \citep[p.~960]{reiss2011structural}

\[
\begin{aligned}
Q &= \beta_1 P + \beta_2 X + \epsilon \\
P &= \pi_1 X + \eta
\end{aligned}
\]

where \(\epsilon\) and \(\eta\) are jointly determined, \(X \perp \epsilon, \eta\)

Without theory, we can just create a new variable \(Z = X + u\) where \(E(u) = 0; u \perp X, \epsilon, \eta\)

Then, \(Z\) satisfied both conditions:

\begin{itemize}
\item
  Relevancy: \(X\) correlates \(P\) \(\rightarrow\) \(Z\) correlates \(P\)
\item
  Exogeneity: \(u \perp \epsilon\) (random noise)
\end{itemize}

But obviously, it's not a valid instrument (intuitively). But theoretically, relevance and exogeneity are not sufficient to identify \(\beta\) because of unsatisfied rank condition for identification.

Moreover, the functional form of the instrument also plays a role when choosing a good instrument. Hence, we always need to check for the robustness of our instrument.

IV methods even with valid instruments can still have poor sampling properties (finite sample bias, large sampling errors) \citep{rossi2014even}

When you have a weak instrument, it's important to report it appropriately. This problem will be exacerbated if you have multiple instruments \citep{larcker2010use}.

\hypertarget{lagged-dependent-variable}{%
\paragraph{Lagged dependent variable}\label{lagged-dependent-variable}}

In time series data sets, we can use lagged dependent variable as an instrument because it is not influenced by current shocks.

Citations for lagged dependent variable in econ \citep{chetty2014measuring},

\hypertarget{internal-instrumental-variable}{%
\subsubsection{Internal instrumental variable}\label{internal-instrumental-variable}}

\begin{itemize}
\item
  (also known as \textbf{instrument free methods}). This section is based on Raluca Gui's \href{https://cran.r-project.org/web/packages/REndo/vignettes/REndo-introduction.pdf}{guide}
\item
  alternative to external instrumental variable approaches
\item
  All approaches here assume a \textbf{continuous dependent variable}
\end{itemize}

\hypertarget{non-hierarchical-data-cross-classified}{%
\paragraph{Non-hierarchical Data (Cross-classified)}\label{non-hierarchical-data-cross-classified}}

\[
Y_t = \beta_0 + \beta_1 P_t + \beta_2 X_t + \epsilon_t
\]

where

\begin{itemize}
\tightlist
\item
  \(t = 1, .., T\) (indexes either time or cross-sectional units)
\item
  \(Y_t\) is a \(k \times 1\) response variable
\item
  \(X_t\) is a \(k \times n\) exogenous regressor
\item
  \(P_t\) is a \(k \times 1\) continuous endogenous regressor
\item
  \(\epsilon_t\) is a structural error term with \(\mu_\epsilon =0\) and \(E(\epsilon^2) = \sigma^2\)
\item
  \(\beta\) are model parameters
\end{itemize}

The endogeneity problem arises from the correlation of \(P_t\) and \(\epsilon_t\):

\[
P_t = \gamma Z_t + v_t
\]

where

\begin{itemize}
\tightlist
\item
  \(Z_t\) is a \(l \times 1\) vector of internal instrumental variables
\item
  \(Î½_t\) is a random error with \(\mu_{v_t}, E(v^2) = \sigma^2_v, E(\epsilon v) = \sigma_{\epsilon v}\)
\item
  \(Z_t\) is assumed to be stochastic with distribution \(G\)
\item
  \(Î½_t\) is assumed to have density \(h(Â·)\)
\end{itemize}

\hypertarget{latent-instrumental-variable}{%
\subparagraph{Latent Instrumental Variable}\label{latent-instrumental-variable}}

\citep{ebbes2005solving}

assume \(Z_t\) (unobserved) to be uncorrelated with \(\epsilon_t\), which is similar to \protect\hyperlink{instrumental-variable}{Instrumental Variable}. Hence, \(Z_t\) and \(Î½_t\) can't be identified without distributional assumptions

The distributions of \(Z_t\) and \(Î½_t\) need to be specified such that:

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  endogeneity of \(P_t\) is corrected
\item
  the distribution of \(P_t\) is empirically close to the integral that expresses the amount of overlap of Z as it is shifted over Î½ (= the convolution between \(Z_t\) and \(Î½_t\)).
\end{enumerate}

When the density h(Â·) = Normal, then G cannot be normal because the parameters would not be identified \citep{ebbes2005solving} .

Hence,

\begin{itemize}
\tightlist
\item
  in the \protect\hyperlink{latent-instrumental-variable}{LIV} model the distribution of \(Z_t\) is discrete
\item
  in the \protect\hyperlink{higher-moments-method}{Higher Moments Method} and \protect\hyperlink{joint-estimation-using-copula}{Joint Estimation Using Copula} methods, the distribution of \(Z_t\) is taken to be skewed.
\end{itemize}

\(Z_t\) are assumed \textbf{unobserved, discrete and exogenous}, with

\begin{itemize}
\tightlist
\item
  an unknown number of groups m
\item
  \(\gamma\) is a vector of group means.
\end{itemize}

Identification of the parameters relies on the distributional assumptions of

\begin{itemize}
\tightlist
\item
  \(P_t\): a non-Gaussian distribution
\item
  \(Z_t\) discrete with \(m \ge 2\)
\end{itemize}

Note:

\begin{itemize}
\tightlist
\item
  If \(Z_t\) is continuous, the model is unidentified
\item
  If \(P_t \sim N\), you have inefficient estimates.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m3.liv }\OtherTok{\textless{}{-}} \FunctionTok{latentIV}\NormalTok{(read }\SpecialCharTok{\textasciitilde{}}\NormalTok{ stratio, }\AttributeTok{data =}\NormalTok{ school)}
\FunctionTok{summary}\NormalTok{(m3.liv)}\SpecialCharTok{$}\NormalTok{coefficients[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{7}\NormalTok{, ]}
\CommentTok{\#\textgreater{}                   Estimate    Std. Error       z{-}score     Pr(\textgreater{}|z|)}
\CommentTok{\#\textgreater{} (Intercept)   6.996014e+02  2.686186e+02  2.604441e+00 9.529597e{-}03}
\CommentTok{\#\textgreater{} stratio      {-}2.272673e+00  1.367757e+01 {-}1.661605e{-}01 8.681108e{-}01}
\CommentTok{\#\textgreater{} pi1          {-}4.896363e+01  5.526907e{-}08 {-}8.859139e+08 0.000000e+00}
\CommentTok{\#\textgreater{} pi2           1.963920e+01  9.225351e{-}02  2.128830e+02 0.000000e+00}
\CommentTok{\#\textgreater{} theta5       6.939432e{-}152 3.354672e{-}160  2.068587e+08 0.000000e+00}
\CommentTok{\#\textgreater{} theta6        3.787512e+02  4.249457e+01  8.912932e+00 1.541524e{-}17}
\CommentTok{\#\textgreater{} theta7       {-}1.227543e+00  4.885276e+01 {-}2.512741e{-}02 9.799653e{-}01}
\end{Highlighting}
\end{Shaded}

it will return a coefficient very different from the other methods since there is only one endogenous variable.

\hypertarget{joint-estimation-using-copula}{%
\subparagraph{Joint Estimation Using Copula}\label{joint-estimation-using-copula}}

assume \(Z_t\) (unobserved) to be uncorrelated with \(\epsilon_t\), which is similar to \protect\hyperlink{instrumental-variable}{Instrumental Variable}. Hence, \(Z_t\) and \(Î½_t\) can't be identified without distributional assumptions

\citep{park2012handling} allows joint estimation of the continuous \(P_t\) and \(\epsilon_t\) using Gaussian copulas, where a copula is a function that maps several conditional distribution functions (CDF) into their joint CDF).

The underlying idea is that using information contained in the observed data, one selects marginal distributions for \(P_t\) and \(\epsilon_t\). Then, the copula model constructs a flexible multivariate joint distribution that allows a wide range of correlations between the two marginals.

The method allows both continuous and discrete \(P_t\).

In the special case of \textbf{one continuous} \(P_t\), estimation is based on MLE\\
Otherwise, based on Gaussian copulas, augmented OLS estimation is used.

\textbf{Assumptions}:

\begin{itemize}
\item
  skewed \(P_t\)
\item
  the recovery of the correct parameter estimates
\item
  \(\epsilon_t \sim\) normal marginal distribution. The marginal distribution of \(P_t\) is obtained using the \textbf{Epanechnikov kernel density estimator}\\
  \[
  \hat{h}_p = \frac{1}{T . b} \sum_{t=1}^TK(\frac{p - P_t}{b})
  \] where
\item
  \(P_t\) = endogenous variables
\item
  \(K(x) = 0.75(1-x^2)I(||x||\le 1)\)
\item
  \(b=0.9T^{-1/5}\times min(s, IQR/1.34)\)

  \begin{itemize}
  \tightlist
  \item
    IQR = interquartile range
  \item
    \(s\) = sample standard deviation
  \item
    \(T\) = n of time periods observed in the data
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 1.34 comes from this}
\FunctionTok{diff}\NormalTok{(}\FunctionTok{qnorm}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FloatTok{0.25}\NormalTok{, }\FloatTok{0.75}\NormalTok{)))}
\CommentTok{\#\textgreater{} [1] 1.34898}
\end{Highlighting}
\end{Shaded}

In augmented OLS and MLE, the inference procedure occurs in two stages:

(1): the empirical distribution of \(P_t\) is computed\\
(2) used in it constructing the likelihood function)\\
Hence, the standard errors would not be correct.

So we use the sampling distributions (from bootstrapping) to get standard errors and the variance-covariance matrix. Since the distribution of the bootstrapped parameters is highly skewed, we report the percentile confidence intervals is preferable.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{110}\NormalTok{)}
\NormalTok{m4.cc }\OtherTok{\textless{}{-}}
    \FunctionTok{copulaCorrection}\NormalTok{(}
\NormalTok{        read }\SpecialCharTok{\textasciitilde{}}\NormalTok{ stratio }\SpecialCharTok{+}\NormalTok{ english }\SpecialCharTok{+}\NormalTok{ lunch }\SpecialCharTok{+}\NormalTok{ calworks }\SpecialCharTok{+}
\NormalTok{            grades }\SpecialCharTok{+}\NormalTok{ income }\SpecialCharTok{+}\NormalTok{ county }\SpecialCharTok{|}
            \FunctionTok{continuous}\NormalTok{(stratio),}
        \AttributeTok{data =}\NormalTok{ school,}
        \AttributeTok{optimx.args =} \FunctionTok{list}\NormalTok{(}\AttributeTok{method =} \FunctionTok{c}\NormalTok{(}\StringTok{"Nelder{-}Mead"}\NormalTok{), }
                           \AttributeTok{itnmax =} \DecValTok{60000}\NormalTok{),}
        \AttributeTok{num.boots =} \DecValTok{2}\NormalTok{,}
        \AttributeTok{verbose =} \ConstantTok{FALSE}
\NormalTok{    )}
\FunctionTok{summary}\NormalTok{(m4.cc)}\SpecialCharTok{$}\NormalTok{coefficients[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{7}\NormalTok{,]}
\CommentTok{\#\textgreater{}             Point Estimate   Boots SE Lower Boots CI (95\%) Upper Boots CI (95\%)}
\CommentTok{\#\textgreater{} (Intercept)   683.06900891 2.80554212                   NA                   NA}
\CommentTok{\#\textgreater{} stratio        {-}0.32434608 0.02075999                   NA                   NA}
\CommentTok{\#\textgreater{} english        {-}0.21576110 0.01450666                   NA                   NA}
\CommentTok{\#\textgreater{} lunch          {-}0.37087664 0.01902052                   NA                   NA}
\CommentTok{\#\textgreater{} calworks       {-}0.05569058 0.02076781                   NA                   NA}
\CommentTok{\#\textgreater{} gradesKK{-}08    {-}1.92286128 0.25684614                   NA                   NA}
\CommentTok{\#\textgreater{} income          0.73595353 0.04725700                   NA                   NA}
\end{Highlighting}
\end{Shaded}

we run this model with only one endogenous continuous regressor (\texttt{stratio}). Sometimes, the code will not converge, in which case you can use different

\begin{itemize}
\tightlist
\item
  optimization algorithm
\item
  starting values
\item
  maximum number of iterations
\end{itemize}

\hypertarget{higher-moments-method}{%
\subparagraph{Higher Moments Method}\label{higher-moments-method}}

suggested by \citep{lewbel1997constructing} to identify \(\epsilon_t\) caused by \textbf{measurement error}.

Identification is achieved by using third moments of the data, with no restrictions on the distribution of \(\epsilon_t\)\\
The following instruments can be used with 2SLS estimation to obtain consistent estimates:

\[
\begin{aligned}
q_{1t} &=  (G_t - \bar{G}) \\
q_{2t} &=  (G_t - \bar{G})(P_t - \bar{P}) \\
q_{3t} &=   (G_t - \bar{G})(Y_t - \bar{Y})\\
q_{4t} &=  (Y_t - \bar{Y})(P_t - \bar{P}) \\
q_{5t} &=  (P_t - \bar{P})^2 \\
q_{6t} &=  (Y_t - \bar{Y})^2 \\
\end{aligned}
\]

where

\begin{itemize}
\tightlist
\item
  \(G_t = G(X_t)\) for any given function G that has finite third own and cross moments
\item
  \(X\) = exogenous variable
\end{itemize}

\(q_{5t}, q_{6t}\) can be used only when the measurement and \(\epsilon_t\) are symmetrically distributed. The rest of the instruments does not require any distributional assumptions for \(\epsilon_t\).

Since the regressors \(G(X) = X\) are included as instruments, \(G(X)\) can't be a linear function of X in \(q_{1t}\)

Since this method has very strong assumptions, \protect\hyperlink{higher-moments-method}{Higher Moments Method} should only be used in case of overidentification

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{111}\NormalTok{)}
\NormalTok{m5.hetEr }\OtherTok{\textless{}{-}}
    \FunctionTok{hetErrorsIV}\NormalTok{(}
\NormalTok{        read }\SpecialCharTok{\textasciitilde{}}\NormalTok{ stratio }\SpecialCharTok{+}\NormalTok{ english }\SpecialCharTok{+}\NormalTok{ lunch }\SpecialCharTok{+}\NormalTok{ calworks }\SpecialCharTok{+}\NormalTok{ income }\SpecialCharTok{+}
\NormalTok{            grades }\SpecialCharTok{+}\NormalTok{ county }\SpecialCharTok{|}
\NormalTok{            stratio }\SpecialCharTok{|} \FunctionTok{IIV}\NormalTok{(income, english),}
        \AttributeTok{data =}\NormalTok{ school}
\NormalTok{    )}
\FunctionTok{summary}\NormalTok{(m5.hetEr)}\SpecialCharTok{$}\NormalTok{coefficients[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{7}\NormalTok{,]}
\CommentTok{\#\textgreater{}                 Estimate  Std. Error    t value     Pr(\textgreater{}|t|)}
\CommentTok{\#\textgreater{} (Intercept) 662.78791557 27.90173069 23.7543657 2.380436e{-}76}
\CommentTok{\#\textgreater{} stratio       0.71480686  1.31077325  0.5453322 5.858545e{-}01}
\CommentTok{\#\textgreater{} english      {-}0.19522271  0.04057527 {-}4.8113717 2.188618e{-}06}
\CommentTok{\#\textgreater{} lunch        {-}0.37834232  0.03927793 {-}9.6324402 9.760809e{-}20}
\CommentTok{\#\textgreater{} calworks     {-}0.05665126  0.06302095 {-}0.8989273 3.692776e{-}01}
\CommentTok{\#\textgreater{} income        0.82693755  0.17236557  4.7975797 2.335271e{-}06}
\CommentTok{\#\textgreater{} gradesKK{-}08  {-}1.93795843  1.38723186 {-}1.3969968 1.632541e{-}01}
\end{Highlighting}
\end{Shaded}

recommend using this approach to create additional instruments to use with external ones for better efficiency.

\hypertarget{heteroskedastic-error-approach}{%
\subparagraph{Heteroskedastic Error Approach}\label{heteroskedastic-error-approach}}

\begin{itemize}
\tightlist
\item
  using means of variables that are uncorrelated with the product of heteroskedastic errors to identify structural parameters.
\item
  This method can be use either when you don't have external instruments or you want to use additional instruments to improve the efficiency of the IV estimator \citep{lewbel2012using}
\item
  The instruments are constructed as simple functions of data
\item
  Model's assumptions:
\end{itemize}

\[
\begin{aligned}
E(X \epsilon) &= 0 \\
E(X v ) &= 0 \\
cov(Z, \epsilon v) &= 0  \\
cov(Z, v^2) &\neq 0 \text{  (for identification)}
\end{aligned}
\]

Structural parameters are identified by 2SLS regression of Y on X and P, using X and {[}Z âˆ’ E(Z){]}Î½ as instruments.

\[
\text{instrument's strength} \propto cov((Z-\bar{Z})v,v)
\]

where \(cov((Z-\bar{Z})v,v)\) is the degree of heteroskedasticity of Î½ with respect to Z \citep{lewbel2012using}, which can be empirically tested.

If it is zero or close to zero (i.e.,the instrument is weak), you might have imprecise estimates, with large standard errors.

\begin{itemize}
\tightlist
\item
  Under homoskedasticity, the parameters of the model are unidentified.
\item
  Under heteroskedasticity related to at least some elements of X, the parameters of the model are identified.
\end{itemize}

\hypertarget{hierarchical-data}{%
\paragraph{Hierarchical Data}\label{hierarchical-data}}

Multiple independent assumptions involving various random components at different levels mean that any moderate correlation between some predictors and a random component or error term can result in a significant bias of the coefficients and of the variance components. \citep{kim2007multilevel} proposed a generalized method of moments which uses both, the between and within variations of the exogenous variables, but only assumes the within variation of the variables to be endogenous.

\textbf{Assumptions}

\begin{itemize}
\tightlist
\item
  the errors at each level \(\sim iid N\)
\item
  the slope variables are exogenous
\item
  the level-1 \(\epsilon \perp X, P\). If this is not the case, additional, external instruments are necessary
\end{itemize}

\textbf{Hierarchical Model}

\[
\begin{aligned}
Y_{cst} &= Z_{cst}^1 \beta_{cs}^1 + X_{cst}^1 \beta_1 + \epsilon_{cst}^1 \\
\beta^1_{cs} &= Z_{cs}^2 \beta_{c}^2 + X_{cst}^2 \beta_2 + \epsilon_{cst}^2 \\
\beta^2_{c} &= X^3_c \beta_3 + \epsilon_c^3
\end{aligned}
\]

Bias could stem from:

\begin{itemize}
\tightlist
\item
  errors at the higher two levels (\(\epsilon_c^3,\epsilon_{cst}^2\)) are correlated with some of the regressors
\item
  only third level errors (\(\epsilon_c^3\)) are correlated with some of the regressors
\end{itemize}

\citep{kim2007multilevel} proposed

\begin{itemize}
\tightlist
\item
  When all variables are assumed exogenous, the proposed estimator equals the random effects estimator
\item
  When all variables are assumed endogenous, it equals the fixed effects estimator
\item
  also use omitted variable test (based on the Hausman-test \citep{hausman1978specification} for panel data), which allows the comparison of a robust estimator and an estimator that is efficient under the null hypothesis of no omitted variables or the comparison of two robust estimators at different levels.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# function \textquotesingle{}cholmod\_factor\_ldetA\textquotesingle{} not provided by package \textquotesingle{}Matrix\textquotesingle{}}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{113}\NormalTok{)}
\NormalTok{school}\SpecialCharTok{$}\NormalTok{gr08 }\OtherTok{\textless{}{-}}\NormalTok{ school}\SpecialCharTok{$}\NormalTok{grades }\SpecialCharTok{==} \StringTok{"KK{-}06"}
\NormalTok{m7.multilevel }\OtherTok{\textless{}{-}}
    \FunctionTok{multilevelIV}\NormalTok{(read }\SpecialCharTok{\textasciitilde{}}\NormalTok{ stratio }\SpecialCharTok{+}\NormalTok{ english }\SpecialCharTok{+}\NormalTok{ lunch }\SpecialCharTok{+}\NormalTok{ income }\SpecialCharTok{+}\NormalTok{ gr08 }\SpecialCharTok{+}
\NormalTok{                     calworks }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ county) }\SpecialCharTok{|} \FunctionTok{endo}\NormalTok{(stratio),}
                 \AttributeTok{data =}\NormalTok{ school)}
\FunctionTok{summary}\NormalTok{(m7.multilevel)}\SpecialCharTok{$}\NormalTok{coefficients[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{7}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

Another example using simulated data

\begin{itemize}
\tightlist
\item
  level-1 regressors: \(X_{11}, X_{12}, X_{13}, X_{14}, X_{15}\), where \(X_{15}\) is correlated with the level-2 error (i.e., endogenous).\\
\item
  level-2 regressors: \(X_{21}, X_{22}, X_{23}, X_{24}\)\\
\item
  level-3 regressors: \(X_{31}, X_{32}, X_{33}\)
\end{itemize}

We estimate a three-level model with X15 assumed endogenous. Having a three-level hierarchy, \texttt{multilevelIV()} returns five estimators, from the most robust to omitted variables (FE\_L2), to the most efficient (REF) (i.e.~lowest mean squared error).

\begin{itemize}
\tightlist
\item
  The random effects estimator (REF) is efficient assuming no omitted variables
\item
  The fixed effects estimator (FE) is unbiased and asymptotically normal even in the presence of omitted variables.
\item
  Because of the efficiency, the random effects estimator is preferable if you think there is no omitted. variables
\item
  The robust estimator would be preferable if you think there is omitted variables.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# function \textquotesingle{}cholmod\_factor\_ldetA\textquotesingle{} not provided by package \textquotesingle{}Matrix\textquotesingle{}â€™}
\FunctionTok{data}\NormalTok{(dataMultilevelIV)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{114}\NormalTok{)}
\NormalTok{formula1 }\OtherTok{\textless{}{-}}
\NormalTok{    y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X11 }\SpecialCharTok{+}\NormalTok{ X12 }\SpecialCharTok{+}\NormalTok{ X13 }\SpecialCharTok{+}\NormalTok{ X14 }\SpecialCharTok{+}\NormalTok{ X15 }\SpecialCharTok{+}\NormalTok{ X21 }\SpecialCharTok{+}\NormalTok{ X22 }\SpecialCharTok{+}\NormalTok{ X23 }\SpecialCharTok{+}\NormalTok{ X24 }\SpecialCharTok{+}
\NormalTok{    X31 }\SpecialCharTok{+}\NormalTok{ X32 }\SpecialCharTok{+}\NormalTok{ X33 }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ CID) }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ SID) }\SpecialCharTok{|} \FunctionTok{endo}\NormalTok{(X15)}
\NormalTok{m8.multilevel }\OtherTok{\textless{}{-}}
    \FunctionTok{multilevelIV}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ formula1, }\AttributeTok{data =}\NormalTok{ dataMultilevelIV)}
\FunctionTok{coef}\NormalTok{(m8.multilevel)}

\FunctionTok{summary}\NormalTok{(m8.multilevel, }\StringTok{"REF"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

True \(\beta_{X_{15}} =-1\). We can see that some estimators are bias because \(X_{15}\) is correlated with the level-two error, to which only FE\_L2 and GMM\_L2 are robust

To select the appropriate estimator, we use the omitted variable test.

In a three-level setting, we can have different estimator comparisons:

\begin{itemize}
\tightlist
\item
  Fixed effects vs.~random effects estimators: Test for omitted level-two and level-three omitted effects, simultaneously, one compares FE\_L2 to REF. But we will not know at which omitted variables exist.\\
\item
  Fixed effects vs.~GMM estimators: Once the existence of omitted effects is established but not sure at which level, we test for level-2 omitted effects by comparing FE\_L2 vs GMM\_L3. If you reject the null, the omitted variables are at level-2 The same is accomplished by testing FE\_L2 vs.~GMM\_L2, since the latter is consistent only if there are no omitted effects at level-2.\\
\item
  Fixed effects vs.~fixed effects estimators: We can test for omitted level-2 effects, while allowing for omitted level-3 effects by comparing FE\_L2 vs.~FE\_L3 since FE\_L2 is robust against both level-2 and level-3 omitted effects while FE\_L3 is only robust to level-3 omitted variables.
\end{itemize}

Summary, use the omitted variable test comparing \texttt{REF\ vs.\ FE\_L2} first.

\begin{itemize}
\item
  If the null hypothesis is rejected, then there are omitted variables either at level-2 or level-3
\item
  Next, test whether there are level-2 omitted effects, since testing for omitted level three effects relies on the assumption there are no level-two omitted effects. You can use any of these pair of comparisons:

  \begin{itemize}
  \tightlist
  \item
    \texttt{FE\_L2\ vs.\ FE\_L3}
  \item
    \texttt{FE\_L2\ vs.\ GMM\_L2}
  \end{itemize}
\item
  If no omitted variables at level-2 are found, test for omitted level-3 effects by comparing either

  \begin{itemize}
  \tightlist
  \item
    \texttt{FE\_L3} vs.~\texttt{GMM\_L3}
  \item
    \texttt{GMM\_L2} vs.~\texttt{GMM\_L3}
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(m8.multilevel, }\StringTok{"REF"}\NormalTok{)}
\CommentTok{\# compare REF with all the other estimators. Testing REF (the most efficient estimator) against FE\_L2 (the most robust estimator), equivalently we are testing simultaneously for level{-}2 and level{-}3 omitted effects. }
\end{Highlighting}
\end{Shaded}

Since the null hypothesis is rejected (p = 0.000139), there is bias in the random effects estimator.

To test for level-2 omitted effects (regardless of level-3 omitted effects), we compare FE\_L2 versus FE\_L3

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(m8.multilevel,}\StringTok{"FE\_L2"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The null hypothesis of no omitted level-2 effects is rejected (\(p = 3.92e âˆ’ 05\)). Hence, there are omitted effects at level-two. We should use FE\_L2 which is consistent with the underlying data that we generated (level-2 error correlated with \(X_15\), which leads to biased FE\_L3 coefficients.

The omitted variable test between FE\_L2 and GMM\_L2 should reject the null hypothesis of no omitted level-2 effects (p-value is 0).

If we assume an endogenous variable as exogenous, the RE and GMM estimators will be biased because of the wrong set of internal instrumental variables. To increase our confidence, we should compare the omitted variable tests when the variable is considered endogenous vs.~exogenous to get a sense whether the variable is truly endogenous.

\hypertarget{proxy-variables}{%
\subsubsection{Proxy Variables}\label{proxy-variables}}

\begin{itemize}
\item
  Can be in place of the omitted variable
\item
  will not be able to estimate the effect of the omitted variable
\item
  will be able to reduce some endogeneity caused bye the omitted variable
\item
  but it can have \protect\hyperlink{measurement-error}{Measurement Error}. Hence, you have to be extremely careful when using proxies.
\end{itemize}

Criteria for a proxy variable:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The proxy is correlated with the omitted variable.
\item
  Having the omitted variable in the regression will solve the problem of endogeneity
\item
  The variation of the omitted variable unexplained by the proxy is uncorrelated with all independent variables, including the proxy.
\end{enumerate}

IQ test can be a proxy for ability in the regression between wage explained education.

For the third requirement

\[
ability = \gamma_0 + \gamma_1 IQ + \epsilon
\]

where \(\epsilon\) is uncorrelated with education and IQ test.

\hypertarget{endogenous-sample-selection}{%
\section{Endogenous Sample Selection}\label{endogenous-sample-selection}}

\begin{itemize}
\item
  Also known as sample selection or self-selection problem
\item
  The omitted variable is how people were selected into the sample
\end{itemize}

Some disciplines consider nonresponse bias and selection bias as sample selection.

\begin{itemize}
\tightlist
\item
  When unobservable factors that affect who is in the sample are independent of unobservable factors that affect the outcome, the sample selection is not endogenous. Hence, the sample selection is ignorable and estimator that ignores sample selection is still consistent.
\item
  when the unobservable factors that affect who is included in the sample are correlated with the unobservable factors that affect the outcome, the sample selection is endogenous and not ignorable, because estimators that ignore endogenous sample selection are not consistent (we don't know which part of the observable outcome is related to the causal relationship and which part is due to different people were selected for the treatment and control groups).
\end{itemize}

Assumptions: - The unobservables that affect the treatment selection and the outcome are jointly distributed as bivariate normal.

\textbf{Notes}:

\begin{itemize}
\item
  If you don't have strong exclusion restriction, identification is driven by the assumed non linearity in the functional form (through inverse Mills ratio). E.g., the estimate depend on the bivariate normal distribution of the error structure:

  \begin{itemize}
  \tightlist
  \item
    With strong exclusion restriction for the covariate in the correction equation, the variation in this variable can help identify the control for selection
  \item
    With weak exclusion restriction, and the variable exists in both steps, it's the assumed error structure that identifies the control for selection.
  \end{itemize}
\end{itemize}

To combat {[}Sample Selection{]}, we can

\begin{itemize}
\tightlist
\item
  Randomization: participants are randomly selected into treatment and control.
\item
  Instruments that determine the treatment status (i.e., treatment vs.~control) but not the outcome (\(Y\))
\item
  Functional form of the selection and outcome processes: originated from \citep{Heckman_1976}, later on generalize by \citep{amemiya1984tobit}
\end{itemize}

We have our main model

\[
\mathbf{y^* = xb + \epsilon}
\]

However, the pattern of missingness (i.e., censored) is related to the unobserved (latent) process:

\[
\mathbf{z^* = w \gamma + u}
\]

and

\[
z_i = 
\begin{cases}
1& \text{if } z_i^*>0 \\
0&\text{if } z_i^*\le0\\
\end{cases}
\]

Equivalently, \(z_i = 1\) (\(y_i\) is observed) when

\[
u_i \ge -w_i \gamma
\]

Hence, the probability of observed \(y_i\) is

\[
\begin{aligned}
P(u_i \ge -w_i \gamma) &= 1 - \Phi(-w_i \gamma) \\
&= \Phi(w_i \gamma) & \text{symmetry of the standard normal distribution}
\end{aligned}
\]

We will \textbf{assume}

\begin{itemize}
\tightlist
\item
  the error term of the selection \(\mathbf{u \sim N(0,I)}\)
\item
  \(Var(u_i) = 1\) for identification purposes
\end{itemize}

Visually, \(P(u_i \ge -w_i \gamma)\) is the shaded area.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{=} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\AttributeTok{length =} \DecValTok{200}\NormalTok{)}
\NormalTok{y }\OtherTok{=} \FunctionTok{dnorm}\NormalTok{(x, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(x,}
\NormalTok{     y,}
     \AttributeTok{type =} \StringTok{"l"}\NormalTok{,}
     \AttributeTok{main =} \FunctionTok{bquote}\NormalTok{(}\StringTok{"Probabibility distribution of"} \SpecialCharTok{\textasciitilde{}}\NormalTok{ u[i]))}
\NormalTok{x }\OtherTok{=} \FunctionTok{seq}\NormalTok{(}\FloatTok{0.3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\AttributeTok{length =} \DecValTok{100}\NormalTok{)}
\NormalTok{y }\OtherTok{=} \FunctionTok{dnorm}\NormalTok{(x, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)}
\FunctionTok{polygon}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FloatTok{0.3}\NormalTok{, x, }\DecValTok{3}\NormalTok{), }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, y, }\DecValTok{0}\NormalTok{), }\AttributeTok{col =} \StringTok{"gray"}\NormalTok{)}
\FunctionTok{text}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\FunctionTok{bquote}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ Phi }\SpecialCharTok{\textasciitilde{}}\NormalTok{ (}\SpecialCharTok{{-}}\NormalTok{w[i] }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gamma)))}
\FunctionTok{arrows}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\DecValTok{0}\NormalTok{, }\AttributeTok{length =}\NormalTok{ .}\DecValTok{15}\NormalTok{)}
\FunctionTok{text}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.12}\NormalTok{, }\FunctionTok{bquote}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{w[i] }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gamma))}
\FunctionTok{legend}\NormalTok{(}
    \StringTok{"topright"}\NormalTok{,}
    \StringTok{"Gray = Prob of Observed"}\NormalTok{,}
    \AttributeTok{pch =} \DecValTok{1}\NormalTok{,}
    \AttributeTok{title =} \StringTok{"legend"}\NormalTok{,}
    \AttributeTok{inset =}\NormalTok{ .}\DecValTok{02}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{31-endogeneity_files/figure-latex/unnamed-chunk-12-1} \end{center}

Hence in our observed model, we see

\begin{equation}
y_i = x_i\beta + \epsilon_i \text{when $z_i=1$}
\end{equation}

and the joint distribution of the selection model (\(u_i\)), and the observed equation (\(\epsilon_i\)) as

\[
\left[
\begin{array}
{c}
u \\
\epsilon \\
\end{array}
\right]
\sim^{iid}N
\left(
\left[
\begin{array}
{c}
0 \\
0 \\
\end{array}
\right],
\left[
\begin{array}
{cc}
1 & \rho \\
\rho & \sigma^2_{\epsilon} \\
\end{array}
\right]
\right)
\]

The relation between the observed and selection models:

\[
\begin{aligned}
E(y_i | y_i \text{ observed}) &= E(y_i| z^*>0) \\
&= E(y_i| -w_i \gamma) \\
&= \mathbf{x}_i \beta + E(\epsilon_i | u_i > -w_i \gamma) \\
&= \mathbf{x}_i \beta + \rho \sigma_\epsilon \frac{\phi(w_i \gamma)}{\Phi(w_i \gamma)}
\end{aligned}
\]

where \(\frac{\phi(w_i \gamma)}{\Phi(w_i \gamma)}\) is the Inverse Mills Ratio. and \(\rho \sigma_\epsilon \frac{\phi(w_i \gamma)}{\Phi(w_i \gamma)} \ge 0\)

A property of IMR: Its derivative is: \(IMR'(x) = -x IMR(x) - IMR(x)^2\)

Great visualization of special cases of correlation patterns among data and errors by professor \href{https://rlhick.people.wm.edu/stories/econ_407_notes_heckman.html}{Rob Hick}

Note:

\citep{bareinboim2014transportability} is an excellent summary of cases that we can still do causal inference in case of selection bias. I'll try to summarize their idea here:

Let \(X\) be an action, \(Y\) be an outcome, and S be a binary indicator of entry into the data pool where (\(S = 1 =\) in the sample, \(S = 0 =\) out of sample) and Q be the conditional distribution \(Q = P(y|x)\).

Usually we want to understand , but because of \(S\), we only have \(P(y, x|S = 1)\). Hence, we'd like to recover \(P(y|x)\) from \(P(y, x|S = 1)\)

\begin{itemize}
\tightlist
\item
  If both X and Y affect S, we can't unbiasedly estimate \(P(y|x)\)
\end{itemize}

In the case of Omitted variable bias (\(U\)) and sample selection bias (\(S\)), you have unblocked extraneous ``flow'' of information between X and \(Y\), which causes spurious correlation for \(X\) and \(Y\). Traditionally, we would recover \(Q\) by parametric assumption of

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  The data generating process (e.g., Heckman 2-step)
\item
  Type of data-generating model (e..g, treatment-dependent or outcome-dependent)
\item
  Selection's probability \(P(S = 1|P a_s)\) with non-parametrically based causal graphical models, the authors proposed more robust way to model misspecification regardless of the type of data-generating model, and do not require selection's probability. Hence, you can recover Q

  \begin{itemize}
  \tightlist
  \item
    Without external data
  \item
    With external data
  \item
    Causal effects with the Selection-backdoor criterion
  \end{itemize}
\end{enumerate}

\hypertarget{tobit-2}{%
\subsection{Tobit-2}\label{tobit-2}}

also known as Heckman's standard sample selection model\\
Assumption: joint normality of the errors

Data here is taken from \citet{mroz1984sensitivity}.

We want to estimate the log(wage) for married women, with education, experience, experience squared, and a dummy variable for living in a big city. But we can only observe the wage for women who are working, which means a lot of married women in 1975 who were out of the labor force are unaccounted for. Hence, an OLS estimate of the wage equation would be bias due to sample selection. Since we have data on non-participants (i.e., those who are not working for pay), we can correct for the selection process.

The Tobit-2 estimates are consistent

\hypertarget{example-1-3}{%
\subsubsection{Example 1}\label{example-1-3}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(sampleSelection)}
\FunctionTok{library}\NormalTok{(dplyr)}
\CommentTok{\# 1975 data on married womenâ€™s pay and labor{-}force participation }
\CommentTok{\# from the Panel Study of Income Dynamics (PSID)}
\FunctionTok{data}\NormalTok{(}\StringTok{"Mroz87"}\NormalTok{) }
\FunctionTok{head}\NormalTok{(Mroz87)}
\CommentTok{\#\textgreater{}   lfp hours kids5 kids618 age educ   wage repwage hushrs husage huseduc huswage}
\CommentTok{\#\textgreater{} 1   1  1610     1       0  32   12 3.3540    2.65   2708     34      12  4.0288}
\CommentTok{\#\textgreater{} 2   1  1656     0       2  30   12 1.3889    2.65   2310     30       9  8.4416}
\CommentTok{\#\textgreater{} 3   1  1980     1       3  35   12 4.5455    4.04   3072     40      12  3.5807}
\CommentTok{\#\textgreater{} 4   1   456     0       3  34   12 1.0965    3.25   1920     53      10  3.5417}
\CommentTok{\#\textgreater{} 5   1  1568     1       2  31   14 4.5918    3.60   2000     32      12 10.0000}
\CommentTok{\#\textgreater{} 6   1  2032     0       0  54   12 4.7421    4.70   1040     57      11  6.7106}
\CommentTok{\#\textgreater{}   faminc    mtr motheduc fatheduc unem city exper  nwifeinc wifecoll huscoll}
\CommentTok{\#\textgreater{} 1  16310 0.7215       12        7  5.0    0    14 10.910060    FALSE   FALSE}
\CommentTok{\#\textgreater{} 2  21800 0.6615        7        7 11.0    1     5 19.499981    FALSE   FALSE}
\CommentTok{\#\textgreater{} 3  21040 0.6915       12        7  5.0    0    15 12.039910    FALSE   FALSE}
\CommentTok{\#\textgreater{} 4   7300 0.7815        7        7  5.0    0     6  6.799996    FALSE   FALSE}
\CommentTok{\#\textgreater{} 5  27300 0.6215       12       14  9.5    1     7 20.100058     TRUE   FALSE}
\CommentTok{\#\textgreater{} 6  19495 0.6915       14        7  7.5    1    33  9.859054    FALSE   FALSE}
\NormalTok{Mroz87 }\OtherTok{=}\NormalTok{ Mroz87 }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{kids =}\NormalTok{ kids5 }\SpecialCharTok{+}\NormalTok{ kids618)}

\FunctionTok{library}\NormalTok{(nnet)}
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(reshape2)}
\end{Highlighting}
\end{Shaded}

2-stage Heckman's model:

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  probit equation estimates the selection process (who is in the labor force?)
\item
  the results from 1st stage are used to construct a variable that captures the selection effect in the wage equation. This correction variable is called the \textbf{inverse Mills ratio}.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# OLS: log wage regression on LF participants only}
\NormalTok{ols1 }\OtherTok{=} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(wage) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ educ }\SpecialCharTok{+}\NormalTok{ exper }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(exper }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ city, }
          \AttributeTok{data =} \FunctionTok{subset}\NormalTok{(Mroz87, lfp }\SpecialCharTok{==} \DecValTok{1}\NormalTok{))}
\CommentTok{\# Heckman\textquotesingle{}s Two{-}step estimation with LFP selection equation}
\NormalTok{heck1 }\OtherTok{=} \FunctionTok{heckit}\NormalTok{(}
    \AttributeTok{selection =}\NormalTok{ lfp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(age }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ kids }\SpecialCharTok{+}\NormalTok{ huswage }\SpecialCharTok{+}\NormalTok{ educ,}
    \CommentTok{\# the selection process, l}
    \CommentTok{\# fp = 1 if the woman is participating in the labor force}
    \AttributeTok{outcome =} \FunctionTok{log}\NormalTok{(wage) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ educ }\SpecialCharTok{+}\NormalTok{ exper }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(exper }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ city,}
    \AttributeTok{data =}\NormalTok{ Mroz87}
\NormalTok{)}

\FunctionTok{summary}\NormalTok{(heck1}\SpecialCharTok{$}\NormalTok{probit)}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} Probit binary choice model/Maximum Likelihood estimation}
\CommentTok{\#\textgreater{} Newton{-}Raphson maximisation, 4 iterations}
\CommentTok{\#\textgreater{} Return code 1: gradient close to zero (gradtol)}
\CommentTok{\#\textgreater{} Log{-}Likelihood: {-}482.8212 }
\CommentTok{\#\textgreater{} Model: Y == \textquotesingle{}1\textquotesingle{} in contrary to \textquotesingle{}0\textquotesingle{}}
\CommentTok{\#\textgreater{} 753 observations (325 \textquotesingle{}negative\textquotesingle{} and 428 \textquotesingle{}positive\textquotesingle{}) and 6 free parameters (df = 747)}
\CommentTok{\#\textgreater{} Estimates:}
\CommentTok{\#\textgreater{}                  Estimate  Std. error t value   Pr(\textgreater{} t)    }
\CommentTok{\#\textgreater{} XS(Intercept) {-}4.18146681  1.40241567 {-}2.9816  0.002867 ** }
\CommentTok{\#\textgreater{} XSage          0.18608901  0.06517476  2.8552  0.004301 ** }
\CommentTok{\#\textgreater{} XSI(age\^{}2)    {-}0.00241491  0.00075857 {-}3.1835  0.001455 ** }
\CommentTok{\#\textgreater{} XSkids        {-}0.14955977  0.03825079 {-}3.9100 9.230e{-}05 ***}
\CommentTok{\#\textgreater{} XShuswage     {-}0.04303635  0.01220791 {-}3.5253  0.000423 ***}
\CommentTok{\#\textgreater{} XSeduc         0.12502818  0.02277645  5.4894 4.034e{-}08 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} Significance test:}
\CommentTok{\#\textgreater{} chi2(5) = 64.10407 (p=1.719042e{-}12)}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\FunctionTok{summary}\NormalTok{(heck1}\SpecialCharTok{$}\NormalTok{lm)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = YO \textasciitilde{} {-}1 + XO + imrData$IMR1, subset = YS == 1, weights = weightsNoNA)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}      Min       1Q   Median       3Q      Max }
\CommentTok{\#\textgreater{} {-}3.09494 {-}0.30953  0.05341  0.36530  2.34770 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}                 Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} XO(Intercept) {-}0.6143381  0.3768796  {-}1.630  0.10383    }
\CommentTok{\#\textgreater{} XOeduc         0.1092363  0.0197062   5.543 5.24e{-}08 ***}
\CommentTok{\#\textgreater{} XOexper        0.0419205  0.0136176   3.078  0.00222 ** }
\CommentTok{\#\textgreater{} XOI(exper\^{}2)  {-}0.0008226  0.0004059  {-}2.026  0.04335 *  }
\CommentTok{\#\textgreater{} XOcity         0.0510492  0.0692414   0.737  0.46137    }
\CommentTok{\#\textgreater{} imrData$IMR1   0.0551177  0.2111916   0.261  0.79423    }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 0.6674 on 422 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.7734, Adjusted R{-}squared:  0.7702 }
\CommentTok{\#\textgreater{} F{-}statistic:   240 on 6 and 422 DF,  p{-}value: \textless{} 2.2e{-}16}
\end{Highlighting}
\end{Shaded}

Use only variables that affect the selection process in the selection equation. Technically, the selection equation and the equation of interest could have the same set of regressors. But it is not recommended because we should only use variables (or at least one) in the selection equation that affect the selection process, but not the wage process (i.e., instruments). Here, variable \texttt{kids} fulfill that role: women with kids may be more likely to stay home, but working moms with kids would not have their wages change.

Alternatively,

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# ML estimation of selection model}
\NormalTok{ml1 }\OtherTok{=} \FunctionTok{selection}\NormalTok{(}
    \AttributeTok{selection =}\NormalTok{ lfp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(age }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ kids }\SpecialCharTok{+}\NormalTok{ huswage }\SpecialCharTok{+}\NormalTok{ educ,}
    \AttributeTok{outcome =} \FunctionTok{log}\NormalTok{(wage) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ educ }\SpecialCharTok{+}\NormalTok{ exper }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(exper }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ city,}
    \AttributeTok{data =}\NormalTok{ Mroz87}
\NormalTok{) }
\FunctionTok{summary}\NormalTok{(ml1)}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} Tobit 2 model (sample selection model)}
\CommentTok{\#\textgreater{} Maximum Likelihood estimation}
\CommentTok{\#\textgreater{} Newton{-}Raphson maximisation, 3 iterations}
\CommentTok{\#\textgreater{} Return code 8: successive function values within relative tolerance limit (reltol)}
\CommentTok{\#\textgreater{} Log{-}Likelihood: {-}914.0777 }
\CommentTok{\#\textgreater{} 753 observations (325 censored and 428 observed)}
\CommentTok{\#\textgreater{} 13 free parameters (df = 740)}
\CommentTok{\#\textgreater{} Probit selection equation:}
\CommentTok{\#\textgreater{}               Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept) {-}4.1484037  1.4109302  {-}2.940 0.003382 ** }
\CommentTok{\#\textgreater{} age          0.1842132  0.0658041   2.799 0.005253 ** }
\CommentTok{\#\textgreater{} I(age\^{}2)    {-}0.0023925  0.0007664  {-}3.122 0.001868 ** }
\CommentTok{\#\textgreater{} kids        {-}0.1488158  0.0384888  {-}3.866 0.000120 ***}
\CommentTok{\#\textgreater{} huswage     {-}0.0434253  0.0123229  {-}3.524 0.000451 ***}
\CommentTok{\#\textgreater{} educ         0.1255639  0.0229229   5.478 5.91e{-}08 ***}
\CommentTok{\#\textgreater{} Outcome equation:}
\CommentTok{\#\textgreater{}               Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept) {-}0.5814781  0.3052031  {-}1.905  0.05714 .  }
\CommentTok{\#\textgreater{} educ         0.1078481  0.0172998   6.234 7.63e{-}10 ***}
\CommentTok{\#\textgreater{} exper        0.0415752  0.0133269   3.120  0.00188 ** }
\CommentTok{\#\textgreater{} I(exper\^{}2)  {-}0.0008125  0.0003974  {-}2.044  0.04129 *  }
\CommentTok{\#\textgreater{} city         0.0522990  0.0682652   0.766  0.44385    }
\CommentTok{\#\textgreater{}    Error terms:}
\CommentTok{\#\textgreater{}       Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} sigma  0.66326    0.02309  28.729   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} rho    0.05048    0.23169   0.218    0.828    }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\# summary(ml1$twoStep)}
\end{Highlighting}
\end{Shaded}

Manual

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{myprob }\OtherTok{\textless{}{-}} \FunctionTok{probit}\NormalTok{(lfp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{( age}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{ ) }\SpecialCharTok{+}\NormalTok{ kids }\SpecialCharTok{+}\NormalTok{ huswage }\SpecialCharTok{+}\NormalTok{ educ, }
                 \CommentTok{\# x = TRUE, }
                 \CommentTok{\# iterlim = 30, }
                 \AttributeTok{data =}\NormalTok{ Mroz87)}
\FunctionTok{summary}\NormalTok{(myprob)}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} Probit binary choice model/Maximum Likelihood estimation}
\CommentTok{\#\textgreater{} Newton{-}Raphson maximisation, 4 iterations}
\CommentTok{\#\textgreater{} Return code 1: gradient close to zero (gradtol)}
\CommentTok{\#\textgreater{} Log{-}Likelihood: {-}482.8212 }
\CommentTok{\#\textgreater{} Model: Y == \textquotesingle{}1\textquotesingle{} in contrary to \textquotesingle{}0\textquotesingle{}}
\CommentTok{\#\textgreater{} 753 observations (325 \textquotesingle{}negative\textquotesingle{} and 428 \textquotesingle{}positive\textquotesingle{}) and 6 free parameters (df = 747)}
\CommentTok{\#\textgreater{} Estimates:}
\CommentTok{\#\textgreater{}                Estimate  Std. error t value   Pr(\textgreater{} t)    }
\CommentTok{\#\textgreater{} (Intercept) {-}4.18146681  1.40241567 {-}2.9816  0.002867 ** }
\CommentTok{\#\textgreater{} age          0.18608901  0.06517476  2.8552  0.004301 ** }
\CommentTok{\#\textgreater{} I(age\^{}2)    {-}0.00241491  0.00075857 {-}3.1835  0.001455 ** }
\CommentTok{\#\textgreater{} kids        {-}0.14955977  0.03825079 {-}3.9100 9.230e{-}05 ***}
\CommentTok{\#\textgreater{} huswage     {-}0.04303635  0.01220791 {-}3.5253  0.000423 ***}
\CommentTok{\#\textgreater{} educ         0.12502818  0.02277645  5.4894 4.034e{-}08 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} Significance test:}
\CommentTok{\#\textgreater{} chi2(5) = 64.10407 (p=1.719042e{-}12)}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}

\NormalTok{imr }\OtherTok{\textless{}{-}} \FunctionTok{invMillsRatio}\NormalTok{(myprob)}
\NormalTok{Mroz87}\SpecialCharTok{$}\NormalTok{IMR1 }\OtherTok{\textless{}{-}}\NormalTok{ imr}\SpecialCharTok{$}\NormalTok{IMR1}

\NormalTok{manually\_est }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(wage) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ educ }\SpecialCharTok{+}\NormalTok{ exper }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{( exper}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{ ) }\SpecialCharTok{+}\NormalTok{ city }\SpecialCharTok{+}\NormalTok{ IMR1,}
                   \AttributeTok{data =}\NormalTok{ Mroz87, }
                   \AttributeTok{subset =}\NormalTok{ (lfp }\SpecialCharTok{==} \DecValTok{1}\NormalTok{))}

\FunctionTok{summary}\NormalTok{(manually\_est)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = log(wage) \textasciitilde{} educ + exper + I(exper\^{}2) + city + IMR1, }
\CommentTok{\#\textgreater{}     data = Mroz87, subset = (lfp == 1))}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}      Min       1Q   Median       3Q      Max }
\CommentTok{\#\textgreater{} {-}3.09494 {-}0.30953  0.05341  0.36530  2.34770 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}               Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept) {-}0.6143381  0.3768796  {-}1.630  0.10383    }
\CommentTok{\#\textgreater{} educ         0.1092363  0.0197062   5.543 5.24e{-}08 ***}
\CommentTok{\#\textgreater{} exper        0.0419205  0.0136176   3.078  0.00222 ** }
\CommentTok{\#\textgreater{} I(exper\^{}2)  {-}0.0008226  0.0004059  {-}2.026  0.04335 *  }
\CommentTok{\#\textgreater{} city         0.0510492  0.0692414   0.737  0.46137    }
\CommentTok{\#\textgreater{} IMR1         0.0551177  0.2111916   0.261  0.79423    }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 0.6674 on 422 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.1582, Adjusted R{-}squared:  0.1482 }
\CommentTok{\#\textgreater{} F{-}statistic: 15.86 on 5 and 422 DF,  p{-}value: 2.505e{-}14}
\end{Highlighting}
\end{Shaded}

Similarly,

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{probit\_selection }\OtherTok{\textless{}{-}}
    \FunctionTok{glm}\NormalTok{(lfp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{( age}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{ ) }\SpecialCharTok{+}\NormalTok{ kids }\SpecialCharTok{+}\NormalTok{ huswage }\SpecialCharTok{+}\NormalTok{ educ,}
        \AttributeTok{data =}\NormalTok{ Mroz87,}
        \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{\textquotesingle{}probit\textquotesingle{}}\NormalTok{))}

\CommentTok{\# library(fixest)}
\CommentTok{\# probit\_selection \textless{}{-}}
\CommentTok{\#     fixest::feglm(lfp \textasciitilde{} age + I( age\^{}2 ) + kids + huswage + educ,}
\CommentTok{\#         data = Mroz87,}
\CommentTok{\#         family = binomial(link = \textquotesingle{}probit\textquotesingle{}))}

\NormalTok{probit\_lp }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\FunctionTok{predict}\NormalTok{(probit\_selection)}
\NormalTok{inv\_mills }\OtherTok{\textless{}{-}} \FunctionTok{dnorm}\NormalTok{(probit\_lp) }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{pnorm}\NormalTok{(probit\_lp))}
\NormalTok{Mroz87}\SpecialCharTok{$}\NormalTok{inv\_mills }\OtherTok{\textless{}{-}}\NormalTok{ inv\_mills}


\NormalTok{probit\_outcome }\OtherTok{\textless{}{-}}
    \FunctionTok{glm}\NormalTok{(}
        \FunctionTok{log}\NormalTok{(wage) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ educ }\SpecialCharTok{+}\NormalTok{ exper }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(exper }\SpecialCharTok{\^{}} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ city }\SpecialCharTok{+}\NormalTok{ inv\_mills,}
        \AttributeTok{data =}\NormalTok{ Mroz87,}
        \AttributeTok{subset =}\NormalTok{ (lfp }\SpecialCharTok{==} \DecValTok{1}\NormalTok{)}
\NormalTok{    )}
\FunctionTok{summary}\NormalTok{(probit\_outcome)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} glm(formula = log(wage) \textasciitilde{} educ + exper + I(exper\^{}2) + city + }
\CommentTok{\#\textgreater{}     inv\_mills, data = Mroz87, subset = (lfp == 1))}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Deviance Residuals: }
\CommentTok{\#\textgreater{}      Min        1Q    Median        3Q       Max  }
\CommentTok{\#\textgreater{} {-}3.09494  {-}0.30953   0.05341   0.36530   2.34770  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}               Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept) {-}0.6143383  0.3768798  {-}1.630  0.10383    }
\CommentTok{\#\textgreater{} educ         0.1092363  0.0197062   5.543 5.24e{-}08 ***}
\CommentTok{\#\textgreater{} exper        0.0419205  0.0136176   3.078  0.00222 ** }
\CommentTok{\#\textgreater{} I(exper\^{}2)  {-}0.0008226  0.0004059  {-}2.026  0.04335 *  }
\CommentTok{\#\textgreater{} city         0.0510492  0.0692414   0.737  0.46137    }
\CommentTok{\#\textgreater{} inv\_mills    0.0551179  0.2111918   0.261  0.79423    }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} (Dispersion parameter for gaussian family taken to be 0.4454809)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Null deviance: 223.33  on 427  degrees of freedom}
\CommentTok{\#\textgreater{} Residual deviance: 187.99  on 422  degrees of freedom}
\CommentTok{\#\textgreater{} AIC: 876.49}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Fisher Scoring iterations: 2}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"stargazer"}\NormalTok{)}
\FunctionTok{library}\NormalTok{(}\StringTok{"Mediana"}\NormalTok{)}
\FunctionTok{library}\NormalTok{(}\StringTok{"plm"}\NormalTok{)}
\CommentTok{\# function to calculate corrected SEs for regression }
\NormalTok{cse }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(reg) \{}
\NormalTok{  rob }\OtherTok{=} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{diag}\NormalTok{(}\FunctionTok{vcovHC}\NormalTok{(reg, }\AttributeTok{type =} \StringTok{"HC1"}\NormalTok{)))}
  \FunctionTok{return}\NormalTok{(rob)}
\NormalTok{\}}

\CommentTok{\# stargazer table}
\FunctionTok{stargazer}\NormalTok{(}
    \CommentTok{\# ols1,}
\NormalTok{    heck1,}
\NormalTok{    ml1,}
    \CommentTok{\# manually\_est,}
    
    \AttributeTok{se =} \FunctionTok{list}\NormalTok{(}\FunctionTok{cse}\NormalTok{(ols1), }\ConstantTok{NULL}\NormalTok{, }\ConstantTok{NULL}\NormalTok{),}
    \AttributeTok{title =} \StringTok{"Married women\textquotesingle{}s wage regressions"}\NormalTok{,}
    \AttributeTok{type =} \StringTok{"text"}\NormalTok{,}
    \AttributeTok{df =} \ConstantTok{FALSE}\NormalTok{,}
    \AttributeTok{digits =} \DecValTok{4}\NormalTok{,}
    \AttributeTok{selection.equation =}\NormalTok{ T}
\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Married women\textquotesingle{}s wage regressions}
\CommentTok{\#\textgreater{} ===================================================}
\CommentTok{\#\textgreater{}                           Dependent variable:      }
\CommentTok{\#\textgreater{}                     {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}                                   lfp              }
\CommentTok{\#\textgreater{}                         Heckman        selection   }
\CommentTok{\#\textgreater{}                        selection                   }
\CommentTok{\#\textgreater{}                           (1)             (2)      }
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} age                    0.1861***       0.1842***   }
\CommentTok{\#\textgreater{}                                        (0.0658)    }
\CommentTok{\#\textgreater{}                                                    }
\CommentTok{\#\textgreater{} I(age2)                 {-}0.0024       {-}0.0024***   }
\CommentTok{\#\textgreater{}                                        (0.0008)    }
\CommentTok{\#\textgreater{}                                                    }
\CommentTok{\#\textgreater{} kids                  {-}0.1496***      {-}0.1488***   }
\CommentTok{\#\textgreater{}                                        (0.0385)    }
\CommentTok{\#\textgreater{}                                                    }
\CommentTok{\#\textgreater{} huswage                 {-}0.0430       {-}0.0434***   }
\CommentTok{\#\textgreater{}                                        (0.0123)    }
\CommentTok{\#\textgreater{}                                                    }
\CommentTok{\#\textgreater{} educ                    0.1250         0.1256***   }
\CommentTok{\#\textgreater{}                        (0.0130)        (0.0229)    }
\CommentTok{\#\textgreater{}                                                    }
\CommentTok{\#\textgreater{} Constant              {-}4.1815***      {-}4.1484***   }
\CommentTok{\#\textgreater{}                        (0.2032)        (1.4109)    }
\CommentTok{\#\textgreater{}                                                    }
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} Observations              753             753      }
\CommentTok{\#\textgreater{} R2                      0.1582                     }
\CommentTok{\#\textgreater{} Adjusted R2             0.1482                     }
\CommentTok{\#\textgreater{} Log Likelihood                         {-}914.0777   }
\CommentTok{\#\textgreater{} rho                     0.0830      0.0505 (0.2317)}
\CommentTok{\#\textgreater{} Inverse Mills Ratio 0.0551 (0.2099)                }
\CommentTok{\#\textgreater{} ===================================================}
\CommentTok{\#\textgreater{} Note:                   *p\textless{}0.1; **p\textless{}0.05; ***p\textless{}0.01}


\FunctionTok{stargazer}\NormalTok{(}
\NormalTok{    ols1,}
    \CommentTok{\# heck1,}
    \CommentTok{\# ml1,}
\NormalTok{    manually\_est,}
    
    \AttributeTok{se =} \FunctionTok{list}\NormalTok{(}\FunctionTok{cse}\NormalTok{(ols1), }\ConstantTok{NULL}\NormalTok{, }\ConstantTok{NULL}\NormalTok{),}
    \AttributeTok{title =} \StringTok{"Married women\textquotesingle{}s wage regressions"}\NormalTok{,}
    \AttributeTok{type =} \StringTok{"text"}\NormalTok{,}
    \AttributeTok{df =} \ConstantTok{FALSE}\NormalTok{,}
    \AttributeTok{digits =} \DecValTok{4}\NormalTok{,}
    \AttributeTok{selection.equation =}\NormalTok{ T}
\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Married women\textquotesingle{}s wage regressions}
\CommentTok{\#\textgreater{} ================================================}
\CommentTok{\#\textgreater{}                         Dependent variable:     }
\CommentTok{\#\textgreater{}                     {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}                              log(wage)          }
\CommentTok{\#\textgreater{}                          (1)            (2)     }
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} educ                  0.1057***      0.1092***  }
\CommentTok{\#\textgreater{}                        (0.0130)      (0.0197)   }
\CommentTok{\#\textgreater{}                                                 }
\CommentTok{\#\textgreater{} exper                 0.0411***      0.0419***  }
\CommentTok{\#\textgreater{}                        (0.0154)      (0.0136)   }
\CommentTok{\#\textgreater{}                                                 }
\CommentTok{\#\textgreater{} I(exper2)              {-}0.0008*      {-}0.0008**  }
\CommentTok{\#\textgreater{}                        (0.0004)      (0.0004)   }
\CommentTok{\#\textgreater{}                                                 }
\CommentTok{\#\textgreater{} city                    0.0542        0.0510    }
\CommentTok{\#\textgreater{}                        (0.0653)      (0.0692)   }
\CommentTok{\#\textgreater{}                                                 }
\CommentTok{\#\textgreater{} IMR1                                  0.0551    }
\CommentTok{\#\textgreater{}                                      (0.2112)   }
\CommentTok{\#\textgreater{}                                                 }
\CommentTok{\#\textgreater{} Constant              {-}0.5308***      {-}0.6143   }
\CommentTok{\#\textgreater{}                        (0.2032)      (0.3769)   }
\CommentTok{\#\textgreater{}                                                 }
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} Observations             428            428     }
\CommentTok{\#\textgreater{} R2                      0.1581        0.1582    }
\CommentTok{\#\textgreater{} Adjusted R2             0.1501        0.1482    }
\CommentTok{\#\textgreater{} Residual Std. Error     0.6667        0.6674    }
\CommentTok{\#\textgreater{} F Statistic           19.8561***    15.8635***  }
\CommentTok{\#\textgreater{} ================================================}
\CommentTok{\#\textgreater{} Note:                *p\textless{}0.1; **p\textless{}0.05; ***p\textless{}0.01}
\end{Highlighting}
\end{Shaded}

Rho is an estimate of the correlation of the errors between the selection and wage equations. In the lower panel, the estimated coefficient on the inverse Mills ratio is given for the Heckman model. The fact that it is not statistically different from zero is consistent with the idea that selection bias was not a serious problem in this case.

If the estimated coefficient of the inverse Mills ratio in the Heckman model is not statistically different from zero, then selection bias was not a serious problem.

\hypertarget{example-2-2}{%
\subsubsection{Example 2}\label{example-2-2}}

This code is from \href{https://cran.r-project.org/web/packages/sampleSelection/vignettes/selection.pdf}{R package sampleSelection}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{0}\NormalTok{)}
\FunctionTok{library}\NormalTok{(}\StringTok{"sampleSelection"}\NormalTok{)}
\FunctionTok{library}\NormalTok{(}\StringTok{"mvtnorm"}\NormalTok{)}
\CommentTok{\# bivariate normal disturbances}
\NormalTok{eps }\OtherTok{\textless{}{-}}
    \FunctionTok{rmvnorm}\NormalTok{(}\DecValTok{500}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{), }\FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{0.7}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{0.7}\NormalTok{, }\DecValTok{1}\NormalTok{), }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)) }

\CommentTok{\# uniformly distributed explanatory variable }
\CommentTok{\# (vectors of explanatory variables for the selection)}
\NormalTok{xs }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\DecValTok{500}\NormalTok{)}

\CommentTok{\# probit data generating process}
\NormalTok{ys }\OtherTok{\textless{}{-}}\NormalTok{ xs }\SpecialCharTok{+}\NormalTok{ eps[, }\DecValTok{1}\NormalTok{] }\SpecialCharTok{\textgreater{}} \DecValTok{0} 

\CommentTok{\# vectors of explanatory variables for outcome equation}
\NormalTok{xo }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\DecValTok{500}\NormalTok{) }
\NormalTok{yoX }\OtherTok{\textless{}{-}}\NormalTok{ xo }\SpecialCharTok{+}\NormalTok{ eps[, }\DecValTok{2}\NormalTok{] }\CommentTok{\# latent outcome}
\NormalTok{yo }\OtherTok{\textless{}{-}}\NormalTok{ yoX }\SpecialCharTok{*}\NormalTok{ (ys }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{) }\CommentTok{\# observable outcome}
\CommentTok{\# true intercepts = 0 and our true slopes = 1}
\CommentTok{\# xs and xo are independent. }
\CommentTok{\# Hence, exclusion restriction is fulfilled}
\FunctionTok{summary}\NormalTok{(}\FunctionTok{selection}\NormalTok{(ys }\SpecialCharTok{\textasciitilde{}}\NormalTok{ xs, yo }\SpecialCharTok{\textasciitilde{}}\NormalTok{ xo))}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} Tobit 2 model (sample selection model)}
\CommentTok{\#\textgreater{} Maximum Likelihood estimation}
\CommentTok{\#\textgreater{} Newton{-}Raphson maximisation, 5 iterations}
\CommentTok{\#\textgreater{} Return code 1: gradient close to zero (gradtol)}
\CommentTok{\#\textgreater{} Log{-}Likelihood: {-}712.3163 }
\CommentTok{\#\textgreater{} 500 observations (172 censored and 328 observed)}
\CommentTok{\#\textgreater{} 6 free parameters (df = 494)}
\CommentTok{\#\textgreater{} Probit selection equation:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)  {-}0.2228     0.1081  {-}2.061   0.0399 *  }
\CommentTok{\#\textgreater{} xs            1.3377     0.2014   6.642 8.18e{-}11 ***}
\CommentTok{\#\textgreater{} Outcome equation:}
\CommentTok{\#\textgreater{}               Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept) {-}0.0002265  0.1294178  {-}0.002    0.999    }
\CommentTok{\#\textgreater{} xo           0.7299070  0.1635925   4.462 1.01e{-}05 ***}
\CommentTok{\#\textgreater{}    Error terms:}
\CommentTok{\#\textgreater{}       Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} sigma   0.9190     0.0574  16.009  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} rho    {-}0.5392     0.1521  {-}3.544 0.000431 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\end{Highlighting}
\end{Shaded}

without the exclusion restriction, we generate yo using xs instead of xo.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{yoX }\OtherTok{\textless{}{-}}\NormalTok{ xs }\SpecialCharTok{+}\NormalTok{ eps[,}\DecValTok{2}\NormalTok{]}
\NormalTok{yo }\OtherTok{\textless{}{-}}\NormalTok{ yoX}\SpecialCharTok{*}\NormalTok{(ys }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(}\FunctionTok{selection}\NormalTok{(ys }\SpecialCharTok{\textasciitilde{}}\NormalTok{ xs, yo }\SpecialCharTok{\textasciitilde{}}\NormalTok{ xs))}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} Tobit 2 model (sample selection model)}
\CommentTok{\#\textgreater{} Maximum Likelihood estimation}
\CommentTok{\#\textgreater{} Newton{-}Raphson maximisation, 14 iterations}
\CommentTok{\#\textgreater{} Return code 8: successive function values within relative tolerance limit (reltol)}
\CommentTok{\#\textgreater{} Log{-}Likelihood: {-}712.8298 }
\CommentTok{\#\textgreater{} 500 observations (172 censored and 328 observed)}
\CommentTok{\#\textgreater{} 6 free parameters (df = 494)}
\CommentTok{\#\textgreater{} Probit selection equation:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)  {-}0.1984     0.1114  {-}1.781   0.0756 .  }
\CommentTok{\#\textgreater{} xs            1.2907     0.2085   6.191 1.25e{-}09 ***}
\CommentTok{\#\textgreater{} Outcome equation:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)   }
\CommentTok{\#\textgreater{} (Intercept)  {-}0.5499     0.5644  {-}0.974  0.33038   }
\CommentTok{\#\textgreater{} xs            1.3987     0.4482   3.120  0.00191 **}
\CommentTok{\#\textgreater{}    Error terms:}
\CommentTok{\#\textgreater{}       Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} sigma  0.85091    0.05352  15.899   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} rho   {-}0.13226    0.72684  {-}0.182    0.856    }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\end{Highlighting}
\end{Shaded}

We can see that our estimates are still unbiased but standard errors are substantially larger. The exclusion restriction (i.e., independent information about the selection process) has a certain identifying power that we desire. Hence, it's better to have different set of variable for the selection process from the interested equation. Without the exclusion restriction, we solely rely on the functional form identification.

\hypertarget{tobit-5}{%
\subsection{Tobit-5}\label{tobit-5}}

Also known as the switching regression model\\
Condition: There is at least one variable in X in the selection process not included in the observed process. Used when there are separate models for participants, and non-participants.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{0}\NormalTok{)}
\NormalTok{vc }\OtherTok{\textless{}{-}} \FunctionTok{diag}\NormalTok{(}\DecValTok{3}\NormalTok{)}
\NormalTok{vc[}\FunctionTok{lower.tri}\NormalTok{(vc)] }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{0.9}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.1}\NormalTok{)}
\NormalTok{vc[}\FunctionTok{upper.tri}\NormalTok{(vc)] }\OtherTok{\textless{}{-}}\NormalTok{ vc[}\FunctionTok{lower.tri}\NormalTok{(vc)]}

\CommentTok{\# 3 disturbance vectors by a 3{-}dimensional normal distribution}
\NormalTok{eps }\OtherTok{\textless{}{-}} \FunctionTok{rmvnorm}\NormalTok{(}\DecValTok{500}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{), vc) }
\NormalTok{xs }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\DecValTok{500}\NormalTok{) }\CommentTok{\# uniformly distributed on [0, 1]}
\NormalTok{ys }\OtherTok{\textless{}{-}}\NormalTok{ xs }\SpecialCharTok{+}\NormalTok{ eps[,}\DecValTok{1}\NormalTok{] }\SpecialCharTok{\textgreater{}} \DecValTok{0}
\NormalTok{xo1 }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\DecValTok{500}\NormalTok{) }\CommentTok{\# uniformly distributed on [0, 1]}
\NormalTok{yo1 }\OtherTok{\textless{}{-}}\NormalTok{ xo1 }\SpecialCharTok{+}\NormalTok{ eps[,}\DecValTok{2}\NormalTok{]}
\NormalTok{xo2 }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\DecValTok{500}\NormalTok{) }\CommentTok{\# uniformly distributed on [0, 1]}
\NormalTok{yo2 }\OtherTok{\textless{}{-}}\NormalTok{ xo2 }\SpecialCharTok{+}\NormalTok{ eps[,}\DecValTok{3}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

exclusion restriction is fulfilled when \(x\)'s are independent.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# one selection equation and a list of two outcome equations}
\FunctionTok{summary}\NormalTok{(}\FunctionTok{selection}\NormalTok{(ys}\SpecialCharTok{\textasciitilde{}}\NormalTok{xs, }\FunctionTok{list}\NormalTok{(yo1 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ xo1, yo2 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ xo2))) }
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} Tobit 5 model (switching regression model)}
\CommentTok{\#\textgreater{} Maximum Likelihood estimation}
\CommentTok{\#\textgreater{} Newton{-}Raphson maximisation, 11 iterations}
\CommentTok{\#\textgreater{} Return code 1: gradient close to zero (gradtol)}
\CommentTok{\#\textgreater{} Log{-}Likelihood: {-}895.8201 }
\CommentTok{\#\textgreater{} 500 observations: 172 selection 1 (FALSE) and 328 selection 2 (TRUE)}
\CommentTok{\#\textgreater{} 10 free parameters (df = 490)}
\CommentTok{\#\textgreater{} Probit selection equation:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)  {-}0.1550     0.1051  {-}1.474    0.141    }
\CommentTok{\#\textgreater{} xs            1.1408     0.1785   6.390 3.86e{-}10 ***}
\CommentTok{\#\textgreater{} Outcome equation 1:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)  0.02708    0.16395   0.165    0.869    }
\CommentTok{\#\textgreater{} xo1          0.83959    0.14968   5.609  3.4e{-}08 ***}
\CommentTok{\#\textgreater{} Outcome equation 2:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)   0.1583     0.1885   0.840    0.401    }
\CommentTok{\#\textgreater{} xo2           0.8375     0.1707   4.908 1.26e{-}06 ***}
\CommentTok{\#\textgreater{}    Error terms:}
\CommentTok{\#\textgreater{}        Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} sigma1  0.93191    0.09211  10.118   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} sigma2  0.90697    0.04434  20.455   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} rho1    0.88988    0.05353  16.623   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} rho2    0.17695    0.33139   0.534    0.594    }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\end{Highlighting}
\end{Shaded}

All the estimates are close to the true values.

Example of functional form misspecification

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{5}\NormalTok{)}
\NormalTok{eps }\OtherTok{\textless{}{-}} \FunctionTok{rmvnorm}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{3}\NormalTok{), vc)}
\NormalTok{eps }\OtherTok{\textless{}{-}}\NormalTok{ eps}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{{-}} \DecValTok{1} \CommentTok{\# subtract 1 in order to get the mean zero disturbances}

\CommentTok{\# interval [âˆ’1, 0] to get an asymmetric distribution over observed choices}
\NormalTok{xs }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{) }
\NormalTok{ys }\OtherTok{\textless{}{-}}\NormalTok{ xs }\SpecialCharTok{+}\NormalTok{ eps[,}\DecValTok{1}\NormalTok{] }\SpecialCharTok{\textgreater{}} \DecValTok{0}
\NormalTok{xo1 }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\DecValTok{1000}\NormalTok{)}
\NormalTok{yo1 }\OtherTok{\textless{}{-}}\NormalTok{ xo1 }\SpecialCharTok{+}\NormalTok{ eps[,}\DecValTok{2}\NormalTok{]}
\NormalTok{xo2 }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\DecValTok{1000}\NormalTok{)}
\NormalTok{yo2 }\OtherTok{\textless{}{-}}\NormalTok{ xo2 }\SpecialCharTok{+}\NormalTok{ eps[,}\DecValTok{3}\NormalTok{]}
\FunctionTok{summary}\NormalTok{(}\FunctionTok{selection}\NormalTok{(ys}\SpecialCharTok{\textasciitilde{}}\NormalTok{xs, }\FunctionTok{list}\NormalTok{(yo1 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ xo1, yo2 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ xo2), }\AttributeTok{iterlim=}\DecValTok{20}\NormalTok{))}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} Tobit 5 model (switching regression model)}
\CommentTok{\#\textgreater{} Maximum Likelihood estimation}
\CommentTok{\#\textgreater{} Newton{-}Raphson maximisation, 4 iterations}
\CommentTok{\#\textgreater{} Return code 3: Last step could not find a value above the current.}
\CommentTok{\#\textgreater{} Boundary of parameter space?  }
\CommentTok{\#\textgreater{} Consider switching to a more robust optimisation method temporarily.}
\CommentTok{\#\textgreater{} Log{-}Likelihood: {-}1665.936 }
\CommentTok{\#\textgreater{} 1000 observations: 760 selection 1 (FALSE) and 240 selection 2 (TRUE)}
\CommentTok{\#\textgreater{} 10 free parameters (df = 990)}
\CommentTok{\#\textgreater{} Probit selection equation:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept) {-}0.53698    0.05808  {-}9.245  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} xs           0.31268    0.09395   3.328 0.000906 ***}
\CommentTok{\#\textgreater{} Outcome equation 1:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept) {-}0.70679    0.03573  {-}19.78   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} xo1          0.91603    0.05626   16.28   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} Outcome equation 2:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)  }
\CommentTok{\#\textgreater{} (Intercept)   0.1446        NaN     NaN      NaN  }
\CommentTok{\#\textgreater{} xo2           1.1196     0.5014   2.233   0.0258 *}
\CommentTok{\#\textgreater{}    Error terms:}
\CommentTok{\#\textgreater{}        Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} sigma1  0.67770    0.01760   38.50   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} sigma2  2.31432    0.07615   30.39   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} rho1   {-}0.97137        NaN     NaN      NaN    }
\CommentTok{\#\textgreater{} rho2    0.17039        NaN     NaN      NaN    }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\end{Highlighting}
\end{Shaded}

Although we still have an exclusion restriction (xo1 and xo2 are independent), we now have problems with the intercepts (i.e., they are statistically significantly different from the true values zero), and convergence problems.

If we don't have the exclusion restriction, we will have a larger variance of xs

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{6}\NormalTok{)}
\NormalTok{xs }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{ys }\OtherTok{\textless{}{-}}\NormalTok{ xs }\SpecialCharTok{+}\NormalTok{ eps[,}\DecValTok{1}\NormalTok{] }\SpecialCharTok{\textgreater{}} \DecValTok{0}
\NormalTok{yo1 }\OtherTok{\textless{}{-}}\NormalTok{ xs }\SpecialCharTok{+}\NormalTok{ eps[,}\DecValTok{2}\NormalTok{]}
\NormalTok{yo2 }\OtherTok{\textless{}{-}}\NormalTok{ xs }\SpecialCharTok{+}\NormalTok{ eps[,}\DecValTok{3}\NormalTok{]}
\FunctionTok{summary}\NormalTok{(tmp }\OtherTok{\textless{}{-}} \FunctionTok{selection}\NormalTok{(ys}\SpecialCharTok{\textasciitilde{}}\NormalTok{xs, }\FunctionTok{list}\NormalTok{(yo1 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ xs, yo2 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ xs), }\AttributeTok{iterlim=}\DecValTok{20}\NormalTok{))}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} Tobit 5 model (switching regression model)}
\CommentTok{\#\textgreater{} Maximum Likelihood estimation}
\CommentTok{\#\textgreater{} Newton{-}Raphson maximisation, 16 iterations}
\CommentTok{\#\textgreater{} Return code 8: successive function values within relative tolerance limit (reltol)}
\CommentTok{\#\textgreater{} Log{-}Likelihood: {-}1936.431 }
\CommentTok{\#\textgreater{} 1000 observations: 626 selection 1 (FALSE) and 374 selection 2 (TRUE)}
\CommentTok{\#\textgreater{} 10 free parameters (df = 990)}
\CommentTok{\#\textgreater{} Probit selection equation:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)  {-}0.3528     0.0424  {-}8.321 2.86e{-}16 ***}
\CommentTok{\#\textgreater{} xs            0.8354     0.0756  11.050  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} Outcome equation 1:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept) {-}0.55448    0.06339  {-}8.748   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} xs           0.81764    0.06048  13.519   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} Outcome equation 2:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)}
\CommentTok{\#\textgreater{} (Intercept)   0.6457     0.4994   1.293    0.196}
\CommentTok{\#\textgreater{} xs            0.3520     0.3197   1.101    0.271}
\CommentTok{\#\textgreater{}    Error terms:}
\CommentTok{\#\textgreater{}        Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} sigma1  0.59187    0.01853  31.935   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} sigma2  1.97257    0.07228  27.289   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} rho1    0.15568    0.15914   0.978    0.328    }
\CommentTok{\#\textgreater{} rho2   {-}0.01541    0.23370  {-}0.066    0.947    }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\end{Highlighting}
\end{Shaded}

Usually it will not converge. Even if it does, the results may be seriously biased.

\textbf{Note}

The log-likelihood function of the models might not be globally concave. Hence, it might not converge, or converge to a local maximum. To combat this, we can use

\begin{itemize}
\tightlist
\item
  Different starting value
\item
  Different maximization methods.
\item
  refer to \protect\hyperlink{non-linear-least-squares}{Non-linear Least Squares} for suggestions.
\end{itemize}

\hypertarget{pattern-mixture-models}{%
\paragraph{Pattern-Mixture Models}\label{pattern-mixture-models}}

\begin{itemize}
\tightlist
\item
  compared to the Heckman's model where it assumes the value of the missing data is predetermined, pattern-mixture models assume missingness affect the distribution of variable of interest (e.g., Y)
\item
  To read more, you can check \href{https://www4.stat.ncsu.edu/~davidian/st790/notes/chap6.pdf}{NCSU}, \href{https://stefvanbuuren.name/fimd/sec-nonignorable.html}{stefvanbuuren}.
\end{itemize}

\hypertarget{other-biases}{%
\chapter{Other Biases}\label{other-biases}}

In econometrics, the main objective is often to uncover causal relationships. However, coefficient estimates can be affected by various biases. Here's a list of common biases that can affect coefficient estimates:

What we've covered so far (see \protect\hyperlink{linear-regression}{Linear Regression} and s\protect\hyperlink{endogeneity}{Endogeneity}):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Omitted Variable Bias (OVB)}:

  \begin{itemize}
  \tightlist
  \item
    Arises when a variable that affects the dependent variable and is correlated with an independent variable is left out of the regression.
  \end{itemize}
\item
  \textbf{Endogeneity Bias}:

  \begin{itemize}
  \item
    Occurs when an error term is correlated with an independent variable. This can be due to:

    \begin{itemize}
    \item
      Simultaneity: When the dependent variable simultaneously affects an independent variable.
    \item
      Omitted variables.
    \item
      Measurement error in the independent variable.
    \end{itemize}
  \end{itemize}
\item
  \textbf{Measurement Error}:

  \begin{itemize}
  \tightlist
  \item
    Bias introduced when variables in a model are measured with error. If the error is in an independent variable and is classical (mean zero and uncorrelated with the true value), it typically biases the coefficient towards zero.
  \end{itemize}
\item
  \textbf{Sample Selection Bias}:

  \begin{itemize}
  \tightlist
  \item
    Arises when the sample is not randomly selected and the selection is related to the dependent variable. A classic example is the Heckman correction for labor market studies where participants self-select into the workforce.
  \end{itemize}
\item
  \textbf{Simultaneity Bias (or Reverse Causality)}:

  \begin{itemize}
  \tightlist
  \item
    Happens when the dependent variable causes changes in the independent variable, leading to a two-way causation.
  \end{itemize}
\item
  \textbf{Multicollinearity}:

  \begin{itemize}
  \tightlist
  \item
    Not a bias in the strictest sense, but in the presence of high multicollinearity (when independent variables are highly correlated), coefficient estimates can become unstable and standard errors large. This makes it hard to determine the individual effect of predictors on the dependent variable.
  \end{itemize}
\item
  \textbf{Specification Errors}:

  \begin{itemize}
  \tightlist
  \item
    Arise when the functional form of the model is incorrectly specified, e.g., omitting interaction terms or polynomial terms when they are needed.
  \end{itemize}
\item
  \textbf{Autocorrelation (or Serial Correlation)}:

  \begin{itemize}
  \tightlist
  \item
    Occurs in time-series data when the error terms are correlated over time. This doesn't cause bias in the coefficient estimates of OLS, but it can make standard errors biased, leading to incorrect inference.
  \end{itemize}
\item
  \textbf{Heteroskedasticity}:

  \begin{itemize}
  \tightlist
  \item
    Occurs when the variance of the error term is not constant across observations. Like autocorrelation, heteroskedasticity doesn't bias the OLS estimates but can bias standard errors.
  \end{itemize}
\end{enumerate}

In this section, we will mention other biases that you may encounter when conducting your research

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{9}
\tightlist
\item
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Introduced when data are aggregated, and analysis is conducted at this aggregate level rather than the individual level.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{10}
\tightlist
\item
  {[}\textbf{Survivorship Bias}{]} \textbf{(very much related to Sample Selection)}:
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Arises when the sample only includes ``survivors'' or those who ``passed'' a certain threshold. Common in finance where only funds or firms that ``survive'' are analyzed.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{11}
\tightlist
\item
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Not a bias in econometric estimation per se, but relevant in the context of empirical studies. It refers to the tendency for journals to publish only significant or positive results, leading to an overrepresentation of such results in the literature.
\end{itemize}

\hypertarget{aggregation-bias}{%
\section{Aggregation Bias}\label{aggregation-bias}}

Aggregation bias, also known as ecological fallacy, refers to the error introduced when data are aggregated and an analysis is conducted at this aggregate level, rather than at the individual level. This can be especially problematic in econometrics, where analysts are often concerned with understanding individual behavior.

When the relationship between variables is different at the aggregate level than at the individual level, aggregation bias can result. The bias arises when inferences about individual behaviors are made based on aggregate data.

\textbf{Example}: Suppose we have data on individuals\textquotesingle{} incomes and their personal consumption. At the individual level, it's possible that as income rises, consumption also rises. However, when we aggregate the data to, say, a neighborhood level, neighborhoods with diverse income levels might all have similar average consumptions due to other unobserved factors.

\textbf{Step 1}: Create individual level data

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\# Generate data for 1000 individuals}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{1000}

\NormalTok{income }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =} \DecValTok{50}\NormalTok{, }\AttributeTok{sd =} \DecValTok{10}\NormalTok{)}
\NormalTok{consumption }\OtherTok{\textless{}{-}} \FloatTok{0.5} \SpecialCharTok{*}\NormalTok{ income }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{5}\NormalTok{)}

\CommentTok{\# Individual level regression}
\NormalTok{individual\_lm }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(consumption }\SpecialCharTok{\textasciitilde{}}\NormalTok{ income)}
\FunctionTok{summary}\NormalTok{(individual\_lm)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = consumption \textasciitilde{} income)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}      Min       1Q   Median       3Q      Max }
\CommentTok{\#\textgreater{} {-}15.1394  {-}3.4572   0.0213   3.5436  16.4557 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept) {-}1.99596    0.82085  {-}2.432   0.0152 *  }
\CommentTok{\#\textgreater{} income       0.54402    0.01605  33.888   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 5.032 on 998 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.535,  Adjusted R{-}squared:  0.5346 }
\CommentTok{\#\textgreater{} F{-}statistic:  1148 on 1 and 998 DF,  p{-}value: \textless{} 2.2e{-}16}
\end{Highlighting}
\end{Shaded}

This would show a significant positive relationship between income and consumption.

\textbf{Step 2}: Aggregate data to `neighborhood' level

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Assume 100 neighborhoods with 10 individuals each}
\NormalTok{n\_neighborhoods }\OtherTok{\textless{}{-}} \DecValTok{100}

\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(income, consumption)}
\NormalTok{df}\SpecialCharTok{$}\NormalTok{neighborhood }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_neighborhoods, }\AttributeTok{each =}\NormalTok{ n }\SpecialCharTok{/}\NormalTok{ n\_neighborhoods)}

\NormalTok{aggregate\_data }\OtherTok{\textless{}{-}} \FunctionTok{aggregate}\NormalTok{(. }\SpecialCharTok{\textasciitilde{}}\NormalTok{ neighborhood, }\AttributeTok{data =}\NormalTok{ df, }\AttributeTok{FUN =}\NormalTok{ mean)}

\CommentTok{\# Aggregate level regression}
\NormalTok{aggregate\_lm }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(consumption }\SpecialCharTok{\textasciitilde{}}\NormalTok{ income, }\AttributeTok{data =}\NormalTok{ aggregate\_data)}
\FunctionTok{summary}\NormalTok{(aggregate\_lm)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = consumption \textasciitilde{} income, data = aggregate\_data)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}     Min      1Q  Median      3Q     Max }
\CommentTok{\#\textgreater{} {-}4.4517 {-}0.9322 {-}0.0826  1.0556  3.5728 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept) {-}4.94338    2.60699  {-}1.896   0.0609 .  }
\CommentTok{\#\textgreater{} income       0.60278    0.05188  11.618   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 1.54 on 98 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.5794, Adjusted R{-}squared:  0.5751 }
\CommentTok{\#\textgreater{} F{-}statistic:   135 on 1 and 98 DF,  p{-}value: \textless{} 2.2e{-}16}
\end{Highlighting}
\end{Shaded}

If aggregation bias is present, the coefficient for income in the aggregate regression might be different from the coefficient in the individual regression, even if the individual relationship is significant and strong.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}

\CommentTok{\# Individual scatterplot}
\NormalTok{p1 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(df, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ income, }\AttributeTok{y =}\NormalTok{ consumption)) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color =}\NormalTok{ neighborhood), }\AttributeTok{alpha =} \FloatTok{0.6}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{,}
                \AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{,}
                \AttributeTok{color =} \StringTok{"black"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Individual Level Data"}\NormalTok{) }\SpecialCharTok{+}
\NormalTok{    causalverse}\SpecialCharTok{::}\FunctionTok{ama\_theme}\NormalTok{()}

\CommentTok{\# Aggregate scatterplot}
\NormalTok{p2 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(aggregate\_data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ income, }\AttributeTok{y =}\NormalTok{ consumption)) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{color =} \StringTok{"red"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{,}
                \AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{,}
                \AttributeTok{color =} \StringTok{"black"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Aggregate Level Data"}\NormalTok{) }\SpecialCharTok{+}
\NormalTok{    causalverse}\SpecialCharTok{::}\FunctionTok{ama\_theme}\NormalTok{()}

\CommentTok{\# print(p1)}
\CommentTok{\# print(p2)}

\NormalTok{gridExtra}\SpecialCharTok{::}\FunctionTok{grid.arrange}\NormalTok{(}\AttributeTok{grobs =} \FunctionTok{list}\NormalTok{(p1, p2), }\AttributeTok{ncol =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{32-biases_files/figure-latex/unnamed-chunk-3-1} \end{center}

From these plots, you can see the relationship at the individual level, with each neighborhood being colored differently in the first plot. The second plot shows the aggregate data, where each point now represents a whole neighborhood.

\textbf{Direction of Bias}: The direction of the aggregation bias isn't predetermined. It depends on the underlying relationship and the data distribution. In some cases, aggregation might attenuate (reduce) a relationship, while in other cases, it might exaggerate it.

\textbf{Relation to Other Biases}: Aggregation bias is closely related to several other biases in econometrics:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Specification bias}: If you don't properly account for the hierarchical structure of your data (like individuals nested within neighborhoods), your model might be mis-specified, leading to biased estimates.
\item
  \textbf{\protect\hyperlink{measurement-error}{Measurement Error}}: Aggregation can introduce or amplify measurement errors. For instance, if you aggregate noisy measures, the aggregate might not accurately represent any underlying signal.
\item
  \textbf{Omitted Variable Bias (see \protect\hyperlink{endogeneity}{Endogeneity})}: When you aggregate data, you lose information. If the loss of this information results in omitting important predictors that are correlated with both the independent and dependent variables, it can introduce omitted variable bias.
\item
\end{enumerate}

\hypertarget{simpsons-paradox}{%
\subsection{Simpson's Paradox}\label{simpsons-paradox}}

Simpson's Paradox, also known as the Yule-Simpson effect, is a phenomenon in probability and statistics where a trend that appears in different groups of data disappears or reverses when the groups are combined. It's a striking example of how aggregated data can sometimes provide a misleading representation of the actual situation.

\textbf{Illustration of Simpson's Paradox:}

Consider a hypothetical scenario involving two hospitals: Hospital A and Hospital B. We want to analyze the success rates of treatments at both hospitals. When we break the data down by the severity of the cases (i.e., minor cases vs.~major cases):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Hospital A}:

  \begin{itemize}
  \item
    Minor cases: 95\% success rate
  \item
    Major cases: 80\% success rate
  \end{itemize}
\item
  \textbf{Hospital B}:

  \begin{itemize}
  \item
    Minor cases: 90\% success rate
  \item
    Major cases: 85\% success rate
  \end{itemize}
\end{enumerate}

From this breakdown, Hospital A appears to be better in treating both minor and major cases since it has a higher success rate in both categories.

However, let's consider the overall success rates without considering case severity:

\begin{itemize}
\item
  \textbf{Hospital A}: 83\% overall success rate
\item
  \textbf{Hospital B}: 86\% overall success rate
\end{itemize}

Suddenly, Hospital B seems better overall. This surprising reversal happens because the two hospitals might handle very different proportions of minor and major cases. For example, if Hospital A treats many more major cases (which have lower success rates) than Hospital B, it can drag down its overall success rate.

\textbf{Causes}:

Simpson's Paradox can arise due to:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  A lurking or confounding variable that wasn't initially considered (in our example, the severity of the medical cases).
\item
  Different group sizes, where one group might be much larger than the other, influencing the aggregate results.
\end{enumerate}

\textbf{Implications}:

Simpson's Paradox highlights the dangers of interpreting aggregated data without considering potential underlying sub-group structures. It underscores the importance of disaggregating data and being aware of the context in which it's analyzed.

\textbf{Relation to \href{}{Aggregation Bias}}

In the most extreme case, aggregation bias can reverse the coefficient sign of the relationship of interest (i.e., Simpson's Paradox).

\textbf{Example}: Suppose we are studying the effect of a new study technique on student grades. We have two groups of students: those who used the new technique (\textbf{\texttt{treatment\ =\ 1}}) and those who did not (\textbf{\texttt{treatment\ =\ 0}}). We want to see if using the new study technique is related to higher grades.

Let's assume grades are influenced by the starting ability of the students. Perhaps in our sample, many high-ability students didn't use the new technique (because they felt they didn't need it), while many low-ability students did.

Here's a setup:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  High-ability students tend to have high grades regardless of the technique.
\item
  The new technique has a positive effect on grades, but this is masked by the fact that many low-ability students use it.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\# Generate data for 1000 students}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{1000}

\CommentTok{\# 500 students are of high ability, 500 of low ability}
\NormalTok{ability }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\StringTok{"high"}\NormalTok{, }\DecValTok{500}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\StringTok{"low"}\NormalTok{, }\DecValTok{500}\NormalTok{))}

\CommentTok{\# High ability students are less likely to use the technique}
\NormalTok{treatment }\OtherTok{\textless{}{-}}
  \FunctionTok{ifelse}\NormalTok{(ability }\SpecialCharTok{==} \StringTok{"high"}\NormalTok{, }\FunctionTok{rbinom}\NormalTok{(}\DecValTok{500}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.2}\NormalTok{), }\FunctionTok{rbinom}\NormalTok{(}\DecValTok{500}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.8}\NormalTok{))}

\CommentTok{\# Grades are influenced by ability and treatment (new technique),}
\CommentTok{\# but the treatment has opposite effects based on ability.}
\NormalTok{grades }\OtherTok{\textless{}{-}}
  \FunctionTok{ifelse}\NormalTok{(}
\NormalTok{    ability }\SpecialCharTok{==} \StringTok{"high"}\NormalTok{,}
    \FunctionTok{rnorm}\NormalTok{(}\DecValTok{500}\NormalTok{, }\AttributeTok{mean =} \DecValTok{85}\NormalTok{, }\AttributeTok{sd =} \DecValTok{5}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ treatment }\SpecialCharTok{*} \SpecialCharTok{{-}}\DecValTok{3}\NormalTok{,}
    \FunctionTok{rnorm}\NormalTok{(}\DecValTok{500}\NormalTok{, }\AttributeTok{mean =} \DecValTok{60}\NormalTok{, }\AttributeTok{sd =} \DecValTok{5}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ treatment }\SpecialCharTok{*} \DecValTok{5}
\NormalTok{  )}

\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(ability, treatment, grades)}

\CommentTok{\# Regression without considering ability}
\NormalTok{overall\_lm }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(grades }\SpecialCharTok{\textasciitilde{}} \FunctionTok{factor}\NormalTok{(treatment), }\AttributeTok{data =}\NormalTok{ df)}
\FunctionTok{summary}\NormalTok{(overall\_lm)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = grades \textasciitilde{} factor(treatment), data = df)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}     Min      1Q  Median      3Q     Max }
\CommentTok{\#\textgreater{} {-}33.490  {-}4.729   0.986   6.368  25.607 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}                    Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)         80.0133     0.4373   183.0   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} factor(treatment)1 {-}11.7461     0.6248   {-}18.8   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 9.877 on 998 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.2615, Adjusted R{-}squared:  0.2608 }
\CommentTok{\#\textgreater{} F{-}statistic: 353.5 on 1 and 998 DF,  p{-}value: \textless{} 2.2e{-}16}

\CommentTok{\# Regression within ability groups}
\NormalTok{high\_ability\_lm }\OtherTok{\textless{}{-}}
  \FunctionTok{lm}\NormalTok{(grades }\SpecialCharTok{\textasciitilde{}} \FunctionTok{factor}\NormalTok{(treatment), }\AttributeTok{data =}\NormalTok{ df[df}\SpecialCharTok{$}\NormalTok{ability }\SpecialCharTok{==} \StringTok{"high"}\NormalTok{,])}
\NormalTok{low\_ability\_lm }\OtherTok{\textless{}{-}}
  \FunctionTok{lm}\NormalTok{(grades }\SpecialCharTok{\textasciitilde{}} \FunctionTok{factor}\NormalTok{(treatment), }\AttributeTok{data =}\NormalTok{ df[df}\SpecialCharTok{$}\NormalTok{ability }\SpecialCharTok{==} \StringTok{"low"}\NormalTok{,])}
\FunctionTok{summary}\NormalTok{(high\_ability\_lm)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = grades \textasciitilde{} factor(treatment), data = df[df$ability == }
\CommentTok{\#\textgreater{}     "high", ])}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}      Min       1Q   Median       3Q      Max }
\CommentTok{\#\textgreater{} {-}14.2156  {-}3.4813   0.1186   3.4952  13.2919 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}                    Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)         85.1667     0.2504 340.088  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} factor(treatment)1  {-}3.9489     0.5776  {-}6.837 2.37e{-}11 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 5.046 on 498 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.08581,    Adjusted R{-}squared:  0.08398 }
\CommentTok{\#\textgreater{} F{-}statistic: 46.75 on 1 and 498 DF,  p{-}value: 2.373e{-}11}
\FunctionTok{summary}\NormalTok{(low\_ability\_lm)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = grades \textasciitilde{} factor(treatment), data = df[df$ability == }
\CommentTok{\#\textgreater{}     "low", ])}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}      Min       1Q   Median       3Q      Max }
\CommentTok{\#\textgreater{} {-}13.3717  {-}3.5413   0.1097   3.3531  17.0568 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}                    Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)         59.8950     0.4871 122.956   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} factor(treatment)1   5.2979     0.5474   9.679   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 4.968 on 498 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.1583, Adjusted R{-}squared:  0.1566 }
\CommentTok{\#\textgreater{} F{-}statistic: 93.68 on 1 and 498 DF,  p{-}value: \textless{} 2.2e{-}16}
\end{Highlighting}
\end{Shaded}

From this simulation:

\begin{itemize}
\item
  The \textbf{\texttt{overall\_lm}} might show that the new study technique is associated with lower grades (negative coefficient), because many of the high-ability students (who naturally have high grades) did not use it.
\item
  The \textbf{\texttt{high\_ability\_lm}} will likely show that high-ability students who used the technique had slightly lower grades than high-ability students who didn't.
\item
  The \textbf{\texttt{low\_ability\_lm}} will likely show that low-ability students who used the technique had much higher grades than low-ability students who didn't.
\end{itemize}

This is a classic example of Simpson's Paradox: within each ability group, the technique appears beneficial, but when data is aggregated, the effect seems negative because of the distribution of the technique across ability groups.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}

\CommentTok{\# Scatterplot for overall data}
\NormalTok{p1 }\OtherTok{\textless{}{-}}
  \FunctionTok{ggplot}\NormalTok{(df, }\FunctionTok{aes}\NormalTok{(}
    \AttributeTok{x =} \FunctionTok{factor}\NormalTok{(treatment),}
    \AttributeTok{y =}\NormalTok{ grades,}
    \AttributeTok{color =}\NormalTok{ ability}
\NormalTok{  )) }\SpecialCharTok{+}
  \FunctionTok{geom\_jitter}\NormalTok{(}\AttributeTok{width =} \FloatTok{0.2}\NormalTok{, }\AttributeTok{height =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.6}\NormalTok{, }\AttributeTok{outlier.shape =} \ConstantTok{NA}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Overall Effect of Study Technique on Grades"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Treatment (0 = No Technique, 1 = New Technique)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Grades"}\NormalTok{)}

\CommentTok{\# Scatterplot for high{-}ability students}
\NormalTok{p2 }\OtherTok{\textless{}{-}}
  \FunctionTok{ggplot}\NormalTok{(df[df}\SpecialCharTok{$}\NormalTok{ability }\SpecialCharTok{==} \StringTok{"high"}\NormalTok{, ], }\FunctionTok{aes}\NormalTok{(}
    \AttributeTok{x =} \FunctionTok{factor}\NormalTok{(treatment),}
    \AttributeTok{y =}\NormalTok{ grades,}
    \AttributeTok{color =}\NormalTok{ ability}
\NormalTok{  )) }\SpecialCharTok{+}
  \FunctionTok{geom\_jitter}\NormalTok{(}\AttributeTok{width =} \FloatTok{0.2}\NormalTok{, }\AttributeTok{height =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.6}\NormalTok{, }\AttributeTok{outlier.shape =} \ConstantTok{NA}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Effect of Study Technique on Grades (High Ability)"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Treatment (0 = No Technique, 1 = New Technique)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Grades"}\NormalTok{)}

\CommentTok{\# Scatterplot for low{-}ability students}
\NormalTok{p3 }\OtherTok{\textless{}{-}}
  \FunctionTok{ggplot}\NormalTok{(df[df}\SpecialCharTok{$}\NormalTok{ability }\SpecialCharTok{==} \StringTok{"low"}\NormalTok{, ], }\FunctionTok{aes}\NormalTok{(}
    \AttributeTok{x =} \FunctionTok{factor}\NormalTok{(treatment),}
    \AttributeTok{y =}\NormalTok{ grades,}
    \AttributeTok{color =}\NormalTok{ ability}
\NormalTok{  )) }\SpecialCharTok{+}
  \FunctionTok{geom\_jitter}\NormalTok{(}\AttributeTok{width =} \FloatTok{0.2}\NormalTok{, }\AttributeTok{height =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.6}\NormalTok{, }\AttributeTok{outlier.shape =} \ConstantTok{NA}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Effect of Study Technique on Grades (Low Ability)"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Treatment (0 = No Technique, 1 = New Technique)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Grades"}\NormalTok{)}

\CommentTok{\# print(p1)}
\CommentTok{\# print(p2)}
\CommentTok{\# print(p3)}
\NormalTok{gridExtra}\SpecialCharTok{::}\FunctionTok{grid.arrange}\NormalTok{(}\AttributeTok{grobs =} \FunctionTok{list}\NormalTok{(p1, p2, p3), }\AttributeTok{ncol =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{32-biases_files/figure-latex/unnamed-chunk-5-1} \end{center}

\hypertarget{survivorship-bias}{%
\section{Survivorship Bias}\label{survivorship-bias}}

Survivorship bias refers to the logical error of concentrating on the entities that have made it past some selection process and overlooking those that didn't, typically because of a lack of visibility. This can skew results and lead to overly optimistic conclusions.

\textbf{Example}: If you were to analyze the success of companies based only on the ones that are still in business today, you'd miss out on the insights from all those that failed. This would give you a distorted view of what makes a successful company, as you wouldn't account for all those that had those same attributes but didn't succeed.

\textbf{Relation to Other Biases}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Sample Selection Bias}: Survivorship bias is a specific form of sample selection bias. While survivorship bias focuses on entities that ``survive'', sample selection bias broadly deals with any non-random sample.
\item
  \textbf{Confirmation Bias}: Survivorship bias can reinforce confirmation bias. By only looking at the ``winners'', we might confirm our existing beliefs about what leads to success, ignoring evidence to the contrary from those that didn't survive.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}

\CommentTok{\# Generating data for 100 companies}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{100}

\CommentTok{\# Randomly generate earnings; assume true average earnings is 50}
\NormalTok{earnings }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =} \DecValTok{50}\NormalTok{, }\AttributeTok{sd =} \DecValTok{10}\NormalTok{)}

\CommentTok{\# Threshold for bankruptcy}
\NormalTok{threshold }\OtherTok{\textless{}{-}} \DecValTok{40}

\CommentTok{\# Only companies with earnings above the threshold "survive"}
\NormalTok{survivor\_earnings }\OtherTok{\textless{}{-}}\NormalTok{ earnings[earnings }\SpecialCharTok{\textgreater{}}\NormalTok{ threshold]}

\CommentTok{\# Average earnings for all companies vs. survivors}
\NormalTok{true\_avg }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(earnings)}
\NormalTok{survivor\_avg }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(survivor\_earnings)}

\NormalTok{true\_avg}
\CommentTok{\#\textgreater{} [1] 50.32515}
\NormalTok{survivor\_avg}
\CommentTok{\#\textgreater{} [1] 53.3898}
\end{Highlighting}
\end{Shaded}

Using a histogram to visualize the distribution of earnings, highlighting the ``survivors''.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}

\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(earnings)}

\NormalTok{p }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(df, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ earnings)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}
    \AttributeTok{binwidth =} \DecValTok{2}\NormalTok{,}
    \AttributeTok{fill =} \StringTok{"grey"}\NormalTok{,}
    \AttributeTok{color =} \StringTok{"black"}\NormalTok{,}
    \AttributeTok{alpha =} \FloatTok{0.7}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{xintercept =}\NormalTok{ true\_avg, }\AttributeTok{color =} \StringTok{"True Avg"}\NormalTok{),}
             \AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{,}
             \AttributeTok{size =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}
    \FunctionTok{aes}\NormalTok{(}\AttributeTok{xintercept =}\NormalTok{ survivor\_avg, }\AttributeTok{color =} \StringTok{"Survivor Avg"}\NormalTok{),}
    \AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{,}
    \AttributeTok{size =} \DecValTok{1}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"True Avg"} \OtherTok{=} \StringTok{"blue"}\NormalTok{, }\StringTok{"Survivor Avg"} \OtherTok{=} \StringTok{"red"}\NormalTok{),}
                     \AttributeTok{name =} \StringTok{"Average Type"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Distribution of Company Earnings"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Earnings"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of Companies"}\NormalTok{) }\SpecialCharTok{+}
\NormalTok{  causalverse}\SpecialCharTok{::}\FunctionTok{ama\_theme}\NormalTok{()}

\FunctionTok{print}\NormalTok{(p)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{32-biases_files/figure-latex/unnamed-chunk-7-1} \end{center}

In the plot, the ``True Avg'' might be lower than the ``Survivor Avg'', indicating that by only looking at the survivors, we overestimate the average earnings.

\textbf{Remedies}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Awareness}: Recognizing the potential for survivorship bias is the first step.
\item
  \textbf{Inclusive Data Collection}: Wherever possible, try to include data from entities that didn't ``survive'' in your sample.
\item
  \textbf{Statistical Techniques}: In cases where the missing data is inherent, methods like Heckman's two-step procedure can be used to correct for sample selection bias.
\item
  \textbf{External Data Sources}: Sometimes, complementary datasets can provide insights into the missing ``non-survivors''.
\item
  \textbf{Sensitivity Analysis}: Test how sensitive your results are to assumptions about the non-survivors.
\end{enumerate}

\hypertarget{publication-bias}{%
\section{Publication Bias}\label{publication-bias}}

Publication bias occurs when the results of studies influence the likelihood of their being published. Typically, studies with significant, positive, or sensational results are more likely to be published than those with non-significant or negative results. This can skew the perceived effectiveness or results when researchers conduct meta-analyses or literature reviews, leading them to draw inaccurate conclusions.

\textbf{Example}: Imagine pharmaceutical research. If 10 studies are done on a new drug, and only 2 show a positive effect while 8 show no effect, but only the 2 positive studies get published, a later review of the literature might erroneously conclude the drug is effective.

\textbf{Relation to Other Biases}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Selection Bias}: Publication bias is a form of selection bias, where the selection (publication in this case) isn't random but based on the results of the study.
\item
  \textbf{Confirmation Bias}: Like survivorship bias, publication bias can reinforce confirmation bias. Researchers might only find and cite studies that confirm their beliefs, overlooking the unpublished studies that might contradict them.
\end{enumerate}

Let's simulate an experiment on a new treatment. We'll assume that the treatment has no effect, but due to random variation, some studies will show significant positive or negative effects.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}

\CommentTok{\# Number of studies}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{100}

\CommentTok{\# Assuming no real effect (effect size = 0)}
\NormalTok{true\_effect }\OtherTok{\textless{}{-}} \DecValTok{0}

\CommentTok{\# Random variation in results}
\NormalTok{results }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =}\NormalTok{ true\_effect, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)}

\CommentTok{\# Only "significant" results get published }
\CommentTok{\# (arbitrarily defining significant as abs(effect) \textgreater{} 1.5)}
\NormalTok{published\_results }\OtherTok{\textless{}{-}}\NormalTok{ results[}\FunctionTok{abs}\NormalTok{(results) }\SpecialCharTok{\textgreater{}} \FloatTok{1.5}\NormalTok{]}

\CommentTok{\# Average effect for all studies vs. published studies}
\NormalTok{true\_avg\_effect }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(results)}
\NormalTok{published\_avg\_effect }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(published\_results)}

\NormalTok{true\_avg\_effect}
\CommentTok{\#\textgreater{} [1] 0.03251482}
\NormalTok{published\_avg\_effect}
\CommentTok{\#\textgreater{} [1] {-}0.3819601}
\end{Highlighting}
\end{Shaded}

Using a histogram to visualize the distribution of study results, highlighting the ``published'' studies.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}

\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(results)}

\NormalTok{p }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(df, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ results)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}
    \AttributeTok{binwidth =} \FloatTok{0.2}\NormalTok{,}
    \AttributeTok{fill =} \StringTok{"grey"}\NormalTok{,}
    \AttributeTok{color =} \StringTok{"black"}\NormalTok{,}
    \AttributeTok{alpha =} \FloatTok{0.7}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}
    \FunctionTok{aes}\NormalTok{(}\AttributeTok{xintercept =}\NormalTok{ true\_avg\_effect,}
        \AttributeTok{color =} \StringTok{"True Avg Effect"}\NormalTok{),}
    \AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{,}
    \AttributeTok{size =} \DecValTok{1}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}
    \FunctionTok{aes}\NormalTok{(}\AttributeTok{xintercept =}\NormalTok{ published\_avg\_effect,}
        \AttributeTok{color =} \StringTok{"Published Avg Effect"}\NormalTok{),}
    \AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{,}
    \AttributeTok{size =} \DecValTok{1}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_manual}\NormalTok{(}
    \AttributeTok{values =} \FunctionTok{c}\NormalTok{(}
      \StringTok{"True Avg Effect"} \OtherTok{=} \StringTok{"blue"}\NormalTok{,}
      \StringTok{"Published Avg Effect"} \OtherTok{=} \StringTok{"red"}
\NormalTok{    ),}
    \AttributeTok{name =} \StringTok{"Effect Type"}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Distribution of Study Results"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Effect Size"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of Studies"}\NormalTok{) }\SpecialCharTok{+}
\NormalTok{  causalverse}\SpecialCharTok{::}\FunctionTok{ama\_theme}\NormalTok{()}

\FunctionTok{print}\NormalTok{(p)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{32-biases_files/figure-latex/unnamed-chunk-9-1} \end{center}

The plot might show that the ``True Avg Effect'' is around zero, while the ``Published Avg Effect'' is likely higher or lower, depending on which studies happen to have significant results in the simulation.

\textbf{Remedies}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Awareness}: Understand and accept that publication bias exists, especially when conducting literature reviews or meta-analyses.
\item
  \textbf{Study Registries}: Encourage the use of study registries where researchers register their studies before they start. This way, one can see all initiated studies, not just the published ones.
\item
  \textbf{Publish All Results}: Journals and researchers should make an effort to publish negative or null results. Some journals, known as ``null result journals'', specialize in this.
\item
  \textbf{Funnel Plots and Egger\textquotesingle s Test}: In meta-analyses, these are methods to visually and statistically detect publication bias.
\item
  \textbf{Use of Preprints}: Promote the use of preprint servers where researchers can upload studies before they're peer-reviewed, ensuring that results are available regardless of eventual publication status.
\end{enumerate}

\hypertarget{controls}{%
\chapter{Controls}\label{controls}}

This section follows \citep{cinelli2022crash} and \href{https://www.kaggle.com/code/carloscinelli/crash-course-in-good-and-bad-controls-linear-r/notebook\#Model-11---Bad-Control-(overcontrol-bias)}{code}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dagitty)}
\FunctionTok{library}\NormalTok{(ggdag)}
\end{Highlighting}
\end{Shaded}

Traditional literature usually considers adding additional control variables is harmless to analysis.

More specifically, this problem is most prevalent in the review process. Reviewers only ask authors to add more variables to ``control'' for such variable, which can be asked with only limited rationale. Rarely ever you will see a reviewer asks an author to remove some variables to see the behavior of the variable of interest (This is also related to \protect\hyperlink{coefficient-stability}{Coefficient stability}).

However, adding more controls is only good in limited cases.

\hypertarget{bad-controls}{%
\section{Bad Controls}\label{bad-controls}}

\hypertarget{m-bias}{%
\subsection{M-bias}\label{m-bias}}

Traditional textbooks \citep{imbens2015causal, angrist2009mostly} consider \(Z\) as a good control because it's a pre-treatment variable, where it correlates with the treatment and the outcome.

This is most prevalent in \protect\hyperlink{matching-methods}{Matching Methods}, where we are recommended to include all ``pre-treatment'' variables.

However, it is a bad control because it opens the back-door path \(Z \leftarrow U_1 \to Z \leftarrow U_2 \to Y\)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cleans workspace}
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\CommentTok{\# DAG}

\DocumentationTok{\#\# specify edges}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; u1{-}\textgreater{}x; u1{-}\textgreater{}z; u2{-}\textgreater{}z; u2{-}\textgreater{}y\}"}\NormalTok{)}

\CommentTok{\# set u as latent}
\FunctionTok{latents}\NormalTok{(model) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"u1"}\NormalTok{, }\StringTok{"u2"}\NormalTok{)}

\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}
    \AttributeTok{x =} \DecValTok{1}\NormalTok{,}
    \AttributeTok{u1 =} \DecValTok{1}\NormalTok{,}
    \AttributeTok{z =} \DecValTok{2}\NormalTok{,}
    \AttributeTok{u2 =} \DecValTok{3}\NormalTok{,}
    \AttributeTok{y =} \DecValTok{3}
\NormalTok{),}
\AttributeTok{y =} \FunctionTok{c}\NormalTok{(}
    \AttributeTok{x =} \DecValTok{1}\NormalTok{,}
    \AttributeTok{u1 =} \DecValTok{2}\NormalTok{,}
    \AttributeTok{z =} \FloatTok{1.5}\NormalTok{,}
    \AttributeTok{u2 =} \DecValTok{2}\NormalTok{,}
    \AttributeTok{y =} \DecValTok{1}
\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{33-controls_files/figure-latex/unnamed-chunk-2-1} \end{center}

Even though \(Z\) can correlate with both \(X\) and \(Y\) very well, it's not a confounder.

Controlling for \(Z\) can bias the \(X \to Y\) estimate, because it opens the colliding path \(X \leftarrow U_1 \rightarrow Z \leftarrow U_2 \leftarrow Y\)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \FloatTok{1e4}
\NormalTok{u1 }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{u2 }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{z }\OtherTok{\textless{}{-}}\NormalTok{ u1 }\SpecialCharTok{+}\NormalTok{ u2 }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{x }\OtherTok{\textless{}{-}}\NormalTok{ u1 }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{causal\_coef }\OtherTok{\textless{}{-}} \DecValTok{2}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ causal\_coef }\SpecialCharTok{*}\NormalTok{ x }\SpecialCharTok{{-}} \DecValTok{4}\SpecialCharTok{*}\NormalTok{u2 }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}


\NormalTok{jtools}\SpecialCharTok{::}\FunctionTok{export\_summs}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x), }\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z))}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-3} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l l}


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 1 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 2 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} (Intercept) \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.03\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.02\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.04)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.03)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} x \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 2.03 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 2.82 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.03)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.03)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} z \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -1.61 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} N \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} R2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.33\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.58\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{3}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  *** p $<$ 0.001;  ** p $<$ 0.01;  * p $<$ 0.05. \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

Another worse variation is

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cleans workspace}
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\CommentTok{\# DAG}

\DocumentationTok{\#\# specify edges}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; u1{-}\textgreater{}x; u1{-}\textgreater{}z; u2{-}\textgreater{}z; u2{-}\textgreater{}y; z{-}\textgreater{}y\}"}\NormalTok{)}

\CommentTok{\# set u as latent}
\FunctionTok{latents}\NormalTok{(model) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"u1"}\NormalTok{, }\StringTok{"u2"}\NormalTok{)}

\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{u1=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{u2=}\DecValTok{3}\NormalTok{, }\AttributeTok{y=}\DecValTok{3}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{u1=}\DecValTok{2}\NormalTok{, }\AttributeTok{z=}\FloatTok{1.5}\NormalTok{, }\AttributeTok{u2=}\DecValTok{2}\NormalTok{, }\AttributeTok{y=}\DecValTok{1}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{33-controls_files/figure-latex/unnamed-chunk-4-1} \end{center}

You can't do much in this case.

\begin{itemize}
\item
  If you don't control for \(Z\), then you have an open back-door path \(X \leftarrow U_1 \to Z \to Y\), and the unadjusted estimate is biased
\item
  If you control for \(Z\), then you open backdoor path \(X \leftarrow U_1 \to Z \leftarrow U_2 \to Y\), and the adjusted estimate is also biased
\end{itemize}

Hence, we cannot identify the causal effect in this case.

We can do sensitivity analyses to examine \citep{cinelli2019sensitivity, cinelli2020making}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  the plausible bounds on the strength of the direct effect of \(Z \to Y\)
\item
  the strength of the effects of the latent variables
\end{enumerate}

\hypertarget{bias-amplification}{%
\subsection{Bias Amplification}\label{bias-amplification}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cleans workspace}
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\CommentTok{\# DAG}

\DocumentationTok{\#\# specify edges}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; u{-}\textgreater{}x; u{-}\textgreater{}y; z{-}\textgreater{}x\}"}\NormalTok{)}

\CommentTok{\# set u as latent}
\FunctionTok{latents}\NormalTok{(model) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"u"}\NormalTok{)}

\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{z=}\DecValTok{1}\NormalTok{, }\AttributeTok{x=}\DecValTok{2}\NormalTok{, }\AttributeTok{u=}\DecValTok{3}\NormalTok{, }\AttributeTok{y=}\DecValTok{4}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{z=}\DecValTok{1}\NormalTok{, }\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{u=}\DecValTok{2}\NormalTok{, }\AttributeTok{y=}\DecValTok{1}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{33-controls_files/figure-latex/unnamed-chunk-5-1} \end{center}

Controlling for Z amplifies the omitted variable bias

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \FloatTok{1e4}
\NormalTok{z }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{u }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{x }\OtherTok{\textless{}{-}} \DecValTok{2}\SpecialCharTok{*}\NormalTok{z }\SpecialCharTok{+}\NormalTok{ u }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{+} \DecValTok{2}\SpecialCharTok{*}\NormalTok{u }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}

\NormalTok{jtools}\SpecialCharTok{::}\FunctionTok{export\_summs}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x), }\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z))}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-6} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l l}


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 1 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 2 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} (Intercept) \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.01\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.01\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} x \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.33 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 2.00 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} z \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -2.01 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.03)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} N \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} R2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.71\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.80\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{3}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  *** p $<$ 0.001;  ** p $<$ 0.01;  * p $<$ 0.05. \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

\hypertarget{overcontrol-bias}{%
\subsection{Overcontrol bias}\label{overcontrol-bias}}

Sometimes, this is similar to controlling for variables that are proxy of the dependent variable.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cleans workspace}
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\CommentTok{\# DAG}

\DocumentationTok{\#\# specify edges}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}z; z{-}\textgreater{}y\}"}\NormalTok{)}


\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{y=}\DecValTok{3}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{1}\NormalTok{, }\AttributeTok{y=}\DecValTok{1}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{33-controls_files/figure-latex/unnamed-chunk-7-1} \end{center}

If X is a proxy for Z (i.e., a mediator between Z and Y), controlling for Z is bad

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \FloatTok{1e4}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{z }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ z }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}

\NormalTok{jtools}\SpecialCharTok{::}\FunctionTok{export\_summs}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x), }\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z))}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-8} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l l}


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 1 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 2 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} (Intercept) \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.02\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.00\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} x \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.98 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.01\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} z \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.99 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} N \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} R2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.33\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.66\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{3}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  *** p $<$ 0.001;  ** p $<$ 0.01;  * p $<$ 0.05. \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

Now you see that \(Z\) is significant, which is technically true, but we are interested in the causal coefficient of \(X\) on \(Y\).

Another setting for overcontrol bias is

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cleans workspace}
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\CommentTok{\# DAG}

\DocumentationTok{\#\# specify edges}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}m; m{-}\textgreater{}z; m{-}\textgreater{}y\}"}\NormalTok{)}


\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{m=}\DecValTok{2}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{y=}\DecValTok{3}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{2}\NormalTok{, }\AttributeTok{m=}\DecValTok{2}\NormalTok{, }\AttributeTok{z=}\DecValTok{1}\NormalTok{, }\AttributeTok{y=}\DecValTok{2}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{33-controls_files/figure-latex/unnamed-chunk-9-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \FloatTok{1e4}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{m }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{z }\OtherTok{\textless{}{-}}\NormalTok{ m }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ m }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}


\NormalTok{jtools}\SpecialCharTok{::}\FunctionTok{export\_summs}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x), }\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z))}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-10} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l l}


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 1 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 2 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} (Intercept) \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.01\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.01\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} x \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.01 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.51 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} z \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.50 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} N \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} R2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.34\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.50\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{3}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  *** p $<$ 0.001;  ** p $<$ 0.01;  * p $<$ 0.05. \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

Another setting for this bias is

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cleans workspace}
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\CommentTok{\# DAG}

\DocumentationTok{\#\# specify edges}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}z; z{-}\textgreater{}y; u{-}\textgreater{}z; u{-}\textgreater{}y\}"}\NormalTok{)}

\CommentTok{\# set u as latent}
\FunctionTok{latents}\NormalTok{(model) }\OtherTok{\textless{}{-}} \StringTok{"u"}

\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{u=}\DecValTok{3}\NormalTok{, }\AttributeTok{y=}\DecValTok{4}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{1}\NormalTok{, }\AttributeTok{u=}\DecValTok{2}\NormalTok{, }\AttributeTok{y=}\DecValTok{1}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{33-controls_files/figure-latex/unnamed-chunk-11-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{n }\OtherTok{\textless{}{-}} \FloatTok{1e4}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{u }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{z }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ u }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ z }\SpecialCharTok{+}\NormalTok{ u }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}

\NormalTok{jtools}\SpecialCharTok{::}\FunctionTok{export\_summs}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x), }\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z))}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-12} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l l}


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 1 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 2 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} (Intercept) \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.01\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.01\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} x \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.01 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.47 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} z \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.48 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} N \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} R2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.15\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.78\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{3}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  *** p $<$ 0.001;  ** p $<$ 0.01;  * p $<$ 0.05. \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

The total effect of \(X\) on \(Y\) is not biased (i.e., \(1.01 \approx 1.48 - 0.47\)).

Controlling for Z will fail to identify the direct effect of \(X\) on \(Y\) and opens the biasing path \(X \rightarrow Z \leftarrow U \rightarrow Y\)

\hypertarget{selection-bias}{%
\subsection{Selection Bias}\label{selection-bias}}

Also known as ``collider stratification bias''

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\CommentTok{\# DAG}

\DocumentationTok{\#\# specify edges}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; x{-}\textgreater{}z; u{-}\textgreater{}z;u{-}\textgreater{}y\}"}\NormalTok{)}

\CommentTok{\# set u as latent}
\FunctionTok{latents}\NormalTok{(model) }\OtherTok{\textless{}{-}} \StringTok{"u"}

\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{u=}\DecValTok{2}\NormalTok{, }\AttributeTok{y=}\DecValTok{3}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{3}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{u=}\DecValTok{4}\NormalTok{, }\AttributeTok{y=}\DecValTok{3}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{33-controls_files/figure-latex/unnamed-chunk-13-1} \end{center}

Adjusting \(Z\) opens the colliding path \(X \to Z \leftarrow U \to Y\)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \FloatTok{1e4}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{u }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{z }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ u }\SpecialCharTok{+}  \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{+} \DecValTok{2}\SpecialCharTok{*}\NormalTok{u }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}

\NormalTok{jtools}\SpecialCharTok{::}\FunctionTok{export\_summs}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x), }\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z))}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-14} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l l}


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 1 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 2 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} (Intercept) \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.00\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.02\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} x \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.98 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.03\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} z \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.00 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} N \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} R2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.17\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.49\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{3}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  *** p $<$ 0.001;  ** p $<$ 0.01;  * p $<$ 0.05. \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

Another setting is

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\CommentTok{\# DAG}

\DocumentationTok{\#\# specify edges}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; x{-}\textgreater{}z; y{-}\textgreater{}z\}"}\NormalTok{)}

\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{y=}\DecValTok{3}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{2}\NormalTok{, }\AttributeTok{z=}\DecValTok{1}\NormalTok{, }\AttributeTok{y=}\DecValTok{2}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{33-controls_files/figure-latex/unnamed-chunk-15-1} \end{center}

Controlling \(Z\) opens the colliding path \(X \to Z \leftarrow Y\)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \FloatTok{1e4}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{z }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ y }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}

\NormalTok{jtools}\SpecialCharTok{::}\FunctionTok{export\_summs}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x), }\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z))}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-16} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l l}


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 1 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 2 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} (Intercept) \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.00\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.01\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} x \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.03 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.01\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} z \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.51 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.00)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} N \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} R2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.51\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.76\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{3}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  *** p $<$ 0.001;  ** p $<$ 0.01;  * p $<$ 0.05. \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

\hypertarget{case-control-bias}{%
\subsection{Case-control Bias}\label{case-control-bias}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\CommentTok{\# DAG}

\DocumentationTok{\#\# specify edges}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; y{-}\textgreater{}z\}"}\NormalTok{)}

\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{y=}\DecValTok{3}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{2}\NormalTok{, }\AttributeTok{z=}\DecValTok{1}\NormalTok{, }\AttributeTok{y=}\DecValTok{2}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{33-controls_files/figure-latex/unnamed-chunk-17-1} \end{center}

Controlling \(Z\) opens a virtual collider (a descendant of a collider).

However, if \(X\) truly has no causal effect on \(Y\). Then, controlling for \(Z\) is valid for testing whether the effect of \(X\) on \(Y\) is 0 because X is d-separated from \(Y\) regardless of adjusting for \(Z\)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \FloatTok{1e4}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{z }\OtherTok{\textless{}{-}}\NormalTok{ y }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}

\NormalTok{jtools}\SpecialCharTok{::}\FunctionTok{export\_summs}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x), }\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z))}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-18} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l l}


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 1 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 2 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} (Intercept) \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.00\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.00\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} x \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.00 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.50 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} z \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.50 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.00)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} N \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} R2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.50\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.75\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{3}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  *** p $<$ 0.001;  ** p $<$ 0.01;  * p $<$ 0.05. \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

\hypertarget{good-controls}{%
\section{Good Controls}\label{good-controls}}

\hypertarget{omitted-variable-bias-correction}{%
\subsection{Omitted Variable Bias Correction}\label{omitted-variable-bias-correction}}

This is when \(Z\) can block all back-door paths.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; z{-}\textgreater{}x; z{-}\textgreater{}y\}"}\NormalTok{)}

\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{y=}\DecValTok{3}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{y=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{33-controls_files/figure-latex/unnamed-chunk-19-1} \end{center}

Unadjusted estimate is biased

adjusting for \(Z\) blocks the backdoor path

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \FloatTok{1e4}
\NormalTok{z }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{causal\_coef }\OtherTok{=} \DecValTok{2}
\NormalTok{beta2 }\OtherTok{=} \DecValTok{3}
\NormalTok{x }\OtherTok{\textless{}{-}}\NormalTok{ z }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ causal\_coef }\SpecialCharTok{*}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ beta2 }\SpecialCharTok{*}\NormalTok{ z }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}

\NormalTok{jtools}\SpecialCharTok{::}\FunctionTok{export\_summs}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x), }\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z))}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-20} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l l}


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 1 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 2 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} (Intercept) \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.01\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.01\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} x \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 3.53 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 2.00 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} z \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 3.00 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} N \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} R2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.82\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.97\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{3}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  *** p $<$ 0.001;  ** p $<$ 0.01;  * p $<$ 0.05. \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cleans workspace}
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\CommentTok{\# Draw DAG}

\CommentTok{\# specify edges}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; u{-}\textgreater{}z; z{-}\textgreater{}x; u{-}\textgreater{}y\}"}\NormalTok{)}

\CommentTok{\# set u as latent}
\FunctionTok{latents}\NormalTok{(model) }\OtherTok{\textless{}{-}} \StringTok{"u"}

\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{u=}\DecValTok{3}\NormalTok{, }\AttributeTok{y =} \DecValTok{4}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{y=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{u =} \DecValTok{3}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{33-controls_files/figure-latex/unnamed-chunk-21-1} \end{center}

Unadjusted estimate is biased

adjusting for \(Z\) blocks the backdoor door path due to \(U\)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \FloatTok{1e4}
\NormalTok{u }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{z }\OtherTok{\textless{}{-}}\NormalTok{ u }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{causal\_coef }\OtherTok{=} \DecValTok{2}
\NormalTok{x }\OtherTok{\textless{}{-}}\NormalTok{ z }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ causal\_coef }\SpecialCharTok{*}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ u }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}

\NormalTok{jtools}\SpecialCharTok{::}\FunctionTok{export\_summs}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x), }\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z))}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-22} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l l}


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 1 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 2 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} (Intercept) \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.02\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.01\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} x \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 2.32 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.98 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} z \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.52 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} N \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} R2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.91\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.92\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{3}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  *** p $<$ 0.001;  ** p $<$ 0.01;  * p $<$ 0.05. \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

Even though \(Z\) is significant, we cannot give it a causal interpretation.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cleans workspace}
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\CommentTok{\# Draw DAG}

\CommentTok{\# specify edges}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; u{-}\textgreater{}z; u{-}\textgreater{}x; z{-}\textgreater{}y\}"}\NormalTok{)}

\CommentTok{\# set u as latent}
\FunctionTok{latents}\NormalTok{(model) }\OtherTok{\textless{}{-}} \StringTok{"u"}

\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{3}\NormalTok{, }\AttributeTok{u=}\DecValTok{2}\NormalTok{, }\AttributeTok{y =} \DecValTok{4}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{y=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{u =} \DecValTok{3}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{33-controls_files/figure-latex/unnamed-chunk-23-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n     }\OtherTok{\textless{}{-}} \FloatTok{1e4}
\NormalTok{u     }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{z     }\OtherTok{\textless{}{-}}\NormalTok{ u }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{x     }\OtherTok{\textless{}{-}}\NormalTok{ u }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{causal\_coef }\OtherTok{\textless{}{-}} \DecValTok{2}
\NormalTok{y     }\OtherTok{\textless{}{-}}\NormalTok{ causal\_coef }\SpecialCharTok{*}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}

\NormalTok{jtools}\SpecialCharTok{::}\FunctionTok{export\_summs}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x), }\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z))}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-24} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l l}


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 1 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 2 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} (Intercept) \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.01\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.00\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} x \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 2.50 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 2.00 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} z \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.98 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} N \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} R2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.84\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.93\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{3}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  *** p $<$ 0.001;  ** p $<$ 0.01;  * p $<$ 0.05. \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

Even though \(Z\) is significant, we cannot give it a causal interpretation.

\textbf{Summary}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cleans workspace}
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\CommentTok{\# Model 1 }

\NormalTok{model1 }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; z{-}\textgreater{}x; z{-}\textgreater{}y\}"}\NormalTok{)}

\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model1) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{y=}\DecValTok{3}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{y=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{))}



\CommentTok{\# Model 2}

\CommentTok{\# specify edges}
\NormalTok{model2 }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; u{-}\textgreater{}z; z{-}\textgreater{}x; u{-}\textgreater{}y\}"}\NormalTok{)}

\CommentTok{\# set u as latent}
\FunctionTok{latents}\NormalTok{(model2) }\OtherTok{\textless{}{-}} \StringTok{"u"}

\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model2) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{u=}\DecValTok{3}\NormalTok{, }\AttributeTok{y =} \DecValTok{4}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{y=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{u =} \DecValTok{3}\NormalTok{))}



\CommentTok{\# Model 3}

\CommentTok{\# specify edges}
\NormalTok{model3 }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; u{-}\textgreater{}z; u{-}\textgreater{}x; z{-}\textgreater{}y\}"}\NormalTok{)}

\CommentTok{\# set u as latent}
\FunctionTok{latents}\NormalTok{(model3) }\OtherTok{\textless{}{-}} \StringTok{"u"}

\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model3) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{3}\NormalTok{, }\AttributeTok{u=}\DecValTok{2}\NormalTok{, }\AttributeTok{y =} \DecValTok{4}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{y=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{u =} \DecValTok{3}\NormalTok{))}

\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model1) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{33-controls_files/figure-latex/unnamed-chunk-25-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model2) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{33-controls_files/figure-latex/unnamed-chunk-25-2} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model3) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{33-controls_files/figure-latex/unnamed-chunk-25-3} \end{center}

\hypertarget{omitted-variable-bias-in-mediation-correction}{%
\subsection{Omitted Variable Bias in Mediation Correction}\label{omitted-variable-bias-in-mediation-correction}}

Common causes of \(X\) and any mediator (between \(X\) and \(Y\)) confound the effect of \(X\) on \(Y\)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cleans workspace}
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\CommentTok{\# DAG}

\DocumentationTok{\#\# specify edges}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; z{-}\textgreater{}x; x{-}\textgreater{}m; z{-}\textgreater{}m; m{-}\textgreater{}y\}"}\NormalTok{)}

\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{m=}\DecValTok{3}\NormalTok{, }\AttributeTok{y=}\DecValTok{4}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{m=}\DecValTok{1}\NormalTok{, }\AttributeTok{y=}\DecValTok{1}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{33-controls_files/figure-latex/unnamed-chunk-26-1} \end{center}

\(Z\) is a confounder of both the mediator \(M\) and \(X\)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n     }\OtherTok{\textless{}{-}} \FloatTok{1e4}
\NormalTok{z     }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{x     }\OtherTok{\textless{}{-}}\NormalTok{ z }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{causal\_coef }\OtherTok{\textless{}{-}} \DecValTok{2}
\NormalTok{m     }\OtherTok{\textless{}{-}}\NormalTok{ causal\_coef }\SpecialCharTok{*}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{y     }\OtherTok{\textless{}{-}}\NormalTok{ m }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}

\NormalTok{jtools}\SpecialCharTok{::}\FunctionTok{export\_summs}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x), }\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z))}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-27} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l l}


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 1 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 2 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} (Intercept) \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.02\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.01\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} x \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 2.49 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 2.01 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} z \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.97 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} N \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} R2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.83\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.87\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{3}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  *** p $<$ 0.001;  ** p $<$ 0.01;  * p $<$ 0.05. \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cleans workspace}
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\CommentTok{\# DAG}

\DocumentationTok{\#\# specify edges}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; u{-}\textgreater{}z; z{-}\textgreater{}x; x{-}\textgreater{}m; u{-}\textgreater{}m; m{-}\textgreater{}y\}"}\NormalTok{)}

\CommentTok{\# set u as latent}
\FunctionTok{latents}\NormalTok{(model) }\OtherTok{\textless{}{-}} \StringTok{"u"}

\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{u=}\DecValTok{3}\NormalTok{, }\AttributeTok{m=}\DecValTok{4}\NormalTok{, }\AttributeTok{y=}\DecValTok{5}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{u=}\DecValTok{3}\NormalTok{, }\AttributeTok{m=}\DecValTok{1}\NormalTok{, }\AttributeTok{y=}\DecValTok{1}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{33-controls_files/figure-latex/unnamed-chunk-28-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n     }\OtherTok{\textless{}{-}} \FloatTok{1e4}
\NormalTok{u     }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{z     }\OtherTok{\textless{}{-}}\NormalTok{ u }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{x     }\OtherTok{\textless{}{-}}\NormalTok{ z }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{causal\_coef }\OtherTok{\textless{}{-}} \DecValTok{2}
\NormalTok{m     }\OtherTok{\textless{}{-}}\NormalTok{ causal\_coef }\SpecialCharTok{*}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ u }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{y     }\OtherTok{\textless{}{-}}\NormalTok{ m }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}

\NormalTok{jtools}\SpecialCharTok{::}\FunctionTok{export\_summs}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x), }\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z))}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-29} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l l}


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 1 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 2 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} (Intercept) \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.01\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.00\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} x \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 2.32 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.98 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} z \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.51 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} N \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} R2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.86\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.87\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{3}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  *** p $<$ 0.001;  ** p $<$ 0.01;  * p $<$ 0.05. \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cleans workspace}
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\CommentTok{\# DAG}

\DocumentationTok{\#\# specify edges}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; u{-}\textgreater{}z; z{-}\textgreater{}m; x{-}\textgreater{}m; u{-}\textgreater{}x; m{-}\textgreater{}y\}"}\NormalTok{)}

\CommentTok{\# set u as latent}
\FunctionTok{latents}\NormalTok{(model) }\OtherTok{\textless{}{-}} \StringTok{"u"}

\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{3}\NormalTok{, }\AttributeTok{u=}\DecValTok{2}\NormalTok{, }\AttributeTok{m=}\DecValTok{4}\NormalTok{, }\AttributeTok{y=}\DecValTok{5}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{u=}\DecValTok{3}\NormalTok{, }\AttributeTok{m=}\DecValTok{1}\NormalTok{, }\AttributeTok{y=}\DecValTok{1}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{33-controls_files/figure-latex/unnamed-chunk-30-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n     }\OtherTok{\textless{}{-}} \FloatTok{1e4}
\NormalTok{u     }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{z     }\OtherTok{\textless{}{-}}\NormalTok{ u }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{x     }\OtherTok{\textless{}{-}}\NormalTok{ u }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{causal\_coef }\OtherTok{\textless{}{-}} \DecValTok{2}
\NormalTok{m     }\OtherTok{\textless{}{-}}\NormalTok{ causal\_coef }\SpecialCharTok{*}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{y     }\OtherTok{\textless{}{-}}\NormalTok{ m }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}

\NormalTok{jtools}\SpecialCharTok{::}\FunctionTok{export\_summs}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x), }\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z))}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-31} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l l}


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 1 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 2 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} (Intercept) \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.01\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.01\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} x \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 2.48 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.99 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} z \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.01 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} N \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} R2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.78\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.87\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{3}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  *** p $<$ 0.001;  ** p $<$ 0.01;  * p $<$ 0.05. \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

\textbf{Summary}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# model 4}
\NormalTok{model4 }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; z{-}\textgreater{}x; x{-}\textgreater{}m; z{-}\textgreater{}m; m{-}\textgreater{}y\}"}\NormalTok{)}

\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model4) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{m=}\DecValTok{3}\NormalTok{, }\AttributeTok{y=}\DecValTok{4}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{m=}\DecValTok{1}\NormalTok{, }\AttributeTok{y=}\DecValTok{1}\NormalTok{))}


\CommentTok{\# model 5}
\NormalTok{model5 }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; u{-}\textgreater{}z; z{-}\textgreater{}x; x{-}\textgreater{}m; u{-}\textgreater{}m; m{-}\textgreater{}y\}"}\NormalTok{)}

\CommentTok{\# set u as latent}
\FunctionTok{latents}\NormalTok{(model5) }\OtherTok{\textless{}{-}} \StringTok{"u"}

\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model5) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{u=}\DecValTok{3}\NormalTok{, }\AttributeTok{m=}\DecValTok{4}\NormalTok{, }\AttributeTok{y=}\DecValTok{5}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{u=}\DecValTok{3}\NormalTok{, }\AttributeTok{m=}\DecValTok{1}\NormalTok{, }\AttributeTok{y=}\DecValTok{1}\NormalTok{))}


\CommentTok{\# model 6}

\NormalTok{model6 }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; u{-}\textgreater{}z; z{-}\textgreater{}m; x{-}\textgreater{}m; u{-}\textgreater{}x; m{-}\textgreater{}y\}"}\NormalTok{)}

\CommentTok{\# set u as latent}
\FunctionTok{latents}\NormalTok{(model6) }\OtherTok{\textless{}{-}} \StringTok{"u"}

\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model6) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{3}\NormalTok{, }\AttributeTok{u=}\DecValTok{2}\NormalTok{, }\AttributeTok{m=}\DecValTok{4}\NormalTok{, }\AttributeTok{y=}\DecValTok{5}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{u=}\DecValTok{3}\NormalTok{, }\AttributeTok{m=}\DecValTok{1}\NormalTok{, }\AttributeTok{y=}\DecValTok{1}\NormalTok{))}

\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model4) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{33-controls_files/figure-latex/unnamed-chunk-32-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model5) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{33-controls_files/figure-latex/unnamed-chunk-32-2} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model6) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{33-controls_files/figure-latex/unnamed-chunk-32-3} \end{center}

\hypertarget{neutral-controls}{%
\section{Neutral Controls}\label{neutral-controls}}

\hypertarget{good-predictive-controls}{%
\subsection{Good Predictive Controls}\label{good-predictive-controls}}

Good for precision

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cleans workspace}
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\CommentTok{\# DAG}

\DocumentationTok{\#\# specify edges}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; z{-}\textgreater{}y\}"}\NormalTok{)}


\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{y=}\DecValTok{2}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{y=}\DecValTok{1}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{33-controls_files/figure-latex/unnamed-chunk-33-1} \end{center}

Controlling for \(Z\) does not help or hurt identification, but it can increase precision (i.e., reducing SE)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \FloatTok{1e4}
\NormalTok{z }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{+} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ z }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}

\NormalTok{jtools}\SpecialCharTok{::}\FunctionTok{export\_summs}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x), }\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z))}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-34} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l l}


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 1 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 2 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} (Intercept) \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.02\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.00\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} x \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.98 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.99 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} z \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 2.01 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} N \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} R2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.16\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.83\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{3}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  *** p $<$ 0.001;  ** p $<$ 0.01;  * p $<$ 0.05. \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

Similar coefficients, but smaller SE when controlling for \(Z\)

Another variation is

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cleans workspace}
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\CommentTok{\# DAG}

\DocumentationTok{\#\# specify edges}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; x{-}\textgreater{}m; z{-}\textgreater{}m; m{-}\textgreater{}y\}"}\NormalTok{)}

\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{m=}\DecValTok{2}\NormalTok{, }\AttributeTok{y=}\DecValTok{3}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{m=}\DecValTok{1}\NormalTok{, }\AttributeTok{y=}\DecValTok{1}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{33-controls_files/figure-latex/unnamed-chunk-35-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \FloatTok{1e4}
\NormalTok{z }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{m }\OtherTok{\textless{}{-}} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ z }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{+} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ m }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}

\NormalTok{jtools}\SpecialCharTok{::}\FunctionTok{export\_summs}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x), }\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z))}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-36} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l l}


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 1 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 2 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} (Intercept) \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.08\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.02\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.05)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} x \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.99 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.00 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.05)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} z \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 3.99 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} N \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} R2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.05\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.77\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{3}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  *** p $<$ 0.001;  ** p $<$ 0.01;  * p $<$ 0.05. \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

Controlling for \(Z\) can reduce SE

\hypertarget{good-selection-bias}{%
\subsection{Good Selection Bias}\label{good-selection-bias}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cleans workspace}
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\CommentTok{\# DAG}

\DocumentationTok{\#\# specify edges}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; x{-}\textgreater{}z; z{-}\textgreater{}w; u{-}\textgreater{}w;u{-}\textgreater{}y\}"}\NormalTok{)}

\CommentTok{\# set u as latent}
\FunctionTok{latents}\NormalTok{(model) }\OtherTok{\textless{}{-}} \StringTok{"u"}

\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{w=}\DecValTok{3}\NormalTok{, }\AttributeTok{u=}\DecValTok{3}\NormalTok{, }\AttributeTok{y=}\DecValTok{5}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{3}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{w=}\DecValTok{1}\NormalTok{, }\AttributeTok{u=}\DecValTok{4}\NormalTok{, }\AttributeTok{y=}\DecValTok{3}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{33-controls_files/figure-latex/unnamed-chunk-37-1} \end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Unadjusted estimate is unbiased
\item
  Controlling for Z can increase SE
\item
  Controlling for Z while having on W can help identify X
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \FloatTok{1e4}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{u }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{z }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{w }\OtherTok{\textless{}{-}}\NormalTok{ z }\SpecialCharTok{+}\NormalTok{ u }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{{-}} \DecValTok{2}\SpecialCharTok{*}\NormalTok{u }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}

\NormalTok{jtools}\SpecialCharTok{::}\FunctionTok{export\_summs}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x), }\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ w), }\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z }\SpecialCharTok{+}\NormalTok{ w))}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-38} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l l l}


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 1 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 3 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} (Intercept) \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.02\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.02\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.03\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} x \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.98 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.66 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.98 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} w \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.67 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -1.00 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} z \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.02 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} N \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} R2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.16\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.38\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.50\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{4}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  *** p $<$ 0.001;  ** p $<$ 0.01;  * p $<$ 0.05. \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

\hypertarget{bad-predictive-controls}{%
\subsection{Bad Predictive Controls}\label{bad-predictive-controls}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cleans workspace}
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\CommentTok{\# DAG}

\DocumentationTok{\#\# specify edges}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; z{-}\textgreater{}x\}"}\NormalTok{)}


\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{1}\NormalTok{, }\AttributeTok{y=}\DecValTok{2}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{y=}\DecValTok{1}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{33-controls_files/figure-latex/unnamed-chunk-39-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \FloatTok{1e4}
\NormalTok{z }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{x }\OtherTok{\textless{}{-}} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ z }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{+} \DecValTok{2} \SpecialCharTok{*} \FunctionTok{rnorm}\NormalTok{(n)}

\NormalTok{jtools}\SpecialCharTok{::}\FunctionTok{export\_summs}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x), }\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z))}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-40} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l l}


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 1 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 2 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} (Intercept) \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.01\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.01\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} x \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.00 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.02 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.01)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} z \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -0.04\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.05)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} N \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} R2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.56\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.56\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{3}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  *** p $<$ 0.001;  ** p $<$ 0.01;  * p $<$ 0.05. \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

Similar coefficients, but greater SE when controlling for \(Z\)

Another variation is

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\CommentTok{\# DAG}

\DocumentationTok{\#\# specify edges}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; x{-}\textgreater{}z\}"}\NormalTok{)}


\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{1}\NormalTok{, }\AttributeTok{y=}\DecValTok{2}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{y=}\DecValTok{1}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{33-controls_files/figure-latex/unnamed-chunk-41-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{n }\OtherTok{\textless{}{-}} \FloatTok{1e4}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{z }\OtherTok{\textless{}{-}} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ x }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{+} \DecValTok{2} \SpecialCharTok{*} \FunctionTok{rnorm}\NormalTok{(n)}

\NormalTok{jtools}\SpecialCharTok{::}\FunctionTok{export\_summs}\NormalTok{(}\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x), }\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ z))}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-42} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l l}


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 1 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 2 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} (Intercept) \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.02\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.02\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} x \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.00 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.99 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.05)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} z \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.00\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.02)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} N \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 10000\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} R2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.20\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.20\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{3}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  *** p $<$ 0.001;  ** p $<$ 0.01;  * p $<$ 0.05. \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

Worse SE when controlling for \(Z\) (\(0.02 < 0.05\))

\hypertarget{bad-selection-bias}{%
\subsection{Bad Selection Bias}\label{bad-selection-bias}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cleans workspace}
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\CommentTok{\# DAG}

\DocumentationTok{\#\# specify edges}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{dagitty}\NormalTok{(}\StringTok{"dag\{x{-}\textgreater{}y; x{-}\textgreater{}z\}"}\NormalTok{)}


\DocumentationTok{\#\# coordinates for plotting}
\FunctionTok{coordinates}\NormalTok{(model) }\OtherTok{\textless{}{-}}  \FunctionTok{list}\NormalTok{(}
  \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{y=}\DecValTok{2}\NormalTok{),}
  \AttributeTok{y =} \FunctionTok{c}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\NormalTok{, }\AttributeTok{z=}\DecValTok{2}\NormalTok{, }\AttributeTok{y=}\DecValTok{1}\NormalTok{))}

\DocumentationTok{\#\# ggplot}
\FunctionTok{ggdag}\NormalTok{(model) }\SpecialCharTok{+} \FunctionTok{theme\_dag}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{33-controls_files/figure-latex/unnamed-chunk-43-1} \end{center}

Not all post-treatment variables are bad.

Controlling for \(Z\) is neutral, but it might hurt the precision of the causal effect.

\hypertarget{choosing-controls}{%
\section{Choosing Controls}\label{choosing-controls}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(pcalg)}
\FunctionTok{library}\NormalTok{(dagitty)}
\FunctionTok{library}\NormalTok{(causaleffect)}
\end{Highlighting}
\end{Shaded}

By providing a causal diagram, deciding the appropriateness of controls are automated.

\begin{itemize}
\item
  \href{https://causalfusion.net/login}{Fusion}
\item
  \href{http://dagitty.net/}{DAGitty}
\end{itemize}

Guide on how to choose confounders: \citet{vanderweele2019principles}

In cases where it's hard to determine the plausibility of controls, we might need to further analysis.

\texttt{sensemakr} provides such tools.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(sensemakr)}
\end{Highlighting}
\end{Shaded}

In simple cases, we can follow the simple rules of thumb provided by \citet{steinmetz2022meta} (p.~614, Fig 2)

\includegraphics[width=6.25in,height=4.16667in]{images/control_var_decision.png}

\hypertarget{part-v.-miscellaneous}{%
\part*{V. MISCELLANEOUS}\label{part-v.-miscellaneous}}
\addcontentsline{toc}{part}{V. MISCELLANEOUS}

\hypertarget{mediation}{%
\chapter{Mediation}\label{mediation}}

\hypertarget{traditional}{%
\section{Traditional}\label{traditional}}

\citep{baron1986moderator} is outdated because of step 1, but we could still see the original idea.

3 regressions

\begin{itemize}
\item
  Step 1: \(X \to Y\)
\item
  Step 2: \(X \to M\)
\item
  Step 3: \(X + M \to Y\)
\end{itemize}

where

\begin{itemize}
\item
  \(X\) = independent variable
\item
  \(Y\) = dependent variable
\item
  \(M\) = mediating variable
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Originally, the first path from \(X \to Y\) suggested by \citep{baron1986moderator} needs to be significant. But there are cases that you could have indirect of \(X\) on \(Y\) without significant direct effect of \(X\) on \(Y\) (e.g., when the effect is absorbed into M, or there are two counteracting effects \(M_1, M_2\) that cancel out each other effect).
\end{enumerate}

Mathematically,

\[
Y = b_0 + b_1 X + \epsilon
\]

\(b_1\) does \textbf{not} need to be \textbf{significant}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  We examine the effect of \(X\) on \(M\). This step requires that there is a significant effect of \(X\) on \(M\) to continue with the analysis
\end{enumerate}

Mathematically,

\[
M = b_0 + b_2 X + \epsilon
\]

where \(b_2\) needs to be \textbf{significant}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  In this step, we want to the effect of \(M\) on \(Y\) ``absorbs'' most of the direct effect of \(X\) on \(Y\) (or at least makes the effect smaller).
\end{enumerate}

Mathematically,

\[
Y = b_0 + b_4 X + b_3 M + \epsilon
\]

\(b_4\) needs to be either smaller or insignificant.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5417}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.4583}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
The effect of \(X\) on \(Y\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
then, \(M\) \ldots{} mediates between \(X\) and \(Y\)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
completely disappear (\(b_4\) insignificant) & Fully (i.e., full mediation) \\
partially disappear (\(b_4\) smaller than in step 1) & Partially (i.e., partial mediation) \\
\end{longtable}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Examine the mediation effect (i.e., whether it is significant)
\end{enumerate}

\begin{itemize}
\item
  Fist approach: Sobel's test \citep{sobel1982asymptotic}
\item
  Second approach: bootstrapping \citep{preacher2004spss} (preferable)
\end{itemize}

More details can be found \href{https://cran.ism.ac.jp/web/packages/mediation/vignettes/mediation-old.pdf}{here}

\hypertarget{example-1-mediation-traditional}{%
\subsection{Example 1}\label{example-1-mediation-traditional}}

from \href{https://data.library.virginia.edu/introduction-to-mediation-analysis/}{Virginia's library}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{myData }\OtherTok{\textless{}{-}}
    \FunctionTok{read.csv}\NormalTok{(}\StringTok{\textquotesingle{}http://static.lib.virginia.edu/statlab/materials/data/mediationData.csv\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Step 1 (no longer necessary)}
\NormalTok{model}\FloatTok{.0} \OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X, myData)}

\CommentTok{\# Step 2}
\NormalTok{model.M }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(M }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X, myData)}

\CommentTok{\# Step 3}
\NormalTok{model.Y }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X }\SpecialCharTok{+}\NormalTok{ M, myData)}

\CommentTok{\# Step 4 (boostrapping)}
\FunctionTok{library}\NormalTok{(mediation)}
\NormalTok{results }\OtherTok{\textless{}{-}} \FunctionTok{mediate}\NormalTok{(}
\NormalTok{    model.M,}
\NormalTok{    model.Y,}
    \AttributeTok{treat =} \StringTok{\textquotesingle{}X\textquotesingle{}}\NormalTok{,}
    \AttributeTok{mediator =} \StringTok{\textquotesingle{}M\textquotesingle{}}\NormalTok{,}
    \AttributeTok{boot =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{sims =} \DecValTok{500}
\NormalTok{)}
\FunctionTok{summary}\NormalTok{(results)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Causal Mediation Analysis }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Nonparametric Bootstrap Confidence Intervals with the Percentile Method}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}                Estimate 95\% CI Lower 95\% CI Upper p{-}value    }
\CommentTok{\#\textgreater{} ACME             0.3565       0.2159         0.54  \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} ADE              0.0396      {-}0.2144         0.30    0.74    }
\CommentTok{\#\textgreater{} Total Effect     0.3961       0.1514         0.65  \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} Prop. Mediated   0.9000       0.4686         2.35  \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Sample Size Used: 100 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Simulations: 500}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  Total Effect = 0.3961 = \(b_1\) (step 1) = total effect of \(X\) on \(Y\) without \(M\)
\item
  Direct Effect = ADE = 0.0396 = \(b_4\) (step 3) = direct effect of \(X\) on \(Y\) accounting for the indirect effect of \(M\)
\item
  ACME = Average Causal Mediation Effects = \(b_1 - b_4\) = 0.3961 - 0.0396 = 0.3565 = \(b_2 \times b_3\) = 0.56102 * 0.6355 = 0.3565
\end{itemize}

Using \texttt{mediation} package suggested by \citep{imai2010general} \citep{imai2010identification}. More on details of the package can be found \href{https://cran.r-project.org/web/packages/mediation/vignettes/mediation.pdf}{here}

2 types of Inference in this package:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Model-based inference:

  \begin{itemize}
  \item
    Assumptions:

    \begin{itemize}
    \item
      Treatment is randomized (could use matching methods to achieve this).
    \item
      Sequential Ignorability: conditional on covariates, there is other confounders that affect the relationship between (1) treatment-mediator, (2) treatment-outcome, (3) mediator-outcome. Typically hard to argue in observational data. This assumption is for the identification of ACME (i.e., average causal mediation effects).
    \end{itemize}
  \end{itemize}
\item
  Design-based inference
\end{enumerate}

Notations: we stay consistent with package instruction

\begin{itemize}
\item
  \(M_i(t)\) = mediator
\item
  \(T_i\) = treatment status \((0,1)\)
\item
  \(Y_i(t,m)\) = outcome where \(t\) = treatment, and \(m\) = mediating variables.
\item
  \(X_i\) = vector of observed pre-treatment confounders
\item
  Treatment effect (per unit \(i\)) = \(\tau_i = Y_i(1,M_i(1)) - Y_i (0,M_i(0))\) which has 2 effects

  \begin{itemize}
  \item
    Causal mediation effects: \(\delta_i (t) \equiv Y_i (t,M_i(1)) - Y_i(t,M_i(0))\)
  \item
    Direct effects: \(\zeta (t) \equiv Y_i (1, M_i(1)) - Y_i(0, M_i(0))\)
  \item
    summing up to the treatment effect: \(\tau_i = \delta_i (t) + \zeta_i (1-t)\)
  \end{itemize}
\end{itemize}

More on sequential ignorability

\[
\{ Y_i (t', m) , M_i (t) \} \perp T_i |X_i = x 
\]

\[
Y_i(t',m) \perp M_i(t) | T_i = t, X_i = x
\]

where

\begin{itemize}
\item
  \(0 < P(T_i = t | X_i = x)\)
\item
  \(0 < P(M_i = m | T_i = t , X_i =x)\)
\end{itemize}

First condition is the standard strong ignorability condition where treatment assignment is random conditional on pre-treatment confounders.

Second condition is stronger where the mediators is also random given the observed treatment and pre-treatment confounders. This condition is satisfied only when there is no unobserved pre-treatment confounders, and post-treatment confounders, and multiple mediators that are correlated.

My understanding is that until the moment I write this note, there is \textbf{no way to test the sequential ignorability assumption}. Hence, researchers can only do sensitivity analysis to argue for their result.

\hypertarget{model-based-causal-mediation-analysis}{%
\section{Model-based causal mediation analysis}\label{model-based-causal-mediation-analysis}}

I only put my understanding of model-based causal mediation analysis because I do not encounter design-based. Maybe in the future when I have to use it, I will start reading on it.

Fit 2 models

\begin{itemize}
\item
  mediator model: conditional distribution of the mediators \(M_i | T_i, X_i\)
\item
  Outcome model: conditional distribution of \(Y_i | T_i, M_i, X_i\)
\end{itemize}

\texttt{mediation} can accommodate almost all types of model for both mediator model and outcome model except Censored mediator model.

The update here is that estimation of ACME does not rely on product or difference of coefficients (see \ref{example-1-mediation-traditional} ,

which requires very strict assumption: (1) linear regression models of mediator and outcome, (2) \(T_i\) and \(M_i\) effects are additive and no interaction

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(mediation)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{2014}\NormalTok{)}
\FunctionTok{data}\NormalTok{(}\StringTok{"framing"}\NormalTok{, }\AttributeTok{package =} \StringTok{"mediation"}\NormalTok{)}

\NormalTok{med.fit }\OtherTok{\textless{}{-}}
    \FunctionTok{lm}\NormalTok{(emo }\SpecialCharTok{\textasciitilde{}}\NormalTok{ treat }\SpecialCharTok{+}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ educ }\SpecialCharTok{+}\NormalTok{ gender }\SpecialCharTok{+}\NormalTok{ income, }\AttributeTok{data =}\NormalTok{ framing)}
\NormalTok{out.fit }\OtherTok{\textless{}{-}}
    \FunctionTok{glm}\NormalTok{(}
\NormalTok{        cong\_mesg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ emo }\SpecialCharTok{+}\NormalTok{ treat }\SpecialCharTok{+}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ educ }\SpecialCharTok{+}\NormalTok{ gender }\SpecialCharTok{+}\NormalTok{ income,}
        \AttributeTok{data =}\NormalTok{ framing,}
        \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\StringTok{"probit"}\NormalTok{)}
\NormalTok{    )}

\CommentTok{\# Quasi{-}Bayesian Monte Carlo }
\NormalTok{med.out }\OtherTok{\textless{}{-}}
    \FunctionTok{mediate}\NormalTok{(}
\NormalTok{        med.fit,}
\NormalTok{        out.fit,}
        \AttributeTok{treat =} \StringTok{"treat"}\NormalTok{,}
        \AttributeTok{mediator =} \StringTok{"emo"}\NormalTok{,}
        \AttributeTok{robustSE =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{sims =} \DecValTok{1000} \CommentTok{\# should be 10000 in practice}
\NormalTok{    )}
\FunctionTok{summary}\NormalTok{(med.out)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Causal Mediation Analysis }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Quasi{-}Bayesian Confidence Intervals}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}                          Estimate 95\% CI Lower 95\% CI Upper p{-}value    }
\CommentTok{\#\textgreater{} ACME (control)             0.0826       0.0356         0.14  \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} ACME (treated)             0.0831       0.0348         0.14  \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} ADE (control)              0.0137      {-}0.0967         0.13    0.82    }
\CommentTok{\#\textgreater{} ADE (treated)              0.0142      {-}0.1101         0.14    0.82    }
\CommentTok{\#\textgreater{} Total Effect               0.0968      {-}0.0290         0.23    0.14    }
\CommentTok{\#\textgreater{} Prop. Mediated (control)   0.7706      {-}6.3968         4.70    0.14    }
\CommentTok{\#\textgreater{} Prop. Mediated (treated)   0.7938      {-}5.7506         4.52    0.14    }
\CommentTok{\#\textgreater{} ACME (average)             0.0829       0.0351         0.14  \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} ADE (average)              0.0140      {-}0.1047         0.13    0.82    }
\CommentTok{\#\textgreater{} Prop. Mediated (average)   0.7822      {-}6.0737         4.61    0.14    }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Sample Size Used: 265 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Simulations: 1000}
\end{Highlighting}
\end{Shaded}

Nonparametric bootstrap version

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{med.out }\OtherTok{\textless{}{-}}
    \FunctionTok{mediate}\NormalTok{(}
\NormalTok{        med.fit,}
\NormalTok{        out.fit,}
        \AttributeTok{boot =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{treat =} \StringTok{"treat"}\NormalTok{,}
        \AttributeTok{mediator =} \StringTok{"emo"}\NormalTok{,}
        \AttributeTok{sims =} \DecValTok{1000}\NormalTok{, }\CommentTok{\# should be 10000 in practice}
        \AttributeTok{boot.ci.type =} \StringTok{"bca"} \CommentTok{\# bias{-}corrected and accelerated intervals}
\NormalTok{    )}
\FunctionTok{summary}\NormalTok{(med.out)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Causal Mediation Analysis }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Nonparametric Bootstrap Confidence Intervals with the BCa Method}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}                          Estimate 95\% CI Lower 95\% CI Upper p{-}value   }
\CommentTok{\#\textgreater{} ACME (control)             0.0833       0.0386         0.15   0.002 **}
\CommentTok{\#\textgreater{} ACME (treated)             0.0844       0.0374         0.15   0.002 **}
\CommentTok{\#\textgreater{} ADE (control)              0.0114      {-}0.0875         0.13   0.792   }
\CommentTok{\#\textgreater{} ADE (treated)              0.0125      {-}0.1033         0.14   0.792   }
\CommentTok{\#\textgreater{} Total Effect               0.0958      {-}0.0291         0.23   0.124   }
\CommentTok{\#\textgreater{} Prop. Mediated (control)   0.8696     {-}97.4552         1.00   0.126   }
\CommentTok{\#\textgreater{} Prop. Mediated (treated)   0.8806     {-}82.9081         1.02   0.126   }
\CommentTok{\#\textgreater{} ACME (average)             0.0839       0.0381         0.15   0.002 **}
\CommentTok{\#\textgreater{} ADE (average)              0.0120      {-}0.0961         0.14   0.792   }
\CommentTok{\#\textgreater{} Prop. Mediated (average)   0.8751     {-}90.6217         1.01   0.126   }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Sample Size Used: 265 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Simulations: 1000}
\end{Highlighting}
\end{Shaded}

If theoretically understanding suggests that there is treatment and mediator interaction

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{med.fit }\OtherTok{\textless{}{-}}
    \FunctionTok{lm}\NormalTok{(emo }\SpecialCharTok{\textasciitilde{}}\NormalTok{ treat }\SpecialCharTok{+}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ educ }\SpecialCharTok{+}\NormalTok{ gender }\SpecialCharTok{+}\NormalTok{ income, }\AttributeTok{data =}\NormalTok{ framing)}
\NormalTok{out.fit }\OtherTok{\textless{}{-}}
    \FunctionTok{glm}\NormalTok{(}
\NormalTok{        cong\_mesg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ emo }\SpecialCharTok{*}\NormalTok{ treat }\SpecialCharTok{+}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ educ }\SpecialCharTok{+}\NormalTok{ gender }\SpecialCharTok{+}\NormalTok{ income,}
        \AttributeTok{data =}\NormalTok{ framing,}
        \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\StringTok{"probit"}\NormalTok{)}
\NormalTok{    )}
\NormalTok{med.out }\OtherTok{\textless{}{-}}
    \FunctionTok{mediate}\NormalTok{(}
\NormalTok{        med.fit,}
\NormalTok{        out.fit,}
        \AttributeTok{treat =} \StringTok{"treat"}\NormalTok{,}
        \AttributeTok{mediator =} \StringTok{"emo"}\NormalTok{,}
        \AttributeTok{robustSE =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{sims =} \DecValTok{100}
\NormalTok{    )}
\FunctionTok{summary}\NormalTok{(med.out)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Causal Mediation Analysis }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Quasi{-}Bayesian Confidence Intervals}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}                           Estimate 95\% CI Lower 95\% CI Upper p{-}value    }
\CommentTok{\#\textgreater{} ACME (control)            0.079925     0.035230         0.14  \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} ACME (treated)            0.097504     0.045279         0.17  \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} ADE (control)            {-}0.000865    {-}0.107228         0.11    0.98    }
\CommentTok{\#\textgreater{} ADE (treated)             0.016714    {-}0.121163         0.14    0.76    }
\CommentTok{\#\textgreater{} Total Effect              0.096640    {-}0.046523         0.23    0.26    }
\CommentTok{\#\textgreater{} Prop. Mediated (control)  0.672278    {-}5.266859         3.40    0.26    }
\CommentTok{\#\textgreater{} Prop. Mediated (treated)  0.860650    {-}6.754965         3.60    0.26    }
\CommentTok{\#\textgreater{} ACME (average)            0.088715     0.040207         0.15  \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} ADE (average)             0.007925    {-}0.111833         0.14    0.88    }
\CommentTok{\#\textgreater{} Prop. Mediated (average)  0.766464    {-}5.848496         3.43    0.26    }
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Sample Size Used: 265 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Simulations: 100}

\FunctionTok{test.TMint}\NormalTok{(med.out, }\AttributeTok{conf.level =}\NormalTok{ .}\DecValTok{95}\NormalTok{) }\CommentTok{\# test treatment{-}mediator interaction effect }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Test of ACME(1) {-} ACME(0) = 0}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  estimates from med.out}
\CommentTok{\#\textgreater{} ACME(1) {-} ACME(0) = 0.017579, p{-}value = 0.44}
\CommentTok{\#\textgreater{} alternative hypothesis: true ACME(1) {-} ACME(0) is not equal to 0}
\CommentTok{\#\textgreater{} 95 percent confidence interval:}
\CommentTok{\#\textgreater{}  {-}0.02676143  0.06257828}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(med.out)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{34-mediation_files/figure-latex/unnamed-chunk-5-1} \end{center}

\texttt{mediation} can be used in conjunction with any of your imputation packages.

And it can also handle \textbf{mediated moderation} or \textbf{non-binary treatment variables}, or \textbf{multi-level data}

Sensitivity Analysis for sequential ignorability

\begin{itemize}
\item
  test for unobserved pre-treatment covariates
\item
  \(\rho\) = correlation between the residuals of the mediator and outcome regressions.
\item
  If \(\rho\) is significant, we have evidence for violation of sequential ignorability (i.e., there is unobserved pre-treatment confounders).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{med.fit }\OtherTok{\textless{}{-}}
    \FunctionTok{lm}\NormalTok{(emo }\SpecialCharTok{\textasciitilde{}}\NormalTok{ treat }\SpecialCharTok{+}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ educ }\SpecialCharTok{+}\NormalTok{ gender }\SpecialCharTok{+}\NormalTok{ income, }\AttributeTok{data =}\NormalTok{ framing)}
\NormalTok{out.fit }\OtherTok{\textless{}{-}}
    \FunctionTok{glm}\NormalTok{(}
\NormalTok{        cong\_mesg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ emo }\SpecialCharTok{+}\NormalTok{ treat }\SpecialCharTok{+}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ educ }\SpecialCharTok{+}\NormalTok{ gender }\SpecialCharTok{+}\NormalTok{ income,}
        \AttributeTok{data =}\NormalTok{ framing,}
        \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\StringTok{"probit"}\NormalTok{)}
\NormalTok{    )}
\NormalTok{med.out }\OtherTok{\textless{}{-}}
    \FunctionTok{mediate}\NormalTok{(}
\NormalTok{        med.fit,}
\NormalTok{        out.fit,}
        \AttributeTok{treat =} \StringTok{"treat"}\NormalTok{,}
        \AttributeTok{mediator =} \StringTok{"emo"}\NormalTok{,}
        \AttributeTok{robustSE =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{sims =} \DecValTok{100}
\NormalTok{    )}
\NormalTok{sens.out }\OtherTok{\textless{}{-}}
    \FunctionTok{medsens}\NormalTok{(med.out,}
            \AttributeTok{rho.by =} \FloatTok{0.1}\NormalTok{, }\CommentTok{\# \textbackslash{}rho varies from {-}0.9 to 0.9 by 0.1}
            \AttributeTok{effect.type =} \StringTok{"indirect"}\NormalTok{, }\CommentTok{\# sensitivity on ACME}
            \CommentTok{\# effect.type = "direct", \# sensitivity on ADE}
            \CommentTok{\# effect.type = "both", \# sensitivity on ACME and ADE}
            \AttributeTok{sims =} \DecValTok{100}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(sens.out)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Mediation Sensitivity Analysis: Average Mediation Effect}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Sensitivity Region: ACME for Control Group}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}      Rho ACME(control) 95\% CI Lower 95\% CI Upper R\^{}2\_M*R\^{}2\_Y* R\^{}2\_M\textasciitilde{}R\^{}2\_Y\textasciitilde{}}
\CommentTok{\#\textgreater{} [1,] 0.3        0.0061      {-}0.0070       0.0163         0.09       0.0493}
\CommentTok{\#\textgreater{} [2,] 0.4       {-}0.0081      {-}0.0254       0.0043         0.16       0.0877}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Rho at which ACME for Control Group = 0: 0.3}
\CommentTok{\#\textgreater{} R\^{}2\_M*R\^{}2\_Y* at which ACME for Control Group = 0: 0.09}
\CommentTok{\#\textgreater{} R\^{}2\_M\textasciitilde{}R\^{}2\_Y\textasciitilde{} at which ACME for Control Group = 0: 0.0493 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Sensitivity Region: ACME for Treatment Group}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}      Rho ACME(treated) 95\% CI Lower 95\% CI Upper R\^{}2\_M*R\^{}2\_Y* R\^{}2\_M\textasciitilde{}R\^{}2\_Y\textasciitilde{}}
\CommentTok{\#\textgreater{} [1,] 0.3        0.0069      {-}0.0085       0.0197         0.09       0.0493}
\CommentTok{\#\textgreater{} [2,] 0.4       {-}0.0099      {-}0.0304       0.0054         0.16       0.0877}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Rho at which ACME for Treatment Group = 0: 0.3}
\CommentTok{\#\textgreater{} R\^{}2\_M*R\^{}2\_Y* at which ACME for Treatment Group = 0: 0.09}
\CommentTok{\#\textgreater{} R\^{}2\_M\textasciitilde{}R\^{}2\_Y\textasciitilde{} at which ACME for Treatment Group = 0: 0.0493}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(sens.out, }\AttributeTok{sens.par =} \StringTok{"rho"}\NormalTok{, }\AttributeTok{main =} \StringTok{"Anxiety"}\NormalTok{, }\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.2}\NormalTok{, }\FloatTok{0.2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{34-mediation_files/figure-latex/unnamed-chunk-7-1} \end{center}

\begin{center}\includegraphics[width=0.9\linewidth]{34-mediation_files/figure-latex/unnamed-chunk-7-2} \end{center}

ACME confidence intervals contains 0 when \(\rho \in (0.3,0.4)\)

Alternatively, using \(R^2\) interpretation, we need to specify the direction of confounder that affects the mediator and outcome variables in \texttt{plot} using \texttt{sign.prod\ =\ "positive"} (i.e., same direction) or \texttt{sign.prod\ =\ "negative"} (i.e., opposite direction).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(sens.out, }\AttributeTok{sens.par =} \StringTok{"R2"}\NormalTok{, }\AttributeTok{r.type =} \StringTok{"total"}\NormalTok{, }\AttributeTok{sign.prod =} \StringTok{"positive"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{34-mediation_files/figure-latex/unnamed-chunk-8-1} \end{center}

\begin{center}\includegraphics[width=0.9\linewidth]{34-mediation_files/figure-latex/unnamed-chunk-8-2} \end{center}

\hypertarget{directed-acyclic-graph}{%
\chapter{Directed Acyclic Graph}\label{directed-acyclic-graph}}

Native R:

\begin{itemize}
\item
  \texttt{dagitty}
\item
  \texttt{ggdag}
\item
  \texttt{dagR}
\item
  \texttt{r-causal}: by \href{https://www.ccd.pitt.edu/data-science/}{Center for Causal Discovery}. Also available in Python
\end{itemize}

Publication-ready (with \texttt{R} and \texttt{Latex}): \href{https://www.gerkelab.com/project/shinydag/}{shinyDAG}

Standalone program: \href{https://hsz.dife.de/dag/}{DAG program} by Sven Knuppel

\hypertarget{basic-notations}{%
\section{Basic Notations}\label{basic-notations}}

Basic building blocks of DAG

\begin{itemize}
\item
  Mediators (chains): \(X \to Z \to Y\)

  \begin{itemize}
  \tightlist
  \item
    controlling for Z blocks (closes) the causal impact of \(X \to Y\)
  \end{itemize}
\item
  Common causes (forks): \(X \leftarrow Z \to Y\)

  \begin{itemize}
  \item
    Z (i.e., confounder) is a common cause in which it induces a non-causal association between \(X\) and \(Y\).
  \item
    Controlling for \(Z\) should close this association.
  \item
    \(Z\) d-separates \(X\) from \(Y\) when it blocks (closes) all paths from \(X\) to \(Y\) (i.e., \(X \perp Y |Z\)). This applies to both common causes and mediators.
  \end{itemize}
\item
  Common effects (colliders): \(X \to Z \leftarrow Y\)

  \begin{itemize}
  \item
    Not controlling for \(Z\) does not induce an association between \(X\) and \(Y\)
  \item
    Controlling for \(Z\) induces a non-causal association between \(X\) and \(Y\)
  \end{itemize}
\end{itemize}

Notes:

\begin{itemize}
\item
  A descendant of a variable behavior similarly to that variable (e.g., a descendant of \(Z\) can behave like \(Z\) and partially control for \(Z\))
\item
  Rule of thumb for multiple \protect\hyperlink{controls}{Controls}: o have \protect\hyperlink{causal-inference}{Causal inference} \(X \to Y\), we must

  \begin{itemize}
  \item
    Close all backdoor path between \(X\) and \(Y\) (to eliminate spurious correlation)
  \item
    Do not close any causal path between \(X\) and \(Y\) (any mediators).
  \end{itemize}
\end{itemize}

\hypertarget{report}{%
\chapter{Report}\label{report}}

Structure

\begin{itemize}
\item
  Exploratory analysis

  \begin{itemize}
  \tightlist
  \item
    plots
  \item
    preliminary results
  \item
    interesting structure/features in the data
  \item
    outliers
  \end{itemize}
\item
  Model

  \begin{itemize}
  \tightlist
  \item
    Assumptions
  \item
    Why this model/ How is this model the best one?
  \item
    Consideration: interactions, collinearity, dependence
  \end{itemize}
\item
  Model Fit

  \begin{itemize}
  \item
    How well does it fit?
  \item
    Are the model assumptions met?

    \begin{itemize}
    \tightlist
    \item
      Residual analysis
    \end{itemize}
  \end{itemize}
\item
  Inference/ Prediction

  \begin{itemize}
  \tightlist
  \item
    Are there different way to support your inference?
  \end{itemize}
\item
  Conclusion

  \begin{itemize}
  \item
    Recommendation
  \item
    Limitation of the analysis
  \item
    How to correct those in the future
  \end{itemize}
\end{itemize}

This chapter is based on the \texttt{jtools} package. More information can be found \href{https://www.rdocumentation.org/packages/jtools/versions/2.1.0}{here.}

\hypertarget{one-summary-table}{%
\section{One summary table}\label{one-summary-table}}

Packages for reporting:

Summary Statistics Table:

\begin{itemize}
\tightlist
\item
  \href{https://cran.r-project.org/web/packages/qwraps2/vignettes/summary-statistics.html}{qwraps2}
\item
  \href{https://cran.r-project.org/web/packages/vtable/vignettes/sumtable.html}{vtable}
\item
  \href{http://www.danieldsjoberg.com/gtsummary/}{gtsummary}
\item
  \href{https://cran.r-project.org/web/packages/apaTables/apaTables.pdf}{apaTables}
\item
  \href{https://cran.r-project.org/web/packages/stargazer/vignettes/stargazer.pdf}{stargazer}
\end{itemize}

Regression Table

\begin{itemize}
\tightlist
\item
  \href{http://www.danieldsjoberg.com/gtsummary/}{gtsummary}
\item
  \href{https://cran.r-project.org/web/packages/sjPlot/vignettes/tab_model_estimates.html}{sjPlot,sjmisc, sjlabelled}
\item
  \href{https://cran.r-project.org/web/packages/stargazer/vignettes/stargazer.pdf}{stargazer}: recommended (\href{https://www.jakeruss.com/cheatsheets/stargazer/}{Example})
\item
  \href{https://github.com/vincentarelbundock/modelsummary\#a-simple-example}{modelsummary}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(jtools)}
\FunctionTok{data}\NormalTok{(movies)}
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(metascore }\SpecialCharTok{\textasciitilde{}}\NormalTok{ budget }\SpecialCharTok{+}\NormalTok{ us\_gross }\SpecialCharTok{+}\NormalTok{ year, }\AttributeTok{data =}\NormalTok{ movies)}
\FunctionTok{summ}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]
\centering
\begin{tabular}{lr}
\toprule
\cellcolor{gray!6}{Observations} & \cellcolor{gray!6}{831 (10 missing obs. deleted)}\\
Dependent variable & metascore\\
\cellcolor{gray!6}{Type} & \cellcolor{gray!6}{OLS linear regression}\\
\bottomrule
\end{tabular}
\end{table} \begin{table}[!h]
\centering
\begin{tabular}{lr}
\toprule
\cellcolor{gray!6}{F(3,827)} & \cellcolor{gray!6}{26.23}\\
RÂ² & 0.09\\
\cellcolor{gray!6}{Adj. RÂ²} & \cellcolor{gray!6}{0.08}\\
\bottomrule
\end{tabular}
\end{table} \begin{table}[!h]
\centering
\begin{threeparttable}
\begin{tabular}{lrrrr}
\toprule
  & Est. & S.E. & t val. & p\\
\midrule
\cellcolor{gray!6}{(Intercept)} & \cellcolor{gray!6}{52.06} & \cellcolor{gray!6}{139.67} & \cellcolor{gray!6}{0.37} & \cellcolor{gray!6}{0.71}\\
budget & -0.00 & 0.00 & -5.89 & 0.00\\
\cellcolor{gray!6}{us\_gross} & \cellcolor{gray!6}{0.00} & \cellcolor{gray!6}{0.00} & \cellcolor{gray!6}{7.61} & \cellcolor{gray!6}{0.00}\\
year & 0.01 & 0.07 & 0.08 & 0.94\\
\bottomrule
\end{tabular}
\begin{tablenotes}
\item Standard errors: OLS
\end{tablenotes}
\end{threeparttable}
\end{table}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summ}\NormalTok{(}
\NormalTok{    fit,}
    \AttributeTok{scale =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{vifs =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{part.corr =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{confint =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{pvals =} \ConstantTok{FALSE}
\NormalTok{) }\CommentTok{\# notice that scale here is TRUE}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]
\centering
\begin{tabular}{lr}
\toprule
\cellcolor{gray!6}{Observations} & \cellcolor{gray!6}{831 (10 missing obs. deleted)}\\
Dependent variable & metascore\\
\cellcolor{gray!6}{Type} & \cellcolor{gray!6}{OLS linear regression}\\
\bottomrule
\end{tabular}
\end{table} \begin{table}[!h]
\centering
\begin{tabular}{lr}
\toprule
\cellcolor{gray!6}{F(3,827)} & \cellcolor{gray!6}{26.23}\\
RÂ² & 0.09\\
\cellcolor{gray!6}{Adj. RÂ²} & \cellcolor{gray!6}{0.08}\\
\bottomrule
\end{tabular}
\end{table} \begin{table}[!h]
\centering
\begin{threeparttable}
\begin{tabular}{lrrrrrrr}
\toprule
  & Est. & 2.5\% & 97.5\% & t val. & VIF & partial.r & part.r\\
\midrule
\cellcolor{gray!6}{(Intercept)} & \cellcolor{gray!6}{63.01} & \cellcolor{gray!6}{61.91} & \cellcolor{gray!6}{64.11} & \cellcolor{gray!6}{112.23} & \cellcolor{gray!6}{NA} & \cellcolor{gray!6}{NA} & \cellcolor{gray!6}{NA}\\
budget & -3.78 & -5.05 & -2.52 & -5.89 & 1.31 & -0.20 & -0.20\\
\cellcolor{gray!6}{us\_gross} & \cellcolor{gray!6}{5.28} & \cellcolor{gray!6}{3.92} & \cellcolor{gray!6}{6.64} & \cellcolor{gray!6}{7.61} & \cellcolor{gray!6}{1.52} & \cellcolor{gray!6}{0.26} & \cellcolor{gray!6}{0.25}\\
year & 0.05 & -1.18 & 1.28 & 0.08 & 1.24 & 0.00 & 0.00\\
\bottomrule
\end{tabular}
\begin{tablenotes}
\item Standard errors: OLS; Continuous predictors are mean-centered and scaled by 1 s.d. The outcome variable remains in its original units.
\end{tablenotes}
\end{threeparttable}
\end{table}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\#obtain clsuter{-}robust SE}
\FunctionTok{data}\NormalTok{(}\StringTok{"PetersenCL"}\NormalTok{, }\AttributeTok{package =} \StringTok{"sandwich"}\NormalTok{)}
\NormalTok{fit2 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{data =}\NormalTok{ PetersenCL)}
\FunctionTok{summ}\NormalTok{(fit2, }\AttributeTok{robust =} \StringTok{"HC3"}\NormalTok{, }\AttributeTok{cluster =} \StringTok{"firm"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]
\centering
\begin{tabular}{lr}
\toprule
\cellcolor{gray!6}{Observations} & \cellcolor{gray!6}{5000}\\
Dependent variable & y\\
\cellcolor{gray!6}{Type} & \cellcolor{gray!6}{OLS linear regression}\\
\bottomrule
\end{tabular}
\end{table} \begin{table}[!h]
\centering
\begin{tabular}{lr}
\toprule
\cellcolor{gray!6}{F(1,4998)} & \cellcolor{gray!6}{1310.74}\\
RÂ² & 0.21\\
\cellcolor{gray!6}{Adj. RÂ²} & \cellcolor{gray!6}{0.21}\\
\bottomrule
\end{tabular}
\end{table} \begin{table}[!h]
\centering
\begin{threeparttable}
\begin{tabular}{lrrrr}
\toprule
  & Est. & S.E. & t val. & p\\
\midrule
\cellcolor{gray!6}{(Intercept)} & \cellcolor{gray!6}{0.03} & \cellcolor{gray!6}{0.07} & \cellcolor{gray!6}{0.44} & \cellcolor{gray!6}{0.66}\\
x & 1.03 & 0.05 & 20.36 & 0.00\\
\bottomrule
\end{tabular}
\begin{tablenotes}
\item Standard errors: Cluster-robust, type = HC3
\end{tablenotes}
\end{threeparttable}
\end{table}

Model to Equation

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# install.packages("equatiomatic") \# not available for R 4.2}
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(metascore }\SpecialCharTok{\textasciitilde{}}\NormalTok{ budget }\SpecialCharTok{+}\NormalTok{ us\_gross }\SpecialCharTok{+}\NormalTok{ year, }\AttributeTok{data =}\NormalTok{ movies)}
\CommentTok{\# show the theoretical model}
\NormalTok{equatiomatic}\SpecialCharTok{::}\FunctionTok{extract\_eq}\NormalTok{(fit)}
\CommentTok{\# display the actual coefficients}
\NormalTok{equatiomatic}\SpecialCharTok{::}\FunctionTok{extract\_eq}\NormalTok{(fit, }\AttributeTok{use\_coefs =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{model-comparison}{%
\section{Model Comparison}\label{model-comparison}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(metascore }\SpecialCharTok{\textasciitilde{}} \FunctionTok{log}\NormalTok{(budget), }\AttributeTok{data =}\NormalTok{ movies)}
\NormalTok{fit\_b }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(metascore }\SpecialCharTok{\textasciitilde{}} \FunctionTok{log}\NormalTok{(budget) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{(us\_gross), }\AttributeTok{data =}\NormalTok{ movies)}
\NormalTok{fit\_c }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(metascore }\SpecialCharTok{\textasciitilde{}} \FunctionTok{log}\NormalTok{(budget) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{(us\_gross) }\SpecialCharTok{+}\NormalTok{ runtime, }\AttributeTok{data =}\NormalTok{ movies)}
\NormalTok{coef\_names }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Budget"} \OtherTok{=} \StringTok{"log(budget)"}\NormalTok{, }\StringTok{"US Gross"} \OtherTok{=} \StringTok{"log(us\_gross)"}\NormalTok{,}
                \StringTok{"Runtime (Hours)"} \OtherTok{=} \StringTok{"runtime"}\NormalTok{, }\StringTok{"Constant"} \OtherTok{=} \StringTok{"(Intercept)"}\NormalTok{)}
\FunctionTok{export\_summs}\NormalTok{(fit, fit\_b, fit\_c, }\AttributeTok{robust =} \StringTok{"HC3"}\NormalTok{, }\AttributeTok{coefs =}\NormalTok{ coef\_names)}
\end{Highlighting}
\end{Shaded}

 
  \providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
  \providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
  \providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
  \providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

\begin{table}[ht]
\begin{centerbox}
\begin{threeparttable}
\captionsetup{justification=centering,singlelinecheck=off}
\caption{\label{tab:unnamed-chunk-3} }
 \setlength{\tabcolsep}{0pt}
\begin{tabular}{l l l l}


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 1 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\centering \hspace{6pt} Model 3 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} Budget \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -2.43 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -5.16 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} -6.70 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.44)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.62)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.67)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} US Gross \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 3.96 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 3.85 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.51)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (0.48)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} Runtime (Hours) \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 14.29 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (1.63)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} Constant \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 105.29 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 81.84 *** \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 83.35 *** \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt}  \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (7.65)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (8.66)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} (8.82)\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{255, 255, 255}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}->{\huxb{0, 0, 0}{0.4}}-}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} N \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 831\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 831\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 831\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}

\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} R2 \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.03\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.09\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} &
\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.17\hphantom{0}\hphantom{0}\hphantom{0}\hphantom{0} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{>{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}->{\huxb{0, 0, 0}{0.8}}-}
\arrayrulecolor{black}

\multicolumn{4}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} Standard errors are heteroskedasticity robust.  *** p $<$ 0.001;  ** p $<$ 0.01;  * p $<$ 0.05. \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


\hhline{}
\arrayrulecolor{black}
\end{tabular}
\end{threeparttable}\par\end{centerbox}

\end{table}
 

Another package is \texttt{modelsummary}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(modelsummary)}
\NormalTok{lm\_mod }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ wt }\SpecialCharTok{+}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ cyl, mtcars)}
\FunctionTok{msummary}\NormalTok{(lm\_mod, }\AttributeTok{vcov =} \FunctionTok{c}\NormalTok{(}\StringTok{"iid"}\NormalTok{,}\StringTok{"robust"}\NormalTok{,}\StringTok{"HC4"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{table}
\centering
\begin{tabular}[t]{lccc}
\toprule
  & (1) & (2) & (3)\\
\midrule
(Intercept) & \num{38.752} & \num{38.752} & \num{38.752}\\
 & (\num{1.787}) & (\num{2.286}) & (\num{2.177})\\
wt & \num{-3.167} & \num{-3.167} & \num{-3.167}\\
 & (\num{0.741}) & (\num{0.833}) & (\num{0.819})\\
hp & \num{-0.018} & \num{-0.018} & \num{-0.018}\\
 & (\num{0.012}) & (\num{0.010}) & (\num{0.013})\\
cyl & \num{-0.942} & \num{-0.942} & \num{-0.942}\\
 & (\num{0.551}) & (\num{0.573}) & (\num{0.572})\\
\midrule
Num.Obs. & \num{32} & \num{32} & \num{32}\\
R2 & \num{0.843} & \num{0.843} & \num{0.843}\\
R2 Adj. & \num{0.826} & \num{0.826} & \num{0.826}\\
AIC & \num{155.5} & \num{155.5} & \num{155.5}\\
BIC & \num{162.8} & \num{162.8} & \num{162.8}\\
Log.Lik. & \num{-72.738} & \num{-72.738} & \num{-72.738}\\
F & \num{50.171} & \num{31.065} & \num{32.623}\\
RMSE & \num{2.35} & \num{2.35} & \num{2.35}\\
Std.Errors & IID & HC3 & HC4\\
\bottomrule
\end{tabular}
\end{table}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{modelplot}\NormalTok{(lm\_mod, }\AttributeTok{vcov =} \FunctionTok{c}\NormalTok{(}\StringTok{"iid"}\NormalTok{,}\StringTok{"robust"}\NormalTok{,}\StringTok{"HC4"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{36-report_files/figure-latex/unnamed-chunk-4-1} \end{center}

Another package is \texttt{stargazer}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"stargazer"}\NormalTok{)}
\FunctionTok{stargazer}\NormalTok{(attitude)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} \% Table created by stargazer v.5.2.3 by Marek Hlavac, Social Policy Institute. E{-}mail: marek.hlavac at gmail.com}
\CommentTok{\#\textgreater{} \% Date and time: Fri, Jan 12, 2024 {-} 5:16:27 PM}
\CommentTok{\#\textgreater{} \textbackslash{}begin\{table\}[!htbp] \textbackslash{}centering }
\CommentTok{\#\textgreater{}   \textbackslash{}caption\{\} }
\CommentTok{\#\textgreater{}   \textbackslash{}label\{\} }
\CommentTok{\#\textgreater{} \textbackslash{}begin\{tabular\}\{@\{\textbackslash{}extracolsep\{5pt\}\}lccccc\} }
\CommentTok{\#\textgreater{} \textbackslash{}\textbackslash{}[{-}1.8ex]\textbackslash{}hline }
\CommentTok{\#\textgreater{} \textbackslash{}hline \textbackslash{}\textbackslash{}[{-}1.8ex] }
\CommentTok{\#\textgreater{} Statistic \& \textbackslash{}multicolumn\{1\}\{c\}\{N\} \& \textbackslash{}multicolumn\{1\}\{c\}\{Mean\} \& \textbackslash{}multicolumn\{1\}\{c\}\{St. Dev.\} \& \textbackslash{}multicolumn\{1\}\{c\}\{Min\} \& \textbackslash{}multicolumn\{1\}\{c\}\{Max\} \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{} \textbackslash{}hline \textbackslash{}\textbackslash{}[{-}1.8ex] }
\CommentTok{\#\textgreater{} rating \& 30 \& 64.633 \& 12.173 \& 40 \& 85 \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{} complaints \& 30 \& 66.600 \& 13.315 \& 37 \& 90 \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{} privileges \& 30 \& 53.133 \& 12.235 \& 30 \& 83 \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{} learning \& 30 \& 56.367 \& 11.737 \& 34 \& 75 \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{} raises \& 30 \& 64.633 \& 10.397 \& 43 \& 88 \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{} critical \& 30 \& 74.767 \& 9.895 \& 49 \& 92 \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{} advance \& 30 \& 42.933 \& 10.289 \& 25 \& 72 \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{} \textbackslash{}hline \textbackslash{}\textbackslash{}[{-}1.8ex] }
\CommentTok{\#\textgreater{} \textbackslash{}end\{tabular\} }
\CommentTok{\#\textgreater{} \textbackslash{}end\{table\}}
\DocumentationTok{\#\# 2 OLS models}
\NormalTok{linear}\FloatTok{.1} \OtherTok{\textless{}{-}}
    \FunctionTok{lm}\NormalTok{(rating }\SpecialCharTok{\textasciitilde{}}\NormalTok{ complaints }\SpecialCharTok{+}\NormalTok{ privileges }\SpecialCharTok{+}\NormalTok{ learning }\SpecialCharTok{+}\NormalTok{ raises }\SpecialCharTok{+}\NormalTok{ critical,}
       \AttributeTok{data =}\NormalTok{ attitude)}
\NormalTok{linear}\FloatTok{.2} \OtherTok{\textless{}{-}}
    \FunctionTok{lm}\NormalTok{(rating }\SpecialCharTok{\textasciitilde{}}\NormalTok{ complaints }\SpecialCharTok{+}\NormalTok{ privileges }\SpecialCharTok{+}\NormalTok{ learning, }\AttributeTok{data =}\NormalTok{ attitude)}
\DocumentationTok{\#\# create an indicator dependent variable, and run a probit model}
\NormalTok{attitude}\SpecialCharTok{$}\NormalTok{high.rating }\OtherTok{\textless{}{-}}\NormalTok{ (attitude}\SpecialCharTok{$}\NormalTok{rating }\SpecialCharTok{\textgreater{}} \DecValTok{70}\NormalTok{)}

\NormalTok{probit.model }\OtherTok{\textless{}{-}}
    \FunctionTok{glm}\NormalTok{(}
\NormalTok{        high.rating }\SpecialCharTok{\textasciitilde{}}\NormalTok{ learning }\SpecialCharTok{+}\NormalTok{ critical }\SpecialCharTok{+}\NormalTok{ advance,}
        \AttributeTok{data =}\NormalTok{ attitude,}
        \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"probit"}\NormalTok{)}
\NormalTok{    )}
\FunctionTok{stargazer}\NormalTok{(linear}\FloatTok{.1}\NormalTok{,}
\NormalTok{          linear}\FloatTok{.2}\NormalTok{,}
\NormalTok{          probit.model,}
          \AttributeTok{title =} \StringTok{"Results"}\NormalTok{,}
          \AttributeTok{align =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} \% Table created by stargazer v.5.2.3 by Marek Hlavac, Social Policy Institute. E{-}mail: marek.hlavac at gmail.com}
\CommentTok{\#\textgreater{} \% Date and time: Fri, Jan 12, 2024 {-} 5:16:27 PM}
\CommentTok{\#\textgreater{} \% Requires LaTeX packages: dcolumn }
\CommentTok{\#\textgreater{} \textbackslash{}begin\{table\}[!htbp] \textbackslash{}centering }
\CommentTok{\#\textgreater{}   \textbackslash{}caption\{Results\} }
\CommentTok{\#\textgreater{}   \textbackslash{}label\{\} }
\CommentTok{\#\textgreater{} \textbackslash{}begin\{tabular\}\{@\{\textbackslash{}extracolsep\{5pt\}\}lD\{.\}\{.\}\{{-}3\} D\{.\}\{.\}\{{-}3\} D\{.\}\{.\}\{{-}3\} \} }
\CommentTok{\#\textgreater{} \textbackslash{}\textbackslash{}[{-}1.8ex]\textbackslash{}hline }
\CommentTok{\#\textgreater{} \textbackslash{}hline \textbackslash{}\textbackslash{}[{-}1.8ex] }
\CommentTok{\#\textgreater{}  \& \textbackslash{}multicolumn\{3\}\{c\}\{\textbackslash{}textit\{Dependent variable:\}\} \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{} \textbackslash{}cline\{2{-}4\} }
\CommentTok{\#\textgreater{} \textbackslash{}\textbackslash{}[{-}1.8ex] \& \textbackslash{}multicolumn\{2\}\{c\}\{rating\} \& \textbackslash{}multicolumn\{1\}\{c\}\{high.rating\} \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{} \textbackslash{}\textbackslash{}[{-}1.8ex] \& \textbackslash{}multicolumn\{2\}\{c\}\{\textbackslash{}textit\{OLS\}\} \& \textbackslash{}multicolumn\{1\}\{c\}\{\textbackslash{}textit\{probit\}\} \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{} \textbackslash{}\textbackslash{}[{-}1.8ex] \& \textbackslash{}multicolumn\{1\}\{c\}\{(1)\} \& \textbackslash{}multicolumn\{1\}\{c\}\{(2)\} \& \textbackslash{}multicolumn\{1\}\{c\}\{(3)\}\textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{} \textbackslash{}hline \textbackslash{}\textbackslash{}[{-}1.8ex] }
\CommentTok{\#\textgreater{}  complaints \& 0.692\^{}\{***\} \& 0.682\^{}\{***\} \&  \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{}   \& (0.149) \& (0.129) \&  \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{}   \& \& \& \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{}  privileges \& {-}0.104 \& {-}0.103 \&  \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{}   \& (0.135) \& (0.129) \&  \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{}   \& \& \& \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{}  learning \& 0.249 \& 0.238\^{}\{*\} \& 0.164\^{}\{***\} \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{}   \& (0.160) \& (0.139) \& (0.053) \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{}   \& \& \& \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{}  raises \& {-}0.033 \&  \&  \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{}   \& (0.202) \&  \&  \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{}   \& \& \& \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{}  critical \& 0.015 \&  \& {-}0.001 \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{}   \& (0.147) \&  \& (0.044) \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{}   \& \& \& \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{}  advance \&  \&  \& {-}0.062 \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{}   \&  \&  \& (0.042) \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{}   \& \& \& \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{}  Constant \& 11.011 \& 11.258 \& {-}7.476\^{}\{**\} \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{}   \& (11.704) \& (7.318) \& (3.570) \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{}   \& \& \& \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{} \textbackslash{}hline \textbackslash{}\textbackslash{}[{-}1.8ex] }
\CommentTok{\#\textgreater{} Observations \& \textbackslash{}multicolumn\{1\}\{c\}\{30\} \& \textbackslash{}multicolumn\{1\}\{c\}\{30\} \& \textbackslash{}multicolumn\{1\}\{c\}\{30\} \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{} R$\^{}\{2\}$ \& \textbackslash{}multicolumn\{1\}\{c\}\{0.715\} \& \textbackslash{}multicolumn\{1\}\{c\}\{0.715\} \&  \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{} Adjusted R$\^{}\{2\}$ \& \textbackslash{}multicolumn\{1\}\{c\}\{0.656\} \& \textbackslash{}multicolumn\{1\}\{c\}\{0.682\} \&  \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{} Log Likelihood \&  \&  \& \textbackslash{}multicolumn\{1\}\{c\}\{{-}9.087\} \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{} Akaike Inf. Crit. \&  \&  \& \textbackslash{}multicolumn\{1\}\{c\}\{26.175\} \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{} Residual Std. Error \& \textbackslash{}multicolumn\{1\}\{c\}\{7.139 (df = 24)\} \& \textbackslash{}multicolumn\{1\}\{c\}\{6.863 (df = 26)\} \&  \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{} F Statistic \& \textbackslash{}multicolumn\{1\}\{c\}\{12.063$\^{}\{***\}$ (df = 5; 24)\} \& \textbackslash{}multicolumn\{1\}\{c\}\{21.743$\^{}\{***\}$ (df = 3; 26)\} \&  \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{} \textbackslash{}hline }
\CommentTok{\#\textgreater{} \textbackslash{}hline \textbackslash{}\textbackslash{}[{-}1.8ex] }
\CommentTok{\#\textgreater{} \textbackslash{}textit\{Note:\}  \& \textbackslash{}multicolumn\{3\}\{r\}\{$\^{}\{*\}$p$\textless{}$0.1; $\^{}\{**\}$p$\textless{}$0.05; $\^{}\{***\}$p$\textless{}$0.01\} \textbackslash{}\textbackslash{} }
\CommentTok{\#\textgreater{} \textbackslash{}end\{tabular\} }
\CommentTok{\#\textgreater{} \textbackslash{}end\{table\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Latex}
\FunctionTok{stargazer}\NormalTok{(}
\NormalTok{    linear}\FloatTok{.1}\NormalTok{,}
\NormalTok{    linear}\FloatTok{.2}\NormalTok{,}
\NormalTok{    probit.model,}
    \AttributeTok{title =} \StringTok{"Regression Results"}\NormalTok{,}
    \AttributeTok{align =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{dep.var.labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Overall Rating"}\NormalTok{, }\StringTok{"High Rating"}\NormalTok{),}
    \AttributeTok{covariate.labels =} \FunctionTok{c}\NormalTok{(}
        \StringTok{"Handling of Complaints"}\NormalTok{,}
        \StringTok{"No Special Privileges"}\NormalTok{,}
        \StringTok{"Opportunity to Learn"}\NormalTok{,}
        \StringTok{"Performance{-}Based Raises"}\NormalTok{,}
        \StringTok{"Too Critical"}\NormalTok{,}
        \StringTok{"Advancement"}
\NormalTok{    ),}
    \AttributeTok{omit.stat =} \FunctionTok{c}\NormalTok{(}\StringTok{"LL"}\NormalTok{, }\StringTok{"ser"}\NormalTok{, }\StringTok{"f"}\NormalTok{),}
    \AttributeTok{no.space =} \ConstantTok{TRUE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# ASCII text output}
\FunctionTok{stargazer}\NormalTok{(}
\NormalTok{    linear}\FloatTok{.1}\NormalTok{,}
\NormalTok{    linear}\FloatTok{.2}\NormalTok{,}
    \AttributeTok{type =} \StringTok{"text"}\NormalTok{,}
    \AttributeTok{title =} \StringTok{"Regression Results"}\NormalTok{,}
    \AttributeTok{dep.var.labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Overall Rating"}\NormalTok{, }\StringTok{"High Rating"}\NormalTok{),}
    \AttributeTok{covariate.labels =} \FunctionTok{c}\NormalTok{(}
        \StringTok{"Handling of Complaints"}\NormalTok{,}
        \StringTok{"No Special Privileges"}\NormalTok{,}
        \StringTok{"Opportunity to Learn"}\NormalTok{,}
        \StringTok{"Performance{-}Based Raises"}\NormalTok{,}
        \StringTok{"Too Critical"}\NormalTok{,}
        \StringTok{"Advancement"}
\NormalTok{    ),}
    \AttributeTok{omit.stat =} \FunctionTok{c}\NormalTok{(}\StringTok{"LL"}\NormalTok{, }\StringTok{"ser"}\NormalTok{, }\StringTok{"f"}\NormalTok{),}
    \AttributeTok{ci =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{ci.level =} \FloatTok{0.90}\NormalTok{,}
    \AttributeTok{single.row =} \ConstantTok{TRUE}
\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Regression Results}
\CommentTok{\#\textgreater{} ========================================================================}
\CommentTok{\#\textgreater{}                                        Dependent variable:              }
\CommentTok{\#\textgreater{}                          {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}                                          Overall Rating                 }
\CommentTok{\#\textgreater{}                                    (1)                     (2)          }
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} Handling of Complaints   0.692*** (0.447, 0.937) 0.682*** (0.470, 0.894)}
\CommentTok{\#\textgreater{} No Special Privileges    {-}0.104 ({-}0.325, 0.118)  {-}0.103 ({-}0.316, 0.109) }
\CommentTok{\#\textgreater{} Opportunity to Learn      0.249 ({-}0.013, 0.512)   0.238* (0.009, 0.467) }
\CommentTok{\#\textgreater{} Performance{-}Based Raises {-}0.033 ({-}0.366, 0.299)                         }
\CommentTok{\#\textgreater{} Too Critical              0.015 ({-}0.227, 0.258)                         }
\CommentTok{\#\textgreater{} Advancement              11.011 ({-}8.240, 30.262) 11.258 ({-}0.779, 23.296)}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} Observations                       30                      30           }
\CommentTok{\#\textgreater{} R2                                0.715                   0.715         }
\CommentTok{\#\textgreater{} Adjusted R2                       0.656                   0.682         }
\CommentTok{\#\textgreater{} ========================================================================}
\CommentTok{\#\textgreater{} Note:                                        *p\textless{}0.1; **p\textless{}0.05; ***p\textless{}0.01}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{stargazer}\NormalTok{(}
\NormalTok{    linear}\FloatTok{.1}\NormalTok{,}
\NormalTok{    linear}\FloatTok{.2}\NormalTok{,}
\NormalTok{    probit.model,}
    \AttributeTok{title =} \StringTok{"Regression Results"}\NormalTok{,}
    \AttributeTok{align =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{dep.var.labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Overall Rating"}\NormalTok{, }\StringTok{"High Rating"}\NormalTok{),}
    \AttributeTok{covariate.labels =} \FunctionTok{c}\NormalTok{(}
        \StringTok{"Handling of Complaints"}\NormalTok{,}
        \StringTok{"No Special Privileges"}\NormalTok{,}
        \StringTok{"Opportunity to Learn"}\NormalTok{,}
        \StringTok{"Performance{-}Based Raises"}\NormalTok{,}
        \StringTok{"Too Critical"}\NormalTok{,}
        \StringTok{"Advancement"}
\NormalTok{    ),}
    \AttributeTok{omit.stat =} \FunctionTok{c}\NormalTok{(}\StringTok{"LL"}\NormalTok{, }\StringTok{"ser"}\NormalTok{, }\StringTok{"f"}\NormalTok{),}
    \AttributeTok{no.space =} \ConstantTok{TRUE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Correlation Table

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{correlation.matrix }\OtherTok{\textless{}{-}}
    \FunctionTok{cor}\NormalTok{(attitude[, }\FunctionTok{c}\NormalTok{(}\StringTok{"rating"}\NormalTok{, }\StringTok{"complaints"}\NormalTok{, }\StringTok{"privileges"}\NormalTok{)])}
\FunctionTok{stargazer}\NormalTok{(correlation.matrix, }\AttributeTok{title =} \StringTok{"Correlation Matrix"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{changes-in-an-estimate}{%
\section{Changes in an estimate}\label{changes-in-an-estimate}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{coef\_names }\OtherTok{\textless{}{-}}\NormalTok{ coef\_names[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{] }\CommentTok{\# Dropping intercept for plots}
\FunctionTok{plot\_summs}\NormalTok{(fit, fit\_b, fit\_c, }\AttributeTok{robust =} \StringTok{"HC3"}\NormalTok{, }\AttributeTok{coefs =}\NormalTok{ coef\_names)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{36-report_files/figure-latex/unnamed-chunk-10-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot\_summs}\NormalTok{(}
\NormalTok{    fit\_c,}
    \AttributeTok{robust =} \StringTok{"HC3"}\NormalTok{,}
    \AttributeTok{coefs =}\NormalTok{ coef\_names,}
    \AttributeTok{plot.distributions =} \ConstantTok{TRUE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{36-report_files/figure-latex/unnamed-chunk-10-2} \end{center}

\hypertarget{standard-errors-2}{%
\section{Standard Errors}\label{standard-errors-2}}

\texttt{sandwich} \href{cran.r-project.org/web/packages/sandwich/vignettes/sandwich-CL.pdf}{vignette}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.0890}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.0890}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.6233}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1849}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Applicable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Usage
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Reference
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{const} & & Assume constant variances & \\
\texttt{HC} \texttt{HC0} & \texttt{vcovCL} & Heterogeneity

White's estimator

All other heterogeneity SE methods are derivatives of this.

No small sample bias adjustment & \citep{white1980} \\
\texttt{HC1} & \texttt{vcovCL} & Uses a degrees of freedom-based correction

When the number of clusters is small, \texttt{HC2} and \texttt{HC3} are better \citep{cameron2008bootstrap} & \citep{mackinnon1985some} \\
\texttt{HC2} & \texttt{vcovCL} & Better with the linear model, but still applicable for \protect\hyperlink{generalized-linear-models}{Generalized Linear Models}

Needs a hat (weighted) matrix & \\
\texttt{HC3} & \texttt{vcovCL} & Better with the linear model, but still applicable for \protect\hyperlink{generalized-linear-models}{Generalized Linear Models}

Needs a hat (weighted) matrix & \\
\texttt{HC4} & \texttt{vcovHC} & & \citep{cribari2004asymptotic} \\
\texttt{HC4m} & \texttt{vcovHC} & & \citep{cribari2007inference} \\
\texttt{HC5} & \texttt{vcovHC} & & \citep{cribari2011new} \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(cars)}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(speed }\SpecialCharTok{\textasciitilde{}}\NormalTok{ dist, }\AttributeTok{data =}\NormalTok{ cars)}
\FunctionTok{summary}\NormalTok{(model)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = speed \textasciitilde{} dist, data = cars)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}     Min      1Q  Median      3Q     Max }
\CommentTok{\#\textgreater{} {-}7.5293 {-}2.1550  0.3615  2.4377  6.4179 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)  8.28391    0.87438   9.474 1.44e{-}12 ***}
\CommentTok{\#\textgreater{} dist         0.16557    0.01749   9.464 1.49e{-}12 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 3.156 on 48 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.6511, Adjusted R{-}squared:  0.6438 }
\CommentTok{\#\textgreater{} F{-}statistic: 89.57 on 1 and 48 DF,  p{-}value: 1.49e{-}12}
\NormalTok{lmtest}\SpecialCharTok{::}\FunctionTok{coeftest}\NormalTok{(model, }\AttributeTok{vcov. =}\NormalTok{ sandwich}\SpecialCharTok{::}\FunctionTok{vcovHC}\NormalTok{(model, }\AttributeTok{type =} \StringTok{"HC1"}\NormalTok{))}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} t test of coefficients:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}             Estimate Std. Error t value  Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept) 8.283906   0.891860  9.2883 2.682e{-}12 ***}
\CommentTok{\#\textgreater{} dist        0.165568   0.019402  8.5335 3.482e{-}11 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

\hypertarget{coefficient-uncertainty-and-distribution}{%
\section{Coefficient Uncertainty and Distribution}\label{coefficient-uncertainty-and-distribution}}

The \texttt{ggdist} allows us to visualize uncertainty under both frequentist and Bayesian frameworks

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggdist)}
\end{Highlighting}
\end{Shaded}

\hypertarget{descriptive-tables}{%
\section{Descriptive Tables}\label{descriptive-tables}}

Export APA theme

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(}\StringTok{"mtcars"}\NormalTok{)}

\FunctionTok{library}\NormalTok{(flextable)}
\FunctionTok{theme\_apa}\NormalTok{(}\FunctionTok{flextable}\NormalTok{(mtcars[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

Export to Latex

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(xtable}\SpecialCharTok{::}\FunctionTok{xtable}\NormalTok{(mtcars, }\AttributeTok{type =} \StringTok{"latex"}\NormalTok{),}
      \AttributeTok{file =} \FunctionTok{file.path}\NormalTok{(}\FunctionTok{getwd}\NormalTok{(), }\StringTok{"output"}\NormalTok{, }\StringTok{"mtcars\_xtable.tex"}\NormalTok{))}

\CommentTok{\# American Economic Review style}
\NormalTok{stargazer}\SpecialCharTok{::}\FunctionTok{stargazer}\NormalTok{(}
\NormalTok{    mtcars,}
    \AttributeTok{title =} \StringTok{"Testing"}\NormalTok{,}
    \AttributeTok{style =} \StringTok{"aer"}\NormalTok{,}
    \AttributeTok{out =} \FunctionTok{file.path}\NormalTok{(}\FunctionTok{getwd}\NormalTok{(), }\StringTok{"output"}\NormalTok{, }\StringTok{"mtcars\_stargazer.tex"}\NormalTok{)}
\NormalTok{)}

\CommentTok{\# other styles include}
\CommentTok{\# Administrative Science Quarterly}
\CommentTok{\# Quarterly Journal of Economics}
\end{Highlighting}
\end{Shaded}

However, the above codes do not play well with notes. Hence, I create my own custom code that follows the AMA guidelines

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ama\_tbl }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, caption, label, note, output\_path) \{}
  \FunctionTok{library}\NormalTok{(tidyverse)}
  \FunctionTok{library}\NormalTok{(xtable)}
  \CommentTok{\# Function to determine column alignment}
\NormalTok{  get\_column\_alignment }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data) \{}
    \CommentTok{\# Start with the alignment for the header row}
\NormalTok{    alignment }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"l"}\NormalTok{, }\StringTok{"l"}\NormalTok{)}
    
    \CommentTok{\# Check each column}
    \ControlFlowTok{for}\NormalTok{ (col }\ControlFlowTok{in} \FunctionTok{seq\_len}\NormalTok{(}\FunctionTok{ncol}\NormalTok{(data))[}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]) \{}
      \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{is.numeric}\NormalTok{(data[[col]])) \{}
\NormalTok{        alignment }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(alignment, }\StringTok{"r"}\NormalTok{)  }\CommentTok{\# Right alignment for numbers}
\NormalTok{      \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{        alignment }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(alignment, }\StringTok{"c"}\NormalTok{)  }\CommentTok{\# Center alignment for other data}
\NormalTok{      \}}
\NormalTok{    \}}
    
    \FunctionTok{return}\NormalTok{(alignment)}
\NormalTok{  \}}
  
\NormalTok{  data }\SpecialCharTok{\%\textgreater{}\%}
    \CommentTok{\# bold + left align first column }
    \FunctionTok{rename\_with}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\FunctionTok{paste}\NormalTok{(}\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{multicolumn\{1\}\{l\}\{}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{textbf\{"}\NormalTok{, ., }\StringTok{"\}\}"}\NormalTok{), }\DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
    \CommentTok{\# bold + center align all other columns}
    \StringTok{\textasciigrave{}}\AttributeTok{colnames\textless{}{-}}\StringTok{\textasciigrave{}}\NormalTok{(}\FunctionTok{ifelse}\NormalTok{(}\FunctionTok{colnames}\NormalTok{(.) }\SpecialCharTok{!=} \FunctionTok{colnames}\NormalTok{(.)[}\DecValTok{1}\NormalTok{],}
                        \FunctionTok{paste}\NormalTok{(}\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{multicolumn\{1\}\{c\}\{}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{textbf\{"}\NormalTok{, }\FunctionTok{colnames}\NormalTok{(.), }\StringTok{"\}\}"}\NormalTok{),}
                        \FunctionTok{colnames}\NormalTok{(.))) }\SpecialCharTok{\%\textgreater{}\%} 
    
    \FunctionTok{xtable}\NormalTok{(}\AttributeTok{caption =}\NormalTok{ caption,}
           \AttributeTok{label =}\NormalTok{ label,}
           \AttributeTok{align =} \FunctionTok{get\_column\_alignment}\NormalTok{(data),}
           \AttributeTok{auto =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{print}\NormalTok{(}
      \AttributeTok{include.rownames =} \ConstantTok{FALSE}\NormalTok{,}
      \AttributeTok{caption.placement =} \StringTok{"top"}\NormalTok{,}
      
      \AttributeTok{hline.after=}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{),}
      
       \CommentTok{\# p\{0.9\textbackslash{}linewidth\} sets the width of the column to 90\% of the line width, and the @\{\} removes any extra padding around the cell.}
      
      \AttributeTok{add.to.row =} \FunctionTok{list}\NormalTok{(}\AttributeTok{pos =} \FunctionTok{list}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(data)), }\CommentTok{\# Add at the bottom of the table}
                        \AttributeTok{command =} \FunctionTok{c}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(}\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{hline }\SpecialCharTok{\textbackslash{}n}\StringTok{ }\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{multicolumn\{"}\NormalTok{,}\FunctionTok{ncol}\NormalTok{(data), }\StringTok{"\}\{l\} \{"}\NormalTok{, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{ }\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{begin\{tabular\}\{@\{\}p\{0.9}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{linewidth\}@\{\}\} }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{,}\StringTok{"Note: "}\NormalTok{, note, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{ }\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{end\{tabular\}  \} }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{))), }\CommentTok{\# Add your note here}
      
      \CommentTok{\# make sure your heading is untouched (because you manually change it above)}
      \AttributeTok{sanitize.colnames.function =}\NormalTok{ identity,}
      
      \CommentTok{\# place a the top of the page}
      \AttributeTok{table.placement =} \StringTok{"h"}\NormalTok{,}
      
      \AttributeTok{file =}\NormalTok{ output\_path}
\NormalTok{    )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ama\_tbl}\NormalTok{(}
\NormalTok{    mtcars,}
    \AttributeTok{caption     =} \StringTok{"This is caption"}\NormalTok{,}
    \AttributeTok{label       =} \StringTok{"tab:this\_is\_label"}\NormalTok{,}
    \AttributeTok{note        =} \StringTok{"this is note"}\NormalTok{,}
    \AttributeTok{output\_path =} \FunctionTok{file.path}\NormalTok{(}\FunctionTok{getwd}\NormalTok{(), }\StringTok{"output"}\NormalTok{, }\StringTok{"mtcars\_custom\_ama.tex"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{visualizations-and-plots}{%
\section{Visualizations and Plots}\label{visualizations-and-plots}}

You can customize your plots based on your preferred journals. Here, I am creating a custom setting for the American Marketing Association.

American-Marketing-Association-ready theme for plots

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}

\CommentTok{\# check available fonts}
\CommentTok{\# windowsFonts()}

\CommentTok{\# for Times New Roman}
\CommentTok{\# names(windowsFonts()[windowsFonts()=="TT Times New Roman"])}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Making a theme}
\NormalTok{amatheme }\OtherTok{=} \FunctionTok{theme\_bw}\NormalTok{(}\AttributeTok{base\_size =} \DecValTok{14}\NormalTok{, }\AttributeTok{base\_family =} \StringTok{"serif"}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# This is Time New Roman}
    
    \FunctionTok{theme}\NormalTok{(}
        \CommentTok{\# remove major gridlines}
        \AttributeTok{panel.grid.major   =} \FunctionTok{element\_blank}\NormalTok{(),}

        \CommentTok{\# remove minor gridlines}
        \AttributeTok{panel.grid.minor   =} \FunctionTok{element\_blank}\NormalTok{(),}

        \CommentTok{\# remove panel border}
        \AttributeTok{panel.border       =} \FunctionTok{element\_blank}\NormalTok{(),}

        \AttributeTok{line               =} \FunctionTok{element\_line}\NormalTok{(),}

        \CommentTok{\# change font}
        \AttributeTok{text               =} \FunctionTok{element\_text}\NormalTok{(),}

        \CommentTok{\# if you want to remove legend title}
        \CommentTok{\# legend.title     = element\_blank(),}

        \AttributeTok{legend.title       =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \FunctionTok{rel}\NormalTok{(}\FloatTok{0.6}\NormalTok{), }\AttributeTok{face =} \StringTok{"bold"}\NormalTok{),}

        \CommentTok{\# change font size of legend}
        \AttributeTok{legend.text        =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \FunctionTok{rel}\NormalTok{(}\FloatTok{0.6}\NormalTok{)),}
        
        \AttributeTok{legend.background  =} \FunctionTok{element\_rect}\NormalTok{(}\AttributeTok{color =} \StringTok{"black"}\NormalTok{),}
        
        \CommentTok{\# legend.margin    = margin(t = 5, l = 5, r = 5, b = 5),}
        \CommentTok{\# legend.key       = element\_rect(color = NA, fill = NA),}

        \CommentTok{\# change font size of main title}
        \AttributeTok{plot.title         =} \FunctionTok{element\_text}\NormalTok{(}
            \AttributeTok{size           =} \FunctionTok{rel}\NormalTok{(}\FloatTok{1.2}\NormalTok{),}
            \AttributeTok{face           =} \StringTok{"bold"}\NormalTok{,}
            \AttributeTok{hjust          =} \FloatTok{0.5}\NormalTok{,}
            \AttributeTok{margin         =} \FunctionTok{margin}\NormalTok{(}\AttributeTok{b =} \DecValTok{15}\NormalTok{)}
\NormalTok{        ),}
        
        \AttributeTok{plot.margin        =} \FunctionTok{unit}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }\StringTok{"cm"}\NormalTok{),}

        \CommentTok{\# add black line along axes}
        \AttributeTok{axis.line          =} \FunctionTok{element\_line}\NormalTok{(}\AttributeTok{colour =} \StringTok{"black"}\NormalTok{, }\AttributeTok{linewidth =}\NormalTok{ .}\DecValTok{8}\NormalTok{),}
        
        \AttributeTok{axis.ticks         =} \FunctionTok{element\_line}\NormalTok{(),}
        

        \CommentTok{\# axis title}
        \AttributeTok{axis.title.x       =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \FunctionTok{rel}\NormalTok{(}\FloatTok{1.2}\NormalTok{), }\AttributeTok{face =} \StringTok{"bold"}\NormalTok{),}
        \AttributeTok{axis.title.y       =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \FunctionTok{rel}\NormalTok{(}\FloatTok{1.2}\NormalTok{), }\AttributeTok{face =} \StringTok{"bold"}\NormalTok{),}

        \CommentTok{\# axis text size}
        \AttributeTok{axis.text.y        =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \FunctionTok{rel}\NormalTok{(}\DecValTok{1}\NormalTok{)),}
        \AttributeTok{axis.text.x        =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \FunctionTok{rel}\NormalTok{(}\DecValTok{1}\NormalTok{))}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

Example

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(ggsci)}
\FunctionTok{data}\NormalTok{(}\StringTok{"mtcars"}\NormalTok{)}
\NormalTok{yourplot }\OtherTok{\textless{}{-}}\NormalTok{ mtcars }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{select}\NormalTok{(mpg, cyl, gear) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{ggplot}\NormalTok{(., }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ mpg, }\AttributeTok{y =}\NormalTok{ cyl, }\AttributeTok{fill =}\NormalTok{ gear)) }\SpecialCharTok{+} 
    \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{title=}\StringTok{"Some Plot"}\NormalTok{) }

\NormalTok{yourplot }\SpecialCharTok{+} 
\NormalTok{    amatheme }\SpecialCharTok{+} 
    \CommentTok{\# choose different color theme}
    \FunctionTok{scale\_color\_npg}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{36-report_files/figure-latex/unnamed-chunk-19-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{yourplot }\SpecialCharTok{+} 
\NormalTok{    amatheme }\SpecialCharTok{+} 
    \FunctionTok{scale\_color\_continuous}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{36-report_files/figure-latex/unnamed-chunk-19-2} \end{center}

Other pre-specified themes

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggthemes)}


\CommentTok{\# Stata theme}
\NormalTok{yourplot }\SpecialCharTok{+}
    \FunctionTok{theme\_stata}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{36-report_files/figure-latex/unnamed-chunk-20-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# The economist theme}
\NormalTok{yourplot }\SpecialCharTok{+} 
    \FunctionTok{theme\_economist}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{36-report_files/figure-latex/unnamed-chunk-20-2} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{yourplot }\SpecialCharTok{+} 
    \FunctionTok{theme\_economist\_white}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{36-report_files/figure-latex/unnamed-chunk-20-3} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Wall street journal theme}
\NormalTok{yourplot }\SpecialCharTok{+} 
    \FunctionTok{theme\_wsj}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{36-report_files/figure-latex/unnamed-chunk-20-4} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# APA theme}
\NormalTok{yourplot }\SpecialCharTok{+}
\NormalTok{    jtools}\SpecialCharTok{::}\FunctionTok{theme\_apa}\NormalTok{(}
        \AttributeTok{legend.font.size =} \DecValTok{24}\NormalTok{,}
        \AttributeTok{x.font.size =} \DecValTok{20}\NormalTok{,}
        \AttributeTok{y.font.size =} \DecValTok{20}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{36-report_files/figure-latex/unnamed-chunk-20-5} \end{center}

\hypertarget{exploratory-data-analysis}{%
\chapter{Exploratory Data Analysis}\label{exploratory-data-analysis}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load to get txhousing data}
\FunctionTok{library}\NormalTok{(ggplot2)}
\end{Highlighting}
\end{Shaded}

Data Report

Feature Engineering

Missing Data

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# install.packages("DataExplorer")}
\FunctionTok{library}\NormalTok{(DataExplorer)}

\CommentTok{\# creat a html file that contain all reports}
\FunctionTok{create\_report}\NormalTok{(txhousing)}

\FunctionTok{introduce}\NormalTok{() }\CommentTok{\# see basic info}


\FunctionTok{dummify}\NormalTok{() }\CommentTok{\# create binary columns from discrete variables}
\FunctionTok{split\_columns}\NormalTok{() }\CommentTok{\# split data into discrete and continuous parts}



\FunctionTok{plot\_correlation}\NormalTok{() }\CommentTok{\# heatmap for discrete var}
\FunctionTok{plot\_intro}\NormalTok{() }

\FunctionTok{plot\_missing}\NormalTok{() }\CommentTok{\# plot missing value}
\FunctionTok{profile\_missing}\NormalTok{() }\CommentTok{\# profile missing values}


\FunctionTok{plot\_prcomp}\NormalTok{() }\CommentTok{\# plot PCA}
\end{Highlighting}
\end{Shaded}

Error Identification

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# install.packages("dataReporter")}
\FunctionTok{library}\NormalTok{(dataReporter)}
\FunctionTok{makeDataReport}\NormalTok{() }\CommentTok{\# detailed report like DataExplorer}
\end{Highlighting}
\end{Shaded}

Summary statistics

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(skimr)}
\FunctionTok{skim}\NormalTok{() }\CommentTok{\# give only few quick summary stat, not as detailed as the other two packages}
\end{Highlighting}
\end{Shaded}

Not so code-y process

Quick and dirty way to look at your data

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# install.packages("rpivotTable")}
\FunctionTok{library}\NormalTok{(rpivotTable)}
\CommentTok{\# give set up just like Excel table }
\NormalTok{data }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{    rpivotTable}\SpecialCharTok{::}\FunctionTok{rpivotTable}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Code generation and wrangling

Shiny-app based Tableu style

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# install.packages("esquisse")}
\FunctionTok{library}\NormalTok{(esquisse)}
\NormalTok{esquisse}\SpecialCharTok{::}\FunctionTok{esquisser}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Customized your daily/automatic report

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# install.packages("chronicle")}
\FunctionTok{library}\NormalTok{(chronicle)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# install.packages("dlookr")}
\CommentTok{\# install.packages("descriptr")}
\end{Highlighting}
\end{Shaded}

\hypertarget{sensitivity-analysis-robustness-check}{%
\chapter{Sensitivity Analysis/ Robustness Check}\label{sensitivity-analysis-robustness-check}}

\hypertarget{specification-curve}{%
\section{Specification curve}\label{specification-curve}}

\begin{itemize}
\tightlist
\item
  also known as Specification robustness graph or coefficient stability plot
\end{itemize}

Resources

\begin{itemize}
\item
  \href{https://github.com/hhsievertsen/speccurve}{In Stata} or \href{https://github.com/martin-andresen/speccurve}{speccurve}
\item
  \citep{simonsohn2020specification}
\end{itemize}

\hypertarget{starbility}{%
\subsection{starbility}\label{starbility}}

\begin{itemize}
\tightlist
\item
  Recommend
\end{itemize}

Installation

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{devtools}\SpecialCharTok{::}\FunctionTok{install\_github}\NormalTok{(}\StringTok{\textquotesingle{}https://github.com/AakaashRao/starbility\textquotesingle{}}\NormalTok{)}
\FunctionTok{library}\NormalTok{(starbility)}
\end{Highlighting}
\end{Shaded}

Example by the \href{https://htmlpreview.github.io/?https://github.com/AakaashRao/starbility/blob/master/doc/starbility.html}{package's author}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(starbility)}
\FunctionTok{library}\NormalTok{(lfe)}
\FunctionTok{data}\NormalTok{(}\StringTok{"diamonds"}\NormalTok{)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{43}\NormalTok{)}
\NormalTok{indices }\OtherTok{=} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(diamonds),}
                 \AttributeTok{replace =}\NormalTok{ F,}
                 \AttributeTok{size =} \FunctionTok{round}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(diamonds) }\SpecialCharTok{/} \DecValTok{20}\NormalTok{))}
\NormalTok{diamonds }\OtherTok{=}\NormalTok{ diamonds[indices, ]}
\end{Highlighting}
\end{Shaded}

Plot different combinations of controls

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# If you want to make the diamond dimensions as base control}
\NormalTok{base\_controls }\OtherTok{=} \FunctionTok{c}\NormalTok{(}
  \StringTok{\textquotesingle{}Diamond dimensions\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}x + y + z\textquotesingle{}} \CommentTok{\# include all variables under 1 dimension}
\NormalTok{)}


\NormalTok{perm\_controls }\OtherTok{=} \FunctionTok{c}\NormalTok{(}
  \StringTok{\textquotesingle{}Depth\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}depth\textquotesingle{}}\NormalTok{,}
  \StringTok{\textquotesingle{}Table width\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}table\textquotesingle{}}
\NormalTok{)}

\NormalTok{nonperm\_fe\_controls }\OtherTok{=} \FunctionTok{c}\NormalTok{(}
  \StringTok{\textquotesingle{}Clarity FE (granular)\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}clarity\textquotesingle{}}\NormalTok{,}
  \StringTok{\textquotesingle{}Clarity FE (binary)\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}high\_clarity\textquotesingle{}}
\NormalTok{)}

\CommentTok{\# Adding fixed effects}
\NormalTok{nonperm\_fe\_controls }\OtherTok{=} \FunctionTok{c}\NormalTok{(}
  \StringTok{\textquotesingle{}Clarity FE (granular)\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}clarity\textquotesingle{}}\NormalTok{,}
  \StringTok{\textquotesingle{}Clarity FE (binary)\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}high\_clarity\textquotesingle{}}
\NormalTok{)}

\CommentTok{\# Adding instrumental variables }
\NormalTok{instruments }\OtherTok{=} \StringTok{\textquotesingle{}x+y+z\textquotesingle{}}

\CommentTok{\# clustering and weights }
\NormalTok{diamonds}\SpecialCharTok{$}\NormalTok{sample\_weights }\OtherTok{=} \FunctionTok{runif}\NormalTok{(}\AttributeTok{n =} \FunctionTok{nrow}\NormalTok{(diamonds))}


\CommentTok{\# robust standard errors }
\NormalTok{starb\_felm\_custom }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(spec, data, rhs, ...) \{}
\NormalTok{  spec }\OtherTok{=} \FunctionTok{as.formula}\NormalTok{(spec)}
\NormalTok{  model }\OtherTok{=}\NormalTok{ lfe}\SpecialCharTok{::}\FunctionTok{felm}\NormalTok{(spec, }\AttributeTok{data=}\NormalTok{data) }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ broom}\SpecialCharTok{::}\FunctionTok{tidy}\NormalTok{()}

\NormalTok{  row }\OtherTok{=} \FunctionTok{which}\NormalTok{(model}\SpecialCharTok{$}\NormalTok{term}\SpecialCharTok{==}\NormalTok{rhs)}
\NormalTok{  coef }\OtherTok{=}\NormalTok{ model[row, }\StringTok{\textquotesingle{}estimate\textquotesingle{}}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as.numeric}\NormalTok{()}
\NormalTok{  se   }\OtherTok{=}\NormalTok{ model[row, }\StringTok{\textquotesingle{}std.error\textquotesingle{}}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as.numeric}\NormalTok{()}
\NormalTok{  p    }\OtherTok{=}\NormalTok{ model[row, }\StringTok{\textquotesingle{}p.value\textquotesingle{}}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as.numeric}\NormalTok{()}
  
  \CommentTok{\# 99\% confidence interval}
\NormalTok{  z }\OtherTok{=} \FunctionTok{qnorm}\NormalTok{(}\FloatTok{0.995}\NormalTok{) }
  \CommentTok{\# one{-}tailed test}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{c}\NormalTok{(coef, p}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, coef}\SpecialCharTok{+}\NormalTok{z}\SpecialCharTok{*}\NormalTok{se, coef}\SpecialCharTok{{-}}\NormalTok{z}\SpecialCharTok{*}\NormalTok{se))}
\NormalTok{\}}

\NormalTok{plots }\OtherTok{=} \FunctionTok{stability\_plot}\NormalTok{(}
    \AttributeTok{data =}\NormalTok{ diamonds,}
    \AttributeTok{lhs =} \StringTok{\textquotesingle{}price\textquotesingle{}}\NormalTok{,}
    \AttributeTok{rhs =} \StringTok{\textquotesingle{}carat\textquotesingle{}}\NormalTok{,}
    \AttributeTok{error\_geom =} \StringTok{\textquotesingle{}ribbon\textquotesingle{}}\NormalTok{, }\CommentTok{\# make the plot more aesthetics}
    \CommentTok{\# error\_geom = \textquotesingle{}none\textquotesingle{}, \# if you don\textquotesingle{}t want ribbon (i.e., error bar)}
    \AttributeTok{model =}\NormalTok{ starb\_felm\_custom,}
    \AttributeTok{cluster =} \StringTok{\textquotesingle{}cut\textquotesingle{}}\NormalTok{,}
    \AttributeTok{weights =} \StringTok{\textquotesingle{}sample\_weights\textquotesingle{}}\NormalTok{,}
    \CommentTok{\# iv = instruments,}
    \AttributeTok{perm =}\NormalTok{ perm\_controls,}
    \AttributeTok{base =}\NormalTok{ base\_controls,}
    \CommentTok{\# perm\_fe = perm\_fe\_controls,}
    
    \CommentTok{\# if you want to include fixed effects sequentially (not all combinations) }
    \CommentTok{\# (e.g., you want to test country or state fixed effect, not both )}
    \CommentTok{\# nonperm\_fe = nonperm\_fe\_controls, }
    \CommentTok{\# fe\_always = F,  \# if you want to have a model without any Fixed Effects}
    
    \CommentTok{\# sort "asc", "desc", or by fixed effects: "asc{-}by{-}fe" or "desc{-}by{-}fe"}
    \AttributeTok{sort =} \StringTok{"asc{-}by{-}fe"}\NormalTok{, }
    
    \CommentTok{\# if you have less variables and want more aesthetics }
    \CommentTok{\# control\_geom = \textquotesingle{}circle\textquotesingle{},}
    \CommentTok{\# point\_size = 2,}
    \CommentTok{\# control\_spacing = 0.3,}
    
    
    \CommentTok{\# error\_alpha = 0.2, \# change alpha of the error geom}
    \CommentTok{\# point\_size = 1.5, \# change the size of the coefficient points}
    \CommentTok{\# control\_text\_size = 10, \# change the size of the control labels}
    \CommentTok{\# coef\_ylim = c({-}5000, 35000), \# change the endpoints of the y{-}axis}
    \CommentTok{\# trip\_top = 3, \# change the spacing between the two panels}
    
    \AttributeTok{rel\_height =} \FloatTok{0.6}
\NormalTok{)}
\NormalTok{plots}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{38-sensitivity-robustness_files/figure-latex/unnamed-chunk-3-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# add comments}
\CommentTok{\# replacement\_coef\_panel = plots[[1]] +}
\CommentTok{\#   scale\_y\_reverse() +}
\CommentTok{\#   theme(panel.grid.minor = element\_blank()) +}
\CommentTok{\#   geom\_vline(xintercept = 41,}
\CommentTok{\#              linetype = \textquotesingle{}dashed\textquotesingle{},}
\CommentTok{\#              alpha = 0.4) +}
\CommentTok{\#   annotate(}
\CommentTok{\#     geom = \textquotesingle{}label\textquotesingle{},}
\CommentTok{\#     x = 52,}
\CommentTok{\#     y = 30000,}
\CommentTok{\#     label = \textquotesingle{}What a great\textbackslash{}nspecification!\textquotesingle{},}
\CommentTok{\#     alpha = 0.75}
\CommentTok{\#   )}
\CommentTok{\# }
\CommentTok{\# combine\_plots(replacement\_coef\_panel,}
\CommentTok{\#               plots[[2]],}
\CommentTok{\#               rel\_height = 0.6)}
\end{Highlighting}
\end{Shaded}

Note:

\begin{itemize}
\tightlist
\item
  \(p < 0.01\): red
\item
  \(p < 0.05\): green
\item
  \(p < 0.1\): blue
\item
  \(p > 0.1\): black
\end{itemize}

\href{https://htmlpreview.github.io/?https://github.com/AakaashRao/starbility/blob/master/doc/starbility-advanced.html}{More Advanced Stuff}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Step 1: Control Grid}

\NormalTok{diamonds}\SpecialCharTok{$}\NormalTok{high\_clarity }\OtherTok{=}\NormalTok{ diamonds}\SpecialCharTok{$}\NormalTok{clarity }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}VS1\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}VVS2\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}VVS1\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}IF\textquotesingle{}}\NormalTok{)}

\NormalTok{base\_controls }\OtherTok{=} \FunctionTok{c}\NormalTok{(}
  \StringTok{\textquotesingle{}Diamond dimensions\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}x + y + z\textquotesingle{}}
\NormalTok{)}

\NormalTok{perm\_controls }\OtherTok{=} \FunctionTok{c}\NormalTok{(}
  \StringTok{\textquotesingle{}Depth\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}depth\textquotesingle{}}\NormalTok{,}
  \StringTok{\textquotesingle{}Table width\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}table\textquotesingle{}}
\NormalTok{)}

\NormalTok{perm\_fe\_controls }\OtherTok{=} \FunctionTok{c}\NormalTok{(}
  \StringTok{\textquotesingle{}Cut FE\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}cut\textquotesingle{}}\NormalTok{,}
  \StringTok{\textquotesingle{}Color FE\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}color\textquotesingle{}}
\NormalTok{)}
\NormalTok{nonperm\_fe\_controls }\OtherTok{=} \FunctionTok{c}\NormalTok{(}
  \StringTok{\textquotesingle{}Clarity FE (granular)\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}clarity\textquotesingle{}}\NormalTok{,}
  \StringTok{\textquotesingle{}Clarity FE (binary)\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}high\_clarity\textquotesingle{}}
\NormalTok{)}

\NormalTok{grid1 }\OtherTok{=} \FunctionTok{stability\_plot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ diamonds, }
                      \AttributeTok{lhs =} \StringTok{\textquotesingle{}price\textquotesingle{}}\NormalTok{, }
                      \AttributeTok{rhs =} \StringTok{\textquotesingle{}carat\textquotesingle{}}\NormalTok{, }
                      \AttributeTok{perm =}\NormalTok{ perm\_controls,}
                      \AttributeTok{base =}\NormalTok{ base\_controls, }
                      \AttributeTok{perm\_fe =}\NormalTok{ perm\_fe\_controls, }
                      \AttributeTok{nonperm\_fe =}\NormalTok{ nonperm\_fe\_controls, }
                      \AttributeTok{run\_to=}\DecValTok{2}\NormalTok{)}

\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(grid1 }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{head}\NormalTok{(}\DecValTok{10}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|r|r|r|r|l}
\hline
Diamond dimensions & Depth & Table width & Cut FE & Color FE & np\_fe\\
\hline
1 & 0 & 0 & 0 & 0 & \\
\hline
1 & 1 & 0 & 0 & 0 & \\
\hline
1 & 0 & 1 & 0 & 0 & \\
\hline
1 & 1 & 1 & 0 & 0 & \\
\hline
1 & 0 & 0 & 1 & 0 & \\
\hline
1 & 1 & 0 & 1 & 0 & \\
\hline
1 & 0 & 1 & 1 & 0 & \\
\hline
1 & 1 & 1 & 1 & 0 & \\
\hline
1 & 0 & 0 & 0 & 1 & \\
\hline
1 & 1 & 0 & 0 & 1 & \\
\hline
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Step 2: Get model expression}

\NormalTok{grid2 }\OtherTok{=} \FunctionTok{stability\_plot}\NormalTok{(}\AttributeTok{grid =}\NormalTok{ grid1,}
                      \AttributeTok{data=}\NormalTok{diamonds, }
                      \AttributeTok{lhs=}\StringTok{\textquotesingle{}price\textquotesingle{}}\NormalTok{, }
                      \AttributeTok{rhs=}\StringTok{\textquotesingle{}carat\textquotesingle{}}\NormalTok{, }
                      \AttributeTok{perm=}\NormalTok{perm\_controls, }
                      \AttributeTok{base=}\NormalTok{base\_controls,}
                      \AttributeTok{run\_from=}\DecValTok{2}\NormalTok{,}
                      \AttributeTok{run\_to=}\DecValTok{3}\NormalTok{)}


\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(grid2 }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{head}\NormalTok{(}\DecValTok{10}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|r|r|l|l}
\hline
Diamond dimensions & Depth & Table width & np\_fe & expr\\
\hline
1 & 0 & 0 & 0 & price\textasciitilde{}carat+x+y+z|0|0|0\\
\hline
1 & 1 & 0 & 0 & price\textasciitilde{}carat+x+y+z+depth|0|0|0\\
\hline
1 & 0 & 1 & 0 & price\textasciitilde{}carat+x+y+z+table|0|0|0\\
\hline
1 & 1 & 1 & 0 & price\textasciitilde{}carat+x+y+z+depth+table|0|0|0\\
\hline
1 & 0 & 0 & 0 & price\textasciitilde{}carat+x+y+z|0|0|0\\
\hline
1 & 1 & 0 & 0 & price\textasciitilde{}carat+x+y+z+depth|0|0|0\\
\hline
1 & 0 & 1 & 0 & price\textasciitilde{}carat+x+y+z+table|0|0|0\\
\hline
1 & 1 & 1 & 0 & price\textasciitilde{}carat+x+y+z+depth+table|0|0|0\\
\hline
1 & 0 & 0 & 0 & price\textasciitilde{}carat+x+y+z|0|0|0\\
\hline
1 & 1 & 0 & 0 & price\textasciitilde{}carat+x+y+z+depth|0|0|0\\
\hline
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Step 3: Estimate models}
\NormalTok{grid3 }\OtherTok{=} \FunctionTok{stability\_plot}\NormalTok{(}\AttributeTok{grid =}\NormalTok{ grid2,}
                      \AttributeTok{data=}\NormalTok{diamonds, }
                      \AttributeTok{lhs=}\StringTok{\textquotesingle{}price\textquotesingle{}}\NormalTok{, }
                      \AttributeTok{rhs=}\StringTok{\textquotesingle{}carat\textquotesingle{}}\NormalTok{, }
                      \AttributeTok{perm=}\NormalTok{perm\_controls, }
                      \AttributeTok{base=}\NormalTok{base\_controls,}
                      \AttributeTok{run\_from=}\DecValTok{3}\NormalTok{,}
                      \AttributeTok{run\_to=}\DecValTok{4}\NormalTok{)}

\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(grid3 }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{head}\NormalTok{(}\DecValTok{10}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|r|r|l|l|r|l|r|r}
\hline
Diamond dimensions & Depth & Table width & np\_fe & expr & coef & p & error\_high & error\_low\\
\hline
1 & 0 & 0 & 0 & price\textasciitilde{}carat+x+y+z|0|0|0 & 10461.86 & p<0.01 & 11031.84 & 9891.876\\
\hline
1 & 1 & 0 & 0 & price\textasciitilde{}carat+x+y+z+depth|0|0|0 & 10808.25 & p<0.01 & 11388.81 & 10227.683\\
\hline
1 & 0 & 1 & 0 & price\textasciitilde{}carat+x+y+z+table|0|0|0 & 10423.42 & p<0.01 & 10992.00 & 9854.849\\
\hline
1 & 1 & 1 & 0 & price\textasciitilde{}carat+x+y+z+depth+table|0|0|0 & 10851.31 & p<0.01 & 11428.58 & 10274.037\\
\hline
1 & 0 & 0 & 0 & price\textasciitilde{}carat+x+y+z|0|0|0 & 10461.86 & p<0.01 & 11031.84 & 9891.876\\
\hline
1 & 1 & 0 & 0 & price\textasciitilde{}carat+x+y+z+depth|0|0|0 & 10808.25 & p<0.01 & 11388.81 & 10227.683\\
\hline
1 & 0 & 1 & 0 & price\textasciitilde{}carat+x+y+z+table|0|0|0 & 10423.42 & p<0.01 & 10992.00 & 9854.849\\
\hline
1 & 1 & 1 & 0 & price\textasciitilde{}carat+x+y+z+depth+table|0|0|0 & 10851.31 & p<0.01 & 11428.58 & 10274.037\\
\hline
1 & 0 & 0 & 0 & price\textasciitilde{}carat+x+y+z|0|0|0 & 10461.86 & p<0.01 & 11031.84 & 9891.876\\
\hline
1 & 1 & 0 & 0 & price\textasciitilde{}carat+x+y+z+depth|0|0|0 & 10808.25 & p<0.01 & 11388.81 & 10227.683\\
\hline
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Step 4: Get dataframe to draw}
\NormalTok{dfs }\OtherTok{=} \FunctionTok{stability\_plot}\NormalTok{(}\AttributeTok{grid =}\NormalTok{ grid3,}
                      \AttributeTok{data=}\NormalTok{diamonds, }
                      \AttributeTok{lhs=}\StringTok{\textquotesingle{}price\textquotesingle{}}\NormalTok{, }
                      \AttributeTok{rhs=}\StringTok{\textquotesingle{}carat\textquotesingle{}}\NormalTok{, }
                      \AttributeTok{perm=}\NormalTok{perm\_controls, }
                      \AttributeTok{base=}\NormalTok{base\_controls,}
                      \AttributeTok{run\_from=}\DecValTok{4}\NormalTok{,}
                      \AttributeTok{run\_to=}\DecValTok{5}\NormalTok{)}

\NormalTok{coef\_grid }\OtherTok{=}\NormalTok{ dfs[[}\DecValTok{1}\NormalTok{]]}
\NormalTok{control\_grid }\OtherTok{=}\NormalTok{ dfs[[}\DecValTok{2}\NormalTok{]]}

\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(coef\_grid }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{head}\NormalTok{(}\DecValTok{10}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|r|r|l|l|r|l|r|r|r}
\hline
Diamond dimensions & Depth & Table width & np\_fe & expr & coef & p & error\_high & error\_low & model\\
\hline
1 & 0 & 0 & 0 & price\textasciitilde{}carat+x+y+z|0|0|0 & 10461.86 & p<0.01 & 11031.84 & 9891.876 & 1\\
\hline
1 & 1 & 0 & 0 & price\textasciitilde{}carat+x+y+z+depth|0|0|0 & 10808.25 & p<0.01 & 11388.81 & 10227.683 & 2\\
\hline
1 & 0 & 1 & 0 & price\textasciitilde{}carat+x+y+z+table|0|0|0 & 10423.42 & p<0.01 & 10992.00 & 9854.849 & 3\\
\hline
1 & 1 & 1 & 0 & price\textasciitilde{}carat+x+y+z+depth+table|0|0|0 & 10851.31 & p<0.01 & 11428.58 & 10274.037 & 4\\
\hline
1 & 0 & 0 & 0 & price\textasciitilde{}carat+x+y+z|0|0|0 & 10461.86 & p<0.01 & 11031.84 & 9891.876 & 5\\
\hline
1 & 1 & 0 & 0 & price\textasciitilde{}carat+x+y+z+depth|0|0|0 & 10808.25 & p<0.01 & 11388.81 & 10227.683 & 6\\
\hline
1 & 0 & 1 & 0 & price\textasciitilde{}carat+x+y+z+table|0|0|0 & 10423.42 & p<0.01 & 10992.00 & 9854.849 & 7\\
\hline
1 & 1 & 1 & 0 & price\textasciitilde{}carat+x+y+z+depth+table|0|0|0 & 10851.31 & p<0.01 & 11428.58 & 10274.037 & 8\\
\hline
1 & 0 & 0 & 0 & price\textasciitilde{}carat+x+y+z|0|0|0 & 10461.86 & p<0.01 & 11031.84 & 9891.876 & 9\\
\hline
1 & 1 & 0 & 0 & price\textasciitilde{}carat+x+y+z+depth|0|0|0 & 10808.25 & p<0.01 & 11388.81 & 10227.683 & 10\\
\hline
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Step 5: plot the sensitivity graph }
\NormalTok{panels }\OtherTok{=} \FunctionTok{stability\_plot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ diamonds, }
                      \AttributeTok{lhs=}\StringTok{\textquotesingle{}price\textquotesingle{}}\NormalTok{, }
                      \AttributeTok{rhs=}\StringTok{\textquotesingle{}carat\textquotesingle{}}\NormalTok{, }
                      \AttributeTok{coef\_grid =}\NormalTok{ coef\_grid,}
                      \AttributeTok{control\_grid =}\NormalTok{ control\_grid,}
                      \AttributeTok{run\_from=}\DecValTok{5}\NormalTok{,}
                      \AttributeTok{run\_to=}\DecValTok{6}\NormalTok{)}

\FunctionTok{stability\_plot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ diamonds,}
               \AttributeTok{lhs=}\StringTok{\textquotesingle{}price\textquotesingle{}}\NormalTok{, }
               \AttributeTok{rhs=}\StringTok{\textquotesingle{}carat\textquotesingle{}}\NormalTok{, }
               \AttributeTok{coef\_panel =}\NormalTok{ panels[[}\DecValTok{1}\NormalTok{]],}
               \AttributeTok{control\_panel =}\NormalTok{ panels[[}\DecValTok{2}\NormalTok{]],}
               \AttributeTok{run\_from =} \DecValTok{6}\NormalTok{,}
               \AttributeTok{run\_to =} \DecValTok{7}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{38-sensitivity-robustness_files/figure-latex/unnamed-chunk-4-1} \end{center}

In step 2, we can modify to use other function (e.g., \texttt{glm})

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diamonds}\SpecialCharTok{$}\NormalTok{above\_med\_price }\OtherTok{=} \FunctionTok{as.numeric}\NormalTok{(diamonds}\SpecialCharTok{$}\NormalTok{price }\SpecialCharTok{\textgreater{}} \FunctionTok{median}\NormalTok{(diamonds}\SpecialCharTok{$}\NormalTok{price))}

\NormalTok{base\_controls }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Diamond dimensions\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}x + y + z\textquotesingle{}}\NormalTok{)}

\NormalTok{perm\_controls }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Depth\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}depth\textquotesingle{}}\NormalTok{,}
                  \StringTok{\textquotesingle{}Table width\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}table\textquotesingle{}}\NormalTok{,}
                  \StringTok{\textquotesingle{}Clarity\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}clarity\textquotesingle{}}\NormalTok{)}
\NormalTok{lhs\_var }\OtherTok{=} \StringTok{\textquotesingle{}above\_med\_price\textquotesingle{}}
\NormalTok{rhs\_var }\OtherTok{=} \StringTok{\textquotesingle{}carat\textquotesingle{}}

\NormalTok{grid1 }\OtherTok{=} \FunctionTok{stability\_plot}\NormalTok{(}
    \AttributeTok{data =}\NormalTok{ diamonds,}
    \AttributeTok{lhs =}\NormalTok{ lhs\_var,}
    \AttributeTok{rhs =}\NormalTok{ rhs\_var,}
    \AttributeTok{perm =}\NormalTok{ perm\_controls,}
    \AttributeTok{base =}\NormalTok{ base\_controls,}
    \AttributeTok{fe\_always =}\NormalTok{ F,}
    \AttributeTok{run\_to =} \DecValTok{2}
\NormalTok{)}

\CommentTok{\# Create control part of formula}
\NormalTok{base\_perm }\OtherTok{=} \FunctionTok{c}\NormalTok{(base\_controls, perm\_controls)}
\NormalTok{grid1}\SpecialCharTok{$}\NormalTok{expr }\OtherTok{=} \FunctionTok{apply}\NormalTok{(grid1[, }\DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(base\_perm)], }\DecValTok{1}\NormalTok{,}
                   \ControlFlowTok{function}\NormalTok{(x)}
                     \FunctionTok{paste}\NormalTok{(base\_perm[}\FunctionTok{names}\NormalTok{(base\_perm)[}\FunctionTok{which}\NormalTok{(x }\SpecialCharTok{==} \DecValTok{1}\NormalTok{)]], }
                           \AttributeTok{collapse =} \StringTok{\textquotesingle{}+\textquotesingle{}}\NormalTok{))}

\CommentTok{\# Complete formula with LHS and RHS variables}
\NormalTok{grid1}\SpecialCharTok{$}\NormalTok{expr }\OtherTok{=} \FunctionTok{paste}\NormalTok{(lhs\_var, }\StringTok{\textquotesingle{}\textasciitilde{}\textquotesingle{}}\NormalTok{, rhs\_var, }\StringTok{\textquotesingle{}+\textquotesingle{}}\NormalTok{, grid1}\SpecialCharTok{$}\NormalTok{expr, }\AttributeTok{sep =} \StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{)}

\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(grid1 }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{head}\NormalTok{(}\DecValTok{10}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|r|r|r|l|l}
\hline
Diamond dimensions & Depth & Table width & Clarity & np\_fe & expr\\
\hline
1 & 0 & 0 & 0 &  & above\_med\_price\textasciitilde{}carat+x + y + z\\
\hline
1 & 1 & 0 & 0 &  & above\_med\_price\textasciitilde{}carat+x + y + z+depth\\
\hline
1 & 0 & 1 & 0 &  & above\_med\_price\textasciitilde{}carat+x + y + z+table\\
\hline
1 & 1 & 1 & 0 &  & above\_med\_price\textasciitilde{}carat+x + y + z+depth+table\\
\hline
1 & 0 & 0 & 1 &  & above\_med\_price\textasciitilde{}carat+x + y + z+clarity\\
\hline
1 & 1 & 0 & 1 &  & above\_med\_price\textasciitilde{}carat+x + y + z+depth+clarity\\
\hline
1 & 0 & 1 & 1 &  & above\_med\_price\textasciitilde{}carat+x + y + z+table+clarity\\
\hline
1 & 1 & 1 & 1 &  & above\_med\_price\textasciitilde{}carat+x + y + z+depth+table+clarity\\
\hline
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# customer function for the logit model}
\NormalTok{starb\_logit }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(spec, data, rhs, ...) \{}
\NormalTok{  spec }\OtherTok{=} \FunctionTok{as.formula}\NormalTok{(spec)}
\NormalTok{  model }\OtherTok{=} \FunctionTok{glm}\NormalTok{(spec, }\AttributeTok{data=}\NormalTok{data, }\AttributeTok{family=}\StringTok{\textquotesingle{}binomial\textquotesingle{}}\NormalTok{, }\AttributeTok{weights=}\NormalTok{data}\SpecialCharTok{$}\NormalTok{weight) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    broom}\SpecialCharTok{::}\FunctionTok{tidy}\NormalTok{()}
\NormalTok{  row }\OtherTok{=} \FunctionTok{which}\NormalTok{(model}\SpecialCharTok{$}\NormalTok{term}\SpecialCharTok{==}\NormalTok{rhs)}
\NormalTok{  coef }\OtherTok{=}\NormalTok{ model[row, }\StringTok{\textquotesingle{}estimate\textquotesingle{}}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as.numeric}\NormalTok{()}
\NormalTok{  se   }\OtherTok{=}\NormalTok{ model[row, }\StringTok{\textquotesingle{}std.error\textquotesingle{}}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as.numeric}\NormalTok{()}
\NormalTok{  p    }\OtherTok{=}\NormalTok{ model[row, }\StringTok{\textquotesingle{}p.value\textquotesingle{}}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as.numeric}\NormalTok{()}

  \FunctionTok{return}\NormalTok{(}\FunctionTok{c}\NormalTok{(coef, p, coef}\FloatTok{+1.96}\SpecialCharTok{*}\NormalTok{se, coef}\FloatTok{{-}1.96}\SpecialCharTok{*}\NormalTok{se))}
\NormalTok{\}}

\FunctionTok{stability\_plot}\NormalTok{(}\AttributeTok{grid =}\NormalTok{ grid1,}
               \AttributeTok{data =}\NormalTok{ diamonds, }
               \AttributeTok{lhs =}\NormalTok{ lhs\_var, }
               \AttributeTok{rhs =}\NormalTok{ rhs\_var,}
               \AttributeTok{model =}\NormalTok{ starb\_logit,}
               \AttributeTok{perm =}\NormalTok{ perm\_controls,}
               \AttributeTok{base =}\NormalTok{ base\_controls,}
               \AttributeTok{fe\_always =}\NormalTok{ F,}
               \AttributeTok{run\_from=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{38-sensitivity-robustness_files/figure-latex/unnamed-chunk-5-1} \end{center}

For getting other specification (e.g., different CI)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(margins)}
\NormalTok{starb\_logit\_enhanced }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(spec, data, rhs, ...) \{}
  \CommentTok{\# Unpack ...}
\NormalTok{  l }\OtherTok{=} \FunctionTok{list}\NormalTok{(...)}
\NormalTok{  get\_mfx }\OtherTok{=} \FunctionTok{ifelse}\NormalTok{(}\FunctionTok{is.null}\NormalTok{(l}\SpecialCharTok{$}\NormalTok{get\_mfx), F, T) }\CommentTok{\# Set a default to F}
  
\NormalTok{  spec }\OtherTok{=} \FunctionTok{as.formula}\NormalTok{(spec)}
  \ControlFlowTok{if}\NormalTok{ (get\_mfx) \{}
\NormalTok{    model }\OtherTok{=} \FunctionTok{glm}\NormalTok{(spec, }\AttributeTok{data=}\NormalTok{data, }\AttributeTok{family=}\StringTok{\textquotesingle{}binomial\textquotesingle{}}\NormalTok{, }\AttributeTok{weights=}\NormalTok{data}\SpecialCharTok{$}\NormalTok{weight) }\SpecialCharTok{\%\textgreater{}\%}
      \FunctionTok{margins}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{      summary}
\NormalTok{    row }\OtherTok{=} \FunctionTok{which}\NormalTok{(model}\SpecialCharTok{$}\NormalTok{factor}\SpecialCharTok{==}\NormalTok{rhs)}
\NormalTok{    coef }\OtherTok{=}\NormalTok{ model[row, }\StringTok{\textquotesingle{}AME\textquotesingle{}}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as.numeric}\NormalTok{()}
\NormalTok{    se   }\OtherTok{=}\NormalTok{ model[row, }\StringTok{\textquotesingle{}SE\textquotesingle{}}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as.numeric}\NormalTok{()}
\NormalTok{    p    }\OtherTok{=}\NormalTok{ model[row, }\StringTok{\textquotesingle{}p\textquotesingle{}}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as.numeric}\NormalTok{()}
\NormalTok{  \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{    model }\OtherTok{=} \FunctionTok{glm}\NormalTok{(spec, }\AttributeTok{data=}\NormalTok{data, }\AttributeTok{family=}\StringTok{\textquotesingle{}binomial\textquotesingle{}}\NormalTok{, }\AttributeTok{weights=}\NormalTok{data}\SpecialCharTok{$}\NormalTok{weight) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{      broom}\SpecialCharTok{::}\FunctionTok{tidy}\NormalTok{()}
\NormalTok{    row }\OtherTok{=} \FunctionTok{which}\NormalTok{(model}\SpecialCharTok{$}\NormalTok{term}\SpecialCharTok{==}\NormalTok{rhs)}
\NormalTok{    coef }\OtherTok{=}\NormalTok{ model[row, }\StringTok{\textquotesingle{}estimate\textquotesingle{}}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as.numeric}\NormalTok{()}
\NormalTok{    se   }\OtherTok{=}\NormalTok{ model[row, }\StringTok{\textquotesingle{}std.error\textquotesingle{}}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as.numeric}\NormalTok{()}
\NormalTok{    p    }\OtherTok{=}\NormalTok{ model[row, }\StringTok{\textquotesingle{}p.value\textquotesingle{}}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as.numeric}\NormalTok{()}
\NormalTok{  \}}

\NormalTok{  z }\OtherTok{=} \FunctionTok{qnorm}\NormalTok{(}\FloatTok{0.995}\NormalTok{)}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{c}\NormalTok{(coef, p, coef}\SpecialCharTok{+}\NormalTok{z}\SpecialCharTok{*}\NormalTok{se, coef}\SpecialCharTok{{-}}\NormalTok{z}\SpecialCharTok{*}\NormalTok{se))}
\NormalTok{\}}

\FunctionTok{stability\_plot}\NormalTok{(}\AttributeTok{grid =}\NormalTok{ grid1,}
               \AttributeTok{data =}\NormalTok{ diamonds, }
               \AttributeTok{lhs =}\NormalTok{ lhs\_var, }
               \AttributeTok{rhs =}\NormalTok{ rhs\_var,}
               \AttributeTok{model =}\NormalTok{ starb\_logit\_enhanced,}
               \AttributeTok{get\_mfx =}\NormalTok{ T,}
               \AttributeTok{perm =}\NormalTok{ perm\_controls,}
               \AttributeTok{base =}\NormalTok{ base\_controls,}
               \AttributeTok{fe\_always =}\NormalTok{ F,}
               \AttributeTok{run\_from =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{38-sensitivity-robustness_files/figure-latex/unnamed-chunk-6-1} \end{center}

To get your customized plot

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dfs }\OtherTok{=} \FunctionTok{stability\_plot}\NormalTok{(}\AttributeTok{grid =}\NormalTok{ grid1,}
               \AttributeTok{data =}\NormalTok{ diamonds, }
               \AttributeTok{lhs =}\NormalTok{ lhs\_var, }
               \AttributeTok{rhs =}\NormalTok{ rhs\_var,}
               \AttributeTok{model =}\NormalTok{ starb\_logit\_enhanced,}
               \AttributeTok{get\_mfx =}\NormalTok{ T,}
               \AttributeTok{perm =}\NormalTok{ perm\_controls,}
               \AttributeTok{base =}\NormalTok{ base\_controls,}
               \AttributeTok{fe\_always =}\NormalTok{ F,}
               \AttributeTok{run\_from =} \DecValTok{3}\NormalTok{,}
               \AttributeTok{run\_to =} \DecValTok{5}\NormalTok{)}

\NormalTok{coef\_grid\_logit }\OtherTok{=}\NormalTok{ dfs[[}\DecValTok{1}\NormalTok{]]}
\NormalTok{control\_grid\_logit }\OtherTok{=}\NormalTok{ dfs[[}\DecValTok{2}\NormalTok{]]}

\NormalTok{min\_space }\OtherTok{=} \FloatTok{0.5}

\NormalTok{coef\_plot }\OtherTok{=}\NormalTok{ ggplot2}\SpecialCharTok{::}\FunctionTok{ggplot}\NormalTok{(coef\_grid\_logit, }\FunctionTok{aes}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ model,}
  \AttributeTok{y =}\NormalTok{ coef,}
  \AttributeTok{shape =}\NormalTok{ p,}
  \AttributeTok{group =}\NormalTok{ p}
\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{geom\_linerange}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{ymin =}\NormalTok{ error\_low, }\AttributeTok{ymax =}\NormalTok{ error\_high), }\AttributeTok{alpha =} \FloatTok{0.75}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{size =} \DecValTok{5}\NormalTok{, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{col =}\NormalTok{ p, }\AttributeTok{fill =}\NormalTok{ p), }\AttributeTok{alpha =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
\NormalTok{  viridis}\SpecialCharTok{::}\FunctionTok{scale\_color\_viridis}\NormalTok{(}\AttributeTok{discrete =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{option =} \StringTok{"D"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_shape\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\DecValTok{15}\NormalTok{, }\DecValTok{17}\NormalTok{, }\DecValTok{18}\NormalTok{, }\DecValTok{19}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{, }\AttributeTok{linetype =} \StringTok{\textquotesingle{}dotted\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{\textquotesingle{}A custom coefficient stability plot!\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{subtitle =} \StringTok{"Error bars represent 99\% confidence intervals"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}
    \AttributeTok{axis.text.x =} \FunctionTok{element\_blank}\NormalTok{(),}
    \AttributeTok{axis.title =} \FunctionTok{element\_blank}\NormalTok{(),}
    \AttributeTok{axis.ticks.x =} \FunctionTok{element\_blank}\NormalTok{()}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{coord\_cartesian}\NormalTok{(}\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ min\_space, }\FunctionTok{max}\NormalTok{(coef\_grid\_logit}\SpecialCharTok{$}\NormalTok{model) }\SpecialCharTok{+}\NormalTok{ min\_space),}
                  \AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.1}\NormalTok{, }\FloatTok{1.6}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{guides}\NormalTok{(}\AttributeTok{fill =}\NormalTok{ F, }\AttributeTok{shape =}\NormalTok{ F, }\AttributeTok{col =}\NormalTok{ F)}


\NormalTok{control\_plot }\OtherTok{=} \FunctionTok{ggplot}\NormalTok{(control\_grid\_logit) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ model, }\AttributeTok{y =}\NormalTok{ y, }\AttributeTok{fill=}\NormalTok{value), }\AttributeTok{shape=}\DecValTok{23}\NormalTok{, }\AttributeTok{size=}\DecValTok{4}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_manual}\NormalTok{(}\AttributeTok{values=}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}\#FFFFFF\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}\#000000\textquotesingle{}}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{guides}\NormalTok{(}\AttributeTok{fill=}\NormalTok{F) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{breaks =} \FunctionTok{unique}\NormalTok{(control\_grid\_logit}\SpecialCharTok{$}\NormalTok{y), }
                     \AttributeTok{labels =} \FunctionTok{unique}\NormalTok{(control\_grid\_logit}\SpecialCharTok{$}\NormalTok{key),}
                     \AttributeTok{limits=}\FunctionTok{c}\NormalTok{(}\FunctionTok{min}\NormalTok{(control\_grid\_logit}\SpecialCharTok{$}\NormalTok{y)}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\FunctionTok{max}\NormalTok{(control\_grid\_logit}\SpecialCharTok{$}\NormalTok{y)}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{breaks=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{max}\NormalTok{(control\_grid\_logit}\SpecialCharTok{$}\NormalTok{model))) }\SpecialCharTok{+}
  \FunctionTok{coord\_cartesian}\NormalTok{(}\AttributeTok{xlim=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{min\_space, }\FunctionTok{max}\NormalTok{(control\_grid\_logit}\SpecialCharTok{$}\NormalTok{model)}\SpecialCharTok{+}\NormalTok{min\_space)) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{panel.grid.major.y =} \FunctionTok{element\_blank}\NormalTok{(),}
        \AttributeTok{panel.grid.minor.y =} \FunctionTok{element\_blank}\NormalTok{(),}
        \AttributeTok{axis.title =} \FunctionTok{element\_blank}\NormalTok{(),}
        \AttributeTok{axis.text.y =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size=}\DecValTok{10}\NormalTok{),}
        \AttributeTok{axis.ticks =} \FunctionTok{element\_blank}\NormalTok{(),}
        \AttributeTok{axis.line =} \FunctionTok{element\_blank}\NormalTok{()) }

\NormalTok{cowplot}\SpecialCharTok{::}\FunctionTok{plot\_grid}\NormalTok{(coef\_plot, control\_plot, }\AttributeTok{rel\_heights=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\FloatTok{0.5}\NormalTok{), }
                   \AttributeTok{align=}\StringTok{\textquotesingle{}v\textquotesingle{}}\NormalTok{, }\AttributeTok{ncol=}\DecValTok{1}\NormalTok{, }\AttributeTok{axis=}\StringTok{\textquotesingle{}b\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{38-sensitivity-robustness_files/figure-latex/unnamed-chunk-7-1} \end{center}

To get different model specification (e.g., probit vs.~logit)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{starb\_probit }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(spec, data, rhs, ...) \{}
    \CommentTok{\# Unpack ...}
\NormalTok{    l }\OtherTok{=} \FunctionTok{list}\NormalTok{(...)}
\NormalTok{    get\_mfx }\OtherTok{=} \FunctionTok{ifelse}\NormalTok{(}\FunctionTok{is.null}\NormalTok{(l}\SpecialCharTok{$}\NormalTok{get\_mfx), F, T) }\CommentTok{\# Set a default to F}
    
\NormalTok{    spec }\OtherTok{=} \FunctionTok{as.formula}\NormalTok{(spec)}
    \ControlFlowTok{if}\NormalTok{ (get\_mfx) \{}
\NormalTok{        model }\OtherTok{=} \FunctionTok{glm}\NormalTok{(}
\NormalTok{            spec,}
            \AttributeTok{data =}\NormalTok{ data,}
            \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{\textquotesingle{}probit\textquotesingle{}}\NormalTok{),}
            \AttributeTok{weights =}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{weight}
\NormalTok{        ) }\SpecialCharTok{\%\textgreater{}\%}
            \FunctionTok{margins}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{            summary}
\NormalTok{        row }\OtherTok{=} \FunctionTok{which}\NormalTok{(model}\SpecialCharTok{$}\NormalTok{factor }\SpecialCharTok{==}\NormalTok{ rhs)}
\NormalTok{        coef }\OtherTok{=}\NormalTok{ model[row, }\StringTok{\textquotesingle{}AME\textquotesingle{}}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as.numeric}\NormalTok{()}
\NormalTok{        se   }\OtherTok{=}\NormalTok{ model[row, }\StringTok{\textquotesingle{}SE\textquotesingle{}}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as.numeric}\NormalTok{()}
\NormalTok{        p    }\OtherTok{=}\NormalTok{ model[row, }\StringTok{\textquotesingle{}p\textquotesingle{}}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as.numeric}\NormalTok{()}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{        model }\OtherTok{=} \FunctionTok{glm}\NormalTok{(}
\NormalTok{            spec,}
            \AttributeTok{data =}\NormalTok{ data,}
            \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{\textquotesingle{}probit\textquotesingle{}}\NormalTok{),}
            \AttributeTok{weights =}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{weight}
\NormalTok{        ) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{            broom}\SpecialCharTok{::}\FunctionTok{tidy}\NormalTok{()}
\NormalTok{        row }\OtherTok{=} \FunctionTok{which}\NormalTok{(model}\SpecialCharTok{$}\NormalTok{term }\SpecialCharTok{==}\NormalTok{ rhs)}
\NormalTok{        coef }\OtherTok{=}\NormalTok{ model[row, }\StringTok{\textquotesingle{}estimate\textquotesingle{}}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as.numeric}\NormalTok{()}
\NormalTok{        se   }\OtherTok{=}\NormalTok{ model[row, }\StringTok{\textquotesingle{}std.error\textquotesingle{}}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as.numeric}\NormalTok{()}
\NormalTok{        p    }\OtherTok{=}\NormalTok{ model[row, }\StringTok{\textquotesingle{}p.value\textquotesingle{}}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as.numeric}\NormalTok{()}
\NormalTok{    \}}
    
\NormalTok{    z }\OtherTok{=} \FunctionTok{qnorm}\NormalTok{(}\FloatTok{0.995}\NormalTok{)}
    \FunctionTok{return}\NormalTok{(}\FunctionTok{c}\NormalTok{(coef, p, coef }\SpecialCharTok{+}\NormalTok{ z }\SpecialCharTok{*}\NormalTok{ se, coef }\SpecialCharTok{{-}}\NormalTok{ z }\SpecialCharTok{*}\NormalTok{ se))}
\NormalTok{\}}

\NormalTok{probit\_dfs }\OtherTok{=} \FunctionTok{stability\_plot}\NormalTok{(}
    \AttributeTok{grid =}\NormalTok{ grid1,}
    \AttributeTok{data =}\NormalTok{ diamonds,}
    \AttributeTok{lhs =}\NormalTok{ lhs\_var,}
    \AttributeTok{rhs =}\NormalTok{ rhs\_var,}
    \AttributeTok{model =}\NormalTok{ starb\_probit,}
    \AttributeTok{get\_mfx =}\NormalTok{ T,}
    \AttributeTok{perm =}\NormalTok{ perm\_controls,}
    \AttributeTok{base =}\NormalTok{ base\_controls,}
    \AttributeTok{fe\_always =}\NormalTok{ F,}
    \AttributeTok{run\_from =} \DecValTok{3}\NormalTok{,}
    \AttributeTok{run\_to =} \DecValTok{5}
\NormalTok{)}

\CommentTok{\# We\textquotesingle{}ll put the probit DFs on the left, }
 \CommentTok{\#so we need to adjust the model numbers accordingly}
\CommentTok{\# so the probit and logit DFs don\textquotesingle{}t plot on top of one another!}
\NormalTok{coef\_grid\_probit }\OtherTok{=}\NormalTok{ probit\_dfs[[}\DecValTok{1}\NormalTok{]] }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{model =}\NormalTok{ model }\SpecialCharTok{+} \FunctionTok{max}\NormalTok{(coef\_grid\_logit}\SpecialCharTok{$}\NormalTok{model))}

\NormalTok{control\_grid\_probit }\OtherTok{=}\NormalTok{ probit\_dfs[[}\DecValTok{2}\NormalTok{]] }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{model =}\NormalTok{ model }\SpecialCharTok{+} \FunctionTok{max}\NormalTok{(control\_grid\_logit}\SpecialCharTok{$}\NormalTok{model))}

\NormalTok{coef\_grid    }\OtherTok{=} \FunctionTok{bind\_rows}\NormalTok{(coef\_grid\_logit, coef\_grid\_probit)}
\NormalTok{control\_grid }\OtherTok{=} \FunctionTok{bind\_rows}\NormalTok{(control\_grid\_logit, control\_grid\_probit)}

\NormalTok{panels }\OtherTok{=} \FunctionTok{stability\_plot}\NormalTok{(}
    \AttributeTok{coef\_grid =}\NormalTok{ coef\_grid,}
    \AttributeTok{control\_grid =}\NormalTok{ control\_grid,}
    \AttributeTok{data =}\NormalTok{ diamonds,}
    \AttributeTok{lhs =}\NormalTok{ lhs\_var,}
    \AttributeTok{rhs =}\NormalTok{ rhs\_var,}
    \AttributeTok{perm =}\NormalTok{ perm\_controls,}
    \AttributeTok{base =}\NormalTok{ base\_controls,}
    \AttributeTok{fe\_always =}\NormalTok{ F,}
    \AttributeTok{run\_from =} \DecValTok{5}\NormalTok{,}
    \AttributeTok{run\_to =} \DecValTok{6}
\NormalTok{)}

\NormalTok{coef\_plot }\OtherTok{=}\NormalTok{ panels[[}\DecValTok{1}\NormalTok{]] }\SpecialCharTok{+} \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \FloatTok{8.5}\NormalTok{,}
                                     \AttributeTok{linetype =} \StringTok{\textquotesingle{}dashed\textquotesingle{}}\NormalTok{,}
                                     \AttributeTok{alpha =} \FloatTok{0.8}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{annotate}\NormalTok{(}
        \AttributeTok{geom =} \StringTok{\textquotesingle{}label\textquotesingle{}}\NormalTok{,}
        \AttributeTok{x =} \FloatTok{4.25}\NormalTok{,}
        \AttributeTok{y =} \FloatTok{1.8}\NormalTok{,}
        \AttributeTok{label =} \StringTok{\textquotesingle{}Logit models\textquotesingle{}}\NormalTok{,}
        \AttributeTok{size =} \DecValTok{6}\NormalTok{,}
        \AttributeTok{fill =} \StringTok{\textquotesingle{}\#D3D3D3\textquotesingle{}}\NormalTok{,}
        \AttributeTok{alpha =} \FloatTok{0.7}
\NormalTok{    ) }\SpecialCharTok{+}
    \FunctionTok{annotate}\NormalTok{(}
        \AttributeTok{geom =} \StringTok{\textquotesingle{}label\textquotesingle{}}\NormalTok{,}
        \AttributeTok{x =} \FloatTok{12.75}\NormalTok{,}
        \AttributeTok{y =} \FloatTok{1.8}\NormalTok{,}
        \AttributeTok{label =} \StringTok{\textquotesingle{}Probit models\textquotesingle{}}\NormalTok{,}
        \AttributeTok{size =} \DecValTok{6}\NormalTok{,}
        \AttributeTok{fill =} \StringTok{\textquotesingle{}\#D3D3D3\textquotesingle{}}\NormalTok{,}
        \AttributeTok{alpha =} \FloatTok{0.7}
\NormalTok{    ) }\SpecialCharTok{+}
    \FunctionTok{coord\_cartesian}\NormalTok{(}\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.5}\NormalTok{, }\FloatTok{1.9}\NormalTok{))}

\NormalTok{control\_plot }\OtherTok{=}\NormalTok{ panels[[}\DecValTok{2}\NormalTok{]] }\SpecialCharTok{+} \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \FloatTok{8.5}\NormalTok{,}
                                        \AttributeTok{linetype =} \StringTok{\textquotesingle{}dashed\textquotesingle{}}\NormalTok{,}
                                        \AttributeTok{alpha =} \FloatTok{0.8}\NormalTok{)}

\NormalTok{cowplot}\SpecialCharTok{::}\FunctionTok{plot\_grid}\NormalTok{(}
\NormalTok{    coef\_plot,}
\NormalTok{    control\_plot,}
    \AttributeTok{rel\_heights =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{),}
    \AttributeTok{align =} \StringTok{\textquotesingle{}v\textquotesingle{}}\NormalTok{,}
    \AttributeTok{ncol =} \DecValTok{1}\NormalTok{,}
    \AttributeTok{axis =} \StringTok{\textquotesingle{}b\textquotesingle{}}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{38-sensitivity-robustness_files/figure-latex/unnamed-chunk-8-1} \end{center}

\hypertarget{rdfanalysis}{%
\subsection{rdfanalysis}\label{rdfanalysis}}

\begin{itemize}
\tightlist
\item
  Not recommend
\end{itemize}

Installation

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{devtools}\SpecialCharTok{::}\FunctionTok{install\_github}\NormalTok{(}\StringTok{"joachim{-}gassen/rdfanalysis"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Example by the \href{https://joachim-gassen.github.io/rdfanalysis/}{package's author}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rdfanalysis)}
\FunctionTok{load}\NormalTok{(}\FunctionTok{url}\NormalTok{(}\StringTok{"https://joachim{-}gassen.github.io/data/rdf\_ests.RData"}\NormalTok{))}
\FunctionTok{plot\_rdf\_spec\_curve}\NormalTok{(ests, }\StringTok{"est"}\NormalTok{, }\StringTok{"lb"}\NormalTok{, }\StringTok{"ub"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{38-sensitivity-robustness_files/figure-latex/unnamed-chunk-10-1} \end{center}

Shiny app for readers to explore

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{design }\OtherTok{\textless{}{-}} \FunctionTok{define\_design}\NormalTok{(}\AttributeTok{steps =} \FunctionTok{c}\NormalTok{(}\StringTok{"read\_data"}\NormalTok{,}
                                  \StringTok{"select\_idvs"}\NormalTok{,}
                                  \StringTok{"treat\_extreme\_obs"}\NormalTok{,}
                                  \StringTok{"specify\_model"}\NormalTok{,}
                                  \StringTok{"est\_model"}\NormalTok{),}
                        \AttributeTok{rel\_dir =} \StringTok{"vignettes/case\_study\_code"}\NormalTok{)}

\FunctionTok{shiny\_rdf\_spec\_curve}\NormalTok{(ests, }\FunctionTok{list}\NormalTok{(}\StringTok{"est"}\NormalTok{, }\StringTok{"lb"}\NormalTok{, }\StringTok{"ub"}\NormalTok{),}
\NormalTok{                     design, }\StringTok{"vignettes/case\_study\_code"}\NormalTok{,}
                     \StringTok{"https://joachim{-}gassen.github.io/data/wb\_new.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{coefficient-stability}{%
\section{Coefficient stability}\label{coefficient-stability}}

\citep{oster2019unobservable}

\begin{itemize}
\item
  Coefficient stability can be evident against omitted variable bias.
\item
  But coefficient stability alone can be misleading, but combing with \(R^2\) movement, it can become informative.
\end{itemize}

Packages

\begin{itemize}
\item
  \texttt{mplot}: graphical Model stability and Variable Selection
\item
  \texttt{robomit}: Robustness checks for omitted variable bias (implementation of
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(robomit)}

\CommentTok{\# estimate beta }
\FunctionTok{o\_beta}\NormalTok{(}
  \AttributeTok{y     =} \StringTok{"mpg"}\NormalTok{,       }\CommentTok{\# dependent variable}
  \AttributeTok{x     =} \StringTok{"wt"}\NormalTok{,        }\CommentTok{\# independent treatment variable}
  \AttributeTok{con   =} \StringTok{"hp + qsec"}\NormalTok{, }\CommentTok{\# related control variables}
  \AttributeTok{delta =} \DecValTok{1}\NormalTok{,           }\CommentTok{\# delta}
  \AttributeTok{R2max =} \FloatTok{0.9}\NormalTok{,         }\CommentTok{\# maximum R{-}square}
  \AttributeTok{type  =} \StringTok{"lm"}\NormalTok{,        }\CommentTok{\# model type}
  \AttributeTok{data  =}\NormalTok{ mtcars       }\CommentTok{\# dataset}
\NormalTok{) }
\CommentTok{\#\textgreater{} \# A tibble: 10 x 2}
\CommentTok{\#\textgreater{}    Name                           Value}
\CommentTok{\#\textgreater{}    \textless{}chr\textgreater{}                          \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1 beta*                         {-}2.00 }
\CommentTok{\#\textgreater{}  2 (beta*{-}beta controlled)\^{}2      5.56 }
\CommentTok{\#\textgreater{}  3 Alternative Solution 1        {-}7.01 }
\CommentTok{\#\textgreater{}  4 (beta[AS1]{-}beta controlled)\^{}2  7.05 }
\CommentTok{\#\textgreater{}  5 Uncontrolled Coefficient      {-}5.34 }
\CommentTok{\#\textgreater{}  6 Controlled Coefficient        {-}4.36 }
\CommentTok{\#\textgreater{}  7 Uncontrolled R{-}square          0.753}
\CommentTok{\#\textgreater{}  8 Controlled R{-}square            0.835}
\CommentTok{\#\textgreater{}  9 Max R{-}square                   0.9  }
\CommentTok{\#\textgreater{} 10 delta                          1}
\end{Highlighting}
\end{Shaded}

\hypertarget{replication-and-synthetic-data}{%
\chapter{Replication and Synthetic Data}\label{replication-and-synthetic-data}}

Access to comprehensive data is pivotal for replication, especially in the realm of social sciences. Yet, often the data are inaccessible, making replication a challenge \citep{king1995replication}. This chapter dives into the nuances of replication, the exceptions to its norms, and the significance of synthetic data.

\hypertarget{the-replication-standard}{%
\section{The Replication Standard}\label{the-replication-standard}}

Replicability in research ensures:

\begin{itemize}
\tightlist
\item
  Credibility and comprehension of empirical studies.
\item
  Continuity and progression in the discipline.
\item
  Enhanced readership and academic citations.
\end{itemize}

For a research to be replicable, the ``replication standard'' is vital: it entails providing all requisite information for replication by third parties. While quantitative research can, to some extent, offer clear data, qualitative studies pose complexities due to data depth.

\hypertarget{solutions-for-empirical-replication}{%
\subsection{Solutions for Empirical Replication}\label{solutions-for-empirical-replication}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Role of Individual Authors}:

  \begin{itemize}
  \tightlist
  \item
    Authors need to vouch for the replication standard for enhancing their work's credibility.
  \item
    Archives like the Inter-University Consortium for Political and Social Research (ICPSR) serve as depositories for replication datasets.
  \end{itemize}
\item
  \textbf{Creation of a Replication Data Set}:

  \begin{itemize}
  \tightlist
  \item
    A public data set, consisting of both original and relevant complementary data, can serve replication purposes.
  \end{itemize}
\item
  \textbf{Professional Data Archives}:

  \begin{itemize}
  \tightlist
  \item
    Organizations like ICPSR provide solutions to data storage and accessibility problems.
  \end{itemize}
\item
  \textbf{Educational Implications}:

  \begin{itemize}
  \tightlist
  \item
    Replication can be an excellent educational tool, and many programs now emphasize its importance.
  \end{itemize}
\end{enumerate}

\hypertarget{free-data-repositories}{%
\subsection{Free Data Repositories}\label{free-data-repositories}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Zenodo}: Hosted by CERN, it provides a place for researchers to deposit datasets. It's not subject-specific, so it caters to various disciplines.
\item
  \textbf{figshare}: Allows researchers to upload, share, and cite their datasets.
\item
  \textbf{Dryad}: Primarily for datasets associated with published articles in the biological and medical sciences.
\item
  \textbf{OpenICPSR}: A public-facing version of the Inter-University Consortium for Political and Social Research (ICPSR) where researchers can deposit data without any cost.
\item
  \textbf{Harvard Dataverse}: Hosted by Harvard University, this is an open-source repository software application dedicated to archiving, sharing, and citing research data.
\item
  \textbf{Mendeley Data}: A multidisciplinary, free-to-use open access data repository where researchers can upload and share their datasets.
\item
  \textbf{Open Science Framework (OSF)}: Offers both a platform for conducting research and a place to deposit datasets.
\item
  \textbf{PubMed Central}: Specific to life sciences, but it's an open repository for journal articles, preprints, and datasets.
\item
  \textbf{Registry of Research Data Repositories (re3data)}: While not a repository itself, it provides a global registry of research data repositories from various academic disciplines.
\item
  \textbf{SocArXiv}: An open archive for the social sciences.
\item
  \textbf{EarthArXiv}: A preprints archive for earth science.
\item
  \textbf{Protein Data Bank (PDB)}: For 3D structures of large biological molecules.
\item
  \textbf{Gene Expression Omnibus (GEO)}: A public functional genomics data repository.
\item
  \textbf{The Language Archive (TLA)}: Dedicated to data on languages worldwide, especially endangered languages.
\item
  \textbf{B2SHARE}: A platform for storing and sharing research data sets in various disciplines, especially from European research projects.
\end{enumerate}

\hypertarget{exceptions-to-replication}{%
\subsection{Exceptions to Replication}\label{exceptions-to-replication}}

Some exceptions to the replication standard are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Confidentiality}: Sometimes data, even fragmented, is too sensitive to share.
\item
  \textbf{Proprietary Data}: Data sets owned by entities might restrict dissemination, but usually, parts of such data can still be shared.
\item
  \textbf{Rights of First Publication}: Embargos might be set, but the essential data used in a study should be accessible.
\end{enumerate}

\hypertarget{synthetic-data-an-overview}{%
\section{Synthetic Data: An Overview}\label{synthetic-data-an-overview}}

Synthetic data, modeling real data while ensuring anonymity, is becoming pivotal in research. While promising, it has its own complexities and should be approached with caution.

\hypertarget{benefits}{%
\subsection{Benefits}\label{benefits}}

\begin{itemize}
\tightlist
\item
  Privacy preservation.
\item
  Data fairness and augmentation.
\item
  Acceleration in research.
\end{itemize}

\hypertarget{concerns}{%
\subsection{Concerns}\label{concerns}}

\begin{itemize}
\tightlist
\item
  Misconceptions about inherent privacy.
\item
  Challenges with data outliers.
\item
  Models relying solely on synthetic data can pose risks.
\end{itemize}

\hypertarget{further-insights-on-synthetic-data}{%
\subsection{Further Insights on Synthetic Data}\label{further-insights-on-synthetic-data}}

Synthetic data bridges the model-centric and data-centric perspectives, making it an essential tool in modern research. Analogously, it's like viewing the Mona Lisa's replica, with the real painting stored securely.

Future projects, such as utilizing the R's diamonds dataset for synthetic data generation, hold promise in demonstrating the vast potentials of this technology.

For a deeper dive into synthetic data and its applications, refer to \citep{jordon2022synthetic}.

\hypertarget{application-15}{%
\section{Application}\label{application-15}}

The easiest way to create synthetic data is to use the \texttt{synthpop} package. Alternatively, you can do it \href{https://towardsdatascience.com/creating-synthetic-data-3774391c851d}{manually}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(synthpop)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(performance)}

\CommentTok{\# library(effectsize)}
\CommentTok{\# library(see)}
\CommentTok{\# library(patchwork)}
\CommentTok{\# library(knitr)}
\CommentTok{\# library(kableExtra)}

\FunctionTok{head}\NormalTok{(iris)}
\CommentTok{\#\textgreater{}   Sepal.Length Sepal.Width Petal.Length Petal.Width Species}
\CommentTok{\#\textgreater{} 1          5.1         3.5          1.4         0.2  setosa}
\CommentTok{\#\textgreater{} 2          4.9         3.0          1.4         0.2  setosa}
\CommentTok{\#\textgreater{} 3          4.7         3.2          1.3         0.2  setosa}
\CommentTok{\#\textgreater{} 4          4.6         3.1          1.5         0.2  setosa}
\CommentTok{\#\textgreater{} 5          5.0         3.6          1.4         0.2  setosa}
\CommentTok{\#\textgreater{} 6          5.4         3.9          1.7         0.4  setosa}

\NormalTok{synthpop}\SpecialCharTok{::}\FunctionTok{codebook.syn}\NormalTok{(iris)}
\CommentTok{\#\textgreater{} $tab}
\CommentTok{\#\textgreater{}       variable   class nmiss perctmiss ndistinct}
\CommentTok{\#\textgreater{} 1 Sepal.Length numeric     0         0        35}
\CommentTok{\#\textgreater{} 2  Sepal.Width numeric     0         0        23}
\CommentTok{\#\textgreater{} 3 Petal.Length numeric     0         0        43}
\CommentTok{\#\textgreater{} 4  Petal.Width numeric     0         0        22}
\CommentTok{\#\textgreater{} 5      Species  factor     0         0         3}
\CommentTok{\#\textgreater{}                             details}
\CommentTok{\#\textgreater{} 1                  Range: 4.3 {-} 7.9}
\CommentTok{\#\textgreater{} 2                    Range: 2 {-} 4.4}
\CommentTok{\#\textgreater{} 3                    Range: 1 {-} 6.9}
\CommentTok{\#\textgreater{} 4                  Range: 0.1 {-} 2.5}
\CommentTok{\#\textgreater{} 5 \textquotesingle{}setosa\textquotesingle{} \textquotesingle{}versicolor\textquotesingle{} \textquotesingle{}virginica\textquotesingle{}}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $labs}
\CommentTok{\#\textgreater{} NULL}

\NormalTok{syn\_df }\OtherTok{\textless{}{-}} \FunctionTok{syn}\NormalTok{(iris, }\AttributeTok{seed =} \DecValTok{3}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Synthesis}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}  Sepal.Length Sepal.Width Petal.Length Petal.Width Species}

\CommentTok{\# check for replciated uniques}
\FunctionTok{replicated.uniques}\NormalTok{(syn\_df, iris)}
\CommentTok{\#\textgreater{} $replications}
\CommentTok{\#\textgreater{}   [1]  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE}
\CommentTok{\#\textgreater{}  [13]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE}
\CommentTok{\#\textgreater{}  [25] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE}
\CommentTok{\#\textgreater{}  [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE}
\CommentTok{\#\textgreater{}  [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE}
\CommentTok{\#\textgreater{}  [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE}
\CommentTok{\#\textgreater{}  [73] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE}
\CommentTok{\#\textgreater{}  [85] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE}
\CommentTok{\#\textgreater{}  [97] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE}
\CommentTok{\#\textgreater{} [109] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE}
\CommentTok{\#\textgreater{} [121] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE}
\CommentTok{\#\textgreater{} [133] FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE}
\CommentTok{\#\textgreater{} [145] FALSE FALSE FALSE FALSE FALSE FALSE}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $no.uniques}
\CommentTok{\#\textgreater{} [1] 148}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $no.replications}
\CommentTok{\#\textgreater{} [1] 17}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $per.replications}
\CommentTok{\#\textgreater{} [1] 11.33333}


\CommentTok{\# remove replicated uniques and adds a FAKE\_DATA label }
\CommentTok{\# (in case a person can see his or own data in }
\CommentTok{\# the replicated data by chance)}

\NormalTok{syn\_df\_sdc }\OtherTok{\textless{}{-}} \FunctionTok{sdc}\NormalTok{(syn\_df, iris, }
                  \AttributeTok{label =} \StringTok{"FAKE\_DATA"}\NormalTok{,}
                  \AttributeTok{rm.replicated.uniques =}\NormalTok{ T)}
\CommentTok{\#\textgreater{} no. of replicated uniques: 17}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris }\SpecialCharTok{|\textgreater{}} 
\NormalTok{    GGally}\SpecialCharTok{::}\FunctionTok{ggpairs}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{39-rep_synthetic_data_files/figure-latex/unnamed-chunk-2-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{syn\_df}\SpecialCharTok{$}\NormalTok{syn }\SpecialCharTok{|\textgreater{}} 
\NormalTok{    GGally}\SpecialCharTok{::}\FunctionTok{ggpairs}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{39-rep_synthetic_data_files/figure-latex/unnamed-chunk-2-2} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm\_ori }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Sepal.Length }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Sepal.Width }\SpecialCharTok{+}\NormalTok{ Petal.Length , }\AttributeTok{data =}\NormalTok{ iris)}
\CommentTok{\# performance::check\_model(lm\_ori)}
\FunctionTok{summary}\NormalTok{(lm\_ori)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = Sepal.Length \textasciitilde{} Sepal.Width + Petal.Length, data = iris)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}      Min       1Q   Median       3Q      Max }
\CommentTok{\#\textgreater{} {-}0.96159 {-}0.23489  0.00077  0.21453  0.78557 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}              Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)   2.24914    0.24797    9.07 7.04e{-}16 ***}
\CommentTok{\#\textgreater{} Sepal.Width   0.59552    0.06933    8.59 1.16e{-}14 ***}
\CommentTok{\#\textgreater{} Petal.Length  0.47192    0.01712   27.57  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 0.3333 on 147 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.8402, Adjusted R{-}squared:  0.838 }
\CommentTok{\#\textgreater{} F{-}statistic: 386.4 on 2 and 147 DF,  p{-}value: \textless{} 2.2e{-}16}

\NormalTok{lm\_syn }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Sepal.Length }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Sepal.Width }\SpecialCharTok{+}\NormalTok{ Petal.Length , }\AttributeTok{data =}\NormalTok{ syn\_df}\SpecialCharTok{$}\NormalTok{syn)}
\CommentTok{\# performance::check\_model(lm\_syn)}
\FunctionTok{summary}\NormalTok{(lm\_syn)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = Sepal.Length \textasciitilde{} Sepal.Width + Petal.Length, data = syn\_df$syn)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}      Min       1Q   Median       3Q      Max }
\CommentTok{\#\textgreater{} {-}0.79165 {-}0.22790 {-}0.01448  0.15893  1.13360 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}              Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)   2.96449    0.24538  12.081  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} Sepal.Width   0.39214    0.06816   5.754  4.9e{-}08 ***}
\CommentTok{\#\textgreater{} Petal.Length  0.45267    0.01743  25.974  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 0.3658 on 147 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.8246, Adjusted R{-}squared:  0.8222 }
\CommentTok{\#\textgreater{} F{-}statistic: 345.6 on 2 and 147 DF,  p{-}value: \textless{} 2.2e{-}16}
\end{Highlighting}
\end{Shaded}

Open data can be assessed for its utility in two distinct ways:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{General Utility}: This refers to the broad resemblances within the dataset, allowing for preliminary data exploration.
\item
  \textbf{Specific Utility}: This focuses on the comparability of models derived from synthetic and original datasets, emphasizing analytical reproducibility.
\end{enumerate}

For General utility

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{compare}\NormalTok{(syn\_df, iris)}
\end{Highlighting}
\end{Shaded}

Specific utility

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# just like regular lm, but for synthetic data}
\NormalTok{lm\_syn }\OtherTok{\textless{}{-}} \FunctionTok{lm.synds}\NormalTok{(Sepal.Length }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Sepal.Width }\SpecialCharTok{+}\NormalTok{ Petal.Length , }\AttributeTok{data =}\NormalTok{ syn\_df)}
\FunctionTok{compare}\NormalTok{(lm\_syn, iris)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call used to fit models to the data:}
\CommentTok{\#\textgreater{} lm.synds(formula = Sepal.Length \textasciitilde{} Sepal.Width + Petal.Length, }
\CommentTok{\#\textgreater{}     data = syn\_df)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Differences between results based on synthetic and observed data:}
\CommentTok{\#\textgreater{}              Synthetic  Observed        Diff Std. coef diff CI overlap}
\CommentTok{\#\textgreater{} (Intercept)  2.9644900 2.2491402  0.71534988       2.884829  0.2640608}
\CommentTok{\#\textgreater{} Sepal.Width  0.3921429 0.5955247 {-}0.20338187      {-}2.933611  0.2516161}
\CommentTok{\#\textgreater{} Petal.Length 0.4526695 0.4719200 {-}0.01925058      {-}1.124602  0.7131064}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Measures for one synthesis and 3 coefficients}
\CommentTok{\#\textgreater{} Mean confidence interval overlap:  0.4095944}
\CommentTok{\#\textgreater{} Mean absolute std. coef diff:  2.314347}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Mahalanobis distance ratio for lack{-}of{-}fit (target 1.0): 3.08}
\CommentTok{\#\textgreater{} Lack{-}of{-}fit test: 9.23442; p{-}value 0.0263 for test that synthesis model is}
\CommentTok{\#\textgreater{} compatible with a chi{-}squared test with 3 degrees of freedom.}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Confidence interval plot:}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{39-rep_synthetic_data_files/figure-latex/unnamed-chunk-5-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# summary(lm\_syn)}
\end{Highlighting}
\end{Shaded}

You basically want your lack-of-fit test to be non-significant.

\hypertarget{appendix-appendix}{%
\appendix}


\hypertarget{appendix}{%
\chapter{Appendix}\label{appendix}}

\hypertarget{git}{%
\section{Git}\label{git}}

\href{https://training.github.com/downloads/github-git-cheat-sheet.pdf}{Cheat Sheet}

\href{https://training.github.com/}{Cheat Sheet in different languages}

\href{http://try.github.io/}{Learn Git}

\href{http://ndpsoftware.com/git-cheatsheet.html\#loc=remote_repo;}{Interactive Cheat Sheet}

\href{https://happygitwithr.com/}{Ultimate Guide of Git and GitHub for R user}

\begin{itemize}
\item
  Setting up Git: \texttt{git\ config} with \texttt{-\/-global} option to configure user name, email, editor, etc.
\item
  Creating a repository: \texttt{git\ init} to initialize a repo. Git stores all of its repo data in the \texttt{.git} directory.
\item
  Tracking changes:

  \begin{itemize}
  \item
    \texttt{git\ status} shows the status of the repo

    \begin{itemize}
    \item
      File are stored in the project's working directory (which users see)
    \item
      The staging area (where the next commit is being built)
    \item
      local repo is where commits are permanently recorded
    \end{itemize}
  \item
    \texttt{git\ add} put files in the staging area
  \item
    \texttt{git\ commit} saves the staged content as a new commit in the local repo.

    \begin{itemize}
    \tightlist
    \item
      \texttt{git\ commit\ -m\ "your\ own\ message"} to give a messages for the purpose of your commit.
    \end{itemize}
  \end{itemize}
\item
  History

  \begin{itemize}
  \item
    \texttt{git\ diff} shows differences between commits
  \item
    \texttt{git\ checkout} recovers old version of fields

    \begin{itemize}
    \item
      \texttt{git\ checkout\ HEAD} to go to the last commit
    \item
      \texttt{git\ checkout\ \textless{}unique\ ID\ of\ your\ commit\textgreater{}} to go to such commit
    \end{itemize}
  \end{itemize}
\item
  Ignoring

  \begin{itemize}
  \item
    \texttt{.gitignore} file tells Git what files to ignore
  \item
    \texttt{cat\ .\ gitignore\ *.dat\ results/} ignore files ending with ``dat'' and folder ``results''.
  \end{itemize}
\item
  Remotes in GitHub

  \begin{itemize}
  \item
    A local git repo can be connected to one or more remote repos.
  \item
    Use the HTTPS protocol to connect to remote repos
  \item
    \texttt{git\ push} copies changes from a local repo to a remote repo
  \item
    \texttt{git\ pull} copies changes from a remote repo to a local repo
  \end{itemize}
\item
  Collaborating

  \begin{itemize}
  \tightlist
  \item
    \texttt{git\ clone} copies remote repo to create a local repo with a remote called \texttt{origin} automatically set up
  \end{itemize}
\item
  Branching

  \begin{itemize}
  \item
    \texttt{git\ check\ -\ b\ \textless{}new-branch-name}
  \item
    \texttt{git\ checkout\ master} to switch to master branch.
  \end{itemize}
\item
  Conflicts

  \begin{itemize}
  \item
    occur when 2 or more people change the same lines of the same file
  \item
    the version control system does not allow to overwrite each other's changes blindly, but highlights conflicts so that they can be resolved.
  \end{itemize}
\item
  Licensing

  \begin{itemize}
  \item
    People who incorporate General Public License (GPL'd) software into their won software must make their software also open under the GPL license; most other open licenses do not require this.
  \item
    The Creative Commons family of licenses allow people to mix and match requirements and restrictions on attribution, creation of derivative works, further sharing and commercialization.
  \end{itemize}
\item
  Citation:

  \begin{itemize}
  \tightlist
  \item
    Add a CITATION file to a repo to explain how you want others to cite your work.
  \end{itemize}
\item
  Hosting

  \begin{itemize}
  \tightlist
  \item
    Rules regarding intellectual property and storage of sensitive info apply no matter where code and data are hosted.
  \end{itemize}
\end{itemize}

\hypertarget{short-cut}{%
\section{Short-cut}\label{short-cut}}

These are shortcuts that you probably you remember when working with R. Even though it might take a bit of time to learn and use them as your second nature, but they will save you a lot of time.\\
Just like learning another language, the more you speak and practice it, the more comfortable you are speaking it.\\

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.6494}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.3506}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
function
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
short-cut
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
navigate folders in console & \texttt{"\ "\ +\ tab} \\
pull up short-cut cheat sheet & \texttt{ctrl\ +\ shift\ +\ k} \\
go to file/function (everything in your project) & \texttt{ctrl\ +\ .} \\
search everything & \texttt{cmd\ +\ shift\ +\ f} \\
navigate between tabs & \texttt{Crtl\ +\ shift\ +\ .} \\
type function faster & \texttt{snip\ +\ shift\ +\ tab} \\
type faster & \texttt{use\ tab\ for\ fuzzy\ match} \\
\texttt{cmd\ +\ up} & \\
\texttt{ctrl\ +\ .} & \\
\end{longtable}

Sometimes you can't stage a folder because it's too large. In such case, use \texttt{Terminal} pane in Rstudio then type \texttt{git\ add\ -A} to stage all changes then commit and push like usual.

\hypertarget{function-short-cut}{%
\section{Function short-cut}\label{function-short-cut}}

apply one function to your data to create a new variable: \texttt{mutate(mod=map(data,function))}\\
instead of using \texttt{i\ in\ 1:length(object)}: \texttt{for\ (i\ in\ seq\_along(object))}\\
apply multiple function: \texttt{map\_dbl}\\
apply multiple function to multiple variables:\texttt{map2}\\
\texttt{autoplot(data)} plot times series data\\
\texttt{mod\_tidy\ =\ linear(reg)\ \%\textgreater{}\%\ set\_engine(\textquotesingle{}lm\textquotesingle{})\ \%\textgreater{}\%\ fit(price\ \textasciitilde{}\ .,\ data=data)} fit lm model. It could also fit other models (stan, spark, glmnet, keras)

\begin{itemize}
\tightlist
\item
  Sometimes, data-masking will not be able to recognize whether you're calling from environment or data variables. To bypass this, we use \texttt{.data\$variable} or \texttt{.env\$variable}. For example \texttt{data\ \%\textgreater{}\%\ mutate(x=.env\$variable/.data\$variable}\\
\item
  Problems with data-masking:\\

  \begin{itemize}
  \tightlist
  \item
    Unexpected masking by data-var: Use \texttt{.data} and \texttt{.env} to disambiguate\\
  \item
    Data-var cant get through:\\
  \item
    Tunnel data-var with \{\{\}\} + Subset \texttt{.data} with {[}{[}{]}{]}
  \end{itemize}
\item
  Passing Data-variables through arguments
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"dplyr"}\NormalTok{)}
\NormalTok{mean\_by }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data,by,var)\{}
\NormalTok{    data }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{group\_by}\NormalTok{(\{\{\{by\}\}\}) }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{summarise}\NormalTok{(}\StringTok{"\{\{var\}\}"}\SpecialCharTok{:=}\FunctionTok{mean}\NormalTok{(\{\{var\}\})) }\CommentTok{\# new name for each var will be created by tunnel data{-}var inside strings}
\NormalTok{\}}

\NormalTok{mean\_by }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data,by,var)\{}
\NormalTok{    data }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{group\_by}\NormalTok{(\{\{\{by\}\}\}) }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{summarise}\NormalTok{(}\StringTok{"\{var\}"}\SpecialCharTok{:=}\FunctionTok{mean}\NormalTok{(\{\{var\}\})) }\CommentTok{\# use single \{\} to glue the string, but hard to reuse code in functions}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Trouble with selection:\\
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"purrr"}\NormalTok{)}
\NormalTok{name }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"mass"}\NormalTok{,}\StringTok{"height"}\NormalTok{)}
\NormalTok{starwars }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(name) }\CommentTok{\# Data{-}var. Here you are referring to variable named "name"}

\NormalTok{starwars }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(}\FunctionTok{all\_of}\NormalTok{((name))) }\CommentTok{\# use all\_of() to disambiguate when }

\NormalTok{averages }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data,vars)\{ }\CommentTok{\# take character vectors with all\_of()}
\NormalTok{    data }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{select}\NormalTok{(}\FunctionTok{all\_of}\NormalTok{(vars)) }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{map\_dbl}\NormalTok{(mean,}\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{)}
\NormalTok{\} }

\NormalTok{x }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"Sepal.Length"}\NormalTok{,}\StringTok{"Petal.Length"}\NormalTok{)}
\NormalTok{iris }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{averages}\NormalTok{(x)}


\CommentTok{\# Another way}
\NormalTok{averages }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data,vars)\{ }\CommentTok{\# Tunnel selectiosn with \{\{\}\}}
\NormalTok{    data }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{select}\NormalTok{(\{\{vars\}\}) }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{map\_dbl}\NormalTok{(mean,}\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{)}
\NormalTok{\} }

\NormalTok{x }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"Sepal.Length"}\NormalTok{,}\StringTok{"Petal.Length"}\NormalTok{)}
\NormalTok{iris }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{averages}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\hypertarget{citation}{%
\section{Citation}\label{citation}}

include a citation by \texttt{{[}@Farjam\_2015{]}}

cite packages used in this session

\texttt{package=ls(sessionInfo()\$loadedOnly)\ for\ (i\ in\ package)\{print(toBibtex(citation(i)))\}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{package}\OtherTok{=}\FunctionTok{ls}\NormalTok{(}\FunctionTok{sessionInfo}\NormalTok{()}\SpecialCharTok{$}\NormalTok{loadedOnly) }
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in}\NormalTok{ package)\{}
    \FunctionTok{print}\NormalTok{(}\FunctionTok{toBibtex}\NormalTok{(}\FunctionTok{citation}\NormalTok{(i)))}
\NormalTok{    \}}
\end{Highlighting}
\end{Shaded}

\hypertarget{install-all-necessary-packageslibaries-on-your-local-machine}{%
\section{Install all necessary packages/libaries on your local machine}\label{install-all-necessary-packageslibaries-on-your-local-machine}}

Get a list of packages you need to install from this book (or your local device)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{installed }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{installed.packages}\NormalTok{())}

\FunctionTok{head}\NormalTok{(installed)}
\CommentTok{\#\textgreater{}         Package                            LibPath Version Priority}
\CommentTok{\#\textgreater{} abind     abind C:/Program Files/R/R{-}4.2.3/library   1.4{-}5     \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{} ade4       ade4 C:/Program Files/R/R{-}4.2.3/library  1.7{-}22     \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{} admisc   admisc C:/Program Files/R/R{-}4.2.3/library    0.33     \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{} AER         AER C:/Program Files/R/R{-}4.2.3/library  1.2{-}10     \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{} afex       afex C:/Program Files/R/R{-}4.2.3/library   1.3{-}0     \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{} agridat agridat C:/Program Files/R/R{-}4.2.3/library    1.21     \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{}                                                                                        Depends}
\CommentTok{\#\textgreater{} abind                                                                             R (\textgreater{}= 1.5.0)}
\CommentTok{\#\textgreater{} ade4                                                                               R (\textgreater{}= 2.10)}
\CommentTok{\#\textgreater{} admisc                                                                            R (\textgreater{}= 3.5.0)}
\CommentTok{\#\textgreater{} AER     R (\textgreater{}= 3.0.0), car (\textgreater{}= 2.0{-}19), lmtest, sandwich (\textgreater{}= 2.4{-}0),\textbackslash{}nsurvival (\textgreater{}= 2.37{-}5), zoo}
\CommentTok{\#\textgreater{} afex                                                             R (\textgreater{}= 3.5.0), lme4 (\textgreater{}= 1.1{-}8)}
\CommentTok{\#\textgreater{} agridat                                                                                   \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{}                                                                                 Imports}
\CommentTok{\#\textgreater{} abind                                                                    methods, utils}
\CommentTok{\#\textgreater{} ade4                graphics, grDevices, methods, stats, utils, MASS, pixmap, sp,\textbackslash{}nRcpp}
\CommentTok{\#\textgreater{} admisc                                                                          methods}
\CommentTok{\#\textgreater{} AER                                                           stats, Formula (\textgreater{}= 0.2{-}0)}
\CommentTok{\#\textgreater{} afex    pbkrtest (\textgreater{}= 0.4{-}1), lmerTest (\textgreater{}= 3.0{-}0), car, reshape2,\textbackslash{}nstats, methods, utils}
\CommentTok{\#\textgreater{} agridat                                                                            \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{}                   LinkingTo}
\CommentTok{\#\textgreater{} abind                  \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{} ade4    Rcpp, RcppArmadillo}
\CommentTok{\#\textgreater{} admisc                 \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{} AER                    \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{} afex                   \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{} agridat                \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{}                                                                                                                                                                                                                                                                                                                                                                                                Suggests}
\CommentTok{\#\textgreater{} abind                                                                                                                                                                                                                                                                                                                                                                                              \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{} ade4                                                                                                                                                                                                                                                  ade4TkGUI, adegraphics, adephylo, ape, CircStats, deldir,\textbackslash{}nlattice, spdep, splancs, waveslim, progress, foreach, parallel,\textbackslash{}ndoParallel, iterators}
\CommentTok{\#\textgreater{} admisc                                                                                                                                                                                                                                                                                                                                                                                     QCA (\textgreater{}= 3.7)}
\CommentTok{\#\textgreater{} AER                                                                                                                                  boot, dynlm, effects, fGarch, forecast, foreign, ineq,\textbackslash{}nKernSmooth, lattice, longmemo, MASS, mlogit, nlme, nnet, np,\textbackslash{}nplm, pscl, quantreg, rgl, ROCR, rugarch, sampleSelection,\textbackslash{}nscatterplot3d, strucchange, systemfit (\textgreater{}= 1.1{-}20), truncreg,\textbackslash{}ntseries, urca, vars}
\CommentTok{\#\textgreater{} afex    emmeans (\textgreater{}= 1.4), coin, xtable, parallel, plyr, optimx,\textbackslash{}nnloptr, knitr, rmarkdown, R.rsp, lattice, latticeExtra,\textbackslash{}nmultcomp, testthat, mlmRev, dplyr, tidyr, dfoptim, Matrix,\textbackslash{}npsychTools, ggplot2, MEMSS, effects, carData, ggbeeswarm, nlme,\textbackslash{}ncowplot, jtools, ggpubr, ggpol, MASS, glmmTMB, brms, rstanarm,\textbackslash{}nstatmod, performance (\textgreater{}= 0.7.2), see (\textgreater{}= 0.6.4), ez,\textbackslash{}nggResidpanel, grid, vdiffr}
\CommentTok{\#\textgreater{} agridat                    AER, agricolae, betareg, broom, car, coin, corrgram, desplot,\textbackslash{}ndplyr, effects, equivalence, emmeans, FrF2, gam, gge, ggplot2,\textbackslash{}ngnm, gstat, HH, knitr, lattice, latticeExtra, lme4, lucid,\textbackslash{}nmapproj, maps, MASS, MCMCglmm, metafor, mgcv, NADA, nlme,\textbackslash{}nnullabor, ordinal, pbkrtest, pls, pscl, reshape2, rgdal,\textbackslash{}nrmarkdown, qicharts, qtl, sp, SpATS, survival, vcd, testthat}
\CommentTok{\#\textgreater{}         Enhances       License License\_is\_FOSS License\_restricts\_use OS\_type}
\CommentTok{\#\textgreater{} abind       \textless{}NA\textgreater{}   LGPL (\textgreater{}= 2)            \textless{}NA\textgreater{}                  \textless{}NA\textgreater{}    \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{} ade4        \textless{}NA\textgreater{}    GPL (\textgreater{}= 2)            \textless{}NA\textgreater{}                  \textless{}NA\textgreater{}    \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{} admisc      \textless{}NA\textgreater{}    GPL (\textgreater{}= 3)            \textless{}NA\textgreater{}                  \textless{}NA\textgreater{}    \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{} AER         \textless{}NA\textgreater{} GPL{-}2 | GPL{-}3            \textless{}NA\textgreater{}                  \textless{}NA\textgreater{}    \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{} afex        \textless{}NA\textgreater{}    GPL (\textgreater{}= 2)            \textless{}NA\textgreater{}                  \textless{}NA\textgreater{}    \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{} agridat     \textless{}NA\textgreater{}  CC BY{-}SA 4.0            \textless{}NA\textgreater{}                  \textless{}NA\textgreater{}    \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{}         MD5sum NeedsCompilation Built}
\CommentTok{\#\textgreater{} abind     \textless{}NA\textgreater{}               no 4.2.0}
\CommentTok{\#\textgreater{} ade4      \textless{}NA\textgreater{}              yes 4.2.3}
\CommentTok{\#\textgreater{} admisc    \textless{}NA\textgreater{}              yes 4.2.3}
\CommentTok{\#\textgreater{} AER       \textless{}NA\textgreater{}               no 4.2.3}
\CommentTok{\#\textgreater{} afex      \textless{}NA\textgreater{}               no 4.2.3}
\CommentTok{\#\textgreater{} agridat   \textless{}NA\textgreater{}               no 4.2.3}

\FunctionTok{write.csv}\NormalTok{(installed, }\FunctionTok{file.path}\NormalTok{(}\FunctionTok{getwd}\NormalTok{(),}\StringTok{\textquotesingle{}installed.csv\textquotesingle{}}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

After having the \texttt{installed.csv} file on your new or local machine, you can just install the list of packages

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# import the list of packages}
\NormalTok{installed }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{\textquotesingle{}installed.csv\textquotesingle{}}\NormalTok{)}

\CommentTok{\# get the list of packages that you have on your device}
\NormalTok{baseR }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{installed.packages}\NormalTok{())}

\CommentTok{\# install only those that you don\textquotesingle{}t have}
\FunctionTok{install.packages}\NormalTok{(}\FunctionTok{setdiff}\NormalTok{(installed, baseR))}
\end{Highlighting}
\end{Shaded}

\hypertarget{bookdown-cheat-sheet}{%
\chapter{Bookdown cheat sheet}\label{bookdown-cheat-sheet}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# to see non{-}scientific notation a result}
\FunctionTok{format}\NormalTok{(}\FloatTok{12e{-}17}\NormalTok{, }\AttributeTok{scientific =} \ConstantTok{FALSE}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] "0.00000000000000012"}
\end{Highlighting}
\end{Shaded}

\hypertarget{operation}{%
\section{Operation}\label{operation}}

R commands to do derivatives of a defined function Taking derivatives in R involves using the \texttt{expression,} \texttt{D,} and \texttt{eval} functions. You wrap the function you want to take the derivative of in expression(), then use D, then eval as follows.

simple example

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#define a function}
\NormalTok{f}\OtherTok{=}\FunctionTok{expression}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(x))}

\CommentTok{\#take the first derivative}
\NormalTok{df.dx}\OtherTok{=}\FunctionTok{D}\NormalTok{(f,}\StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{)}
\NormalTok{df.dx}
\CommentTok{\#\textgreater{} 0.5 * x\^{}{-}0.5}

\CommentTok{\#take the second derivative}
\NormalTok{d2f.dx2}\OtherTok{=}\FunctionTok{D}\NormalTok{(}\FunctionTok{D}\NormalTok{(f,}\StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{),}\StringTok{\textquotesingle{}x\textquotesingle{}}\NormalTok{)}
\NormalTok{d2f.dx2}
\CommentTok{\#\textgreater{} 0.5 * ({-}0.5 * x\^{}{-}1.5)}
\end{Highlighting}
\end{Shaded}

Evaluate

\begin{itemize}
\tightlist
\item
  The first argument passed to eval is the expression you want to evaluate
\item
  the second is a list containing the values of all quantities that are not defined elsewhere.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#evaluate the function at a given x}
\FunctionTok{eval}\NormalTok{(f,}\FunctionTok{list}\NormalTok{(}\AttributeTok{x=}\DecValTok{3}\NormalTok{))}
\CommentTok{\#\textgreater{} [1] 1.732051}

\CommentTok{\#evaluate the first derivative at a given x}
\FunctionTok{eval}\NormalTok{(df.dx,}\FunctionTok{list}\NormalTok{(}\AttributeTok{x=}\DecValTok{3}\NormalTok{))}
\CommentTok{\#\textgreater{} [1] 0.2886751}

\CommentTok{\#evaluate the second derivative at a given x}
\FunctionTok{eval}\NormalTok{(d2f.dx2,}\FunctionTok{list}\NormalTok{(}\AttributeTok{x=}\DecValTok{3}\NormalTok{))}
\CommentTok{\#\textgreater{} [1] {-}0.04811252}
\end{Highlighting}
\end{Shaded}

\hypertarget{math-expression-syntax}{%
\section{Math Expression/ Syntax}\label{math-expression-syntax}}

\href{http://tug.ctan.org/info/symbols/comprehensive/symbols-a4.pdf}{Full list}

Aligning equations

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textbackslash{}begin\{aligned\}}
\NormalTok{a }\SpecialCharTok{\&} \ErrorTok{=}\NormalTok{ b \textbackslash{}\textbackslash{}}
\NormalTok{X }\SpecialCharTok{\&}\NormalTok{\textbackslash{}sim \{Norm\}(}\DecValTok{10}\NormalTok{, }\DecValTok{3}\NormalTok{) \textbackslash{}\textbackslash{}}
\DecValTok{5} \SpecialCharTok{\&}\NormalTok{ \textbackslash{}le }\DecValTok{10}
\NormalTok{\textbackslash{}end\{aligned\}}
\end{Highlighting}
\end{Shaded}

\[
\begin{aligned}
a & = b \\
X &\sim {Norm}(10, 3) \\
5 & \le 10
\end{aligned}
\]

Cross-reference equation

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textbackslash{}begin\{equation\} }
\NormalTok{a }\OtherTok{=}\NormalTok{ b}
\NormalTok{(\textbackslash{}}\CommentTok{\#eq:test)}
\NormalTok{\textbackslash{}end\{equation\}}
\end{Highlighting}
\end{Shaded}

\begin{equation} 
a = b
\label{eq:test}
\end{equation}

to refer in a sentence \eqref{eq:test} (\texttt{\textbackslash{}@ref(eq:test)})

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5042}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.4958}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Math} Syntax
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Notation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{\$\textbackslash{}pm\$} & \(\pm\) \\
\texttt{\$\textbackslash{}ge\$} & \(\ge\) \\
\texttt{\$\textbackslash{}le\$} & \(\le\) \\
\texttt{\$\textbackslash{}neq\$} & \(\neq\) \\
\texttt{\$\textbackslash{}equiv\$} & \(\equiv\) \\
\texttt{\$\^{}\textbackslash{}circ\$} & \(^\circ\) \\
\texttt{\$\textbackslash{}times\$} & \(\times\) \\
\texttt{\$\textbackslash{}cdot\$} & \(\cdot\) \\
\texttt{\$\textbackslash{}leq\$} & \(\leq\) \\
\texttt{\$\textbackslash{}geq\$} & \(\geq\) \\
\texttt{\textbackslash{}propto} & \(\propto\) \\
\texttt{\$\textbackslash{}subset\$} & \(\subset\) \\
\texttt{\$\textbackslash{}subseteq\$} & \(\subseteq\) \\
\texttt{\$\textbackslash{}leftarrow\$} & \(\leftarrow\) \\
\texttt{\$\textbackslash{}rightarrow\$} & \(\rightarrow\) \\
\texttt{\$\textbackslash{}Leftarrow\$} & \(\Leftarrow\) \\
\texttt{\$\textbackslash{}Rightarrow\$} & \(\Rightarrow\) \\
\texttt{\$\textbackslash{}approx\$} & \(\approx\) \\
\texttt{\$\textbackslash{}mathbb\{R\}\$} & \(\mathbb{R}\) \\
\texttt{\$\textbackslash{}sum\_\{n=1\}\^{}\{10\}\ n\^{}2\$} & \(\sum_{n=1}^{10} n^2\) \\
\texttt{\$\$\textbackslash{}sum\_\{n=1\}\^{}\{10\}\ n\^{}2\$\$} & \(\sum_{n=1}^{10} n^2\) \\
\texttt{\$x\^{}\{n\}\$} & \(x^{n}\) \\
\texttt{\$x\_\{n\}\$} & \(x_{n}\) \\
\texttt{\$\textbackslash{}overline\{x\}\$} & \(\overline{x}\) \\
\texttt{\$\textbackslash{}hat\{x\}\$} & \(\hat{x}\) \\
\texttt{\$\textbackslash{}tilde\{x\}\$} & \(\tilde{x}\) \\
\texttt{\textbackslash{}check\{\}} & \(\check{}\) \\
\texttt{\textbackslash{}underset\{\textbackslash{}gamma\}\{\textbackslash{}operatorname\{argmin\}\}} & \(\underset{\gamma}{\operatorname{argmin}}\) \\
\texttt{\$\textbackslash{}frac\{a\}\{b\}\$} & \(\frac{a}{b}\) \\
\texttt{\$\textbackslash{}frac\{a\}\{b\}\$} & \(\frac{a}{b}\) \\
\texttt{\$\textbackslash{}displaystyle\ \textbackslash{}frac\{a\}\{b\}\$} & \(\displaystyle \frac{a}{b}\) \\
\texttt{\$\textbackslash{}binom\{n\}\{k\}\$} & \(\binom{n}{k}\) \\
\texttt{\$x\_\{1\}\ +\ x\_\{2\}\ +\ \textbackslash{}cdots\ +\ x\_\{n\}\$} & \(x_{1} + x_{2} + \cdots + x_{n}\) \\
\texttt{\$x\_\{1\},\ x\_\{2\},\ \textbackslash{}dots,\ x\_\{n\}\$} & \(x_{1}, x_{2}, \dots, x_{n}\) \\
\texttt{\textbackslash{}mathbf\{x\}\ =\ \textbackslash{}langle\ x\_\{1\},\ x\_\{2\},\ \textbackslash{}dots,\ x\_\{n\}\textbackslash{}rangle\$} & \(\mathbf{x} = \langle x_{1}, x_{2}, \dots, x_{n}\rangle\) \\
\texttt{\$x\ \textbackslash{}in\ A\$} & \(x \in A\) \\
\texttt{\$\textbar{}A\textbar{}\$} & \(|A|\) \\
\texttt{\$x\ \textbackslash{}in\ A\$} & \(x \in A\) \\
\texttt{\$x\ \textbackslash{}subset\ B\$} & \(x \subset B\) \\
\texttt{\$x\ \textbackslash{}subseteq\ B\$} & \(x \subseteq B\) \\
\texttt{\$A\ \textbackslash{}cup\ B\$} & \(A \cup B\) \\
\texttt{\$A\ \textbackslash{}cap\ B\$} & \(A \cap B\) \\
\texttt{\$X\ \textbackslash{}sim\ Binom(n,\ \textbackslash{}pi)\$} & \(X \sim Binom(n, \pi)\) \\
\texttt{\$\textbackslash{}mathrm\{P\}(X\ \textbackslash{}le\ x)\ =\ \textbackslash{}text\{pbinom\}(x,\ n,\ \textbackslash{}pi)\$} & \(\mathrm{P}(X \le x) = \text{pbinom}(x, n, \pi)\) \\
\texttt{\$P(A\ \textbackslash{}mid\ B)\$} & \(P(A \mid B)\) \\
\texttt{\$\textbackslash{}mathrm\{P\}(A\ \textbackslash{}mid\ B)\$} & \(\mathrm{P}(A \mid B)\) \\
\texttt{\$\textbackslash{}\{1,\ 2,\ 3\textbackslash{}\}\$} & \(\{1, 2, 3\}\) \\
\texttt{\$\textbackslash{}sin(x)\$} & \(\sin(x)\) \\
\texttt{\$\textbackslash{}log(x)\$} & \(\log(x)\) \\
\texttt{\$\textbackslash{}int\_\{a\}\^{}\{b\}\$} & \(\int_{a}^{b}\) \\
\texttt{\$\textbackslash{}left(\textbackslash{}int\_\{a\}\^{}\{b\}\ f(x)\ \textbackslash{};\ dx\textbackslash{}right)\$} & \(\left(\int_{a}^{b} f(x) \; dx\right)\) \\
\texttt{\$\textbackslash{}left{[}\textbackslash{}int\_\{\textbackslash{}-infty\}\^{}\{\textbackslash{}infty\}\ f(x)\ \textbackslash{};\ dx\textbackslash{}right{]}\$} & \(\left[\int_{-\infty}^{\infty} f(x) \; dx\right]\) \\
\texttt{\$\textbackslash{}left.\ F(x)\ \textbackslash{}right\textbar{}\_\{a\}\^{}\{b\}\$} & \(\left. F(x) \right|_{a}^{b}\) \\
\texttt{\$\textbackslash{}sum\_\{x\ =\ a\}\^{}\{b\}\ f(x)\$} & \(\sum_{x = a}^{b} f(x)\) \\
\texttt{\$\textbackslash{}prod\_\{x\ =\ a\}\^{}\{b\}\ f(x)\$} & \(\prod_{x = a}^{b} f(x)\) \\
\texttt{\$\textbackslash{}lim\_\{x\ \textbackslash{}to\ \textbackslash{}infty\}\ f(x)\$} & \(\lim_{x \to \infty} f(x)\) \\
\texttt{\$\textbackslash{}displaystyle\ \textbackslash{}lim\_\{x\ \textbackslash{}to\ \textbackslash{}infty\}\ f(x)\$} & \(\displaystyle \lim_{x \to \infty} f(x)\) \\
\textbf{Greek Letters} & \\
\texttt{\$\textbackslash{}alpha\ A\$} & \(\alpha A\) \\
\texttt{\$\textbackslash{}beta\ B\$} & \(\beta B\) \\
\texttt{\$\textbackslash{}gamma\ \textbackslash{}Gamma\$} & \(\gamma \Gamma\) \\
\texttt{\$\textbackslash{}delta\ \textbackslash{}Delta\$} & \(\delta \Delta\) \\
\texttt{\$\textbackslash{}epsilon\ \textbackslash{}varepsilon\ E\$} & \(\epsilon \varepsilon E\) \\
\texttt{\$\textbackslash{}zeta\ Z\ \textbackslash{}sigma\ \$} & \(\zeta Z \sigma\) \\
\texttt{\$\textbackslash{}eta\ H\$} & \(\eta H\) \\
\texttt{\$\textbackslash{}theta\ \textbackslash{}vartheta\ \textbackslash{}Theta\$} & \(\theta \vartheta \Theta\) \\
\texttt{\$\textbackslash{}iota\ I\$} & \(\iota I\) \\
\texttt{\$\textbackslash{}kappa\ K\$} & \(\kappa K\) \\
\texttt{\$\textbackslash{}lambda\ \textbackslash{}Lambda\$} & \(\lambda \Lambda\) \\
\texttt{\$\textbackslash{}mu\ M\$} & \(\mu M\) \\
\texttt{\$\textbackslash{}nu\ N\$} & \(\nu N\) \\
\texttt{\$\textbackslash{}xi\textbackslash{}Xi\$} & \(\xi\Xi\) \\
\texttt{\$o\ O\$} & \(o O\) \\
\texttt{\$\textbackslash{}pi\ \textbackslash{}Pi\$} & \(\pi \Pi\) \\
\texttt{\$\textbackslash{}rho\textbackslash{}varrho\ P\$} & \(\rho\varrho P\) \\
\texttt{\$\textbackslash{}sigma\ \textbackslash{}Sigma\$} & \(\sigma \Sigma\) \\
\texttt{\$\textbackslash{}tau\ T\$} & \(\tau T\) \\
\texttt{\$\textbackslash{}upsilon\ \textbackslash{}Upsilon\$} & \(\upsilon \Upsilon\) \\
\texttt{\$\textbackslash{}phi\ \textbackslash{}varphi\ \textbackslash{}Phi\$} & \(\phi \varphi \Phi\) \\
\texttt{\$\textbackslash{}chi\ X\$} & \(\chi X\) \\
\texttt{\$\textbackslash{}psi\ \textbackslash{}Psi\$} & \(\psi \Psi\) \\
\texttt{\$\textbackslash{}omega\ \textbackslash{}Omega\$} & \(\omega \Omega\) \\
\texttt{\$\textbackslash{}cdot\$} & \(\cdot\) \\
\texttt{\$\textbackslash{}cdots\$} & \(\cdots\) \\
\texttt{\$\textbackslash{}ddots\$} & \(\ddots\) \\
\texttt{\$\textbackslash{}ldots\$} & \(\ldots\) \\
\end{longtable}

Limit \texttt{P(\textbackslash{}lim\_\{n\textbackslash{}to\ \textbackslash{}infty\}\textbackslash{}bar\{X\}\_n\ =\textbackslash{}mu)\ =1}

\[
P(\lim_{n\to \infty}\bar{X}_n =\mu) =1
\]

Matrices

\begin{Shaded}
\begin{Highlighting}[]
\SpecialCharTok{$}\ErrorTok{$}\NormalTok{\textbackslash{}begin\{array\}}
\NormalTok{\{rrr\}}
\DecValTok{1} \SpecialCharTok{\&} \DecValTok{2} \SpecialCharTok{\&} \DecValTok{3}\NormalTok{ \textbackslash{}\textbackslash{}}
\DecValTok{4} \SpecialCharTok{\&} \DecValTok{5} \SpecialCharTok{\&} \DecValTok{6}\NormalTok{ \textbackslash{}\textbackslash{}}
\DecValTok{7} \SpecialCharTok{\&} \DecValTok{8} \SpecialCharTok{\&} \DecValTok{9}
\NormalTok{\textbackslash{}end\{array\}}
\SpecialCharTok{$}\ErrorTok{$}
\end{Highlighting}
\end{Shaded}

\[
\begin{array}
{rrr}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9
\end{array}
\]

\begin{Shaded}
\begin{Highlighting}[]
\SpecialCharTok{$}\ErrorTok{$}\NormalTok{\textbackslash{}mathbf\{X\} }\OtherTok{=}\NormalTok{ \textbackslash{}left[\textbackslash{}begin\{array\}}
\NormalTok{\{rrr\}}
\DecValTok{1} \SpecialCharTok{\&} \DecValTok{2} \SpecialCharTok{\&} \DecValTok{3}\NormalTok{ \textbackslash{}\textbackslash{}}
\DecValTok{4} \SpecialCharTok{\&} \DecValTok{5} \SpecialCharTok{\&} \DecValTok{6}\NormalTok{ \textbackslash{}\textbackslash{}}
\DecValTok{7} \SpecialCharTok{\&} \DecValTok{8} \SpecialCharTok{\&} \DecValTok{9}
\NormalTok{\textbackslash{}end\{array\}\textbackslash{}right]}
\SpecialCharTok{$}\ErrorTok{$}
\end{Highlighting}
\end{Shaded}

\[
\mathbf{X} = \left[\begin{array}
{rrr}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9
\end{array}\right]
\]

Aligning Equations

Aligning Equations with Comments

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textbackslash{}begin\{aligned\}}
    \DecValTok{3}\SpecialCharTok{+}\NormalTok{x }\SpecialCharTok{\&}\ErrorTok{=}\DecValTok{4} \SpecialCharTok{\&\&}\NormalTok{ \textbackslash{}text\{(Solve }\ControlFlowTok{for}\ErrorTok{\}}\NormalTok{ x \textbackslash{}text\{.}\ErrorTok{)}\NormalTok{\}\textbackslash{}}\SpecialCharTok{\textbackslash{}}
\NormalTok{    x }\SpecialCharTok{\&}\ErrorTok{=}\DecValTok{4{-}3} \SpecialCharTok{\&\&}\NormalTok{ \textbackslash{}text\{(Subtract }\DecValTok{3}\NormalTok{ from both sides.)\}\textbackslash{}}\SpecialCharTok{\textbackslash{}}
\NormalTok{    x }\SpecialCharTok{\&}\ErrorTok{=}\DecValTok{1}   \SpecialCharTok{\&\&}\NormalTok{ \textbackslash{}text\{(Yielding the solution.)\}}
\NormalTok{\textbackslash{}end\{aligned\}}
\end{Highlighting}
\end{Shaded}

\[
\begin{aligned}
    3+x &=4 & &\text{(Solve for} x \text{.)} \\
    x &=4-3 && \text{(Subtract 3 from both sides.)} \\
    x &=1   && \text{(Yielding the solution.)}
\end{aligned}
\]

\hypertarget{statistics-notation}{%
\subsection{Statistics Notation}\label{statistics-notation}}

\begin{Shaded}
\begin{Highlighting}[]
\SpecialCharTok{$}\ErrorTok{$}
\FunctionTok{f}\NormalTok{(y}\SpecialCharTok{|}\NormalTok{N,p) }\OtherTok{=}\NormalTok{ \textbackslash{}frac\{N}\SpecialCharTok{!}\NormalTok{\}\{y}\SpecialCharTok{!}\NormalTok{(N}\SpecialCharTok{{-}}\NormalTok{y)}\SpecialCharTok{!}\NormalTok{\}\textbackslash{}cdot p}\SpecialCharTok{\^{}}\NormalTok{y \textbackslash{}}\FunctionTok{cdot}\NormalTok{ (}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{p)}\SpecialCharTok{\^{}}\NormalTok{\{N}\SpecialCharTok{{-}}\NormalTok{y\} }\OtherTok{=}\NormalTok{ \{\{N\}\textbackslash{}choose\{y\}\} \textbackslash{}cdot p}\SpecialCharTok{\^{}}\NormalTok{y \textbackslash{}}\FunctionTok{cdot}\NormalTok{ (}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{p)}\SpecialCharTok{\^{}}\NormalTok{\{N}\SpecialCharTok{{-}}\NormalTok{y\}}
\SpecialCharTok{$}\ErrorTok{$}
\end{Highlighting}
\end{Shaded}

\[
f(y|N,p) = \frac{N!}{y!(N-y)!}\cdot p^y \cdot (1-p)^{N-y} = {{N}\choose{y}} \cdot p^y \cdot (1-p)^{N-y}
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textbackslash{}begin\{cases\}}
\NormalTok{\textbackslash{}frac\{}\DecValTok{1}\NormalTok{\}\{b}\SpecialCharTok{{-}}\NormalTok{a\}}\SpecialCharTok{\&}\NormalTok{\textbackslash{}text\{}\ControlFlowTok{for} \SpecialCharTok{$}\NormalTok{x\textbackslash{}}\ControlFlowTok{in}\NormalTok{[a,b]}\SpecialCharTok{$}\NormalTok{\}\textbackslash{}\textbackslash{}}
\DecValTok{0}\SpecialCharTok{\&}\NormalTok{\textbackslash{}text\{otherwise\}\textbackslash{}\textbackslash{}}
\NormalTok{\textbackslash{}end\{cases\}}
\end{Highlighting}
\end{Shaded}

\[
\begin{cases}
\frac{1}{b-a} & \text{for } x\in[a,b]\\
0 & \text{otherwise}\\
\end{cases}
\]

\hypertarget{table}{%
\section{Table}\label{table}}

\begin{Shaded}
\begin{Highlighting}[]
\SpecialCharTok{+{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}+{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}+{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}+}
\ErrorTok{|}\NormalTok{ Fruit         }\SpecialCharTok{|}\NormalTok{ Price         }\SpecialCharTok{|}\NormalTok{ Advantages         }\SpecialCharTok{|}
\SpecialCharTok{+}\ErrorTok{===============}\SpecialCharTok{+}\ErrorTok{===============}\SpecialCharTok{+}\ErrorTok{====================}\SpecialCharTok{+}
\ErrorTok{|} \ErrorTok{*}\NormalTok{Bananas}\SpecialCharTok{*}     \ErrorTok{|} \ErrorTok{$}\FloatTok{1.34}         \SpecialCharTok{|} \SpecialCharTok{{-}}\NormalTok{ built}\SpecialCharTok{{-}}\ControlFlowTok{in}\NormalTok{ wrapper }\SpecialCharTok{|}
\ErrorTok{|}               \ErrorTok{|}               \ErrorTok{|} \SpecialCharTok{{-}}\NormalTok{ bright color     }\SpecialCharTok{|}
\SpecialCharTok{+{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}+{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}+{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}+}
\ErrorTok{|}\NormalTok{ Oranges       }\SpecialCharTok{|} \ErrorTok{$}\FloatTok{2.10}         \SpecialCharTok{|} \SpecialCharTok{{-}}\NormalTok{ cures scurvy     }\SpecialCharTok{|}
\ErrorTok{|}               \ErrorTok{|}               \ErrorTok{|} \SpecialCharTok{{-}} \ErrorTok{**}\NormalTok{tasty}\SpecialCharTok{**}        \ErrorTok{|}
\SpecialCharTok{+{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}+{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}+{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}+}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2083}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2083}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3194}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Fruit
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Price
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Advantages
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\emph{Bananas} & \$1.34 & \begin{minipage}[t]{\linewidth}\raggedright
\begin{itemize}
\tightlist
\item
  built-in wrapper
\item
  bright color
\end{itemize}
\end{minipage} \\
Oranges & \$2.10 & \begin{minipage}[t]{\linewidth}\raggedright
\begin{itemize}
\tightlist
\item
  cures scurvy
\item
  \textbf{tasty}
\end{itemize}
\end{minipage} \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(\textbackslash{}mathbf\{x\}}\SpecialCharTok{\^{}}\NormalTok{T\textbackslash{}mathbf\{x\})}\SpecialCharTok{\^{}}\NormalTok{\{}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{\}\textbackslash{}mathbf\{x\}}\SpecialCharTok{\^{}}\NormalTok{T\textbackslash{}mathbf\{y\}}
\end{Highlighting}
\end{Shaded}

\((\mathbf{x}^T\mathbf{x})^{-1}\mathbf{x}^T\mathbf{y}\)

  \bibliography{book.bib,packages.bib,references.bib}

\end{document}
